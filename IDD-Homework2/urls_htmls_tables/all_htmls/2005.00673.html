<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2005.00673] PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data</title><meta property="og:description" content="In comparison with person re-identification (ReID), which has been widely studied in the research community, vehicle ReID has received less attention.
Vehicle ReID is challenging due to 1) high intra-class variability …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2005.00673">

<!--Generated on Thu Mar 14 15:01:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification 
<br class="ltx_break">Using Highly Randomized Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zheng Tang  Milind Naphade  Stan Birchfield  Jonathan Tremblay 
<br class="ltx_break">William Hodge  Ratnesh Kumar  Shuo Wang  Xiaodong Yang 
<br class="ltx_break">NVIDIA
</span><span class="ltx_author_notes">Work done as an intern at NVIDIA. Zheng is now with Amazon.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">In comparison with person re-identification (ReID), which has been widely studied in the research community, vehicle ReID has received less attention.
Vehicle ReID is challenging due to 1) high intra-class variability (caused by the dependency of shape and appearance on viewpoint), and 2) small inter-class variability (caused by the similarity in shape and appearance between vehicles produced by different manufacturers).
To address these challenges, we propose a Pose-Aware Multi-Task Re-Identification (PAMTRI) framework.
This approach includes two innovations compared with previous methods.
First, it overcomes viewpoint-dependency by explicitly reasoning about vehicle pose and shape via keypoints, heatmaps and segments from pose estimation.
Second, it jointly classifies semantic vehicle attributes (colors and types) while performing ReID, through multi-task learning with the embedded pose representations.
Since manually labeling images with detailed pose and attribute information is prohibitive, we create a large-scale highly randomized synthetic dataset with automatically annotated vehicle attributes for training.
Extensive experiments validate the effectiveness of each proposed component, showing that PAMTRI achieves significant improvement over state-of-the-art on two mainstream vehicle ReID benchmarks: VeRi and CityFlow-ReID.
Code and models are available at <a target="_blank" href="https://github.com/NVlabs/PAMTRI" title="" class="ltx_ref ltx_href">https://github.com/NVlabs/PAMTRI</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The wide deployment of traffic cameras presents an immense opportunity for video analytics in a variety of applications such as logistics, transportation, and smart cities.
A particularly crucial problem in such analytics is the cross-camera association of targets like pedestrians and vehicles, <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">i.e.</span>, re-identification (ReID), which is illustrated in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2005.00673/assets/fig_illustration.jpg" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="393" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The problem of vehicle ReID involves identifying the same vehicle across different viewing perspectives and cameras, based solely on appearance in the images. Our approach uses multi-task learning to leverage information about the vehicle’s pose and semantic attributes (color and type). Synthetic data play a key role in training, enabling highly detailed annotations to be generated automatically and inexpensively. <span id="S1.F1.3.1" class="ltx_text ltx_font_bold">Best viewed in color.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Although both pedestrians and vehicles are common objects in smart city applications, in recent years most attention has been paid to person ReID.
This is mainly due to the abundance of well-annotated pedestrian data, along with the historical focus of computer vision research on human faces and bodies.
Furthermore, compared to pedestrians, vehicle ReID is arguably more challenging due to high <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">intra-class variability</em> caused by the variety of shapes from different viewing angles, coupled with small <em id="S1.p2.1.2" class="ltx_emph ltx_font_italic">inter-class variability</em> since car models produced by various manufacturers are limited in their shapes and colors.
To verify this intuition, we compared the feature distributions in both person-based and vehicle-based ReID tasks.
Specifically, we used GoogLeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> to extract 1,024-dimensional features from Market1501 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> and CityFlow-ReID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, respectively.
For each dataset, the ratio of the intra- to inter-class variability (based on Euclidean feature distance) was calculated.
The results are as follows: 0.921 for pedestrians (Market1501) and 0.946 for vehicles (CityFlow-ReID), which support the notion that vehicle-based ReID is more difficult.
Although license plates could potentially be useful to identify each vehicle, they often cannot be read from traffic cameras due to occlusion, oblique viewpoint, or low image resolution, and they present privacy concerns.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Recent methods to vehicle ReID exploit feature learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> and/or distance metric learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> to train deep neural networks (DNNs) to distinguish between vehicle pairs, but the current state-of-the-art performance is still far from its counterpart in person ReID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>.
Moreover, it has been shown <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> that directly using state-of-the-art person ReID methods for vehicles does not close this gap, indicating fundamental differences between the two tasks.
We believe the key to vehicle ReID is to exploit viewpoint-invariant information such as color, type, and deformable shape models encoding pose.
To jointly learn these attributes along with pose information, we propose to use synthetic data to overcome the prohibitive cost of manually labeling real training images with such detailed information.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we propose a novel framework named PAMTRI, for Pose-Aware Multi-Task Re-Identification.
Our major contribution is threefold:</p>
</div>
<div id="S1.p5" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">PAMTRI embeds keypoints, heatmaps and segments from pose estimation into the multi-task learning pipeline for vehicle ReID, which guides the network to pay attention to viewpoint-related information.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">PAMTRI is trained with large-scale synthetic data that include randomized vehicle models, color and orientation under different backgrounds, lighting conditions and occlusion. Annotations of vehicle identity, color, type and 2D pose are automatically generated for training.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Our proposed method achieves significant improvement over the state-of-the-art on two mainstream benchmarks: VeRi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and CityFlow-ReID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Additional experiments validate that our unique architecture exploiting explicit pose information, along with our use of randomized synthetic data for training, are key to the method’s success.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Vehicle ReID.</span>
Among the earliest attempts for vehicle ReID that involve deep learning, Liu <em id="S2.p1.1.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> propose a progressive framework that employs a Siamese neural network with contrastive loss for training, and they also introduced VeRi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> as the first large-scale benchmark specifically for vehicle ReID.
Bai <em id="S2.p1.1.4" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.5" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and Kumar <em id="S2.p1.1.6" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.7" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> also take advantage of distance metric learning by extending the success of triplet embedding in person ReID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> to the vehicle-based task.
Especially, the batch-sampling variant from Kumar <em id="S2.p1.1.8" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.9" class="ltx_text"></span> is the current state-of-the-art on both VeRi and CityFlow-ReID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, the latter being a subset of a recent multi-target multi-camera vehicle tracking benchmark.
On the other hand, some methods focus on exploiting viewpoint-invariant features, <em id="S2.p1.1.10" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p1.1.11" class="ltx_text"></span>, the approach by Wang <em id="S2.p1.1.12" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.13" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> that embeds local region features from extracted vehicle keypoints for training with cross-entropy loss.
Similarly, Zhou <em id="S2.p1.1.14" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.15" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> use a generative adversarial network (GAN) to generate multi-view features to be selected by a viewpoint-aware attention model, in which attribute classification is also trained through the discriminative net.
In addition, Yan <em id="S2.p1.1.16" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.17" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> apply multi-task learning to address multi-grain ranking and attribute classification simultaneously, but the search for visually similar vehicles is different from our goal of ReID.
To our knowledge, none of the methods jointly embody pose information and multi-task learning to address vehicle ReID.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Vehicle pose estimation.</span>
Vehicle pose estimation via deformable (<em id="S2.p2.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.p2.1.3" class="ltx_text"></span>, keypoint-based) modeling is a promising approach to deal with viewpoint information.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, Tang <em id="S2.p2.1.4" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p2.1.5" class="ltx_text"></span> propose to use a 16-keypoint-based car model generated from evolutionary optimization to build multiple kernels for 3D tracking.
Ansari <em id="S2.p2.1.6" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p2.1.7" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> designed a more complex vehicle model with 36 keypoints for 3D localization and shape estimation from a dash camera.
The ReID method by Wang <em id="S2.p2.1.8" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p2.1.9" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> also employs a 20-keypoint model to extract orientation-based features for region proposal.
However, instead of explicitly locating keypoint coordinates, their network is trained for estimating response maps only, and semantic attributes are not exploited in their framework.
Other methods can directly regress to the car pose with 6 degrees of freedom (DoF) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, but they are limited for our purposes as detailed vehicle shape modeling via keypoints is not provided.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2005.00673/assets/fig_flowdiagram.jpg" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="196" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of the proposed method. Each training batch includes both real and synthesized images. To embed pose information for multi-task learning, the heatmaps or segments output by a pre-trained network are stacked with the original RGB channels as input. The estimated keypoint coordinates and confidence scores are also concatenated with deep learning features for ReID and attribute (color and type) classification. The pose estimation network (top, blue) is based on HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, while the multi-task learning network (bottom, orange) is based on DenseNet121 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. <span id="S2.F2.3.1" class="ltx_text ltx_font_bold">Best viewed in color.</span></figcaption>
</figure>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Synthetic data.</span>
To generate sufficiently detailed labels on training images, our approach leverages synthetic data.
Our method is trained on a mix of rendered and real images.
This places our work in the context of other research on using simulated data to train DNNs.
A popular approach to overcome the so-called <em id="S2.p3.1.2" class="ltx_emph ltx_font_italic">reality gap</em> is domain randomization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, in which a model is trained with extreme visual variety so that when presented with a real-world image the model treats it as just another variation.
Synthetic data have been successfully applied to a variety of problems, such as optical flow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, car detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, object pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, vision-based robotic manipulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, and robotic control <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.
We extend this research to ReID and semantic attribute understanding.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we describe the algorithmic design of our proposed PAMTRI framework. An overview flow diagram of the proposed system is presented in Fig. <a href="#S2.F2" title="Figure 2 ‣ 2 Related work ‣ PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Randomized synthetic dataset</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Besides vehicle identities, our approach requires additional labels of vehicle attributes and locations of keypoints.
These values, particularly the keypoints, would require considerable, even prohibitive effort, if annotated manually.
To overcome this problem, we generated a large-scale synthetic dataset by employing
our deep learning dataset synthesizer (NDDS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> to create a randomized environment in Unreal Engine 4 (UE4), into which 3D vehicle meshes from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> were imported.
We added to NDDS the ability to label and export specific 3D locations, <em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.p1.1.2" class="ltx_text"></span>, keypoints (denoted as <em id="S3.SS1.p1.1.3" class="ltx_emph ltx_font_italic">sockets</em> in UE4), on a CAD model.
As such we manually annotated each vehicle model with the 36 3D keypoints defined by Ansari <em id="S3.SS1.p1.1.4" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS1.p1.1.5" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>; the projected 2D locations were then output with the synthesized images.
For randomization we used 42 vehicle 3D CAD models with 10 body colors.
To train the data for ReID, we define a unique identity for each combination of vehicle model with a particular color.
The final generated dataset consists of 41,000 unique images with 402 identities,<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The concrete mixer truck and the school bus did not get color variation and as such we exported 500 unique images for each of them. 100 images were generated for each of the remaining identities.</span></span></span> including the following annotations:
keypoints, orientation, and vehicle attributes (color and type).
When generating the dataset, background images were sampled from CityFlow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, and
we also randomized the vehicle position and intensity of light.
Additionally, during training we perform randomized post-processing such as scaling, cropping, horizontal flipping, and adding occlusion.
Some examples are shown in Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.2 Vehicle pose estimation ‣ 3 Methodology ‣ PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Vehicle pose estimation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To leverage viewpoint-aware information for multi-task learning,
we train a robust DNN for extracting pose-related representations.
Similar to Tremblay <em id="S3.SS2.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS2.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> we mix real and synthetic data to bridge the reality gap.
More specifically, in each dataset, we utilize the pre-trained model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> to process sampled images and manually select about 10,000 successful samples as real training data.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Instead of using the stacked hourglass network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> as the backbone like previous approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, we modify the state-of-the-art DNN for human pose estimation, HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, for our purposes.
Compared to the stacked hourglass architecture and other methods that recover high-resolution representations from low-resolution representations, HRNet maintains high-resolution representations and gradually add high-to-low resolution sub-networks with multi-scale fusion.
As a result, the predicted keypoints and heatmaps are more accurate and spatially more precise, which benefit our embedding for multi-task learning.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">We propose two ways to embed the vehicle pose information as additional channels at the input layer of the multi-task network, based on heatmaps and segments, respectively.
In one approach, after the final deconvolutional layer, we extract the 36 heatmaps for each of the keypoints used to capture the vehicle shape and pose.
In the other approach, the predicted keypoint coordinates from the final fully-connected (FC) layer are used to segment the vehicle body. For example, in Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.2 Vehicle pose estimation ‣ 3 Methodology ‣ PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> the keypoints #16, #17, #35 and #34 from the deformable model form a segment that represents the car hood.
Accordingly, we define 13 segmentation masks for each vehicle (see Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.2 Vehicle pose estimation ‣ 3 Methodology ‣ PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> <span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_smallcaps">Top-left</span>), where those formed by keypoint(s) with low confidence are set to be blank.
The feedback of heatmaps or segments from the pose estimation network is then scaled and appended to the original RGB channels for further processing.
We also send the explicit keypoint coordinates and confidence to the multi-task network for further embedding.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2005.00673/assets/fig_vcityflow.jpg" id="S3.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="331" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S3.F3.8.1" class="ltx_text ltx_font_smallcaps">Top-left:</span> The 36-keypoint model from Ansari <em id="S3.F3.9.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.F3.10.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> with 13 segments defined by us. <span id="S3.F3.11.4" class="ltx_text ltx_font_smallcaps">Top-right:</span> 3D keypoints selected in UE4. <span id="S3.F3.12.5" class="ltx_text ltx_font_smallcaps">Bottom:</span> Example images from our randomized synthetic dataset for training, with automatically annotated poses overlaid. <span id="S3.F3.13.6" class="ltx_text ltx_font_bold">Best viewed in color.</span></figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Multi-task learning for vehicle ReID</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Pose-aware representations are beneficial to both ReID and attribute classification tasks.
First, vehicle pose describes the 3D shape model that is invariant to the camera viewpoint, and thus the ReID sub-branch can learn to relate features from different views.
Second, the vehicle shape is directly connected with the car type to which the target belongs.
Third, the segments by 2D keypoints enable the color classification sub-branch to extract the main vehicle color while neglecting the non-painted areas such as windshields and wheels.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Hence, we embed the predicted keypoints and heatmaps (or segments) into our multi-task network to help guide the attention to viewpoint-related representations.
First, all the feedback heatmaps/segments from pose estimation are stacked with the RGB channels of the original input to form a new image.
Accordingly, we modified the first layer of our backbone convolutional neural network (CNN) based on DenseNet121 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> to allow additional input channels.
While we use the pre-trained weights for the RGB channels, the new channels are initialized with Gaussian random weights.
The stacked image provides the DNN with extra information about vehicle shape, and thus helps the feature extraction to concentrate on viewpoint-aware representations.
Both synthesized and real identities are batched together and sent to the backbone CNN.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">To the deep learning feature vector extracted from the final pooling layer, we append the keypoint coordinates and confidence scores from pose prediction, which are normalized between -0.5 and 0.5.
Since the keypoints are explicitly represented and ordered, they enable the neural network to learn a more reliable shape description in the final FC layers for multi-task learning.
Finally, the concatenated feature vector is fed to three separate branches for multi-task learning, including a branch for vehicle ReID and two other branches for color and type classification.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.16" class="ltx_p">The final loss function of our network is the combined loss of the three tasks.
For vehicle ReID, the hard-mining triplet loss is combined with cross-entropy loss to jointly exploit distance metric learning and identity classification, described as follows:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.6" class="ltx_Math" alttext="{\cal L}_{\text{ID}}=\lambda_{\text{htri}}{\cal L}_{\text{htri}}\left(a,p,n\right)+\lambda_{\text{xent}}{\cal L}_{\text{xent}}\left(y,\hat{y}\right)," display="block"><semantics id="S3.E1.m1.6a"><mrow id="S3.E1.m1.6.6.1" xref="S3.E1.m1.6.6.1.1.cmml"><mrow id="S3.E1.m1.6.6.1.1" xref="S3.E1.m1.6.6.1.1.cmml"><msub id="S3.E1.m1.6.6.1.1.2" xref="S3.E1.m1.6.6.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.6.6.1.1.2.2" xref="S3.E1.m1.6.6.1.1.2.2.cmml">ℒ</mi><mtext id="S3.E1.m1.6.6.1.1.2.3" xref="S3.E1.m1.6.6.1.1.2.3a.cmml">ID</mtext></msub><mo id="S3.E1.m1.6.6.1.1.1" xref="S3.E1.m1.6.6.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.6.6.1.1.3" xref="S3.E1.m1.6.6.1.1.3.cmml"><mrow id="S3.E1.m1.6.6.1.1.3.2" xref="S3.E1.m1.6.6.1.1.3.2.cmml"><msub id="S3.E1.m1.6.6.1.1.3.2.2" xref="S3.E1.m1.6.6.1.1.3.2.2.cmml"><mi id="S3.E1.m1.6.6.1.1.3.2.2.2" xref="S3.E1.m1.6.6.1.1.3.2.2.2.cmml">λ</mi><mtext id="S3.E1.m1.6.6.1.1.3.2.2.3" xref="S3.E1.m1.6.6.1.1.3.2.2.3a.cmml">htri</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.1.1.3.2.1" xref="S3.E1.m1.6.6.1.1.3.2.1.cmml">​</mo><msub id="S3.E1.m1.6.6.1.1.3.2.3" xref="S3.E1.m1.6.6.1.1.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.6.6.1.1.3.2.3.2" xref="S3.E1.m1.6.6.1.1.3.2.3.2.cmml">ℒ</mi><mtext id="S3.E1.m1.6.6.1.1.3.2.3.3" xref="S3.E1.m1.6.6.1.1.3.2.3.3a.cmml">htri</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.1.1.3.2.1a" xref="S3.E1.m1.6.6.1.1.3.2.1.cmml">​</mo><mrow id="S3.E1.m1.6.6.1.1.3.2.4.2" xref="S3.E1.m1.6.6.1.1.3.2.4.1.cmml"><mo id="S3.E1.m1.6.6.1.1.3.2.4.2.1" xref="S3.E1.m1.6.6.1.1.3.2.4.1.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">a</mi><mo id="S3.E1.m1.6.6.1.1.3.2.4.2.2" xref="S3.E1.m1.6.6.1.1.3.2.4.1.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">p</mi><mo id="S3.E1.m1.6.6.1.1.3.2.4.2.3" xref="S3.E1.m1.6.6.1.1.3.2.4.1.cmml">,</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">n</mi><mo id="S3.E1.m1.6.6.1.1.3.2.4.2.4" xref="S3.E1.m1.6.6.1.1.3.2.4.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.6.6.1.1.3.1" xref="S3.E1.m1.6.6.1.1.3.1.cmml">+</mo><mrow id="S3.E1.m1.6.6.1.1.3.3" xref="S3.E1.m1.6.6.1.1.3.3.cmml"><msub id="S3.E1.m1.6.6.1.1.3.3.2" xref="S3.E1.m1.6.6.1.1.3.3.2.cmml"><mi id="S3.E1.m1.6.6.1.1.3.3.2.2" xref="S3.E1.m1.6.6.1.1.3.3.2.2.cmml">λ</mi><mtext id="S3.E1.m1.6.6.1.1.3.3.2.3" xref="S3.E1.m1.6.6.1.1.3.3.2.3a.cmml">xent</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.1.1.3.3.1" xref="S3.E1.m1.6.6.1.1.3.3.1.cmml">​</mo><msub id="S3.E1.m1.6.6.1.1.3.3.3" xref="S3.E1.m1.6.6.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.6.6.1.1.3.3.3.2" xref="S3.E1.m1.6.6.1.1.3.3.3.2.cmml">ℒ</mi><mtext id="S3.E1.m1.6.6.1.1.3.3.3.3" xref="S3.E1.m1.6.6.1.1.3.3.3.3a.cmml">xent</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.1.1.3.3.1a" xref="S3.E1.m1.6.6.1.1.3.3.1.cmml">​</mo><mrow id="S3.E1.m1.6.6.1.1.3.3.4.2" xref="S3.E1.m1.6.6.1.1.3.3.4.1.cmml"><mo id="S3.E1.m1.6.6.1.1.3.3.4.2.1" xref="S3.E1.m1.6.6.1.1.3.3.4.1.cmml">(</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">y</mi><mo id="S3.E1.m1.6.6.1.1.3.3.4.2.2" xref="S3.E1.m1.6.6.1.1.3.3.4.1.cmml">,</mo><mover accent="true" id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml"><mi id="S3.E1.m1.5.5.2" xref="S3.E1.m1.5.5.2.cmml">y</mi><mo id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.cmml">^</mo></mover><mo id="S3.E1.m1.6.6.1.1.3.3.4.2.3" xref="S3.E1.m1.6.6.1.1.3.3.4.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.6.6.1.2" xref="S3.E1.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.6b"><apply id="S3.E1.m1.6.6.1.1.cmml" xref="S3.E1.m1.6.6.1"><eq id="S3.E1.m1.6.6.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1"></eq><apply id="S3.E1.m1.6.6.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.2.1.cmml" xref="S3.E1.m1.6.6.1.1.2">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.2.2.cmml" xref="S3.E1.m1.6.6.1.1.2.2">ℒ</ci><ci id="S3.E1.m1.6.6.1.1.2.3a.cmml" xref="S3.E1.m1.6.6.1.1.2.3"><mtext mathsize="70%" id="S3.E1.m1.6.6.1.1.2.3.cmml" xref="S3.E1.m1.6.6.1.1.2.3">ID</mtext></ci></apply><apply id="S3.E1.m1.6.6.1.1.3.cmml" xref="S3.E1.m1.6.6.1.1.3"><plus id="S3.E1.m1.6.6.1.1.3.1.cmml" xref="S3.E1.m1.6.6.1.1.3.1"></plus><apply id="S3.E1.m1.6.6.1.1.3.2.cmml" xref="S3.E1.m1.6.6.1.1.3.2"><times id="S3.E1.m1.6.6.1.1.3.2.1.cmml" xref="S3.E1.m1.6.6.1.1.3.2.1"></times><apply id="S3.E1.m1.6.6.1.1.3.2.2.cmml" xref="S3.E1.m1.6.6.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.3.2.2.1.cmml" xref="S3.E1.m1.6.6.1.1.3.2.2">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.3.2.2.2.cmml" xref="S3.E1.m1.6.6.1.1.3.2.2.2">𝜆</ci><ci id="S3.E1.m1.6.6.1.1.3.2.2.3a.cmml" xref="S3.E1.m1.6.6.1.1.3.2.2.3"><mtext mathsize="70%" id="S3.E1.m1.6.6.1.1.3.2.2.3.cmml" xref="S3.E1.m1.6.6.1.1.3.2.2.3">htri</mtext></ci></apply><apply id="S3.E1.m1.6.6.1.1.3.2.3.cmml" xref="S3.E1.m1.6.6.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.3.2.3.1.cmml" xref="S3.E1.m1.6.6.1.1.3.2.3">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.3.2.3.2.cmml" xref="S3.E1.m1.6.6.1.1.3.2.3.2">ℒ</ci><ci id="S3.E1.m1.6.6.1.1.3.2.3.3a.cmml" xref="S3.E1.m1.6.6.1.1.3.2.3.3"><mtext mathsize="70%" id="S3.E1.m1.6.6.1.1.3.2.3.3.cmml" xref="S3.E1.m1.6.6.1.1.3.2.3.3">htri</mtext></ci></apply><vector id="S3.E1.m1.6.6.1.1.3.2.4.1.cmml" xref="S3.E1.m1.6.6.1.1.3.2.4.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑎</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝑝</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝑛</ci></vector></apply><apply id="S3.E1.m1.6.6.1.1.3.3.cmml" xref="S3.E1.m1.6.6.1.1.3.3"><times id="S3.E1.m1.6.6.1.1.3.3.1.cmml" xref="S3.E1.m1.6.6.1.1.3.3.1"></times><apply id="S3.E1.m1.6.6.1.1.3.3.2.cmml" xref="S3.E1.m1.6.6.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.3.3.2.1.cmml" xref="S3.E1.m1.6.6.1.1.3.3.2">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.3.3.2.2.cmml" xref="S3.E1.m1.6.6.1.1.3.3.2.2">𝜆</ci><ci id="S3.E1.m1.6.6.1.1.3.3.2.3a.cmml" xref="S3.E1.m1.6.6.1.1.3.3.2.3"><mtext mathsize="70%" id="S3.E1.m1.6.6.1.1.3.3.2.3.cmml" xref="S3.E1.m1.6.6.1.1.3.3.2.3">xent</mtext></ci></apply><apply id="S3.E1.m1.6.6.1.1.3.3.3.cmml" xref="S3.E1.m1.6.6.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.3.3.3.1.cmml" xref="S3.E1.m1.6.6.1.1.3.3.3">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.3.3.3.2.cmml" xref="S3.E1.m1.6.6.1.1.3.3.3.2">ℒ</ci><ci id="S3.E1.m1.6.6.1.1.3.3.3.3a.cmml" xref="S3.E1.m1.6.6.1.1.3.3.3.3"><mtext mathsize="70%" id="S3.E1.m1.6.6.1.1.3.3.3.3.cmml" xref="S3.E1.m1.6.6.1.1.3.3.3.3">xent</mtext></ci></apply><interval closure="open" id="S3.E1.m1.6.6.1.1.3.3.4.1.cmml" xref="S3.E1.m1.6.6.1.1.3.3.4.2"><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">𝑦</ci><apply id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5"><ci id="S3.E1.m1.5.5.1.cmml" xref="S3.E1.m1.5.5.1">^</ci><ci id="S3.E1.m1.5.5.2.cmml" xref="S3.E1.m1.5.5.2">𝑦</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.6c">{\cal L}_{\text{ID}}=\lambda_{\text{htri}}{\cal L}_{\text{htri}}\left(a,p,n\right)+\lambda_{\text{xent}}{\cal L}_{\text{xent}}\left(y,\hat{y}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p4.4" class="ltx_p">where <math id="S3.SS3.p4.1.m1.3" class="ltx_Math" alttext="{\cal L}_{\text{htri}}\left(a,p,n\right)" display="inline"><semantics id="S3.SS3.p4.1.m1.3a"><mrow id="S3.SS3.p4.1.m1.3.4" xref="S3.SS3.p4.1.m1.3.4.cmml"><msub id="S3.SS3.p4.1.m1.3.4.2" xref="S3.SS3.p4.1.m1.3.4.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p4.1.m1.3.4.2.2" xref="S3.SS3.p4.1.m1.3.4.2.2.cmml">ℒ</mi><mtext id="S3.SS3.p4.1.m1.3.4.2.3" xref="S3.SS3.p4.1.m1.3.4.2.3a.cmml">htri</mtext></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p4.1.m1.3.4.1" xref="S3.SS3.p4.1.m1.3.4.1.cmml">​</mo><mrow id="S3.SS3.p4.1.m1.3.4.3.2" xref="S3.SS3.p4.1.m1.3.4.3.1.cmml"><mo id="S3.SS3.p4.1.m1.3.4.3.2.1" xref="S3.SS3.p4.1.m1.3.4.3.1.cmml">(</mo><mi id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">a</mi><mo id="S3.SS3.p4.1.m1.3.4.3.2.2" xref="S3.SS3.p4.1.m1.3.4.3.1.cmml">,</mo><mi id="S3.SS3.p4.1.m1.2.2" xref="S3.SS3.p4.1.m1.2.2.cmml">p</mi><mo id="S3.SS3.p4.1.m1.3.4.3.2.3" xref="S3.SS3.p4.1.m1.3.4.3.1.cmml">,</mo><mi id="S3.SS3.p4.1.m1.3.3" xref="S3.SS3.p4.1.m1.3.3.cmml">n</mi><mo id="S3.SS3.p4.1.m1.3.4.3.2.4" xref="S3.SS3.p4.1.m1.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.3b"><apply id="S3.SS3.p4.1.m1.3.4.cmml" xref="S3.SS3.p4.1.m1.3.4"><times id="S3.SS3.p4.1.m1.3.4.1.cmml" xref="S3.SS3.p4.1.m1.3.4.1"></times><apply id="S3.SS3.p4.1.m1.3.4.2.cmml" xref="S3.SS3.p4.1.m1.3.4.2"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.3.4.2.1.cmml" xref="S3.SS3.p4.1.m1.3.4.2">subscript</csymbol><ci id="S3.SS3.p4.1.m1.3.4.2.2.cmml" xref="S3.SS3.p4.1.m1.3.4.2.2">ℒ</ci><ci id="S3.SS3.p4.1.m1.3.4.2.3a.cmml" xref="S3.SS3.p4.1.m1.3.4.2.3"><mtext mathsize="70%" id="S3.SS3.p4.1.m1.3.4.2.3.cmml" xref="S3.SS3.p4.1.m1.3.4.2.3">htri</mtext></ci></apply><vector id="S3.SS3.p4.1.m1.3.4.3.1.cmml" xref="S3.SS3.p4.1.m1.3.4.3.2"><ci id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">𝑎</ci><ci id="S3.SS3.p4.1.m1.2.2.cmml" xref="S3.SS3.p4.1.m1.2.2">𝑝</ci><ci id="S3.SS3.p4.1.m1.3.3.cmml" xref="S3.SS3.p4.1.m1.3.3">𝑛</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.3c">{\cal L}_{\text{htri}}\left(a,p,n\right)</annotation></semantics></math> is the hard triplet loss with <math id="S3.SS3.p4.2.m2.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS3.p4.2.m2.1a"><mi id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><ci id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">a</annotation></semantics></math>, <math id="S3.SS3.p4.3.m3.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.SS3.p4.3.m3.1a"><mi id="S3.SS3.p4.3.m3.1.1" xref="S3.SS3.p4.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><ci id="S3.SS3.p4.3.m3.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">p</annotation></semantics></math> and <math id="S3.SS3.p4.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS3.p4.4.m4.1a"><mi id="S3.SS3.p4.4.m4.1.1" xref="S3.SS3.p4.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.4.m4.1b"><ci id="S3.SS3.p4.4.m4.1.1.cmml" xref="S3.SS3.p4.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.4.m4.1c">n</annotation></semantics></math> as anchor, positive and negative samples, respectively:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.6" class="ltx_Math" alttext="{\cal L}_{\text{htri}}\left(a,p,n\right)=\left[\alpha+\max{\left(D_{ap}\right)}-\min{\left(D_{an}\right)}\right]_{+}," display="block"><semantics id="S3.E2.m1.6a"><mrow id="S3.E2.m1.6.6.1" xref="S3.E2.m1.6.6.1.1.cmml"><mrow id="S3.E2.m1.6.6.1.1" xref="S3.E2.m1.6.6.1.1.cmml"><mrow id="S3.E2.m1.6.6.1.1.3" xref="S3.E2.m1.6.6.1.1.3.cmml"><msub id="S3.E2.m1.6.6.1.1.3.2" xref="S3.E2.m1.6.6.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.6.6.1.1.3.2.2" xref="S3.E2.m1.6.6.1.1.3.2.2.cmml">ℒ</mi><mtext id="S3.E2.m1.6.6.1.1.3.2.3" xref="S3.E2.m1.6.6.1.1.3.2.3a.cmml">htri</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.6.6.1.1.3.1" xref="S3.E2.m1.6.6.1.1.3.1.cmml">​</mo><mrow id="S3.E2.m1.6.6.1.1.3.3.2" xref="S3.E2.m1.6.6.1.1.3.3.1.cmml"><mo id="S3.E2.m1.6.6.1.1.3.3.2.1" xref="S3.E2.m1.6.6.1.1.3.3.1.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">a</mi><mo id="S3.E2.m1.6.6.1.1.3.3.2.2" xref="S3.E2.m1.6.6.1.1.3.3.1.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">p</mi><mo id="S3.E2.m1.6.6.1.1.3.3.2.3" xref="S3.E2.m1.6.6.1.1.3.3.1.cmml">,</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">n</mi><mo id="S3.E2.m1.6.6.1.1.3.3.2.4" xref="S3.E2.m1.6.6.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.6.6.1.1.2" xref="S3.E2.m1.6.6.1.1.2.cmml">=</mo><msub id="S3.E2.m1.6.6.1.1.1" xref="S3.E2.m1.6.6.1.1.1.cmml"><mrow id="S3.E2.m1.6.6.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.2.cmml"><mo id="S3.E2.m1.6.6.1.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.2.1.cmml">[</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.3.cmml">α</mi><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.cmml">+</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">max</mi><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1a" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.2.cmml">(</mo><msub id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.2.cmml">D</mi><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">p</mi></mrow></msub><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.3.cmml">−</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1.2.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E2.m1.5.5" xref="S3.E2.m1.5.5.cmml">min</mi><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.2.1a" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.cmml">⁡</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.cmml"><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.cmml">(</mo><msub id="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.cmml">D</mi><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.3.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.3.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.3.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.3.1.cmml">​</mo><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.3.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.3.3.cmml">n</mi></mrow></msub><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.6.6.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.2.1.cmml">]</mo></mrow><mo id="S3.E2.m1.6.6.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.3.cmml">+</mo></msub></mrow><mo id="S3.E2.m1.6.6.1.2" xref="S3.E2.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.6b"><apply id="S3.E2.m1.6.6.1.1.cmml" xref="S3.E2.m1.6.6.1"><eq id="S3.E2.m1.6.6.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.2"></eq><apply id="S3.E2.m1.6.6.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.3"><times id="S3.E2.m1.6.6.1.1.3.1.cmml" xref="S3.E2.m1.6.6.1.1.3.1"></times><apply id="S3.E2.m1.6.6.1.1.3.2.cmml" xref="S3.E2.m1.6.6.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.3.2.1.cmml" xref="S3.E2.m1.6.6.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.3.2.2.cmml" xref="S3.E2.m1.6.6.1.1.3.2.2">ℒ</ci><ci id="S3.E2.m1.6.6.1.1.3.2.3a.cmml" xref="S3.E2.m1.6.6.1.1.3.2.3"><mtext mathsize="70%" id="S3.E2.m1.6.6.1.1.3.2.3.cmml" xref="S3.E2.m1.6.6.1.1.3.2.3">htri</mtext></ci></apply><vector id="S3.E2.m1.6.6.1.1.3.3.1.cmml" xref="S3.E2.m1.6.6.1.1.3.3.2"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑎</ci><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝑝</ci><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">𝑛</ci></vector></apply><apply id="S3.E2.m1.6.6.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1">subscript</csymbol><apply id="S3.E2.m1.6.6.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.6.6.1.1.1.1.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1"><minus id="S3.E2.m1.6.6.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.3"></minus><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1"><plus id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.2"></plus><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.3">𝛼</ci><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1"><max id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4"></max><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.2">𝐷</ci><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3"><times id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑎</ci><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑝</ci></apply></apply></apply></apply><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.1"><min id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5"></min><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.2">𝐷</ci><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.3"><times id="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.3.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.3.1"></times><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.3.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.3.2">𝑎</ci><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.3.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.1.1.3.3">𝑛</ci></apply></apply></apply></apply></apply><plus id="S3.E2.m1.6.6.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.3"></plus></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.6c">{\cal L}_{\text{htri}}\left(a,p,n\right)=\left[\alpha+\max{\left(D_{ap}\right)}-\min{\left(D_{an}\right)}\right]_{+},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p4.10" class="ltx_p">in which <math id="S3.SS3.p4.5.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS3.p4.5.m1.1a"><mi id="S3.SS3.p4.5.m1.1.1" xref="S3.SS3.p4.5.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.5.m1.1b"><ci id="S3.SS3.p4.5.m1.1.1.cmml" xref="S3.SS3.p4.5.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.5.m1.1c">\alpha</annotation></semantics></math> is the distance margin, <math id="S3.SS3.p4.6.m2.1" class="ltx_Math" alttext="D_{ap}" display="inline"><semantics id="S3.SS3.p4.6.m2.1a"><msub id="S3.SS3.p4.6.m2.1.1" xref="S3.SS3.p4.6.m2.1.1.cmml"><mi id="S3.SS3.p4.6.m2.1.1.2" xref="S3.SS3.p4.6.m2.1.1.2.cmml">D</mi><mrow id="S3.SS3.p4.6.m2.1.1.3" xref="S3.SS3.p4.6.m2.1.1.3.cmml"><mi id="S3.SS3.p4.6.m2.1.1.3.2" xref="S3.SS3.p4.6.m2.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.6.m2.1.1.3.1" xref="S3.SS3.p4.6.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p4.6.m2.1.1.3.3" xref="S3.SS3.p4.6.m2.1.1.3.3.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.6.m2.1b"><apply id="S3.SS3.p4.6.m2.1.1.cmml" xref="S3.SS3.p4.6.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.6.m2.1.1.1.cmml" xref="S3.SS3.p4.6.m2.1.1">subscript</csymbol><ci id="S3.SS3.p4.6.m2.1.1.2.cmml" xref="S3.SS3.p4.6.m2.1.1.2">𝐷</ci><apply id="S3.SS3.p4.6.m2.1.1.3.cmml" xref="S3.SS3.p4.6.m2.1.1.3"><times id="S3.SS3.p4.6.m2.1.1.3.1.cmml" xref="S3.SS3.p4.6.m2.1.1.3.1"></times><ci id="S3.SS3.p4.6.m2.1.1.3.2.cmml" xref="S3.SS3.p4.6.m2.1.1.3.2">𝑎</ci><ci id="S3.SS3.p4.6.m2.1.1.3.3.cmml" xref="S3.SS3.p4.6.m2.1.1.3.3">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.6.m2.1c">D_{ap}</annotation></semantics></math> and <math id="S3.SS3.p4.7.m3.1" class="ltx_Math" alttext="D_{an}" display="inline"><semantics id="S3.SS3.p4.7.m3.1a"><msub id="S3.SS3.p4.7.m3.1.1" xref="S3.SS3.p4.7.m3.1.1.cmml"><mi id="S3.SS3.p4.7.m3.1.1.2" xref="S3.SS3.p4.7.m3.1.1.2.cmml">D</mi><mrow id="S3.SS3.p4.7.m3.1.1.3" xref="S3.SS3.p4.7.m3.1.1.3.cmml"><mi id="S3.SS3.p4.7.m3.1.1.3.2" xref="S3.SS3.p4.7.m3.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.7.m3.1.1.3.1" xref="S3.SS3.p4.7.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p4.7.m3.1.1.3.3" xref="S3.SS3.p4.7.m3.1.1.3.3.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.7.m3.1b"><apply id="S3.SS3.p4.7.m3.1.1.cmml" xref="S3.SS3.p4.7.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.7.m3.1.1.1.cmml" xref="S3.SS3.p4.7.m3.1.1">subscript</csymbol><ci id="S3.SS3.p4.7.m3.1.1.2.cmml" xref="S3.SS3.p4.7.m3.1.1.2">𝐷</ci><apply id="S3.SS3.p4.7.m3.1.1.3.cmml" xref="S3.SS3.p4.7.m3.1.1.3"><times id="S3.SS3.p4.7.m3.1.1.3.1.cmml" xref="S3.SS3.p4.7.m3.1.1.3.1"></times><ci id="S3.SS3.p4.7.m3.1.1.3.2.cmml" xref="S3.SS3.p4.7.m3.1.1.3.2">𝑎</ci><ci id="S3.SS3.p4.7.m3.1.1.3.3.cmml" xref="S3.SS3.p4.7.m3.1.1.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.7.m3.1c">D_{an}</annotation></semantics></math> are the distance metrics between the anchor and all positive/negative samples in feature space, and <math id="S3.SS3.p4.8.m4.1" class="ltx_Math" alttext="[\cdot]_{+}" display="inline"><semantics id="S3.SS3.p4.8.m4.1a"><msub id="S3.SS3.p4.8.m4.1.2" xref="S3.SS3.p4.8.m4.1.2.cmml"><mrow id="S3.SS3.p4.8.m4.1.2.2.2" xref="S3.SS3.p4.8.m4.1.2.2.1.cmml"><mo stretchy="false" id="S3.SS3.p4.8.m4.1.2.2.2.1" xref="S3.SS3.p4.8.m4.1.2.2.1.1.cmml">[</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p4.8.m4.1.1" xref="S3.SS3.p4.8.m4.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS3.p4.8.m4.1.2.2.2.2" xref="S3.SS3.p4.8.m4.1.2.2.1.1.cmml">]</mo></mrow><mo id="S3.SS3.p4.8.m4.1.2.3" xref="S3.SS3.p4.8.m4.1.2.3.cmml">+</mo></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.8.m4.1b"><apply id="S3.SS3.p4.8.m4.1.2.cmml" xref="S3.SS3.p4.8.m4.1.2"><csymbol cd="ambiguous" id="S3.SS3.p4.8.m4.1.2.1.cmml" xref="S3.SS3.p4.8.m4.1.2">subscript</csymbol><apply id="S3.SS3.p4.8.m4.1.2.2.1.cmml" xref="S3.SS3.p4.8.m4.1.2.2.2"><csymbol cd="latexml" id="S3.SS3.p4.8.m4.1.2.2.1.1.cmml" xref="S3.SS3.p4.8.m4.1.2.2.2.1">delimited-[]</csymbol><ci id="S3.SS3.p4.8.m4.1.1.cmml" xref="S3.SS3.p4.8.m4.1.1">⋅</ci></apply><plus id="S3.SS3.p4.8.m4.1.2.3.cmml" xref="S3.SS3.p4.8.m4.1.2.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.8.m4.1c">[\cdot]_{+}</annotation></semantics></math> indicates <math id="S3.SS3.p4.9.m5.3" class="ltx_Math" alttext="\max(\cdot,0)" display="inline"><semantics id="S3.SS3.p4.9.m5.3a"><mrow id="S3.SS3.p4.9.m5.3.4.2" xref="S3.SS3.p4.9.m5.3.4.1.cmml"><mi id="S3.SS3.p4.9.m5.1.1" xref="S3.SS3.p4.9.m5.1.1.cmml">max</mi><mo id="S3.SS3.p4.9.m5.3.4.2a" xref="S3.SS3.p4.9.m5.3.4.1.cmml">⁡</mo><mrow id="S3.SS3.p4.9.m5.3.4.2.1" xref="S3.SS3.p4.9.m5.3.4.1.cmml"><mo stretchy="false" id="S3.SS3.p4.9.m5.3.4.2.1.1" xref="S3.SS3.p4.9.m5.3.4.1.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p4.9.m5.2.2" xref="S3.SS3.p4.9.m5.2.2.cmml">⋅</mo><mo id="S3.SS3.p4.9.m5.3.4.2.1.2" xref="S3.SS3.p4.9.m5.3.4.1.cmml">,</mo><mn id="S3.SS3.p4.9.m5.3.3" xref="S3.SS3.p4.9.m5.3.3.cmml">0</mn><mo stretchy="false" id="S3.SS3.p4.9.m5.3.4.2.1.3" xref="S3.SS3.p4.9.m5.3.4.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.9.m5.3b"><apply id="S3.SS3.p4.9.m5.3.4.1.cmml" xref="S3.SS3.p4.9.m5.3.4.2"><max id="S3.SS3.p4.9.m5.1.1.cmml" xref="S3.SS3.p4.9.m5.1.1"></max><ci id="S3.SS3.p4.9.m5.2.2.cmml" xref="S3.SS3.p4.9.m5.2.2">⋅</ci><cn type="integer" id="S3.SS3.p4.9.m5.3.3.cmml" xref="S3.SS3.p4.9.m5.3.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.9.m5.3c">\max(\cdot,0)</annotation></semantics></math>; and <math id="S3.SS3.p4.10.m6.2" class="ltx_Math" alttext="{\cal L}_{\text{xent}}\left(y,\hat{y}\right)" display="inline"><semantics id="S3.SS3.p4.10.m6.2a"><mrow id="S3.SS3.p4.10.m6.2.3" xref="S3.SS3.p4.10.m6.2.3.cmml"><msub id="S3.SS3.p4.10.m6.2.3.2" xref="S3.SS3.p4.10.m6.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p4.10.m6.2.3.2.2" xref="S3.SS3.p4.10.m6.2.3.2.2.cmml">ℒ</mi><mtext id="S3.SS3.p4.10.m6.2.3.2.3" xref="S3.SS3.p4.10.m6.2.3.2.3a.cmml">xent</mtext></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p4.10.m6.2.3.1" xref="S3.SS3.p4.10.m6.2.3.1.cmml">​</mo><mrow id="S3.SS3.p4.10.m6.2.3.3.2" xref="S3.SS3.p4.10.m6.2.3.3.1.cmml"><mo id="S3.SS3.p4.10.m6.2.3.3.2.1" xref="S3.SS3.p4.10.m6.2.3.3.1.cmml">(</mo><mi id="S3.SS3.p4.10.m6.1.1" xref="S3.SS3.p4.10.m6.1.1.cmml">y</mi><mo id="S3.SS3.p4.10.m6.2.3.3.2.2" xref="S3.SS3.p4.10.m6.2.3.3.1.cmml">,</mo><mover accent="true" id="S3.SS3.p4.10.m6.2.2" xref="S3.SS3.p4.10.m6.2.2.cmml"><mi id="S3.SS3.p4.10.m6.2.2.2" xref="S3.SS3.p4.10.m6.2.2.2.cmml">y</mi><mo id="S3.SS3.p4.10.m6.2.2.1" xref="S3.SS3.p4.10.m6.2.2.1.cmml">^</mo></mover><mo id="S3.SS3.p4.10.m6.2.3.3.2.3" xref="S3.SS3.p4.10.m6.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.10.m6.2b"><apply id="S3.SS3.p4.10.m6.2.3.cmml" xref="S3.SS3.p4.10.m6.2.3"><times id="S3.SS3.p4.10.m6.2.3.1.cmml" xref="S3.SS3.p4.10.m6.2.3.1"></times><apply id="S3.SS3.p4.10.m6.2.3.2.cmml" xref="S3.SS3.p4.10.m6.2.3.2"><csymbol cd="ambiguous" id="S3.SS3.p4.10.m6.2.3.2.1.cmml" xref="S3.SS3.p4.10.m6.2.3.2">subscript</csymbol><ci id="S3.SS3.p4.10.m6.2.3.2.2.cmml" xref="S3.SS3.p4.10.m6.2.3.2.2">ℒ</ci><ci id="S3.SS3.p4.10.m6.2.3.2.3a.cmml" xref="S3.SS3.p4.10.m6.2.3.2.3"><mtext mathsize="70%" id="S3.SS3.p4.10.m6.2.3.2.3.cmml" xref="S3.SS3.p4.10.m6.2.3.2.3">xent</mtext></ci></apply><interval closure="open" id="S3.SS3.p4.10.m6.2.3.3.1.cmml" xref="S3.SS3.p4.10.m6.2.3.3.2"><ci id="S3.SS3.p4.10.m6.1.1.cmml" xref="S3.SS3.p4.10.m6.1.1">𝑦</ci><apply id="S3.SS3.p4.10.m6.2.2.cmml" xref="S3.SS3.p4.10.m6.2.2"><ci id="S3.SS3.p4.10.m6.2.2.1.cmml" xref="S3.SS3.p4.10.m6.2.2.1">^</ci><ci id="S3.SS3.p4.10.m6.2.2.2.cmml" xref="S3.SS3.p4.10.m6.2.2.2">𝑦</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.10.m6.2c">{\cal L}_{\text{xent}}\left(y,\hat{y}\right)</annotation></semantics></math> is the cross entropy loss:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.4" class="ltx_Math" alttext="{\cal L}_{\text{xent}}\left(y,\hat{y}\right)=-\frac{1}{N}\sum_{i=1}^{N}y_{i}\log{\left(\hat{y}_{i}\right)}," display="block"><semantics id="S3.E3.m1.4a"><mrow id="S3.E3.m1.4.4.1" xref="S3.E3.m1.4.4.1.1.cmml"><mrow id="S3.E3.m1.4.4.1.1" xref="S3.E3.m1.4.4.1.1.cmml"><mrow id="S3.E3.m1.4.4.1.1.3" xref="S3.E3.m1.4.4.1.1.3.cmml"><msub id="S3.E3.m1.4.4.1.1.3.2" xref="S3.E3.m1.4.4.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.4.4.1.1.3.2.2" xref="S3.E3.m1.4.4.1.1.3.2.2.cmml">ℒ</mi><mtext id="S3.E3.m1.4.4.1.1.3.2.3" xref="S3.E3.m1.4.4.1.1.3.2.3a.cmml">xent</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.1.1.3.1" xref="S3.E3.m1.4.4.1.1.3.1.cmml">​</mo><mrow id="S3.E3.m1.4.4.1.1.3.3.2" xref="S3.E3.m1.4.4.1.1.3.3.1.cmml"><mo id="S3.E3.m1.4.4.1.1.3.3.2.1" xref="S3.E3.m1.4.4.1.1.3.3.1.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">y</mi><mo id="S3.E3.m1.4.4.1.1.3.3.2.2" xref="S3.E3.m1.4.4.1.1.3.3.1.cmml">,</mo><mover accent="true" id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml"><mi id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml">y</mi><mo id="S3.E3.m1.2.2.1" xref="S3.E3.m1.2.2.1.cmml">^</mo></mover><mo id="S3.E3.m1.4.4.1.1.3.3.2.3" xref="S3.E3.m1.4.4.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.4.4.1.1.2" xref="S3.E3.m1.4.4.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.4.4.1.1.1" xref="S3.E3.m1.4.4.1.1.1.cmml"><mo id="S3.E3.m1.4.4.1.1.1a" xref="S3.E3.m1.4.4.1.1.1.cmml">−</mo><mrow id="S3.E3.m1.4.4.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.cmml"><mfrac id="S3.E3.m1.4.4.1.1.1.1.3" xref="S3.E3.m1.4.4.1.1.1.1.3.cmml"><mn id="S3.E3.m1.4.4.1.1.1.1.3.2" xref="S3.E3.m1.4.4.1.1.1.1.3.2.cmml">1</mn><mi id="S3.E3.m1.4.4.1.1.1.1.3.3" xref="S3.E3.m1.4.4.1.1.1.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.1.1.1.1.2" xref="S3.E3.m1.4.4.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.4.4.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.cmml"><munderover id="S3.E3.m1.4.4.1.1.1.1.1.2" xref="S3.E3.m1.4.4.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E3.m1.4.4.1.1.1.1.1.2.2.2" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E3.m1.4.4.1.1.1.1.1.2.2.3" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E3.m1.4.4.1.1.1.1.1.2.2.3.2" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E3.m1.4.4.1.1.1.1.1.2.2.3.1" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E3.m1.4.4.1.1.1.1.1.2.2.3.3" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E3.m1.4.4.1.1.1.1.1.2.3" xref="S3.E3.m1.4.4.1.1.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S3.E3.m1.4.4.1.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.4.4.1.1.1.1.1.1.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.3.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.2.cmml">y</mi><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0.167em" rspace="0em" id="S3.E3.m1.4.4.1.1.1.1.1.1.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.3.3" xref="S3.E3.m1.3.3.cmml">log</mi><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1a" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.2.cmml">(</mo><msub id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.2.cmml">y</mi><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow><mo id="S3.E3.m1.4.4.1.2" xref="S3.E3.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.4b"><apply id="S3.E3.m1.4.4.1.1.cmml" xref="S3.E3.m1.4.4.1"><eq id="S3.E3.m1.4.4.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.2"></eq><apply id="S3.E3.m1.4.4.1.1.3.cmml" xref="S3.E3.m1.4.4.1.1.3"><times id="S3.E3.m1.4.4.1.1.3.1.cmml" xref="S3.E3.m1.4.4.1.1.3.1"></times><apply id="S3.E3.m1.4.4.1.1.3.2.cmml" xref="S3.E3.m1.4.4.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.3.2.1.cmml" xref="S3.E3.m1.4.4.1.1.3.2">subscript</csymbol><ci id="S3.E3.m1.4.4.1.1.3.2.2.cmml" xref="S3.E3.m1.4.4.1.1.3.2.2">ℒ</ci><ci id="S3.E3.m1.4.4.1.1.3.2.3a.cmml" xref="S3.E3.m1.4.4.1.1.3.2.3"><mtext mathsize="70%" id="S3.E3.m1.4.4.1.1.3.2.3.cmml" xref="S3.E3.m1.4.4.1.1.3.2.3">xent</mtext></ci></apply><interval closure="open" id="S3.E3.m1.4.4.1.1.3.3.1.cmml" xref="S3.E3.m1.4.4.1.1.3.3.2"><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝑦</ci><apply id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2"><ci id="S3.E3.m1.2.2.1.cmml" xref="S3.E3.m1.2.2.1">^</ci><ci id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2">𝑦</ci></apply></interval></apply><apply id="S3.E3.m1.4.4.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1"><minus id="S3.E3.m1.4.4.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1"></minus><apply id="S3.E3.m1.4.4.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1"><times id="S3.E3.m1.4.4.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.2"></times><apply id="S3.E3.m1.4.4.1.1.1.1.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.3"><divide id="S3.E3.m1.4.4.1.1.1.1.3.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.3"></divide><cn type="integer" id="S3.E3.m1.4.4.1.1.1.1.3.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.3.2">1</cn><ci id="S3.E3.m1.4.4.1.1.1.1.3.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.3.3">𝑁</ci></apply><apply id="S3.E3.m1.4.4.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1"><apply id="S3.E3.m1.4.4.1.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E3.m1.4.4.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.1.1.1.2.2.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.2"></sum><apply id="S3.E3.m1.4.4.1.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.3"><eq id="S3.E3.m1.4.4.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E3.m1.4.4.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E3.m1.4.4.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E3.m1.4.4.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.2.3">𝑁</ci></apply><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1"><times id="S3.E3.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.2"></times><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.2">𝑦</ci><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.3.3">𝑖</ci></apply><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1"><log id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3"></log><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2"><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.1">^</ci><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.2.2">𝑦</ci></apply><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.4c">{\cal L}_{\text{xent}}\left(y,\hat{y}\right)=-\frac{1}{N}\sum_{i=1}^{N}y_{i}\log{\left(\hat{y}_{i}\right)},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p4.15" class="ltx_p">where <math id="S3.SS3.p4.11.m1.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS3.p4.11.m1.1a"><mi id="S3.SS3.p4.11.m1.1.1" xref="S3.SS3.p4.11.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.11.m1.1b"><ci id="S3.SS3.p4.11.m1.1.1.cmml" xref="S3.SS3.p4.11.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.11.m1.1c">y</annotation></semantics></math> is the ground-truth vector, <math id="S3.SS3.p4.12.m2.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="S3.SS3.p4.12.m2.1a"><mover accent="true" id="S3.SS3.p4.12.m2.1.1" xref="S3.SS3.p4.12.m2.1.1.cmml"><mi id="S3.SS3.p4.12.m2.1.1.2" xref="S3.SS3.p4.12.m2.1.1.2.cmml">y</mi><mo id="S3.SS3.p4.12.m2.1.1.1" xref="S3.SS3.p4.12.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.12.m2.1b"><apply id="S3.SS3.p4.12.m2.1.1.cmml" xref="S3.SS3.p4.12.m2.1.1"><ci id="S3.SS3.p4.12.m2.1.1.1.cmml" xref="S3.SS3.p4.12.m2.1.1.1">^</ci><ci id="S3.SS3.p4.12.m2.1.1.2.cmml" xref="S3.SS3.p4.12.m2.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.12.m2.1c">\hat{y}</annotation></semantics></math> is the estimation, and <math id="S3.SS3.p4.13.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS3.p4.13.m3.1a"><mi id="S3.SS3.p4.13.m3.1.1" xref="S3.SS3.p4.13.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.13.m3.1b"><ci id="S3.SS3.p4.13.m3.1.1.cmml" xref="S3.SS3.p4.13.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.13.m3.1c">N</annotation></semantics></math> is the number of classes (in our case IDs). In Eq. (<a href="#S3.E1" title="In 3.3 Multi-task learning for vehicle ReID ‣ 3 Methodology ‣ PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), <math id="S3.SS3.p4.14.m4.1" class="ltx_Math" alttext="\lambda_{\text{htri}}" display="inline"><semantics id="S3.SS3.p4.14.m4.1a"><msub id="S3.SS3.p4.14.m4.1.1" xref="S3.SS3.p4.14.m4.1.1.cmml"><mi id="S3.SS3.p4.14.m4.1.1.2" xref="S3.SS3.p4.14.m4.1.1.2.cmml">λ</mi><mtext id="S3.SS3.p4.14.m4.1.1.3" xref="S3.SS3.p4.14.m4.1.1.3a.cmml">htri</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.14.m4.1b"><apply id="S3.SS3.p4.14.m4.1.1.cmml" xref="S3.SS3.p4.14.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.14.m4.1.1.1.cmml" xref="S3.SS3.p4.14.m4.1.1">subscript</csymbol><ci id="S3.SS3.p4.14.m4.1.1.2.cmml" xref="S3.SS3.p4.14.m4.1.1.2">𝜆</ci><ci id="S3.SS3.p4.14.m4.1.1.3a.cmml" xref="S3.SS3.p4.14.m4.1.1.3"><mtext mathsize="70%" id="S3.SS3.p4.14.m4.1.1.3.cmml" xref="S3.SS3.p4.14.m4.1.1.3">htri</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.14.m4.1c">\lambda_{\text{htri}}</annotation></semantics></math> and <math id="S3.SS3.p4.15.m5.1" class="ltx_Math" alttext="\lambda_{\text{xent}}" display="inline"><semantics id="S3.SS3.p4.15.m5.1a"><msub id="S3.SS3.p4.15.m5.1.1" xref="S3.SS3.p4.15.m5.1.1.cmml"><mi id="S3.SS3.p4.15.m5.1.1.2" xref="S3.SS3.p4.15.m5.1.1.2.cmml">λ</mi><mtext id="S3.SS3.p4.15.m5.1.1.3" xref="S3.SS3.p4.15.m5.1.1.3a.cmml">xent</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.15.m5.1b"><apply id="S3.SS3.p4.15.m5.1.1.cmml" xref="S3.SS3.p4.15.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.15.m5.1.1.1.cmml" xref="S3.SS3.p4.15.m5.1.1">subscript</csymbol><ci id="S3.SS3.p4.15.m5.1.1.2.cmml" xref="S3.SS3.p4.15.m5.1.1.2">𝜆</ci><ci id="S3.SS3.p4.15.m5.1.1.3a.cmml" xref="S3.SS3.p4.15.m5.1.1.3"><mtext mathsize="70%" id="S3.SS3.p4.15.m5.1.1.3.cmml" xref="S3.SS3.p4.15.m5.1.1.3">xent</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.15.m5.1c">\lambda_{\text{xent}}</annotation></semantics></math> are the regularization factors, both set to 1.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">For the other two sub-tasks of attribute classification, we again employ the cross-entropy loss as follows:</p>
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4.m1.1" class="ltx_Math" alttext="\displaystyle{\cal L}_{\text{color}}" display="inline"><semantics id="S3.E4.m1.1a"><msub id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml">ℒ</mi><mtext id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3a.cmml">color</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1">subscript</csymbol><ci id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2">ℒ</ci><ci id="S3.E4.m1.1.1.3a.cmml" xref="S3.E4.m1.1.1.3"><mtext mathsize="70%" id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3">color</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\displaystyle{\cal L}_{\text{color}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E4.m2.1" class="ltx_Math" alttext="\displaystyle={\cal L}_{\text{xent}}\left(y_{\text{color}},\hat{y}_{\text{color}}\right)," display="inline"><semantics id="S3.E4.m2.1a"><mrow id="S3.E4.m2.1.1.1" xref="S3.E4.m2.1.1.1.1.cmml"><mrow id="S3.E4.m2.1.1.1.1" xref="S3.E4.m2.1.1.1.1.cmml"><mi id="S3.E4.m2.1.1.1.1.4" xref="S3.E4.m2.1.1.1.1.4.cmml"></mi><mo id="S3.E4.m2.1.1.1.1.3" xref="S3.E4.m2.1.1.1.1.3.cmml">=</mo><mrow id="S3.E4.m2.1.1.1.1.2" xref="S3.E4.m2.1.1.1.1.2.cmml"><msub id="S3.E4.m2.1.1.1.1.2.4" xref="S3.E4.m2.1.1.1.1.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m2.1.1.1.1.2.4.2" xref="S3.E4.m2.1.1.1.1.2.4.2.cmml">ℒ</mi><mtext id="S3.E4.m2.1.1.1.1.2.4.3" xref="S3.E4.m2.1.1.1.1.2.4.3a.cmml">xent</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E4.m2.1.1.1.1.2.3" xref="S3.E4.m2.1.1.1.1.2.3.cmml">​</mo><mrow id="S3.E4.m2.1.1.1.1.2.2.2" xref="S3.E4.m2.1.1.1.1.2.2.3.cmml"><mo id="S3.E4.m2.1.1.1.1.2.2.2.3" xref="S3.E4.m2.1.1.1.1.2.2.3.cmml">(</mo><msub id="S3.E4.m2.1.1.1.1.1.1.1.1" xref="S3.E4.m2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m2.1.1.1.1.1.1.1.1.2" xref="S3.E4.m2.1.1.1.1.1.1.1.1.2.cmml">y</mi><mtext id="S3.E4.m2.1.1.1.1.1.1.1.1.3" xref="S3.E4.m2.1.1.1.1.1.1.1.1.3a.cmml">color</mtext></msub><mo id="S3.E4.m2.1.1.1.1.2.2.2.4" xref="S3.E4.m2.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.E4.m2.1.1.1.1.2.2.2.2" xref="S3.E4.m2.1.1.1.1.2.2.2.2.cmml"><mover accent="true" id="S3.E4.m2.1.1.1.1.2.2.2.2.2" xref="S3.E4.m2.1.1.1.1.2.2.2.2.2.cmml"><mi id="S3.E4.m2.1.1.1.1.2.2.2.2.2.2" xref="S3.E4.m2.1.1.1.1.2.2.2.2.2.2.cmml">y</mi><mo id="S3.E4.m2.1.1.1.1.2.2.2.2.2.1" xref="S3.E4.m2.1.1.1.1.2.2.2.2.2.1.cmml">^</mo></mover><mtext id="S3.E4.m2.1.1.1.1.2.2.2.2.3" xref="S3.E4.m2.1.1.1.1.2.2.2.2.3a.cmml">color</mtext></msub><mo id="S3.E4.m2.1.1.1.1.2.2.2.5" xref="S3.E4.m2.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4.m2.1.1.1.2" xref="S3.E4.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m2.1b"><apply id="S3.E4.m2.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1"><eq id="S3.E4.m2.1.1.1.1.3.cmml" xref="S3.E4.m2.1.1.1.1.3"></eq><csymbol cd="latexml" id="S3.E4.m2.1.1.1.1.4.cmml" xref="S3.E4.m2.1.1.1.1.4">absent</csymbol><apply id="S3.E4.m2.1.1.1.1.2.cmml" xref="S3.E4.m2.1.1.1.1.2"><times id="S3.E4.m2.1.1.1.1.2.3.cmml" xref="S3.E4.m2.1.1.1.1.2.3"></times><apply id="S3.E4.m2.1.1.1.1.2.4.cmml" xref="S3.E4.m2.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.2.4.1.cmml" xref="S3.E4.m2.1.1.1.1.2.4">subscript</csymbol><ci id="S3.E4.m2.1.1.1.1.2.4.2.cmml" xref="S3.E4.m2.1.1.1.1.2.4.2">ℒ</ci><ci id="S3.E4.m2.1.1.1.1.2.4.3a.cmml" xref="S3.E4.m2.1.1.1.1.2.4.3"><mtext mathsize="70%" id="S3.E4.m2.1.1.1.1.2.4.3.cmml" xref="S3.E4.m2.1.1.1.1.2.4.3">xent</mtext></ci></apply><interval closure="open" id="S3.E4.m2.1.1.1.1.2.2.3.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2"><apply id="S3.E4.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.2">𝑦</ci><ci id="S3.E4.m2.1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.3"><mtext mathsize="70%" id="S3.E4.m2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.3">color</mtext></ci></apply><apply id="S3.E4.m2.1.1.1.1.2.2.2.2.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.2">subscript</csymbol><apply id="S3.E4.m2.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.2.2"><ci id="S3.E4.m2.1.1.1.1.2.2.2.2.2.1.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.2.2.1">^</ci><ci id="S3.E4.m2.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.2.2.2">𝑦</ci></apply><ci id="S3.E4.m2.1.1.1.1.2.2.2.2.3a.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.2.3"><mtext mathsize="70%" id="S3.E4.m2.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.2.3">color</mtext></ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m2.1c">\displaystyle={\cal L}_{\text{xent}}\left(y_{\text{color}},\hat{y}_{\text{color}}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E5.m1.1" class="ltx_Math" alttext="\displaystyle{\cal L}_{\text{type}}" display="inline"><semantics id="S3.E5.m1.1a"><msub id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.1.1.2" xref="S3.E5.m1.1.1.2.cmml">ℒ</mi><mtext id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.3a.cmml">type</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1">subscript</csymbol><ci id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.2">ℒ</ci><ci id="S3.E5.m1.1.1.3a.cmml" xref="S3.E5.m1.1.1.3"><mtext mathsize="70%" id="S3.E5.m1.1.1.3.cmml" xref="S3.E5.m1.1.1.3">type</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\displaystyle{\cal L}_{\text{type}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E5.m2.1" class="ltx_Math" alttext="\displaystyle={\cal L}_{\text{xent}}\left(y_{\text{type}},\hat{y}_{\text{type}}\right)." display="inline"><semantics id="S3.E5.m2.1a"><mrow id="S3.E5.m2.1.1.1" xref="S3.E5.m2.1.1.1.1.cmml"><mrow id="S3.E5.m2.1.1.1.1" xref="S3.E5.m2.1.1.1.1.cmml"><mi id="S3.E5.m2.1.1.1.1.4" xref="S3.E5.m2.1.1.1.1.4.cmml"></mi><mo id="S3.E5.m2.1.1.1.1.3" xref="S3.E5.m2.1.1.1.1.3.cmml">=</mo><mrow id="S3.E5.m2.1.1.1.1.2" xref="S3.E5.m2.1.1.1.1.2.cmml"><msub id="S3.E5.m2.1.1.1.1.2.4" xref="S3.E5.m2.1.1.1.1.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m2.1.1.1.1.2.4.2" xref="S3.E5.m2.1.1.1.1.2.4.2.cmml">ℒ</mi><mtext id="S3.E5.m2.1.1.1.1.2.4.3" xref="S3.E5.m2.1.1.1.1.2.4.3a.cmml">xent</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E5.m2.1.1.1.1.2.3" xref="S3.E5.m2.1.1.1.1.2.3.cmml">​</mo><mrow id="S3.E5.m2.1.1.1.1.2.2.2" xref="S3.E5.m2.1.1.1.1.2.2.3.cmml"><mo id="S3.E5.m2.1.1.1.1.2.2.2.3" xref="S3.E5.m2.1.1.1.1.2.2.3.cmml">(</mo><msub id="S3.E5.m2.1.1.1.1.1.1.1.1" xref="S3.E5.m2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E5.m2.1.1.1.1.1.1.1.1.2" xref="S3.E5.m2.1.1.1.1.1.1.1.1.2.cmml">y</mi><mtext id="S3.E5.m2.1.1.1.1.1.1.1.1.3" xref="S3.E5.m2.1.1.1.1.1.1.1.1.3a.cmml">type</mtext></msub><mo id="S3.E5.m2.1.1.1.1.2.2.2.4" xref="S3.E5.m2.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.E5.m2.1.1.1.1.2.2.2.2" xref="S3.E5.m2.1.1.1.1.2.2.2.2.cmml"><mover accent="true" id="S3.E5.m2.1.1.1.1.2.2.2.2.2" xref="S3.E5.m2.1.1.1.1.2.2.2.2.2.cmml"><mi id="S3.E5.m2.1.1.1.1.2.2.2.2.2.2" xref="S3.E5.m2.1.1.1.1.2.2.2.2.2.2.cmml">y</mi><mo id="S3.E5.m2.1.1.1.1.2.2.2.2.2.1" xref="S3.E5.m2.1.1.1.1.2.2.2.2.2.1.cmml">^</mo></mover><mtext id="S3.E5.m2.1.1.1.1.2.2.2.2.3" xref="S3.E5.m2.1.1.1.1.2.2.2.2.3a.cmml">type</mtext></msub><mo id="S3.E5.m2.1.1.1.1.2.2.2.5" xref="S3.E5.m2.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E5.m2.1.1.1.2" xref="S3.E5.m2.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m2.1b"><apply id="S3.E5.m2.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1"><eq id="S3.E5.m2.1.1.1.1.3.cmml" xref="S3.E5.m2.1.1.1.1.3"></eq><csymbol cd="latexml" id="S3.E5.m2.1.1.1.1.4.cmml" xref="S3.E5.m2.1.1.1.1.4">absent</csymbol><apply id="S3.E5.m2.1.1.1.1.2.cmml" xref="S3.E5.m2.1.1.1.1.2"><times id="S3.E5.m2.1.1.1.1.2.3.cmml" xref="S3.E5.m2.1.1.1.1.2.3"></times><apply id="S3.E5.m2.1.1.1.1.2.4.cmml" xref="S3.E5.m2.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S3.E5.m2.1.1.1.1.2.4.1.cmml" xref="S3.E5.m2.1.1.1.1.2.4">subscript</csymbol><ci id="S3.E5.m2.1.1.1.1.2.4.2.cmml" xref="S3.E5.m2.1.1.1.1.2.4.2">ℒ</ci><ci id="S3.E5.m2.1.1.1.1.2.4.3a.cmml" xref="S3.E5.m2.1.1.1.1.2.4.3"><mtext mathsize="70%" id="S3.E5.m2.1.1.1.1.2.4.3.cmml" xref="S3.E5.m2.1.1.1.1.2.4.3">xent</mtext></ci></apply><interval closure="open" id="S3.E5.m2.1.1.1.1.2.2.3.cmml" xref="S3.E5.m2.1.1.1.1.2.2.2"><apply id="S3.E5.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E5.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1.2">𝑦</ci><ci id="S3.E5.m2.1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1.3"><mtext mathsize="70%" id="S3.E5.m2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1.3">type</mtext></ci></apply><apply id="S3.E5.m2.1.1.1.1.2.2.2.2.cmml" xref="S3.E5.m2.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E5.m2.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E5.m2.1.1.1.1.2.2.2.2">subscript</csymbol><apply id="S3.E5.m2.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E5.m2.1.1.1.1.2.2.2.2.2"><ci id="S3.E5.m2.1.1.1.1.2.2.2.2.2.1.cmml" xref="S3.E5.m2.1.1.1.1.2.2.2.2.2.1">^</ci><ci id="S3.E5.m2.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E5.m2.1.1.1.1.2.2.2.2.2.2">𝑦</ci></apply><ci id="S3.E5.m2.1.1.1.1.2.2.2.2.3a.cmml" xref="S3.E5.m2.1.1.1.1.2.2.2.2.3"><mtext mathsize="70%" id="S3.E5.m2.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E5.m2.1.1.1.1.2.2.2.2.3">type</mtext></ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m2.1c">\displaystyle={\cal L}_{\text{xent}}\left(y_{\text{type}},\hat{y}_{\text{type}}\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.5" class="ltx_p">The final loss is the weighted combination of all tasks:</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.3" class="ltx_Math" alttext="{\cal L}\left(\theta,{\cal X}\right)={\cal L}_{\text{ID}}+\lambda_{\text{color}}{\cal L}_{\text{color}}+\lambda_{\text{type}}{\cal L}_{\text{type}}," display="block"><semantics id="S3.E6.m1.3a"><mrow id="S3.E6.m1.3.3.1" xref="S3.E6.m1.3.3.1.1.cmml"><mrow id="S3.E6.m1.3.3.1.1" xref="S3.E6.m1.3.3.1.1.cmml"><mrow id="S3.E6.m1.3.3.1.1.2" xref="S3.E6.m1.3.3.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.3.3.1.1.2.2" xref="S3.E6.m1.3.3.1.1.2.2.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.3.3.1.1.2.1" xref="S3.E6.m1.3.3.1.1.2.1.cmml">​</mo><mrow id="S3.E6.m1.3.3.1.1.2.3.2" xref="S3.E6.m1.3.3.1.1.2.3.1.cmml"><mo id="S3.E6.m1.3.3.1.1.2.3.2.1" xref="S3.E6.m1.3.3.1.1.2.3.1.cmml">(</mo><mi id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml">θ</mi><mo id="S3.E6.m1.3.3.1.1.2.3.2.2" xref="S3.E6.m1.3.3.1.1.2.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.2.2" xref="S3.E6.m1.2.2.cmml">𝒳</mi><mo id="S3.E6.m1.3.3.1.1.2.3.2.3" xref="S3.E6.m1.3.3.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E6.m1.3.3.1.1.1" xref="S3.E6.m1.3.3.1.1.1.cmml">=</mo><mrow id="S3.E6.m1.3.3.1.1.3" xref="S3.E6.m1.3.3.1.1.3.cmml"><msub id="S3.E6.m1.3.3.1.1.3.2" xref="S3.E6.m1.3.3.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.3.3.1.1.3.2.2" xref="S3.E6.m1.3.3.1.1.3.2.2.cmml">ℒ</mi><mtext id="S3.E6.m1.3.3.1.1.3.2.3" xref="S3.E6.m1.3.3.1.1.3.2.3a.cmml">ID</mtext></msub><mo id="S3.E6.m1.3.3.1.1.3.1" xref="S3.E6.m1.3.3.1.1.3.1.cmml">+</mo><mrow id="S3.E6.m1.3.3.1.1.3.3" xref="S3.E6.m1.3.3.1.1.3.3.cmml"><msub id="S3.E6.m1.3.3.1.1.3.3.2" xref="S3.E6.m1.3.3.1.1.3.3.2.cmml"><mi id="S3.E6.m1.3.3.1.1.3.3.2.2" xref="S3.E6.m1.3.3.1.1.3.3.2.2.cmml">λ</mi><mtext id="S3.E6.m1.3.3.1.1.3.3.2.3" xref="S3.E6.m1.3.3.1.1.3.3.2.3a.cmml">color</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E6.m1.3.3.1.1.3.3.1" xref="S3.E6.m1.3.3.1.1.3.3.1.cmml">​</mo><msub id="S3.E6.m1.3.3.1.1.3.3.3" xref="S3.E6.m1.3.3.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.3.3.1.1.3.3.3.2" xref="S3.E6.m1.3.3.1.1.3.3.3.2.cmml">ℒ</mi><mtext id="S3.E6.m1.3.3.1.1.3.3.3.3" xref="S3.E6.m1.3.3.1.1.3.3.3.3a.cmml">color</mtext></msub></mrow><mo id="S3.E6.m1.3.3.1.1.3.1a" xref="S3.E6.m1.3.3.1.1.3.1.cmml">+</mo><mrow id="S3.E6.m1.3.3.1.1.3.4" xref="S3.E6.m1.3.3.1.1.3.4.cmml"><msub id="S3.E6.m1.3.3.1.1.3.4.2" xref="S3.E6.m1.3.3.1.1.3.4.2.cmml"><mi id="S3.E6.m1.3.3.1.1.3.4.2.2" xref="S3.E6.m1.3.3.1.1.3.4.2.2.cmml">λ</mi><mtext id="S3.E6.m1.3.3.1.1.3.4.2.3" xref="S3.E6.m1.3.3.1.1.3.4.2.3a.cmml">type</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E6.m1.3.3.1.1.3.4.1" xref="S3.E6.m1.3.3.1.1.3.4.1.cmml">​</mo><msub id="S3.E6.m1.3.3.1.1.3.4.3" xref="S3.E6.m1.3.3.1.1.3.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.3.3.1.1.3.4.3.2" xref="S3.E6.m1.3.3.1.1.3.4.3.2.cmml">ℒ</mi><mtext id="S3.E6.m1.3.3.1.1.3.4.3.3" xref="S3.E6.m1.3.3.1.1.3.4.3.3a.cmml">type</mtext></msub></mrow></mrow></mrow><mo id="S3.E6.m1.3.3.1.2" xref="S3.E6.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.3b"><apply id="S3.E6.m1.3.3.1.1.cmml" xref="S3.E6.m1.3.3.1"><eq id="S3.E6.m1.3.3.1.1.1.cmml" xref="S3.E6.m1.3.3.1.1.1"></eq><apply id="S3.E6.m1.3.3.1.1.2.cmml" xref="S3.E6.m1.3.3.1.1.2"><times id="S3.E6.m1.3.3.1.1.2.1.cmml" xref="S3.E6.m1.3.3.1.1.2.1"></times><ci id="S3.E6.m1.3.3.1.1.2.2.cmml" xref="S3.E6.m1.3.3.1.1.2.2">ℒ</ci><interval closure="open" id="S3.E6.m1.3.3.1.1.2.3.1.cmml" xref="S3.E6.m1.3.3.1.1.2.3.2"><ci id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1">𝜃</ci><ci id="S3.E6.m1.2.2.cmml" xref="S3.E6.m1.2.2">𝒳</ci></interval></apply><apply id="S3.E6.m1.3.3.1.1.3.cmml" xref="S3.E6.m1.3.3.1.1.3"><plus id="S3.E6.m1.3.3.1.1.3.1.cmml" xref="S3.E6.m1.3.3.1.1.3.1"></plus><apply id="S3.E6.m1.3.3.1.1.3.2.cmml" xref="S3.E6.m1.3.3.1.1.3.2"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.3.2.1.cmml" xref="S3.E6.m1.3.3.1.1.3.2">subscript</csymbol><ci id="S3.E6.m1.3.3.1.1.3.2.2.cmml" xref="S3.E6.m1.3.3.1.1.3.2.2">ℒ</ci><ci id="S3.E6.m1.3.3.1.1.3.2.3a.cmml" xref="S3.E6.m1.3.3.1.1.3.2.3"><mtext mathsize="70%" id="S3.E6.m1.3.3.1.1.3.2.3.cmml" xref="S3.E6.m1.3.3.1.1.3.2.3">ID</mtext></ci></apply><apply id="S3.E6.m1.3.3.1.1.3.3.cmml" xref="S3.E6.m1.3.3.1.1.3.3"><times id="S3.E6.m1.3.3.1.1.3.3.1.cmml" xref="S3.E6.m1.3.3.1.1.3.3.1"></times><apply id="S3.E6.m1.3.3.1.1.3.3.2.cmml" xref="S3.E6.m1.3.3.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.3.3.2.1.cmml" xref="S3.E6.m1.3.3.1.1.3.3.2">subscript</csymbol><ci id="S3.E6.m1.3.3.1.1.3.3.2.2.cmml" xref="S3.E6.m1.3.3.1.1.3.3.2.2">𝜆</ci><ci id="S3.E6.m1.3.3.1.1.3.3.2.3a.cmml" xref="S3.E6.m1.3.3.1.1.3.3.2.3"><mtext mathsize="70%" id="S3.E6.m1.3.3.1.1.3.3.2.3.cmml" xref="S3.E6.m1.3.3.1.1.3.3.2.3">color</mtext></ci></apply><apply id="S3.E6.m1.3.3.1.1.3.3.3.cmml" xref="S3.E6.m1.3.3.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.3.3.3.1.cmml" xref="S3.E6.m1.3.3.1.1.3.3.3">subscript</csymbol><ci id="S3.E6.m1.3.3.1.1.3.3.3.2.cmml" xref="S3.E6.m1.3.3.1.1.3.3.3.2">ℒ</ci><ci id="S3.E6.m1.3.3.1.1.3.3.3.3a.cmml" xref="S3.E6.m1.3.3.1.1.3.3.3.3"><mtext mathsize="70%" id="S3.E6.m1.3.3.1.1.3.3.3.3.cmml" xref="S3.E6.m1.3.3.1.1.3.3.3.3">color</mtext></ci></apply></apply><apply id="S3.E6.m1.3.3.1.1.3.4.cmml" xref="S3.E6.m1.3.3.1.1.3.4"><times id="S3.E6.m1.3.3.1.1.3.4.1.cmml" xref="S3.E6.m1.3.3.1.1.3.4.1"></times><apply id="S3.E6.m1.3.3.1.1.3.4.2.cmml" xref="S3.E6.m1.3.3.1.1.3.4.2"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.3.4.2.1.cmml" xref="S3.E6.m1.3.3.1.1.3.4.2">subscript</csymbol><ci id="S3.E6.m1.3.3.1.1.3.4.2.2.cmml" xref="S3.E6.m1.3.3.1.1.3.4.2.2">𝜆</ci><ci id="S3.E6.m1.3.3.1.1.3.4.2.3a.cmml" xref="S3.E6.m1.3.3.1.1.3.4.2.3"><mtext mathsize="70%" id="S3.E6.m1.3.3.1.1.3.4.2.3.cmml" xref="S3.E6.m1.3.3.1.1.3.4.2.3">type</mtext></ci></apply><apply id="S3.E6.m1.3.3.1.1.3.4.3.cmml" xref="S3.E6.m1.3.3.1.1.3.4.3"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.1.1.3.4.3.1.cmml" xref="S3.E6.m1.3.3.1.1.3.4.3">subscript</csymbol><ci id="S3.E6.m1.3.3.1.1.3.4.3.2.cmml" xref="S3.E6.m1.3.3.1.1.3.4.3.2">ℒ</ci><ci id="S3.E6.m1.3.3.1.1.3.4.3.3a.cmml" xref="S3.E6.m1.3.3.1.1.3.4.3.3"><mtext mathsize="70%" id="S3.E6.m1.3.3.1.1.3.4.3.3.cmml" xref="S3.E6.m1.3.3.1.1.3.4.3.3">type</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.3c">{\cal L}\left(\theta,{\cal X}\right)={\cal L}_{\text{ID}}+\lambda_{\text{color}}{\cal L}_{\text{color}}+\lambda_{\text{type}}{\cal L}_{\text{type}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p6.4" class="ltx_p">where <math id="S3.SS3.p6.1.m1.1" class="ltx_Math" alttext="{\cal X}=\{\left(x_{i},y_{i}\right)\}" display="inline"><semantics id="S3.SS3.p6.1.m1.1a"><mrow id="S3.SS3.p6.1.m1.1.1" xref="S3.SS3.p6.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p6.1.m1.1.1.3" xref="S3.SS3.p6.1.m1.1.1.3.cmml">𝒳</mi><mo id="S3.SS3.p6.1.m1.1.1.2" xref="S3.SS3.p6.1.m1.1.1.2.cmml">=</mo><mrow id="S3.SS3.p6.1.m1.1.1.1.1" xref="S3.SS3.p6.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p6.1.m1.1.1.1.1.2" xref="S3.SS3.p6.1.m1.1.1.1.2.cmml">{</mo><mrow id="S3.SS3.p6.1.m1.1.1.1.1.1.2" xref="S3.SS3.p6.1.m1.1.1.1.1.1.3.cmml"><mo id="S3.SS3.p6.1.m1.1.1.1.1.1.2.3" xref="S3.SS3.p6.1.m1.1.1.1.1.1.3.cmml">(</mo><msub id="S3.SS3.p6.1.m1.1.1.1.1.1.1.1" xref="S3.SS3.p6.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p6.1.m1.1.1.1.1.1.1.1.2" xref="S3.SS3.p6.1.m1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS3.p6.1.m1.1.1.1.1.1.1.1.3" xref="S3.SS3.p6.1.m1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS3.p6.1.m1.1.1.1.1.1.2.4" xref="S3.SS3.p6.1.m1.1.1.1.1.1.3.cmml">,</mo><msub id="S3.SS3.p6.1.m1.1.1.1.1.1.2.2" xref="S3.SS3.p6.1.m1.1.1.1.1.1.2.2.cmml"><mi id="S3.SS3.p6.1.m1.1.1.1.1.1.2.2.2" xref="S3.SS3.p6.1.m1.1.1.1.1.1.2.2.2.cmml">y</mi><mi id="S3.SS3.p6.1.m1.1.1.1.1.1.2.2.3" xref="S3.SS3.p6.1.m1.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo id="S3.SS3.p6.1.m1.1.1.1.1.1.2.5" xref="S3.SS3.p6.1.m1.1.1.1.1.1.3.cmml">)</mo></mrow><mo stretchy="false" id="S3.SS3.p6.1.m1.1.1.1.1.3" xref="S3.SS3.p6.1.m1.1.1.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.1.m1.1b"><apply id="S3.SS3.p6.1.m1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1"><eq id="S3.SS3.p6.1.m1.1.1.2.cmml" xref="S3.SS3.p6.1.m1.1.1.2"></eq><ci id="S3.SS3.p6.1.m1.1.1.3.cmml" xref="S3.SS3.p6.1.m1.1.1.3">𝒳</ci><set id="S3.SS3.p6.1.m1.1.1.1.2.cmml" xref="S3.SS3.p6.1.m1.1.1.1.1"><interval closure="open" id="S3.SS3.p6.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS3.p6.1.m1.1.1.1.1.1.2"><apply id="S3.SS3.p6.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p6.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p6.1.m1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.SS3.p6.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p6.1.m1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS3.p6.1.m1.1.1.1.1.1.2.2.cmml" xref="S3.SS3.p6.1.m1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p6.1.m1.1.1.1.1.1.2.2.1.cmml" xref="S3.SS3.p6.1.m1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.SS3.p6.1.m1.1.1.1.1.1.2.2.2.cmml" xref="S3.SS3.p6.1.m1.1.1.1.1.1.2.2.2">𝑦</ci><ci id="S3.SS3.p6.1.m1.1.1.1.1.1.2.2.3.cmml" xref="S3.SS3.p6.1.m1.1.1.1.1.1.2.2.3">𝑖</ci></apply></interval></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.1.m1.1c">{\cal X}=\{\left(x_{i},y_{i}\right)\}</annotation></semantics></math> represents the input training set and <math id="S3.SS3.p6.2.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS3.p6.2.m2.1a"><mi id="S3.SS3.p6.2.m2.1.1" xref="S3.SS3.p6.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.2.m2.1b"><ci id="S3.SS3.p6.2.m2.1.1.cmml" xref="S3.SS3.p6.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.2.m2.1c">\theta</annotation></semantics></math> is the set of network parameters.
Following the practice of other researchers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, we set the regularization parameters of both <math id="S3.SS3.p6.3.m3.1" class="ltx_Math" alttext="\lambda_{\text{color}}" display="inline"><semantics id="S3.SS3.p6.3.m3.1a"><msub id="S3.SS3.p6.3.m3.1.1" xref="S3.SS3.p6.3.m3.1.1.cmml"><mi id="S3.SS3.p6.3.m3.1.1.2" xref="S3.SS3.p6.3.m3.1.1.2.cmml">λ</mi><mtext id="S3.SS3.p6.3.m3.1.1.3" xref="S3.SS3.p6.3.m3.1.1.3a.cmml">color</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.3.m3.1b"><apply id="S3.SS3.p6.3.m3.1.1.cmml" xref="S3.SS3.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.3.m3.1.1.1.cmml" xref="S3.SS3.p6.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p6.3.m3.1.1.2.cmml" xref="S3.SS3.p6.3.m3.1.1.2">𝜆</ci><ci id="S3.SS3.p6.3.m3.1.1.3a.cmml" xref="S3.SS3.p6.3.m3.1.1.3"><mtext mathsize="70%" id="S3.SS3.p6.3.m3.1.1.3.cmml" xref="S3.SS3.p6.3.m3.1.1.3">color</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.3.m3.1c">\lambda_{\text{color}}</annotation></semantics></math> and <math id="S3.SS3.p6.4.m4.1" class="ltx_Math" alttext="\lambda_{\text{type}}" display="inline"><semantics id="S3.SS3.p6.4.m4.1a"><msub id="S3.SS3.p6.4.m4.1.1" xref="S3.SS3.p6.4.m4.1.1.cmml"><mi id="S3.SS3.p6.4.m4.1.1.2" xref="S3.SS3.p6.4.m4.1.1.2.cmml">λ</mi><mtext id="S3.SS3.p6.4.m4.1.1.3" xref="S3.SS3.p6.4.m4.1.1.3a.cmml">type</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.4.m4.1b"><apply id="S3.SS3.p6.4.m4.1.1.cmml" xref="S3.SS3.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.4.m4.1.1.1.cmml" xref="S3.SS3.p6.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p6.4.m4.1.1.2.cmml" xref="S3.SS3.p6.4.m4.1.1.2">𝜆</ci><ci id="S3.SS3.p6.4.m4.1.1.3a.cmml" xref="S3.SS3.p6.4.m4.1.1.3"><mtext mathsize="70%" id="S3.SS3.p6.4.m4.1.1.3.cmml" xref="S3.SS3.p6.4.m4.1.1.3">type</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.4.m4.1c">\lambda_{\text{type}}</annotation></semantics></math> to be much lower than 1, in our case 0.125.
This is because, in some circumstances, vehicle ReID and attribute classification are conflicting tasks, <span id="S3.SS3.p6.4.1" class="ltx_text ltx_font_italic">i.e.</span>, two vehicles of the same color and/or type may not share the same identity.</p>
</div>
<div id="S3.SS3.p7" class="ltx_para">
<p id="S3.SS3.p7.1" class="ltx_p">At the testing stage, the final ReID classification layer is removed.
For each vehicle image a 1024-dimensional feature vector is extracted from the last FC layer.
The features from each pair of query and test images are compared using Euclidean distance to determine their similarity.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we present the datasets used for evaluating our proposed approach, the implementation details, experimental results showing state-of-the-art performance, and a detailed analysis of the effects of various components of PAMTRI.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Dataset</span></th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.2.1" class="ltx_text" style="font-size:80%;"># total</span></th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.3.1" class="ltx_text" style="font-size:80%;"># train</span></th>
<th id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.4.1" class="ltx_text" style="font-size:80%;"># test</span></th>
<th id="S4.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.5.1" class="ltx_text" style="font-size:80%;"># query</span></th>
<th id="S4.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.6.1" class="ltx_text" style="font-size:80%;"># total</span></th>
</tr>
<tr id="S4.T1.1.2.2" class="ltx_tr">
<th id="S4.T1.1.2.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T1.1.2.2.2.1" class="ltx_text" style="font-size:80%;">IDs</span></th>
<th id="S4.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T1.1.2.2.3.1" class="ltx_text" style="font-size:80%;">IDs</span></th>
<th id="S4.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T1.1.2.2.4.1" class="ltx_text" style="font-size:80%;">IDs</span></th>
<th id="S4.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T1.1.2.2.5.1" class="ltx_text" style="font-size:80%;">images</span></th>
<th id="S4.T1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T1.1.2.2.6.1" class="ltx_text" style="font-size:80%;">images</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.3.1" class="ltx_tr">
<th id="S4.T1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T1.1.3.1.1.1" class="ltx_text" style="font-size:80%;">VeRi</span></th>
<td id="S4.T1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.3.1.2.1" class="ltx_text" style="font-size:80%;">776</span></td>
<td id="S4.T1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.3.1.3.1" class="ltx_text" style="font-size:80%;">576</span></td>
<td id="S4.T1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.3.1.4.1" class="ltx_text" style="font-size:80%;">200</span></td>
<td id="S4.T1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.3.1.5.1" class="ltx_text" style="font-size:80%;">1,678</span></td>
<td id="S4.T1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.3.1.6.1" class="ltx_text" style="font-size:80%;">51,038</span></td>
</tr>
<tr id="S4.T1.1.4.2" class="ltx_tr">
<th id="S4.T1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">CityFlow-ReID</span></th>
<td id="S4.T1.1.4.2.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.4.2.2.1" class="ltx_text" style="font-size:80%;">666</span></td>
<td id="S4.T1.1.4.2.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.4.2.3.1" class="ltx_text" style="font-size:80%;">333</span></td>
<td id="S4.T1.1.4.2.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.4.2.4.1" class="ltx_text" style="font-size:80%;">333</span></td>
<td id="S4.T1.1.4.2.5" class="ltx_td ltx_align_center"><span id="S4.T1.1.4.2.5.1" class="ltx_text" style="font-size:80%;">1,052</span></td>
<td id="S4.T1.1.4.2.6" class="ltx_td ltx_align_center"><span id="S4.T1.1.4.2.6.1" class="ltx_text" style="font-size:80%;">56,277</span></td>
</tr>
<tr id="S4.T1.1.5.3" class="ltx_tr">
<th id="S4.T1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T1.1.5.3.1.1" class="ltx_text" style="font-size:80%;">Synthetic</span></th>
<td id="S4.T1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.5.3.2.1" class="ltx_text" style="font-size:80%;">402</span></td>
<td id="S4.T1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.5.3.3.1" class="ltx_text" style="font-size:80%;">402</span></td>
<td id="S4.T1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.5.3.4.1" class="ltx_text" style="font-size:80%;">–</span></td>
<td id="S4.T1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.5.3.5.1" class="ltx_text" style="font-size:80%;">–</span></td>
<td id="S4.T1.1.5.3.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.5.3.6.1" class="ltx_text" style="font-size:80%;">41,000</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Statistics of the datasets used for training and evaluation.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets and evaluation protocol</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Our PAMTRI system was evaluated on two mainstream large-scale vehicle ReID benchmarks, namely, VeRi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and CityFlow-ReID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, whose statistics are summarized in Tab. <a href="#S4.T1" title="Table 1 ‣ 4 Evaluation ‣ PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> together with the details of the synthetic data we generated for training.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">VeRi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> has been widely used by most recent research in vehicle ReID, as it provides multiple views of vehicles captured from 20 cameras.
CityFlow-ReID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> is a subset of the recent multi-target multi-camera vehicle tracking benchmark, CityFlow, which has been adopted for the AI City Challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> at CVPR 2019.
The latter is significantly more challenging, as the footage is captured with more cameras (40) in more diverse environments (residential areas and highways).
Unlike VeRi, the original videos are available in CityFlow, which enable us to extract background images for randomization to generate realistic synthetic data.
Whereas the color and type information is available with the VeRi dataset, such attribute annotation is not provided by CityFlow.
Hence, another contribution of this work is that we manually labeled vehicle attributes (color and type) for all the 666 identities in CityFlow-ReID.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.7" class="ltx_p">In our experiments, we strictly follow the evaluation protocol proposed in Market1501 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> measuring the mean Average Precision (mAP) and the rank-<math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mi id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><ci id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">K</annotation></semantics></math> hit rates.
For mAP, we compute the mean of all queries’ average precision, <span id="S4.SS1.p3.7.1" class="ltx_text ltx_font_italic">i.e.</span>, the area under the Precision-Recall curve.
The rank-<math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mi id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><ci id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">K</annotation></semantics></math> hit rate denotes the possibility that at least one true positive is ranked within the top <math id="S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.SS1.p3.3.m3.1a"><mi id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><ci id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">K</annotation></semantics></math> positions.
When all the rank-<math id="S4.SS1.p3.4.m4.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.SS1.p3.4.m4.1a"><mi id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.1b"><ci id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.1c">K</annotation></semantics></math> hit rates are plotted against <math id="S4.SS1.p3.5.m5.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.SS1.p3.5.m5.1a"><mi id="S4.SS1.p3.5.m5.1.1" xref="S4.SS1.p3.5.m5.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.5.m5.1b"><ci id="S4.SS1.p3.5.m5.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.5.m5.1c">K</annotation></semantics></math>, we have the Cumulative Matching Characteristic (CMC).
In addition, rank-<math id="S4.SS1.p3.6.m6.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.SS1.p3.6.m6.1a"><mi id="S4.SS1.p3.6.m6.1.1" xref="S4.SS1.p3.6.m6.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.6.m6.1b"><ci id="S4.SS1.p3.6.m6.1.1.cmml" xref="S4.SS1.p3.6.m6.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.6.m6.1c">K</annotation></semantics></math> mAP is introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> that measures the mean of average precision for each query considering only the top <math id="S4.SS1.p3.7.m7.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.SS1.p3.7.m7.1a"><mi id="S4.SS1.p3.7.m7.1.1" xref="S4.SS1.p3.7.m7.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.7.m7.1b"><ci id="S4.SS1.p3.7.m7.1.1.cmml" xref="S4.SS1.p3.7.m7.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.7.m7.1c">K</annotation></semantics></math> matches.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation details</h3>

<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S4.T2.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Method</span></th>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.1.1.1.2.1" class="ltx_text" style="font-size:80%;">mAP</span></td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.1.1.1.3.1" class="ltx_text" style="font-size:80%;">Rank-1</span></td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.1.1.1.4.1" class="ltx_text" style="font-size:80%;">Rank-5</span></td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.1.1.1.5.1" class="ltx_text" style="font-size:80%;">Rank-20</span></td>
</tr>
<tr id="S4.T2.1.2.2" class="ltx_tr">
<th id="S4.T2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S4.T2.1.2.2.1.1" class="ltx_text" style="font-size:80%;">FACT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.2.2.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S4.T2.1.2.2.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.2.2.1" class="ltx_text" style="font-size:80%;">18.73</span></td>
<td id="S4.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.2.3.1" class="ltx_text" style="font-size:80%;">51.85</span></td>
<td id="S4.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.2.4.1" class="ltx_text" style="font-size:80%;">67.16</span></td>
<td id="S4.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.2.5.1" class="ltx_text" style="font-size:80%;">79.56</span></td>
</tr>
<tr id="S4.T2.1.3.3" class="ltx_tr">
<th id="S4.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.1.3.3.1.1" class="ltx_text" style="font-size:80%;">PROVID* </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.3.3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S4.T2.1.3.3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T2.1.3.3.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.3.3.2.1" class="ltx_text" style="font-size:80%;">48.47</span></td>
<td id="S4.T2.1.3.3.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.3.3.3.1" class="ltx_text" style="font-size:80%;">76.76</span></td>
<td id="S4.T2.1.3.3.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.3.3.4.1" class="ltx_text" style="font-size:80%;">91.40</span></td>
<td id="S4.T2.1.3.3.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.3.3.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T2.1.4.4" class="ltx_tr">
<th id="S4.T2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.1.4.4.1.1" class="ltx_text" style="font-size:80%;">OIFE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.4.4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S4.T2.1.4.4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T2.1.4.4.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.4.4.2.1" class="ltx_text" style="font-size:80%;">48.00</span></td>
<td id="S4.T2.1.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.4.4.3.1" class="ltx_text" style="font-size:80%;">65.92</span></td>
<td id="S4.T2.1.4.4.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.4.4.4.1" class="ltx_text" style="font-size:80%;">87.66</span></td>
<td id="S4.T2.1.4.4.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.4.4.5.1" class="ltx_text" style="font-size:80%;">96.63</span></td>
</tr>
<tr id="S4.T2.1.5.5" class="ltx_tr">
<th id="S4.T2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.1.5.5.1.1" class="ltx_text" style="font-size:80%;">PathLSTM* </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.5.5.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S4.T2.1.5.5.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T2.1.5.5.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.5.2.1" class="ltx_text" style="font-size:80%;">58.27</span></td>
<td id="S4.T2.1.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.5.3.1" class="ltx_text" style="font-size:80%;">83.49</span></td>
<td id="S4.T2.1.5.5.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.5.4.1" class="ltx_text" style="font-size:80%;">90.04</span></td>
<td id="S4.T2.1.5.5.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.5.5.1" class="ltx_text" style="font-size:80%;">97.16</span></td>
</tr>
<tr id="S4.T2.1.6.6" class="ltx_tr">
<th id="S4.T2.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.1.6.6.1.1" class="ltx_text" style="font-size:80%;">GSTE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.6.6.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S4.T2.1.6.6.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T2.1.6.6.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.6.6.2.1" class="ltx_text" style="font-size:80%;">59.47</span></td>
<td id="S4.T2.1.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.6.6.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">96.24</span></td>
<td id="S4.T2.1.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.6.6.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">98.97</span></td>
<td id="S4.T2.1.6.6.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.6.6.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T2.1.7.7" class="ltx_tr">
<th id="S4.T2.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.1.7.7.1.1" class="ltx_text" style="font-size:80%;">VAMI </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.7.7.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib45" title="" class="ltx_ref">45</a><span id="S4.T2.1.7.7.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T2.1.7.7.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.7.7.2.1" class="ltx_text" style="font-size:80%;">50.13</span></td>
<td id="S4.T2.1.7.7.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.7.7.3.1" class="ltx_text" style="font-size:80%;">77.03</span></td>
<td id="S4.T2.1.7.7.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.7.7.4.1" class="ltx_text" style="font-size:80%;">90.82</span></td>
<td id="S4.T2.1.7.7.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.7.7.5.1" class="ltx_text" style="font-size:80%;">97.16</span></td>
</tr>
<tr id="S4.T2.1.8.8" class="ltx_tr">
<th id="S4.T2.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.1.8.8.1.1" class="ltx_text" style="font-size:80%;">VAMI* </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.8.8.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib45" title="" class="ltx_ref">45</a><span id="S4.T2.1.8.8.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T2.1.8.8.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.8.8.2.1" class="ltx_text" style="font-size:80%;">61.32</span></td>
<td id="S4.T2.1.8.8.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.8.8.3.1" class="ltx_text" style="font-size:80%;">85.92</span></td>
<td id="S4.T2.1.8.8.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.8.8.4.1" class="ltx_text" style="font-size:80%;">91.84</span></td>
<td id="S4.T2.1.8.8.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.8.8.5.1" class="ltx_text" style="font-size:80%;">97.70</span></td>
</tr>
<tr id="S4.T2.1.9.9" class="ltx_tr">
<th id="S4.T2.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.1.9.9.1.1" class="ltx_text" style="font-size:80%;">ABLN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.9.9.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib46" title="" class="ltx_ref">46</a><span id="S4.T2.1.9.9.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T2.1.9.9.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.9.9.2.1" class="ltx_text" style="font-size:80%;">24.92</span></td>
<td id="S4.T2.1.9.9.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.9.9.3.1" class="ltx_text" style="font-size:80%;">60.49</span></td>
<td id="S4.T2.1.9.9.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.9.9.4.1" class="ltx_text" style="font-size:80%;">77.33</span></td>
<td id="S4.T2.1.9.9.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.9.9.5.1" class="ltx_text" style="font-size:80%;">88.27</span></td>
</tr>
<tr id="S4.T2.1.10.10" class="ltx_tr">
<th id="S4.T2.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.1.10.10.1.1" class="ltx_text" style="font-size:80%;">BA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.10.10.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S4.T2.1.10.10.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T2.1.10.10.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.10.10.2.1" class="ltx_text" style="font-size:80%;">66.91</span></td>
<td id="S4.T2.1.10.10.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.10.10.3.1" class="ltx_text" style="font-size:80%;">90.11</span></td>
<td id="S4.T2.1.10.10.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.10.10.4.1" class="ltx_text" style="font-size:80%;">96.01</span></td>
<td id="S4.T2.1.10.10.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.10.10.5.1" class="ltx_text" style="font-size:80%;">98.27</span></td>
</tr>
<tr id="S4.T2.1.11.11" class="ltx_tr">
<th id="S4.T2.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T2.1.11.11.1.1" class="ltx_text" style="font-size:80%;">BS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.11.11.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S4.T2.1.11.11.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T2.1.11.11.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.11.11.2.1" class="ltx_text" style="font-size:80%;">67.55</span></td>
<td id="S4.T2.1.11.11.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.11.11.3.1" class="ltx_text" style="font-size:80%;">90.23</span></td>
<td id="S4.T2.1.11.11.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.11.11.4.1" class="ltx_text" style="font-size:80%;">96.42</span></td>
<td id="S4.T2.1.11.11.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.11.11.5.1" class="ltx_text" style="font-size:80%;">98.63</span></td>
</tr>
<tr id="S4.T2.1.12.12" class="ltx_tr">
<th id="S4.T2.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T2.1.12.12.1.1" class="ltx_text" style="font-size:80%;">RS</span></th>
<td id="S4.T2.1.12.12.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.12.12.2.1" class="ltx_text" style="font-size:80%;">63.76</span></td>
<td id="S4.T2.1.12.12.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.12.12.3.1" class="ltx_text" style="font-size:80%;">90.70</span></td>
<td id="S4.T2.1.12.12.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.12.12.4.1" class="ltx_text" style="font-size:80%;">94.40</span></td>
<td id="S4.T2.1.12.12.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.12.12.5.1" class="ltx_text" style="font-size:80%;">97.47</span></td>
</tr>
<tr id="S4.T2.1.13.13" class="ltx_tr">
<th id="S4.T2.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.1.13.13.1.1" class="ltx_text" style="font-size:80%;">RS+MT</span></th>
<td id="S4.T2.1.13.13.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.13.13.2.1" class="ltx_text" style="font-size:80%;">66.18</span></td>
<td id="S4.T2.1.13.13.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.13.13.3.1" class="ltx_text" style="font-size:80%;">91.90</span></td>
<td id="S4.T2.1.13.13.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.13.13.4.1" class="ltx_text" style="font-size:80%;">96.90</span></td>
<td id="S4.T2.1.13.13.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.13.13.5.1" class="ltx_text" style="font-size:80%;">98.99</span></td>
</tr>
<tr id="S4.T2.1.14.14" class="ltx_tr">
<th id="S4.T2.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.1.14.14.1.1" class="ltx_text" style="font-size:80%;">RS+MT+K</span></th>
<td id="S4.T2.1.14.14.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.14.14.2.1" class="ltx_text" style="font-size:80%;">68.64</span></td>
<td id="S4.T2.1.14.14.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.14.14.3.1" class="ltx_text" style="font-size:80%;">91.60</span></td>
<td id="S4.T2.1.14.14.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.14.14.4.1" class="ltx_text" style="font-size:80%;">96.78</span></td>
<td id="S4.T2.1.14.14.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.14.14.5.1" class="ltx_text" style="font-size:80%;">98.75</span></td>
</tr>
<tr id="S4.T2.1.15.15" class="ltx_tr">
<th id="S4.T2.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.1.15.15.1.1" class="ltx_text" style="font-size:80%;">RS+MT+K+H</span></th>
<td id="S4.T2.1.15.15.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.15.15.2.1" class="ltx_text" style="font-size:80%;">71.16</span></td>
<td id="S4.T2.1.15.15.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.15.15.3.1" class="ltx_text" style="font-size:80%;">92.74</span></td>
<td id="S4.T2.1.15.15.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.15.15.4.1" class="ltx_text" style="font-size:80%;">96.68</span></td>
<td id="S4.T2.1.15.15.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.15.15.5.1" class="ltx_text" style="font-size:80%;">98.40</span></td>
</tr>
<tr id="S4.T2.1.16.16" class="ltx_tr">
<th id="S4.T2.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.1.16.16.1.1" class="ltx_text" style="font-size:80%;">RS+MT+K+S</span></th>
<td id="S4.T2.1.16.16.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.16.16.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">71.88</span></td>
<td id="S4.T2.1.16.16.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.16.16.3.1" class="ltx_text" style="font-size:80%;">92.86</span></td>
<td id="S4.T2.1.16.16.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.16.16.4.1" class="ltx_text" style="font-size:80%;">96.97</span></td>
<td id="S4.T2.1.16.16.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.16.16.5.1" class="ltx_text" style="font-size:80%;">98.23</span></td>
</tr>
<tr id="S4.T2.1.17.17" class="ltx_tr">
<th id="S4.T2.1.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T2.1.17.17.1.1" class="ltx_text" style="font-size:80%;">RS w/ Xent only</span></th>
<td id="S4.T2.1.17.17.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.17.17.2.1" class="ltx_text" style="font-size:80%;">56.52</span></td>
<td id="S4.T2.1.17.17.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.17.17.3.1" class="ltx_text" style="font-size:80%;">83.41</span></td>
<td id="S4.T2.1.17.17.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.17.17.4.1" class="ltx_text" style="font-size:80%;">92.07</span></td>
<td id="S4.T2.1.17.17.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.17.17.5.1" class="ltx_text" style="font-size:80%;">97.02</span></td>
</tr>
<tr id="S4.T2.1.18.18" class="ltx_tr">
<th id="S4.T2.1.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.1.18.18.1.1" class="ltx_text" style="font-size:80%;">RS w/ Htri only</span></th>
<td id="S4.T2.1.18.18.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.18.18.2.1" class="ltx_text" style="font-size:80%;">47.50</span></td>
<td id="S4.T2.1.18.18.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.18.18.3.1" class="ltx_text" style="font-size:80%;">73.54</span></td>
<td id="S4.T2.1.18.18.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.18.18.4.1" class="ltx_text" style="font-size:80%;">87.25</span></td>
<td id="S4.T2.1.18.18.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.18.18.5.1" class="ltx_text" style="font-size:80%;">96.01</span></td>
</tr>
<tr id="S4.T2.1.19.19" class="ltx_tr">
<th id="S4.T2.1.19.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.1.19.19.1.1" class="ltx_text" style="font-size:80%;">RS+MT w/ DN201</span></th>
<td id="S4.T2.1.19.19.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.19.19.2.1" class="ltx_text" style="font-size:80%;">64.42</span></td>
<td id="S4.T2.1.19.19.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.19.19.3.1" class="ltx_text" style="font-size:80%;">90.58</span></td>
<td id="S4.T2.1.19.19.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.19.19.4.1" class="ltx_text" style="font-size:80%;">96.36</span></td>
<td id="S4.T2.1.19.19.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.19.19.5.1" class="ltx_text" style="font-size:80%;">98.81</span></td>
</tr>
<tr id="S4.T2.1.20.20" class="ltx_tr">
<th id="S4.T2.1.20.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T2.1.20.20.1.1" class="ltx_text" style="font-size:80%;">R+MT+K</span></th>
<td id="S4.T2.1.20.20.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.20.20.2.1" class="ltx_text" style="font-size:80%;">65.44</span></td>
<td id="S4.T2.1.20.20.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.20.20.3.1" class="ltx_text" style="font-size:80%;">90.94</span></td>
<td id="S4.T2.1.20.20.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.20.20.4.1" class="ltx_text" style="font-size:80%;">96.72</span></td>
<td id="S4.T2.1.20.20.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.20.20.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">99.11</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Experimental comparison of state-of-the-art in vehicle ReID on VeRi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. All values are shown as percentages. For our proposed method, MT, K, H, S, RS and R respectively denote multi-task learning, explicit keypoints embedded, heatmaps embedded, segments embedded, training with both real and synthetic data, and training with real data only. Xent, Htri and DN201 stand for cross-entropy loss, hard triplet loss and DenseNet201, respectively. (*) indicates the usage of spatio-temporal information.</figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.3" class="ltx_p"><span id="S4.SS2.p1.3.1" class="ltx_text ltx_font_bold">Training for multi-task learning.</span>
Leveraging the off-the-shelf implementation in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, we use DenseNet121 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> as our backbone CNN for multi-task learning, whose initial weights are from the model pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
The input images are resized to <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">256</cn><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">256\times 256</annotation></semantics></math> and the training batch size is set as 32.
We utilize the Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> to train the base model for 60 maximum epochs.
The initial learning rate was set to 3e-4, which decays to 3e-5 and 3e-6 at the 20<math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="{}^{\text{th}}" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><msup id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1a" xref="S4.SS2.p1.2.m2.1.1.cmml"></mi><mtext id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1a.cmml">th</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><ci id="S4.SS2.p1.2.m2.1.1.1a.cmml" xref="S4.SS2.p1.2.m2.1.1.1"><mtext mathsize="70%" id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1">th</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">{}^{\text{th}}</annotation></semantics></math> and 40<math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="{}^{\text{th}}" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><msup id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mi id="S4.SS2.p1.3.m3.1.1a" xref="S4.SS2.p1.3.m3.1.1.cmml"></mi><mtext id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1a.cmml">th</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><ci id="S4.SS2.p1.3.m3.1.1.1a.cmml" xref="S4.SS2.p1.3.m3.1.1.1"><mtext mathsize="70%" id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1">th</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">{}^{\text{th}}</annotation></semantics></math> epochs, respectively.
For multi-task learning the dimension of the last FC layer for ReID is 1,024, whereas the two FC layers for attribute classification share the size of 512 each.
For all the final FC layers, we adopt the leaky rectified linear unit (Leaky ReLU) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> as the activation function.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.3" class="ltx_p"><span id="S4.SS2.p2.3.1" class="ltx_text ltx_font_bold">Training for pose estimation.</span>
The state-of-the-art HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> for human pose estimation is used as our backbone for vehicle pose estimation, which is built upon the original implementation by Sun <em id="S4.SS2.p2.3.2" class="ltx_emph ltx_font_italic">et al</em>.
Again we adopt the pre-trained weights on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> for initialization.
Each input image is also resized to <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mn id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><times id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">256</cn><cn type="integer" id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">256\times 256</annotation></semantics></math> and the size of the heatmap/segment output is <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="64\times 64" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mn id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><times id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">64</cn><cn type="integer" id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">64\times 64</annotation></semantics></math>.
We set the training batch size to be 32, and the maximum number of epochs is 210 with learning rate of 1e-3.
The final FC layer is adjusted to output a <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="108" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mn id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">108</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><cn type="integer" id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">108</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">108</annotation></semantics></math>-dimensional vector, as our vehicle model consists of 36 keypoints in 2D whose visibility (indicated by confidence scores) is also computed.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S4.T3.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Method</span></th>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T3.1.1.1.2.1" class="ltx_text" style="font-size:80%;">mAP (</span><em id="S4.T3.1.1.1.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">r100</em><span id="S4.T3.1.1.1.2.3" class="ltx_text" style="font-size:80%;">)</span>
</td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.1.3.1" class="ltx_text" style="font-size:80%;">Rank-1</span></td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.1.4.1" class="ltx_text" style="font-size:80%;">Rank-5</span></td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.1.5.1" class="ltx_text" style="font-size:80%;">Rank-20</span></td>
</tr>
<tr id="S4.T3.1.2.2" class="ltx_tr">
<th id="S4.T3.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S4.T3.1.2.2.1.1" class="ltx_text" style="font-size:80%;">FVS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.2.2.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S4.T3.1.2.2.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.2.2.2.1" class="ltx_text" style="font-size:80%;">6.33 (5.08)</span></td>
<td id="S4.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.2.2.3.1" class="ltx_text" style="font-size:80%;">20.82</span></td>
<td id="S4.T3.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.2.2.4.1" class="ltx_text" style="font-size:80%;">24.52</span></td>
<td id="S4.T3.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.2.2.5.1" class="ltx_text" style="font-size:80%;">31.27</span></td>
</tr>
<tr id="S4.T3.1.3.3" class="ltx_tr">
<th id="S4.T3.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.3.3.1.1" class="ltx_text" style="font-size:80%;">Xent </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.3.3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib44" title="" class="ltx_ref">44</a><span id="S4.T3.1.3.3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T3.1.3.3.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.3.3.2.1" class="ltx_text" style="font-size:80%;">23.18 (18.62)</span></td>
<td id="S4.T3.1.3.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.3.3.3.1" class="ltx_text" style="font-size:80%;">39.92</span></td>
<td id="S4.T3.1.3.3.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.3.3.4.1" class="ltx_text" style="font-size:80%;">52.66</span></td>
<td id="S4.T3.1.3.3.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.3.3.5.1" class="ltx_text" style="font-size:80%;">66.06</span></td>
</tr>
<tr id="S4.T3.1.4.4" class="ltx_tr">
<th id="S4.T3.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.4.4.1.1" class="ltx_text" style="font-size:80%;">Htri </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.4.4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib44" title="" class="ltx_ref">44</a><span id="S4.T3.1.4.4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T3.1.4.4.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.4.4.2.1" class="ltx_text" style="font-size:80%;">30.46 (24.04)</span></td>
<td id="S4.T3.1.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.4.4.3.1" class="ltx_text" style="font-size:80%;">45.75</span></td>
<td id="S4.T3.1.4.4.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.4.4.4.1" class="ltx_text" style="font-size:80%;">61.24</span></td>
<td id="S4.T3.1.4.4.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.4.4.5.1" class="ltx_text" style="font-size:80%;">75.94</span></td>
</tr>
<tr id="S4.T3.1.5.5" class="ltx_tr">
<th id="S4.T3.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.5.5.1.1" class="ltx_text" style="font-size:80%;">Cent </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.5.5.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib44" title="" class="ltx_ref">44</a><span id="S4.T3.1.5.5.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T3.1.5.5.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.5.5.2.1" class="ltx_text" style="font-size:80%;">10.73 (9.49)</span></td>
<td id="S4.T3.1.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.5.5.3.1" class="ltx_text" style="font-size:80%;">27.92</span></td>
<td id="S4.T3.1.5.5.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.5.5.4.1" class="ltx_text" style="font-size:80%;">39.77</span></td>
<td id="S4.T3.1.5.5.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.5.5.5.1" class="ltx_text" style="font-size:80%;">52.83</span></td>
</tr>
<tr id="S4.T3.1.6.6" class="ltx_tr">
<th id="S4.T3.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.6.6.1.1" class="ltx_text" style="font-size:80%;">Xent+Htri </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.6.6.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib44" title="" class="ltx_ref">44</a><span id="S4.T3.1.6.6.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T3.1.6.6.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.6.6.2.1" class="ltx_text" style="font-size:80%;">31.02 (25.06)</span></td>
<td id="S4.T3.1.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.6.6.3.1" class="ltx_text" style="font-size:80%;">51.69</span></td>
<td id="S4.T3.1.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.6.6.4.1" class="ltx_text" style="font-size:80%;">62.84</span></td>
<td id="S4.T3.1.6.6.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.6.6.5.1" class="ltx_text" style="font-size:80%;">74.91</span></td>
</tr>
<tr id="S4.T3.1.7.7" class="ltx_tr">
<th id="S4.T3.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.7.7.1.1" class="ltx_text" style="font-size:80%;">BA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.7.7.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S4.T3.1.7.7.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T3.1.7.7.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.7.7.2.1" class="ltx_text" style="font-size:80%;">31.30 (25.61)</span></td>
<td id="S4.T3.1.7.7.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.7.7.3.1" class="ltx_text" style="font-size:80%;">49.62</span></td>
<td id="S4.T3.1.7.7.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.7.7.4.1" class="ltx_text" style="font-size:80%;">65.02</span></td>
<td id="S4.T3.1.7.7.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.7.7.5.1" class="ltx_text" style="font-size:80%;">80.04</span></td>
</tr>
<tr id="S4.T3.1.8.8" class="ltx_tr">
<th id="S4.T3.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T3.1.8.8.1.1" class="ltx_text" style="font-size:80%;">BS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.8.8.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S4.T3.1.8.8.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T3.1.8.8.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.8.8.2.1" class="ltx_text" style="font-size:80%;">31.34 (25.57)</span></td>
<td id="S4.T3.1.8.8.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.8.8.3.1" class="ltx_text" style="font-size:80%;">49.05</span></td>
<td id="S4.T3.1.8.8.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.8.8.4.1" class="ltx_text" style="font-size:80%;">63.12</span></td>
<td id="S4.T3.1.8.8.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.8.8.5.1" class="ltx_text" style="font-size:80%;">78.80</span></td>
</tr>
<tr id="S4.T3.1.9.9" class="ltx_tr">
<th id="S4.T3.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.1.9.9.1.1" class="ltx_text" style="font-size:80%;">RS</span></th>
<td id="S4.T3.1.9.9.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.9.9.2.1" class="ltx_text" style="font-size:80%;">31.41 (25.66)</span></td>
<td id="S4.T3.1.9.9.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.9.9.3.1" class="ltx_text" style="font-size:80%;">50.37</span></td>
<td id="S4.T3.1.9.9.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.9.9.4.1" class="ltx_text" style="font-size:80%;">61.48</span></td>
<td id="S4.T3.1.9.9.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.9.9.5.1" class="ltx_text" style="font-size:80%;">74.26</span></td>
</tr>
<tr id="S4.T3.1.10.10" class="ltx_tr">
<th id="S4.T3.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.1.10.10.1.1" class="ltx_text" style="font-size:80%;">RS+MT</span></th>
<td id="S4.T3.1.10.10.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.10.10.2.1" class="ltx_text" style="font-size:80%;">32.80 (27.09)</span></td>
<td id="S4.T3.1.10.10.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.10.10.3.1" class="ltx_text" style="font-size:80%;">50.93</span></td>
<td id="S4.T3.1.10.10.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.10.10.4.1" class="ltx_text" style="font-size:80%;">66.09</span></td>
<td id="S4.T3.1.10.10.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.10.10.5.1" class="ltx_text" style="font-size:80%;">79.46</span></td>
</tr>
<tr id="S4.T3.1.11.11" class="ltx_tr">
<th id="S4.T3.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.1.11.11.1.1" class="ltx_text" style="font-size:80%;">RS+MT+K</span></th>
<td id="S4.T3.1.11.11.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.11.11.2.1" class="ltx_text" style="font-size:80%;">37.18 (31.03)</span></td>
<td id="S4.T3.1.11.11.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.11.11.3.1" class="ltx_text" style="font-size:80%;">55.80</span></td>
<td id="S4.T3.1.11.11.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.11.11.4.1" class="ltx_text" style="font-size:80%;">67.49</span></td>
<td id="S4.T3.1.11.11.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.11.11.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">81.08</span></td>
</tr>
<tr id="S4.T3.1.12.12" class="ltx_tr">
<th id="S4.T3.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.1.12.12.1.1" class="ltx_text" style="font-size:80%;">RS+MT+K+H</span></th>
<td id="S4.T3.1.12.12.2" class="ltx_td ltx_align_center">
<span id="S4.T3.1.12.12.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">40.39</span><span id="S4.T3.1.12.12.2.2" class="ltx_text" style="font-size:80%;"> (</span><span id="S4.T3.1.12.12.2.3" class="ltx_text ltx_font_bold" style="font-size:80%;">33.81</span><span id="S4.T3.1.12.12.2.4" class="ltx_text" style="font-size:80%;">)</span>
</td>
<td id="S4.T3.1.12.12.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.12.12.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">59.70</span></td>
<td id="S4.T3.1.12.12.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.12.12.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">70.91</span></td>
<td id="S4.T3.1.12.12.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.12.12.5.1" class="ltx_text" style="font-size:80%;">80.13</span></td>
</tr>
<tr id="S4.T3.1.13.13" class="ltx_tr">
<th id="S4.T3.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.1.13.13.1.1" class="ltx_text" style="font-size:80%;">RS+MT+K+S</span></th>
<td id="S4.T3.1.13.13.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.13.13.2.1" class="ltx_text" style="font-size:80%;">38.64 (32.67)</span></td>
<td id="S4.T3.1.13.13.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.13.13.3.1" class="ltx_text" style="font-size:80%;">57.32</span></td>
<td id="S4.T3.1.13.13.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.13.13.4.1" class="ltx_text" style="font-size:80%;">68.44</span></td>
<td id="S4.T3.1.13.13.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.13.13.5.1" class="ltx_text" style="font-size:80%;">79.37</span></td>
</tr>
<tr id="S4.T3.1.14.14" class="ltx_tr">
<th id="S4.T3.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.1.14.14.1.1" class="ltx_text" style="font-size:80%;">RS w/ Xent only</span></th>
<td id="S4.T3.1.14.14.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.14.14.2.1" class="ltx_text" style="font-size:80%;">29.59 (23.74)</span></td>
<td id="S4.T3.1.14.14.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.14.14.3.1" class="ltx_text" style="font-size:80%;">41.91</span></td>
<td id="S4.T3.1.14.14.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.14.14.4.1" class="ltx_text" style="font-size:80%;">56.77</span></td>
<td id="S4.T3.1.14.14.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.14.14.5.1" class="ltx_text" style="font-size:80%;">73.95</span></td>
</tr>
<tr id="S4.T3.1.15.15" class="ltx_tr">
<th id="S4.T3.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.1.15.15.1.1" class="ltx_text" style="font-size:80%;">RS w/ Htri only</span></th>
<td id="S4.T3.1.15.15.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.15.15.2.1" class="ltx_text" style="font-size:80%;">28.09 (21.95)</span></td>
<td id="S4.T3.1.15.15.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.15.15.3.1" class="ltx_text" style="font-size:80%;">40.02</span></td>
<td id="S4.T3.1.15.15.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.15.15.4.1" class="ltx_text" style="font-size:80%;">56.94</span></td>
<td id="S4.T3.1.15.15.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.15.15.5.1" class="ltx_text" style="font-size:80%;">74.05</span></td>
</tr>
<tr id="S4.T3.1.16.16" class="ltx_tr">
<th id="S4.T3.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.1.16.16.1.1" class="ltx_text" style="font-size:80%;">RS+MT w/ DN201</span></th>
<td id="S4.T3.1.16.16.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.16.16.2.1" class="ltx_text" style="font-size:80%;">33.18 (27.10)</span></td>
<td id="S4.T3.1.16.16.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.16.16.3.1" class="ltx_text" style="font-size:80%;">51.80</span></td>
<td id="S4.T3.1.16.16.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.16.16.4.1" class="ltx_text" style="font-size:80%;">65.49</span></td>
<td id="S4.T3.1.16.16.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.16.16.5.1" class="ltx_text" style="font-size:80%;">79.08</span></td>
</tr>
<tr id="S4.T3.1.17.17" class="ltx_tr">
<th id="S4.T3.1.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T3.1.17.17.1.1" class="ltx_text" style="font-size:80%;">R+MT+K</span></th>
<td id="S4.T3.1.17.17.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.1.17.17.2.1" class="ltx_text" style="font-size:80%;">36.67 (30.57)</span></td>
<td id="S4.T3.1.17.17.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.1.17.17.3.1" class="ltx_text" style="font-size:80%;">54.56</span></td>
<td id="S4.T3.1.17.17.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.1.17.17.4.1" class="ltx_text" style="font-size:80%;">66.54</span></td>
<td id="S4.T3.1.17.17.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.1.17.17.5.1" class="ltx_text" style="font-size:80%;">80.89</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Experimental comparison of state-of-the-art in vehicle ReID on CityFlow-ReID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. All values are shown as percentages, with <em id="S4.T3.9.1" class="ltx_emph ltx_font_italic">r100</em> indicating the rank-100 mAP. For our proposed method, MT, K, H, S, RS and R respectively denote multi-task learning, explicit keypoints embedded, heatmaps embedded, segments embedded, training with both real and synthetic data, and training with real data only. Xent, Htri and DN201 stand for cross-entropy loss, hard triplet loss and DenseNet201, respectively.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparison of ReID with the state-of-the-art</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Tab. <a href="#S4.T2" title="Table 2 ‣ 4.2 Implementation details ‣ 4 Evaluation ‣ PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> compares PAMTRI’s performance with state-of-the-art in vehicle ReID.
Notice that our method outperforms all the others in terms of the mAP metric.
Although GSTE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> achieves higher rank-<math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">K</annotation></semantics></math> hit rates, its mAP score is lower than ours by about 10%, which demonstrates our robust performance at all ranks.
Note also that GSTE exploits additional group information, <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_italic">i.e.</span>, signatures of the same identity from the same camera are grouped together, which is not required in our proposed scheme.
Moreover, VeRi also provides spatio-temporal information that enables association in time and space rather than purely using appearance information.
Surprisingly our proposed method achieves better performance over several methods that leverage this additional spatio-temporal information, which further validates the reliability of our extracted features based on pose-aware multi-task learning.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">We also conducted an ablation study while comparing with state-of-the-art.
It can be seen from the results that all the proposed algorithmic components, including multi-task learning and embedded pose representations, contribute to our performance gain. Though not all the components of our system contribute equally to the improved results, they all deliver viewpoint-aware information to aid feature learning.
The combination of both triplet loss and cross-entropy loss outperforms the individual loss functions, because the metric in feature space and identity classification are jointly learned.
The classification loss in ReID itself is generally too “lazy” to capture the useful but subtle attribute cues besides global appearance.
Moreover, we experimented with DenseNet201, which has almost twice as many parameters compared to DenseNet121, but the results did not improve and even decreased due to overfitting.
Therefore, the importance of the specific structure of HRNet for pose estimation is validated.
Finally, we find that the additional synthetic data can significantly improve the ReID performance.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2005.00673/assets/fig_cmc.jpg" id="S4.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="356" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The CMC curves of state-of-the-art methods on CityFlow-ReID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Note that variants of our proposed method improve the state-of-the-art performance. <span id="S4.F4.3.1" class="ltx_text ltx_font_bold">Best viewed in color.</span></figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Tab. <a href="#S4.T3" title="Table 3 ‣ 4.2 Implementation details ‣ 4 Evaluation ‣ PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, compares PAMTRI with state-of-the-art on the CityFlow-ReID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> benchmark.
Notice the drop in accuracy of state-of-the-art compared to VeRi, which validates that this dataset is more challenging.
BA and BS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, which rely on triplet embeddings, are the same methods shown in the previous table for VeRi.
Besides, we also compare with state-of-the-art metric learning methods in person ReID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> using cross-entropy loss (Xent) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, hard triplet loss (Htri) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, center loss (Cent) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, and the combination of both cross-entropy loss and hard triplet loss (Xent+Htri).
Like ours, they all share DenseNet121 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> as the backbone CNN.
Finally, FVS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> is the winner of the vehicle ReID track at the AI City Challenge 2018 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. This method directly extracts features from a pre-trained network and computes their distance with Bhattacharyya norm.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">As shown in the experimental results, PAMTRI significantly improves upon state-of-the-art performance by incorporating pose information with multi-task learning.
Again, all the proposed algorithmic components contribute to the performance gain.
The experimental results of other ablation study align with the trends in Tab. <a href="#S4.T2" title="Table 2 ‣ 4.2 Implementation details ‣ 4 Evaluation ‣ PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2005.00673/assets/fig_qual.jpg" id="S4.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="228" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Qualitative visualization of PAMTRI’s performance on public benchmarks: VeRi (top rows, using RS+MT+K+S) and CityFlow-ReID (bottom rows, using RS+MT+K+H). For each dataset, 5 successful cases and 1 failure case are presented. For each row, the top 30 matched gallery images are shown for each query image (first column, blue). Green and red boxes represent the same identity (true) and different identities (false), respectively. <span id="S4.F5.3.1" class="ltx_text ltx_font_bold">Best viewed in color.</span></figcaption>
</figure>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.1" class="ltx_p">In Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.3 Comparison of ReID with the state-of-the-art ‣ 4 Evaluation ‣ PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the CMC curves of the methods from Tab. <a href="#S4.T3" title="Table 3 ‣ 4.2 Implementation details ‣ 4 Evaluation ‣ PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> are plotted to better view the quantitative experimental comparison.
We also show in Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.3 Comparison of ReID with the state-of-the-art ‣ 4 Evaluation ‣ PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> some examples of successful and failure cases using our proposed method.
As shown in the examples, most failures are caused by high inter-class similarity for common vehicles like taxi and strong occlusion by objects in the scene, (<em id="S4.SS3.p5.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS3.p5.1.2" class="ltx_text"></span>, signs and poles).</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T4.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Method</span></th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T4.1.1.1.2.1" class="ltx_text" style="font-size:80%;">VeRi</span></th>
<th id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T4.1.1.1.3.1" class="ltx_text" style="font-size:80%;">CityFlow-ReID</span></th>
</tr>
<tr id="S4.T4.1.2.2" class="ltx_tr">
<th id="S4.T4.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.2.2.1.1" class="ltx_text" style="font-size:80%;">Color acc.</span></th>
<th id="S4.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.2.2.2.1" class="ltx_text" style="font-size:80%;">Type acc.</span></th>
<th id="S4.T4.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.2.2.3.1" class="ltx_text" style="font-size:80%;">Color acc.</span></th>
<th id="S4.T4.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.1.2.2.4.1" class="ltx_text" style="font-size:80%;">Type acc.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.3.1" class="ltx_tr">
<th id="S4.T4.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.1.3.1.1.1" class="ltx_text" style="font-size:80%;">RS+MT</span></th>
<td id="S4.T4.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.3.1.2.1" class="ltx_text" style="font-size:80%;">93.42</span></td>
<td id="S4.T4.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.3.1.3.1" class="ltx_text" style="font-size:80%;">93.27</span></td>
<td id="S4.T4.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.3.1.4.1" class="ltx_text" style="font-size:80%;">80.16</span></td>
<td id="S4.T4.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.3.1.5.1" class="ltx_text" style="font-size:80%;">78.97</span></td>
</tr>
<tr id="S4.T4.1.4.2" class="ltx_tr">
<th id="S4.T4.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T4.1.4.2.1.1" class="ltx_text" style="font-size:80%;">RS+MT+K</span></th>
<td id="S4.T4.1.4.2.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.4.2.2.1" class="ltx_text" style="font-size:80%;">93.86</span></td>
<td id="S4.T4.1.4.2.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.4.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">93.53</span></td>
<td id="S4.T4.1.4.2.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.4.2.4.1" class="ltx_text" style="font-size:80%;">83.06</span></td>
<td id="S4.T4.1.4.2.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.4.2.5.1" class="ltx_text" style="font-size:80%;">79.17</span></td>
</tr>
<tr id="S4.T4.1.5.3" class="ltx_tr">
<th id="S4.T4.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T4.1.5.3.1.1" class="ltx_text" style="font-size:80%;">RS+MT+K+H</span></th>
<td id="S4.T4.1.5.3.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.5.3.2.1" class="ltx_text" style="font-size:80%;">94.06</span></td>
<td id="S4.T4.1.5.3.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.5.3.3.1" class="ltx_text" style="font-size:80%;">92.77</span></td>
<td id="S4.T4.1.5.3.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.5.3.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">84.80</span></td>
<td id="S4.T4.1.5.3.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.5.3.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">80.04</span></td>
</tr>
<tr id="S4.T4.1.6.4" class="ltx_tr">
<th id="S4.T4.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T4.1.6.4.1.1" class="ltx_text" style="font-size:80%;">RS+MT+K+S</span></th>
<td id="S4.T4.1.6.4.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.6.4.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">94.66</span></td>
<td id="S4.T4.1.6.4.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.6.4.3.1" class="ltx_text" style="font-size:80%;">92.80</span></td>
<td id="S4.T4.1.6.4.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.6.4.4.1" class="ltx_text" style="font-size:80%;">83.47</span></td>
<td id="S4.T4.1.6.4.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.6.4.5.1" class="ltx_text" style="font-size:80%;">79.41</span></td>
</tr>
<tr id="S4.T4.1.7.5" class="ltx_tr">
<th id="S4.T4.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T4.1.7.5.1.1" class="ltx_text" style="font-size:80%;">R+MT+K</span></th>
<td id="S4.T4.1.7.5.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.7.5.2.1" class="ltx_text" style="font-size:80%;">74.99</span></td>
<td id="S4.T4.1.7.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.7.5.3.1" class="ltx_text" style="font-size:80%;">90.38</span></td>
<td id="S4.T4.1.7.5.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.7.5.4.1" class="ltx_text" style="font-size:80%;">79.56</span></td>
<td id="S4.T4.1.7.5.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.1.7.5.5.1" class="ltx_text" style="font-size:80%;">76.84</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Experimental results of different variants of PAMTRI on color and type classification. The percentage of accuracy is shown. MT, K, H, S, RS and R respectively denote multi-task learning, explicit keypoints embedded, heatmaps embedded, segments embedded, training with both real and synthetic data, and training with real data only.</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Comparison of attribute classification</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Experimental results of color and type classification are given in Tab. <a href="#S4.T4" title="Table 4 ‣ 4.3 Comparison of ReID with the state-of-the-art ‣ 4 Evaluation ‣ PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
The evaluation metric is the accuracy in correctly identifying attributes.
Again, these results confirm that CityFlow-ReID exhibits higher difficulty compared to VeRi, due to the diversity of viewpoints and environments.
We also observe that the accuracy of type prediction is usually lower than that of color prediction, because some vehicle types look similar from the same viewpoint, <em id="S4.SS4.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS4.p1.1.2" class="ltx_text"></span>, a hatchback and a sedan from the same car model are likely to look the same from the front.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">It is worth noting that the pose embeddings significantly improve the classification performance.
As explained in Sec. <a href="#S3.SS3" title="3.3 Multi-task learning for vehicle ReID ‣ 3 Methodology ‣ PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>, pose information is directly linked with the definition of vehicle type, and the shape deformation by segments enables color estimation on the main body only.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">In general, the accuracy of attribute classification is much higher than that of identity recovery, which could be used to filter out vehicle pairs with low matching likelihood, and thus improve the computational efficiency of target association across cameras.
We leave this as future work.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Comparison of vehicle pose estimation</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">To evaluate vehicle pose estimation in 2D, we follow similar evaluation protocol as human pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, in which the threshold of errors is adaptively determined by the object’s size.
The standard in human-based evaluation is to use 50% of the head length which corresponds to 60% of the diagonal length of the ground-truth head bounding box.
Unlike humans, all the lengths between vehicle keypoints may change abruptly corresponding to viewing perspectives.
Therefore, we use 25% of the diagonal length of the entire vehicle bounding box as the reference, whereas the threshold is set the same as human-based evaluation.
For convenience, we divide the 36 keypoints into 6 body parts for individual accuracy measurements, and the mean accuracy of all the estimated keypoints is also presented.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Test set</span></th>
<th id="S4.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.1.1.2.1" class="ltx_text" style="font-size:80%;">Train set</span></th>
<th id="S4.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.1.1.3.1" class="ltx_text" style="font-size:80%;">Wheel acc.</span></th>
<th id="S4.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.1.1.4.1" class="ltx_text" style="font-size:80%;">Fender acc.</span></th>
<th id="S4.T5.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.1.1.5.1" class="ltx_text" style="font-size:80%;">Rear acc.</span></th>
<th id="S4.T5.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.1.1.6.1" class="ltx_text" style="font-size:80%;">Front acc.</span></th>
<th id="S4.T5.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.1.1.7.1" class="ltx_text" style="font-size:80%;">Rear win. acc.</span></th>
<th id="S4.T5.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.1.1.8.1" class="ltx_text" style="font-size:80%;">Front win. acc.</span></th>
<th id="S4.T5.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T5.1.1.1.9.1" class="ltx_text" style="font-size:80%;">Mean</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.2.1" class="ltx_tr">
<td id="S4.T5.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S4.T5.1.2.1.1.1" class="ltx_text" style="font-size:80%;">VeRi</span></td>
<td id="S4.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.2.1.2.1" class="ltx_text" style="font-size:80%;">VeRi</span></td>
<td id="S4.T5.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">85.10</span></td>
<td id="S4.T5.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.2.1.4.1" class="ltx_text" style="font-size:80%;">81.14</span></td>
<td id="S4.T5.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.2.1.5.1" class="ltx_text" style="font-size:80%;">69.20</span></td>
<td id="S4.T5.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.2.1.6.1" class="ltx_text" style="font-size:80%;">77.44</span></td>
<td id="S4.T5.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.2.1.7.1" class="ltx_text" style="font-size:80%;">85.67</span></td>
<td id="S4.T5.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.2.1.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">89.92</span></td>
<td id="S4.T5.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.2.1.9.1" class="ltx_text" style="font-size:80%;">82.15</span></td>
</tr>
<tr id="S4.T5.1.3.2" class="ltx_tr">
<td id="S4.T5.1.3.2.1" class="ltx_td ltx_align_center"><span id="S4.T5.1.3.2.1.1" class="ltx_text" style="font-size:80%;">CityFlow</span></td>
<td id="S4.T5.1.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T5.1.3.2.2.1" class="ltx_text" style="font-size:80%;">58.62</span></td>
<td id="S4.T5.1.3.2.3" class="ltx_td ltx_align_center"><span id="S4.T5.1.3.2.3.1" class="ltx_text" style="font-size:80%;">54.99</span></td>
<td id="S4.T5.1.3.2.4" class="ltx_td ltx_align_center"><span id="S4.T5.1.3.2.4.1" class="ltx_text" style="font-size:80%;">45.32</span></td>
<td id="S4.T5.1.3.2.5" class="ltx_td ltx_align_center"><span id="S4.T5.1.3.2.5.1" class="ltx_text" style="font-size:80%;">54.86</span></td>
<td id="S4.T5.1.3.2.6" class="ltx_td ltx_align_center"><span id="S4.T5.1.3.2.6.1" class="ltx_text" style="font-size:80%;">65.74</span></td>
<td id="S4.T5.1.3.2.7" class="ltx_td ltx_align_center"><span id="S4.T5.1.3.2.7.1" class="ltx_text" style="font-size:80%;">74.38</span></td>
<td id="S4.T5.1.3.2.8" class="ltx_td ltx_align_center"><span id="S4.T5.1.3.2.8.1" class="ltx_text" style="font-size:80%;">60.14</span></td>
</tr>
<tr id="S4.T5.1.4.3" class="ltx_tr">
<td id="S4.T5.1.4.3.1" class="ltx_td ltx_align_center"><span id="S4.T5.1.4.3.1.1" class="ltx_text" style="font-size:80%;">VeRi+Synthetic</span></td>
<td id="S4.T5.1.4.3.2" class="ltx_td ltx_align_center"><span id="S4.T5.1.4.3.2.1" class="ltx_text" style="font-size:80%;">84.93</span></td>
<td id="S4.T5.1.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T5.1.4.3.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">82.66</span></td>
<td id="S4.T5.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T5.1.4.3.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">71.73</span></td>
<td id="S4.T5.1.4.3.5" class="ltx_td ltx_align_center"><span id="S4.T5.1.4.3.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">77.72</span></td>
<td id="S4.T5.1.4.3.6" class="ltx_td ltx_align_center"><span id="S4.T5.1.4.3.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">86.41</span></td>
<td id="S4.T5.1.4.3.7" class="ltx_td ltx_align_center"><span id="S4.T5.1.4.3.7.1" class="ltx_text" style="font-size:80%;">89.86</span></td>
<td id="S4.T5.1.4.3.8" class="ltx_td ltx_align_center"><span id="S4.T5.1.4.3.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">83.16</span></td>
</tr>
<tr id="S4.T5.1.5.4" class="ltx_tr">
<td id="S4.T5.1.5.4.1" class="ltx_td ltx_align_center"><span id="S4.T5.1.5.4.1.1" class="ltx_text" style="font-size:80%;">CityFlow+Synthetic</span></td>
<td id="S4.T5.1.5.4.2" class="ltx_td ltx_align_center"><span id="S4.T5.1.5.4.2.1" class="ltx_text" style="font-size:80%;">64.03</span></td>
<td id="S4.T5.1.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T5.1.5.4.3.1" class="ltx_text" style="font-size:80%;">59.73</span></td>
<td id="S4.T5.1.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T5.1.5.4.4.1" class="ltx_text" style="font-size:80%;">45.10</span></td>
<td id="S4.T5.1.5.4.5" class="ltx_td ltx_align_center"><span id="S4.T5.1.5.4.5.1" class="ltx_text" style="font-size:80%;">54.73</span></td>
<td id="S4.T5.1.5.4.6" class="ltx_td ltx_align_center"><span id="S4.T5.1.5.4.6.1" class="ltx_text" style="font-size:80%;">63.93</span></td>
<td id="S4.T5.1.5.4.7" class="ltx_td ltx_align_center"><span id="S4.T5.1.5.4.7.1" class="ltx_text" style="font-size:80%;">76.14</span></td>
<td id="S4.T5.1.5.4.8" class="ltx_td ltx_align_center"><span id="S4.T5.1.5.4.8.1" class="ltx_text" style="font-size:80%;">62.13</span></td>
</tr>
<tr id="S4.T5.1.6.5" class="ltx_tr">
<td id="S4.T5.1.6.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="4"><span id="S4.T5.1.6.5.1.1" class="ltx_text" style="font-size:80%;">CityFlow</span></td>
<td id="S4.T5.1.6.5.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.6.5.2.1" class="ltx_text" style="font-size:80%;">VeRi</span></td>
<td id="S4.T5.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.6.5.3.1" class="ltx_text" style="font-size:80%;">70.89</span></td>
<td id="S4.T5.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.6.5.4.1" class="ltx_text" style="font-size:80%;">60.68</span></td>
<td id="S4.T5.1.6.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.6.5.5.1" class="ltx_text" style="font-size:80%;">46.66</span></td>
<td id="S4.T5.1.6.5.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.6.5.6.1" class="ltx_text" style="font-size:80%;">48.34</span></td>
<td id="S4.T5.1.6.5.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.6.5.7.1" class="ltx_text" style="font-size:80%;">56.77</span></td>
<td id="S4.T5.1.6.5.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.6.5.8.1" class="ltx_text" style="font-size:80%;">63.51</span></td>
<td id="S4.T5.1.6.5.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.6.5.9.1" class="ltx_text" style="font-size:80%;">58.27</span></td>
</tr>
<tr id="S4.T5.1.7.6" class="ltx_tr">
<td id="S4.T5.1.7.6.1" class="ltx_td ltx_align_center"><span id="S4.T5.1.7.6.1.1" class="ltx_text" style="font-size:80%;">CityFlow</span></td>
<td id="S4.T5.1.7.6.2" class="ltx_td ltx_align_center"><span id="S4.T5.1.7.6.2.1" class="ltx_text" style="font-size:80%;">83.75</span></td>
<td id="S4.T5.1.7.6.3" class="ltx_td ltx_align_center"><span id="S4.T5.1.7.6.3.1" class="ltx_text" style="font-size:80%;">79.89</span></td>
<td id="S4.T5.1.7.6.4" class="ltx_td ltx_align_center"><span id="S4.T5.1.7.6.4.1" class="ltx_text" style="font-size:80%;">65.87</span></td>
<td id="S4.T5.1.7.6.5" class="ltx_td ltx_align_center"><span id="S4.T5.1.7.6.5.1" class="ltx_text" style="font-size:80%;">71.48</span></td>
<td id="S4.T5.1.7.6.6" class="ltx_td ltx_align_center"><span id="S4.T5.1.7.6.6.1" class="ltx_text" style="font-size:80%;">75.38</span></td>
<td id="S4.T5.1.7.6.7" class="ltx_td ltx_align_center"><span id="S4.T5.1.7.6.7.1" class="ltx_text" style="font-size:80%;">80.80</span></td>
<td id="S4.T5.1.7.6.8" class="ltx_td ltx_align_center"><span id="S4.T5.1.7.6.8.1" class="ltx_text" style="font-size:80%;">77.07</span></td>
</tr>
<tr id="S4.T5.1.8.7" class="ltx_tr">
<td id="S4.T5.1.8.7.1" class="ltx_td ltx_align_center"><span id="S4.T5.1.8.7.1.1" class="ltx_text" style="font-size:80%;">VeRi+Synthetic</span></td>
<td id="S4.T5.1.8.7.2" class="ltx_td ltx_align_center"><span id="S4.T5.1.8.7.2.1" class="ltx_text" style="font-size:80%;">69.77</span></td>
<td id="S4.T5.1.8.7.3" class="ltx_td ltx_align_center"><span id="S4.T5.1.8.7.3.1" class="ltx_text" style="font-size:80%;">61.68</span></td>
<td id="S4.T5.1.8.7.4" class="ltx_td ltx_align_center"><span id="S4.T5.1.8.7.4.1" class="ltx_text" style="font-size:80%;">52.40</span></td>
<td id="S4.T5.1.8.7.5" class="ltx_td ltx_align_center"><span id="S4.T5.1.8.7.5.1" class="ltx_text" style="font-size:80%;">52.07</span></td>
<td id="S4.T5.1.8.7.6" class="ltx_td ltx_align_center"><span id="S4.T5.1.8.7.6.1" class="ltx_text" style="font-size:80%;">63.00</span></td>
<td id="S4.T5.1.8.7.7" class="ltx_td ltx_align_center"><span id="S4.T5.1.8.7.7.1" class="ltx_text" style="font-size:80%;">65.92</span></td>
<td id="S4.T5.1.8.7.8" class="ltx_td ltx_align_center"><span id="S4.T5.1.8.7.8.1" class="ltx_text" style="font-size:80%;">61.03</span></td>
</tr>
<tr id="S4.T5.1.9.8" class="ltx_tr">
<td id="S4.T5.1.9.8.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.9.8.1.1" class="ltx_text" style="font-size:80%;">CityFlow+Synthetic</span></td>
<td id="S4.T5.1.9.8.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.9.8.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">84.19</span></td>
<td id="S4.T5.1.9.8.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.9.8.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">80.91</span></td>
<td id="S4.T5.1.9.8.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.9.8.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">70.18</span></td>
<td id="S4.T5.1.9.8.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.9.8.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">72.37</span></td>
<td id="S4.T5.1.9.8.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.9.8.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">78.35</span></td>
<td id="S4.T5.1.9.8.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.9.8.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">82.12</span></td>
<td id="S4.T5.1.9.8.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.1.9.8.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">78.70</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>Experimental results of pose estimation using HRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> as backbone network. The 36 keypoints are grouped into 6 categories for individual evaluation. Shown are the percentage of keypoints located within the threshold; see text for details.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2005.00673/assets/fig_pose.jpg" id="S4.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="170" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Qualitative visualization of the performance of pose estimation (only high-confidence keypoints shown). The top 4 rows show results on VeRi, whereas the bottom 4 show results on CityFlow-ReID. For each, the rows represent the output from a different training set: VeRi, CityFlow-ReID, VeRi+synthetic, and CityFlow-ReID+synthetic, respectively. <span id="S4.F6.3.1" class="ltx_text ltx_font_bold">Best viewed in color.</span></figcaption>
</figure>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">We randomly withhold 10% of the real annotated identities to form the test set.
The training set consists of synthetic data and the remaining real data.
Our experimental results are displayed in Tab. <a href="#S4.T5" title="Table 5 ‣ 4.5 Comparison of vehicle pose estimation ‣ 4 Evaluation ‣ PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
It is important to note that though the domain gap still exists in pose estimation, the combination with synthetic data can help mitigate the inconsistency across real datasets.
In all the scenarios compared, when the network trained on one dataset is tested on the other, the keypoint accuracy increases as synthetic data are added during training.
On the other hand, when the network model is trained and tested on the same dataset, the performance gain is more obvious on CityFlow-ReID, because the synthetic data look visually similar.
Even with VeRi, improved precision can be seen in most of the individual parts, as well as the mean.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">From these results, we learn that the keypoints around the wheels, fenders, and windshield areas are more easily located, because of the strong edges around them.
On the contrary, the frontal and rear boundaries are harder to predict, as they usually vary across different car models.</p>
</div>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.1" class="ltx_p">Some qualitative results are demonstrated in Fig. <a href="#S4.F6" title="Figure 6 ‣ 4.5 Comparison of vehicle pose estimation ‣ 4 Evaluation ‣ PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
Most failure cases are from cross-domain learning, and it is noticeable that incorporating synthetic data improves robustness against unseen vehicle models and environments in the training set.
Moreover, as randomized lighting and occlusion are enforced in the generation of our synthetic data, they also lead to more reliable performance against such noise in the real world.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we propose a pose-aware multi-task learning network called PAMTRI for joint vehicle ReID and attribute classification.
Previous works either focus on one aspect or exploit metric learning and spatio-temporal information to match vehicle identities.
However, we note that vehicle attributes such as color and type are highly related to the deformable vehicle shape expressed through pose representations.
Therefore, in our designed framework, estimated heatmaps or segments are embedded with input batch images for training, and the predicted keypoint coordinates and confidence are concatenated with the deep learning features for multi-task learning.
This proposal relies on heavily annotated vehicle information on large-scale datasets, which has not yet been available.
Hence, we also generate a highly randomized synthetic dataset, in which a large variety of viewing angles and random noise such as strong shadow, occlusion, and cropped images are simulated.
Finally, extensive experiments are conducted on VeRi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and CityFlow-ReID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> to evaluate PAMTRI against state-of-the-art in vehicle ReID.
Our proposed framework achieves the top performance in both benchmarks, and an ablation study shows that each proposed component helps enhance robustness.
Furthermore, experiments show that our schemes also benefit the sub-tasks on attribute classification and vehicle pose estimation.
In the future, we plan to study how to more effectively bridge the domain gap between real and synthetic data.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">2D human pose estimation: New benchmark and state of the art
analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages 3686–3693, 2014.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Junaid Ahmed Ansari, Sarthak Sharma, Anshuman Majumdar, J. Krishna Murthy, and
K. Madhava Krishna.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">The earth ain’t flat: Monocular reconstruction of vehicles on steep
and graded roads from a moving camera.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IROS</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, pages 8404–8410, 2018.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Yan Bai, Yihang Lou, Feng Gao, Shiqi Wang, Yuwei Wu, and Ling-Yu Duan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Group sensitive triplet embedding for vehicle re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TMM</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 20(9):2385–2399, 2018.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac,
Nathan Ratliff, and Dieter Fox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Closing the sim-to-real loop: Adapting simulation randomization
with real world experience.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">arXiv:1810.05687, 2018.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">ImageNet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 248–255, 2009.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Alexander Hermans, Lucas Beyer, and Bastian Leibe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">In defense of the triplet loss for person re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">arXiv:1703.07737, 2017.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Densely connected convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 2261–2269, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Stephen James, Andrew J. Davison, and Edward Johns.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Transferring end-to-end visuomotor control from simulation to real
world for a multi-stage task.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">arXiv:1707.02267, 2017.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Diederik P. Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">arXiv:1412.6980, 2014.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Ratnesh Kumar, Edwin Weill, Farzin Aghdasi, and Parthsarathy Sriram.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Vehicle re-identification: An efficient baseline using triplet
embedding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">arXiv:1901.01015v3, 2019.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Abhijit Kundu, Yin Li, and James M. Rehg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">3D-RCNN: Instance-level 3D object reconstruction via
render-and-compare.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 3559–3568, 2018.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Yutian Lin, Liang Zheng, Zhedong Zheng, Yu Wu, Zhilan Hu, Chenggang Yan, and Yi
Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Improving person re-identification by attribute and identity
learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">arXiv:1703.07220, 2017.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Hongye Liu, Yonghong Tian, Yaowei Yang, Lu Pang, and Tiejun Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Deep relative distance learning: Tell the difference between
similar vehicles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 2167–2175, 2016.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Xinchen Liu, Wu Liu, Tao Mei, and Huadong Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">A deep learning-based approach to progressive vehicle
re-identification for urban surveillance.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. ECCV</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 869–884, 2016.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Xinchen Liu, Wu Liu, Tao Mei, and Huadong Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">PROVID: Progressive and multimodal vehicle reidentification for
large-scale urban surveillance.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TMM</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 20(3):645–658, 2017.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Fabian Manhardt, Wadim Kehl, and Adrien Gaidon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">ROI-10D: Monocular lifting of 2D detection to 6D pose and
metric shape.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 2069–2078, 2019.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers,
Alexey Dosovitskiy, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">A large dataset to train convolutional networks for disparity,
optical flow, and scene flow estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 4040–4048, 2016.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Arsalan Mousavian, Dragomir Anguelov, John Flynn, and Jana Kosecka.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">3D bounding box estimation using deep learning and geometry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 7074–7082, 2017.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Milind Naphade, Ming-Ching Chang, Anuj Sharma, David C. Anastasiu, Vamsi
Jagarlamudi, Pranamesh Chakraborty, Tingting Huang, Shuo Wang, Ming-Yu Liu,
Rama Chellappa, Jenq-Neng Hwang, and Siwei Lyu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">The 2018 NVIDIA AI City Challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR Workshops</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 53––60, 2018.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Milind Naphade, Zheng Tang, Ming-Ching Chang, David C. Anastasiu, Anuj Sharma,
Rama Chellappa, Shuo Wang, Pranamesh Chakraborty, Tingting Huang, Jenq-Neng
Hwang, and Siwei Lyu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">The 2019 AI City Challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR Workshops</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 452–460, 2019.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Alejandro Newell, Kaiyu Yang, and Jia Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Stacked hourglass networks for human pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. ECCV</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 483–499, 2016.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Aayush Prakash, Shaad Boochoon, Mark Brophy, David Acuna, Eric Cameracci,
Gavriel State, Omer Shapira, and Stan Birchfield.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Structured domain randomization: Bridging the reality gap by
context-aware synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">arXiv:1810.10093, 2018.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Ozan Sener and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Multi-task learning as multi-objective optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. NeurIPS</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 525–536, 2018.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Yantao Shen, Tong Xiao, Hongsheng Li, Shuai Yi, and Xiaogang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Learning deep neural networks for vehicle Re-ID with
visual-spatio-temporal path proposals.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. ICCV</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 1900–1909, 2017.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Deep high-resolution representation learning for human pose
estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 5693–5703, 2019.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, Manuel Brucker, and
Rudolph Triebel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Implicit 3D orientation learning for 6D object detection from
RGB images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. ECCV</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages 699–715, 2018.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Going deeper with convolutions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 1–9, 2015.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
Wojna.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Rethinking the Inception architecture for computer vision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 2818–2826, 2016.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner,
Steven Bohez, and Vincent Vanhoucke.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Sim-to-real: Learning agile locomotion for quadruped robots.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">arXiv:1804.10332, 2018.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Zheng Tang, Milind Naphade, Ming-Yu Liu, Xiaodong Yang, Stan Birchfield, Shuo
Wang, Ratnesh Kumar, David Anastasiu, and Jenq-Neng Hwang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">CityFlow: A city-scale benchmark for multi-target multi-camera
vehicle tracking and re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages 8797–8806, 2019.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Zheng Tang, Gaoang Wang, Tao Liu, Young-Gun Lee, Adwin Jahn, Xu Liu, Xiaodong
He, and Jenq-Neng Hwang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Multiple-kernel based vehicle tracking using 3D deformable model
and camera self-calibration.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">arXiv:1708.06831, 2017.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Zheng Tang, Gaoang Wang, Hao Xiao, Aotian Zheng, and Jenq-Neng Hwang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Single-camera and inter-camera vehicle tracking and 3D speed
estimation based on fusion of visual and semantic features.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR Workshops</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, pages 108–115, 2018.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Thang To, Jonathan Tremblay, Duncan McKay, Yukie Yamaguchi, Kirby Leung, Adrian
Balanon, Jia Cheng, and Stan Birchfield.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">NDDS: NVIDIA deep learning dataset synthesizer, 2018.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/NVIDIA/Dataset_Synthesizer" title="" class="ltx_ref ltx_href" style="font-size:90%;">https://github.com/NVIDIA/Dataset_Synthesizer</a><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and
Pieter Abbeel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Domain randomization for transferring deep neural networks from
simulation to the real world.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IROS</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 23–30, 2017.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem
Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Training deep networks with synthetic data: Bridging the reality
gap by domain randomization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR Workshops</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, pages 969–977, 2018.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Tremblay, Thang To, Balakumar Sundaralingam, Yu Xiang, Dieter Fox, and
Stan Birchfield.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Deep object pose estimation for semantic robotic grasping of
household objects.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CoRL</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages 306–316, 2018.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Zhongdao Wang, Luming Tang, Xihui Liu, Zhuliang Yao, Shuai Yi, Jing Shao,
Junjie Yan, Shengjin Wang, Hongsheng Li, and Xiaogang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Orientation invariant feature embedding and spatial temporal
regularization for vehicle re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. ICCV</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages 379–387, 2017.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">A discriminative feature learning approach for deep face recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. ECCV</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, pages 499–515, 2016.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Paul Wohlhart and Vincent Lepetit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Learning descriptors for object recognition and 3D pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, pages 3109–3118, 2015.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Empirical evaluation of rectified activations in convolutional
network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">arXiv:1505.00853, 2015.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Ke Yan, Yonghong Tian, Yaowei Wang, Wei Zeng, and Tiejun Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Exploiting multi-grain ranking constraints for precisely searching
visually-similar vehicles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. ICCV</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, pages 562–570, 2017.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Scalable person re-identification: A benchmark.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. ICCV</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, pages 1116–1124, 2015.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Zhedong Zheng, Xiaodong Yang, Zhiding Yu, Liang Zheng, Yi Yang, and Jan Kautz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Joint discriminative and generative learning for person
re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR</span><span id="bib.bib43.4.2" class="ltx_text" style="font-size:90%;">, pages 2138–2147, 2019.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Kaiyang Zhou and Tao Xiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Torchreid: A library for deep learning person re-identification in
PyTorch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">arXiv:1910.10093, 2019.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Yi Zhou and Ling Shao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Aware attentive multi-view inference for vehicle re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. CVPR</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, pages 6489–6498, 2018.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Yi Zhou and Ling Shao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Vehicle re-identification by adversarial bi-directional LSTM
network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. WACV</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, pages 653–662, 2018.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2005.00672" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2005.00673" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2005.00673">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2005.00673" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2005.00674" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 15:01:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
