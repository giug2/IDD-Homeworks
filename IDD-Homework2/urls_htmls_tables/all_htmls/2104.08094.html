<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2104.08094] Personalized Semi-Supervised Federated Learning for Human Activity Recognition</title><meta property="og:description" content="Sensor-based Human Activity Recognition (HAR) is a widely explored research area. The most effective data-driven methods for human activities prediction are based on supervised machine learning applied to the continuou…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Personalized Semi-Supervised Federated Learning for Human Activity Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Personalized Semi-Supervised Federated Learning for Human Activity Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2104.08094">

<!--Generated on Sun Mar 17 03:04:02 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="activity recognition,  federated learning,  semi-supervised learning,  mobile computing">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Personalized Semi-Supervised Federated Learning for Human Activity Recognition</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Claudio Bettini
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:claudio.bettini@unimi.it">claudio.bettini@unimi.it</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">EveryWare Lab, Dept. of Computer Science, University of Milan</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_city">Milan</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_country">Italy</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gabriele Civitarese
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:gabriele.civitarese@unimi.it">gabriele.civitarese@unimi.it</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_affiliation_institution">EveryWare Lab, Dept. of Computer Science, University of Milan</span><span id="id5.2.id2" class="ltx_text ltx_affiliation_city">Milan</span><span id="id6.3.id3" class="ltx_text ltx_affiliation_country">Italy</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Riccardo Presotto
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:riccardo.presotto@unimi.it">riccardo.presotto@unimi.it</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">EveryWare Lab, Dept. of Computer Science, University of Milan</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_city">Milan</span><span id="id9.3.id3" class="ltx_text ltx_affiliation_country">Italy</span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id10.id1" class="ltx_p">Sensor-based Human Activity Recognition (HAR) is a widely explored research area. The most effective data-driven methods for human activities prediction are based on supervised machine learning applied to the continuous stream of sensors data.
However, these methods perform well on restricted sets of activities in domains for which there is a fully labeled dataset. It is still a challenge to cope with the intra- and inter-variability of activity execution among different subjects that characterizes a large scale real world deployment.
Semi-supervised learning approaches for HAR
have been proposed to address the challenge of acquiring the large amount of labeled data
that is necessary in realistic settings.
However, their centralised architecture incurs in the scalability and privacy problems that arise when the process involves a large number of users.</p>
<p id="id11.id2" class="ltx_p">Federated Learning (FL) is a promising paradigm to address these problems. However, the FL methods that have been proposed for HAR assume that the participating users can always obtain labels to train their local models (i.e., they assume a fully supervised setting). In this work, we propose FedHAR: a novel hybrid method for HAR that combines semi-supervised and federated learning to take advantage of the strengths of both approaches. Indeed, FedHAR  combines active learning and label propagation to semi-automatically annotate the local streams of unlabeled sensor data, and it relies on FL to build a global activity model in a scalable and privacy-aware fashion. FedHAR  also includes a transfer learning strategy to personalize the global model on each user.
We evaluated our method on two public datasets, showing that FedHAR  reaches recognition rates and personalization capabilities similar to state-of-the-art
FL supervised approaches. As a major advantage, FedHAR  only requires a very limited number of annotated data to populate a pre-trained model and a small number of active learning questions that quickly decrease while using the system, leading to an effective and scalable solution for the data scarcity problem of HAR.
</p>
</div>
<div class="ltx_keywords">activity recognition, federated learning, semi-supervised learning, mobile computing
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>none</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_journal"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>TIST</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_journalvolume"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalvolume: </span>37</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_journalnumber"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalnumber: </span>4</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_article"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">article: </span>111</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_publicationmonth"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">publicationmonth: </span>8</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Ubiquitous and mobile computing systems and tools</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Semi-supervised learning settings</span></span></span><span id="id9" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Distributed algorithms</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The evolution of mobile computing and sensor technologies in the last decades made it possible to develop intelligent applications that continuously monitor our daily activities to enable context-aware services <cite class="ltx_cite ltx_citemacro_citep">(Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2012</a>)</cite>.
Off-the-shelf mobile and wearable devices that are pervasively adopted in our everyday life can be used to track physical movements and hence to constantly monitor daily activities. The majority of approaches for Human Activity Recognition (HAR) based on mobile and wearable devices in the literature rely on supervised learning methods to infer activities from the continuous stream of data generated by the sensors embedded in those devices <cite class="ltx_cite ltx_citemacro_citep">(Lara
et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2013</a>)</cite>.
While supervised learning leads to high recognition rates, collecting a significant amount of labeled data to train the recognition model is often a real challenge <cite class="ltx_cite ltx_citemacro_citep">(Cook
et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2013b</a>)</cite>.
For instance, data annotation can be performed directly by the monitored subject while performing activities (self-annotation). However, this approach is very obtrusive and error-prone. Alternatively, external observers can annotate the activity execution of a subject (in real-time or by semi-automatic video annotation). Annotation by external observation is particularly time-consuming and privacy-intrusive.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Among the many solutions that have been proposed to tackle the labeled data scarcity problem, semi-supervised learning techniques represent a promising research direction that has been explored in the last few years <cite class="ltx_cite ltx_citemacro_citep">(Abdallah et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite>. Semi-supervised methods only use a small amount of labeled data to initialize the recognition model, which is continuously updated taking advantage of unlabelled data. However, there are still several challenges that limit the deployment of those solutions in realistic scenarios. Indeed, even though semi-supervised approaches mitigate the data scarcity problem, they do not consider the scalability and privacy issues that arise in training a real-world recognition model that includes data from a large number of different users. From the scalability point of view, the computational effort that is required to train a global model significantly grows as the number of users increases. Considering privacy aspects, activity data may reveal sensitive information, like the daily behavior of a subject and her habits <cite class="ltx_cite ltx_citemacro_citep">(Bettini and
Riboni, <a href="#bib.bib7" title="" class="ltx_ref">2015</a>)</cite>.
Accurate HAR also requires a certain amount of personalization on the end users <cite class="ltx_cite ltx_citemacro_citep">(Weiss and
Lockhart, <a href="#bib.bib54" title="" class="ltx_ref">2012</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In 2016, Google introduced the Federated Learning (FL) framework <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2017</a>; Yang
et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2019</a>; Kairouz
et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite>. In the FL paradigm, the model training task is distributed over a multitude of nodes (e.g., mobile devices).
Each node uses its own labeled data to train a local model. The resulting model parameters of each participating node are forwarded to a server that is in charge of aggregating them. Finally, the server shares the aggregated parameters to the participating nodes.
FL is a promising direction to make activity recognition scalable for a large number of users. Moreover, FL
mitigates the privacy problem since only model parameters, and not actual data, are shared with the server, and privacy-preserving mechanisms (e.g., Secure MultiParty Computation, Differential Privacy) are used when aggregating parameters <cite class="ltx_cite ltx_citemacro_citep">(Yang
et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">FL has been recently applied to HAR showing that it can reach an accuracy very close to centralised methods <cite class="ltx_cite ltx_citemacro_citep">(Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>.
However, all existing solutions assume that each node has complete availability of labeled sensor data.
This is actually the general setting of existing works based on FL, that in the literature has been primarily considered for fully supervised learning tasks <cite class="ltx_cite ltx_citemacro_citep">(Kairouz
et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite>.
While this assumption may be valid for some applications (e.g., the Google approach for keyboard suggestions improvement relies on labeled data implicitly provided by users when typing or confirming suggestions <cite class="ltx_cite ltx_citemacro_citep">(Hard et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2018</a>)</cite>), it is not realistic for applications like HAR where labeled data availability is significantly limited. Extending FL to semi-supervised learning is one of the open challenges in this area <cite class="ltx_cite ltx_citemacro_citep">(Kairouz
et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this work, we propose FedHAR: a hybrid semi-supervised and FL framework that enables personalised privacy-aware and scalable HAR based on mobile and wearable devices. Differently from existing approaches, FedHAR  considers a limited availability of labeled data. In particular, FedHAR  combines active learning and label propagation to provide labels to a significantly large amount of unlabeled data. Semi-automatically labeled data are then used by each node to perform local training, thus obtaining the model parameters that are then transmitted to the server that aggregates them using Secure Multiparty Computation. Similar to existing approaches, FedHAR  also relies on transfer learning to fine-tune the global model for each user <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2020b</a>)</cite>. At the same time, FedHAR  is also designed to generate a global model that generalizes over unseen users.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Considering the limitations of existing evaluation methodologies for FL applied to HAR <cite class="ltx_cite ltx_citemacro_citep">(Ek
et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite>, we designed a novel evaluation methodology to robustly assess both the generalization and the personalization capabilities of our approach. The results of our experimental evaluation on two publicly available datasets show that FedHAR  reaches recognition rates close to state-of-the-art solutions that assume the complete availability of labeled data. Moreover, both the generalization and the personalization capabilities of FedHAR  keep increasing over time. Last but not least, the amount of triggered active learning questions is small and acceptable for a real-word deployment.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">To the best of our knowledge, FedHAR  is the first FL framework for HAR that tackles the data scarcity problem. Hence, we believe that FedHAR  is a significant step towards realistic deployments of HAR systems based on FL. Even if we evaluated FedHAR  only considering as target application activity recognition based on mobile and wearable devices, our method can also be applied to other HAR applications (e.g., HAR in smart-homes with environmental sensors) and more generally to other human-centered domains characterized by low availability of labeled data <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We better characterize the generality of our approach in Section <a href="#S5.SS1" title="5.1. Generality of the approach ‣ 5. Discussion ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>.</span></span></span>.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">In summary, the contributions of this work are the following:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We present FedHAR, a novel hybrid approach that combines federated, semi-supervised, and transfer learning to tackle the data scarcity problem for real-world personalized HAR.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We propose a novel strategy to reliably evaluate the evolution of the personalization and generalization capabilities of FedHAR  over time.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">An extensive evaluation on public datasets shows that FedHAR  reaches similar recognition rates with respect to well-known approaches that assume high-availability of labeled data. At the same time, FedHAR  triggers a small number of active learning questions that quickly decreases while using the system.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Labeled data scarcity in HAR</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Considering HAR based on data collected from mobile devices’ inertial sensors, the majority of approaches rely on supervised machine learning <cite class="ltx_cite ltx_citemacro_citep">(Kwapisz
et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2011</a>; Györbíró et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2009</a>; Sun
et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2010</a>; Bao and Intille, <a href="#bib.bib5" title="" class="ltx_ref">2004</a>; Bulling
et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2014</a>)</cite>. However, these approaches need a significant amount of labeled data to train the classifier. Indeed, different users may perform the same activities in very different ways, but also distinct activities may be associated with similar motion patterns. The annotation task is costly, time-consuming, intrusive, and hence prohibitive on a large scale <cite class="ltx_cite ltx_citemacro_citep">(Cook
et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2013b</a>)</cite>.
In the following, we summarize the main methodologies that have been proposed in the literature to mitigate this problem.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Some research efforts focused on knowledge-based approaches based on logical formalisms, especially targeting smart-home environments <cite class="ltx_cite ltx_citemacro_citep">(Chen and Nugent, <a href="#bib.bib14" title="" class="ltx_ref">2009</a>; Civitarese et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite>. These approaches usually rely on ontologies to represent the common-sense relationships between activities and sensed data. One of the main issues of knowledge-based approaches is their inadequacy to model the intrinsic uncertainty of sensor-based systems and the large variety of activity execution modalities.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Data augmentation is a more popular solution adopted in the literature to mitigate the data scarcity problem, especially considering imbalanced datasets <cite class="ltx_cite ltx_citemacro_citep">(Chawla
et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2002</a>; Rashid and Louis, <a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite>. In these approaches, the available labeled data are slightly perturbed to generate new labeled samples. With respect to our method, data augmentation is an orthogonal approach that could be integrated to further increase the amount of labeled data. Recently, data augmentation in HAR has also been tackled taking advantage of GAN models to generate synthetic data more realistic than the ones obtained by the above-mentioned approaches <cite class="ltx_cite ltx_citemacro_citep">(Wang
et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2018a</a>; Chan and Noor, <a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite>. However, GANs require to be trained with a significant amount of data.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">Many transfer learning approaches have been applied to HAR to fine-tune models learned from a source domain with available labeled data to a target domain with low-availability of labeled data <cite class="ltx_cite ltx_citemacro_citep">(Cook
et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2013a</a>; Wang
et al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2018b</a>; Sanabria
et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2021</a>; Soleimani and
Nazerfard, <a href="#bib.bib43" title="" class="ltx_ref">2021</a>)</cite>. FedHAR  relies on transfer learning to fine-tune the personal local model taking advantage of the global model trained by all the participating devices.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">An effective method to tackle data scarcity for HAR when the feature space is homogeneous (like in FedHAR) is semi-supervised learning <cite class="ltx_cite ltx_citemacro_citep">(Abdallah et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2018</a>; Stikic
et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2008</a>; Guan
et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2007</a>; Longstaff
et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2010</a>)</cite>. Semi-supervised methods only use a restricted labeled dataset to initialize the activity model. Then, a significant amount of unlabeled data is semi-automatically annotated. The most common semi-supervised approaches for HAR are self-learning  <cite class="ltx_cite ltx_citemacro_citep">(Longstaff
et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2010</a>)</cite>, label propagation <cite class="ltx_cite ltx_citemacro_citep">(Stikic
et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2009</a>)</cite>, co-learning <cite class="ltx_cite ltx_citemacro_citep">(Lee and Cho, <a href="#bib.bib34" title="" class="ltx_ref">2014</a>)</cite>, and active learning <cite class="ltx_cite ltx_citemacro_citep">(Miu
et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2015</a>; Abdallah et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2015</a>; Hossain
et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2017</a>)</cite>. Also, hybrid solutions based on semi-supervised learning and knowledge-based reasoning have been proposed in <cite class="ltx_cite ltx_citemacro_citep">(Bettini
et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>.
Existing semi-supervised solutions do not consider the scalability problems related to building a recognition model with a large number of users for real-world deployments.
Moreover, the data required to build such collaborative models is sensitive, as it could reveal private information about the users (e.g., user health condition and habits) <cite class="ltx_cite ltx_citemacro_citep">(Voigt and Von dem
Bussche, <a href="#bib.bib51" title="" class="ltx_ref">2017</a>; Samarati, <a href="#bib.bib40" title="" class="ltx_ref">2014</a>; Bettini and
Riboni, <a href="#bib.bib7" title="" class="ltx_ref">2015</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Federated Learning for HAR</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Recently, the FL paradigm has been proposed to distribute model training over a multitude of nodes <cite class="ltx_cite ltx_citemacro_citep">(Yang
et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2019</a>; Konečnỳ et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2016</a>; Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2017</a>; McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2017</a>; Kairouz
et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2019</a>; Damaskinos et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite>. A recent survey divides FL methods in three categories: horizontal, vertical, and transfer FL <cite class="ltx_cite ltx_citemacro_citep">(Yang
et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2019</a>)</cite>. FedHAR  is a horizontal FL method: the participating mobile devices share the same feature space, but they have a different space in samples (i.e., each device considers data for a specific user).</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">FL has been previously applied to mobile/wearable HAR to distribute the training of the activity recognition model among the participating devices <cite class="ltx_cite ltx_citemacro_citep">(Sozinov
et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2018</a>; Wu et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2020b</a>; Zhao
et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2020</a>; Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2020</a>; Wu
et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2020a</a>; Ek
et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite>.
Existing works show that FL solutions for HAR reach recognition accuracy similar to standard centralized models <cite class="ltx_cite ltx_citemacro_citep">(Sozinov
et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2018</a>)</cite>. Moreover, since personalization is an important aspect for HAR <cite class="ltx_cite ltx_citemacro_citep">(Weiss and
Lockhart, <a href="#bib.bib54" title="" class="ltx_ref">2012</a>)</cite>, existing works also show that applying transfer learning strategies to fine-tune the global model on each client leads to a significantly improved recognition rate <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2020b</a>; Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>.
One of the major drawbacks of these solutions is that they assume high availability of labeled data, hence considering a fully supervised setting.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">The combination of semi-supervised and federated learning for HAR was partially explored in <cite class="ltx_cite ltx_citemacro_citep">(Zhao
et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2020</a>)</cite>. However, that work has a different objective, since it focuses on collaboratively learning a deep feature representation of sensor data through autoencoders. The learned feature representation is then used to recognize activities on a labeled dataset in a fully supervised setting.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">A common limitation of the previously cited approaches is the methodology adopted to evaluate FL for HAR applications <cite class="ltx_cite ltx_citemacro_citep">(Ek
et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2020</a>; Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2020</a>; Wu
et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2020a</a>)</cite>. Indeed, none of the proposed methodologies truly assess the generality of the global model over users whose data have never been used for training. Moreover, only one iteration of the FL process is evaluated, while in a realistic deployment this process is repeated periodically (e.g., every night) with different data. In this work, we propose an evaluation methodology that overcomes the above-mentioned issues (see Section <a href="#S4.SS2" title="4.2. Evaluation methodology ‣ 4. Experimental evaluation ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>).</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>The FedHAR  framework</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Overview of FedHAR</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In the following, we summarize the architecture of FedHAR.
For the sake of this work and without loss of generality, we
illustrate FedHAR  applied to physical activity recognition based on inertial sensors data collected from personal mobile devices.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Federated Learning</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">The overall data flow of our FL approach is depicted in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1.1. Federated Learning ‣ 3.1. Overview of FedHAR ‣ 3. The FedHAR framework ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2104.08094/assets/global_arch.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="151" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Overall data flow of FedHAR.</figcaption>
</figure>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">Following the FL paradigm, the actors of FedHAR  are a server and a set of devices that cooperate to train and update the weights of a global activity recognition model.
Each device uses a certain amount of labeled data to perform local training of the model. As we will explain later, the labels are not naturally available, and they are obtained taking advantage of semi-supervised learning techniques. The resulting weights of the local models are transmitted to the server. The server takes care of performing a privacy-preserving aggregation of the weights deriving from the local models of the participating devices. The updated global model weights are then transmitted to each device, which will apply transfer learning methods to personalize it. The personalized local model is then used for activity classification.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>Local models</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">In FedHAR, each device stores two distinct local instances of the activity model (as depicted in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1.2. Local models ‣ 3.1. Overview of FedHAR ‣ 3. The FedHAR framework ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Note that Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1.2. Local models ‣ 3.1. Overview of FedHAR ‣ 3. The FedHAR framework ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows different architectures to clearly separate two different data flows. The first is related to the usage of the <span id="footnote2.1" class="ltx_text ltx_font_italic">Personalized Local Model</span> for real-time classification when the device receives the updated global weights from the server. The second is about using the <span id="footnote2.2" class="ltx_text ltx_font_italic">Local Model</span> during a FL process.</span></span></span>.
The first is the one used for classification, and it is called <span id="S3.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_italic">Personalized Local Model</span>. When a device receives an update about global model weights from the server, the <span id="S3.SS1.SSS2.p1.1.2" class="ltx_text ltx_font_italic">Personalized Local Model</span> is fine-tuned on the specific user using transfer learning methods.
As we will discuss in Section <a href="#S3.SS5" title="3.5. Model Personalization ‣ 3. The FedHAR framework ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a>, fine-tuning leads to a personal model that is specialized in recognizing the activities of the specific user.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">Each device also trains a twin model without applying personalization.
We refer to this model as <span id="S3.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_italic">Local Model</span> and its weights are the ones used during FL. The advantage of using the <span id="S3.SS1.SSS2.p2.1.2" class="ltx_text ltx_font_italic">Local Model</span> for FL is that it leads to a global model that better generalizes over unseen users. Keeping private the weights of the <span id="S3.SS1.SSS2.p2.1.3" class="ltx_text ltx_font_italic">Personalized Local Model</span> has also a positive impact on privacy protection.</p>
</div>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.1" class="ltx_p">In the following, we explain in detail how the local models are used in FedHAR.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2104.08094/assets/data_labelling.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="224" height="180" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Classification and active learning</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2104.08094/assets/local_training.png" id="S3.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="271" height="175" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Labels propagation and local model training</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>An overview of the FedHAR  framework.</figcaption>
</figure>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3. </span>Classification and data labeling</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">Figure <a href="#S3.F2.sf1" title="In Figure 2 ‣ 3.1.2. Local models ‣ 3.1. Overview of FedHAR ‣ 3. The FedHAR framework ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2a</span></a> depicts how the <span id="S3.SS1.SSS3.p1.1.1" class="ltx_text ltx_font_italic">Personalized Local Model</span> is used in the FedHAR  framework. When a node receives updated global model weights from the server, the <span id="S3.SS1.SSS3.p1.1.2" class="ltx_text ltx_font_smallcaps">Model Personalization</span> module applies transfer learning methods to fine-tune the global model on the user, thus updating the <span id="S3.SS1.SSS3.p1.1.3" class="ltx_text ltx_font_italic">Personalized Local Model</span>. This model is used for the classification of activities in real-time on the continuous stream of pre-processed sensor data. Before classification, each unlabeled data sample is stored in the <span id="S3.SS1.SSS3.p1.1.4" class="ltx_text ltx_font_italic">User Device Storage</span>. After classification, if the confidence on the current prediction is low, an active learning process is started, and the system asks the user about the activity that she was actually performing. The feedback from the user is finally associated with the corresponding data sample in the <span id="S3.SS1.SSS3.p1.1.5" class="ltx_text ltx_font_italic">User Device Storage</span>.</p>
</div>
</section>
<section id="S3.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.4. </span>Label propagation and local training</h4>

<div id="S3.SS1.SSS4.p1" class="ltx_para">
<p id="S3.SS1.SSS4.p1.1" class="ltx_p">Figure <a href="#S3.F2.sf2" title="In Figure 2 ‣ 3.1.2. Local models ‣ 3.1. Overview of FedHAR ‣ 3. The FedHAR framework ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2b</span></a> shows how the periodic training of the local models is performed in FedHAR. This process is triggered when the server requires to update the global model.
Before starting the FL process, the labeled and unlabeled data points stored in the <span id="S3.SS1.SSS4.p1.1.1" class="ltx_text ltx_font_italic">User Device Storage</span> are used to update the <span id="S3.SS1.SSS4.p1.1.2" class="ltx_text ltx_font_smallcaps">Label Propagation</span> model, which will automatically label a significant portion of unlabeled data. Then, the newly labeled data obtained after label propagation, as well as the ones obtained by active learning, are used to perform local training of both the <span id="S3.SS1.SSS4.p1.1.3" class="ltx_text ltx_font_italic">Personalized Local Model</span> and the <span id="S3.SS1.SSS4.p1.1.4" class="ltx_text ltx_font_italic">Local Model</span> during the FL process.
As we previously mentioned, only the weights of the <span id="S3.SS1.SSS4.p1.1.5" class="ltx_text ltx_font_italic">Local Model</span> are actually transmitted to the server during the FL process to update the global model. We will describe our federated learning strategy in more details in Section <a href="#S3.SS3" title="3.3. The Federated Learning strategy of FedHAR ‣ 3. The FedHAR framework ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Initialization of the global model</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">FedHAR  assumes a limited availability of labeled data. In particular, it only considers a restricted annotated dataset (we will call it <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">pre-training dataset</span> in the following) that is used to initialize the label propagation model and the weights of the global model <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Note that, considering out target application, a labeled dataset is a collection of timestamped inertial sensors data acquired from mobile/wearable devices during activity execution. Examples of such sensors are accelerometer, gyroscope, and magnetometer. The labels are annotated time intervals that indicate the time-span of each performed activity.</span></span></span>. In realistic settings, the <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">pre-training dataset </span> can be, for example, a combination of publicly available datasets, or a small training set specifically collected by a restricted number of volunteers.
Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2. Initialization of the global model ‣ 3. The FedHAR framework ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> describes the initialization mechanism of FedHAR.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2104.08094/assets/pre_train.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="254" height="113" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Initialization of the global model in FedHAR.</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">In FedHAR, the activity model is based on deep learning. However, learning features from such a small training set would lead to an unreliable feature representation, negatively impacting the recognition rate during classification.
Hence, FedHAR  relies on a pre-processing pipeline based on standard segmentation and handcrafted feature extraction that in the literature proved to be effective for HAR <cite class="ltx_cite ltx_citemacro_citep">(Bettini
et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>. In particular, for each axis of each inertial sensor, we consider the following features: <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_italic">average, variance, standard deviation, median, mean squared error, kurtosis, symmetry, zero-crossing rate, number of peaks, energy, and difference between maximum and minimum</span>. Hence, the global model is trained by feeding the network with the feature vectors extracted from the <span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_italic">pre-training dataset</span> as described above. Then, the resulting weights are transmitted to the participating devices. Clearly, the same pre-processing pipeline is also applied for classification on each device.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">The <span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_italic">pre-training dataset</span> is also used to initialize the Label Propagation model that is distributed to all the participating devices. More details about Label Propagation will be presented in Section <a href="#S3.SS4.SSS2" title="3.4.2. Label Propagation ‣ 3.4. Semi-supervised learning ‣ 3. The FedHAR framework ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4.2</span></a>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>The Federated Learning strategy of FedHAR</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">FedHAR  takes advantage of FL to distribute the training of the global activity model over a large number of devices.
Each participant contributes to training the global model with a certain amount of labeled data. We will explain in Section <a href="#S3.SS4" title="3.4. Semi-supervised learning ‣ 3. The FedHAR framework ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a> how labeled data are actually collected thanks to semi-supervised learning.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Periodically (e.g., each night) the server starts a global model update process. The devices that are available to perform computation (e.g, the ones idle and charging) inform the server that they are eligible to take part in the FL process. Afterwards, the server executes several communication rounds to update the weights of the global model.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">A <span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_italic">communication round</span> consists of the following steps:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">The server sends the latest version of the global weights to a fraction of the eligible devices </p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Each device uses its labeled data to perform training locally (both <span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Personalized Local Model</span> and <span id="S3.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">Local Model</span>)</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">When local training is completed, each device sends the new weights of the <span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Local Model</span> to the server</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">The server aggregates the local weights to compute the new global weights</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">The communication rounds are repeated until the global model converges.
Then, the new weights are transmitted to each participating device including the ones that did not contribute to the communication rounds. The FedHAR  component on the device will personalize the model (as explained in Section <a href="#S3.SS5" title="3.5. Model Personalization ‣ 3. The FedHAR framework ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a>), and use it for local activity classification.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">As proposed in FedAVG <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2017</a>)</cite>, the server updates the global model weights by executing a weighted average of the locally learned model weights provided from clients.
Since the local weights may reveal private information, the aggregation is performed using the Secure Multiparty Computation approach presented in <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Semi-supervised learning</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">As previously mentioned, in order to tackle the labeled data scarcity issue, FedHAR  relies on a combination of two semi-supervised learning techniques: <span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_italic">Active Learning</span> and <span id="S3.SS4.p1.1.2" class="ltx_text ltx_font_italic">Label Propagation</span>. The former is used to obtain the ground truth from the user only when the confidence on the prediction is low. The latter is used to propagate available labels to unlabeled data.</p>
</div>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1. </span>Active Learning</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.16" class="ltx_p">FedHAR  takes advantage of active learning to ask the user feedback about her currently performed activity when there is uncertainty in the classifier’s prediction.
In particular, we adopted a state-of-the-art non-parametric method called <span id="S3.SS4.SSS1.p1.16.1" class="ltx_text ltx_font_italic">VAR-UNCERTAINTY</span> <cite class="ltx_cite ltx_citemacro_citep">(Žliobaitė et al<span class="ltx_text">.</span>, <a href="#bib.bib63" title="" class="ltx_ref">2013</a>)</cite>.
This approach compares the prediction confidence with a threshold <math id="S3.SS4.SSS1.p1.1.m1.2" class="ltx_Math" alttext="\theta\in[0,1]" display="inline"><semantics id="S3.SS4.SSS1.p1.1.m1.2a"><mrow id="S3.SS4.SSS1.p1.1.m1.2.3" xref="S3.SS4.SSS1.p1.1.m1.2.3.cmml"><mi id="S3.SS4.SSS1.p1.1.m1.2.3.2" xref="S3.SS4.SSS1.p1.1.m1.2.3.2.cmml">θ</mi><mo id="S3.SS4.SSS1.p1.1.m1.2.3.1" xref="S3.SS4.SSS1.p1.1.m1.2.3.1.cmml">∈</mo><mrow id="S3.SS4.SSS1.p1.1.m1.2.3.3.2" xref="S3.SS4.SSS1.p1.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS4.SSS1.p1.1.m1.2.3.3.2.1" xref="S3.SS4.SSS1.p1.1.m1.2.3.3.1.cmml">[</mo><mn id="S3.SS4.SSS1.p1.1.m1.1.1" xref="S3.SS4.SSS1.p1.1.m1.1.1.cmml">0</mn><mo id="S3.SS4.SSS1.p1.1.m1.2.3.3.2.2" xref="S3.SS4.SSS1.p1.1.m1.2.3.3.1.cmml">,</mo><mn id="S3.SS4.SSS1.p1.1.m1.2.2" xref="S3.SS4.SSS1.p1.1.m1.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS4.SSS1.p1.1.m1.2.3.3.2.3" xref="S3.SS4.SSS1.p1.1.m1.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.1.m1.2b"><apply id="S3.SS4.SSS1.p1.1.m1.2.3.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.3"><in id="S3.SS4.SSS1.p1.1.m1.2.3.1.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.3.1"></in><ci id="S3.SS4.SSS1.p1.1.m1.2.3.2.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.3.2">𝜃</ci><interval closure="closed" id="S3.SS4.SSS1.p1.1.m1.2.3.3.1.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.3.3.2"><cn type="integer" id="S3.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS1.p1.1.m1.1.1">0</cn><cn type="integer" id="S3.SS4.SSS1.p1.1.m1.2.2.cmml" xref="S3.SS4.SSS1.p1.1.m1.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.1.m1.2c">\theta\in[0,1]</annotation></semantics></math> that is dynamically adjusted over time. Initially, <math id="S3.SS4.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS4.SSS1.p1.2.m2.1a"><mi id="S3.SS4.SSS1.p1.2.m2.1.1" xref="S3.SS4.SSS1.p1.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.2.m2.1b"><ci id="S3.SS4.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS4.SSS1.p1.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.2.m2.1c">\theta</annotation></semantics></math> is initialised to <math id="S3.SS4.SSS1.p1.3.m3.1" class="ltx_Math" alttext="\theta=1" display="inline"><semantics id="S3.SS4.SSS1.p1.3.m3.1a"><mrow id="S3.SS4.SSS1.p1.3.m3.1.1" xref="S3.SS4.SSS1.p1.3.m3.1.1.cmml"><mi id="S3.SS4.SSS1.p1.3.m3.1.1.2" xref="S3.SS4.SSS1.p1.3.m3.1.1.2.cmml">θ</mi><mo id="S3.SS4.SSS1.p1.3.m3.1.1.1" xref="S3.SS4.SSS1.p1.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS4.SSS1.p1.3.m3.1.1.3" xref="S3.SS4.SSS1.p1.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.3.m3.1b"><apply id="S3.SS4.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS4.SSS1.p1.3.m3.1.1"><eq id="S3.SS4.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS4.SSS1.p1.3.m3.1.1.1"></eq><ci id="S3.SS4.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS4.SSS1.p1.3.m3.1.1.2">𝜃</ci><cn type="integer" id="S3.SS4.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS4.SSS1.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.3.m3.1c">\theta=1</annotation></semantics></math>.
Let <math id="S3.SS4.SSS1.p1.4.m4.4" class="ltx_Math" alttext="\mathbf{A}=\{A_{1},A_{2},\dots,A_{n}\}" display="inline"><semantics id="S3.SS4.SSS1.p1.4.m4.4a"><mrow id="S3.SS4.SSS1.p1.4.m4.4.4" xref="S3.SS4.SSS1.p1.4.m4.4.4.cmml"><mi id="S3.SS4.SSS1.p1.4.m4.4.4.5" xref="S3.SS4.SSS1.p1.4.m4.4.4.5.cmml">𝐀</mi><mo id="S3.SS4.SSS1.p1.4.m4.4.4.4" xref="S3.SS4.SSS1.p1.4.m4.4.4.4.cmml">=</mo><mrow id="S3.SS4.SSS1.p1.4.m4.4.4.3.3" xref="S3.SS4.SSS1.p1.4.m4.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS4.SSS1.p1.4.m4.4.4.3.3.4" xref="S3.SS4.SSS1.p1.4.m4.4.4.3.4.cmml">{</mo><msub id="S3.SS4.SSS1.p1.4.m4.2.2.1.1.1" xref="S3.SS4.SSS1.p1.4.m4.2.2.1.1.1.cmml"><mi id="S3.SS4.SSS1.p1.4.m4.2.2.1.1.1.2" xref="S3.SS4.SSS1.p1.4.m4.2.2.1.1.1.2.cmml">A</mi><mn id="S3.SS4.SSS1.p1.4.m4.2.2.1.1.1.3" xref="S3.SS4.SSS1.p1.4.m4.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS4.SSS1.p1.4.m4.4.4.3.3.5" xref="S3.SS4.SSS1.p1.4.m4.4.4.3.4.cmml">,</mo><msub id="S3.SS4.SSS1.p1.4.m4.3.3.2.2.2" xref="S3.SS4.SSS1.p1.4.m4.3.3.2.2.2.cmml"><mi id="S3.SS4.SSS1.p1.4.m4.3.3.2.2.2.2" xref="S3.SS4.SSS1.p1.4.m4.3.3.2.2.2.2.cmml">A</mi><mn id="S3.SS4.SSS1.p1.4.m4.3.3.2.2.2.3" xref="S3.SS4.SSS1.p1.4.m4.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS4.SSS1.p1.4.m4.4.4.3.3.6" xref="S3.SS4.SSS1.p1.4.m4.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS4.SSS1.p1.4.m4.1.1" xref="S3.SS4.SSS1.p1.4.m4.1.1.cmml">…</mi><mo id="S3.SS4.SSS1.p1.4.m4.4.4.3.3.7" xref="S3.SS4.SSS1.p1.4.m4.4.4.3.4.cmml">,</mo><msub id="S3.SS4.SSS1.p1.4.m4.4.4.3.3.3" xref="S3.SS4.SSS1.p1.4.m4.4.4.3.3.3.cmml"><mi id="S3.SS4.SSS1.p1.4.m4.4.4.3.3.3.2" xref="S3.SS4.SSS1.p1.4.m4.4.4.3.3.3.2.cmml">A</mi><mi id="S3.SS4.SSS1.p1.4.m4.4.4.3.3.3.3" xref="S3.SS4.SSS1.p1.4.m4.4.4.3.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="S3.SS4.SSS1.p1.4.m4.4.4.3.3.8" xref="S3.SS4.SSS1.p1.4.m4.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.4.m4.4b"><apply id="S3.SS4.SSS1.p1.4.m4.4.4.cmml" xref="S3.SS4.SSS1.p1.4.m4.4.4"><eq id="S3.SS4.SSS1.p1.4.m4.4.4.4.cmml" xref="S3.SS4.SSS1.p1.4.m4.4.4.4"></eq><ci id="S3.SS4.SSS1.p1.4.m4.4.4.5.cmml" xref="S3.SS4.SSS1.p1.4.m4.4.4.5">𝐀</ci><set id="S3.SS4.SSS1.p1.4.m4.4.4.3.4.cmml" xref="S3.SS4.SSS1.p1.4.m4.4.4.3.3"><apply id="S3.SS4.SSS1.p1.4.m4.2.2.1.1.1.cmml" xref="S3.SS4.SSS1.p1.4.m4.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.4.m4.2.2.1.1.1.1.cmml" xref="S3.SS4.SSS1.p1.4.m4.2.2.1.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.p1.4.m4.2.2.1.1.1.2.cmml" xref="S3.SS4.SSS1.p1.4.m4.2.2.1.1.1.2">𝐴</ci><cn type="integer" id="S3.SS4.SSS1.p1.4.m4.2.2.1.1.1.3.cmml" xref="S3.SS4.SSS1.p1.4.m4.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS4.SSS1.p1.4.m4.3.3.2.2.2.cmml" xref="S3.SS4.SSS1.p1.4.m4.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.4.m4.3.3.2.2.2.1.cmml" xref="S3.SS4.SSS1.p1.4.m4.3.3.2.2.2">subscript</csymbol><ci id="S3.SS4.SSS1.p1.4.m4.3.3.2.2.2.2.cmml" xref="S3.SS4.SSS1.p1.4.m4.3.3.2.2.2.2">𝐴</ci><cn type="integer" id="S3.SS4.SSS1.p1.4.m4.3.3.2.2.2.3.cmml" xref="S3.SS4.SSS1.p1.4.m4.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS4.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS4.SSS1.p1.4.m4.1.1">…</ci><apply id="S3.SS4.SSS1.p1.4.m4.4.4.3.3.3.cmml" xref="S3.SS4.SSS1.p1.4.m4.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.4.m4.4.4.3.3.3.1.cmml" xref="S3.SS4.SSS1.p1.4.m4.4.4.3.3.3">subscript</csymbol><ci id="S3.SS4.SSS1.p1.4.m4.4.4.3.3.3.2.cmml" xref="S3.SS4.SSS1.p1.4.m4.4.4.3.3.3.2">𝐴</ci><ci id="S3.SS4.SSS1.p1.4.m4.4.4.3.3.3.3.cmml" xref="S3.SS4.SSS1.p1.4.m4.4.4.3.3.3.3">𝑛</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.4.m4.4c">\mathbf{A}=\{A_{1},A_{2},\dots,A_{n}\}</annotation></semantics></math> be the set of target activities. Given the probability distribution over the possible activities of the current prediction <math id="S3.SS4.SSS1.p1.5.m5.4" class="ltx_Math" alttext="\langle p_{1},p_{2},\dots,p_{n}\rangle" display="inline"><semantics id="S3.SS4.SSS1.p1.5.m5.4a"><mrow id="S3.SS4.SSS1.p1.5.m5.4.4.3" xref="S3.SS4.SSS1.p1.5.m5.4.4.4.cmml"><mo stretchy="false" id="S3.SS4.SSS1.p1.5.m5.4.4.3.4" xref="S3.SS4.SSS1.p1.5.m5.4.4.4.cmml">⟨</mo><msub id="S3.SS4.SSS1.p1.5.m5.2.2.1.1" xref="S3.SS4.SSS1.p1.5.m5.2.2.1.1.cmml"><mi id="S3.SS4.SSS1.p1.5.m5.2.2.1.1.2" xref="S3.SS4.SSS1.p1.5.m5.2.2.1.1.2.cmml">p</mi><mn id="S3.SS4.SSS1.p1.5.m5.2.2.1.1.3" xref="S3.SS4.SSS1.p1.5.m5.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS4.SSS1.p1.5.m5.4.4.3.5" xref="S3.SS4.SSS1.p1.5.m5.4.4.4.cmml">,</mo><msub id="S3.SS4.SSS1.p1.5.m5.3.3.2.2" xref="S3.SS4.SSS1.p1.5.m5.3.3.2.2.cmml"><mi id="S3.SS4.SSS1.p1.5.m5.3.3.2.2.2" xref="S3.SS4.SSS1.p1.5.m5.3.3.2.2.2.cmml">p</mi><mn id="S3.SS4.SSS1.p1.5.m5.3.3.2.2.3" xref="S3.SS4.SSS1.p1.5.m5.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS4.SSS1.p1.5.m5.4.4.3.6" xref="S3.SS4.SSS1.p1.5.m5.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS4.SSS1.p1.5.m5.1.1" xref="S3.SS4.SSS1.p1.5.m5.1.1.cmml">…</mi><mo id="S3.SS4.SSS1.p1.5.m5.4.4.3.7" xref="S3.SS4.SSS1.p1.5.m5.4.4.4.cmml">,</mo><msub id="S3.SS4.SSS1.p1.5.m5.4.4.3.3" xref="S3.SS4.SSS1.p1.5.m5.4.4.3.3.cmml"><mi id="S3.SS4.SSS1.p1.5.m5.4.4.3.3.2" xref="S3.SS4.SSS1.p1.5.m5.4.4.3.3.2.cmml">p</mi><mi id="S3.SS4.SSS1.p1.5.m5.4.4.3.3.3" xref="S3.SS4.SSS1.p1.5.m5.4.4.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="S3.SS4.SSS1.p1.5.m5.4.4.3.8" xref="S3.SS4.SSS1.p1.5.m5.4.4.4.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.5.m5.4b"><list id="S3.SS4.SSS1.p1.5.m5.4.4.4.cmml" xref="S3.SS4.SSS1.p1.5.m5.4.4.3"><apply id="S3.SS4.SSS1.p1.5.m5.2.2.1.1.cmml" xref="S3.SS4.SSS1.p1.5.m5.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.5.m5.2.2.1.1.1.cmml" xref="S3.SS4.SSS1.p1.5.m5.2.2.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.p1.5.m5.2.2.1.1.2.cmml" xref="S3.SS4.SSS1.p1.5.m5.2.2.1.1.2">𝑝</ci><cn type="integer" id="S3.SS4.SSS1.p1.5.m5.2.2.1.1.3.cmml" xref="S3.SS4.SSS1.p1.5.m5.2.2.1.1.3">1</cn></apply><apply id="S3.SS4.SSS1.p1.5.m5.3.3.2.2.cmml" xref="S3.SS4.SSS1.p1.5.m5.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.5.m5.3.3.2.2.1.cmml" xref="S3.SS4.SSS1.p1.5.m5.3.3.2.2">subscript</csymbol><ci id="S3.SS4.SSS1.p1.5.m5.3.3.2.2.2.cmml" xref="S3.SS4.SSS1.p1.5.m5.3.3.2.2.2">𝑝</ci><cn type="integer" id="S3.SS4.SSS1.p1.5.m5.3.3.2.2.3.cmml" xref="S3.SS4.SSS1.p1.5.m5.3.3.2.2.3">2</cn></apply><ci id="S3.SS4.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS4.SSS1.p1.5.m5.1.1">…</ci><apply id="S3.SS4.SSS1.p1.5.m5.4.4.3.3.cmml" xref="S3.SS4.SSS1.p1.5.m5.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.5.m5.4.4.3.3.1.cmml" xref="S3.SS4.SSS1.p1.5.m5.4.4.3.3">subscript</csymbol><ci id="S3.SS4.SSS1.p1.5.m5.4.4.3.3.2.cmml" xref="S3.SS4.SSS1.p1.5.m5.4.4.3.3.2">𝑝</ci><ci id="S3.SS4.SSS1.p1.5.m5.4.4.3.3.3.cmml" xref="S3.SS4.SSS1.p1.5.m5.4.4.3.3.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.5.m5.4c">\langle p_{1},p_{2},\dots,p_{n}\rangle</annotation></semantics></math>, we denote with <math id="S3.SS4.SSS1.p1.6.m6.1" class="ltx_Math" alttext="p^{\star}=\max_{i}p_{i}" display="inline"><semantics id="S3.SS4.SSS1.p1.6.m6.1a"><mrow id="S3.SS4.SSS1.p1.6.m6.1.1" xref="S3.SS4.SSS1.p1.6.m6.1.1.cmml"><msup id="S3.SS4.SSS1.p1.6.m6.1.1.2" xref="S3.SS4.SSS1.p1.6.m6.1.1.2.cmml"><mi id="S3.SS4.SSS1.p1.6.m6.1.1.2.2" xref="S3.SS4.SSS1.p1.6.m6.1.1.2.2.cmml">p</mi><mo id="S3.SS4.SSS1.p1.6.m6.1.1.2.3" xref="S3.SS4.SSS1.p1.6.m6.1.1.2.3.cmml">⋆</mo></msup><mo id="S3.SS4.SSS1.p1.6.m6.1.1.1" xref="S3.SS4.SSS1.p1.6.m6.1.1.1.cmml">=</mo><mrow id="S3.SS4.SSS1.p1.6.m6.1.1.3" xref="S3.SS4.SSS1.p1.6.m6.1.1.3.cmml"><msub id="S3.SS4.SSS1.p1.6.m6.1.1.3.1" xref="S3.SS4.SSS1.p1.6.m6.1.1.3.1.cmml"><mi id="S3.SS4.SSS1.p1.6.m6.1.1.3.1.2" xref="S3.SS4.SSS1.p1.6.m6.1.1.3.1.2.cmml">max</mi><mi id="S3.SS4.SSS1.p1.6.m6.1.1.3.1.3" xref="S3.SS4.SSS1.p1.6.m6.1.1.3.1.3.cmml">i</mi></msub><mo lspace="0.167em" id="S3.SS4.SSS1.p1.6.m6.1.1.3a" xref="S3.SS4.SSS1.p1.6.m6.1.1.3.cmml">⁡</mo><msub id="S3.SS4.SSS1.p1.6.m6.1.1.3.2" xref="S3.SS4.SSS1.p1.6.m6.1.1.3.2.cmml"><mi id="S3.SS4.SSS1.p1.6.m6.1.1.3.2.2" xref="S3.SS4.SSS1.p1.6.m6.1.1.3.2.2.cmml">p</mi><mi id="S3.SS4.SSS1.p1.6.m6.1.1.3.2.3" xref="S3.SS4.SSS1.p1.6.m6.1.1.3.2.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.6.m6.1b"><apply id="S3.SS4.SSS1.p1.6.m6.1.1.cmml" xref="S3.SS4.SSS1.p1.6.m6.1.1"><eq id="S3.SS4.SSS1.p1.6.m6.1.1.1.cmml" xref="S3.SS4.SSS1.p1.6.m6.1.1.1"></eq><apply id="S3.SS4.SSS1.p1.6.m6.1.1.2.cmml" xref="S3.SS4.SSS1.p1.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.6.m6.1.1.2.1.cmml" xref="S3.SS4.SSS1.p1.6.m6.1.1.2">superscript</csymbol><ci id="S3.SS4.SSS1.p1.6.m6.1.1.2.2.cmml" xref="S3.SS4.SSS1.p1.6.m6.1.1.2.2">𝑝</ci><ci id="S3.SS4.SSS1.p1.6.m6.1.1.2.3.cmml" xref="S3.SS4.SSS1.p1.6.m6.1.1.2.3">⋆</ci></apply><apply id="S3.SS4.SSS1.p1.6.m6.1.1.3.cmml" xref="S3.SS4.SSS1.p1.6.m6.1.1.3"><apply id="S3.SS4.SSS1.p1.6.m6.1.1.3.1.cmml" xref="S3.SS4.SSS1.p1.6.m6.1.1.3.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.6.m6.1.1.3.1.1.cmml" xref="S3.SS4.SSS1.p1.6.m6.1.1.3.1">subscript</csymbol><max id="S3.SS4.SSS1.p1.6.m6.1.1.3.1.2.cmml" xref="S3.SS4.SSS1.p1.6.m6.1.1.3.1.2"></max><ci id="S3.SS4.SSS1.p1.6.m6.1.1.3.1.3.cmml" xref="S3.SS4.SSS1.p1.6.m6.1.1.3.1.3">𝑖</ci></apply><apply id="S3.SS4.SSS1.p1.6.m6.1.1.3.2.cmml" xref="S3.SS4.SSS1.p1.6.m6.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.6.m6.1.1.3.2.1.cmml" xref="S3.SS4.SSS1.p1.6.m6.1.1.3.2">subscript</csymbol><ci id="S3.SS4.SSS1.p1.6.m6.1.1.3.2.2.cmml" xref="S3.SS4.SSS1.p1.6.m6.1.1.3.2.2">𝑝</ci><ci id="S3.SS4.SSS1.p1.6.m6.1.1.3.2.3.cmml" xref="S3.SS4.SSS1.p1.6.m6.1.1.3.2.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.6.m6.1c">p^{\star}=\max_{i}p_{i}</annotation></semantics></math> the probability value of the most likely activity <math id="S3.SS4.SSS1.p1.7.m7.1" class="ltx_Math" alttext="A^{\star}\in\mathbf{A}" display="inline"><semantics id="S3.SS4.SSS1.p1.7.m7.1a"><mrow id="S3.SS4.SSS1.p1.7.m7.1.1" xref="S3.SS4.SSS1.p1.7.m7.1.1.cmml"><msup id="S3.SS4.SSS1.p1.7.m7.1.1.2" xref="S3.SS4.SSS1.p1.7.m7.1.1.2.cmml"><mi id="S3.SS4.SSS1.p1.7.m7.1.1.2.2" xref="S3.SS4.SSS1.p1.7.m7.1.1.2.2.cmml">A</mi><mo id="S3.SS4.SSS1.p1.7.m7.1.1.2.3" xref="S3.SS4.SSS1.p1.7.m7.1.1.2.3.cmml">⋆</mo></msup><mo id="S3.SS4.SSS1.p1.7.m7.1.1.1" xref="S3.SS4.SSS1.p1.7.m7.1.1.1.cmml">∈</mo><mi id="S3.SS4.SSS1.p1.7.m7.1.1.3" xref="S3.SS4.SSS1.p1.7.m7.1.1.3.cmml">𝐀</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.7.m7.1b"><apply id="S3.SS4.SSS1.p1.7.m7.1.1.cmml" xref="S3.SS4.SSS1.p1.7.m7.1.1"><in id="S3.SS4.SSS1.p1.7.m7.1.1.1.cmml" xref="S3.SS4.SSS1.p1.7.m7.1.1.1"></in><apply id="S3.SS4.SSS1.p1.7.m7.1.1.2.cmml" xref="S3.SS4.SSS1.p1.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.7.m7.1.1.2.1.cmml" xref="S3.SS4.SSS1.p1.7.m7.1.1.2">superscript</csymbol><ci id="S3.SS4.SSS1.p1.7.m7.1.1.2.2.cmml" xref="S3.SS4.SSS1.p1.7.m7.1.1.2.2">𝐴</ci><ci id="S3.SS4.SSS1.p1.7.m7.1.1.2.3.cmml" xref="S3.SS4.SSS1.p1.7.m7.1.1.2.3">⋆</ci></apply><ci id="S3.SS4.SSS1.p1.7.m7.1.1.3.cmml" xref="S3.SS4.SSS1.p1.7.m7.1.1.3">𝐀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.7.m7.1c">A^{\star}\in\mathbf{A}</annotation></semantics></math> (i.e., the predicted activity).
If <math id="S3.SS4.SSS1.p1.8.m8.1" class="ltx_Math" alttext="p^{\star}" display="inline"><semantics id="S3.SS4.SSS1.p1.8.m8.1a"><msup id="S3.SS4.SSS1.p1.8.m8.1.1" xref="S3.SS4.SSS1.p1.8.m8.1.1.cmml"><mi id="S3.SS4.SSS1.p1.8.m8.1.1.2" xref="S3.SS4.SSS1.p1.8.m8.1.1.2.cmml">p</mi><mo id="S3.SS4.SSS1.p1.8.m8.1.1.3" xref="S3.SS4.SSS1.p1.8.m8.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.8.m8.1b"><apply id="S3.SS4.SSS1.p1.8.m8.1.1.cmml" xref="S3.SS4.SSS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.8.m8.1.1.1.cmml" xref="S3.SS4.SSS1.p1.8.m8.1.1">superscript</csymbol><ci id="S3.SS4.SSS1.p1.8.m8.1.1.2.cmml" xref="S3.SS4.SSS1.p1.8.m8.1.1.2">𝑝</ci><ci id="S3.SS4.SSS1.p1.8.m8.1.1.3.cmml" xref="S3.SS4.SSS1.p1.8.m8.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.8.m8.1c">p^{\star}</annotation></semantics></math> is below <math id="S3.SS4.SSS1.p1.9.m9.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS4.SSS1.p1.9.m9.1a"><mi id="S3.SS4.SSS1.p1.9.m9.1.1" xref="S3.SS4.SSS1.p1.9.m9.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.9.m9.1b"><ci id="S3.SS4.SSS1.p1.9.m9.1.1.cmml" xref="S3.SS4.SSS1.p1.9.m9.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.9.m9.1c">\theta</annotation></semantics></math>, we consider the system uncertain about the current activity performed by the user. In this case, an active learning process is started by asking the user the ground truth <math id="S3.SS4.SSS1.p1.10.m10.1" class="ltx_Math" alttext="A^{f}\in\mathbf{A}" display="inline"><semantics id="S3.SS4.SSS1.p1.10.m10.1a"><mrow id="S3.SS4.SSS1.p1.10.m10.1.1" xref="S3.SS4.SSS1.p1.10.m10.1.1.cmml"><msup id="S3.SS4.SSS1.p1.10.m10.1.1.2" xref="S3.SS4.SSS1.p1.10.m10.1.1.2.cmml"><mi id="S3.SS4.SSS1.p1.10.m10.1.1.2.2" xref="S3.SS4.SSS1.p1.10.m10.1.1.2.2.cmml">A</mi><mi id="S3.SS4.SSS1.p1.10.m10.1.1.2.3" xref="S3.SS4.SSS1.p1.10.m10.1.1.2.3.cmml">f</mi></msup><mo id="S3.SS4.SSS1.p1.10.m10.1.1.1" xref="S3.SS4.SSS1.p1.10.m10.1.1.1.cmml">∈</mo><mi id="S3.SS4.SSS1.p1.10.m10.1.1.3" xref="S3.SS4.SSS1.p1.10.m10.1.1.3.cmml">𝐀</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.10.m10.1b"><apply id="S3.SS4.SSS1.p1.10.m10.1.1.cmml" xref="S3.SS4.SSS1.p1.10.m10.1.1"><in id="S3.SS4.SSS1.p1.10.m10.1.1.1.cmml" xref="S3.SS4.SSS1.p1.10.m10.1.1.1"></in><apply id="S3.SS4.SSS1.p1.10.m10.1.1.2.cmml" xref="S3.SS4.SSS1.p1.10.m10.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.10.m10.1.1.2.1.cmml" xref="S3.SS4.SSS1.p1.10.m10.1.1.2">superscript</csymbol><ci id="S3.SS4.SSS1.p1.10.m10.1.1.2.2.cmml" xref="S3.SS4.SSS1.p1.10.m10.1.1.2.2">𝐴</ci><ci id="S3.SS4.SSS1.p1.10.m10.1.1.2.3.cmml" xref="S3.SS4.SSS1.p1.10.m10.1.1.2.3">𝑓</ci></apply><ci id="S3.SS4.SSS1.p1.10.m10.1.1.3.cmml" xref="S3.SS4.SSS1.p1.10.m10.1.1.3">𝐀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.10.m10.1c">A^{f}\in\mathbf{A}</annotation></semantics></math> about the current activity. The feedback <math id="S3.SS4.SSS1.p1.11.m11.1" class="ltx_Math" alttext="A^{f}" display="inline"><semantics id="S3.SS4.SSS1.p1.11.m11.1a"><msup id="S3.SS4.SSS1.p1.11.m11.1.1" xref="S3.SS4.SSS1.p1.11.m11.1.1.cmml"><mi id="S3.SS4.SSS1.p1.11.m11.1.1.2" xref="S3.SS4.SSS1.p1.11.m11.1.1.2.cmml">A</mi><mi id="S3.SS4.SSS1.p1.11.m11.1.1.3" xref="S3.SS4.SSS1.p1.11.m11.1.1.3.cmml">f</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.11.m11.1b"><apply id="S3.SS4.SSS1.p1.11.m11.1.1.cmml" xref="S3.SS4.SSS1.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.11.m11.1.1.1.cmml" xref="S3.SS4.SSS1.p1.11.m11.1.1">superscript</csymbol><ci id="S3.SS4.SSS1.p1.11.m11.1.1.2.cmml" xref="S3.SS4.SSS1.p1.11.m11.1.1.2">𝐴</ci><ci id="S3.SS4.SSS1.p1.11.m11.1.1.3.cmml" xref="S3.SS4.SSS1.p1.11.m11.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.11.m11.1c">A^{f}</annotation></semantics></math> is stored in the <span id="S3.SS4.SSS1.p1.16.2" class="ltx_text ltx_font_italic">User Device Storage</span>. When <math id="S3.SS4.SSS1.p1.12.m12.1" class="ltx_Math" alttext="A^{f}=A^{\star}" display="inline"><semantics id="S3.SS4.SSS1.p1.12.m12.1a"><mrow id="S3.SS4.SSS1.p1.12.m12.1.1" xref="S3.SS4.SSS1.p1.12.m12.1.1.cmml"><msup id="S3.SS4.SSS1.p1.12.m12.1.1.2" xref="S3.SS4.SSS1.p1.12.m12.1.1.2.cmml"><mi id="S3.SS4.SSS1.p1.12.m12.1.1.2.2" xref="S3.SS4.SSS1.p1.12.m12.1.1.2.2.cmml">A</mi><mi id="S3.SS4.SSS1.p1.12.m12.1.1.2.3" xref="S3.SS4.SSS1.p1.12.m12.1.1.2.3.cmml">f</mi></msup><mo id="S3.SS4.SSS1.p1.12.m12.1.1.1" xref="S3.SS4.SSS1.p1.12.m12.1.1.1.cmml">=</mo><msup id="S3.SS4.SSS1.p1.12.m12.1.1.3" xref="S3.SS4.SSS1.p1.12.m12.1.1.3.cmml"><mi id="S3.SS4.SSS1.p1.12.m12.1.1.3.2" xref="S3.SS4.SSS1.p1.12.m12.1.1.3.2.cmml">A</mi><mo id="S3.SS4.SSS1.p1.12.m12.1.1.3.3" xref="S3.SS4.SSS1.p1.12.m12.1.1.3.3.cmml">⋆</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.12.m12.1b"><apply id="S3.SS4.SSS1.p1.12.m12.1.1.cmml" xref="S3.SS4.SSS1.p1.12.m12.1.1"><eq id="S3.SS4.SSS1.p1.12.m12.1.1.1.cmml" xref="S3.SS4.SSS1.p1.12.m12.1.1.1"></eq><apply id="S3.SS4.SSS1.p1.12.m12.1.1.2.cmml" xref="S3.SS4.SSS1.p1.12.m12.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.12.m12.1.1.2.1.cmml" xref="S3.SS4.SSS1.p1.12.m12.1.1.2">superscript</csymbol><ci id="S3.SS4.SSS1.p1.12.m12.1.1.2.2.cmml" xref="S3.SS4.SSS1.p1.12.m12.1.1.2.2">𝐴</ci><ci id="S3.SS4.SSS1.p1.12.m12.1.1.2.3.cmml" xref="S3.SS4.SSS1.p1.12.m12.1.1.2.3">𝑓</ci></apply><apply id="S3.SS4.SSS1.p1.12.m12.1.1.3.cmml" xref="S3.SS4.SSS1.p1.12.m12.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.12.m12.1.1.3.1.cmml" xref="S3.SS4.SSS1.p1.12.m12.1.1.3">superscript</csymbol><ci id="S3.SS4.SSS1.p1.12.m12.1.1.3.2.cmml" xref="S3.SS4.SSS1.p1.12.m12.1.1.3.2">𝐴</ci><ci id="S3.SS4.SSS1.p1.12.m12.1.1.3.3.cmml" xref="S3.SS4.SSS1.p1.12.m12.1.1.3.3">⋆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.12.m12.1c">A^{f}=A^{\star}</annotation></semantics></math>, it means that the most likely activity <math id="S3.SS4.SSS1.p1.13.m13.1" class="ltx_Math" alttext="A^{\star}" display="inline"><semantics id="S3.SS4.SSS1.p1.13.m13.1a"><msup id="S3.SS4.SSS1.p1.13.m13.1.1" xref="S3.SS4.SSS1.p1.13.m13.1.1.cmml"><mi id="S3.SS4.SSS1.p1.13.m13.1.1.2" xref="S3.SS4.SSS1.p1.13.m13.1.1.2.cmml">A</mi><mo id="S3.SS4.SSS1.p1.13.m13.1.1.3" xref="S3.SS4.SSS1.p1.13.m13.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.13.m13.1b"><apply id="S3.SS4.SSS1.p1.13.m13.1.1.cmml" xref="S3.SS4.SSS1.p1.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.13.m13.1.1.1.cmml" xref="S3.SS4.SSS1.p1.13.m13.1.1">superscript</csymbol><ci id="S3.SS4.SSS1.p1.13.m13.1.1.2.cmml" xref="S3.SS4.SSS1.p1.13.m13.1.1.2">𝐴</ci><ci id="S3.SS4.SSS1.p1.13.m13.1.1.3.cmml" xref="S3.SS4.SSS1.p1.13.m13.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.13.m13.1c">A^{\star}</annotation></semantics></math> is actually the one performed by the user, and hence the threshold <math id="S3.SS4.SSS1.p1.14.m14.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS4.SSS1.p1.14.m14.1a"><mi id="S3.SS4.SSS1.p1.14.m14.1.1" xref="S3.SS4.SSS1.p1.14.m14.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.14.m14.1b"><ci id="S3.SS4.SSS1.p1.14.m14.1.1.cmml" xref="S3.SS4.SSS1.p1.14.m14.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.14.m14.1c">\theta</annotation></semantics></math> is decreased to reduce the number of questions. On the other hand, when <math id="S3.SS4.SSS1.p1.15.m15.1" class="ltx_Math" alttext="A^{f}\neq A^{\star}" display="inline"><semantics id="S3.SS4.SSS1.p1.15.m15.1a"><mrow id="S3.SS4.SSS1.p1.15.m15.1.1" xref="S3.SS4.SSS1.p1.15.m15.1.1.cmml"><msup id="S3.SS4.SSS1.p1.15.m15.1.1.2" xref="S3.SS4.SSS1.p1.15.m15.1.1.2.cmml"><mi id="S3.SS4.SSS1.p1.15.m15.1.1.2.2" xref="S3.SS4.SSS1.p1.15.m15.1.1.2.2.cmml">A</mi><mi id="S3.SS4.SSS1.p1.15.m15.1.1.2.3" xref="S3.SS4.SSS1.p1.15.m15.1.1.2.3.cmml">f</mi></msup><mo id="S3.SS4.SSS1.p1.15.m15.1.1.1" xref="S3.SS4.SSS1.p1.15.m15.1.1.1.cmml">≠</mo><msup id="S3.SS4.SSS1.p1.15.m15.1.1.3" xref="S3.SS4.SSS1.p1.15.m15.1.1.3.cmml"><mi id="S3.SS4.SSS1.p1.15.m15.1.1.3.2" xref="S3.SS4.SSS1.p1.15.m15.1.1.3.2.cmml">A</mi><mo id="S3.SS4.SSS1.p1.15.m15.1.1.3.3" xref="S3.SS4.SSS1.p1.15.m15.1.1.3.3.cmml">⋆</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.15.m15.1b"><apply id="S3.SS4.SSS1.p1.15.m15.1.1.cmml" xref="S3.SS4.SSS1.p1.15.m15.1.1"><neq id="S3.SS4.SSS1.p1.15.m15.1.1.1.cmml" xref="S3.SS4.SSS1.p1.15.m15.1.1.1"></neq><apply id="S3.SS4.SSS1.p1.15.m15.1.1.2.cmml" xref="S3.SS4.SSS1.p1.15.m15.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.15.m15.1.1.2.1.cmml" xref="S3.SS4.SSS1.p1.15.m15.1.1.2">superscript</csymbol><ci id="S3.SS4.SSS1.p1.15.m15.1.1.2.2.cmml" xref="S3.SS4.SSS1.p1.15.m15.1.1.2.2">𝐴</ci><ci id="S3.SS4.SSS1.p1.15.m15.1.1.2.3.cmml" xref="S3.SS4.SSS1.p1.15.m15.1.1.2.3">𝑓</ci></apply><apply id="S3.SS4.SSS1.p1.15.m15.1.1.3.cmml" xref="S3.SS4.SSS1.p1.15.m15.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS1.p1.15.m15.1.1.3.1.cmml" xref="S3.SS4.SSS1.p1.15.m15.1.1.3">superscript</csymbol><ci id="S3.SS4.SSS1.p1.15.m15.1.1.3.2.cmml" xref="S3.SS4.SSS1.p1.15.m15.1.1.3.2">𝐴</ci><ci id="S3.SS4.SSS1.p1.15.m15.1.1.3.3.cmml" xref="S3.SS4.SSS1.p1.15.m15.1.1.3.3">⋆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.15.m15.1c">A^{f}\neq A^{\star}</annotation></semantics></math>, <math id="S3.SS4.SSS1.p1.16.m16.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS4.SSS1.p1.16.m16.1a"><mi id="S3.SS4.SSS1.p1.16.m16.1.1" xref="S3.SS4.SSS1.p1.16.m16.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.p1.16.m16.1b"><ci id="S3.SS4.SSS1.p1.16.m16.1.1.cmml" xref="S3.SS4.SSS1.p1.16.m16.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.p1.16.m16.1c">\theta</annotation></semantics></math> is increased. More details about this active learning strategy can be found in <cite class="ltx_cite ltx_citemacro_citep">(Žliobaitė et al<span class="ltx_text">.</span>, <a href="#bib.bib63" title="" class="ltx_ref">2013</a>)</cite>.</p>
</div>
<div id="S3.SS4.SSS1.p2" class="ltx_para">
<p id="S3.SS4.SSS1.p2.1" class="ltx_p">In this work, we assume that active learning questions are prompted to the user in real-time through a dedicated application with a user-friendly interface. For the sake of usability, when querying the user, FedHAR  only presents a couple of alternatives taken from the most probable activities. Figure <a href="#S3.F4" title="Figure 4 ‣ 3.4.1. Active Learning ‣ 3.4. Semi-supervised learning ‣ 3. The FedHAR framework ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows a screenshot of an active learning application that we implemented for smart-watches in another research work.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2104.08094/assets/x1.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="72" height="86" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Example of an active learning interface for smart-watches.</figcaption>
</figure>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2. </span>Label Propagation</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p">In order to increase the number of labeled samples to perform local training, FedHAR  also relies on label propagation.
Given a set of labeled and unlabeled data points, the goal of label propagation is to automatically annotate a portion of unlabeled data <cite class="ltx_cite ltx_citemacro_citep">(Zhu and
Ghahramani, <a href="#bib.bib62" title="" class="ltx_ref">2002</a>)</cite>. In FedHAR , the label propagation model is initialized with the <span id="S3.SS4.SSS2.p1.1.1" class="ltx_text ltx_font_italic">pre-training dataset</span>, and new labeled data points are obtained thanks to active learning.</p>
</div>
<div id="S3.SS4.SSS2.p2" class="ltx_para">
<p id="S3.SS4.SSS2.p2.9" class="ltx_p">The intuition behind label propagation is that data points close in the feature space likely correspond to the same class label. The Label Propagation model is a fully connected graph <math id="S3.SS4.SSS2.p2.1.m1.2" class="ltx_Math" alttext="g=(V,E)" display="inline"><semantics id="S3.SS4.SSS2.p2.1.m1.2a"><mrow id="S3.SS4.SSS2.p2.1.m1.2.3" xref="S3.SS4.SSS2.p2.1.m1.2.3.cmml"><mi id="S3.SS4.SSS2.p2.1.m1.2.3.2" xref="S3.SS4.SSS2.p2.1.m1.2.3.2.cmml">g</mi><mo id="S3.SS4.SSS2.p2.1.m1.2.3.1" xref="S3.SS4.SSS2.p2.1.m1.2.3.1.cmml">=</mo><mrow id="S3.SS4.SSS2.p2.1.m1.2.3.3.2" xref="S3.SS4.SSS2.p2.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS4.SSS2.p2.1.m1.2.3.3.2.1" xref="S3.SS4.SSS2.p2.1.m1.2.3.3.1.cmml">(</mo><mi id="S3.SS4.SSS2.p2.1.m1.1.1" xref="S3.SS4.SSS2.p2.1.m1.1.1.cmml">V</mi><mo id="S3.SS4.SSS2.p2.1.m1.2.3.3.2.2" xref="S3.SS4.SSS2.p2.1.m1.2.3.3.1.cmml">,</mo><mi id="S3.SS4.SSS2.p2.1.m1.2.2" xref="S3.SS4.SSS2.p2.1.m1.2.2.cmml">E</mi><mo stretchy="false" id="S3.SS4.SSS2.p2.1.m1.2.3.3.2.3" xref="S3.SS4.SSS2.p2.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p2.1.m1.2b"><apply id="S3.SS4.SSS2.p2.1.m1.2.3.cmml" xref="S3.SS4.SSS2.p2.1.m1.2.3"><eq id="S3.SS4.SSS2.p2.1.m1.2.3.1.cmml" xref="S3.SS4.SSS2.p2.1.m1.2.3.1"></eq><ci id="S3.SS4.SSS2.p2.1.m1.2.3.2.cmml" xref="S3.SS4.SSS2.p2.1.m1.2.3.2">𝑔</ci><interval closure="open" id="S3.SS4.SSS2.p2.1.m1.2.3.3.1.cmml" xref="S3.SS4.SSS2.p2.1.m1.2.3.3.2"><ci id="S3.SS4.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS4.SSS2.p2.1.m1.1.1">𝑉</ci><ci id="S3.SS4.SSS2.p2.1.m1.2.2.cmml" xref="S3.SS4.SSS2.p2.1.m1.2.2">𝐸</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p2.1.m1.2c">g=(V,E)</annotation></semantics></math> where the nodes <math id="S3.SS4.SSS2.p2.2.m2.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS4.SSS2.p2.2.m2.1a"><mi id="S3.SS4.SSS2.p2.2.m2.1.1" xref="S3.SS4.SSS2.p2.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p2.2.m2.1b"><ci id="S3.SS4.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS4.SSS2.p2.2.m2.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p2.2.m2.1c">V</annotation></semantics></math> are the data samples (labeled or unlabeled) and the weight on each edge in <math id="S3.SS4.SSS2.p2.3.m3.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.SS4.SSS2.p2.3.m3.1a"><mi id="S3.SS4.SSS2.p2.3.m3.1.1" xref="S3.SS4.SSS2.p2.3.m3.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p2.3.m3.1b"><ci id="S3.SS4.SSS2.p2.3.m3.1.1.cmml" xref="S3.SS4.SSS2.p2.3.m3.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p2.3.m3.1c">E</annotation></semantics></math> is the similarity between the connected data points.
In the literature, this similarity is usually computed using K-Nearest Neighbors (KNN) or Radial Basis Function Kernel (RBF kernel). FedHAR  relies on the RBF kernel due to its trade-off between computational costs and accuracy <cite class="ltx_cite ltx_citemacro_citep">(Widmann and
Verberne, <a href="#bib.bib55" title="" class="ltx_ref">2017</a>)</cite>. Formally, the RBF kernel function is defined as <math id="S3.SS4.SSS2.p2.4.m4.3" class="ltx_Math" alttext="K(x,x^{\prime})=e^{-\gamma||x-x^{\prime}||^{2}}" display="inline"><semantics id="S3.SS4.SSS2.p2.4.m4.3a"><mrow id="S3.SS4.SSS2.p2.4.m4.3.3" xref="S3.SS4.SSS2.p2.4.m4.3.3.cmml"><mrow id="S3.SS4.SSS2.p2.4.m4.3.3.1" xref="S3.SS4.SSS2.p2.4.m4.3.3.1.cmml"><mi id="S3.SS4.SSS2.p2.4.m4.3.3.1.3" xref="S3.SS4.SSS2.p2.4.m4.3.3.1.3.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS2.p2.4.m4.3.3.1.2" xref="S3.SS4.SSS2.p2.4.m4.3.3.1.2.cmml">​</mo><mrow id="S3.SS4.SSS2.p2.4.m4.3.3.1.1.1" xref="S3.SS4.SSS2.p2.4.m4.3.3.1.1.2.cmml"><mo stretchy="false" id="S3.SS4.SSS2.p2.4.m4.3.3.1.1.1.2" xref="S3.SS4.SSS2.p2.4.m4.3.3.1.1.2.cmml">(</mo><mi id="S3.SS4.SSS2.p2.4.m4.2.2" xref="S3.SS4.SSS2.p2.4.m4.2.2.cmml">x</mi><mo id="S3.SS4.SSS2.p2.4.m4.3.3.1.1.1.3" xref="S3.SS4.SSS2.p2.4.m4.3.3.1.1.2.cmml">,</mo><msup id="S3.SS4.SSS2.p2.4.m4.3.3.1.1.1.1" xref="S3.SS4.SSS2.p2.4.m4.3.3.1.1.1.1.cmml"><mi id="S3.SS4.SSS2.p2.4.m4.3.3.1.1.1.1.2" xref="S3.SS4.SSS2.p2.4.m4.3.3.1.1.1.1.2.cmml">x</mi><mo id="S3.SS4.SSS2.p2.4.m4.3.3.1.1.1.1.3" xref="S3.SS4.SSS2.p2.4.m4.3.3.1.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S3.SS4.SSS2.p2.4.m4.3.3.1.1.1.4" xref="S3.SS4.SSS2.p2.4.m4.3.3.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.SS4.SSS2.p2.4.m4.3.3.2" xref="S3.SS4.SSS2.p2.4.m4.3.3.2.cmml">=</mo><msup id="S3.SS4.SSS2.p2.4.m4.3.3.3" xref="S3.SS4.SSS2.p2.4.m4.3.3.3.cmml"><mi id="S3.SS4.SSS2.p2.4.m4.3.3.3.2" xref="S3.SS4.SSS2.p2.4.m4.3.3.3.2.cmml">e</mi><mrow id="S3.SS4.SSS2.p2.4.m4.1.1.1" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.cmml"><mo id="S3.SS4.SSS2.p2.4.m4.1.1.1a" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.cmml">−</mo><mrow id="S3.SS4.SSS2.p2.4.m4.1.1.1.1" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.cmml"><mi id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.3" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.3.cmml">γ</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.2" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.2.cmml">​</mo><msup id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.cmml"><mrow id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.2.cmml"><mo maxsize="142%" minsize="142%" id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.2" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.2" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.2.cmml">x</mi><mo id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.1" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.1.cmml">−</mo><msup id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.3" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.3.2" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.3.2.cmml">x</mi><mo id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.3.3" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.3.3.cmml">′</mo></msup></mrow><mo maxsize="142%" minsize="142%" id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.3" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.3" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p2.4.m4.3b"><apply id="S3.SS4.SSS2.p2.4.m4.3.3.cmml" xref="S3.SS4.SSS2.p2.4.m4.3.3"><eq id="S3.SS4.SSS2.p2.4.m4.3.3.2.cmml" xref="S3.SS4.SSS2.p2.4.m4.3.3.2"></eq><apply id="S3.SS4.SSS2.p2.4.m4.3.3.1.cmml" xref="S3.SS4.SSS2.p2.4.m4.3.3.1"><times id="S3.SS4.SSS2.p2.4.m4.3.3.1.2.cmml" xref="S3.SS4.SSS2.p2.4.m4.3.3.1.2"></times><ci id="S3.SS4.SSS2.p2.4.m4.3.3.1.3.cmml" xref="S3.SS4.SSS2.p2.4.m4.3.3.1.3">𝐾</ci><interval closure="open" id="S3.SS4.SSS2.p2.4.m4.3.3.1.1.2.cmml" xref="S3.SS4.SSS2.p2.4.m4.3.3.1.1.1"><ci id="S3.SS4.SSS2.p2.4.m4.2.2.cmml" xref="S3.SS4.SSS2.p2.4.m4.2.2">𝑥</ci><apply id="S3.SS4.SSS2.p2.4.m4.3.3.1.1.1.1.cmml" xref="S3.SS4.SSS2.p2.4.m4.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p2.4.m4.3.3.1.1.1.1.1.cmml" xref="S3.SS4.SSS2.p2.4.m4.3.3.1.1.1.1">superscript</csymbol><ci id="S3.SS4.SSS2.p2.4.m4.3.3.1.1.1.1.2.cmml" xref="S3.SS4.SSS2.p2.4.m4.3.3.1.1.1.1.2">𝑥</ci><ci id="S3.SS4.SSS2.p2.4.m4.3.3.1.1.1.1.3.cmml" xref="S3.SS4.SSS2.p2.4.m4.3.3.1.1.1.1.3">′</ci></apply></interval></apply><apply id="S3.SS4.SSS2.p2.4.m4.3.3.3.cmml" xref="S3.SS4.SSS2.p2.4.m4.3.3.3"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p2.4.m4.3.3.3.1.cmml" xref="S3.SS4.SSS2.p2.4.m4.3.3.3">superscript</csymbol><ci id="S3.SS4.SSS2.p2.4.m4.3.3.3.2.cmml" xref="S3.SS4.SSS2.p2.4.m4.3.3.3.2">𝑒</ci><apply id="S3.SS4.SSS2.p2.4.m4.1.1.1.cmml" xref="S3.SS4.SSS2.p2.4.m4.1.1.1"><minus id="S3.SS4.SSS2.p2.4.m4.1.1.1.2.cmml" xref="S3.SS4.SSS2.p2.4.m4.1.1.1"></minus><apply id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.cmml" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1"><times id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.2.cmml" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.2"></times><ci id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.3.cmml" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.3">𝛾</ci><apply id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.cmml" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1">superscript</csymbol><apply id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.2.1.cmml" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1"><minus id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.1"></minus><ci id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.2">𝑥</ci><apply id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.3.2">𝑥</ci><ci id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.1.1.1.3.3">′</ci></apply></apply></apply><cn type="integer" id="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS4.SSS2.p2.4.m4.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p2.4.m4.3c">K(x,x^{\prime})=e^{-\gamma||x-x^{\prime}||^{2}}</annotation></semantics></math> where <math id="S3.SS4.SSS2.p2.5.m5.1" class="ltx_Math" alttext="||x-x^{\prime}||^{2}" display="inline"><semantics id="S3.SS4.SSS2.p2.5.m5.1a"><msup id="S3.SS4.SSS2.p2.5.m5.1.1" xref="S3.SS4.SSS2.p2.5.m5.1.1.cmml"><mrow id="S3.SS4.SSS2.p2.5.m5.1.1.1.1" xref="S3.SS4.SSS2.p2.5.m5.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS4.SSS2.p2.5.m5.1.1.1.1.2" xref="S3.SS4.SSS2.p2.5.m5.1.1.1.2.1.cmml">‖</mo><mrow id="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1" xref="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.cmml"><mi id="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.2" xref="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.2.cmml">x</mi><mo id="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.1" xref="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.1.cmml">−</mo><msup id="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.3" xref="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.3.cmml"><mi id="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.3.2" xref="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.3.2.cmml">x</mi><mo id="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.3.3" xref="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.3.3.cmml">′</mo></msup></mrow><mo stretchy="false" id="S3.SS4.SSS2.p2.5.m5.1.1.1.1.3" xref="S3.SS4.SSS2.p2.5.m5.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.SS4.SSS2.p2.5.m5.1.1.3" xref="S3.SS4.SSS2.p2.5.m5.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p2.5.m5.1b"><apply id="S3.SS4.SSS2.p2.5.m5.1.1.cmml" xref="S3.SS4.SSS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p2.5.m5.1.1.2.cmml" xref="S3.SS4.SSS2.p2.5.m5.1.1">superscript</csymbol><apply id="S3.SS4.SSS2.p2.5.m5.1.1.1.2.cmml" xref="S3.SS4.SSS2.p2.5.m5.1.1.1.1"><csymbol cd="latexml" id="S3.SS4.SSS2.p2.5.m5.1.1.1.2.1.cmml" xref="S3.SS4.SSS2.p2.5.m5.1.1.1.1.2">norm</csymbol><apply id="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.cmml" xref="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1"><minus id="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.1"></minus><ci id="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.2.cmml" xref="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.2">𝑥</ci><apply id="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.3.cmml" xref="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.3.1.cmml" xref="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.3.2.cmml" xref="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.3.2">𝑥</ci><ci id="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.3.3.cmml" xref="S3.SS4.SSS2.p2.5.m5.1.1.1.1.1.3.3">′</ci></apply></apply></apply><cn type="integer" id="S3.SS4.SSS2.p2.5.m5.1.1.3.cmml" xref="S3.SS4.SSS2.p2.5.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p2.5.m5.1c">||x-x^{\prime}||^{2}</annotation></semantics></math> is the squared Euclidean distance between the feature vectors of two nodes <math id="S3.SS4.SSS2.p2.6.m6.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS4.SSS2.p2.6.m6.1a"><mi id="S3.SS4.SSS2.p2.6.m6.1.1" xref="S3.SS4.SSS2.p2.6.m6.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p2.6.m6.1b"><ci id="S3.SS4.SSS2.p2.6.m6.1.1.cmml" xref="S3.SS4.SSS2.p2.6.m6.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p2.6.m6.1c">x</annotation></semantics></math> and <math id="S3.SS4.SSS2.p2.7.m7.1" class="ltx_Math" alttext="x^{\prime}" display="inline"><semantics id="S3.SS4.SSS2.p2.7.m7.1a"><msup id="S3.SS4.SSS2.p2.7.m7.1.1" xref="S3.SS4.SSS2.p2.7.m7.1.1.cmml"><mi id="S3.SS4.SSS2.p2.7.m7.1.1.2" xref="S3.SS4.SSS2.p2.7.m7.1.1.2.cmml">x</mi><mo id="S3.SS4.SSS2.p2.7.m7.1.1.3" xref="S3.SS4.SSS2.p2.7.m7.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p2.7.m7.1b"><apply id="S3.SS4.SSS2.p2.7.m7.1.1.cmml" xref="S3.SS4.SSS2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p2.7.m7.1.1.1.cmml" xref="S3.SS4.SSS2.p2.7.m7.1.1">superscript</csymbol><ci id="S3.SS4.SSS2.p2.7.m7.1.1.2.cmml" xref="S3.SS4.SSS2.p2.7.m7.1.1.2">𝑥</ci><ci id="S3.SS4.SSS2.p2.7.m7.1.1.3.cmml" xref="S3.SS4.SSS2.p2.7.m7.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p2.7.m7.1c">x^{\prime}</annotation></semantics></math> (where <math id="S3.SS4.SSS2.p2.8.m8.1" class="ltx_Math" alttext="x^{\prime}" display="inline"><semantics id="S3.SS4.SSS2.p2.8.m8.1a"><msup id="S3.SS4.SSS2.p2.8.m8.1.1" xref="S3.SS4.SSS2.p2.8.m8.1.1.cmml"><mi id="S3.SS4.SSS2.p2.8.m8.1.1.2" xref="S3.SS4.SSS2.p2.8.m8.1.1.2.cmml">x</mi><mo id="S3.SS4.SSS2.p2.8.m8.1.1.3" xref="S3.SS4.SSS2.p2.8.m8.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p2.8.m8.1b"><apply id="S3.SS4.SSS2.p2.8.m8.1.1.cmml" xref="S3.SS4.SSS2.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS2.p2.8.m8.1.1.1.cmml" xref="S3.SS4.SSS2.p2.8.m8.1.1">superscript</csymbol><ci id="S3.SS4.SSS2.p2.8.m8.1.1.2.cmml" xref="S3.SS4.SSS2.p2.8.m8.1.1.2">𝑥</ci><ci id="S3.SS4.SSS2.p2.8.m8.1.1.3.cmml" xref="S3.SS4.SSS2.p2.8.m8.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p2.8.m8.1c">x^{\prime}</annotation></semantics></math> is a labeled node), and <math id="S3.SS4.SSS2.p2.9.m9.1" class="ltx_Math" alttext="\gamma\in\mathbb{R}+" display="inline"><semantics id="S3.SS4.SSS2.p2.9.m9.1a"><mrow id="S3.SS4.SSS2.p2.9.m9.1.1" xref="S3.SS4.SSS2.p2.9.m9.1.1.cmml"><mi id="S3.SS4.SSS2.p2.9.m9.1.1.2" xref="S3.SS4.SSS2.p2.9.m9.1.1.2.cmml">γ</mi><mo id="S3.SS4.SSS2.p2.9.m9.1.1.1" xref="S3.SS4.SSS2.p2.9.m9.1.1.1.cmml">∈</mo><mrow id="S3.SS4.SSS2.p2.9.m9.1.1.3" xref="S3.SS4.SSS2.p2.9.m9.1.1.3.cmml"><mi id="S3.SS4.SSS2.p2.9.m9.1.1.3.2" xref="S3.SS4.SSS2.p2.9.m9.1.1.3.2.cmml">ℝ</mi><mo id="S3.SS4.SSS2.p2.9.m9.1.1.3.3" xref="S3.SS4.SSS2.p2.9.m9.1.1.3.3.cmml">+</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS2.p2.9.m9.1b"><apply id="S3.SS4.SSS2.p2.9.m9.1.1.cmml" xref="S3.SS4.SSS2.p2.9.m9.1.1"><in id="S3.SS4.SSS2.p2.9.m9.1.1.1.cmml" xref="S3.SS4.SSS2.p2.9.m9.1.1.1"></in><ci id="S3.SS4.SSS2.p2.9.m9.1.1.2.cmml" xref="S3.SS4.SSS2.p2.9.m9.1.1.2">𝛾</ci><apply id="S3.SS4.SSS2.p2.9.m9.1.1.3.cmml" xref="S3.SS4.SSS2.p2.9.m9.1.1.3"><csymbol cd="latexml" id="S3.SS4.SSS2.p2.9.m9.1.1.3.1.cmml" xref="S3.SS4.SSS2.p2.9.m9.1.1.3">limit-from</csymbol><ci id="S3.SS4.SSS2.p2.9.m9.1.1.3.2.cmml" xref="S3.SS4.SSS2.p2.9.m9.1.1.3.2">ℝ</ci><plus id="S3.SS4.SSS2.p2.9.m9.1.1.3.3.cmml" xref="S3.SS4.SSS2.p2.9.m9.1.1.3.3"></plus></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS2.p2.9.m9.1c">\gamma\in\mathbb{R}+</annotation></semantics></math>. Hence, the value of the RBF kernel function increases as the distance between data points decreases. The kernel is used to perform inductive inference to predict the labels on unlabeled data points (only when the prediction is reliable). This process is repeated until convergence (i.e., when there are no more unlabeled data points for which label propagation is reliable).</p>
</div>
<div id="S3.SS4.SSS2.p3" class="ltx_para">
<p id="S3.SS4.SSS2.p3.1" class="ltx_p">The Label Propagation process is started when the server requires to update the global model, and it is executed considering all the labeled and unlabeled samples stored in the <span id="S3.SS4.SSS2.p3.1.1" class="ltx_text ltx_font_italic">User Local Storage</span>. The Label Propagation model is personal and never shared with other users nor with the server.</p>
</div>
</section>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5. </span>Model Personalization</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">FedHAR  adopts a transfer learning strategy to fine-tune the <span id="S3.SS5.p1.1.1" class="ltx_text ltx_font_italic">Personalized Local Model</span> on each user. The intuition behind the personalization mechanism is that the last layers of the neural network (i.e., the ones closer to the output) encode personal characteristics of activity execution, while the remaining layers encode more general features that are common between different users <cite class="ltx_cite ltx_citemacro_citep">(Yosinski
et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2014</a>)</cite>.
A similar mechanism has been also used in a recent FL approach to HAR <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2020b</a>)</cite>.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2104.08094/assets/arch_model.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="248" height="99" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Shared and Personal Layers.</figcaption>
</figure>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">As depicted in Figure <a href="#S3.F5" title="Figure 5 ‣ 3.5. Model Personalization ‣ 3. The FedHAR framework ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we refer to the last <math id="S3.SS5.p2.1.m1.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS5.p2.1.m1.1a"><mi id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><ci id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">l</annotation></semantics></math> layers of the neural network as the <span id="S3.SS5.p2.1.1" class="ltx_text ltx_font_italic">User Personalized Layers</span>, while we refer to the remaining ones as <span id="S3.SS5.p2.1.2" class="ltx_text ltx_font_italic">Shared Hidden Layers</span>.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">In FedHAR, when a node receives the updated global weights from the server, each layer of the <span id="S3.SS5.p3.1.1" class="ltx_text ltx_font_italic">Personalized Local Model</span> is updated according to these weights except for the <span id="S3.SS5.p3.1.2" class="ltx_text ltx_font_italic">User Personalized Layers</span>. As we previously mentioned, the weights of the <span id="S3.SS5.p3.1.3" class="ltx_text ltx_font_italic">Personalized Local Model</span> are not used for the FL process, and hence the <span id="S3.SS5.p3.1.4" class="ltx_text ltx_font_italic">User Personalised Layers</span> are only stored locally and never shared with the server.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experimental evaluation</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we describe in detail the extensive experimental evaluation that we carried out to quantitatively assess the effectiveness of FedHAR. First, we describe the public datasets that we considered in our experiments. Then, we present our novel evaluation methodology that aims at assessing both the generalization and personalization capabilities of FedHAR. Finally, we discuss the results that we obtained
on the target datasets.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Since FL makes sense when many users participate in collaborative training the global model, we considered publicly available datasets of physical activities (performed both in outdoor and indoor environments) that were collected involving a significant number of subjects. However, there are only a few public datasets with these characteristics. One of them is MobiAct <cite class="ltx_cite ltx_citemacro_citep">(Vavoulas et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2016</a>)</cite>, which includes labeled data from <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="60" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><cn type="integer" id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">60</annotation></semantics></math> different subjects with high variance in age and physical characteristics. The dataset contains data from inertial sensors (i.e., accelerometer, gyroscope, and magnetometer) of a smartphone placed in the pocket of each subject during activity execution. Due to its characteristics, this dataset was also used in other works that proposed FL applied to HAR (e.g., the work presented in <cite class="ltx_cite ltx_citemacro_citep">(Wu
et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2020a</a>)</cite>).
In our experiments, we considered the following physical activities <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Note that we omitted from MobiAct those physical activities with a limited number of samples. Indeed, as we will explain later, our evaluation methodology requires to partition the data of each user. Activities with a small number of samples would be insufficiently represented in each partition and hence they are not suitable for our evaluation. We believe that this problem is only related to this specific dataset and that, in realistic settings, even short activities would be represented by a sufficient number of samples.</span></span></span>: <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">standing</span>, <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">walking</span>, <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_italic">jogging</span>, <span id="S4.SS1.p1.1.4" class="ltx_text ltx_font_italic">jumping</span>, and <span id="S4.SS1.p1.1.5" class="ltx_text ltx_font_italic">sitting</span>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">We also consider the well-known WISDM dataset <cite class="ltx_cite ltx_citemacro_citep">(Kwapisz
et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2011</a>)</cite>. This dataset has been widely adopted as benchmark for HAR. WISDM contains accelerometer data collected from a smartphone in the pocket of each subject during activity execution. WISDM includes data from <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="36" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">36</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><cn type="integer" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">36</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">36</annotation></semantics></math> subjects. The activities included in this dataset are the following: <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">walking</span>, <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_italic">jogging</span>, <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_italic">sitting</span>, <span id="S4.SS1.p2.1.4" class="ltx_text ltx_font_italic">standing</span>, and <span id="S4.SS1.p2.1.5" class="ltx_text ltx_font_italic">taking stairs</span>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Evaluation methodology</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.12" class="ltx_p">In the following, we describe our novel methodology that we
designed to evaluate the effectiveness of FedHAR, both in terms of personalization and generalization.
We split each dataset into three partitions that we call <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="Pt" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝑃</ci><ci id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">Pt</annotation></semantics></math>, <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="Tr" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><times id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></times><ci id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">𝑇</ci><ci id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">Tr</annotation></semantics></math>, and <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="Ts" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mi id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><times id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1"></times><ci id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">𝑇</ci><ci id="S4.SS2.p1.3.m3.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">Ts</annotation></semantics></math>.
The partition <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="Pt" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mrow id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml"><mi id="S4.SS2.p1.4.m4.1.1.2" xref="S4.SS2.p1.4.m4.1.1.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.4.m4.1.1.1" xref="S4.SS2.p1.4.m4.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.4.m4.1.1.3" xref="S4.SS2.p1.4.m4.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"><times id="S4.SS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1.1"></times><ci id="S4.SS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.p1.4.m4.1.1.2">𝑃</ci><ci id="S4.SS2.p1.4.m4.1.1.3.cmml" xref="S4.SS2.p1.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">Pt</annotation></semantics></math> (i.e., pre-training data) contains data of users that we only use to initialize the global model.
<math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="Tr" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><mrow id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml"><mi id="S4.SS2.p1.5.m5.1.1.2" xref="S4.SS2.p1.5.m5.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.5.m5.1.1.1" xref="S4.SS2.p1.5.m5.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.5.m5.1.1.3" xref="S4.SS2.p1.5.m5.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><apply id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"><times id="S4.SS2.p1.5.m5.1.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1.1"></times><ci id="S4.SS2.p1.5.m5.1.1.2.cmml" xref="S4.SS2.p1.5.m5.1.1.2">𝑇</ci><ci id="S4.SS2.p1.5.m5.1.1.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">Tr</annotation></semantics></math> (i.e., training data) is the dataset partition that includes data of users who participate in FL. Finally, <math id="S4.SS2.p1.6.m6.1" class="ltx_Math" alttext="Ts" display="inline"><semantics id="S4.SS2.p1.6.m6.1a"><mrow id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml"><mi id="S4.SS2.p1.6.m6.1.1.2" xref="S4.SS2.p1.6.m6.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.6.m6.1.1.1" xref="S4.SS2.p1.6.m6.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.6.m6.1.1.3" xref="S4.SS2.p1.6.m6.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><apply id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1"><times id="S4.SS2.p1.6.m6.1.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1.1"></times><ci id="S4.SS2.p1.6.m6.1.1.2.cmml" xref="S4.SS2.p1.6.m6.1.1.2">𝑇</ci><ci id="S4.SS2.p1.6.m6.1.1.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">Ts</annotation></semantics></math> (i.e., test data) is a dataset partition that includes data of left-out users that we only consider to periodically evaluate the generalization capabilities of the global model. In our experiments, we randomly partition the users as follows: <math id="S4.SS2.p1.7.m7.1" class="ltx_Math" alttext="15\%" display="inline"><semantics id="S4.SS2.p1.7.m7.1a"><mrow id="S4.SS2.p1.7.m7.1.1" xref="S4.SS2.p1.7.m7.1.1.cmml"><mn id="S4.SS2.p1.7.m7.1.1.2" xref="S4.SS2.p1.7.m7.1.1.2.cmml">15</mn><mo id="S4.SS2.p1.7.m7.1.1.1" xref="S4.SS2.p1.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.7.m7.1b"><apply id="S4.SS2.p1.7.m7.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1"><csymbol cd="latexml" id="S4.SS2.p1.7.m7.1.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.p1.7.m7.1.1.2.cmml" xref="S4.SS2.p1.7.m7.1.1.2">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.7.m7.1c">15\%</annotation></semantics></math> whose data will populate <math id="S4.SS2.p1.8.m8.1" class="ltx_Math" alttext="Pt" display="inline"><semantics id="S4.SS2.p1.8.m8.1a"><mrow id="S4.SS2.p1.8.m8.1.1" xref="S4.SS2.p1.8.m8.1.1.cmml"><mi id="S4.SS2.p1.8.m8.1.1.2" xref="S4.SS2.p1.8.m8.1.1.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.8.m8.1.1.1" xref="S4.SS2.p1.8.m8.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.8.m8.1.1.3" xref="S4.SS2.p1.8.m8.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.8.m8.1b"><apply id="S4.SS2.p1.8.m8.1.1.cmml" xref="S4.SS2.p1.8.m8.1.1"><times id="S4.SS2.p1.8.m8.1.1.1.cmml" xref="S4.SS2.p1.8.m8.1.1.1"></times><ci id="S4.SS2.p1.8.m8.1.1.2.cmml" xref="S4.SS2.p1.8.m8.1.1.2">𝑃</ci><ci id="S4.SS2.p1.8.m8.1.1.3.cmml" xref="S4.SS2.p1.8.m8.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.8.m8.1c">Pt</annotation></semantics></math>, <math id="S4.SS2.p1.9.m9.1" class="ltx_Math" alttext="65\%" display="inline"><semantics id="S4.SS2.p1.9.m9.1a"><mrow id="S4.SS2.p1.9.m9.1.1" xref="S4.SS2.p1.9.m9.1.1.cmml"><mn id="S4.SS2.p1.9.m9.1.1.2" xref="S4.SS2.p1.9.m9.1.1.2.cmml">65</mn><mo id="S4.SS2.p1.9.m9.1.1.1" xref="S4.SS2.p1.9.m9.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.9.m9.1b"><apply id="S4.SS2.p1.9.m9.1.1.cmml" xref="S4.SS2.p1.9.m9.1.1"><csymbol cd="latexml" id="S4.SS2.p1.9.m9.1.1.1.cmml" xref="S4.SS2.p1.9.m9.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.p1.9.m9.1.1.2.cmml" xref="S4.SS2.p1.9.m9.1.1.2">65</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.9.m9.1c">65\%</annotation></semantics></math> whose data will populate <math id="S4.SS2.p1.10.m10.1" class="ltx_Math" alttext="Tr" display="inline"><semantics id="S4.SS2.p1.10.m10.1a"><mrow id="S4.SS2.p1.10.m10.1.1" xref="S4.SS2.p1.10.m10.1.1.cmml"><mi id="S4.SS2.p1.10.m10.1.1.2" xref="S4.SS2.p1.10.m10.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.10.m10.1.1.1" xref="S4.SS2.p1.10.m10.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.10.m10.1.1.3" xref="S4.SS2.p1.10.m10.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.10.m10.1b"><apply id="S4.SS2.p1.10.m10.1.1.cmml" xref="S4.SS2.p1.10.m10.1.1"><times id="S4.SS2.p1.10.m10.1.1.1.cmml" xref="S4.SS2.p1.10.m10.1.1.1"></times><ci id="S4.SS2.p1.10.m10.1.1.2.cmml" xref="S4.SS2.p1.10.m10.1.1.2">𝑇</ci><ci id="S4.SS2.p1.10.m10.1.1.3.cmml" xref="S4.SS2.p1.10.m10.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.10.m10.1c">Tr</annotation></semantics></math>, and <math id="S4.SS2.p1.11.m11.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S4.SS2.p1.11.m11.1a"><mrow id="S4.SS2.p1.11.m11.1.1" xref="S4.SS2.p1.11.m11.1.1.cmml"><mn id="S4.SS2.p1.11.m11.1.1.2" xref="S4.SS2.p1.11.m11.1.1.2.cmml">20</mn><mo id="S4.SS2.p1.11.m11.1.1.1" xref="S4.SS2.p1.11.m11.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.11.m11.1b"><apply id="S4.SS2.p1.11.m11.1.1.cmml" xref="S4.SS2.p1.11.m11.1.1"><csymbol cd="latexml" id="S4.SS2.p1.11.m11.1.1.1.cmml" xref="S4.SS2.p1.11.m11.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.p1.11.m11.1.1.2.cmml" xref="S4.SS2.p1.11.m11.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.11.m11.1c">20\%</annotation></semantics></math> whose data will populate <math id="S4.SS2.p1.12.m12.1" class="ltx_Math" alttext="Ts" display="inline"><semantics id="S4.SS2.p1.12.m12.1a"><mrow id="S4.SS2.p1.12.m12.1.1" xref="S4.SS2.p1.12.m12.1.1.cmml"><mi id="S4.SS2.p1.12.m12.1.1.2" xref="S4.SS2.p1.12.m12.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.12.m12.1.1.1" xref="S4.SS2.p1.12.m12.1.1.1.cmml">​</mo><mi id="S4.SS2.p1.12.m12.1.1.3" xref="S4.SS2.p1.12.m12.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.12.m12.1b"><apply id="S4.SS2.p1.12.m12.1.1.cmml" xref="S4.SS2.p1.12.m12.1.1"><times id="S4.SS2.p1.12.m12.1.1.1.cmml" xref="S4.SS2.p1.12.m12.1.1.1"></times><ci id="S4.SS2.p1.12.m12.1.1.2.cmml" xref="S4.SS2.p1.12.m12.1.1.2">𝑇</ci><ci id="S4.SS2.p1.12.m12.1.1.3.cmml" xref="S4.SS2.p1.12.m12.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.12.m12.1c">Ts</annotation></semantics></math>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.5" class="ltx_p">We partition the data for each user in <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="Tr" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><times id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></times><ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">𝑇</ci><ci id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">Tr</annotation></semantics></math> into <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="sh" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><times id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1"></times><ci id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">𝑠</ci><ci id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">sh</annotation></semantics></math> shards of equal size.
In realistic scenarios, each shard should contain data collected during a relatively long time period (e.g., a day) where a user executes many different activities. However, the considered datasets only have a limited amount of data for each user (usually less than one hour of activities for each user).
Hence, we generate shards as follows. Given a user <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="u\in Tr" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mrow id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mi id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">u</mi><mo id="S4.SS2.p2.3.m3.1.1.1" xref="S4.SS2.p2.3.m3.1.1.1.cmml">∈</mo><mrow id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml"><mi id="S4.SS2.p2.3.m3.1.1.3.2" xref="S4.SS2.p2.3.m3.1.1.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.1.1.3.1" xref="S4.SS2.p2.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.3.m3.1.1.3.3" xref="S4.SS2.p2.3.m3.1.1.3.3.cmml">r</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><in id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1"></in><ci id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">𝑢</ci><apply id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3"><times id="S4.SS2.p2.3.m3.1.1.3.1.cmml" xref="S4.SS2.p2.3.m3.1.1.3.1"></times><ci id="S4.SS2.p2.3.m3.1.1.3.2.cmml" xref="S4.SS2.p2.3.m3.1.1.3.2">𝑇</ci><ci id="S4.SS2.p2.3.m3.1.1.3.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3.3">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">u\in Tr</annotation></semantics></math>, we randomly assign to each shard a fraction <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="\frac{1}{sh}" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mfrac id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml"><mn id="S4.SS2.p2.4.m4.1.1.2" xref="S4.SS2.p2.4.m4.1.1.2.cmml">1</mn><mrow id="S4.SS2.p2.4.m4.1.1.3" xref="S4.SS2.p2.4.m4.1.1.3.cmml"><mi id="S4.SS2.p2.4.m4.1.1.3.2" xref="S4.SS2.p2.4.m4.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.4.m4.1.1.3.1" xref="S4.SS2.p2.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.4.m4.1.1.3.3" xref="S4.SS2.p2.4.m4.1.1.3.3.cmml">h</mi></mrow></mfrac><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><apply id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1"><divide id="S4.SS2.p2.4.m4.1.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1"></divide><cn type="integer" id="S4.SS2.p2.4.m4.1.1.2.cmml" xref="S4.SS2.p2.4.m4.1.1.2">1</cn><apply id="S4.SS2.p2.4.m4.1.1.3.cmml" xref="S4.SS2.p2.4.m4.1.1.3"><times id="S4.SS2.p2.4.m4.1.1.3.1.cmml" xref="S4.SS2.p2.4.m4.1.1.3.1"></times><ci id="S4.SS2.p2.4.m4.1.1.3.2.cmml" xref="S4.SS2.p2.4.m4.1.1.3.2">𝑠</ci><ci id="S4.SS2.p2.4.m4.1.1.3.3.cmml" xref="S4.SS2.p2.4.m4.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">\frac{1}{sh}</annotation></semantics></math> of the available data samples associated to <math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="u" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><mi id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><ci id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1">𝑢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">u</annotation></semantics></math> in the dataset. Note that each data sample of a user is associated to exactly one shard. This mechanism allows us to simulate the realistic scenario described before, where users perform several types of activities in each shard. 
<br class="ltx_break"></p>
</div>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Evaluation Algorithm</h5>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.3" class="ltx_p">In the following, we describe our novel evaluation methodology step by step.
First, the labeled data in <math id="S4.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="Pt" display="inline"><semantics id="S4.SS2.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1"><times id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.1"></times><ci id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.2">𝑃</ci><ci id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.1.m1.1c">Pt</annotation></semantics></math> are used to initialize the global model, that is then distributed to the devices of all the users in <math id="S4.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="Tr" display="inline"><semantics id="S4.SS2.SSS0.Px1.p1.2.m2.1a"><mrow id="S4.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.2" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.1" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.3" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1"><times id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.1"></times><ci id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.2">𝑇</ci><ci id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.2.m2.1c">Tr</annotation></semantics></math>.
Then, we evaluate the recognition capabilities of this initial model on the partition <math id="S4.SS2.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="Ts" display="inline"><semantics id="S4.SS2.SSS0.Px1.p1.3.m3.1a"><mrow id="S4.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.1" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.3.m3.1b"><apply id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1"><times id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.1"></times><ci id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.2">𝑇</ci><ci id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.3.m3.1c">Ts</annotation></semantics></math> in terms of F1 score. This assessment allows us to measure how the initial global model generalizes on unseen users before any FL step.</p>
</div>
<div id="S4.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p2.5" class="ltx_p">As we previously mentioned, we partitioned the data of each user in <math id="S4.SS2.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="Tr" display="inline"><semantics id="S4.SS2.SSS0.Px1.p2.1.m1.1a"><mrow id="S4.SS2.SSS0.Px1.p2.1.m1.1.1" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.cmml"><mi id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.2" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.1" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p2.1.m1.1b"><apply id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1"><times id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.1"></times><ci id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.2">𝑇</ci><ci id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p2.1.m1.1c">Tr</annotation></semantics></math> into exactly <math id="S4.SS2.SSS0.Px1.p2.2.m2.1" class="ltx_Math" alttext="sh" display="inline"><semantics id="S4.SS2.SSS0.Px1.p2.2.m2.1a"><mrow id="S4.SS2.SSS0.Px1.p2.2.m2.1.1" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.cmml"><mi id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.2" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.1" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p2.2.m2.1b"><apply id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1"><times id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.1"></times><ci id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.2">𝑠</ci><ci id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p2.2.m2.1c">sh</annotation></semantics></math> shards. For the sake of evaluation, we assume a synchronous system in which the shards of the different users in <math id="S4.SS2.SSS0.Px1.p2.3.m3.1" class="ltx_Math" alttext="Tr" display="inline"><semantics id="S4.SS2.SSS0.Px1.p2.3.m3.1a"><mrow id="S4.SS2.SSS0.Px1.p2.3.m3.1.1" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.cmml"><mi id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.2" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.1" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p2.3.m3.1b"><apply id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.cmml" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1"><times id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.1"></times><ci id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.2">𝑇</ci><ci id="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p2.3.m3.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p2.3.m3.1c">Tr</annotation></semantics></math> are actually temporally aligned and occur simultaneously (i.e, the first shards of every user occur at the same time interval, the second shards of every user occur at the same time interval, and so on).
The evaluation process is composed by <math id="S4.SS2.SSS0.Px1.p2.4.m4.1" class="ltx_Math" alttext="sh" display="inline"><semantics id="S4.SS2.SSS0.Px1.p2.4.m4.1a"><mrow id="S4.SS2.SSS0.Px1.p2.4.m4.1.1" xref="S4.SS2.SSS0.Px1.p2.4.m4.1.1.cmml"><mi id="S4.SS2.SSS0.Px1.p2.4.m4.1.1.2" xref="S4.SS2.SSS0.Px1.p2.4.m4.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px1.p2.4.m4.1.1.1" xref="S4.SS2.SSS0.Px1.p2.4.m4.1.1.1.cmml">​</mo><mi id="S4.SS2.SSS0.Px1.p2.4.m4.1.1.3" xref="S4.SS2.SSS0.Px1.p2.4.m4.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p2.4.m4.1b"><apply id="S4.SS2.SSS0.Px1.p2.4.m4.1.1.cmml" xref="S4.SS2.SSS0.Px1.p2.4.m4.1.1"><times id="S4.SS2.SSS0.Px1.p2.4.m4.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p2.4.m4.1.1.1"></times><ci id="S4.SS2.SSS0.Px1.p2.4.m4.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p2.4.m4.1.1.2">𝑠</ci><ci id="S4.SS2.SSS0.Px1.p2.4.m4.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p2.4.m4.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p2.4.m4.1c">sh</annotation></semantics></math> iterations, one for each shard.
Considering the <math id="S4.SS2.SSS0.Px1.p2.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.SSS0.Px1.p2.5.m5.1a"><mi id="S4.SS2.SSS0.Px1.p2.5.m5.1.1" xref="S4.SS2.SSS0.Px1.p2.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p2.5.m5.1b"><ci id="S4.SS2.SSS0.Px1.p2.5.m5.1.1.cmml" xref="S4.SS2.SSS0.Px1.p2.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p2.5.m5.1c">i</annotation></semantics></math>-th shard we proceed as follows:</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">The devices of the users in <math id="S4.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="Tr" display="inline"><semantics id="S4.I1.i1.p1.1.m1.1a"><mrow id="S4.I1.i1.p1.1.m1.1.1" xref="S4.I1.i1.p1.1.m1.1.1.cmml"><mi id="S4.I1.i1.p1.1.m1.1.1.2" xref="S4.I1.i1.p1.1.m1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.I1.i1.p1.1.m1.1.1.1" xref="S4.I1.i1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.I1.i1.p1.1.m1.1.1.3" xref="S4.I1.i1.p1.1.m1.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.1.m1.1b"><apply id="S4.I1.i1.p1.1.m1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.1.1"><times id="S4.I1.i1.p1.1.m1.1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.1.1.1"></times><ci id="S4.I1.i1.p1.1.m1.1.1.2.cmml" xref="S4.I1.i1.p1.1.m1.1.1.2">𝑇</ci><ci id="S4.I1.i1.p1.1.m1.1.1.3.cmml" xref="S4.I1.i1.p1.1.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.1.m1.1c">Tr</annotation></semantics></math> exploits the <span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Personalized Local Model</span> to classify the continuous stream of inertial sensor data in its shard. We use the classification output to evaluate the recognition rate in terms of F1 score providing an assessment of personalization. Note that, during this phase, we also apply our active learning strategy and we keep track of the number of triggered questions.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">When all data in the shard have been processed (by all devices), the server starts a number <math id="S4.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S4.I1.i2.p1.1.m1.1a"><mi id="S4.I1.i2.p1.1.m1.1.1" xref="S4.I1.i2.p1.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.1.m1.1b"><ci id="S4.I1.i2.p1.1.m1.1.1.cmml" xref="S4.I1.i2.p1.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.1.m1.1c">r</annotation></semantics></math> of communication rounds with a subset of the devices in order to update the global weights. Each round is implemented as follows:</p>
<ol id="S4.I1.i2.I1" class="ltx_enumerate">
<li id="S4.I1.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="S4.I1.i2.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i2.I1.i1.p1.2" class="ltx_p">The server randomly selects a certain percentage <math id="S4.I1.i2.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="p\%" display="inline"><semantics id="S4.I1.i2.I1.i1.p1.1.m1.1a"><mrow id="S4.I1.i2.I1.i1.p1.1.m1.1.1" xref="S4.I1.i2.I1.i1.p1.1.m1.1.1.cmml"><mi id="S4.I1.i2.I1.i1.p1.1.m1.1.1.2" xref="S4.I1.i2.I1.i1.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.I1.i2.I1.i1.p1.1.m1.1.1.1" xref="S4.I1.i2.I1.i1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i2.I1.i1.p1.1.m1.1b"><apply id="S4.I1.i2.I1.i1.p1.1.m1.1.1.cmml" xref="S4.I1.i2.I1.i1.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.I1.i2.I1.i1.p1.1.m1.1.1.1.cmml" xref="S4.I1.i2.I1.i1.p1.1.m1.1.1.1">percent</csymbol><ci id="S4.I1.i2.I1.i1.p1.1.m1.1.1.2.cmml" xref="S4.I1.i2.I1.i1.p1.1.m1.1.1.2">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.I1.i1.p1.1.m1.1c">p\%</annotation></semantics></math> of users in <math id="S4.I1.i2.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="Tr" display="inline"><semantics id="S4.I1.i2.I1.i1.p1.2.m2.1a"><mrow id="S4.I1.i2.I1.i1.p1.2.m2.1.1" xref="S4.I1.i2.I1.i1.p1.2.m2.1.1.cmml"><mi id="S4.I1.i2.I1.i1.p1.2.m2.1.1.2" xref="S4.I1.i2.I1.i1.p1.2.m2.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.I1.i2.I1.i1.p1.2.m2.1.1.1" xref="S4.I1.i2.I1.i1.p1.2.m2.1.1.1.cmml">​</mo><mi id="S4.I1.i2.I1.i1.p1.2.m2.1.1.3" xref="S4.I1.i2.I1.i1.p1.2.m2.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i2.I1.i1.p1.2.m2.1b"><apply id="S4.I1.i2.I1.i1.p1.2.m2.1.1.cmml" xref="S4.I1.i2.I1.i1.p1.2.m2.1.1"><times id="S4.I1.i2.I1.i1.p1.2.m2.1.1.1.cmml" xref="S4.I1.i2.I1.i1.p1.2.m2.1.1.1"></times><ci id="S4.I1.i2.I1.i1.p1.2.m2.1.1.2.cmml" xref="S4.I1.i2.I1.i1.p1.2.m2.1.1.2">𝑇</ci><ci id="S4.I1.i2.I1.i1.p1.2.m2.1.1.3.cmml" xref="S4.I1.i2.I1.i1.p1.2.m2.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.I1.i1.p1.2.m2.1c">Tr</annotation></semantics></math> and sends to their devices the last update of the global weights.</p>
</div>
</li>
<li id="S4.I1.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="S4.I1.i2.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.I1.i2.p1.1" class="ltx_p">Each user’s device, by receiving the global weights, applies Label Propagation (See Section <a href="#S3.SS4.SSS2" title="3.4.2. Label Propagation ‣ 3.4. Semi-supervised learning ‣ 3. The FedHAR framework ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4.2</span></a>) and uses the obtained labeled data to train both its <span id="S4.I1.i2.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Local Model</span> and its <span id="S4.I1.i2.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">Personalized Local Model</span>. Each device finally sends the weights of the <span id="S4.I1.i2.I1.i2.p1.1.3" class="ltx_text ltx_font_italic">Local Model</span> to the server.</p>
</div>
</li>
<li id="S4.I1.i2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span> 
<div id="S4.I1.i2.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i2.I1.i3.p1.1" class="ltx_p">The server merges the received weights obtaining a new version the global model weights.</p>
</div>
</li>
<li id="S4.I1.i2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(d)</span> 
<div id="S4.I1.i2.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i2.I1.i4.p1.1" class="ltx_p">We evaluate in terms of F1 score the recognition rate of the resulting global model on the left-out users in <math id="S4.I1.i2.I1.i4.p1.1.m1.1" class="ltx_Math" alttext="Ts" display="inline"><semantics id="S4.I1.i2.I1.i4.p1.1.m1.1a"><mrow id="S4.I1.i2.I1.i4.p1.1.m1.1.1" xref="S4.I1.i2.I1.i4.p1.1.m1.1.1.cmml"><mi id="S4.I1.i2.I1.i4.p1.1.m1.1.1.2" xref="S4.I1.i2.I1.i4.p1.1.m1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.I1.i2.I1.i4.p1.1.m1.1.1.1" xref="S4.I1.i2.I1.i4.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.I1.i2.I1.i4.p1.1.m1.1.1.3" xref="S4.I1.i2.I1.i4.p1.1.m1.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i2.I1.i4.p1.1.m1.1b"><apply id="S4.I1.i2.I1.i4.p1.1.m1.1.1.cmml" xref="S4.I1.i2.I1.i4.p1.1.m1.1.1"><times id="S4.I1.i2.I1.i4.p1.1.m1.1.1.1.cmml" xref="S4.I1.i2.I1.i4.p1.1.m1.1.1.1"></times><ci id="S4.I1.i2.I1.i4.p1.1.m1.1.1.2.cmml" xref="S4.I1.i2.I1.i4.p1.1.m1.1.1.2">𝑇</ci><ci id="S4.I1.i2.I1.i4.p1.1.m1.1.1.3.cmml" xref="S4.I1.i2.I1.i4.p1.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.I1.i4.p1.1.m1.1c">Ts</annotation></semantics></math> (providing an assessment of generalization).</p>
</div>
</li>
</ol>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">After the execution of all the communication rounds, the latest version of the global weights is sent to devices of the users in <math id="S4.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="Tr" display="inline"><semantics id="S4.I1.i3.p1.1.m1.1a"><mrow id="S4.I1.i3.p1.1.m1.1.1" xref="S4.I1.i3.p1.1.m1.1.1.cmml"><mi id="S4.I1.i3.p1.1.m1.1.1.2" xref="S4.I1.i3.p1.1.m1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.I1.i3.p1.1.m1.1.1.1" xref="S4.I1.i3.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.I1.i3.p1.1.m1.1.1.3" xref="S4.I1.i3.p1.1.m1.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.1.m1.1b"><apply id="S4.I1.i3.p1.1.m1.1.1.cmml" xref="S4.I1.i3.p1.1.m1.1.1"><times id="S4.I1.i3.p1.1.m1.1.1.1.cmml" xref="S4.I1.i3.p1.1.m1.1.1.1"></times><ci id="S4.I1.i3.p1.1.m1.1.1.2.cmml" xref="S4.I1.i3.p1.1.m1.1.1.2">𝑇</ci><ci id="S4.I1.i3.p1.1.m1.1.1.3.cmml" xref="S4.I1.i3.p1.1.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.1.m1.1c">Tr</annotation></semantics></math>, that will apply the personalization process described in Section <a href="#S3.SS5" title="3.5. Model Personalization ‣ 3. The FedHAR framework ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a>.</p>
</div>
</li>
</ol>
</div>
<div id="S4.SS2.SSS0.Px1.p3" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p3.4" class="ltx_p">Note that our evaluation methodology introduces several levels of randomness: assigning users to <math id="S4.SS2.SSS0.Px1.p3.1.m1.1" class="ltx_Math" alttext="Ts" display="inline"><semantics id="S4.SS2.SSS0.Px1.p3.1.m1.1a"><mrow id="S4.SS2.SSS0.Px1.p3.1.m1.1.1" xref="S4.SS2.SSS0.Px1.p3.1.m1.1.1.cmml"><mi id="S4.SS2.SSS0.Px1.p3.1.m1.1.1.2" xref="S4.SS2.SSS0.Px1.p3.1.m1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px1.p3.1.m1.1.1.1" xref="S4.SS2.SSS0.Px1.p3.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.SSS0.Px1.p3.1.m1.1.1.3" xref="S4.SS2.SSS0.Px1.p3.1.m1.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p3.1.m1.1b"><apply id="S4.SS2.SSS0.Px1.p3.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p3.1.m1.1.1"><times id="S4.SS2.SSS0.Px1.p3.1.m1.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p3.1.m1.1.1.1"></times><ci id="S4.SS2.SSS0.Px1.p3.1.m1.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p3.1.m1.1.1.2">𝑇</ci><ci id="S4.SS2.SSS0.Px1.p3.1.m1.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p3.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p3.1.m1.1c">Ts</annotation></semantics></math>, <math id="S4.SS2.SSS0.Px1.p3.2.m2.1" class="ltx_Math" alttext="Tr" display="inline"><semantics id="S4.SS2.SSS0.Px1.p3.2.m2.1a"><mrow id="S4.SS2.SSS0.Px1.p3.2.m2.1.1" xref="S4.SS2.SSS0.Px1.p3.2.m2.1.1.cmml"><mi id="S4.SS2.SSS0.Px1.p3.2.m2.1.1.2" xref="S4.SS2.SSS0.Px1.p3.2.m2.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px1.p3.2.m2.1.1.1" xref="S4.SS2.SSS0.Px1.p3.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS2.SSS0.Px1.p3.2.m2.1.1.3" xref="S4.SS2.SSS0.Px1.p3.2.m2.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p3.2.m2.1b"><apply id="S4.SS2.SSS0.Px1.p3.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px1.p3.2.m2.1.1"><times id="S4.SS2.SSS0.Px1.p3.2.m2.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p3.2.m2.1.1.1"></times><ci id="S4.SS2.SSS0.Px1.p3.2.m2.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p3.2.m2.1.1.2">𝑇</ci><ci id="S4.SS2.SSS0.Px1.p3.2.m2.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p3.2.m2.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p3.2.m2.1c">Tr</annotation></semantics></math> and <math id="S4.SS2.SSS0.Px1.p3.3.m3.1" class="ltx_Math" alttext="Pt" display="inline"><semantics id="S4.SS2.SSS0.Px1.p3.3.m3.1a"><mrow id="S4.SS2.SSS0.Px1.p3.3.m3.1.1" xref="S4.SS2.SSS0.Px1.p3.3.m3.1.1.cmml"><mi id="S4.SS2.SSS0.Px1.p3.3.m3.1.1.2" xref="S4.SS2.SSS0.Px1.p3.3.m3.1.1.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px1.p3.3.m3.1.1.1" xref="S4.SS2.SSS0.Px1.p3.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS2.SSS0.Px1.p3.3.m3.1.1.3" xref="S4.SS2.SSS0.Px1.p3.3.m3.1.1.3.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p3.3.m3.1b"><apply id="S4.SS2.SSS0.Px1.p3.3.m3.1.1.cmml" xref="S4.SS2.SSS0.Px1.p3.3.m3.1.1"><times id="S4.SS2.SSS0.Px1.p3.3.m3.1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p3.3.m3.1.1.1"></times><ci id="S4.SS2.SSS0.Px1.p3.3.m3.1.1.2.cmml" xref="S4.SS2.SSS0.Px1.p3.3.m3.1.1.2">𝑃</ci><ci id="S4.SS2.SSS0.Px1.p3.3.m3.1.1.3.cmml" xref="S4.SS2.SSS0.Px1.p3.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p3.3.m3.1c">Pt</annotation></semantics></math>; assigning data samples to shards; selecting devices at each communication round. We iterate experiments <math id="S4.SS2.SSS0.Px1.p3.4.m4.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS2.SSS0.Px1.p3.4.m4.1a"><mn id="S4.SS2.SSS0.Px1.p3.4.m4.1.1" xref="S4.SS2.SSS0.Px1.p3.4.m4.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p3.4.m4.1b"><cn type="integer" id="S4.SS2.SSS0.Px1.p3.4.m4.1.1.cmml" xref="S4.SS2.SSS0.Px1.p3.4.m4.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p3.4.m4.1c">10</annotation></semantics></math> times and average the results in order to make our estimates more robust.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In the following, we report the results of the evaluation of FedHAR.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1. </span>Classification model and hyper-parameters</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.11" class="ltx_p">As classification model, we used a fully-connected deep neural network. The network consists of four fully connected layers having respectively <math id="S4.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="128" display="inline"><semantics id="S4.SS3.SSS1.p1.1.m1.1a"><mn id="S4.SS3.SSS1.p1.1.m1.1.1" xref="S4.SS3.SSS1.p1.1.m1.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.1.m1.1b"><cn type="integer" id="S4.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.1.m1.1c">128</annotation></semantics></math>, <math id="S4.SS3.SSS1.p1.2.m2.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S4.SS3.SSS1.p1.2.m2.1a"><mn id="S4.SS3.SSS1.p1.2.m2.1.1" xref="S4.SS3.SSS1.p1.2.m2.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.2.m2.1b"><cn type="integer" id="S4.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.2.m2.1c">64</annotation></semantics></math>, <math id="S4.SS3.SSS1.p1.3.m3.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S4.SS3.SSS1.p1.3.m3.1a"><mn id="S4.SS3.SSS1.p1.3.m3.1.1" xref="S4.SS3.SSS1.p1.3.m3.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.3.m3.1b"><cn type="integer" id="S4.SS3.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS3.SSS1.p1.3.m3.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.3.m3.1c">32</annotation></semantics></math>, and <math id="S4.SS3.SSS1.p1.4.m4.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S4.SS3.SSS1.p1.4.m4.1a"><mn id="S4.SS3.SSS1.p1.4.m4.1.1" xref="S4.SS3.SSS1.p1.4.m4.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.4.m4.1b"><cn type="integer" id="S4.SS3.SSS1.p1.4.m4.1.1.cmml" xref="S4.SS3.SSS1.p1.4.m4.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.4.m4.1c">16</annotation></semantics></math> neurons, and a softmax layer for classification. We use Adam <cite class="ltx_cite ltx_citemacro_citep">(Kingma and Ba, <a href="#bib.bib29" title="" class="ltx_ref">2017</a>)</cite> as optimizer. We chose this network for three reasons: 1) our method does not rely on feature learning, 2) training a fully-connected network is more suitable for mobile devices since it requires less computational resources with respect to more complex networks (e.g., CNN, LSTM), and 3) a similar configuration exhibited good performances in the literature <cite class="ltx_cite ltx_citemacro_citep">(Wu
et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2020a</a>)</cite>.
As hyper-parameters, we empirically chose <math id="S4.SS3.SSS1.p1.5.m5.1" class="ltx_Math" alttext="p=30\%" display="inline"><semantics id="S4.SS3.SSS1.p1.5.m5.1a"><mrow id="S4.SS3.SSS1.p1.5.m5.1.1" xref="S4.SS3.SSS1.p1.5.m5.1.1.cmml"><mi id="S4.SS3.SSS1.p1.5.m5.1.1.2" xref="S4.SS3.SSS1.p1.5.m5.1.1.2.cmml">p</mi><mo id="S4.SS3.SSS1.p1.5.m5.1.1.1" xref="S4.SS3.SSS1.p1.5.m5.1.1.1.cmml">=</mo><mrow id="S4.SS3.SSS1.p1.5.m5.1.1.3" xref="S4.SS3.SSS1.p1.5.m5.1.1.3.cmml"><mn id="S4.SS3.SSS1.p1.5.m5.1.1.3.2" xref="S4.SS3.SSS1.p1.5.m5.1.1.3.2.cmml">30</mn><mo id="S4.SS3.SSS1.p1.5.m5.1.1.3.1" xref="S4.SS3.SSS1.p1.5.m5.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.5.m5.1b"><apply id="S4.SS3.SSS1.p1.5.m5.1.1.cmml" xref="S4.SS3.SSS1.p1.5.m5.1.1"><eq id="S4.SS3.SSS1.p1.5.m5.1.1.1.cmml" xref="S4.SS3.SSS1.p1.5.m5.1.1.1"></eq><ci id="S4.SS3.SSS1.p1.5.m5.1.1.2.cmml" xref="S4.SS3.SSS1.p1.5.m5.1.1.2">𝑝</ci><apply id="S4.SS3.SSS1.p1.5.m5.1.1.3.cmml" xref="S4.SS3.SSS1.p1.5.m5.1.1.3"><csymbol cd="latexml" id="S4.SS3.SSS1.p1.5.m5.1.1.3.1.cmml" xref="S4.SS3.SSS1.p1.5.m5.1.1.3.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS1.p1.5.m5.1.1.3.2.cmml" xref="S4.SS3.SSS1.p1.5.m5.1.1.3.2">30</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.5.m5.1c">p=30\%</annotation></semantics></math>, <math id="S4.SS3.SSS1.p1.6.m6.1" class="ltx_Math" alttext="r=10" display="inline"><semantics id="S4.SS3.SSS1.p1.6.m6.1a"><mrow id="S4.SS3.SSS1.p1.6.m6.1.1" xref="S4.SS3.SSS1.p1.6.m6.1.1.cmml"><mi id="S4.SS3.SSS1.p1.6.m6.1.1.2" xref="S4.SS3.SSS1.p1.6.m6.1.1.2.cmml">r</mi><mo id="S4.SS3.SSS1.p1.6.m6.1.1.1" xref="S4.SS3.SSS1.p1.6.m6.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS1.p1.6.m6.1.1.3" xref="S4.SS3.SSS1.p1.6.m6.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.6.m6.1b"><apply id="S4.SS3.SSS1.p1.6.m6.1.1.cmml" xref="S4.SS3.SSS1.p1.6.m6.1.1"><eq id="S4.SS3.SSS1.p1.6.m6.1.1.1.cmml" xref="S4.SS3.SSS1.p1.6.m6.1.1.1"></eq><ci id="S4.SS3.SSS1.p1.6.m6.1.1.2.cmml" xref="S4.SS3.SSS1.p1.6.m6.1.1.2">𝑟</ci><cn type="integer" id="S4.SS3.SSS1.p1.6.m6.1.1.3.cmml" xref="S4.SS3.SSS1.p1.6.m6.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.6.m6.1c">r=10</annotation></semantics></math>, <math id="S4.SS3.SSS1.p1.7.m7.1" class="ltx_Math" alttext="l=2" display="inline"><semantics id="S4.SS3.SSS1.p1.7.m7.1a"><mrow id="S4.SS3.SSS1.p1.7.m7.1.1" xref="S4.SS3.SSS1.p1.7.m7.1.1.cmml"><mi id="S4.SS3.SSS1.p1.7.m7.1.1.2" xref="S4.SS3.SSS1.p1.7.m7.1.1.2.cmml">l</mi><mo id="S4.SS3.SSS1.p1.7.m7.1.1.1" xref="S4.SS3.SSS1.p1.7.m7.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS1.p1.7.m7.1.1.3" xref="S4.SS3.SSS1.p1.7.m7.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.7.m7.1b"><apply id="S4.SS3.SSS1.p1.7.m7.1.1.cmml" xref="S4.SS3.SSS1.p1.7.m7.1.1"><eq id="S4.SS3.SSS1.p1.7.m7.1.1.1.cmml" xref="S4.SS3.SSS1.p1.7.m7.1.1.1"></eq><ci id="S4.SS3.SSS1.p1.7.m7.1.1.2.cmml" xref="S4.SS3.SSS1.p1.7.m7.1.1.2">𝑙</ci><cn type="integer" id="S4.SS3.SSS1.p1.7.m7.1.1.3.cmml" xref="S4.SS3.SSS1.p1.7.m7.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.7.m7.1c">l=2</annotation></semantics></math>, <math id="S4.SS3.SSS1.p1.8.m8.1" class="ltx_Math" alttext="sh=3" display="inline"><semantics id="S4.SS3.SSS1.p1.8.m8.1a"><mrow id="S4.SS3.SSS1.p1.8.m8.1.1" xref="S4.SS3.SSS1.p1.8.m8.1.1.cmml"><mrow id="S4.SS3.SSS1.p1.8.m8.1.1.2" xref="S4.SS3.SSS1.p1.8.m8.1.1.2.cmml"><mi id="S4.SS3.SSS1.p1.8.m8.1.1.2.2" xref="S4.SS3.SSS1.p1.8.m8.1.1.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS3.SSS1.p1.8.m8.1.1.2.1" xref="S4.SS3.SSS1.p1.8.m8.1.1.2.1.cmml">​</mo><mi id="S4.SS3.SSS1.p1.8.m8.1.1.2.3" xref="S4.SS3.SSS1.p1.8.m8.1.1.2.3.cmml">h</mi></mrow><mo id="S4.SS3.SSS1.p1.8.m8.1.1.1" xref="S4.SS3.SSS1.p1.8.m8.1.1.1.cmml">=</mo><mn id="S4.SS3.SSS1.p1.8.m8.1.1.3" xref="S4.SS3.SSS1.p1.8.m8.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.8.m8.1b"><apply id="S4.SS3.SSS1.p1.8.m8.1.1.cmml" xref="S4.SS3.SSS1.p1.8.m8.1.1"><eq id="S4.SS3.SSS1.p1.8.m8.1.1.1.cmml" xref="S4.SS3.SSS1.p1.8.m8.1.1.1"></eq><apply id="S4.SS3.SSS1.p1.8.m8.1.1.2.cmml" xref="S4.SS3.SSS1.p1.8.m8.1.1.2"><times id="S4.SS3.SSS1.p1.8.m8.1.1.2.1.cmml" xref="S4.SS3.SSS1.p1.8.m8.1.1.2.1"></times><ci id="S4.SS3.SSS1.p1.8.m8.1.1.2.2.cmml" xref="S4.SS3.SSS1.p1.8.m8.1.1.2.2">𝑠</ci><ci id="S4.SS3.SSS1.p1.8.m8.1.1.2.3.cmml" xref="S4.SS3.SSS1.p1.8.m8.1.1.2.3">ℎ</ci></apply><cn type="integer" id="S4.SS3.SSS1.p1.8.m8.1.1.3.cmml" xref="S4.SS3.SSS1.p1.8.m8.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.8.m8.1c">sh=3</annotation></semantics></math>, and <math id="S4.SS3.SSS1.p1.9.m9.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS3.SSS1.p1.9.m9.1a"><mn id="S4.SS3.SSS1.p1.9.m9.1.1" xref="S4.SS3.SSS1.p1.9.m9.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.9.m9.1b"><cn type="integer" id="S4.SS3.SSS1.p1.9.m9.1.1.cmml" xref="S4.SS3.SSS1.p1.9.m9.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.9.m9.1c">10</annotation></semantics></math> local training epochs with a batch size of <math id="S4.SS3.SSS1.p1.10.m10.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S4.SS3.SSS1.p1.10.m10.1a"><mn id="S4.SS3.SSS1.p1.10.m10.1.1" xref="S4.SS3.SSS1.p1.10.m10.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.10.m10.1b"><cn type="integer" id="S4.SS3.SSS1.p1.10.m10.1.1.cmml" xref="S4.SS3.SSS1.p1.10.m10.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.10.m10.1c">30</annotation></semantics></math> samples. These hyper-parameters have been empirically determined based on data in <math id="S4.SS3.SSS1.p1.11.m11.1" class="ltx_Math" alttext="Ts" display="inline"><semantics id="S4.SS3.SSS1.p1.11.m11.1a"><mrow id="S4.SS3.SSS1.p1.11.m11.1.1" xref="S4.SS3.SSS1.p1.11.m11.1.1.cmml"><mi id="S4.SS3.SSS1.p1.11.m11.1.1.2" xref="S4.SS3.SSS1.p1.11.m11.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS3.SSS1.p1.11.m11.1.1.1" xref="S4.SS3.SSS1.p1.11.m11.1.1.1.cmml">​</mo><mi id="S4.SS3.SSS1.p1.11.m11.1.1.3" xref="S4.SS3.SSS1.p1.11.m11.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.11.m11.1b"><apply id="S4.SS3.SSS1.p1.11.m11.1.1.cmml" xref="S4.SS3.SSS1.p1.11.m11.1.1"><times id="S4.SS3.SSS1.p1.11.m11.1.1.1.cmml" xref="S4.SS3.SSS1.p1.11.m11.1.1.1"></times><ci id="S4.SS3.SSS1.p1.11.m11.1.1.2.cmml" xref="S4.SS3.SSS1.p1.11.m11.1.1.2">𝑇</ci><ci id="S4.SS3.SSS1.p1.11.m11.1.1.3.cmml" xref="S4.SS3.SSS1.p1.11.m11.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.11.m11.1c">Ts</annotation></semantics></math>.
The low number of epochs and communication rounds is due to the small size of the public datasets. This also limits the data in each shard. In a large scale deployment, these parameters should be accurately calibrated.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2. </span>Impact of semi-supervised learning</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">Figure <a href="#S4.F6" title="Figure 6 ‣ 4.3.2. Impact of semi-supervised learning ‣ 4.3. Results ‣ 4. Experimental evaluation ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and Figure <a href="#S4.F7" title="Figure 7 ‣ 4.3.2. Impact of semi-supervised learning ‣ 4.3. Results ‣ 4. Experimental evaluation ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> show how the F1 score and the percentage of active learning questions change at each shard for the users in <math id="S4.SS3.SSS2.p1.1.m1.1" class="ltx_Math" alttext="Tr" display="inline"><semantics id="S4.SS3.SSS2.p1.1.m1.1a"><mrow id="S4.SS3.SSS2.p1.1.m1.1.1" xref="S4.SS3.SSS2.p1.1.m1.1.1.cmml"><mi id="S4.SS3.SSS2.p1.1.m1.1.1.2" xref="S4.SS3.SSS2.p1.1.m1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS3.SSS2.p1.1.m1.1.1.1" xref="S4.SS3.SSS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS3.SSS2.p1.1.m1.1.1.3" xref="S4.SS3.SSS2.p1.1.m1.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.1.m1.1b"><apply id="S4.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1"><times id="S4.SS3.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1.1"></times><ci id="S4.SS3.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1.2">𝑇</ci><ci id="S4.SS3.SSS2.p1.1.m1.1.1.3.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.1.m1.1c">Tr</annotation></semantics></math>.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2104.08094/assets/confronto_train_f1_2.png" id="S4.F6.sf1.g1" class="ltx_graphics ltx_img_landscape" width="252" height="172" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Average F1 score</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2104.08094/assets/confronto_train_quest_2.png" id="S4.F6.sf2.g1" class="ltx_graphics ltx_img_landscape" width="252" height="172" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Average percentage of active learning questions.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>MobiAct: The impact of label propagation and active learning on the subjects that participated in the FL process.</figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2104.08094/assets/confronto_train_f1_W.png" id="S4.F7.sf1.g1" class="ltx_graphics ltx_img_landscape" width="252" height="172" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Average F1 score</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2104.08094/assets/confronto_train_q_W.png" id="S4.F7.sf2.g1" class="ltx_graphics ltx_img_landscape" width="252" height="172" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Average percentage of active learning questions.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>WISDM: The impact of label propagation (LP) and active learning (AL) on the subjects that participated in the FL process.</figcaption>
</figure>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.p2.2" class="ltx_p">We observe that, while the F1 score significantly improves shard after shard, the number of active learning questions decreases.
Averaging the results of both datasets, the number of active learning questions at the first shard is around <math id="S4.SS3.SSS2.p2.1.m1.1" class="ltx_Math" alttext="25\%" display="inline"><semantics id="S4.SS3.SSS2.p2.1.m1.1a"><mrow id="S4.SS3.SSS2.p2.1.m1.1.1" xref="S4.SS3.SSS2.p2.1.m1.1.1.cmml"><mn id="S4.SS3.SSS2.p2.1.m1.1.1.2" xref="S4.SS3.SSS2.p2.1.m1.1.1.2.cmml">25</mn><mo id="S4.SS3.SSS2.p2.1.m1.1.1.1" xref="S4.SS3.SSS2.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.1.m1.1b"><apply id="S4.SS3.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.SSS2.p2.1.m1.1.1.1.cmml" xref="S4.SS3.SSS2.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS2.p2.1.m1.1.1.2.cmml" xref="S4.SS3.SSS2.p2.1.m1.1.1.2">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.1.m1.1c">25\%</annotation></semantics></math>, while at the last shard is only around <math id="S4.SS3.SSS2.p2.2.m2.1" class="ltx_Math" alttext="5\%" display="inline"><semantics id="S4.SS3.SSS2.p2.2.m2.1a"><mrow id="S4.SS3.SSS2.p2.2.m2.1.1" xref="S4.SS3.SSS2.p2.2.m2.1.1.cmml"><mn id="S4.SS3.SSS2.p2.2.m2.1.1.2" xref="S4.SS3.SSS2.p2.2.m2.1.1.2.cmml">5</mn><mo id="S4.SS3.SSS2.p2.2.m2.1.1.1" xref="S4.SS3.SSS2.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.2.m2.1b"><apply id="S4.SS3.SSS2.p2.2.m2.1.1.cmml" xref="S4.SS3.SSS2.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.SSS2.p2.2.m2.1.1.1.cmml" xref="S4.SS3.SSS2.p2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS2.p2.2.m2.1.1.2.cmml" xref="S4.SS3.SSS2.p2.2.m2.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.2.m2.1c">5\%</annotation></semantics></math>. This result indicates that our method continuously improves the recognition rate with a limited amount of labels provided by the users. Moreover, the continuous decrease of the number of questions militates for the usability of our method, which will prompt fewer and fewer questions with time.
These figures also show the impact of combining active learning with label propagation. Without label propagation, active learning alone leads to a lower recognition rate and a higher number of questions. This means that the labeled data points derived by label propagation positively improve the activity model.
On the other hand, we observe that label propagation leads to unsatisfying results without active learning. Indeed, the labeled samples obtained by active learning represent informative data that are crucial for label propagation.
Hence, the evaluation on both datasets confirms that the combination of active learning and label propagation leads to the best results.</p>
</div>
<div id="S4.SS3.SSS2.p3" class="ltx_para">
<p id="S4.SS3.SSS2.p3.2" class="ltx_p">In Figure <a href="#S4.F8" title="Figure 8 ‣ 4.3.2. Impact of semi-supervised learning ‣ 4.3. Results ‣ 4. Experimental evaluation ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> and Figure <a href="#S4.F9" title="Figure 9 ‣ 4.3.2. Impact of semi-supervised learning ‣ 4.3. Results ‣ 4. Experimental evaluation ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> we show the generalization capability of the global model on left-out users (i.e., users in partition <math id="S4.SS3.SSS2.p3.1.m1.1" class="ltx_Math" alttext="Ts" display="inline"><semantics id="S4.SS3.SSS2.p3.1.m1.1a"><mrow id="S4.SS3.SSS2.p3.1.m1.1.1" xref="S4.SS3.SSS2.p3.1.m1.1.1.cmml"><mi id="S4.SS3.SSS2.p3.1.m1.1.1.2" xref="S4.SS3.SSS2.p3.1.m1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS3.SSS2.p3.1.m1.1.1.1" xref="S4.SS3.SSS2.p3.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS3.SSS2.p3.1.m1.1.1.3" xref="S4.SS3.SSS2.p3.1.m1.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p3.1.m1.1b"><apply id="S4.SS3.SSS2.p3.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p3.1.m1.1.1"><times id="S4.SS3.SSS2.p3.1.m1.1.1.1.cmml" xref="S4.SS3.SSS2.p3.1.m1.1.1.1"></times><ci id="S4.SS3.SSS2.p3.1.m1.1.1.2.cmml" xref="S4.SS3.SSS2.p3.1.m1.1.1.2">𝑇</ci><ci id="S4.SS3.SSS2.p3.1.m1.1.1.3.cmml" xref="S4.SS3.SSS2.p3.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p3.1.m1.1c">Ts</annotation></semantics></math>) after each communication round performed during the FL process with the users in <math id="S4.SS3.SSS2.p3.2.m2.1" class="ltx_Math" alttext="Tr" display="inline"><semantics id="S4.SS3.SSS2.p3.2.m2.1a"><mrow id="S4.SS3.SSS2.p3.2.m2.1.1" xref="S4.SS3.SSS2.p3.2.m2.1.1.cmml"><mi id="S4.SS3.SSS2.p3.2.m2.1.1.2" xref="S4.SS3.SSS2.p3.2.m2.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS3.SSS2.p3.2.m2.1.1.1" xref="S4.SS3.SSS2.p3.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS3.SSS2.p3.2.m2.1.1.3" xref="S4.SS3.SSS2.p3.2.m2.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p3.2.m2.1b"><apply id="S4.SS3.SSS2.p3.2.m2.1.1.cmml" xref="S4.SS3.SSS2.p3.2.m2.1.1"><times id="S4.SS3.SSS2.p3.2.m2.1.1.1.cmml" xref="S4.SS3.SSS2.p3.2.m2.1.1.1"></times><ci id="S4.SS3.SSS2.p3.2.m2.1.1.2.cmml" xref="S4.SS3.SSS2.p3.2.m2.1.1.2">𝑇</ci><ci id="S4.SS3.SSS2.p3.2.m2.1.1.3.cmml" xref="S4.SS3.SSS2.p3.2.m2.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p3.2.m2.1c">Tr</annotation></semantics></math>.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2104.08094/assets/confronto_history_2.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="258" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>MobiAct: the trend of F1 score on the left-out users after each communication round. This Figure also shows the impact of active learning and label propagation. Each red line marks the end of a shard.</figcaption>
</figure>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2104.08094/assets/history_train_W.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="258" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>WISDM: the trend of F1 score on the left-out users after each communication round. This Figure also shows the impact of active learning and label propagation. Each red line marks the end of a shard.</figcaption>
</figure>
<div id="S4.SS3.SSS2.p4" class="ltx_para">
<p id="S4.SS3.SSS2.p4.1" class="ltx_p">The red lines mark the end of each shard. The results indicate that the federated model constantly improves also for those users that did not contribute with training data, even if the active learning questions continuously decrease.
These plots also confirm that the combination of label propagation and active learning leads to the best results on both datasets.</p>
</div>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3. </span>FedHAR  versus approaches based on fully labeled data</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">We compared our approach with two existing FL methods based on fully labeled data. The first one is the well-known FedAVG <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2017</a>)</cite>, which is the most common FL method in the literature. FedAVG simply averages the model parameters derived by the local training on the participating nodes (without any personalization).</p>
</div>
<div id="S4.SS3.SSS3.p2" class="ltx_para">
<p id="S4.SS3.SSS3.p2.1" class="ltx_p">The second method that we use for comparison is called FedHealth <cite class="ltx_cite ltx_citemacro_citep">(Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>. This is one of the first FL approaches proposed for activity recognition on wearable sensors data. Similarly to our approach, FedHealth applies personalization using transfer learning.</p>
</div>
<div id="S4.SS3.SSS3.p3" class="ltx_para">
<p id="S4.SS3.SSS3.p3.1" class="ltx_p">For the sake of fairness in our experiments, we adapted FedAVG and FedHealth to use the same neural network that we use in FedHAR. Hence, we performed our experiments using our evaluation methodology by simulating that, for FedAVG and FedHealth, each node has the ground truth for each data sample on each shard. Hence, the evaluation of those methods does not include active learning and label propagation. Moreover, differently from FedHAR, FedAVG and FedHealth only use a single local model.</p>
</div>
<div id="S4.SS3.SSS3.p4" class="ltx_para">
<p id="S4.SS3.SSS3.p4.1" class="ltx_p">The results of this comparison for the users in <math id="S4.SS3.SSS3.p4.1.m1.1" class="ltx_Math" alttext="Tr" display="inline"><semantics id="S4.SS3.SSS3.p4.1.m1.1a"><mrow id="S4.SS3.SSS3.p4.1.m1.1.1" xref="S4.SS3.SSS3.p4.1.m1.1.1.cmml"><mi id="S4.SS3.SSS3.p4.1.m1.1.1.2" xref="S4.SS3.SSS3.p4.1.m1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS3.SSS3.p4.1.m1.1.1.1" xref="S4.SS3.SSS3.p4.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS3.SSS3.p4.1.m1.1.1.3" xref="S4.SS3.SSS3.p4.1.m1.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p4.1.m1.1b"><apply id="S4.SS3.SSS3.p4.1.m1.1.1.cmml" xref="S4.SS3.SSS3.p4.1.m1.1.1"><times id="S4.SS3.SSS3.p4.1.m1.1.1.1.cmml" xref="S4.SS3.SSS3.p4.1.m1.1.1.1"></times><ci id="S4.SS3.SSS3.p4.1.m1.1.1.2.cmml" xref="S4.SS3.SSS3.p4.1.m1.1.1.2">𝑇</ci><ci id="S4.SS3.SSS3.p4.1.m1.1.1.3.cmml" xref="S4.SS3.SSS3.p4.1.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p4.1.m1.1c">Tr</annotation></semantics></math> (i.e., the ones that actively participated in the FL process) are reported in Figure <a href="#S4.F10.sf1" title="In Figure 10 ‣ 4.3.3. FedHAR versus approaches based on fully labeled data ‣ 4.3. Results ‣ 4. Experimental evaluation ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10a</span></a> and Figure <a href="#S4.F10.sf2" title="In Figure 10 ‣ 4.3.3. FedHAR versus approaches based on fully labeled data ‣ 4.3. Results ‣ 4. Experimental evaluation ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10b</span></a>.</p>
</div>
<figure id="S4.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F10.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2104.08094/assets/conf_mobiact.png" id="S4.F10.sf1.g1" class="ltx_graphics ltx_img_landscape" width="252" height="170" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>MobiAct</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F10.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2104.08094/assets/conf_wisdm.png" id="S4.F10.sf2.g1" class="ltx_graphics ltx_img_landscape" width="252" height="170" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>WISDM</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10. </span>Comparison of FedHAR  with methods that assume based on fully labeled data</figcaption>
</figure>
<div id="S4.SS3.SSS3.p5" class="ltx_para">
<p id="S4.SS3.SSS3.p5.1" class="ltx_p">From these plots, we observe that FedHAR  converges to recognition rates that are similar to solutions based on fully labeled data at each shard. The advantage of FedHAR  is that it can be used for realistic HAR deployments where the availability of labeled data is scarce. Despite a reduced number of required annotations, FedHAR  performs even better than FedAVG on the WISDM dataset, while on MobiAct it performs slightly worse. Moreover, FedHAR  is only <math id="S4.SS3.SSS3.p5.1.m1.1" class="ltx_Math" alttext="\approx 3\%" display="inline"><semantics id="S4.SS3.SSS3.p5.1.m1.1a"><mrow id="S4.SS3.SSS3.p5.1.m1.1.1" xref="S4.SS3.SSS3.p5.1.m1.1.1.cmml"><mi id="S4.SS3.SSS3.p5.1.m1.1.1.2" xref="S4.SS3.SSS3.p5.1.m1.1.1.2.cmml"></mi><mo id="S4.SS3.SSS3.p5.1.m1.1.1.1" xref="S4.SS3.SSS3.p5.1.m1.1.1.1.cmml">≈</mo><mrow id="S4.SS3.SSS3.p5.1.m1.1.1.3" xref="S4.SS3.SSS3.p5.1.m1.1.1.3.cmml"><mn id="S4.SS3.SSS3.p5.1.m1.1.1.3.2" xref="S4.SS3.SSS3.p5.1.m1.1.1.3.2.cmml">3</mn><mo id="S4.SS3.SSS3.p5.1.m1.1.1.3.1" xref="S4.SS3.SSS3.p5.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS3.p5.1.m1.1b"><apply id="S4.SS3.SSS3.p5.1.m1.1.1.cmml" xref="S4.SS3.SSS3.p5.1.m1.1.1"><approx id="S4.SS3.SSS3.p5.1.m1.1.1.1.cmml" xref="S4.SS3.SSS3.p5.1.m1.1.1.1"></approx><csymbol cd="latexml" id="S4.SS3.SSS3.p5.1.m1.1.1.2.cmml" xref="S4.SS3.SSS3.p5.1.m1.1.1.2">absent</csymbol><apply id="S4.SS3.SSS3.p5.1.m1.1.1.3.cmml" xref="S4.SS3.SSS3.p5.1.m1.1.1.3"><csymbol cd="latexml" id="S4.SS3.SSS3.p5.1.m1.1.1.3.1.cmml" xref="S4.SS3.SSS3.p5.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S4.SS3.SSS3.p5.1.m1.1.1.3.2.cmml" xref="S4.SS3.SSS3.p5.1.m1.1.1.3.2">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS3.p5.1.m1.1c">\approx 3\%</annotation></semantics></math> behind FedHealth on both datasets.</p>
</div>
</section>
<section id="S4.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.4. </span>FedHAR  performance on each activity</h4>

<div id="S4.SS3.SSS4.p1" class="ltx_para">
<p id="S4.SS3.SSS4.p1.1" class="ltx_p">Figure <a href="#S4.F11" title="Figure 11 ‣ 4.3.4. FedHAR performance on each activity ‣ 4.3. Results ‣ 4. Experimental evaluation ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> shows how the recognition rate improves between shards for each activity for the users in <math id="S4.SS3.SSS4.p1.1.m1.1" class="ltx_Math" alttext="Tr" display="inline"><semantics id="S4.SS3.SSS4.p1.1.m1.1a"><mrow id="S4.SS3.SSS4.p1.1.m1.1.1" xref="S4.SS3.SSS4.p1.1.m1.1.1.cmml"><mi id="S4.SS3.SSS4.p1.1.m1.1.1.2" xref="S4.SS3.SSS4.p1.1.m1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS3.SSS4.p1.1.m1.1.1.1" xref="S4.SS3.SSS4.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS3.SSS4.p1.1.m1.1.1.3" xref="S4.SS3.SSS4.p1.1.m1.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS4.p1.1.m1.1b"><apply id="S4.SS3.SSS4.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS4.p1.1.m1.1.1"><times id="S4.SS3.SSS4.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS4.p1.1.m1.1.1.1"></times><ci id="S4.SS3.SSS4.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS4.p1.1.m1.1.1.2">𝑇</ci><ci id="S4.SS3.SSS4.p1.1.m1.1.1.3.cmml" xref="S4.SS3.SSS4.p1.1.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS4.p1.1.m1.1c">Tr</annotation></semantics></math> on both datasets.</p>
</div>
<figure id="S4.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F11.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2104.08094/assets/train_single_act_v2.png" id="S4.F11.sf1.g1" class="ltx_graphics ltx_img_landscape" width="252" height="169" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>MobiAct</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F11.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2104.08094/assets/train_single_act_W.png" id="S4.F11.sf2.g1" class="ltx_graphics ltx_img_landscape" width="252" height="169" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>WISDM</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11. </span>F1 score at each shard for each activity on the users that participated in the FL process.</figcaption>
</figure>
<div id="S4.SS3.SSS4.p2" class="ltx_para">
<p id="S4.SS3.SSS4.p2.1" class="ltx_p">We observed an improvement of the recognition rate shard after shard for each considered activity. The only exception is the <span id="S4.SS3.SSS4.p2.1.1" class="ltx_text ltx_font_italic">standing</span> activity on the MobiAct dataset in the third shard, which maintains the same F1 score. The greatest improvement occurs between the first and the second shard. This is due to the fact that, in the first shard, activities are recognized using the initial global model only trained with the <span id="S4.SS3.SSS4.p2.1.2" class="ltx_text ltx_font_italic">pre-training dataset</span>. Starting from the second shard, classification is performed with the <span id="S4.SS3.SSS4.p2.1.3" class="ltx_text ltx_font_italic">Personalized Local Model</span> updated thanks to FL and personalized using our transfer learning approach.</p>
</div>
</section>
<section id="S4.SS3.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.5. </span>Impact of personalization</h4>

<div id="S4.SS3.SSS5.p1" class="ltx_para">
<p id="S4.SS3.SSS5.p1.1" class="ltx_p">Figure <a href="#S4.F12" title="Figure 12 ‣ 4.3.5. Impact of personalization ‣ 4.3. Results ‣ 4. Experimental evaluation ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> and Figure <a href="#S4.F13" title="Figure 13 ‣ 4.3.5. Impact of personalization ‣ 4.3. Results ‣ 4. Experimental evaluation ‣ Personalized Semi-Supervised Federated Learning for Human Activity Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> show the impact of the FedHAR  personalization strategy based on transfer learning. This evaluation is performed on the users in the <math id="S4.SS3.SSS5.p1.1.m1.1" class="ltx_Math" alttext="Tr" display="inline"><semantics id="S4.SS3.SSS5.p1.1.m1.1a"><mrow id="S4.SS3.SSS5.p1.1.m1.1.1" xref="S4.SS3.SSS5.p1.1.m1.1.1.cmml"><mi id="S4.SS3.SSS5.p1.1.m1.1.1.2" xref="S4.SS3.SSS5.p1.1.m1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS3.SSS5.p1.1.m1.1.1.1" xref="S4.SS3.SSS5.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS3.SSS5.p1.1.m1.1.1.3" xref="S4.SS3.SSS5.p1.1.m1.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS5.p1.1.m1.1b"><apply id="S4.SS3.SSS5.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS5.p1.1.m1.1.1"><times id="S4.SS3.SSS5.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS5.p1.1.m1.1.1.1"></times><ci id="S4.SS3.SSS5.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS5.p1.1.m1.1.1.2">𝑇</ci><ci id="S4.SS3.SSS5.p1.1.m1.1.1.3.cmml" xref="S4.SS3.SSS5.p1.1.m1.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS5.p1.1.m1.1c">Tr</annotation></semantics></math> partition.</p>
</div>
<figure id="S4.F12" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F12.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2104.08094/assets/train_f1.png" id="S4.F12.sf1.g1" class="ltx_graphics ltx_img_landscape" width="252" height="168" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Average F1 score</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F12.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2104.08094/assets/train_q.png" id="S4.F12.sf2.g1" class="ltx_graphics ltx_img_landscape" width="252" height="172" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Average percentage of active learning questions</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12. </span>MobiAct: results on the users that participated to the FL process for each shard, with and without personalization.</figcaption>
</figure>
<figure id="S4.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F13.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2104.08094/assets/train_f1_W.png" id="S4.F13.sf1.g1" class="ltx_graphics ltx_img_landscape" width="252" height="172" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Average F1 score</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F13.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2104.08094/assets/train_q_W.png" id="S4.F13.sf2.g1" class="ltx_graphics ltx_img_landscape" width="252" height="172" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Average percentage of active learning questions</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13. </span>WISDM: results on the users that participated to the FL process for each shard, with and without personalization.</figcaption>
</figure>
<div id="S4.SS3.SSS5.p2" class="ltx_para">
<p id="S4.SS3.SSS5.p2.1" class="ltx_p">As expected, fine-tuning the personal models leads to an improvement both on the recognition rate and on the number of questions in active learning. Note that, during the first shard, classification is performed using the weights derived from the <span id="S4.SS3.SSS5.p2.1.1" class="ltx_text ltx_font_italic">pre-trained dataset</span> and personalization is applied starting from the second shard.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Discussion</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Generality of the approach</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">While we designed FedHAR  with wearable-based activity recognition as target application, we believe that this combination of semi-supervised and FL can be applied also to many other applications. Our method is suitable for human-centered classification tasks that include the following characteristics:</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">There is a large number of clients that participate in the FL process.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">Classification needs to be performed on a continuous data stream, where labels are not naturally available.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p">Each node generates a significant amount of unlabeled data.</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p id="S5.I1.i4.p1.1" class="ltx_p">It is possible to periodically obtain the ground truth by delivering active learning questions to users that are available to provide a small number of labels. Note that, in real-time applications like HAR, for the sake of usability active learning questions should be prompted temporally close to the prediction.</p>
</div>
</li>
<li id="S5.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i5.p1" class="ltx_para">
<p id="S5.I1.i5.p1.1" class="ltx_p">It is possible to obtain a limited training set to initialize the global model. Hence, a small group of volunteers should be available (in an initial phase) for annotated data acquisition.</p>
</div>
</li>
<li id="S5.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i6.p1" class="ltx_para">
<p id="S5.I1.i6.p1.1" class="ltx_p">The nodes should be capable of computing training operations. Clearly, nodes can also rely on trusted edge gateways/servers (like proposed in <cite class="ltx_cite ltx_citemacro_citep">(Wu
et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2020a</a>)</cite>).</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Privacy concerns</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Despite FL is a significant step towards protecting user privacy in distributed machine learning, the shared model weights may still reveal some sensitive information about the participating users <cite class="ltx_cite ltx_citemacro_citep">(Nasr
et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2018</a>; Shokri
et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2017</a>)</cite>.
Similarly to other works, FedHAR  uses Secure Multiparty Computation (SMC) <cite class="ltx_cite ltx_citemacro_citep">(Bhowmick et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2018</a>; Cramer
et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2000</a>)</cite> to mitigate this problem.
However, other approaches have been proposed, including differential privacy (DP) <cite class="ltx_cite ltx_citemacro_citep">(Agarwal et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2018</a>; Dwork, <a href="#bib.bib21" title="" class="ltx_ref">2008</a>)</cite>, and hybrid approaches that combine SMC and DP <cite class="ltx_cite ltx_citemacro_citep">(Truex et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">The advantage of DP is the reduced communication overhead, with the cost of affecting the accuracy of the model. For the sake of this work, we opted for SMC in order to more realistically compare the effectiveness of our semi-supervised approach with other approaches, considering privacy as an orthogonal problem. However, we also plan to investigate how to integrate differential privacy in our framework and its impact on the recognition rate.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Need of larger datasets for evaluation</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We evaluated FedHAR  choosing those well-known public HAR datasets that involved the highest number of users, simulating the periodicity of FL iterations by partitioning the dataset. However, the effectiveness of FedHAR  on large scale scenarios should be evaluated on significantly larger datasets. By larger, we mean in terms of the number of users involved, the amount of available data for each user, and the number of target activities. Indeed, FedHAR  makes sense when thousands of users are involved, continuously performing activities day after day. However, observing the encouraging results on our limited datasets, we are confident that FedHAR  would perform even better on such large scale evaluations.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Resource efficiency</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">It is important to mention that FedHAR  is not optimized in terms of computational efficiency.
Indeed, training two deep learning models on mobile devices may be computationally demanding and it may be problematic especially for those devices with low computational capabilities. This problem could be mitigated by relying on trusted edge gateways, as proposed in <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2020b</a>)</cite>.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">We want to point out that several research groups are proposing effective ways to dramatically reduce computational efforts for deep learning processes on mobile devices <cite class="ltx_cite ltx_citemacro_citep">(Lane et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2016</a>; Zhang
et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2019</a>)</cite>.
Moreover, the GPU modules embedded in recent smartphones exhibit performances similar to the ones of entry-level desktop GPUs and this trend is expected to improve in the next few years <cite class="ltx_cite ltx_citemacro_citep">(Ignatov et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">Another limitation of our work is that the label propagation model requires storing the collected feature vectors as a graph. This is clearly not sustainable for a long time on a mobile device. This problem could be solved by imposing a limit on the size of the label propagation graph and periodically deleting old or poorly informative nodes.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion and future work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this work we presented FedHAR, a novel semi-supervised federated learning framework for activity recognition on mobile devices. FedHAR  takes into account the data scarcity problem, combining active learning and label propagation to semi-automatically annotate sensor data for each user.
To the best of our knowledge, FedHAR  is the first application of federated learning to activity recognition that is not based on the assumption that labeled data exists for all participating clients.
Our results show that the combination of active learning and label propagation leads to recognition rates that are close to the ones reached by solutions that rely on fully supervised learning to train the local models.
</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">In future work,
we plan to use a different federated model depending on the profile of each user (e.g., age, fitness, weight). Indeed, HAR is more effective when the collaborative model is trained considering users with similar physical traits <cite class="ltx_cite ltx_citemacro_citep">(Sztyler and
Stuckenschmidt, <a href="#bib.bib48" title="" class="ltx_ref">2017</a>)</cite>. This approach would require to adapt FedHAR  to automatically group users who share similar characteristics and, at the same time, protecting their privacy.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdallah et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Zahraa Said Abdallah,
Mohamed Medhat Gaber, Bala Srinivasan,
and Shonali Krishnaswamy.
2015.

</span>
<span class="ltx_bibblock">Adaptive mobile activity recognition system with
evolving data streams.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">Neurocomputing</em> 150
(2015), 304–317.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdallah et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Zahraa S Abdallah,
Mohamed Medhat Gaber, Bala Srinivasan,
and Shonali Krishnaswamy.
2018.

</span>
<span class="ltx_bibblock">Activity recognition with evolving data streams: A
review.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys (CSUR)</em>
51, 4 (2018),
71.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agarwal et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Naman Agarwal,
Ananda Theertha Suresh, Felix Xinnan X
Yu, Sanjiv Kumar, and Brendan
McMahan. 2018.

</span>
<span class="ltx_bibblock">cpsgd: Communication-efficient and
differentially-private distributed sgd. In
<em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em>. 7564–7575.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao and Intille (2004)</span>
<span class="ltx_bibblock">
Ling Bao and Stephen S.
Intille. 2004.

</span>
<span class="ltx_bibblock">Activity Recognition from User-Annotated
Acceleration Data. In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Pervasive Computing: Second
International Conference, PERVASIVE 2004, Linz/Vienna, Austria, April 21-23,
2004. Proceedings</em>. Springer, Berlin,
Heidelberg, 1–17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bettini
et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Claudio Bettini, Gabriele
Civitarese, and Riccardo Presotto.
2020.

</span>
<span class="ltx_bibblock">CAVIAR: Context-driven active and incremental
activity recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">Knowledge-Based Systems</em>
196 (2020), 105816.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bettini and
Riboni (2015)</span>
<span class="ltx_bibblock">
Claudio Bettini and
Daniele Riboni. 2015.

</span>
<span class="ltx_bibblock">Privacy protection in pervasive systems: State of
the art and technical challenges.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Pervasive and Mobile Computing</em>
17 (2015), 159–174.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhowmick et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Abhishek Bhowmick, John
Duchi, Julien Freudiger, Gaurav Kapoor,
and Ryan Rogers. 2018.

</span>
<span class="ltx_bibblock">Protection against reconstruction and its
applications in private federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.00984</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Keith Bonawitz, Vladimir
Ivanov, Ben Kreuter, Antonio Marcedone,
H Brendan McMahan, Sarvar Patel,
Daniel Ramage, Aaron Segal, and
Karn Seth. 2017.

</span>
<span class="ltx_bibblock">Practical secure aggregation for privacy-preserving
machine learning. In <em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 ACM
SIGSAC Conference on Computer and Communications Security</em>.
1175–1191.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bulling
et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Andreas Bulling, Ulf
Blanke, and Bernt Schiele.
2014.

</span>
<span class="ltx_bibblock">A Tutorial on Human Activity Recognition Using
Body-worn Inertial Sensors.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">Comput. Surveys</em> 46,
3 (2014), 33:1–33:33.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan and Noor (2020)</span>
<span class="ltx_bibblock">
Mang Hong Chan and Mohd
Halim Mohd Noor. 2020.

</span>
<span class="ltx_bibblock">A unified generative model using generative
adversarial network for activity recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Journal of Ambient Intelligence and Humanized
Computing</em> (2020), 1–10.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chawla
et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2002)</span>
<span class="ltx_bibblock">
Nitesh V Chawla, Kevin W
Bowyer, Lawrence O Hall, and W Philip
Kegelmeyer. 2002.

</span>
<span class="ltx_bibblock">SMOTE: synthetic minority over-sampling technique.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">Journal of artificial intelligence research</em>
16 (2002), 321–357.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Liming Chen, Jesse Hoey,
Chris D Nugent, Diane J Cook, and
Zhiwen Yu. 2012.

</span>
<span class="ltx_bibblock">Sensor-based activity recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Systems, Man, and
Cybernetics, Part C (Applications and Reviews)</em> 42,
6 (2012), 790–808.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Nugent (2009)</span>
<span class="ltx_bibblock">
Liming Chen and Chris
Nugent. 2009.

</span>
<span class="ltx_bibblock">Ontology-based activity recognition in intelligent
pervasive environments.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">International Journal of Web Information
Systems</em> (2009).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yiqiang Chen, Xin Qin,
Jindong Wang, Chaohui Yu, and
Wen Gao. 2020.

</span>
<span class="ltx_bibblock">Fedhealth: A federated transfer learning framework
for wearable healthcare.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">IEEE Intelligent Systems</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Civitarese et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Gabriele Civitarese, Timo
Sztyler, Daniele Riboni, Claudio
Bettini, and Heiner Stuckenschmidt.
2019.

</span>
<span class="ltx_bibblock">POLARIS: Probabilistic and ontological activity
recognition in smart-homes.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Knowledge and Data
Engineering</em> 33, 1
(2019), 209–223.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cook
et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2013a)</span>
<span class="ltx_bibblock">
Diane Cook, Kyle D Feuz,
and Narayanan C Krishnan.
2013a.

</span>
<span class="ltx_bibblock">Transfer learning for activity recognition: A
survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">Knowledge and information systems</em>
36, 3 (2013),
537–556.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cook
et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2013b)</span>
<span class="ltx_bibblock">
Diane J. Cook, Kyle D.
Feuz, and Narayanan C. Krishnan.
2013b.

</span>
<span class="ltx_bibblock">Transfer learning for activity recognition: A
survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">Knowledge and Information Systems</em>
36, 3 (2013),
537–556.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cramer
et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2000)</span>
<span class="ltx_bibblock">
Ronald Cramer, Ivan
Damgård, and Ueli Maurer.
2000.

</span>
<span class="ltx_bibblock">General secure multi-party computation from any
linear secret-sharing scheme. In <em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">International
Conference on the Theory and Applications of Cryptographic Techniques</em>.
Springer, 316–334.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Damaskinos et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Georgios Damaskinos,
Rachid Guerraoui, Anne-Marie Kermarrec,
Vlad Nitu, Rhicheek Patra, and
Francois Taiani. 2020.

</span>
<span class="ltx_bibblock">Fleet: Online federated learning via staleness
awareness and performance prediction. In
<em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 21st International Middleware
Conference</em>. 163–177.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork (2008)</span>
<span class="ltx_bibblock">
Cynthia Dwork.
2008.

</span>
<span class="ltx_bibblock">Differential privacy: A survey of results. In
<em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">International conference on theory and applications
of models of computation</em>. Springer, 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ek
et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Sannara Ek, François
Portet, Philippe Lalanda, and German
Vega. 2020.

</span>
<span class="ltx_bibblock">Evaluation of federated learning aggregation
algorithms: application to human activity recognition. In
<em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">Adjunct Proceedings of the 2020 ACM International
Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the
2020 ACM International Symposium on Wearable Computers</em>.
638–643.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guan
et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2007)</span>
<span class="ltx_bibblock">
Donghai Guan, Weiwei
Yuan, Young-Koo Lee, Andrey Gavrilov,
and Sungyoung Lee. 2007.

</span>
<span class="ltx_bibblock">Activity recognition based on semi-supervised
learning. In <em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">Embedded and Real-Time Computing
Systems and Applications, 2007. RTCSA 2007. 13th IEEE International
Conference on</em>. IEEE, 469–475.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Györbíró et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2009)</span>
<span class="ltx_bibblock">
Norbert Györbíró,
Ákos Fábián, and Gergely
Hományi. 2009.

</span>
<span class="ltx_bibblock">An activity recognition system for mobile phones.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">Mobile Networks and Applications</em>
14, 1 (2009),
82–91.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hard et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Andrew Hard, Kanishka
Rao, Rajiv Mathews, Swaroop Ramaswamy,
Françoise Beaufays, Sean
Augenstein, Hubert Eichner, Chloé
Kiddon, and Daniel Ramage.
2018.

</span>
<span class="ltx_bibblock">Federated learning for mobile keyboard prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.03604</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hossain
et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
HM Sajjad Hossain,
Md Abdullah Al Hafiz Khan, and Nirmalya
Roy. 2017.

</span>
<span class="ltx_bibblock">Active learning enabled activity recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">Pervasive and Mobile Computing</em>
38 (2017), 312–330.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ignatov et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Andrey Ignatov, Radu
Timofte, Andrei Kulik, Seungsoo Yang,
Ke Wang, Felix Baum, Max
Wu, Lirong Xu, and Luc Van Gool.
2019.

</span>
<span class="ltx_bibblock">Ai benchmark: All about deep learning on
smartphones in 2019. In <em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">2019 IEEE/CVF
International Conference on Computer Vision Workshop (ICCVW)</em>. IEEE,
3617–3635.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz
et al<span id="bib.bib28.3.3.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan
McMahan, Brendan Avent, Aurélien
Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary
Charles, Graham Cormode, Rachel
Cummings, et al<span id="bib.bib28.4.1" class="ltx_text">.</span> 2019.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.5.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.04977</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2017)</span>
<span class="ltx_bibblock">
Diederik P. Kingma and
Jimmy Ba. 2017.

</span>
<span class="ltx_bibblock">Adam: A Method for Stochastic Optimization.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1412.6980 [cs.LG]

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečnỳ et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Jakub Konečnỳ,
H Brendan McMahan, Felix X Yu,
Peter Richtárik, Ananda Theertha
Suresh, and Dave Bacon.
2016.

</span>
<span class="ltx_bibblock">Federated learning: Strategies for improving
communication efficiency.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.05492</em>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwapisz
et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2011)</span>
<span class="ltx_bibblock">
Jennifer R Kwapisz, Gary M
Weiss, and Samuel A Moore.
2011.

</span>
<span class="ltx_bibblock">Activity recognition using cell phone
accelerometers.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">ACM SigKDD Explorations Newsletter</em>
12, 2 (2011),
74–82.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lane et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Nicholas D Lane, Sourav
Bhattacharya, Petko Georgiev, Claudio
Forlivesi, Lei Jiao, Lorena Qendro,
and Fahim Kawsar. 2016.

</span>
<span class="ltx_bibblock">Deepx: A software accelerator for low-power deep
learning inference on mobile devices. In <em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">2016 15th
ACM/IEEE International Conference on Information Processing in Sensor
Networks (IPSN)</em>. IEEE, 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lara
et al<span id="bib.bib33.3.3.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Oscar D Lara, Miguel A
Labrador, et al<span id="bib.bib33.4.1" class="ltx_text">.</span> 2013.

</span>
<span class="ltx_bibblock">A survey on human activity recognition using
wearable sensors.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.5.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys and Tutorials</em>
15, 3 (2013),
1192–1209.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee and Cho (2014)</span>
<span class="ltx_bibblock">
Young-Seol Lee and
Sung-Bae Cho. 2014.

</span>
<span class="ltx_bibblock">Activity recognition with android phone using
mixture-of-experts co-trained with labeled and unlabeled data.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em> 126
(2014), 106–115.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Longstaff
et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2010)</span>
<span class="ltx_bibblock">
Brent Longstaff, Sasank
Reddy, and Deborah Estrin.
2010.

</span>
<span class="ltx_bibblock">Improving activity classification for health
applications on mobile devices using active and semi-supervised learning. In
<em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">Pervasive Computing Technologies for Healthcare
(PervasiveHealth), 2010 4th International Conference on Pervasive Computing
Technologies for Healthcare</em>. IEEE, 1–7.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider
Moore, Daniel Ramage, Seth Hampson,
and Blaise Aguera y Arcas.
2017.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks
from decentralized data. In <em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">Artificial
Intelligence and Statistics</em>. PMLR, 1273–1282.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miu
et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Tudor Miu, Paolo Missier,
and Thomas Plötz. 2015.

</span>
<span class="ltx_bibblock">Bootstrapping personalised human activity
recognition models using online active learning. In
<em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">2015 IEEE International Conference on Computer and
Information Technology; Ubiquitous Computing and Communications; Dependable,
Autonomic and Secure Computing; Pervasive Intelligence and Computing</em>. IEEE,
1138–1147.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nasr
et al<span id="bib.bib38.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Milad Nasr, Reza Shokri,
and Amir Houmansadr. 2018.

</span>
<span class="ltx_bibblock">Comprehensive privacy analysis of deep learning:
Stand-alone and federated learning under passive and active white-box
inference attacks.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.00910</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rashid and Louis (2019)</span>
<span class="ltx_bibblock">
Khandakar M Rashid and
Joseph Louis. 2019.

</span>
<span class="ltx_bibblock">Times-series data augmentation and deep learning
for construction equipment activity recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Advanced Engineering Informatics</em>
42 (2019), 100944.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Samarati (2014)</span>
<span class="ltx_bibblock">
Pierangela Samarati.
2014.

</span>
<span class="ltx_bibblock">Data security and privacy in the cloud. In
<em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">International Conference on Information Security
Practice and Experience</em>. Springer, 28–41.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanabria
et al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Andrea Rosales Sanabria,
Franco Zambonelli, and Juan Ye.
2021.

</span>
<span class="ltx_bibblock">Unsupervised Domain Adaptation in Activity
Recognition: A GAN-Based Approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.3.1" class="ltx_emph ltx_font_italic">IEEE Access</em> 9
(2021), 19421–19438.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shokri
et al<span id="bib.bib42.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Reza Shokri, Marco
Stronati, Congzheng Song, and Vitaly
Shmatikov. 2017.

</span>
<span class="ltx_bibblock">Membership inference attacks against machine
learning models. In <em id="bib.bib42.3.1" class="ltx_emph ltx_font_italic">2017 IEEE Symposium on
Security and Privacy (SP)</em>. IEEE, 3–18.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soleimani and
Nazerfard (2021)</span>
<span class="ltx_bibblock">
Elnaz Soleimani and
Ehsan Nazerfard. 2021.

</span>
<span class="ltx_bibblock">Cross-subject transfer learning in human activity
recognition systems using generative adversarial networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em> 426
(2021), 26–34.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sozinov
et al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Konstantin Sozinov,
Vladimir Vlassov, and Sarunas
Girdzijauskas. 2018.

</span>
<span class="ltx_bibblock">Human activity recognition using federated
learning. In <em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">2018 IEEE Intl Conf on Parallel &amp;
Distributed Processing with Applications, Ubiquitous Computing &amp;
Communications, Big Data &amp; Cloud Computing, Social Computing &amp; Networking,
Sustainable Computing &amp; Communications
(ISPA/IUCC/BDCloud/SocialCom/SustainCom)</em>. IEEE,
1103–1111.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stikic
et al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> (2009)</span>
<span class="ltx_bibblock">
Maja Stikic, Diane
Larlus, and Bernt Schiele.
2009.

</span>
<span class="ltx_bibblock">Multi-graph based semi-supervised learning for
activity recognition. In <em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">2009 international
symposium on wearable computers</em>. IEEE, 85–92.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stikic
et al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2008)</span>
<span class="ltx_bibblock">
Maja Stikic, Kristof
Van Laerhoven, and Bernt Schiele.
2008.

</span>
<span class="ltx_bibblock">Exploring semi-supervised and active learning for
activity recognition. In <em id="bib.bib46.3.1" class="ltx_emph ltx_font_italic">2008 12th IEEE
International Symposium on Wearable Computers</em>. IEEE,
81–88.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun
et al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2010)</span>
<span class="ltx_bibblock">
Lin Sun, Daqing Zhang,
Bin Li, Bin Guo, and
Shijian Li. 2010.

</span>
<span class="ltx_bibblock">Activity recognition on an accelerometer embedded
mobile phone with varying positions and orientations. In
<em id="bib.bib47.3.1" class="ltx_emph ltx_font_italic">International conference on ubiquitous intelligence
and computing</em>. Springer, 548–562.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sztyler and
Stuckenschmidt (2017)</span>
<span class="ltx_bibblock">
Timo Sztyler and Heiner
Stuckenschmidt. 2017.

</span>
<span class="ltx_bibblock">Online personalization of cross-subjects based
activity recognition models on wearable devices. In
<em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">2017 IEEE International Conference on Pervasive
Computing and Communications (PerCom)</em>. IEEE, 180–189.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Truex et al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Stacey Truex, Nathalie
Baracaldo, Ali Anwar, Thomas Steinke,
Heiko Ludwig, Rui Zhang, and
Yi Zhou. 2019.

</span>
<span class="ltx_bibblock">A hybrid approach to privacy-preserving federated
learning. In <em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th ACM Workshop
on Artificial Intelligence and Security</em>. 1–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vavoulas et al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
George Vavoulas,
Charikleia Chatzaki, Thodoris
Malliotakis, Matthew Pediaditis, and
Manolis Tsiknakis. 2016.

</span>
<span class="ltx_bibblock">The MobiAct Dataset: Recognition of Activities of
Daily Living using Smartphones.. In
<em id="bib.bib50.3.1" class="ltx_emph ltx_font_italic">ICT4AgeingWell</em>. 143–151.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voigt and Von dem
Bussche (2017)</span>
<span class="ltx_bibblock">
Paul Voigt and Axel
Von dem Bussche. 2017.

</span>
<span class="ltx_bibblock">The eu general data protection regulation (gdpr).

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">A Practical Guide, 1st Ed., Cham: Springer
International Publishing</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
et al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> (2018a)</span>
<span class="ltx_bibblock">
Jiwei Wang, Yiqiang Chen,
Yang Gu, Yunlong Xiao, and
Haonan Pan. 2018a.

</span>
<span class="ltx_bibblock">SensoryGANs: An effective generative adversarial
framework for sensor-based human activity recognition. In
<em id="bib.bib52.3.1" class="ltx_emph ltx_font_italic">2018 International Joint Conference on Neural
Networks (IJCNN)</em>. IEEE, 1–8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
et al<span id="bib.bib53.2.2.1" class="ltx_text">.</span> (2018b)</span>
<span class="ltx_bibblock">
Jindong Wang, Vincent W
Zheng, Yiqiang Chen, and Meiyu Huang.
2018b.

</span>
<span class="ltx_bibblock">Deep transfer learning for cross-domain activity
recognition. In <em id="bib.bib53.3.1" class="ltx_emph ltx_font_italic">proceedings of the 3rd
International Conference on Crowd Science and Engineering</em>.
1–8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weiss and
Lockhart (2012)</span>
<span class="ltx_bibblock">
Gary Mitchell Weiss and
Jeffrey Lockhart. 2012.

</span>
<span class="ltx_bibblock">The impact of personalization on smartphone-based
activity recognition. In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Workshops at the
Twenty-Sixth AAAI Conference on Artificial Intelligence</em>. Citeseer.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Widmann and
Verberne (2017)</span>
<span class="ltx_bibblock">
Natalie Widmann and
Suzan Verberne. 2017.

</span>
<span class="ltx_bibblock">Graph-based semi-supervised learning for text
classification. In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM SIGIR
international conference on theory of information retrieval</em>.
59–66.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu
et al<span id="bib.bib56.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Qiong Wu, Xu Chen,
Zhi Zhou, and Junshan Zhang.
2020a.

</span>
<span class="ltx_bibblock">FedHome: Cloud-Edge based Personalized Federated
Learning for In-Home Health Monitoring.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Mobile Computing</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Qiong Wu, Kaiwen He,
and Xu Chen. 2020b.

</span>
<span class="ltx_bibblock">Personalized federated learning for intelligent iot
applications: A cloud-edge based framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">IEEE Computer Graphics and Applications</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang
et al<span id="bib.bib58.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu,
Tianjian Chen, and Yongxin Tong.
2019.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and
applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.3.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and
Technology (TIST)</em> 10, 2
(2019), 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yosinski
et al<span id="bib.bib59.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Jason Yosinski, Jeff
Clune, Yoshua Bengio, and Hod Lipson.
2014.

</span>
<span class="ltx_bibblock">How transferable are features in deep neural
networks?. In <em id="bib.bib59.3.1" class="ltx_emph ltx_font_italic">Advances in neural information
processing systems</em>. 3320–3328.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang
et al<span id="bib.bib60.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Chaoyun Zhang, Paul
Patras, and Hamed Haddadi.
2019.

</span>
<span class="ltx_bibblock">Deep learning in mobile and wireless networking: A
survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.3.1" class="ltx_emph ltx_font_italic">IEEE Communications surveys &amp; tutorials</em>
21, 3 (2019),
2224–2287.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao
et al<span id="bib.bib61.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yuchen Zhao, Hanyang Liu,
Honglin Li, Payam Barnaghi, and
Hamed Haddadi. 2020.

</span>
<span class="ltx_bibblock">Semi-supervised Federated Learning for Activity
Recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib61.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2011.00851</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu and
Ghahramani (2002)</span>
<span class="ltx_bibblock">
Xiaojin Zhu and Zoubin
Ghahramani. 2002.

</span>
<span class="ltx_bibblock">Learning from labeled and unlabeled data with label
propagation.

</span>
<span class="ltx_bibblock">(2002).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Žliobaitė et al<span id="bib.bib63.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Indrė Žliobaitė,
Albert Bifet, Bernhard Pfahringer, and
Geoffrey Holmes. 2013.

</span>
<span class="ltx_bibblock">Active learning with drifting streaming data.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.3.1" class="ltx_emph ltx_font_italic">IEEE transactions on neural networks and
learning systems</em> 25, 1
(2013), 27–39.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2104.08093" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2104.08094" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2104.08094">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2104.08094" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2104.08095" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 17 03:04:02 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
