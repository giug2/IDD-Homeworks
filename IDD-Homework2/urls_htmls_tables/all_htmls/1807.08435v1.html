<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1807.08435] Question Relevance in Visual Question Answering</title><meta property="og:description" content="Free-form and open-ended Visual Question Answering systems solve the problem of providing an accurate natural language answer to a question pertaining to an image. Current VQA systems do not evaluate if the posed quest…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Question Relevance in Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Question Relevance in Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1807.08435">

<!--Generated on Sat Mar  9 05:46:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Question Relevance in Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Prakruthi Prabhakar
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">ML with Large Datasets (10-805) 
<br class="ltx_break">Carnegie Mellon University</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_city">Pittsburgh</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_state">PA</span><span id="id4.4.id4" class="ltx_text ltx_affiliation_postcode">15217</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:prakrutp@andrew.cmu.edu">prakrutp@andrew.cmu.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nitish Kulkarni
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_affiliation_institution">ML with Large Datasets (10-805) 
<br class="ltx_break">Carnegie Mellon University</span><span id="id6.2.id2" class="ltx_text ltx_affiliation_city">Pittsburgh</span><span id="id7.3.id3" class="ltx_text ltx_affiliation_state">PA</span><span id="id8.4.id4" class="ltx_text ltx_affiliation_postcode">15217</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:nitishkk@andrew.cmu.edu">nitishkk@andrew.cmu.edu</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Linghao Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id9.1.id1" class="ltx_text ltx_affiliation_institution">ML with Large Datasets (10-805) 
<br class="ltx_break">Carnegie Mellon University</span><span id="id10.2.id2" class="ltx_text ltx_affiliation_city">Pittsburgh</span><span id="id11.3.id3" class="ltx_text ltx_affiliation_state">PA</span><span id="id12.4.id4" class="ltx_text ltx_affiliation_postcode">15217</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:linghaoz@andrew.cmu.edu">linghaoz@andrew.cmu.edu</a>
</span></span></span>
</div>
<div class="ltx_dates">(;)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id13.id1" class="ltx_p">Free-form and open-ended Visual Question Answering systems solve the problem of providing an accurate natural language answer to a question pertaining to an image. Current VQA systems do not evaluate if the posed question is relevant to the input image and hence provide nonsensical answers when posed with irrelevant questions to an image. In this paper, we solve the problem of identifying the relevance of the posed question to an image. We address the problem as two sub-problems. We first identify if the question is visual or not. If the question is visual, we then determine if it’s relevant to the image or not.
For the second problem, we generate a large dataset from existing visual question answering datasets in order to enable the training of complex architectures and model the relevance of a visual question to an image. We also compare the results of our Long Short-Term Memory Recurrent Neural Network based models to Logistic Regression, XGBoost and multi-layer perceptron based approaches to the problem.</p>
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>;  </span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_article"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">article: </span>4</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_price"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">price: </span>15.00</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The task of automatically answering questions in the context of visual information has gained prominence in the last few years. Being able to answer open-ended questions about an image is a challenging task, but one of great practical significance. For instance, visually impaired individuals might inquire about different aspects of an image in the form of free-form questions. However, when Visual Question Answering (VQA) systems are provided with irrelevant questions, they tend to provide nonsensical answers. VQA systems in real world scenarios are expected to be sophisticated to identify the relevance of the posed free-form questions to an input image, to answer them better.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">There are two aspects of relevance of a question to the input image:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Non-visual questions which do not require any input image to answer the question</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">False-premise questions which require an input image but do not pertain to the provided input image to answer the question
<br class="ltx_break"></p>
</div>
</li>
</ol>
<p id="S1.p2.2" class="ltx_p">In this project, we formulate the problem as follows:</p>
<blockquote id="S1.p2.3" class="ltx_quote">
<p id="S1.p2.3.1" class="ltx_p">Given an image and a natural language question, identify if the question is relevant to the input image.</p>
</blockquote>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">For visual versus non-visual question detection, we present the results of two approaches. The first approach is based on training a Logistic Regression model using unigrams, bigrams and trigrams of Part-of-Speech (POS) tags of the question. In the second approach, we use a Long Short-Term Memory (LSTM) Recurrent Neural Network trained on Part-of-Speech (POS) tags to capture the linguistic structure of questions.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">For the next sub-problem of identifying true versus false premise of a visual question to an image, we curate a much larger dataset compared to the existing datasets for the problem using variations in different existing data extraction methodologies. We also present the results of different models used for modeling the true versus false premise problem. In our initial approaches, we use Logistic Regression and XGBoost classifier using both visual and textual features to model the problem. We also explore several Long Short-Term Memory (LSTM) Recurrent Neural Network architectures and a multi-layer perceptron network to model the problem. Our code is available at <cite class="ltx_cite ltx_citemacro_citep">(git, <a href="#bib.bib2" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">There have been significant advances in recent years on identifying the similarity between images and textual information. Text-based image retrieval <cite class="ltx_cite ltx_citemacro_citep">(Liu
et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2009</a>)</cite> systems and visual semantic alignments in image captioning models <cite class="ltx_cite ltx_citemacro_citep">(Karpathy and
Fei-Fei, <a href="#bib.bib12" title="" class="ltx_ref">2015</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Fang
et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2015</a>)</cite> are some examples of efforts in that direction. While some systems do not answer when the input is ill-formed or likely to result in failure, some others try to find the most meaningful answer to such inputs. <cite class="ltx_cite ltx_citemacro_citep">(Dodge
et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2012</a>)</cite> tries separating visual text from non-visual text in image descriptions and use them for enhancing image captioning systems. These ideas can be used to boost the performance of visual question relevance task.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Much of our work is based on the problem and approaches presented in <cite class="ltx_cite ltx_citemacro_citep">(Ray
et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite>. In this paper, the authors identify the two facets of question relevance for Visual Question Answering, i.e. categorizing the questions as visual versus non-visual questions, and then identifying if a question has a true premise for an image. The paper also provides several baselines for both problems. For the visual versus non-visual classification, the authors propose a heuristic-based and an LSTM-based approach. And for the true versus false premise problem, there are three baselines based on entropy from VQA models, question-caption similarity and question-question similarity.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">For the problem to identifying false premise, <cite class="ltx_cite ltx_citemacro_citep">(Mahendru et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2017</a>)</cite> also makes significant contribution for extracting the premise from a question. The authors of this work also go on to create a well-curated, much larger and more class-balanced dataset for the true versus false premise based on the VQA dataset <cite class="ltx_cite ltx_citemacro_citep">(Antol et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2015</a>)</cite>. In this paper, the concept of a premise in a question is explored in greater detail and a more diverse set of problems are addressed, such as - given an image and a question with a false premise, can we predict the premise in the question that cannot be answered by the image. However, the focus of our work is on exploring scalable algorithms and architectures for answering <span id="S2.p3.1.1" class="ltx_text ltx_font_italic">if</span> the question can be answered in the context of the image.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">In the context of establishing semantic relationships between images and text, the work done in <cite class="ltx_cite ltx_citemacro_citep">(Akata
et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2016</a>)</cite> is also quite relevant, where the authors of this paper propose a holistic embedding technique for combining multiple and diverse language representations for better transfer of knowledge, by mapping visual and language parts into a common embedding space. Although our problem is supervised, we believe that such an embedding could help improve the identification of question relevance for an image.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Dataset</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">While Visual Question Answering has been a widely explored problem, question relevance for the same is relatively very new, owing to which there are no existing large-sized and diversely represented datasets.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">For the first task of detecting visual versus non-visual questions, we refer to the methodology used in <cite class="ltx_cite ltx_citemacro_citep">(Ray
et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite>. Since VQA 2.0 dataset <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2016</a>)</cite> is now available, we use the training, validation and test questions for images from this dataset as visual questions. For non-visual questions, we use the philosophical and general knowledge questions provided by <cite class="ltx_cite ltx_citemacro_citep">(Ray
et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite>. Combining the two sources and eliminating duplicate questions, we have
160,010 questions for training, 82,067 questions for validation and 148,927 questions for test datasets. The key limitations of this dataset are the class bias, and that the non-visual questions are not diversely representative of all possible non-visual questions.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">The dataset for the second sub-problem of detecting questions with false premise is based on the VQA corpus <cite class="ltx_cite ltx_citemacro_citep">(Antol et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2015</a>)</cite>. The data acquisition involves creating image-question pairs with true and false premises for the question. The questions in the VQA dataset can be assumed to have true premises, since they were manually generated.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">To identify the questions with false premises, we choose to use the questions from the same dataset, but for other images. Here, we explore three approaches:</p>
</div>
<div id="S3.p5" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Question Similarity</span> 
<br class="ltx_break"></p>
</div>
<div id="S3.I1.i1.p2" class="ltx_para">
<p id="S3.I1.i1.p2.1" class="ltx_p">In this approach, for every image, we use the set of true-premise questions from the VQA dataset <cite class="ltx_cite ltx_citemacro_citep">(Antol et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2015</a>)</cite>, and extract <math id="S3.I1.i1.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.I1.i1.p2.1.m1.1a"><mi id="S3.I1.i1.p2.1.m1.1.1" xref="S3.I1.i1.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p2.1.m1.1b"><ci id="S3.I1.i1.p2.1.m1.1.1.cmml" xref="S3.I1.i1.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p2.1.m1.1c">k</annotation></semantics></math> least similar questions from the set. As similarity measure, we try using doc2vec similarity and word2vec similarity for keywords in a question (nouns, verbs and adjectives). 
<br class="ltx_break"></p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Visual True vs False Premise Questions (VTFQ) Dataset</span> 
<br class="ltx_break"></p>
</div>
<div id="S3.I1.i2.p2" class="ltx_para">
<p id="S3.I1.i2.p2.1" class="ltx_p">In this approach, we use the dataset presented in <cite class="ltx_cite ltx_citemacro_citep">(Ray
et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite>, where the methodology is similar to the one described for question similarity, but instead of using a question similarity measure, the authors sample random questions from the set, and have Amazon Mechanical Turk (AMT) workers annotate the questions as relevant or irrelevant to the corresponding images. The VTFQ dataset consists of 10,793 question-image pairs with 1,500 unique images of which 79% of the pairs have false premise. 
<br class="ltx_break"></p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Question Relevance Prediction and Explanation (QRPE) Dataset</span> 
<br class="ltx_break"></p>
</div>
<div id="S3.I1.i3.p2" class="ltx_para">
<p id="S3.I1.i3.p2.6" class="ltx_p">This dataset is presented by <cite class="ltx_cite ltx_citemacro_citep">(Mahendru et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2017</a>)</cite>, with a methodology that uses VQA 1.0 questions, COCO images and Visual Genome annotations. The approach first involves a premise-extraction for a given question, where a question premise is defined as a semantic tuple comprising of objects, objects and their attributes or objects and their relationships in an image. Then, for a given question-image pair in the VQA dataset, a set of all images is created which has exactly one premise as false for the question, which are referred as negative images. To make the false-premise detection problem challenging, the negative images that are most similar to the positive image (i.e. image with true premise for the question) are chosen. The QRPE dataset consists of 53,911 tuples of the form <math id="S3.I1.i3.p2.1.m1.4" class="ltx_Math" alttext="(I^{+},Q,P,I^{-})" display="inline"><semantics id="S3.I1.i3.p2.1.m1.4a"><mrow id="S3.I1.i3.p2.1.m1.4.4.2" xref="S3.I1.i3.p2.1.m1.4.4.3.cmml"><mo stretchy="false" id="S3.I1.i3.p2.1.m1.4.4.2.3" xref="S3.I1.i3.p2.1.m1.4.4.3.cmml">(</mo><msup id="S3.I1.i3.p2.1.m1.3.3.1.1" xref="S3.I1.i3.p2.1.m1.3.3.1.1.cmml"><mi id="S3.I1.i3.p2.1.m1.3.3.1.1.2" xref="S3.I1.i3.p2.1.m1.3.3.1.1.2.cmml">I</mi><mo id="S3.I1.i3.p2.1.m1.3.3.1.1.3" xref="S3.I1.i3.p2.1.m1.3.3.1.1.3.cmml">+</mo></msup><mo id="S3.I1.i3.p2.1.m1.4.4.2.4" xref="S3.I1.i3.p2.1.m1.4.4.3.cmml">,</mo><mi id="S3.I1.i3.p2.1.m1.1.1" xref="S3.I1.i3.p2.1.m1.1.1.cmml">Q</mi><mo id="S3.I1.i3.p2.1.m1.4.4.2.5" xref="S3.I1.i3.p2.1.m1.4.4.3.cmml">,</mo><mi id="S3.I1.i3.p2.1.m1.2.2" xref="S3.I1.i3.p2.1.m1.2.2.cmml">P</mi><mo id="S3.I1.i3.p2.1.m1.4.4.2.6" xref="S3.I1.i3.p2.1.m1.4.4.3.cmml">,</mo><msup id="S3.I1.i3.p2.1.m1.4.4.2.2" xref="S3.I1.i3.p2.1.m1.4.4.2.2.cmml"><mi id="S3.I1.i3.p2.1.m1.4.4.2.2.2" xref="S3.I1.i3.p2.1.m1.4.4.2.2.2.cmml">I</mi><mo id="S3.I1.i3.p2.1.m1.4.4.2.2.3" xref="S3.I1.i3.p2.1.m1.4.4.2.2.3.cmml">−</mo></msup><mo stretchy="false" id="S3.I1.i3.p2.1.m1.4.4.2.7" xref="S3.I1.i3.p2.1.m1.4.4.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p2.1.m1.4b"><vector id="S3.I1.i3.p2.1.m1.4.4.3.cmml" xref="S3.I1.i3.p2.1.m1.4.4.2"><apply id="S3.I1.i3.p2.1.m1.3.3.1.1.cmml" xref="S3.I1.i3.p2.1.m1.3.3.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p2.1.m1.3.3.1.1.1.cmml" xref="S3.I1.i3.p2.1.m1.3.3.1.1">superscript</csymbol><ci id="S3.I1.i3.p2.1.m1.3.3.1.1.2.cmml" xref="S3.I1.i3.p2.1.m1.3.3.1.1.2">𝐼</ci><plus id="S3.I1.i3.p2.1.m1.3.3.1.1.3.cmml" xref="S3.I1.i3.p2.1.m1.3.3.1.1.3"></plus></apply><ci id="S3.I1.i3.p2.1.m1.1.1.cmml" xref="S3.I1.i3.p2.1.m1.1.1">𝑄</ci><ci id="S3.I1.i3.p2.1.m1.2.2.cmml" xref="S3.I1.i3.p2.1.m1.2.2">𝑃</ci><apply id="S3.I1.i3.p2.1.m1.4.4.2.2.cmml" xref="S3.I1.i3.p2.1.m1.4.4.2.2"><csymbol cd="ambiguous" id="S3.I1.i3.p2.1.m1.4.4.2.2.1.cmml" xref="S3.I1.i3.p2.1.m1.4.4.2.2">superscript</csymbol><ci id="S3.I1.i3.p2.1.m1.4.4.2.2.2.cmml" xref="S3.I1.i3.p2.1.m1.4.4.2.2.2">𝐼</ci><minus id="S3.I1.i3.p2.1.m1.4.4.2.2.3.cmml" xref="S3.I1.i3.p2.1.m1.4.4.2.2.3"></minus></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p2.1.m1.4c">(I^{+},Q,P,I^{-})</annotation></semantics></math>, where <math id="S3.I1.i3.p2.2.m2.2" class="ltx_Math" alttext="(I^{+},Q)" display="inline"><semantics id="S3.I1.i3.p2.2.m2.2a"><mrow id="S3.I1.i3.p2.2.m2.2.2.1" xref="S3.I1.i3.p2.2.m2.2.2.2.cmml"><mo stretchy="false" id="S3.I1.i3.p2.2.m2.2.2.1.2" xref="S3.I1.i3.p2.2.m2.2.2.2.cmml">(</mo><msup id="S3.I1.i3.p2.2.m2.2.2.1.1" xref="S3.I1.i3.p2.2.m2.2.2.1.1.cmml"><mi id="S3.I1.i3.p2.2.m2.2.2.1.1.2" xref="S3.I1.i3.p2.2.m2.2.2.1.1.2.cmml">I</mi><mo id="S3.I1.i3.p2.2.m2.2.2.1.1.3" xref="S3.I1.i3.p2.2.m2.2.2.1.1.3.cmml">+</mo></msup><mo id="S3.I1.i3.p2.2.m2.2.2.1.3" xref="S3.I1.i3.p2.2.m2.2.2.2.cmml">,</mo><mi id="S3.I1.i3.p2.2.m2.1.1" xref="S3.I1.i3.p2.2.m2.1.1.cmml">Q</mi><mo stretchy="false" id="S3.I1.i3.p2.2.m2.2.2.1.4" xref="S3.I1.i3.p2.2.m2.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p2.2.m2.2b"><interval closure="open" id="S3.I1.i3.p2.2.m2.2.2.2.cmml" xref="S3.I1.i3.p2.2.m2.2.2.1"><apply id="S3.I1.i3.p2.2.m2.2.2.1.1.cmml" xref="S3.I1.i3.p2.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p2.2.m2.2.2.1.1.1.cmml" xref="S3.I1.i3.p2.2.m2.2.2.1.1">superscript</csymbol><ci id="S3.I1.i3.p2.2.m2.2.2.1.1.2.cmml" xref="S3.I1.i3.p2.2.m2.2.2.1.1.2">𝐼</ci><plus id="S3.I1.i3.p2.2.m2.2.2.1.1.3.cmml" xref="S3.I1.i3.p2.2.m2.2.2.1.1.3"></plus></apply><ci id="S3.I1.i3.p2.2.m2.1.1.cmml" xref="S3.I1.i3.p2.2.m2.1.1">𝑄</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p2.2.m2.2c">(I^{+},Q)</annotation></semantics></math> is a pair of positive image and true premise question, <math id="S3.I1.i3.p2.3.m3.1" class="ltx_Math" alttext="I^{-}" display="inline"><semantics id="S3.I1.i3.p2.3.m3.1a"><msup id="S3.I1.i3.p2.3.m3.1.1" xref="S3.I1.i3.p2.3.m3.1.1.cmml"><mi id="S3.I1.i3.p2.3.m3.1.1.2" xref="S3.I1.i3.p2.3.m3.1.1.2.cmml">I</mi><mo id="S3.I1.i3.p2.3.m3.1.1.3" xref="S3.I1.i3.p2.3.m3.1.1.3.cmml">−</mo></msup><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p2.3.m3.1b"><apply id="S3.I1.i3.p2.3.m3.1.1.cmml" xref="S3.I1.i3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p2.3.m3.1.1.1.cmml" xref="S3.I1.i3.p2.3.m3.1.1">superscript</csymbol><ci id="S3.I1.i3.p2.3.m3.1.1.2.cmml" xref="S3.I1.i3.p2.3.m3.1.1.2">𝐼</ci><minus id="S3.I1.i3.p2.3.m3.1.1.3.cmml" xref="S3.I1.i3.p2.3.m3.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p2.3.m3.1c">I^{-}</annotation></semantics></math> is the negative image with <math id="S3.I1.i3.p2.4.m4.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S3.I1.i3.p2.4.m4.1a"><mi id="S3.I1.i3.p2.4.m4.1.1" xref="S3.I1.i3.p2.4.m4.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p2.4.m4.1b"><ci id="S3.I1.i3.p2.4.m4.1.1.cmml" xref="S3.I1.i3.p2.4.m4.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p2.4.m4.1c">P</annotation></semantics></math> as the premise in <math id="S3.I1.i3.p2.5.m5.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.I1.i3.p2.5.m5.1a"><mi id="S3.I1.i3.p2.5.m5.1.1" xref="S3.I1.i3.p2.5.m5.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p2.5.m5.1b"><ci id="S3.I1.i3.p2.5.m5.1.1.cmml" xref="S3.I1.i3.p2.5.m5.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p2.5.m5.1c">Q</annotation></semantics></math> that is false for <math id="S3.I1.i3.p2.6.m6.1" class="ltx_Math" alttext="I^{-}" display="inline"><semantics id="S3.I1.i3.p2.6.m6.1a"><msup id="S3.I1.i3.p2.6.m6.1.1" xref="S3.I1.i3.p2.6.m6.1.1.cmml"><mi id="S3.I1.i3.p2.6.m6.1.1.2" xref="S3.I1.i3.p2.6.m6.1.1.2.cmml">I</mi><mo id="S3.I1.i3.p2.6.m6.1.1.3" xref="S3.I1.i3.p2.6.m6.1.1.3.cmml">−</mo></msup><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p2.6.m6.1b"><apply id="S3.I1.i3.p2.6.m6.1.1.cmml" xref="S3.I1.i3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p2.6.m6.1.1.1.cmml" xref="S3.I1.i3.p2.6.m6.1.1">superscript</csymbol><ci id="S3.I1.i3.p2.6.m6.1.1.2.cmml" xref="S3.I1.i3.p2.6.m6.1.1.2">𝐼</ci><minus id="S3.I1.i3.p2.6.m6.1.1.3.cmml" xref="S3.I1.i3.p2.6.m6.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p2.6.m6.1c">I^{-}</annotation></semantics></math>. While a single QI pair for true and false premises is extracted using this approach.</p>
</div>
</li>
</ol>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">The problem with the first two approaches is that using random or least similar questions for an image would make the problem of false-premise detection much easier than the case where the a single premise of the question were to be false in the context of the image or if the negative images are very similar to the positive images. While this is addressed in QRPE dataset, it has only 53,911 positive and negative image pairs, generated from the VQA dataset that is much larger to begin with. This is largely because of the constraints imposed in the dataset construction such as restricting the questions and objects to be of specific categories. The constraints are placed in order to minimize the noisy samples since the data is generated heuristically. Since the dataset is too small, most of the current state-of-the-art models use pre-trained VQA or image captioning models trained on COCO dataset.</p>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p">We hypothesize that a much larger dataset with only a marginal reduction in robustness would help build a more effective model for classifying true vs false premise, since a large dataset would enable end-to-end training of deep architectures to optimize the classification performance for this task. To test this hypothesis, we present an extended QRPE dataset that is built by making some modifications to the methodology in <cite class="ltx_cite ltx_citemacro_citep">(Mahendru et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S3.p8" class="ltx_para">
<p id="S3.p8.1" class="ltx_p">The first difference is that we use VQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2016</a>)</cite>, which has twice as many image-question pairs and also contains complementary image-question pairs. In addition, we relax some constraints on question types (while regulating robustness of the dataset) as well as increase the number of negative images generated for each question from 1 to 10. Lastly, we use all the image-question pairs in VQA 2.0 as true-premise pairs, including the ones that do not have negative images. This is because the goal of our dataset is to have a large dataset with good representation of both the classes, and not to generate <math id="S3.p8.1.m1.4" class="ltx_Math" alttext="(I^{+},Q,P,I^{-})" display="inline"><semantics id="S3.p8.1.m1.4a"><mrow id="S3.p8.1.m1.4.4.2" xref="S3.p8.1.m1.4.4.3.cmml"><mo stretchy="false" id="S3.p8.1.m1.4.4.2.3" xref="S3.p8.1.m1.4.4.3.cmml">(</mo><msup id="S3.p8.1.m1.3.3.1.1" xref="S3.p8.1.m1.3.3.1.1.cmml"><mi id="S3.p8.1.m1.3.3.1.1.2" xref="S3.p8.1.m1.3.3.1.1.2.cmml">I</mi><mo id="S3.p8.1.m1.3.3.1.1.3" xref="S3.p8.1.m1.3.3.1.1.3.cmml">+</mo></msup><mo id="S3.p8.1.m1.4.4.2.4" xref="S3.p8.1.m1.4.4.3.cmml">,</mo><mi id="S3.p8.1.m1.1.1" xref="S3.p8.1.m1.1.1.cmml">Q</mi><mo id="S3.p8.1.m1.4.4.2.5" xref="S3.p8.1.m1.4.4.3.cmml">,</mo><mi id="S3.p8.1.m1.2.2" xref="S3.p8.1.m1.2.2.cmml">P</mi><mo id="S3.p8.1.m1.4.4.2.6" xref="S3.p8.1.m1.4.4.3.cmml">,</mo><msup id="S3.p8.1.m1.4.4.2.2" xref="S3.p8.1.m1.4.4.2.2.cmml"><mi id="S3.p8.1.m1.4.4.2.2.2" xref="S3.p8.1.m1.4.4.2.2.2.cmml">I</mi><mo id="S3.p8.1.m1.4.4.2.2.3" xref="S3.p8.1.m1.4.4.2.2.3.cmml">−</mo></msup><mo stretchy="false" id="S3.p8.1.m1.4.4.2.7" xref="S3.p8.1.m1.4.4.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p8.1.m1.4b"><vector id="S3.p8.1.m1.4.4.3.cmml" xref="S3.p8.1.m1.4.4.2"><apply id="S3.p8.1.m1.3.3.1.1.cmml" xref="S3.p8.1.m1.3.3.1.1"><csymbol cd="ambiguous" id="S3.p8.1.m1.3.3.1.1.1.cmml" xref="S3.p8.1.m1.3.3.1.1">superscript</csymbol><ci id="S3.p8.1.m1.3.3.1.1.2.cmml" xref="S3.p8.1.m1.3.3.1.1.2">𝐼</ci><plus id="S3.p8.1.m1.3.3.1.1.3.cmml" xref="S3.p8.1.m1.3.3.1.1.3"></plus></apply><ci id="S3.p8.1.m1.1.1.cmml" xref="S3.p8.1.m1.1.1">𝑄</ci><ci id="S3.p8.1.m1.2.2.cmml" xref="S3.p8.1.m1.2.2">𝑃</ci><apply id="S3.p8.1.m1.4.4.2.2.cmml" xref="S3.p8.1.m1.4.4.2.2"><csymbol cd="ambiguous" id="S3.p8.1.m1.4.4.2.2.1.cmml" xref="S3.p8.1.m1.4.4.2.2">superscript</csymbol><ci id="S3.p8.1.m1.4.4.2.2.2.cmml" xref="S3.p8.1.m1.4.4.2.2.2">𝐼</ci><minus id="S3.p8.1.m1.4.4.2.2.3.cmml" xref="S3.p8.1.m1.4.4.2.2.3"></minus></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.p8.1.m1.4c">(I^{+},Q,P,I^{-})</annotation></semantics></math> tuples. 
<br class="ltx_break"></p>
</div>
<div id="S3.p9" class="ltx_para ltx_noindent">
<p id="S3.p9.1" class="ltx_p"><span id="S3.p9.1.1" class="ltx_text ltx_font_bold">Dataset Construction:</span></p>
</div>
<div id="S3.p10" class="ltx_para ltx_noindent">
<p id="S3.p10.1" class="ltx_p">Since the true premise instances are directly derived from the VQA 2.0 dataset, the key challenge lies in generating a mapping of images to questions with false premises. To ensure that the irrelevance of the question does not become trivial, we use only the top 10 most similar images to the positive image for the question. To compute the similarity of images, cosine similarity of VGG 16 image features<cite class="ltx_cite ltx_citemacro_citep">(Simonyan and
Zisserman, <a href="#bib.bib18" title="" class="ltx_ref">2014a</a>)</cite> is used, since it has been pre-trained on 1.3M images. To identify the set of image-question pairs with false premise, two kinds of premises are considered:</p>
<ol id="S3.I2" class="ltx_enumerate">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">First Order Premises</span>:
These are the existential premises that represent only the presence of objects, such as <span id="S3.I2.i1.p1.1.2" class="ltx_text ltx_font_italic">cat</span>, <span id="S3.I2.i1.p1.1.3" class="ltx_text ltx_font_italic">dog</span> etc. To generate image-question pairs with negative first order premises, we use the class annotations from the COCO dataset and check for their presence in the question.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Second Order Premises</span>:
These contain the objects and their attributes, such as <span id="S3.I2.i2.p1.1.2" class="ltx_text ltx_font_italic">small cat</span>, <span id="S3.I2.i2.p1.1.3" class="ltx_text ltx_font_italic">black dog</span> etc. For a false second order premise, we consider those images with true first order premise i.e. with the object present in the image, and look for the opposite attribute. To get the premises for a given image, we construct scene graphs by using the semantic tuple extraction pipeline used in the SPICE metric <cite class="ltx_cite ltx_citemacro_citep">(Anderson et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2016</a>)</cite>, which is an image captioning metric. We compare these premises with the Visual Genome scene graphs for the COCO images, and use the images whose attribute is an antonym of the attribute in the question.</p>
</div>
</li>
</ol>
</div>
<div id="S3.p11" class="ltx_para">
<p id="S3.p11.1" class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‣ 3. Dataset ‣ Question Relevance in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the data characteristics of the dataset created using this methodology and the applied modifications.</p>
</div>
<div id="S3.p12" class="ltx_para">
<p id="S3.p12.1" class="ltx_p">Some negative images generated for false first and second order premises are illustrated in Figures <a href="#S3.F1" title="Figure 1 ‣ 3. Dataset ‣ Question Relevance in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S3.F2" title="Figure 2 ‣ 3. Dataset ‣ Question Relevance in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We can notice how the objects in the negative images for the first order premise are different but look very similar to the object in the positive image (in this case, the dog). On the other hand, for the second order premise, the object (container) is the same in the negative images, but the attribute is different (large vs small). Thus, it is much harder to identify false second order premises than false first order premises. In some cases, it is not very obvious if the premise is false, for example, in this case, it may not be clear if the container should be considered large or small. This is why we focus more on classifying the first order pairs, as they are more definitive.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt"></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Total</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Relevant</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Non-relevant</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Total</td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3,697,728</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1,551,009</td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">2,146,719</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">First order</td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2,671,037</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">658,111</td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">2,012,926</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">Second order</td>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">1,026,691</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">892,898</td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">133,793</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Extended QRPE dataset characteristics for true vs false premise</figcaption>
</figure>
<figure id="S3.F1" class="ltx_figure"><img src="/html/1807.08435/assets/images/First_Order.png" id="S3.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="538" height="494" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Positive (top left) and negative (top right, bottom left and bottom right) images based on first-order premises for a sample question (top).</figcaption>
</figure>
<figure id="S3.F2" class="ltx_figure"><img src="/html/1807.08435/assets/images/Second_Order.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="538" height="495" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Positive (top left) and negative (top right, bottom left and bottom right) images based on second-order premises for a sample question (top).</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Approach</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Among the different models experimented with, we first present the ones for visual-vs-non-visual question detection, followed by the models used for true-vs-false premise question relevance problem.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Visual vs. Non-visual Question Detection</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We identify that non-visual questions have different linguistic structure than visual questions. For example, non-visual questions such as ”Name the national rugby team of Argentina.” or ”Who is the president of Zimbabwe?” often have differences in structure in comparison to visual questions such as ”Is this truck yellow?” and ”What color are the giraffes?”. Hence, we use Spacy <cite class="ltx_cite ltx_citemacro_citep">(Honnibal and
Johnson, <a href="#bib.bib11" title="" class="ltx_ref">2015</a>)</cite> to process all questions to obtain Part-of-Speech (POS) tags as features. We compare two models, a Logistic Regression model versus an LSTM-RNN based approach.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Logistic Regression</span> We trained a Logistic Regression model using POS tags of the questions as features. We also experimented with larger feature sets using bigrams and trigrams of POS tags. We implemented a scalable streaming version of logistic regression for training. We assume that the validation and test datasets fit in memory for this problem.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">LSTM</span> We also trained an LSTM model using the architecture from <cite class="ltx_cite ltx_citemacro_citep">(Ray
et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite> for modeling visual vs. non-visual question detection. This architecture is shown in Figure <a href="#S4.F3" title="Figure 3 ‣ item 2 ‣ 4.1. Visual vs. Non-visual Question Detection ‣ 4. Approach ‣ Question Relevance in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. This model uses a dimensionality of 100 for hidden vectors and POS tags of the words in the question as the sequence input to the LSTM. We also experimented with alternate architectures with varying dimensionality of the hidden vectors as well as POS-tag embeddings.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/1807.08435/assets/images/lstm_p1.png" id="S4.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="299" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>LSTM based architecture for modeling visual versus non-visual question detection. The POS tag of every word is input to the LSTM layer, which is used to predict if the question is visual or not.</figcaption>
</figure>
</li>
</ol>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>True vs. False Premise Question Relevance Detection</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">While visual versus non-visual question detection depended only on the posed textual question, the true versus false premise question relevance requires joint modeling of image and the question. We obtain visual and textual representations to model them together. For all our models, we use a pre-trained VGGNet <cite class="ltx_cite ltx_citemacro_citep">(Simonyan and
Zisserman, <a href="#bib.bib19" title="" class="ltx_ref">2014b</a>)</cite> convolutional neural network and obtain the fully connected seventh layer of the network output as visual features for the images.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<ol id="S4.I2" class="ltx_enumerate">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p"><span id="S4.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Logistic Regression</span> Our first approach to modeling the problem of true versus false premise question relevance was to use a scalable Logistic Regression classifier combining visual and textual features for the problem. For the visual features, we used Principal Component Analysis to reduce the 4096 dimensional FC7 features of an image obtained from a pre-trained VGGNet to 300 dimensions. For the textual features, we trained a FastText model <cite class="ltx_cite ltx_citemacro_citep">(Bojanowski et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2016</a>)</cite> on all the VQA questions to obtain average word embeddings for the questions as features. We combine these representations to learn a model to classify if the question is relevant to the image or not.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p"><span id="S4.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">XGBoost Gradient Boosting Classifier</span> We use the same visual and textual feature representations described above for training an XGBoost Gradient Boosting Classifier <cite class="ltx_cite ltx_citemacro_citep">(Chen and Guestrin, <a href="#bib.bib7" title="" class="ltx_ref">2016</a>)</cite>. We use disk-based implementation of XGBoost to accommodate the large training dataset size.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p"><span id="S4.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Multi-Layer Perceptron</span> We also explore a multi-layer perceptron using 4096 image features concatenated with 300 dimensional Glove embedding features of the words in the question as input. We use two hidden layers with 5000 and 500 hidden units respectively. The output layer has one unit modeling the probability of question relevance using a binary cross-entropy loss.</p>
</div>
</li>
<li id="S4.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span> 
<div id="S4.I2.i4.p1" class="ltx_para">
<p id="S4.I2.i4.p1.1" class="ltx_p"><span id="S4.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">LSTM</span> We also train different variants of LSTM architecture for jointly modeling image and textual inputs. These architectures are described briefly below. All our architecture variants are named as RelNet (Relevance Net for Question Relevance) for reference. In all architectures, the question is input the the LSTM using an Embedding layer. We initialize the embedding layer using 300 dimensional Glove embeddings <cite class="ltx_cite ltx_citemacro_citep">(Pennington
et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2014</a>)</cite>.</p>
<ol id="S4.I2.i4.I1" class="ltx_enumerate">
<li id="S4.I2.i4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="S4.I2.i4.I1.i1.p1" class="ltx_para">
<p id="S4.I2.i4.I1.i1.p1.1" class="ltx_p"><span id="S4.I2.i4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">RelNet1</span> This architecture is shown in Figure <a href="#S4.F4" title="Figure 4 ‣ item 4a ‣ item 4 ‣ 4.2. True vs. False Premise Question Relevance Detection ‣ 4. Approach ‣ Question Relevance in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. In this network, we used Principal Component Analysis to reduce the 4096 dimensional FC7 features of an image obtained from a pre-trained VGGNet to 300 dimensions. We use an LSTM layer to model the input question. The output of this LSTM layer is concatenated with the image features at every time step and then fed to another LSTM layer. This LSTM layer is trained to model the probability of the question being relevant to the image or not using a binary cross entropy loss.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/1807.08435/assets/images/lstm_p2_4.png" id="S4.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="317" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>RelNet1: Image features after PCA is input to an LSTM layer at every time step. The Question is modeled using another LSTM layer whose output is also input to the final LSTM layer.</figcaption>
</figure>
</li>
<li id="S4.I2.i4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="S4.I2.i4.I1.i2.p1" class="ltx_para">
<p id="S4.I2.i4.I1.i2.p1.1" class="ltx_p"><span id="S4.I2.i4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">RelNet2</span> This architecture is shown in Figure <a href="#S4.F5" title="Figure 5 ‣ item 4b ‣ item 4 ‣ 4.2. True vs. False Premise Question Relevance Detection ‣ 4. Approach ‣ Question Relevance in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. In this network, we modify the architecture of RelNet1 by training an embedding layer to reduce the 4096 dimensional FC7 features of an image to 300 dimensions, instead of using Principal Component Analysis.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/1807.08435/assets/images/lstm_p2_3.png" id="S4.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="314" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>RelNet2: Image features are input to an LSTM layer at every time step by passing through a linear embedding layer. The Question is modeled using another LSTM layer whose output is also input to the final LSTM layer.</figcaption>
</figure>
</li>
<li id="S4.I2.i4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span> 
<div id="S4.I2.i4.I1.i3.p1" class="ltx_para">
<p id="S4.I2.i4.I1.i3.p1.1" class="ltx_p"><span id="S4.I2.i4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">RelNet3</span> This architecture is shown in Figure <a href="#S4.F6" title="Figure 6 ‣ item 4c ‣ item 4 ‣ 4.2. True vs. False Premise Question Relevance Detection ‣ 4. Approach ‣ Question Relevance in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. In this network, we modify the architecture of RelNet2. We do not concatenate the image features with every time step output of the first LSTM layer. Instead, we input the image features at the first time step only to the final LSTM layer. The output of the language LSTM layer is fed to the final LSTM layer from the second time step.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/1807.08435/assets/images/lstm_p2_2.png" id="S4.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="311" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>RelNet3: Image features are input to an LSTM layer only at the first time step by passing through a linear embedding layer. The Question is modeled using another LSTM layer whose output is input to the final LSTM layer from the second time step onwards.</figcaption>
</figure>
</li>
<li id="S4.I2.i4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(d)</span> 
<div id="S4.I2.i4.I1.i4.p1" class="ltx_para">
<p id="S4.I2.i4.I1.i4.p1.1" class="ltx_p"><span id="S4.I2.i4.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">RelNet4</span> This architecture is shown in Figure <a href="#S4.F7" title="Figure 7 ‣ item 4d ‣ item 4 ‣ 4.2. True vs. False Premise Question Relevance Detection ‣ 4. Approach ‣ Question Relevance in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. In this network, we do not have the first LSTM layer to model the input question. We directly input the question to the final LSTM layer from the second time step using an Embedding layer.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/1807.08435/assets/images/lstm_p2.png" id="S4.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="362" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>RelNet4: Image features are input to an LSTM layer only at the first time step by passing through a linear embedding layer. The Question is input to the LSTM layer from the second time step onwards using an Embedding layer.</figcaption>
</figure>
</li>
</ol>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Experiments and results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We compare our models with several baseline approaches. We model LSTM based methods on a NVIDIA Tesla K80 GPU. The following sections presents the results for various models for both sub-problems.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Visual vs. Non-Visual Question Detection</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The results for visual vs. non-visual models are presented in Table <a href="#S5.T2" title="Table 2 ‣ 5.1. Visual vs. Non-Visual Question Detection ‣ 5. Experiments and results ‣ Question Relevance in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and Table <a href="#S5.T3" title="Table 3 ‣ 5.1. Visual vs. Non-Visual Question Detection ‣ 5. Experiments and results ‣ Question Relevance in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Since there is a class imbalance problem in the datasets, we report the average per-class (i.e., normalized) metrics for all approaches.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Table <a href="#S5.T2" title="Table 2 ‣ 5.1. Visual vs. Non-Visual Question Detection ‣ 5. Experiments and results ‣ Question Relevance in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> compares the results for the three models of Logistic Regression, using unigrams, bigrams and trigrams of POS tags as features. As can be observed from the table, addition of bigrams of POS tags as features helped improve the precision and recall of both classes significantly in comparison to using only unigrams of POS tags as features. However, using trigrams as additional features didn’t give significant improvement in the metrics. Additionally, using trigrams as features increases computational time exponentially. Hence, we use unigram and bigram based logistic regression to compare with results from LSTM model.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">We trained several models of LSTM with varying embedding and hidden vector dimensionality using a NVIDIA Tesla K80 GPU. We observed that all models performed similarly across different metrics. Hence, we provide the results of replicating the model provided in <cite class="ltx_cite ltx_citemacro_citep">(Ray
et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite>. Since we use VQA 2.0 dataset <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2016</a>)</cite> and different set of POS tags from Spacy, we identify that the results are different from the original paper for the same model.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>Results of visual versus non-visual question detection using different POS features in Logistic Regression. Here, Uni represents using unigram POS tags as features, Uni+Bi represents using unigram as well as bigram POS tags as features, Uni+Bi+Tri represents using unigram, bigram and trigram as features.</figcaption>
<table id="S5.T2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Metric</th>
<th id="S5.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Uni</th>
<th id="S5.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Uni+Bi</th>
<th id="S5.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Uni+Bi+Tri</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.2.1" class="ltx_tr">
<th id="S5.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Precision (Visual class)</th>
<td id="S5.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.9990</td>
<td id="S5.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.9996</td>
<td id="S5.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.9997</td>
</tr>
<tr id="S5.T2.1.3.2" class="ltx_tr">
<th id="S5.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Recall (Visual class)</th>
<td id="S5.T2.1.3.2.2" class="ltx_td ltx_align_center">0.9980</td>
<td id="S5.T2.1.3.2.3" class="ltx_td ltx_align_center">0.9997</td>
<td id="S5.T2.1.3.2.4" class="ltx_td ltx_align_center">0.9993</td>
</tr>
<tr id="S5.T2.1.4.3" class="ltx_tr">
<th id="S5.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Precision (Non-visual class)</th>
<td id="S5.T2.1.4.3.2" class="ltx_td ltx_align_center">0.8193</td>
<td id="S5.T2.1.4.3.3" class="ltx_td ltx_align_center">0.9690</td>
<td id="S5.T2.1.4.3.4" class="ltx_td ltx_align_center">0.9354</td>
</tr>
<tr id="S5.T2.1.5.4" class="ltx_tr">
<th id="S5.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Recall (Non-visual class)</th>
<td id="S5.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">0.9049</td>
<td id="S5.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">0.9638</td>
<td id="S5.T2.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb">0.9705</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>Comparison of results of Logistic Regression and LSTM based models for visual versus non-visual question detection</figcaption>
<table id="S5.T3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Metric</th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Logistic Regression</th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">LSTM</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<th id="S5.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Precision (Visual class)</th>
<td id="S5.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.9996</td>
<td id="S5.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.9995</td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<th id="S5.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Recall (Visual class)</th>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_center">0.9997</td>
<td id="S5.T3.1.3.2.3" class="ltx_td ltx_align_center">1.0</td>
</tr>
<tr id="S5.T3.1.4.3" class="ltx_tr">
<th id="S5.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Precision (Non-visual class)</th>
<td id="S5.T3.1.4.3.2" class="ltx_td ltx_align_center">0.9690</td>
<td id="S5.T3.1.4.3.3" class="ltx_td ltx_align_center">1.0</td>
</tr>
<tr id="S5.T3.1.5.4" class="ltx_tr">
<th id="S5.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Recall (Non-visual class)</th>
<td id="S5.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">0.9638</td>
<td id="S5.T3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">0.9511</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>True vs. False Premise Question Relevance Detection</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Table <a href="#S5.T4" title="Table 4 ‣ 5.2. True vs. False Premise Question Relevance Detection ‣ 5. Experiments and results ‣ Question Relevance in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> compares the accuracy metric for various models on the three datasets. The two baseline models presented in the table are obtained from <cite class="ltx_cite ltx_citemacro_citep">(Mahendru et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2017</a>)</cite>. VQA-Bin baseline is modeled using a pre-trained deeper LSTM VQA architecture with fine-tuning for the binary question relevance detection task. QPC-Sim baseline uses a pre-trained image captioning model to automatically provide natural language image descriptions
and identifies question relevance based on a learned similarity between the question, the premise and the generated image caption. Since the baselines were not trained on our dataset, we present the baseline results for the QRPE dataset’s first order and second order image question pairs. All our models are trained on the generated first order train dataset and tested on the generated first order test dataset, second order test dataset and QRPE test dataset.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>Results of true premise versus false premise question relevance detection using different models.</figcaption>
<table id="S5.T4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">Model</th>
<th id="S5.T4.1.1.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_tt">
<span id="S5.T4.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.2.1.1" class="ltx_p">First-order dataset</span>
</span>
</th>
<th id="S5.T4.1.1.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_tt">
<span id="S5.T4.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.3.1.1" class="ltx_p">Second-order dataset</span>
</span>
</th>
<th id="S5.T4.1.1.1.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_tt">
<span id="S5.T4.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.1.1.4.1.1" class="ltx_p">QRPE dataset</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.2.1" class="ltx_tr">
<th id="S5.T4.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">VQA-Bin (Baseline 1)</th>
<td id="S5.T4.1.2.1.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T4.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.2.1.2.1.1" class="ltx_p">0.6736</span>
</span>
</td>
<td id="S5.T4.1.2.1.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T4.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.2.1.3.1.1" class="ltx_p">0.53</span>
</span>
</td>
<td id="S5.T4.1.2.1.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S5.T4.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.2.1.4.1.1" class="ltx_p">0.665</span>
</span>
</td>
</tr>
<tr id="S5.T4.1.3.2" class="ltx_tr">
<th id="S5.T4.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">QPC-Sim (Baseline 2)</th>
<td id="S5.T4.1.3.2.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.3.2.2.1.1" class="ltx_p">0.7667</span>
</span>
</td>
<td id="S5.T4.1.3.2.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.3.2.3.1.1" class="ltx_p">0.5595</span>
</span>
</td>
<td id="S5.T4.1.3.2.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.3.2.4.1.1" class="ltx_p"><span id="S5.T4.1.3.2.4.1.1.1" class="ltx_text ltx_font_bold">0.7531</span></span>
</span>
</td>
</tr>
<tr id="S5.T4.1.4.3" class="ltx_tr">
<th id="S5.T4.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Logistic Regression</th>
<td id="S5.T4.1.4.3.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.4.3.2.1.1" class="ltx_p">0.6784</span>
</span>
</td>
<td id="S5.T4.1.4.3.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.4.3.3.1.1" class="ltx_p">0.4293</span>
</span>
</td>
<td id="S5.T4.1.4.3.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.4.3.4.1.1" class="ltx_p">0.5746</span>
</span>
</td>
</tr>
<tr id="S5.T4.1.5.4" class="ltx_tr">
<th id="S5.T4.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">XGBoost</th>
<td id="S5.T4.1.5.4.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.5.4.2.1.1" class="ltx_p">0.8622</span>
</span>
</td>
<td id="S5.T4.1.5.4.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.5.4.3.1.1" class="ltx_p">0.7466</span>
</span>
</td>
<td id="S5.T4.1.5.4.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.5.4.4.1.1" class="ltx_p">0.6206</span>
</span>
</td>
</tr>
<tr id="S5.T4.1.6.5" class="ltx_tr">
<th id="S5.T4.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">MLP</th>
<td id="S5.T4.1.6.5.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.6.5.2.1.1" class="ltx_p">0.8886</span>
</span>
</td>
<td id="S5.T4.1.6.5.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.6.5.3.1.1" class="ltx_p">0.7507</span>
</span>
</td>
<td id="S5.T4.1.6.5.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.6.5.4.1.1" class="ltx_p">0.6083</span>
</span>
</td>
</tr>
<tr id="S5.T4.1.7.6" class="ltx_tr">
<th id="S5.T4.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">RelNet1</th>
<td id="S5.T4.1.7.6.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.7.6.2.1.1" class="ltx_p"><span id="S5.T4.1.7.6.2.1.1.1" class="ltx_text ltx_font_bold">0.8889</span></span>
</span>
</td>
<td id="S5.T4.1.7.6.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.7.6.3.1.1" class="ltx_p">-</span>
</span>
</td>
<td id="S5.T4.1.7.6.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.7.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.7.6.4.1.1" class="ltx_p">0.6573</span>
</span>
</td>
</tr>
<tr id="S5.T4.1.8.7" class="ltx_tr">
<th id="S5.T4.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">RelNet2</th>
<td id="S5.T4.1.8.7.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.8.7.2.1.1" class="ltx_p">0.8825</span>
</span>
</td>
<td id="S5.T4.1.8.7.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.8.7.3.1.1" class="ltx_p">-</span>
</span>
</td>
<td id="S5.T4.1.8.7.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.8.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.8.7.4.1.1" class="ltx_p">0.6606</span>
</span>
</td>
</tr>
<tr id="S5.T4.1.9.8" class="ltx_tr">
<th id="S5.T4.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">RelNet3</th>
<td id="S5.T4.1.9.8.2" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.9.8.2.1.1" class="ltx_p">0.8878</span>
</span>
</td>
<td id="S5.T4.1.9.8.3" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.9.8.3.1.1" class="ltx_p">0.7893</span>
</span>
</td>
<td id="S5.T4.1.9.8.4" class="ltx_td ltx_align_justify ltx_border_r">
<span id="S5.T4.1.9.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.9.8.4.1.1" class="ltx_p">0.6564</span>
</span>
</td>
</tr>
<tr id="S5.T4.1.10.9" class="ltx_tr">
<th id="S5.T4.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_l ltx_border_r">RelNet4</th>
<td id="S5.T4.1.10.9.2" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_r">
<span id="S5.T4.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.10.9.2.1.1" class="ltx_p">0.8848</span>
</span>
</td>
<td id="S5.T4.1.10.9.3" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_r">
<span id="S5.T4.1.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.10.9.3.1.1" class="ltx_p"><span id="S5.T4.1.10.9.3.1.1.1" class="ltx_text ltx_font_bold">0.7945</span></span>
</span>
</td>
<td id="S5.T4.1.10.9.4" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_r">
<span id="S5.T4.1.10.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T4.1.10.9.4.1.1" class="ltx_p">0.6623</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">All our models are trained end-to-end without relying on other tasks like image captioning and visual question answering. By using a larger dataset, we are able to obtain good performance on the QRPE dataset by directly modeling question relevance. Usage of pre-trained models like VQA and image captioning is limiting because of the need to have relevant datasets for those tasks which have similar images as the question relevance task. These models have inherent errors in their tasks and also induce additional errors for question relevance datasets which don’t share the same images. Hence, by generating a larger dataset, we were able to directly model the question relevance detection task reasonably well.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Analysis</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">For the first task of visual vs non-visual question detection, from table <a href="#S5.T3" title="Table 3 ‣ 5.1. Visual vs. Non-Visual Question Detection ‣ 5. Experiments and results ‣ Question Relevance in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> we can observe that logistic regression performs better than LSTM based approach on some metrics and performs comparatively on others. However, we provide a scalable streaming implementation of logistic regression, which takes significantly less time for training in comparison to LSTM. It can also be inferred from the high precision and recall that this is a much simpler problem compared to the second sub-problem. One of the possible reasons for this, we believe, is that the data for non-visual questions is not well represented (since most of them are general-knowledge or philosophical questions) and the complex models are undesirably learning the ad-hoc attributes of the two classes. A possible future work in this regard would be to collate a richer set of datasets for non-visual questions from multiple sources.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">For the true versus false premise detection, it can be observed from the results in table <a href="#S5.T4" title="Table 4 ‣ 5.2. True vs. False Premise Question Relevance Detection ‣ 5. Experiments and results ‣ Question Relevance in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> that our models perform quite well compared to the baselines on the extended first order and second order dataset. Among the LSTM architectures, RelNet1 and RelNet4 performed well on the test dataset. RelNet1 has the PCA dimensionality reduced information in it’s image features, thereby eliminating the need to learn a rich image representation from the training data. RelNet4 is a much simpler model in terms of number of parameters and network layers for the model to learn. Hence, we believe that these two networks provided good results on the test datasets.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">From a computational perspective, all the RelNets and the multi-layer perceptron model took 12-17 minutes per epoch and we trained each of the models for 20-30 epochs. Since the generated first-order dataset is large, we used a training data generator to generate images batch by batch. We use Keras to code all our models. For the XGBoost model, we use the open source disk-based implementation of XGBoost <cite class="ltx_cite ltx_citemacro_citep">(Chen and Guestrin, <a href="#bib.bib7" title="" class="ltx_ref">2016</a>)</cite>. The Logistic Regression model is a streaming implementation which avoids loading the entire training data into memory.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">In order to test the generalization of our models as well as the quality of our dataset, we have also tested our models on the QRPE dataset, and found that the performance is as good as some of the baselines (VQA-bin), but not as good as the best performing models (QPC-Sim). This can be attributed to two key reasons. The first one is that the models on QRPE dataset are a lot more complex, since they use pre-trained VQA and image captioning models. Secondly, the construction of the dataset may itself introduce a bias in the model. A possible way to circumvent this bias is to use a combination of differently constructed datasets (like QRPE and VTFQ) as validation sets and minimize the generalization loss while training. Lastly, from the consistent under-performance of our models on the QRPE dataset compared to our test dataset, we can also infer that the classification task on QRPE dataset is more challenging, since we have loosen the constraints for filtering question types while constructing the extended dataset.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion And Future Work</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this project, we attempt the problem of identifying relevance of posed questions to visual question answering systems by exploring several approaches that yield similar or better results (by virtue of larger training data). For the first sub-problem of identifying visual versus non-visual question detection, we provide a time-efficient and scalable implementation of logistic regression. This approach provides comparative or better results on all metrics in comparison to strong baselines set by LSTM based approaches. To solve the second sub-problem of true versus false premise using end-to-end classifiers, we propose an extended QRPE dataset using a modified QRPE data-generation pipeline and VQA 2.0, consisting of over 3.6M image-question pairs. We then experiment with multiple families of models for classifying true versus false premise such as XGBoost, Logistic Regression and LSTM-based RNN models. Each of these approaches have been trained end-to-end on the generated first order dataset, thereby avoiding using pre-trained VQA and image captioning models. While the models performed better than the baselines on our dataset, it did not outperform the state-of-the-art model (QPC-Sim) on the QRPE dataset, which we believe is because our models have much simpler architectures compared to the pre-trained models that QPC-Sim uses.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">There are many directions for future work in this area. For the dataset of true versus false premise detection, third order false premises (which include the relationships between objects in the image) could also be included. Given the larger data, many deep generative models can be trained to outperform the pre-trained models. As for features, we have considered only CNN features for images and word embeddings for words, but many different possibilities of imaged and words can be explored, with an option of training the language model and CNN specifically for this task. A natural extension to the problem of question relevance using premises is to explain why the question is not relevant, i.e. which premises are false and what additional information is required to answer the question.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">With greater capabilities in identifying and commenting on the relevance of the questions for images, visual question answering would have a much larger applicability in a practical setting.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">git (2017)</span>
<span class="ltx_bibblock">
2017.

</span>
<span class="ltx_bibblock">Code for Question Relevance in VQA.

</span>
<span class="ltx_bibblock"><a href="www.github.com/nitish-kulkarni/Question-Relevance-in-VQA" title="" class="ltx_ref ltx_url ltx_font_typewriter">www.github.com/nitish-kulkarni/Question-Relevance-in-VQA</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akata
et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Zeynep Akata, Mateusz
Malinowski, Mario Fritz, and Bernt
Schiele. 2016.

</span>
<span class="ltx_bibblock">Multi-Cue Zero-Shot Learning with Strong
Supervision.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic">CoRR</span> abs/1603.08754
(2016).

</span>
<span class="ltx_bibblock">arXiv:1603.08754
<a target="_blank" href="http://arxiv.org/abs/1603.08754" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1603.08754</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Peter Anderson, Basura
Fernando, Mark Johnson, and Stephen
Gould. 2016.

</span>
<span class="ltx_bibblock">SPICE: Semantic Propositional Image Caption
Evaluation.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic">CoRR</span> abs/1607.08822
(2016).

</span>
<span class="ltx_bibblock">arXiv:1607.08822
<a target="_blank" href="http://arxiv.org/abs/1607.08822" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1607.08822</a>

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya
Agrawal, Jiasen Lu, Margaret Mitchell,
Dhruv Batra, C. Lawrence Zitnick, and
Devi Parikh. 2015.

</span>
<span class="ltx_bibblock">VQA: Visual Question Answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic">CoRR</span> abs/1505.00468
(2015).

</span>
<span class="ltx_bibblock">arXiv:1505.00468
<a target="_blank" href="http://arxiv.org/abs/1505.00468" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1505.00468</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bojanowski et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Piotr Bojanowski, Edouard
Grave, Armand Joulin, and Tomas
Mikolov. 2016.

</span>
<span class="ltx_bibblock">Enriching Word Vectors with Subword Information.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1607.04606</span>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Guestrin (2016)</span>
<span class="ltx_bibblock">
Tianqi Chen and Carlos
Guestrin. 2016.

</span>
<span class="ltx_bibblock">Xgboost: A scalable tree boosting system. In
<span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proceedings of the 22nd acm sigkdd international
conference on knowledge discovery and data mining</span>. ACM,
785–794.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dodge
et al<span id="bib.bib8.3.3.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Jesse Dodge, Amit Goyal,
Xufeng Han, Alyssa Mensch,
Margaret Mitchell, Karl Stratos,
Kota Yamaguchi, Yejin Choi,
Hal Daumé III, Alexander C Berg,
et al<span id="bib.bib8.4.1" class="ltx_text">.</span> 2012.

</span>
<span class="ltx_bibblock">Detecting visual text. In <span id="bib.bib8.5.1" class="ltx_text ltx_font_italic">Proceedings of the 2012 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies</span>.
Association for Computational Linguistics, 762–772.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang
et al<span id="bib.bib9.3.3.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Hao Fang, Saurabh Gupta,
Forrest Iandola, Rupesh K Srivastava,
Li Deng, Piotr Dollár,
Jianfeng Gao, Xiaodong He,
Margaret Mitchell, John C Platt,
et al<span id="bib.bib9.4.1" class="ltx_text">.</span> 2015.

</span>
<span class="ltx_bibblock">From captions to visual concepts and back. In
<span id="bib.bib9.5.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer
vision and pattern recognition</span>. 1473–1482.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot,
Douglas Summers-Stay, Dhruv Batra, and
Devi Parikh. 2016.

</span>
<span class="ltx_bibblock">Making the V in VQA matter: Elevating the role of
image understanding in Visual Question Answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1612.00837</span>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honnibal and
Johnson (2015)</span>
<span class="ltx_bibblock">
Matthew Honnibal and
Mark Johnson. 2015.

</span>
<span class="ltx_bibblock">An Improved Non-monotonic Transition System for
Dependency Parsing. In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">EMNLP</span>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpathy and
Fei-Fei (2015)</span>
<span class="ltx_bibblock">
Andrej Karpathy and Li
Fei-Fei. 2015.

</span>
<span class="ltx_bibblock">Deep visual-semantic alignments for generating
image descriptions. In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition</span>.
3128–3137.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu
et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2009)</span>
<span class="ltx_bibblock">
Dong Liu, Xian-Sheng Hua,
Meng Wang, and HongJiang Zhang.
2009.

</span>
<span class="ltx_bibblock">Boost search relevance for tag-based social image
retrieval. In <span id="bib.bib13.3.1" class="ltx_text ltx_font_italic">Multimedia and Expo, 2009. ICME 2009.
IEEE International Conference on</span>. IEEE, 1636–1639.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahendru et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Aroma Mahendru, Viraj
Prabhu, Akrit Mohapatra, Dhruv Batra,
and Stefan Lee. 2017.

</span>
<span class="ltx_bibblock">The promise of premise: Harnessing question
premises in visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1705.00601</span>
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olah (2015)</span>
<span class="ltx_bibblock">
Christopher Olah.
2015.

</span>
<span class="ltx_bibblock">Image source for LSTMs.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington
et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Jeffrey Pennington,
Richard Socher, and Christopher
Manning. 2014.

</span>
<span class="ltx_bibblock">Glove: Global vectors for word representation. In
<span id="bib.bib16.3.1" class="ltx_text ltx_font_italic">Proceedings of the 2014 conference on empirical
methods in natural language processing (EMNLP)</span>.
1532–1543.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ray
et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Arijit Ray, Gordon
Christie, Mohit Bansal, Dhruv Batra,
and Devi Parikh. 2016.

</span>
<span class="ltx_bibblock">Question relevance in VQA: identifying non-visual
and false-premise questions.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.06622</span>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and
Zisserman (2014a)</span>
<span class="ltx_bibblock">
Karen Simonyan and
Andrew Zisserman. 2014a.

</span>
<span class="ltx_bibblock">Very Deep Convolutional Networks for Large-Scale
Image Recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">CoRR</span> abs/1409.1556
(2014).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and
Zisserman (2014b)</span>
<span class="ltx_bibblock">
Karen Simonyan and
Andrew Zisserman. 2014b.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale
image recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1409.1556</span>
(2014).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"></p>
</div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1807.08434" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1807.08435" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1807.08435">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1807.08435" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1807.08437" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  9 05:46:24 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
