<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1908.03289] Question-Agnostic Attention for Visual Question Answering</title><meta property="og:description" content="Visual Question Answering (VQA) models employ attention mechanisms to discover image locations that are most relevant for answering a specific question.
For this purpose, several multimodal fusion strategies have been …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Question-Agnostic Attention for Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Question-Agnostic Attention for Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1908.03289">

<!--Generated on Thu Mar  7 23:44:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Visual Question Answering,  Visual Attention,  Object Map,  Multimodal Fusion
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Question-Agnostic Attention for Visual Question Answering
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Moshiur R Farazi

<br class="ltx_break">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">Australian National University</span>
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_italic">and Data61, CSIRO
<br class="ltx_break"></span>Canberra, Australia 
<br class="ltx_break">moshiur.farazi@anu.edu.au
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Salman Khan

<br class="ltx_break">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_font_italic">Inception Institute of AI</span>
<br class="ltx_break"><span id="id4.2.id2" class="ltx_text ltx_font_italic">and Australian National University
<br class="ltx_break"></span>Abu Dhabi, UAE 
<br class="ltx_break">salman.khan@anu.edu.au
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nick Barnes
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_font_italic">Australian National University</span>
<br class="ltx_break">Canberra, Australia 
<br class="ltx_break">nick.barnes@anu.edu,au
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id6.id1" class="ltx_p">Visual Question Answering (VQA) models employ attention mechanisms to discover image locations that are most relevant for answering a specific question.
For this purpose, several multimodal fusion strategies have been proposed, ranging from relatively simple operations (<em id="id6.id1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="id6.id1.2" class="ltx_text"></span> linear sum) to more complex ones (<em id="id6.id1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="id6.id1.4" class="ltx_text"></span> Block <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>).
The resulting multimodal representations define an intermediate feature space for capturing the interplay between visual and semantic features, that is helpful in selectively focusing on image content.
In this paper, we propose a question-agnostic attention mechanism that is complementary to the existing question-dependent attention mechanisms.
Our proposed model parses object instances to obtain an ‘object map’ and applies this map on the visual features to generate Question-Agnostic Attention (QAA) features.
In contrast to question-dependent attention approaches that are learned end-to-end, the proposed QAA does not involve question-specific training, and can be easily included in almost any existing VQA model as a generic light-weight pre-processing step, thereby adding minimal computation overhead for training.
Further, when used in complement with the question-dependent attention, the QAA allows the model to focus on the regions containing objects that might have been overlooked by the learned attention representation.
Through extensive evaluation on VQAv1, VQAv2 and TDIUC datasets, we show that incorporating complementary QAA allows state-of-the-art VQA models to perform better, and provides significant boost to simplistic VQA models, enabling them to performance on par with highly sophisticated fusion strategies.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Visual Question Answering, Visual Attention, Object Map, Multimodal Fusion

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">An attention mechanism in a VQA system identifies the relevant visual information to intelligently answer a given question. Therefore, attention is central to recent state-of-the-art VQA models. Existing VQA models generally use grid-level or object-level convolutional features to <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">learn</span> an attention distribution over the given image. In the former case, this attention is dispersed over the spatial grid <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> while in the later case, attention is applied on a set of object proposals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Recent best performing methods combine the strengths of both these approaches to obtain attention maps with a better context <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/1908.03289/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="346" height="296" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A comparison of various multimodal fusion schemes for VQA evaluated on VQAv2 validation dataset. In general, methods with low-parametric complexity (such as linear sum, concatenation followed by MLP) deliver low performance compared to more sophisticated ones (<em id="S1.F1.3.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.F1.4.2" class="ltx_text"></span> Mutan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, Block <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>). Using our proposed Question-Agnostic Attention, we observe a consistent boost for all fusion mechanisms. The improvement is especially more pronounced for simple models, bringing them on par with highly sophisticated methods.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Learned attention mechanisms have been shown to significantly enhance the performance of VQA systems. However, learning attention on dense grid- and object-level features is a computationally demanding task that results in increased model complexity. Furthermore, learned attention is tuned for a specific dataset and thereby fails to generalize well to novel scenarios. To address these problems, we undertake a tangential path and propose a Question-Agnostic Attention (QAA) approach that is independent of a given question. Our approach is based on the insight that questions generally relate to the state, number, type and actions of the ‘<span id="S1.p2.1.1" class="ltx_text ltx_font_italic">objects</span>’ present in an image and their ‘<span id="S1.p2.1.2" class="ltx_text ltx_font_italic">mutual relationships</span>’. Therefore, we propose to use an object parsing module to generate question-agnostic attention maps based only on the given images. This attention generation procedure acts as a simple pre-processing step that encodes salient instance-centric visual cues (<em id="S1.p2.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p2.1.4" class="ltx_text"></span> location, shape) and object-relationship information which in turn leads to a performance boost for all evaluated models and difficult question types (<em id="S1.p2.1.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p2.1.6" class="ltx_text"></span> <span id="S1.p2.1.7" class="ltx_text ltx_font_italic">‘What sports are they playing?’</span>, <span id="S1.p2.1.8" class="ltx_text ltx_font_italic">‘What kind of furniture is in the picture?’</span>).</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Furthermore, several efforts in VQA literature show the importance of object-aware visual attention for improved VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> which emphasizes the notion that better localization of object instances results in higher VQA accuracy. However, these attention procedures are learned on top of object proposals while we propose an attention approach with minimal training cost. Our approach uses instance segmentation to generate an <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">object map</span> on the spatial image grid in a bottom-up fashion that is demonstrated to improve performance for simple as well as complex VQA models.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our results provide an interesting perspective on VQA showing that question-agnostic attention can help achieve competitive VQA performance and provides complementary information for existing VQA models, that results in notable performance gain. In an extreme case, when we apply a fixed attention map computed from a prior based on the training data, the VQA model still performs on par with existing models with learned attention. <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">Firstly,</span> this highlights the performance-complexity trade-off that is offered by recent multimodal fusion mechanisms for VQA task.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our results show that even with very simple multimodal operations, a VQA model can perform as well as more sophisticated models if question-agnostic attention is used. <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">Secondly,</span> the performance improvement across all the models illustrates the complementary nature of QAA, that highlights the room for improvement in learned ‘question-aware’ attention. <span id="S1.p5.1.2" class="ltx_text ltx_font_italic">Finally,</span> the relatively stronger improvement for simpler models shows that the information learned with QAA features is somewhat similar to the high-order representation modelling through complex multimodal fusion techniques.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The main contributions of our paper include:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">An inexpensive VQA pre-possessing step, dubbed Question-Agnostic Attention (QAA), that localizes object instances in an image irrespective of the question.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">A modular co-attention architecture that allows any off-the-shelf VQA model to incorporate complementary QAA features.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">An extensive set of experiments on large scale VQA datasets and the TDIUC dataset to showcase the effectiveness of using complementary QAA features, especially helping simplistic VQA models achieve near state-of-the art performance.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Works</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Visual feature extraction:</span> State-of-the-art VQA models either use deep CNN based feature extraction networks (<em id="S2.p1.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p1.1.3" class="ltx_text"></span> ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>) to generate visual features for each grid location in an image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, or employ object detectors (<em id="S2.p1.1.4" class="ltx_emph ltx_font_italic">e.g</em>. Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>) to detect the best object proposals for a given image and extract a corresponding set of object-level features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Attention mechanisms are then applied to selectively consider the relevant information on the spatial grid or the object proposals. Some recent approaches also combine spatial grid and object-level attention to leverage the best of both approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. However, the feature maps generated from the object proposals are discrete and do not encode the spatial relationship between the objects present in the image. Thus there exists a semantic gap since the two sets of approaches look at different kinds of features, one from image level and one from object level. Here, we propose to use the object location information on top of spatial-grid features to bridge this semantic gap. 
<br class="ltx_break"></p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Attention mechanisms:</span> For an improved VQA capability, attention has been focused on either or both the image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and the natural language questions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Stacked attention networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> generate an attention map by recursively attending to salient image details. VQA-HAT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> studies human attention maps for VQA and quantifies how they correlate with automatically learned attention maps. Task-independent saliency prediction methods have also been used as an attention mechanism for VQA. There exists a strong center-bias in human eye-fixation and saliency based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Our study also confirms this behaviour as we show that the global representation of <span id="S2.p2.1.2" class="ltx_text ltx_font_italic">object map</span> is more concentrated towards the image-center.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/1908.03289/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="437" height="138" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Architecture of our Question-Agnostic Attention (QAA) based VQA model. QAA features are generated using instance segmentation (generated by Mask-RCNN) to create a binary <span id="S2.F2.3.1" class="ltx_text ltx_font_italic">object map</span> with the same resolution of the convolutional feature map. The <span id="S2.F2.4.2" class="ltx_text ltx_font_italic">object map</span> is applied as a mask on the convolutional feature map (generated by ResNet) of the whole image. This ‘modular attention’ with minimal training cost delivers strong improvement while used in complement with existing VQA models on a number of VQA benchmarks.</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Method</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.3" class="ltx_p">Given a question <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">Q</annotation></semantics></math> about an image <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">I</annotation></semantics></math>, an AI agent designed for the VQA task will predict an answer <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="a^{*}" display="inline"><semantics id="S3.p1.3.m3.1a"><msup id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml"><mi id="S3.p1.3.m3.1.1.2" xref="S3.p1.3.m3.1.1.2.cmml">a</mi><mo id="S3.p1.3.m3.1.1.3" xref="S3.p1.3.m3.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><apply id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.1.1.1.cmml" xref="S3.p1.3.m3.1.1">superscript</csymbol><ci id="S3.p1.3.m3.1.1.2.cmml" xref="S3.p1.3.m3.1.1.2">𝑎</ci><times id="S3.p1.3.m3.1.1.3.cmml" xref="S3.p1.3.m3.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">a^{*}</annotation></semantics></math> based on the learning acquired from training examples. Benchmark VQA models frame this task as a multi-class classification problem in the candidate answer space, and the models learn to predict the correct answer for a given Image-Question (IQ) pair. This task can be formulated as:</p>
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.4" class="ltx_Math" alttext="\displaystyle a^{*}=\underset{\hat{a}\in\mathcal{D}}{\arg\max}\,P(\hat{a}|Q,I;\theta)," display="inline"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4.1" xref="S3.E1.m1.4.4.1.1.cmml"><mrow id="S3.E1.m1.4.4.1.1" xref="S3.E1.m1.4.4.1.1.cmml"><msup id="S3.E1.m1.4.4.1.1.3" xref="S3.E1.m1.4.4.1.1.3.cmml"><mi id="S3.E1.m1.4.4.1.1.3.2" xref="S3.E1.m1.4.4.1.1.3.2.cmml">a</mi><mo id="S3.E1.m1.4.4.1.1.3.3" xref="S3.E1.m1.4.4.1.1.3.3.cmml">∗</mo></msup><mo id="S3.E1.m1.4.4.1.1.2" xref="S3.E1.m1.4.4.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.4.4.1.1.1" xref="S3.E1.m1.4.4.1.1.1.cmml"><munder accentunder="true" id="S3.E1.m1.4.4.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.3.cmml"><mrow id="S3.E1.m1.4.4.1.1.1.3.2" xref="S3.E1.m1.4.4.1.1.1.3.2.cmml"><mi id="S3.E1.m1.4.4.1.1.1.3.2.1" xref="S3.E1.m1.4.4.1.1.1.3.2.1.cmml">arg</mi><mo lspace="0.167em" id="S3.E1.m1.4.4.1.1.1.3.2a" xref="S3.E1.m1.4.4.1.1.1.3.2.cmml">⁡</mo><mi id="S3.E1.m1.4.4.1.1.1.3.2.2" xref="S3.E1.m1.4.4.1.1.1.3.2.2.cmml">max</mi></mrow><mrow id="S3.E1.m1.4.4.1.1.1.3.1" xref="S3.E1.m1.4.4.1.1.1.3.1.cmml"><mover accent="true" id="S3.E1.m1.4.4.1.1.1.3.1.2" xref="S3.E1.m1.4.4.1.1.1.3.1.2.cmml"><mi id="S3.E1.m1.4.4.1.1.1.3.1.2.2" xref="S3.E1.m1.4.4.1.1.1.3.1.2.2.cmml">a</mi><mo id="S3.E1.m1.4.4.1.1.1.3.1.2.1" xref="S3.E1.m1.4.4.1.1.1.3.1.2.1.cmml">^</mo></mover><mo id="S3.E1.m1.4.4.1.1.1.3.1.1" xref="S3.E1.m1.4.4.1.1.1.3.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.4.4.1.1.1.3.1.3" xref="S3.E1.m1.4.4.1.1.1.3.1.3.cmml">𝒟</mi></mrow></munder><mo lspace="0.167em" rspace="0em" id="S3.E1.m1.4.4.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.4.4.1.1.1.4" xref="S3.E1.m1.4.4.1.1.1.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.1.1.2a" xref="S3.E1.m1.4.4.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E1.m1.4.4.1.1.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.1.1.2.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.2.2.cmml">a</mi><mo id="S3.E1.m1.4.4.1.1.1.1.1.1.2.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mo fence="false" id="S3.E1.m1.4.4.1.1.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.cmml">|</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1.1.3.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.1.cmml"><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">Q</mi><mo id="S3.E1.m1.4.4.1.1.1.1.1.1.3.2.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">I</mi><mo id="S3.E1.m1.4.4.1.1.1.1.1.1.3.2.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.1.cmml">;</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">θ</mi></mrow></mrow><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.4.4.1.2" xref="S3.E1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.1.1.cmml" xref="S3.E1.m1.4.4.1"><eq id="S3.E1.m1.4.4.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.2"></eq><apply id="S3.E1.m1.4.4.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.3.1.cmml" xref="S3.E1.m1.4.4.1.1.3">superscript</csymbol><ci id="S3.E1.m1.4.4.1.1.3.2.cmml" xref="S3.E1.m1.4.4.1.1.3.2">𝑎</ci><times id="S3.E1.m1.4.4.1.1.3.3.cmml" xref="S3.E1.m1.4.4.1.1.3.3"></times></apply><apply id="S3.E1.m1.4.4.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1"><times id="S3.E1.m1.4.4.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.2"></times><apply id="S3.E1.m1.4.4.1.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.3"><apply id="S3.E1.m1.4.4.1.1.1.3.1.cmml" xref="S3.E1.m1.4.4.1.1.1.3.1"><in id="S3.E1.m1.4.4.1.1.1.3.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.3.1.1"></in><apply id="S3.E1.m1.4.4.1.1.1.3.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.3.1.2"><ci id="S3.E1.m1.4.4.1.1.1.3.1.2.1.cmml" xref="S3.E1.m1.4.4.1.1.1.3.1.2.1">^</ci><ci id="S3.E1.m1.4.4.1.1.1.3.1.2.2.cmml" xref="S3.E1.m1.4.4.1.1.1.3.1.2.2">𝑎</ci></apply><ci id="S3.E1.m1.4.4.1.1.1.3.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.3.1.3">𝒟</ci></apply><apply id="S3.E1.m1.4.4.1.1.1.3.2.cmml" xref="S3.E1.m1.4.4.1.1.1.3.2"><arg id="S3.E1.m1.4.4.1.1.1.3.2.1.cmml" xref="S3.E1.m1.4.4.1.1.1.3.2.1"></arg><max id="S3.E1.m1.4.4.1.1.1.3.2.2.cmml" xref="S3.E1.m1.4.4.1.1.1.3.2.2"></max></apply></apply><ci id="S3.E1.m1.4.4.1.1.1.4.cmml" xref="S3.E1.m1.4.4.1.1.1.4">𝑃</ci><apply id="S3.E1.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.E1.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.2"><ci id="S3.E1.m1.4.4.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.2.1">^</ci><ci id="S3.E1.m1.4.4.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.2.2">𝑎</ci></apply><list id="S3.E1.m1.4.4.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.3.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑄</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝐼</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝜃</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">\displaystyle a^{*}=\underset{\hat{a}\in\mathcal{D}}{\arg\max}\,P(\hat{a}|Q,I;\theta),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.p1.6" class="ltx_p">where <math id="S3.p1.4.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.p1.4.m1.1a"><mi id="S3.p1.4.m1.1.1" xref="S3.p1.4.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.4.m1.1b"><ci id="S3.p1.4.m1.1.1.cmml" xref="S3.p1.4.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m1.1c">\theta</annotation></semantics></math> denotes the model parameters and <math id="S3.p1.5.m2.1" class="ltx_Math" alttext="a^{*}" display="inline"><semantics id="S3.p1.5.m2.1a"><msup id="S3.p1.5.m2.1.1" xref="S3.p1.5.m2.1.1.cmml"><mi id="S3.p1.5.m2.1.1.2" xref="S3.p1.5.m2.1.1.2.cmml">a</mi><mo id="S3.p1.5.m2.1.1.3" xref="S3.p1.5.m2.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p1.5.m2.1b"><apply id="S3.p1.5.m2.1.1.cmml" xref="S3.p1.5.m2.1.1"><csymbol cd="ambiguous" id="S3.p1.5.m2.1.1.1.cmml" xref="S3.p1.5.m2.1.1">superscript</csymbol><ci id="S3.p1.5.m2.1.1.2.cmml" xref="S3.p1.5.m2.1.1.2">𝑎</ci><times id="S3.p1.5.m2.1.1.3.cmml" xref="S3.p1.5.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m2.1c">a^{*}</annotation></semantics></math> is predicted from the dictionary of candidate answers <math id="S3.p1.6.m3.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S3.p1.6.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p1.6.m3.1.1" xref="S3.p1.6.m3.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S3.p1.6.m3.1b"><ci id="S3.p1.6.m3.1.1.cmml" xref="S3.p1.6.m3.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m3.1c">\mathcal{D}</annotation></semantics></math>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.6" class="ltx_p">A simplistic VQA model consists of two major parts: (1) Feature extraction module, and (2) Multimodal feature embedding. The <span id="S3.p2.6.1" class="ltx_text ltx_font_bold">first</span> part of the model extracts visual features from an Image <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">I</annotation></semantics></math> and semantic features from a Question <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.p2.2.m2.1a"><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">Q</annotation></semantics></math>. The visual features from an image are extracted using deep CNN based object recognition models (<em id="S3.p2.6.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.p2.6.3" class="ltx_text"></span> ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>) which are pretrained on large-scale image recognition datasets such as ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. The image feature map from the last convolution layer of the model is extracted as the visual feature <math id="S3.p2.3.m3.1" class="ltx_Math" alttext="\bm{v}\in\mathbb{R}^{g\times d_{v}}" display="inline"><semantics id="S3.p2.3.m3.1a"><mrow id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml"><mi id="S3.p2.3.m3.1.1.2" xref="S3.p2.3.m3.1.1.2.cmml">𝒗</mi><mo id="S3.p2.3.m3.1.1.1" xref="S3.p2.3.m3.1.1.1.cmml">∈</mo><msup id="S3.p2.3.m3.1.1.3" xref="S3.p2.3.m3.1.1.3.cmml"><mi id="S3.p2.3.m3.1.1.3.2" xref="S3.p2.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.p2.3.m3.1.1.3.3" xref="S3.p2.3.m3.1.1.3.3.cmml"><mi id="S3.p2.3.m3.1.1.3.3.2" xref="S3.p2.3.m3.1.1.3.3.2.cmml">g</mi><mo lspace="0.222em" rspace="0.222em" id="S3.p2.3.m3.1.1.3.3.1" xref="S3.p2.3.m3.1.1.3.3.1.cmml">×</mo><msub id="S3.p2.3.m3.1.1.3.3.3" xref="S3.p2.3.m3.1.1.3.3.3.cmml"><mi id="S3.p2.3.m3.1.1.3.3.3.2" xref="S3.p2.3.m3.1.1.3.3.3.2.cmml">d</mi><mi id="S3.p2.3.m3.1.1.3.3.3.3" xref="S3.p2.3.m3.1.1.3.3.3.3.cmml">v</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><apply id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1"><in id="S3.p2.3.m3.1.1.1.cmml" xref="S3.p2.3.m3.1.1.1"></in><ci id="S3.p2.3.m3.1.1.2.cmml" xref="S3.p2.3.m3.1.1.2">𝒗</ci><apply id="S3.p2.3.m3.1.1.3.cmml" xref="S3.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.p2.3.m3.1.1.3.1.cmml" xref="S3.p2.3.m3.1.1.3">superscript</csymbol><ci id="S3.p2.3.m3.1.1.3.2.cmml" xref="S3.p2.3.m3.1.1.3.2">ℝ</ci><apply id="S3.p2.3.m3.1.1.3.3.cmml" xref="S3.p2.3.m3.1.1.3.3"><times id="S3.p2.3.m3.1.1.3.3.1.cmml" xref="S3.p2.3.m3.1.1.3.3.1"></times><ci id="S3.p2.3.m3.1.1.3.3.2.cmml" xref="S3.p2.3.m3.1.1.3.3.2">𝑔</ci><apply id="S3.p2.3.m3.1.1.3.3.3.cmml" xref="S3.p2.3.m3.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.p2.3.m3.1.1.3.3.3.1.cmml" xref="S3.p2.3.m3.1.1.3.3.3">subscript</csymbol><ci id="S3.p2.3.m3.1.1.3.3.3.2.cmml" xref="S3.p2.3.m3.1.1.3.3.3.2">𝑑</ci><ci id="S3.p2.3.m3.1.1.3.3.3.3.cmml" xref="S3.p2.3.m3.1.1.3.3.3.3">𝑣</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">\bm{v}\in\mathbb{R}^{g\times d_{v}}</annotation></semantics></math>, where <math id="S3.p2.4.m4.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S3.p2.4.m4.1a"><mi id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><ci id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">g</annotation></semantics></math> is the index of the spatial location in the image over a coarse grid and <math id="S3.p2.5.m5.1" class="ltx_Math" alttext="d_{v}" display="inline"><semantics id="S3.p2.5.m5.1a"><msub id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml"><mi id="S3.p2.5.m5.1.1.2" xref="S3.p2.5.m5.1.1.2.cmml">d</mi><mi id="S3.p2.5.m5.1.1.3" xref="S3.p2.5.m5.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><apply id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p2.5.m5.1.1.1.cmml" xref="S3.p2.5.m5.1.1">subscript</csymbol><ci id="S3.p2.5.m5.1.1.2.cmml" xref="S3.p2.5.m5.1.1.2">𝑑</ci><ci id="S3.p2.5.m5.1.1.3.cmml" xref="S3.p2.5.m5.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">d_{v}</annotation></semantics></math> is the feature embedding dimension for each spatial grid location. On the other hand, for extracting the language feature from a Question, each word is fed to a pretrained encoder (<em id="S3.p2.6.4" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.p2.6.5" class="ltx_text"></span> GloVe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, Skip-thought <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>) to get vector embeddings of the question words. These vectors are then passed through a language model which consists of Gated Recurrent Units (GRUs) to generate a semantic feature <math id="S3.p2.6.m6.1" class="ltx_Math" alttext="\bm{q}\in\mathbb{R}^{d_{q}}" display="inline"><semantics id="S3.p2.6.m6.1a"><mrow id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml"><mi id="S3.p2.6.m6.1.1.2" xref="S3.p2.6.m6.1.1.2.cmml">𝒒</mi><mo id="S3.p2.6.m6.1.1.1" xref="S3.p2.6.m6.1.1.1.cmml">∈</mo><msup id="S3.p2.6.m6.1.1.3" xref="S3.p2.6.m6.1.1.3.cmml"><mi id="S3.p2.6.m6.1.1.3.2" xref="S3.p2.6.m6.1.1.3.2.cmml">ℝ</mi><msub id="S3.p2.6.m6.1.1.3.3" xref="S3.p2.6.m6.1.1.3.3.cmml"><mi id="S3.p2.6.m6.1.1.3.3.2" xref="S3.p2.6.m6.1.1.3.3.2.cmml">d</mi><mi id="S3.p2.6.m6.1.1.3.3.3" xref="S3.p2.6.m6.1.1.3.3.3.cmml">q</mi></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><apply id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1"><in id="S3.p2.6.m6.1.1.1.cmml" xref="S3.p2.6.m6.1.1.1"></in><ci id="S3.p2.6.m6.1.1.2.cmml" xref="S3.p2.6.m6.1.1.2">𝒒</ci><apply id="S3.p2.6.m6.1.1.3.cmml" xref="S3.p2.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.p2.6.m6.1.1.3.1.cmml" xref="S3.p2.6.m6.1.1.3">superscript</csymbol><ci id="S3.p2.6.m6.1.1.3.2.cmml" xref="S3.p2.6.m6.1.1.3.2">ℝ</ci><apply id="S3.p2.6.m6.1.1.3.3.cmml" xref="S3.p2.6.m6.1.1.3.3"><csymbol cd="ambiguous" id="S3.p2.6.m6.1.1.3.3.1.cmml" xref="S3.p2.6.m6.1.1.3.3">subscript</csymbol><ci id="S3.p2.6.m6.1.1.3.3.2.cmml" xref="S3.p2.6.m6.1.1.3.3.2">𝑑</ci><ci id="S3.p2.6.m6.1.1.3.3.3.cmml" xref="S3.p2.6.m6.1.1.3.3.3">𝑞</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">\bm{q}\in\mathbb{R}^{d_{q}}</annotation></semantics></math>.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.2" class="ltx_p">In the <span id="S3.p3.2.1" class="ltx_text ltx_font_bold">second</span> part, extracted visual and semantic features are combined into a multimodal representation, which in turn is used to minimize a loss function to predict the correct answer. A VQA model employs a joint embedding function <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="\Psi(\cdot)" display="inline"><semantics id="S3.p3.1.m1.1a"><mrow id="S3.p3.1.m1.1.2" xref="S3.p3.1.m1.1.2.cmml"><mi mathvariant="normal" id="S3.p3.1.m1.1.2.2" xref="S3.p3.1.m1.1.2.2.cmml">Ψ</mi><mo lspace="0em" rspace="0em" id="S3.p3.1.m1.1.2.1" xref="S3.p3.1.m1.1.2.1.cmml">​</mo><mrow id="S3.p3.1.m1.1.2.3.2" xref="S3.p3.1.m1.1.2.cmml"><mo stretchy="false" id="S3.p3.1.m1.1.2.3.2.1" xref="S3.p3.1.m1.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.p3.1.m1.1.2.3.2.2" xref="S3.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.2.cmml" xref="S3.p3.1.m1.1.2"><times id="S3.p3.1.m1.1.2.1.cmml" xref="S3.p3.1.m1.1.2.1"></times><ci id="S3.p3.1.m1.1.2.2.cmml" xref="S3.p3.1.m1.1.2.2">Ψ</ci><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">\Psi(\cdot)</annotation></semantics></math> to merge the extracted features in a common multimodal space. The function <math id="S3.p3.2.m2.1" class="ltx_Math" alttext="\Psi(\cdot)" display="inline"><semantics id="S3.p3.2.m2.1a"><mrow id="S3.p3.2.m2.1.2" xref="S3.p3.2.m2.1.2.cmml"><mi mathvariant="normal" id="S3.p3.2.m2.1.2.2" xref="S3.p3.2.m2.1.2.2.cmml">Ψ</mi><mo lspace="0em" rspace="0em" id="S3.p3.2.m2.1.2.1" xref="S3.p3.2.m2.1.2.1.cmml">​</mo><mrow id="S3.p3.2.m2.1.2.3.2" xref="S3.p3.2.m2.1.2.cmml"><mo stretchy="false" id="S3.p3.2.m2.1.2.3.2.1" xref="S3.p3.2.m2.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.p3.2.m2.1.2.3.2.2" xref="S3.p3.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><apply id="S3.p3.2.m2.1.2.cmml" xref="S3.p3.2.m2.1.2"><times id="S3.p3.2.m2.1.2.1.cmml" xref="S3.p3.2.m2.1.2.1"></times><ci id="S3.p3.2.m2.1.2.2.cmml" xref="S3.p3.2.m2.1.2.2">Ψ</ci><ci id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">\Psi(\cdot)</annotation></semantics></math> can be a simple fixed function (<em id="S3.p3.2.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.p3.2.3" class="ltx_text"></span> a linear sum, concatenation followed by MLP) or a complex operation (<em id="S3.p3.2.4" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.p3.2.5" class="ltx_text"></span> multimodal pooling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> or fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>). Most importantly, the multimodal embedding is used to selectively attend to visual features using a learned attention mechanism. This attention is derived jointly from the given question and image pair. Different from these attention approaches, we propose a pre-prcoessing step that estimates an attention map <span id="S3.p3.2.6" class="ltx_text ltx_font_italic">without</span> considering at-all the input questions. This simple approach with no-training cost surprisingly gives highly compelling results.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Our proposed question-agnostic attention model is illustrated in Fig. <a href="#S2.F2" title="Figure 2 ‣ II Related Works ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We first employ an attention mechanism that focuses on different object instances by creating an ‘<span id="S3.p4.1.1" class="ltx_text ltx_font_italic">object map</span>’ with which the question-agnostic features are generated (Sec. <a href="#S3.SS1" title="III-A Question-Agnostic Attention ‣ III Method ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>). The question-agnostic attention enables the model to focus on arbitrary object shapes and object parts which results in an improved model attention. Instead of the original CNN extracted spatial grid feature map, the question-agnostic features are passed through the VQA model where the given language query is used to further refine the visual features. These refined visual features are used to generate final predictions for classification. The modular architecture of our model enables it to combine predictions from other VQA models that aggregates multiple predictions to generate an intelligent answer for the given question (Sec. <a href="#S3.SS2" title="III-B Multiple Prediction Embedding ‣ III Method ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Question-Agnostic Attention</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.2" class="ltx_p">The input image is passed through a pre-trained instance segmentation module to predict the pixels that correspond to object instances. Notably, we ensure that the pre-trained network has not seen any of the test images for the evaluated datasets and is pre-trained on an altogether different task (<em id="S3.SS1.p1.2.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.p1.2.2" class="ltx_text"></span> instance segmentation as opposed to VQA). These instances have arbitrary shape and size which makes it harder to encode them and computationally infeasible for a VQA model to train with instance-level features. To remedy this, a coarse representation of the object instances is generated by creating a grid of size <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">g</annotation></semantics></math> over the whole image and the object instances are mapped onto this grid. A binary representation of this grid is called the <span id="S3.SS1.p1.2.3" class="ltx_text ltx_font_italic">object map</span> <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{M}\in\mathbb{R}^{g}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">ℳ</mi><mo id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml"><mi id="S3.SS1.p1.2.m2.1.1.3.2" xref="S3.SS1.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS1.p1.2.m2.1.1.3.3" xref="S3.SS1.p1.2.m2.1.1.3.3.cmml">g</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><in id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></in><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">ℳ</ci><apply id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS1.p1.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS1.p1.2.m2.1.1.3.2">ℝ</ci><ci id="S3.SS1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3.3">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\mathcal{M}\in\mathbb{R}^{g}</annotation></semantics></math>, which essentially identifies if an object instance occupies a grid location or not.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.5" class="ltx_p">One can learn a non-linear mapping function that maps the object instances to an arbitrary high-dimensional space. However, we adopt a simplistic approach to set the size of the <span id="S3.SS1.p2.5.1" class="ltx_text ltx_font_italic">object map</span> equal to spatial grid size <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">g</annotation></semantics></math> for primarily three reasons. <span id="S3.SS1.p2.5.2" class="ltx_text ltx_font_italic">Firstly</span>, having the grid size equal to the CNN features allows our approach to establish a one-to-one correspondence to the spatial grids of the CNN extracted convolutional feature map, which enables the model to access the visual features of that grid region without requiring another explicit ROI pooling like Faster-RCNN. This avoids expensive computations in our model. <span id="S3.SS1.p2.5.3" class="ltx_text ltx_font_italic">Secondly</span>, the binary <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">g</annotation></semantics></math>-dimensional <span id="S3.SS1.p2.5.4" class="ltx_text ltx_font_italic">object map</span> can be applied as a mask to select only the visual features at grid locations that have an object instance with a computationally inexpensive element-wise multiplication between <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="\bm{v}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">𝒗</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝒗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\bm{v}</annotation></semantics></math> and <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">\mathcal{M}</annotation></semantics></math> to generate question-agnostic feature <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="\bm{v}^{\mathcal{M}}\in\mathbb{R}^{g\times d_{v}}" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mrow id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><msup id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml"><mi id="S3.SS1.p2.5.m5.1.1.2.2" xref="S3.SS1.p2.5.m5.1.1.2.2.cmml">𝒗</mi><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.5.m5.1.1.2.3" xref="S3.SS1.p2.5.m5.1.1.2.3.cmml">ℳ</mi></msup><mo id="S3.SS1.p2.5.m5.1.1.1" xref="S3.SS1.p2.5.m5.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml"><mi id="S3.SS1.p2.5.m5.1.1.3.2" xref="S3.SS1.p2.5.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.5.m5.1.1.3.3" xref="S3.SS1.p2.5.m5.1.1.3.3.cmml"><mi id="S3.SS1.p2.5.m5.1.1.3.3.2" xref="S3.SS1.p2.5.m5.1.1.3.3.2.cmml">g</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.5.m5.1.1.3.3.1" xref="S3.SS1.p2.5.m5.1.1.3.3.1.cmml">×</mo><msub id="S3.SS1.p2.5.m5.1.1.3.3.3" xref="S3.SS1.p2.5.m5.1.1.3.3.3.cmml"><mi id="S3.SS1.p2.5.m5.1.1.3.3.3.2" xref="S3.SS1.p2.5.m5.1.1.3.3.3.2.cmml">d</mi><mi id="S3.SS1.p2.5.m5.1.1.3.3.3.3" xref="S3.SS1.p2.5.m5.1.1.3.3.3.3.cmml">v</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><in id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1.1"></in><apply id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.2.1.cmml" xref="S3.SS1.p2.5.m5.1.1.2">superscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.2.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2.2">𝒗</ci><ci id="S3.SS1.p2.5.m5.1.1.2.3.cmml" xref="S3.SS1.p2.5.m5.1.1.2.3">ℳ</ci></apply><apply id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.3.1.cmml" xref="S3.SS1.p2.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.3.2.cmml" xref="S3.SS1.p2.5.m5.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.5.m5.1.1.3.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3"><times id="S3.SS1.p2.5.m5.1.1.3.3.1.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3.1"></times><ci id="S3.SS1.p2.5.m5.1.1.3.3.2.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3.2">𝑔</ci><apply id="S3.SS1.p2.5.m5.1.1.3.3.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.3.3.3.1.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3.3">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.3.3.3.2.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3.3.2">𝑑</ci><ci id="S3.SS1.p2.5.m5.1.1.3.3.3.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3.3.3.3">𝑣</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">\bm{v}^{\mathcal{M}}\in\mathbb{R}^{g\times d_{v}}</annotation></semantics></math>. <span id="S3.SS1.p2.5.5" class="ltx_text ltx_font_italic">Finally</span>, as the question-agnostic features have the same size as CNN extracted visual features, it can be easy for any VQA model to incorporate the question-agnostic features by only adjusting the size of <span id="S3.SS1.p2.5.6" class="ltx_text ltx_font_italic">object map</span> equal to the size of CNN spatial grid. Thus, this simplistic approach fashions question-agnostic attention mechanism as an inexpensive pre-processing step that is easily applicable to any CNN based VQA model.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Multiple Prediction Embedding</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.4" class="ltx_p">The modular architecture of QAA enables it to jointly consider predictions from any other VQA model to generate a final prediction vector. In order to further validate the complementary nature of our proposed QAA model, we include a simple spatial attention mechanism, commonly used in most VQA models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> to refine the visual features according to the question. In addition to a fixed <span id="S3.SS2.p1.4.1" class="ltx_text ltx_font_italic">object map</span> used to generate question-agnostic features, this optional module can be used to refine the question-agnostic features according to the question, providing flexibility to incorporate a spatial attention mechanism on top of QAA. We achieve this by calculating a similarity measure between each question-agnostic feature grid location <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\bm{v}^{\mathcal{M}}_{i}\in\mathbb{R}^{d_{v}}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><msubsup id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2.2.2" xref="S3.SS2.p1.1.m1.1.1.2.2.2.cmml">𝒗</mi><mi id="S3.SS2.p1.1.m1.1.1.2.3" xref="S3.SS2.p1.1.m1.1.1.2.3.cmml">i</mi><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.1.m1.1.1.2.2.3" xref="S3.SS2.p1.1.m1.1.1.2.2.3.cmml">ℳ</mi></msubsup><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">ℝ</mi><msub id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.3.2" xref="S3.SS2.p1.1.m1.1.1.3.3.2.cmml">d</mi><mi id="S3.SS2.p1.1.m1.1.1.3.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.3.cmml">v</mi></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><in id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></in><apply id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.1.1.2">subscript</csymbol><apply id="S3.SS2.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.2.2.1.cmml" xref="S3.SS2.p1.1.m1.1.1.2">superscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.2.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2.2.2">𝒗</ci><ci id="S3.SS2.p1.1.m1.1.1.2.2.3.cmml" xref="S3.SS2.p1.1.m1.1.1.2.2.3">ℳ</ci></apply><ci id="S3.SS2.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.p1.1.m1.1.1.2.3">𝑖</ci></apply><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.2">𝑑</ci><ci id="S3.SS2.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.3">𝑣</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\bm{v}^{\mathcal{M}}_{i}\in\mathbb{R}^{d_{v}}</annotation></semantics></math> and <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\bm{q}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">𝒒</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝒒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\bm{q}</annotation></semantics></math> by projecting them in a common space by a joint embedding function <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="\Psi(\cdot)" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.2" xref="S3.SS2.p1.3.m3.1.2.cmml"><mi mathvariant="normal" id="S3.SS2.p1.3.m3.1.2.2" xref="S3.SS2.p1.3.m3.1.2.2.cmml">Ψ</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.2.1" xref="S3.SS2.p1.3.m3.1.2.1.cmml">​</mo><mrow id="S3.SS2.p1.3.m3.1.2.3.2" xref="S3.SS2.p1.3.m3.1.2.cmml"><mo stretchy="false" id="S3.SS2.p1.3.m3.1.2.3.2.1" xref="S3.SS2.p1.3.m3.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS2.p1.3.m3.1.2.3.2.2" xref="S3.SS2.p1.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.2.cmml" xref="S3.SS2.p1.3.m3.1.2"><times id="S3.SS2.p1.3.m3.1.2.1.cmml" xref="S3.SS2.p1.3.m3.1.2.1"></times><ci id="S3.SS2.p1.3.m3.1.2.2.cmml" xref="S3.SS2.p1.3.m3.1.2.2">Ψ</ci><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\Psi(\cdot)</annotation></semantics></math>. This, in general, represents the relevance of a spatial grid location for answering that input question. This similarity measure is applied as a semantic weighting function, called Spatial Attention <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="\alpha\in\mathbb{R}^{g}" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mrow id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">α</mi><mo id="S3.SS2.p1.4.m4.1.1.1" xref="S3.SS2.p1.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml"><mi id="S3.SS2.p1.4.m4.1.1.3.2" xref="S3.SS2.p1.4.m4.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS2.p1.4.m4.1.1.3.3" xref="S3.SS2.p1.4.m4.1.1.3.3.cmml">g</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><in id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1"></in><ci id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">𝛼</ci><apply id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.p1.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.p1.4.m4.1.1.3.2">ℝ</ci><ci id="S3.SS2.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3.3">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">\alpha\in\mathbb{R}^{g}</annotation></semantics></math>, that takes a weighted sum over the spatial grids of input visual features. It can be expressed as:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.3" class="ltx_Math" alttext="\tilde{\bm{v}}^{\mathcal{M}}=\sum_{i=1}^{g}\alpha_{i}\bm{v}^{\mathcal{M}}_{i}\quad\textrm{where}\;\;\alpha_{i}=\textup{softmax}(\Psi(\bm{q},\bm{v}_{i}))" display="block"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3.2" xref="S3.E2.m1.3.3.3.cmml"><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><msup id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml"><mover accent="true" id="S3.E2.m1.2.2.1.1.2.2" xref="S3.E2.m1.2.2.1.1.2.2.cmml"><mi id="S3.E2.m1.2.2.1.1.2.2.2" xref="S3.E2.m1.2.2.1.1.2.2.2.cmml">𝒗</mi><mo id="S3.E2.m1.2.2.1.1.2.2.1" xref="S3.E2.m1.2.2.1.1.2.2.1.cmml">~</mo></mover><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.2.1.1.2.3" xref="S3.E2.m1.2.2.1.1.2.3.cmml">ℳ</mi></msup><mo rspace="0.111em" id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3.cmml"><munderover id="S3.E2.m1.2.2.1.1.3.1" xref="S3.E2.m1.2.2.1.1.3.1.cmml"><mo movablelimits="false" id="S3.E2.m1.2.2.1.1.3.1.2.2" xref="S3.E2.m1.2.2.1.1.3.1.2.2.cmml">∑</mo><mrow id="S3.E2.m1.2.2.1.1.3.1.2.3" xref="S3.E2.m1.2.2.1.1.3.1.2.3.cmml"><mi id="S3.E2.m1.2.2.1.1.3.1.2.3.2" xref="S3.E2.m1.2.2.1.1.3.1.2.3.2.cmml">i</mi><mo id="S3.E2.m1.2.2.1.1.3.1.2.3.1" xref="S3.E2.m1.2.2.1.1.3.1.2.3.1.cmml">=</mo><mn id="S3.E2.m1.2.2.1.1.3.1.2.3.3" xref="S3.E2.m1.2.2.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E2.m1.2.2.1.1.3.1.3" xref="S3.E2.m1.2.2.1.1.3.1.3.cmml">g</mi></munderover><mrow id="S3.E2.m1.2.2.1.1.3.2" xref="S3.E2.m1.2.2.1.1.3.2.cmml"><msub id="S3.E2.m1.2.2.1.1.3.2.2" xref="S3.E2.m1.2.2.1.1.3.2.2.cmml"><mi id="S3.E2.m1.2.2.1.1.3.2.2.2" xref="S3.E2.m1.2.2.1.1.3.2.2.2.cmml">α</mi><mi id="S3.E2.m1.2.2.1.1.3.2.2.3" xref="S3.E2.m1.2.2.1.1.3.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.1.1.3.2.1" xref="S3.E2.m1.2.2.1.1.3.2.1.cmml">​</mo><msubsup id="S3.E2.m1.2.2.1.1.3.2.3" xref="S3.E2.m1.2.2.1.1.3.2.3.cmml"><mi id="S3.E2.m1.2.2.1.1.3.2.3.2.2" xref="S3.E2.m1.2.2.1.1.3.2.3.2.2.cmml">𝒗</mi><mi id="S3.E2.m1.2.2.1.1.3.2.3.3" xref="S3.E2.m1.2.2.1.1.3.2.3.3.cmml">i</mi><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.2.1.1.3.2.3.2.3" xref="S3.E2.m1.2.2.1.1.3.2.3.2.3.cmml">ℳ</mi></msubsup></mrow></mrow></mrow><mspace width="1em" id="S3.E2.m1.3.3.2.3" xref="S3.E2.m1.3.3.3a.cmml"></mspace><mrow id="S3.E2.m1.3.3.2.2" xref="S3.E2.m1.3.3.2.2.cmml"><mrow id="S3.E2.m1.3.3.2.2.3" xref="S3.E2.m1.3.3.2.2.3.cmml"><mtext id="S3.E2.m1.3.3.2.2.3.2" xref="S3.E2.m1.3.3.2.2.3.2a.cmml">where</mtext><mo lspace="0.560em" rspace="0em" id="S3.E2.m1.3.3.2.2.3.1" xref="S3.E2.m1.3.3.2.2.3.1.cmml">​</mo><msub id="S3.E2.m1.3.3.2.2.3.3" xref="S3.E2.m1.3.3.2.2.3.3.cmml"><mi id="S3.E2.m1.3.3.2.2.3.3.2" xref="S3.E2.m1.3.3.2.2.3.3.2.cmml">α</mi><mi id="S3.E2.m1.3.3.2.2.3.3.3" xref="S3.E2.m1.3.3.2.2.3.3.3.cmml">i</mi></msub></mrow><mo id="S3.E2.m1.3.3.2.2.2" xref="S3.E2.m1.3.3.2.2.2.cmml">=</mo><mrow id="S3.E2.m1.3.3.2.2.1" xref="S3.E2.m1.3.3.2.2.1.cmml"><mtext id="S3.E2.m1.3.3.2.2.1.3" xref="S3.E2.m1.3.3.2.2.1.3a.cmml">softmax</mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.2.2.1.2" xref="S3.E2.m1.3.3.2.2.1.2.cmml">​</mo><mrow id="S3.E2.m1.3.3.2.2.1.1.1" xref="S3.E2.m1.3.3.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.2.2.1.1.1.2" xref="S3.E2.m1.3.3.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.2.2.1.1.1.1" xref="S3.E2.m1.3.3.2.2.1.1.1.1.cmml"><mi mathvariant="normal" id="S3.E2.m1.3.3.2.2.1.1.1.1.3" xref="S3.E2.m1.3.3.2.2.1.1.1.1.3.cmml">Ψ</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.2.2.1.1.1.1.2" xref="S3.E2.m1.3.3.2.2.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.3.3.2.2.1.1.1.1.1.1" xref="S3.E2.m1.3.3.2.2.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.2.2.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.2.2.1.1.1.1.1.2.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">𝒒</mi><mo id="S3.E2.m1.3.3.2.2.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.2.2.1.1.1.1.1.2.cmml">,</mo><msub id="S3.E2.m1.3.3.2.2.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.2.2.1.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.2.2.1.1.1.1.1.1.1.2.cmml">𝒗</mi><mi id="S3.E2.m1.3.3.2.2.1.1.1.1.1.1.1.3" xref="S3.E2.m1.3.3.2.2.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E2.m1.3.3.2.2.1.1.1.1.1.1.4" xref="S3.E2.m1.3.3.2.2.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m1.3.3.2.2.1.1.1.3" xref="S3.E2.m1.3.3.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.3.cmml" xref="S3.E2.m1.3.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3a.cmml" xref="S3.E2.m1.3.3.2.3">formulae-sequence</csymbol><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1.1"><eq id="S3.E2.m1.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1"></eq><apply id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.2">superscript</csymbol><apply id="S3.E2.m1.2.2.1.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2"><ci id="S3.E2.m1.2.2.1.1.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.2.2.1">~</ci><ci id="S3.E2.m1.2.2.1.1.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.2.2.2">𝒗</ci></apply><ci id="S3.E2.m1.2.2.1.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.2.3">ℳ</ci></apply><apply id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3"><apply id="S3.E2.m1.2.2.1.1.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.1.1.cmml" xref="S3.E2.m1.2.2.1.1.3.1">superscript</csymbol><apply id="S3.E2.m1.2.2.1.1.3.1.2.cmml" xref="S3.E2.m1.2.2.1.1.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.1">subscript</csymbol><sum id="S3.E2.m1.2.2.1.1.3.1.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.1.2.2"></sum><apply id="S3.E2.m1.2.2.1.1.3.1.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.1.2.3"><eq id="S3.E2.m1.2.2.1.1.3.1.2.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.1.2.3.1"></eq><ci id="S3.E2.m1.2.2.1.1.3.1.2.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.1.2.3.2">𝑖</ci><cn type="integer" id="S3.E2.m1.2.2.1.1.3.1.2.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.2.2.1.1.3.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3.1.3">𝑔</ci></apply><apply id="S3.E2.m1.2.2.1.1.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2"><times id="S3.E2.m1.2.2.1.1.3.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.2.1"></times><apply id="S3.E2.m1.2.2.1.1.3.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.2.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.2.2">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.2.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2.2.2">𝛼</ci><ci id="S3.E2.m1.2.2.1.1.3.2.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.2.2.3">𝑖</ci></apply><apply id="S3.E2.m1.2.2.1.1.3.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.2.3.1.cmml" xref="S3.E2.m1.2.2.1.1.3.2.3">subscript</csymbol><apply id="S3.E2.m1.2.2.1.1.3.2.3.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.3.2.3.2.1.cmml" xref="S3.E2.m1.2.2.1.1.3.2.3">superscript</csymbol><ci id="S3.E2.m1.2.2.1.1.3.2.3.2.2.cmml" xref="S3.E2.m1.2.2.1.1.3.2.3.2.2">𝒗</ci><ci id="S3.E2.m1.2.2.1.1.3.2.3.2.3.cmml" xref="S3.E2.m1.2.2.1.1.3.2.3.2.3">ℳ</ci></apply><ci id="S3.E2.m1.2.2.1.1.3.2.3.3.cmml" xref="S3.E2.m1.2.2.1.1.3.2.3.3">𝑖</ci></apply></apply></apply></apply><apply id="S3.E2.m1.3.3.2.2.cmml" xref="S3.E2.m1.3.3.2.2"><eq id="S3.E2.m1.3.3.2.2.2.cmml" xref="S3.E2.m1.3.3.2.2.2"></eq><apply id="S3.E2.m1.3.3.2.2.3.cmml" xref="S3.E2.m1.3.3.2.2.3"><times id="S3.E2.m1.3.3.2.2.3.1.cmml" xref="S3.E2.m1.3.3.2.2.3.1"></times><ci id="S3.E2.m1.3.3.2.2.3.2a.cmml" xref="S3.E2.m1.3.3.2.2.3.2"><mtext id="S3.E2.m1.3.3.2.2.3.2.cmml" xref="S3.E2.m1.3.3.2.2.3.2">where</mtext></ci><apply id="S3.E2.m1.3.3.2.2.3.3.cmml" xref="S3.E2.m1.3.3.2.2.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.2.2.3.3.1.cmml" xref="S3.E2.m1.3.3.2.2.3.3">subscript</csymbol><ci id="S3.E2.m1.3.3.2.2.3.3.2.cmml" xref="S3.E2.m1.3.3.2.2.3.3.2">𝛼</ci><ci id="S3.E2.m1.3.3.2.2.3.3.3.cmml" xref="S3.E2.m1.3.3.2.2.3.3.3">𝑖</ci></apply></apply><apply id="S3.E2.m1.3.3.2.2.1.cmml" xref="S3.E2.m1.3.3.2.2.1"><times id="S3.E2.m1.3.3.2.2.1.2.cmml" xref="S3.E2.m1.3.3.2.2.1.2"></times><ci id="S3.E2.m1.3.3.2.2.1.3a.cmml" xref="S3.E2.m1.3.3.2.2.1.3"><mtext id="S3.E2.m1.3.3.2.2.1.3.cmml" xref="S3.E2.m1.3.3.2.2.1.3">softmax</mtext></ci><apply id="S3.E2.m1.3.3.2.2.1.1.1.1.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1"><times id="S3.E2.m1.3.3.2.2.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.1.2"></times><ci id="S3.E2.m1.3.3.2.2.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.1.3">Ψ</ci><interval closure="open" id="S3.E2.m1.3.3.2.2.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.1.1.1"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝒒</ci><apply id="S3.E2.m1.3.3.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.3.3.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.1.1.1.1.2">𝒗</ci><ci id="S3.E2.m1.3.3.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.2.2.1.1.1.1.1.1.1.3">𝑖</ci></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">\tilde{\bm{v}}^{\mathcal{M}}=\sum_{i=1}^{g}\alpha_{i}\bm{v}^{\mathcal{M}}_{i}\quad\textrm{where}\;\;\alpha_{i}=\textup{softmax}(\Psi(\bm{q},\bm{v}_{i}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.9" class="ltx_p">where <math id="S3.SS2.p1.5.m1.1" class="ltx_Math" alttext="\tilde{\bm{v}}^{\mathcal{M}}\in\mathbb{R}^{d_{v}}" display="inline"><semantics id="S3.SS2.p1.5.m1.1a"><mrow id="S3.SS2.p1.5.m1.1.1" xref="S3.SS2.p1.5.m1.1.1.cmml"><msup id="S3.SS2.p1.5.m1.1.1.2" xref="S3.SS2.p1.5.m1.1.1.2.cmml"><mover accent="true" id="S3.SS2.p1.5.m1.1.1.2.2" xref="S3.SS2.p1.5.m1.1.1.2.2.cmml"><mi id="S3.SS2.p1.5.m1.1.1.2.2.2" xref="S3.SS2.p1.5.m1.1.1.2.2.2.cmml">𝒗</mi><mo id="S3.SS2.p1.5.m1.1.1.2.2.1" xref="S3.SS2.p1.5.m1.1.1.2.2.1.cmml">~</mo></mover><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.5.m1.1.1.2.3" xref="S3.SS2.p1.5.m1.1.1.2.3.cmml">ℳ</mi></msup><mo id="S3.SS2.p1.5.m1.1.1.1" xref="S3.SS2.p1.5.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.p1.5.m1.1.1.3" xref="S3.SS2.p1.5.m1.1.1.3.cmml"><mi id="S3.SS2.p1.5.m1.1.1.3.2" xref="S3.SS2.p1.5.m1.1.1.3.2.cmml">ℝ</mi><msub id="S3.SS2.p1.5.m1.1.1.3.3" xref="S3.SS2.p1.5.m1.1.1.3.3.cmml"><mi id="S3.SS2.p1.5.m1.1.1.3.3.2" xref="S3.SS2.p1.5.m1.1.1.3.3.2.cmml">d</mi><mi id="S3.SS2.p1.5.m1.1.1.3.3.3" xref="S3.SS2.p1.5.m1.1.1.3.3.3.cmml">v</mi></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m1.1b"><apply id="S3.SS2.p1.5.m1.1.1.cmml" xref="S3.SS2.p1.5.m1.1.1"><in id="S3.SS2.p1.5.m1.1.1.1.cmml" xref="S3.SS2.p1.5.m1.1.1.1"></in><apply id="S3.SS2.p1.5.m1.1.1.2.cmml" xref="S3.SS2.p1.5.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m1.1.1.2.1.cmml" xref="S3.SS2.p1.5.m1.1.1.2">superscript</csymbol><apply id="S3.SS2.p1.5.m1.1.1.2.2.cmml" xref="S3.SS2.p1.5.m1.1.1.2.2"><ci id="S3.SS2.p1.5.m1.1.1.2.2.1.cmml" xref="S3.SS2.p1.5.m1.1.1.2.2.1">~</ci><ci id="S3.SS2.p1.5.m1.1.1.2.2.2.cmml" xref="S3.SS2.p1.5.m1.1.1.2.2.2">𝒗</ci></apply><ci id="S3.SS2.p1.5.m1.1.1.2.3.cmml" xref="S3.SS2.p1.5.m1.1.1.2.3">ℳ</ci></apply><apply id="S3.SS2.p1.5.m1.1.1.3.cmml" xref="S3.SS2.p1.5.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m1.1.1.3.1.cmml" xref="S3.SS2.p1.5.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.5.m1.1.1.3.2.cmml" xref="S3.SS2.p1.5.m1.1.1.3.2">ℝ</ci><apply id="S3.SS2.p1.5.m1.1.1.3.3.cmml" xref="S3.SS2.p1.5.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m1.1.1.3.3.1.cmml" xref="S3.SS2.p1.5.m1.1.1.3.3">subscript</csymbol><ci id="S3.SS2.p1.5.m1.1.1.3.3.2.cmml" xref="S3.SS2.p1.5.m1.1.1.3.3.2">𝑑</ci><ci id="S3.SS2.p1.5.m1.1.1.3.3.3.cmml" xref="S3.SS2.p1.5.m1.1.1.3.3.3">𝑣</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m1.1c">\tilde{\bm{v}}^{\mathcal{M}}\in\mathbb{R}^{d_{v}}</annotation></semantics></math> represents a combination of question-agnostic attention features that are emphasized by the input question. Finally, it undergoes a second multimodal embedding with the question feature <math id="S3.SS2.p1.6.m2.1" class="ltx_Math" alttext="\bm{q}" display="inline"><semantics id="S3.SS2.p1.6.m2.1a"><mi id="S3.SS2.p1.6.m2.1.1" xref="S3.SS2.p1.6.m2.1.1.cmml">𝒒</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m2.1b"><ci id="S3.SS2.p1.6.m2.1.1.cmml" xref="S3.SS2.p1.6.m2.1.1">𝒒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m2.1c">\bm{q}</annotation></semantics></math> to generate a prediction vector <math id="S3.SS2.p1.7.m3.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S3.SS2.p1.7.m3.1a"><mi id="S3.SS2.p1.7.m3.1.1" xref="S3.SS2.p1.7.m3.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m3.1b"><ci id="S3.SS2.p1.7.m3.1.1.cmml" xref="S3.SS2.p1.7.m3.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m3.1c">P</annotation></semantics></math> which has the same dimension as the candidate answer dictionary <math id="S3.SS2.p1.8.m4.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S3.SS2.p1.8.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.8.m4.1.1" xref="S3.SS2.p1.8.m4.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m4.1b"><ci id="S3.SS2.p1.8.m4.1.1.cmml" xref="S3.SS2.p1.8.m4.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m4.1c">\mathcal{D}</annotation></semantics></math>. Predictions from any other VQA model can be concatenated with the prediction of our model. The concatenated predictions are passed through a multiple prediction embedding layer that learns to generate a <math id="S3.SS2.p1.9.m5.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S3.SS2.p1.9.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.9.m5.1.1" xref="S3.SS2.p1.9.m5.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m5.1b"><ci id="S3.SS2.p1.9.m5.1.1.cmml" xref="S3.SS2.p1.9.m5.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m5.1c">\mathcal{D}</annotation></semantics></math> dimensional final prediction vector.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments and Results</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, at first. we describe the experimental setup that includes our instance segmentation pipeline, VQA model architecture, dataset and evaluation metric. Then we discuss the findings from our ablative experiments to study effectiveness of our proposed approach in different settings. Finally, we present the qualitative and quantitative results of our model and do a comparative analysis with other state-of-the-art models.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.3" class="ltx_p"><span id="S4.p2.3.1" class="ltx_text ltx_font_bold">VQA dataset:</span> Firstly, we evaluate our QAA model on two large scale benchmark VQA datasets, namely VQAv1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and VQAv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Among the two datasets, VQAv2 contains complementary question-answer pairs which mitigates language bias present in the VQAv1 dataset, making VQAv2 a more challenging test setting. Both versions of the VQA dataset contain over 200K real images sourced from the MS COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and placed into respective train/val/test splits. These images are paired with complex open-ended natural language questions and answers. The ground truth answers for train and val split are publicly available, but the test split is not. To evaluate on the test split (both test-dev and test-std), the prediction needs to be submitted to the VQA test server. We perform ablation experiments on validation sets of VQAv1 and VQAv2 (Tab. <a href="#S4.T1" title="TABLE I ‣ IV Experiments and Results ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>) and compare with other state-of-the-art methods on VQAv2 test-dev and test-std dataset (Tab. <a href="#S4.T2" title="TABLE II ‣ IV-A Ablation on Different Multimodal Operations ‣ IV Experiments and Results ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>). Following the standard evaluation strategy, we calculate the accuracy <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="\hat{a}" display="inline"><semantics id="S4.p2.1.m1.1a"><mover accent="true" id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mi id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">a</mi><mo id="S4.p2.1.m1.1.1.1" xref="S4.p2.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><ci id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1.1">^</ci><ci id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">\hat{a}</annotation></semantics></math> of the predicted answer <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="a^{*}" display="inline"><semantics id="S4.p2.2.m2.1a"><msup id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mi id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">a</mi><mo id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1">superscript</csymbol><ci id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">𝑎</ci><times id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">a^{*}</annotation></semantics></math> as <math id="S4.p2.3.m3.2" class="ltx_Math" alttext="\hat{a}=\mathrm{min}(\#\;\text{of humans answered }a^{*}/3,1)" display="inline"><semantics id="S4.p2.3.m3.2a"><mrow id="S4.p2.3.m3.2.2" xref="S4.p2.3.m3.2.2.cmml"><mover accent="true" id="S4.p2.3.m3.2.2.3" xref="S4.p2.3.m3.2.2.3.cmml"><mi id="S4.p2.3.m3.2.2.3.2" xref="S4.p2.3.m3.2.2.3.2.cmml">a</mi><mo id="S4.p2.3.m3.2.2.3.1" xref="S4.p2.3.m3.2.2.3.1.cmml">^</mo></mover><mo id="S4.p2.3.m3.2.2.2" xref="S4.p2.3.m3.2.2.2.cmml">=</mo><mrow id="S4.p2.3.m3.2.2.1" xref="S4.p2.3.m3.2.2.1.cmml"><mi id="S4.p2.3.m3.2.2.1.3" xref="S4.p2.3.m3.2.2.1.3.cmml">min</mi><mo lspace="0em" rspace="0em" id="S4.p2.3.m3.2.2.1.2" xref="S4.p2.3.m3.2.2.1.2.cmml">​</mo><mrow id="S4.p2.3.m3.2.2.1.1.1" xref="S4.p2.3.m3.2.2.1.1.2.cmml"><mo stretchy="false" id="S4.p2.3.m3.2.2.1.1.1.2" xref="S4.p2.3.m3.2.2.1.1.2.cmml">(</mo><mrow id="S4.p2.3.m3.2.2.1.1.1.1" xref="S4.p2.3.m3.2.2.1.1.1.1.cmml"><mrow id="S4.p2.3.m3.2.2.1.1.1.1.2" xref="S4.p2.3.m3.2.2.1.1.1.1.2.cmml"><mi mathvariant="normal" id="S4.p2.3.m3.2.2.1.1.1.1.2.2" xref="S4.p2.3.m3.2.2.1.1.1.1.2.2.cmml">#</mi><mo lspace="0.280em" rspace="0em" id="S4.p2.3.m3.2.2.1.1.1.1.2.1" xref="S4.p2.3.m3.2.2.1.1.1.1.2.1.cmml">​</mo><mtext id="S4.p2.3.m3.2.2.1.1.1.1.2.3" xref="S4.p2.3.m3.2.2.1.1.1.1.2.3a.cmml">of humans answered </mtext><mo lspace="0em" rspace="0em" id="S4.p2.3.m3.2.2.1.1.1.1.2.1a" xref="S4.p2.3.m3.2.2.1.1.1.1.2.1.cmml">​</mo><msup id="S4.p2.3.m3.2.2.1.1.1.1.2.4" xref="S4.p2.3.m3.2.2.1.1.1.1.2.4.cmml"><mi id="S4.p2.3.m3.2.2.1.1.1.1.2.4.2" xref="S4.p2.3.m3.2.2.1.1.1.1.2.4.2.cmml">a</mi><mo id="S4.p2.3.m3.2.2.1.1.1.1.2.4.3" xref="S4.p2.3.m3.2.2.1.1.1.1.2.4.3.cmml">∗</mo></msup></mrow><mo id="S4.p2.3.m3.2.2.1.1.1.1.1" xref="S4.p2.3.m3.2.2.1.1.1.1.1.cmml">/</mo><mn id="S4.p2.3.m3.2.2.1.1.1.1.3" xref="S4.p2.3.m3.2.2.1.1.1.1.3.cmml">3</mn></mrow><mo id="S4.p2.3.m3.2.2.1.1.1.3" xref="S4.p2.3.m3.2.2.1.1.2.cmml">,</mo><mn id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml">1</mn><mo stretchy="false" id="S4.p2.3.m3.2.2.1.1.1.4" xref="S4.p2.3.m3.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.2b"><apply id="S4.p2.3.m3.2.2.cmml" xref="S4.p2.3.m3.2.2"><eq id="S4.p2.3.m3.2.2.2.cmml" xref="S4.p2.3.m3.2.2.2"></eq><apply id="S4.p2.3.m3.2.2.3.cmml" xref="S4.p2.3.m3.2.2.3"><ci id="S4.p2.3.m3.2.2.3.1.cmml" xref="S4.p2.3.m3.2.2.3.1">^</ci><ci id="S4.p2.3.m3.2.2.3.2.cmml" xref="S4.p2.3.m3.2.2.3.2">𝑎</ci></apply><apply id="S4.p2.3.m3.2.2.1.cmml" xref="S4.p2.3.m3.2.2.1"><times id="S4.p2.3.m3.2.2.1.2.cmml" xref="S4.p2.3.m3.2.2.1.2"></times><ci id="S4.p2.3.m3.2.2.1.3.cmml" xref="S4.p2.3.m3.2.2.1.3">min</ci><interval closure="open" id="S4.p2.3.m3.2.2.1.1.2.cmml" xref="S4.p2.3.m3.2.2.1.1.1"><apply id="S4.p2.3.m3.2.2.1.1.1.1.cmml" xref="S4.p2.3.m3.2.2.1.1.1.1"><divide id="S4.p2.3.m3.2.2.1.1.1.1.1.cmml" xref="S4.p2.3.m3.2.2.1.1.1.1.1"></divide><apply id="S4.p2.3.m3.2.2.1.1.1.1.2.cmml" xref="S4.p2.3.m3.2.2.1.1.1.1.2"><times id="S4.p2.3.m3.2.2.1.1.1.1.2.1.cmml" xref="S4.p2.3.m3.2.2.1.1.1.1.2.1"></times><ci id="S4.p2.3.m3.2.2.1.1.1.1.2.2.cmml" xref="S4.p2.3.m3.2.2.1.1.1.1.2.2">#</ci><ci id="S4.p2.3.m3.2.2.1.1.1.1.2.3a.cmml" xref="S4.p2.3.m3.2.2.1.1.1.1.2.3"><mtext id="S4.p2.3.m3.2.2.1.1.1.1.2.3.cmml" xref="S4.p2.3.m3.2.2.1.1.1.1.2.3">of humans answered </mtext></ci><apply id="S4.p2.3.m3.2.2.1.1.1.1.2.4.cmml" xref="S4.p2.3.m3.2.2.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S4.p2.3.m3.2.2.1.1.1.1.2.4.1.cmml" xref="S4.p2.3.m3.2.2.1.1.1.1.2.4">superscript</csymbol><ci id="S4.p2.3.m3.2.2.1.1.1.1.2.4.2.cmml" xref="S4.p2.3.m3.2.2.1.1.1.1.2.4.2">𝑎</ci><times id="S4.p2.3.m3.2.2.1.1.1.1.2.4.3.cmml" xref="S4.p2.3.m3.2.2.1.1.1.1.2.4.3"></times></apply></apply><cn type="integer" id="S4.p2.3.m3.2.2.1.1.1.1.3.cmml" xref="S4.p2.3.m3.2.2.1.1.1.1.3">3</cn></apply><cn type="integer" id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1">1</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.2c">\hat{a}=\mathrm{min}(\#\;\text{of humans answered }a^{*}/3,1)</annotation></semantics></math>,
which means that the answer provided by the model is given 100% accuracy if at least 3 human annotators who helped create the VQA dataset gave the exact answer.
</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison of different multimodal operations when using complementary QAA features on VQA datasets. The models are evaluated on validation sets of VQAv1<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and VQAv2<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> dataset, and we report the overall accuracy (higher the better). Models in rows (1)-(3) do not have any spatial attention mechanism whereas the models in rows (4)-(6) learn spatial attention as described in Sec.<a href="#S3.SS2" title="III-B Multiple Prediction Embedding ‣ III Method ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a></figcaption>
<div id="S4.T1.16" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:467.6pt;height:195.3pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-26.0pt,10.8pt) scale(0.9,0.9) ;">
<table id="S4.T1.16.16" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.16.16.17.1" class="ltx_tr">
<td id="S4.T1.16.16.17.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T1.16.16.17.1.2" class="ltx_td ltx_border_tt"></td>
<td id="S4.T1.16.16.17.1.3" class="ltx_td ltx_border_tt"></td>
<td id="S4.T1.16.16.17.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S4.T1.16.16.17.1.4.1" class="ltx_text" style="background-color:#F2F2F2;">VQAv1 Dataset</span></td>
<td id="S4.T1.16.16.17.1.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S4.T1.16.16.17.1.5.1" class="ltx_text" style="background-color:#F2F2F2;">VQAv2 Dataset</span></td>
</tr>
<tr id="S4.T1.16.16.18.2" class="ltx_tr">
<td id="S4.T1.16.16.18.2.1" class="ltx_td"></td>
<td id="S4.T1.16.16.18.2.2" class="ltx_td"></td>
<td id="S4.T1.16.16.18.2.3" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.18.2.3.1" class="ltx_text" style="background-color:#F2F2F2;">Spatial</span></td>
<td id="S4.T1.16.16.18.2.4" class="ltx_td ltx_align_center ltx_border_t" colspan="4"><span id="S4.T1.16.16.18.2.4.1" class="ltx_text" style="background-color:#F2F2F2;">Multimodal Operation</span></td>
<td id="S4.T1.16.16.18.2.5" class="ltx_td ltx_align_center ltx_border_t" colspan="4"><span id="S4.T1.16.16.18.2.5.1" class="ltx_text" style="background-color:#F2F2F2;">Multimodal Operation</span></td>
</tr>
<tr id="S4.T1.16.16.19.3" class="ltx_tr">
<td id="S4.T1.16.16.19.3.1" class="ltx_td"></td>
<td id="S4.T1.16.16.19.3.2" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.19.3.2.1" class="ltx_text" style="background-color:#F2F2F2;">Visual Feature</span></td>
<td id="S4.T1.16.16.19.3.3" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.19.3.3.1" class="ltx_text" style="background-color:#F2F2F2;">Attention</span></td>
<td id="S4.T1.16.16.19.3.4" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.19.3.4.1" class="ltx_text" style="background-color:#F2F2F2;">Linear</span></td>
<td id="S4.T1.16.16.19.3.5" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.19.3.5.1" class="ltx_text" style="background-color:#F2F2F2;">C-MLP</span></td>
<td id="S4.T1.16.16.19.3.6" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.19.3.6.1" class="ltx_text" style="background-color:#F2F2F2;">Mutan</span></td>
<td id="S4.T1.16.16.19.3.7" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.19.3.7.1" class="ltx_text" style="background-color:#F2F2F2;">Block</span></td>
<td id="S4.T1.16.16.19.3.8" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.19.3.8.1" class="ltx_text" style="background-color:#F2F2F2;">Linear</span></td>
<td id="S4.T1.16.16.19.3.9" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.19.3.9.1" class="ltx_text" style="background-color:#F2F2F2;">C-MLP</span></td>
<td id="S4.T1.16.16.19.3.10" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.19.3.10.1" class="ltx_text" style="background-color:#F2F2F2;">Mutan</span></td>
<td id="S4.T1.16.16.19.3.11" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.19.3.11.1" class="ltx_text" style="background-color:#F2F2F2;">Block</span></td>
</tr>
<tr id="S4.T1.16.16.20.4" class="ltx_tr">
<td id="S4.T1.16.16.20.4.1" class="ltx_td ltx_align_left">(1)</td>
<td id="S4.T1.16.16.20.4.2" class="ltx_td ltx_align_center ltx_border_t">Spatial Grid (SG)</td>
<td id="S4.T1.16.16.20.4.3" class="ltx_td ltx_align_center ltx_border_t">✗</td>
<td id="S4.T1.16.16.20.4.4" class="ltx_td ltx_align_center ltx_border_t">39.7</td>
<td id="S4.T1.16.16.20.4.5" class="ltx_td ltx_align_center ltx_border_t">57.2</td>
<td id="S4.T1.16.16.20.4.6" class="ltx_td ltx_align_center ltx_border_t">56.3</td>
<td id="S4.T1.16.16.20.4.7" class="ltx_td ltx_align_center ltx_border_t">58.2</td>
<td id="S4.T1.16.16.20.4.8" class="ltx_td ltx_align_center ltx_border_t">38.2</td>
<td id="S4.T1.16.16.20.4.9" class="ltx_td ltx_align_center ltx_border_t">51.9</td>
<td id="S4.T1.16.16.20.4.10" class="ltx_td ltx_align_center ltx_border_t">55.6</td>
<td id="S4.T1.16.16.20.4.11" class="ltx_td ltx_align_center ltx_border_t">56.8</td>
</tr>
<tr id="S4.T1.16.16.21.5" class="ltx_tr">
<td id="S4.T1.16.16.21.5.1" class="ltx_td ltx_align_left">(2)</td>
<td id="S4.T1.16.16.21.5.2" class="ltx_td ltx_align_center">QAA</td>
<td id="S4.T1.16.16.21.5.3" class="ltx_td ltx_align_center">✗</td>
<td id="S4.T1.16.16.21.5.4" class="ltx_td ltx_align_center">41.4</td>
<td id="S4.T1.16.16.21.5.5" class="ltx_td ltx_align_center">40.5</td>
<td id="S4.T1.16.16.21.5.6" class="ltx_td ltx_align_center">57.3</td>
<td id="S4.T1.16.16.21.5.7" class="ltx_td ltx_align_center">58.4</td>
<td id="S4.T1.16.16.21.5.8" class="ltx_td ltx_align_center">39.7</td>
<td id="S4.T1.16.16.21.5.9" class="ltx_td ltx_align_center">53.2</td>
<td id="S4.T1.16.16.21.5.10" class="ltx_td ltx_align_center">56.3</td>
<td id="S4.T1.16.16.21.5.11" class="ltx_td ltx_align_center">56.5</td>
</tr>
<tr id="S4.T1.16.16.22.6" class="ltx_tr" style="background-color:#F5F5DB;">
<td id="S4.T1.16.16.22.6.1" class="ltx_td ltx_align_left"><span id="S4.T1.16.16.22.6.1.1" class="ltx_text" style="background-color:#F5F5DB;">(3)</span></td>
<td id="S4.T1.16.16.22.6.2" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.22.6.2.1" class="ltx_text" style="background-color:#F5F5DB;">Ours(SG+QAA)</span></td>
<td id="S4.T1.16.16.22.6.3" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.22.6.3.1" class="ltx_text" style="background-color:#F5F5DB;">✗✗</span></td>
<td id="S4.T1.16.16.22.6.4" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.22.6.4.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">57.9</span></td>
<td id="S4.T1.16.16.22.6.5" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.22.6.5.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">58.3</span></td>
<td id="S4.T1.16.16.22.6.6" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.22.6.6.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">57.5</span></td>
<td id="S4.T1.16.16.22.6.7" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.22.6.7.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">58.4</span></td>
<td id="S4.T1.16.16.22.6.8" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.22.6.8.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">56.2</span></td>
<td id="S4.T1.16.16.22.6.9" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.22.6.9.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">56.8</span></td>
<td id="S4.T1.16.16.22.6.10" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.22.6.10.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">57.0</span></td>
<td id="S4.T1.16.16.22.6.11" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.22.6.11.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">57.1</span></td>
</tr>
<tr id="S4.T1.8.8.8" class="ltx_tr">
<td id="S4.T1.8.8.8.9" class="ltx_td"></td>
<td id="S4.T1.8.8.8.10" class="ltx_td"></td>
<td id="S4.T1.8.8.8.11" class="ltx_td"></td>
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">18.2 <math id="S4.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">1.1 <math id="S4.T1.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T1.2.2.2.2.m1.1.1" xref="S4.T1.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t">1.2 <math id="S4.T1.3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T1.3.3.3.3.m1.1.1" xref="S4.T1.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t">0.2 <math id="S4.T1.4.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.4.4.4.4.m1.1a"><mo stretchy="false" id="S4.T1.4.4.4.4.m1.1.1" xref="S4.T1.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.4.m1.1b"><ci id="S4.T1.4.4.4.4.m1.1.1.cmml" xref="S4.T1.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.4.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.5.5.5.5" class="ltx_td ltx_align_center ltx_border_t">18.2 <math id="S4.T1.5.5.5.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.5.5.5.5.m1.1a"><mo stretchy="false" id="S4.T1.5.5.5.5.m1.1.1" xref="S4.T1.5.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.5.m1.1b"><ci id="S4.T1.5.5.5.5.m1.1.1.cmml" xref="S4.T1.5.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.5.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.6.6.6.6" class="ltx_td ltx_align_center ltx_border_t">4.9 <math id="S4.T1.6.6.6.6.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.6.6.6.6.m1.1a"><mo stretchy="false" id="S4.T1.6.6.6.6.m1.1.1" xref="S4.T1.6.6.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.6.m1.1b"><ci id="S4.T1.6.6.6.6.m1.1.1.cmml" xref="S4.T1.6.6.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.6.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.7.7.7.7" class="ltx_td ltx_align_center ltx_border_t">1.4 <math id="S4.T1.7.7.7.7.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.7.7.7.7.m1.1a"><mo stretchy="false" id="S4.T1.7.7.7.7.m1.1.1" xref="S4.T1.7.7.7.7.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.7.7.m1.1b"><ci id="S4.T1.7.7.7.7.m1.1.1.cmml" xref="S4.T1.7.7.7.7.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.7.7.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.8.8.8.8" class="ltx_td ltx_align_center ltx_border_t">0.3 <math id="S4.T1.8.8.8.8.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.8.8.8.8.m1.1a"><mo stretchy="false" id="S4.T1.8.8.8.8.m1.1.1" xref="S4.T1.8.8.8.8.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.8.8.m1.1b"><ci id="S4.T1.8.8.8.8.m1.1.1.cmml" xref="S4.T1.8.8.8.8.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.8.8.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T1.16.16.23.7" class="ltx_tr">
<td id="S4.T1.16.16.23.7.1" class="ltx_td ltx_align_left ltx_border_t">(4)</td>
<td id="S4.T1.16.16.23.7.2" class="ltx_td ltx_align_center ltx_border_t">Spatial Grid (SG)</td>
<td id="S4.T1.16.16.23.7.3" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S4.T1.16.16.23.7.4" class="ltx_td ltx_align_center ltx_border_t">41.8</td>
<td id="S4.T1.16.16.23.7.5" class="ltx_td ltx_align_center ltx_border_t">60.4</td>
<td id="S4.T1.16.16.23.7.6" class="ltx_td ltx_align_center ltx_border_t">58.6</td>
<td id="S4.T1.16.16.23.7.7" class="ltx_td ltx_align_center ltx_border_t">61.2</td>
<td id="S4.T1.16.16.23.7.8" class="ltx_td ltx_align_center ltx_border_t">41.0</td>
<td id="S4.T1.16.16.23.7.9" class="ltx_td ltx_align_center ltx_border_t">54.4</td>
<td id="S4.T1.16.16.23.7.10" class="ltx_td ltx_align_center ltx_border_t">57.9</td>
<td id="S4.T1.16.16.23.7.11" class="ltx_td ltx_align_center ltx_border_t">60.1</td>
</tr>
<tr id="S4.T1.16.16.24.8" class="ltx_tr">
<td id="S4.T1.16.16.24.8.1" class="ltx_td ltx_align_left">(5)</td>
<td id="S4.T1.16.16.24.8.2" class="ltx_td ltx_align_center">QAA</td>
<td id="S4.T1.16.16.24.8.3" class="ltx_td ltx_align_center">✓</td>
<td id="S4.T1.16.16.24.8.4" class="ltx_td ltx_align_center">41.4</td>
<td id="S4.T1.16.16.24.8.5" class="ltx_td ltx_align_center">59.6</td>
<td id="S4.T1.16.16.24.8.6" class="ltx_td ltx_align_center">57.9</td>
<td id="S4.T1.16.16.24.8.7" class="ltx_td ltx_align_center">60.5</td>
<td id="S4.T1.16.16.24.8.8" class="ltx_td ltx_align_center">37.3</td>
<td id="S4.T1.16.16.24.8.9" class="ltx_td ltx_align_center">57.3</td>
<td id="S4.T1.16.16.24.8.10" class="ltx_td ltx_align_center">56.5</td>
<td id="S4.T1.16.16.24.8.11" class="ltx_td ltx_align_center">59.3</td>
</tr>
<tr id="S4.T1.16.16.25.9" class="ltx_tr" style="background-color:#F5F5DB;">
<td id="S4.T1.16.16.25.9.1" class="ltx_td ltx_align_left"><span id="S4.T1.16.16.25.9.1.1" class="ltx_text" style="background-color:#F5F5DB;">(6)</span></td>
<td id="S4.T1.16.16.25.9.2" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.25.9.2.1" class="ltx_text" style="background-color:#F5F5DB;">Ours(SG+QAA)</span></td>
<td id="S4.T1.16.16.25.9.3" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.25.9.3.1" class="ltx_text" style="background-color:#F5F5DB;">✓✓</span></td>
<td id="S4.T1.16.16.25.9.4" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.25.9.4.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">60.6</span></td>
<td id="S4.T1.16.16.25.9.5" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.25.9.5.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">60.7</span></td>
<td id="S4.T1.16.16.25.9.6" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.25.9.6.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">59.2</span></td>
<td id="S4.T1.16.16.25.9.7" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.25.9.7.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">61.6</span></td>
<td id="S4.T1.16.16.25.9.8" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.25.9.8.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">59.1</span></td>
<td id="S4.T1.16.16.25.9.9" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.25.9.9.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">59.5</span></td>
<td id="S4.T1.16.16.25.9.10" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.25.9.10.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">58.2</span></td>
<td id="S4.T1.16.16.25.9.11" class="ltx_td ltx_align_center"><span id="S4.T1.16.16.25.9.11.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">60.5</span></td>
</tr>
<tr id="S4.T1.16.16.16" class="ltx_tr">
<td id="S4.T1.16.16.16.9" class="ltx_td"></td>
<td id="S4.T1.16.16.16.10" class="ltx_td"></td>
<td id="S4.T1.16.16.16.11" class="ltx_td"></td>
<td id="S4.T1.9.9.9.1" class="ltx_td ltx_align_center ltx_border_t">18.8 <math id="S4.T1.9.9.9.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.9.9.9.1.m1.1a"><mo stretchy="false" id="S4.T1.9.9.9.1.m1.1.1" xref="S4.T1.9.9.9.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.9.1.m1.1b"><ci id="S4.T1.9.9.9.1.m1.1.1.cmml" xref="S4.T1.9.9.9.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.9.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.10.10.10.2" class="ltx_td ltx_align_center ltx_border_t">0.3 <math id="S4.T1.10.10.10.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.10.10.10.2.m1.1a"><mo stretchy="false" id="S4.T1.10.10.10.2.m1.1.1" xref="S4.T1.10.10.10.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.10.2.m1.1b"><ci id="S4.T1.10.10.10.2.m1.1.1.cmml" xref="S4.T1.10.10.10.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.10.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.11.11.11.3" class="ltx_td ltx_align_center ltx_border_t">0.6 <math id="S4.T1.11.11.11.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.11.11.11.3.m1.1a"><mo stretchy="false" id="S4.T1.11.11.11.3.m1.1.1" xref="S4.T1.11.11.11.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.11.11.11.3.m1.1b"><ci id="S4.T1.11.11.11.3.m1.1.1.cmml" xref="S4.T1.11.11.11.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.11.11.3.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.12.12.12.4" class="ltx_td ltx_align_center ltx_border_t">0.3 <math id="S4.T1.12.12.12.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.12.12.12.4.m1.1a"><mo stretchy="false" id="S4.T1.12.12.12.4.m1.1.1" xref="S4.T1.12.12.12.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.12.12.12.4.m1.1b"><ci id="S4.T1.12.12.12.4.m1.1.1.cmml" xref="S4.T1.12.12.12.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.12.12.4.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.13.13.13.5" class="ltx_td ltx_align_center ltx_border_t">18.1 <math id="S4.T1.13.13.13.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.13.13.13.5.m1.1a"><mo stretchy="false" id="S4.T1.13.13.13.5.m1.1.1" xref="S4.T1.13.13.13.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.13.13.13.5.m1.1b"><ci id="S4.T1.13.13.13.5.m1.1.1.cmml" xref="S4.T1.13.13.13.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.13.13.5.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.14.14.14.6" class="ltx_td ltx_align_center ltx_border_t">5.2 <math id="S4.T1.14.14.14.6.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.14.14.14.6.m1.1a"><mo stretchy="false" id="S4.T1.14.14.14.6.m1.1.1" xref="S4.T1.14.14.14.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.14.14.14.6.m1.1b"><ci id="S4.T1.14.14.14.6.m1.1.1.cmml" xref="S4.T1.14.14.14.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.14.14.6.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.15.15.15.7" class="ltx_td ltx_align_center ltx_border_t">0.3 <math id="S4.T1.15.15.15.7.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.15.15.15.7.m1.1a"><mo stretchy="false" id="S4.T1.15.15.15.7.m1.1.1" xref="S4.T1.15.15.15.7.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.15.15.15.7.m1.1b"><ci id="S4.T1.15.15.15.7.m1.1.1.cmml" xref="S4.T1.15.15.15.7.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.15.15.15.7.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.16.16.16.8" class="ltx_td ltx_align_center ltx_border_t">0.4 <math id="S4.T1.16.16.16.8.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.16.16.16.8.m1.1a"><mo stretchy="false" id="S4.T1.16.16.16.8.m1.1.1" xref="S4.T1.16.16.16.8.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.16.16.16.8.m1.1b"><ci id="S4.T1.16.16.16.8.m1.1.1.cmml" xref="S4.T1.16.16.16.8.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.16.16.16.8.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T1.16.16.26.10" class="ltx_tr">
<td id="S4.T1.16.16.26.10.1" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="S4.T1.16.16.26.10.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Multimodal Parameters</td>
<td id="S4.T1.16.16.26.10.3" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="S4.T1.16.16.26.10.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">8M</td>
<td id="S4.T1.16.16.26.10.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">13M</td>
<td id="S4.T1.16.16.26.10.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">14M</td>
<td id="S4.T1.16.16.26.10.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">18M</td>
<td id="S4.T1.16.16.26.10.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">8M</td>
<td id="S4.T1.16.16.26.10.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">13M</td>
<td id="S4.T1.16.16.26.10.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">14M</td>
<td id="S4.T1.16.16.26.10.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">18M</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.3" class="ltx_p"><span id="S4.p3.3.1" class="ltx_text ltx_font_bold">TDIUC dataset:</span> Task Directed Image Understanding Challenge (TDIUC) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> consists of <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="~{}1.6M" display="inline"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mn id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">1.6</mn><mo lspace="0em" rspace="0em" id="S4.p3.1.m1.1.1.1" xref="S4.p3.1.m1.1.1.1.cmml">​</mo><mi id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><times id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1"></times><cn type="float" id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2">1.6</cn><ci id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">~{}1.6M</annotation></semantics></math> questions and <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="~{}170K" display="inline"><semantics id="S4.p3.2.m2.1a"><mrow id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml"><mn id="S4.p3.2.m2.1.1.2" xref="S4.p3.2.m2.1.1.2.cmml">170</mn><mo lspace="0em" rspace="0em" id="S4.p3.2.m2.1.1.1" xref="S4.p3.2.m2.1.1.1.cmml">​</mo><mi id="S4.p3.2.m2.1.1.3" xref="S4.p3.2.m2.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><apply id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1"><times id="S4.p3.2.m2.1.1.1.cmml" xref="S4.p3.2.m2.1.1.1"></times><cn type="integer" id="S4.p3.2.m2.1.1.2.cmml" xref="S4.p3.2.m2.1.1.2">170</cn><ci id="S4.p3.2.m2.1.1.3.cmml" xref="S4.p3.2.m2.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">~{}170K</annotation></semantics></math> images sourced from MS COCO and the Visual Genome Dataset. These Image-Question pairs are split into 12 categories and 4 additional evaluation matrices (<math id="S4.p3.3.m3.1" class="ltx_Math" alttext="1^{st}" display="inline"><semantics id="S4.p3.3.m3.1a"><msup id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml"><mn id="S4.p3.3.m3.1.1.2" xref="S4.p3.3.m3.1.1.2.cmml">1</mn><mrow id="S4.p3.3.m3.1.1.3" xref="S4.p3.3.m3.1.1.3.cmml"><mi id="S4.p3.3.m3.1.1.3.2" xref="S4.p3.3.m3.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.p3.3.m3.1.1.3.1" xref="S4.p3.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.p3.3.m3.1.1.3.3" xref="S4.p3.3.m3.1.1.3.3.cmml">t</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><apply id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p3.3.m3.1.1.1.cmml" xref="S4.p3.3.m3.1.1">superscript</csymbol><cn type="integer" id="S4.p3.3.m3.1.1.2.cmml" xref="S4.p3.3.m3.1.1.2">1</cn><apply id="S4.p3.3.m3.1.1.3.cmml" xref="S4.p3.3.m3.1.1.3"><times id="S4.p3.3.m3.1.1.3.1.cmml" xref="S4.p3.3.m3.1.1.3.1"></times><ci id="S4.p3.3.m3.1.1.3.2.cmml" xref="S4.p3.3.m3.1.1.3.2">𝑠</ci><ci id="S4.p3.3.m3.1.1.3.3.cmml" xref="S4.p3.3.m3.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">1^{st}</annotation></semantics></math> column of Tab. <a href="#S4.T3" title="TABLE III ‣ IV-A Ablation on Different Multimodal Operations ‣ IV Experiments and Results ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>) which help evaluate a model’s robustness against answer imbalance and its ability to answer questions that require higher reasoning capability. We evaluate and perform ablation on TDUIC testset, and report accuracy for all 12 question types along with overall arithmetic mean-per-type (MPT) and harmonic MPT, and overall normalized arithmetic MPT and harmonic MPT in Tab. <a href="#S4.T3" title="TABLE III ‣ IV-A Ablation on Different Multimodal Operations ‣ IV Experiments and Results ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Instance Segmentation:</span> We employ a pre-trained Mask-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> model<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="font-size:70%;">github.com/facebookresearch/maskrcnn-benchmark</span></span></span></span> to generate instance masks by running inference on the input image. Specifically, the Mask-RCNN model was trained on COCO <span id="S4.p4.1.2" class="ltx_text ltx_font_italic">train</span> and the <span id="S4.p4.1.3" class="ltx_text ltx_font_italic">val-minus-minival</span> split with a ResNet-50-FPN backbone. Note that although the ‘training data’ (<em id="S4.p4.1.4" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.p4.1.5" class="ltx_text"></span> images) of the VQA datasets have an overlap with the ‘training set’ of COCO, none of the test images have been previously seen by the pre-trained model. Also, we do not use any object-level information in our attention map, rather only a simple binary mask showing the location of detected objects is used in our approach. Therefore, our setting has no extra advantage or external supervision compared to other approaches.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.13" class="ltx_p"><span id="S4.p5.13.1" class="ltx_text ltx_font_bold">Model Architecture:</span> We use ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> pretrained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> to extract the visual features of an image with dimensions <math id="S4.p5.1.m1.1" class="ltx_Math" alttext="196\times 2048" display="inline"><semantics id="S4.p5.1.m1.1a"><mrow id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml"><mn id="S4.p5.1.m1.1.1.2" xref="S4.p5.1.m1.1.1.2.cmml">196</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p5.1.m1.1.1.1" xref="S4.p5.1.m1.1.1.1.cmml">×</mo><mn id="S4.p5.1.m1.1.1.3" xref="S4.p5.1.m1.1.1.3.cmml">2048</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><apply id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1"><times id="S4.p5.1.m1.1.1.1.cmml" xref="S4.p5.1.m1.1.1.1"></times><cn type="integer" id="S4.p5.1.m1.1.1.2.cmml" xref="S4.p5.1.m1.1.1.2">196</cn><cn type="integer" id="S4.p5.1.m1.1.1.3.cmml" xref="S4.p5.1.m1.1.1.3">2048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">196\times 2048</annotation></semantics></math>. Here, <math id="S4.p5.2.m2.1" class="ltx_Math" alttext="g=196" display="inline"><semantics id="S4.p5.2.m2.1a"><mrow id="S4.p5.2.m2.1.1" xref="S4.p5.2.m2.1.1.cmml"><mi id="S4.p5.2.m2.1.1.2" xref="S4.p5.2.m2.1.1.2.cmml">g</mi><mo id="S4.p5.2.m2.1.1.1" xref="S4.p5.2.m2.1.1.1.cmml">=</mo><mn id="S4.p5.2.m2.1.1.3" xref="S4.p5.2.m2.1.1.3.cmml">196</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.2.m2.1b"><apply id="S4.p5.2.m2.1.1.cmml" xref="S4.p5.2.m2.1.1"><eq id="S4.p5.2.m2.1.1.1.cmml" xref="S4.p5.2.m2.1.1.1"></eq><ci id="S4.p5.2.m2.1.1.2.cmml" xref="S4.p5.2.m2.1.1.2">𝑔</ci><cn type="integer" id="S4.p5.2.m2.1.1.3.cmml" xref="S4.p5.2.m2.1.1.3">196</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.2.m2.1c">g=196</annotation></semantics></math> which represents the <math id="S4.p5.3.m3.1" class="ltx_Math" alttext="14\times 14" display="inline"><semantics id="S4.p5.3.m3.1a"><mrow id="S4.p5.3.m3.1.1" xref="S4.p5.3.m3.1.1.cmml"><mn id="S4.p5.3.m3.1.1.2" xref="S4.p5.3.m3.1.1.2.cmml">14</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p5.3.m3.1.1.1" xref="S4.p5.3.m3.1.1.1.cmml">×</mo><mn id="S4.p5.3.m3.1.1.3" xref="S4.p5.3.m3.1.1.3.cmml">14</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.3.m3.1b"><apply id="S4.p5.3.m3.1.1.cmml" xref="S4.p5.3.m3.1.1"><times id="S4.p5.3.m3.1.1.1.cmml" xref="S4.p5.3.m3.1.1.1"></times><cn type="integer" id="S4.p5.3.m3.1.1.2.cmml" xref="S4.p5.3.m3.1.1.2">14</cn><cn type="integer" id="S4.p5.3.m3.1.1.3.cmml" xref="S4.p5.3.m3.1.1.3">14</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.3.m3.1c">14\times 14</annotation></semantics></math> spatial grid corresponding to image regions and <math id="S4.p5.4.m4.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S4.p5.4.m4.1a"><mn id="S4.p5.4.m4.1.1" xref="S4.p5.4.m4.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S4.p5.4.m4.1b"><cn type="integer" id="S4.p5.4.m4.1.1.cmml" xref="S4.p5.4.m4.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.4.m4.1c">2048</annotation></semantics></math> is the dimension of visual features for each grid location. The language model generates a <math id="S4.p5.5.m5.1" class="ltx_Math" alttext="d_{q}=2400" display="inline"><semantics id="S4.p5.5.m5.1a"><mrow id="S4.p5.5.m5.1.1" xref="S4.p5.5.m5.1.1.cmml"><msub id="S4.p5.5.m5.1.1.2" xref="S4.p5.5.m5.1.1.2.cmml"><mi id="S4.p5.5.m5.1.1.2.2" xref="S4.p5.5.m5.1.1.2.2.cmml">d</mi><mi id="S4.p5.5.m5.1.1.2.3" xref="S4.p5.5.m5.1.1.2.3.cmml">q</mi></msub><mo id="S4.p5.5.m5.1.1.1" xref="S4.p5.5.m5.1.1.1.cmml">=</mo><mn id="S4.p5.5.m5.1.1.3" xref="S4.p5.5.m5.1.1.3.cmml">2400</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.5.m5.1b"><apply id="S4.p5.5.m5.1.1.cmml" xref="S4.p5.5.m5.1.1"><eq id="S4.p5.5.m5.1.1.1.cmml" xref="S4.p5.5.m5.1.1.1"></eq><apply id="S4.p5.5.m5.1.1.2.cmml" xref="S4.p5.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.p5.5.m5.1.1.2.1.cmml" xref="S4.p5.5.m5.1.1.2">subscript</csymbol><ci id="S4.p5.5.m5.1.1.2.2.cmml" xref="S4.p5.5.m5.1.1.2.2">𝑑</ci><ci id="S4.p5.5.m5.1.1.2.3.cmml" xref="S4.p5.5.m5.1.1.2.3">𝑞</ci></apply><cn type="integer" id="S4.p5.5.m5.1.1.3.cmml" xref="S4.p5.5.m5.1.1.3">2400</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.5.m5.1c">d_{q}=2400</annotation></semantics></math> dimensional feature for each question in a fashion similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. The question words are first preprocessed, tokenized and encoded through a embedding layer that consists of GRUs and uses a pretrained skip-thought encoder. For the models <span id="S4.p5.13.2" class="ltx_text ltx_font_italic">without</span> the optional spatial attention mechanism, the input visual feature is averaged across the spatial grid to generate a <math id="S4.p5.6.m6.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S4.p5.6.m6.1a"><mn id="S4.p5.6.m6.1.1" xref="S4.p5.6.m6.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S4.p5.6.m6.1b"><cn type="integer" id="S4.p5.6.m6.1.1.cmml" xref="S4.p5.6.m6.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.6.m6.1c">2048</annotation></semantics></math>-d feature vector from the <math id="S4.p5.7.m7.1" class="ltx_Math" alttext="2048{\times}14{\times}14" display="inline"><semantics id="S4.p5.7.m7.1a"><mrow id="S4.p5.7.m7.1.1" xref="S4.p5.7.m7.1.1.cmml"><mn id="S4.p5.7.m7.1.1.2" xref="S4.p5.7.m7.1.1.2.cmml">2048</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p5.7.m7.1.1.1" xref="S4.p5.7.m7.1.1.1.cmml">×</mo><mn id="S4.p5.7.m7.1.1.3" xref="S4.p5.7.m7.1.1.3.cmml">14</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p5.7.m7.1.1.1a" xref="S4.p5.7.m7.1.1.1.cmml">×</mo><mn id="S4.p5.7.m7.1.1.4" xref="S4.p5.7.m7.1.1.4.cmml">14</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.7.m7.1b"><apply id="S4.p5.7.m7.1.1.cmml" xref="S4.p5.7.m7.1.1"><times id="S4.p5.7.m7.1.1.1.cmml" xref="S4.p5.7.m7.1.1.1"></times><cn type="integer" id="S4.p5.7.m7.1.1.2.cmml" xref="S4.p5.7.m7.1.1.2">2048</cn><cn type="integer" id="S4.p5.7.m7.1.1.3.cmml" xref="S4.p5.7.m7.1.1.3">14</cn><cn type="integer" id="S4.p5.7.m7.1.1.4.cmml" xref="S4.p5.7.m7.1.1.4">14</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.7.m7.1c">2048{\times}14{\times}14</annotation></semantics></math> dimensional feature map and passed on to be jointly embedded with the question feature. On the other hand, the models <span id="S4.p5.13.3" class="ltx_text ltx_font_italic">with</span> spatial attention learn to generate <math id="S4.p5.8.m8.1" class="ltx_Math" alttext="2048" display="inline"><semantics id="S4.p5.8.m8.1a"><mn id="S4.p5.8.m8.1.1" xref="S4.p5.8.m8.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S4.p5.8.m8.1b"><cn type="integer" id="S4.p5.8.m8.1.1.cmml" xref="S4.p5.8.m8.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.8.m8.1c">2048</annotation></semantics></math>-d feature vector as discussed in Sec. <a href="#S3.SS2" title="III-B Multiple Prediction Embedding ‣ III Method ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>. Following the VQA benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, the dictionary of candidate answers <math id="S4.p5.9.m9.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S4.p5.9.m9.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p5.9.m9.1.1" xref="S4.p5.9.m9.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S4.p5.9.m9.1b"><ci id="S4.p5.9.m9.1.1.cmml" xref="S4.p5.9.m9.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.9.m9.1c">\mathcal{D}</annotation></semantics></math> consists of the top <math id="S4.p5.10.m10.1" class="ltx_Math" alttext="3000" display="inline"><semantics id="S4.p5.10.m10.1a"><mn id="S4.p5.10.m10.1.1" xref="S4.p5.10.m10.1.1.cmml">3000</mn><annotation-xml encoding="MathML-Content" id="S4.p5.10.m10.1b"><cn type="integer" id="S4.p5.10.m10.1.1.cmml" xref="S4.p5.10.m10.1.1">3000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.10.m10.1c">3000</annotation></semantics></math> frequent answers from the respective versions of VQA dataset. A cross entropy loss is minimized to predict the correct answer from the dictionary <math id="S4.p5.11.m11.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S4.p5.11.m11.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p5.11.m11.1.1" xref="S4.p5.11.m11.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S4.p5.11.m11.1b"><ci id="S4.p5.11.m11.1.1.cmml" xref="S4.p5.11.m11.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.11.m11.1c">\mathcal{D}</annotation></semantics></math>. While performing experiments on the TUDIC dataset, dimension of <math id="S4.p5.12.m12.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S4.p5.12.m12.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p5.12.m12.1.1" xref="S4.p5.12.m12.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S4.p5.12.m12.1b"><ci id="S4.p5.12.m12.1.1.cmml" xref="S4.p5.12.m12.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.12.m12.1c">\mathcal{D}</annotation></semantics></math> is set to <math id="S4.p5.13.m13.1" class="ltx_Math" alttext="1480" display="inline"><semantics id="S4.p5.13.m13.1a"><mn id="S4.p5.13.m13.1.1" xref="S4.p5.13.m13.1.1.cmml">1480</mn><annotation-xml encoding="MathML-Content" id="S4.p5.13.m13.1b"><cn type="integer" id="S4.p5.13.m13.1.1.cmml" xref="S4.p5.13.m13.1.1">1480</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.13.m13.1c">1480</annotation></semantics></math>.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_bold">Baseline Model:</span> We setup our VQA baseline model with four variants where the model employs different multimodal operations for combining the question and image features. All other setup and hyperparameters are kept exactly the same for fair comparison. Each variant can have the optional spatial attention module. The first two variant of our baseline model incorporate simpler multimodal operation (<em id="S4.p6.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.p6.1.3" class="ltx_text"></span> liner summation and concatenation followed by MLP). The latter two variants use a more complex multimodal operation, namely Mutan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and Block <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, which achieve the state-of-the-art performance for the VQA task, and have a considerably higher number of trainable parameters for multimodal embedding. Mutan and Block operation are implemented using their publicly available code<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="font-size:70%;">github.com/Cadene/block.bootstrap.pytorch</span></span></span></span>. The following are the four variants of our baseline model:</p>
</div>
<div id="S4.p7" class="ltx_para ltx_noindent">
<p id="S4.p7.3" class="ltx_p"><span id="S4.p7.3.1" class="ltx_text ltx_font_italic">Linear:</span> The question and image features are projected onto a common space using fully connected layers and the projected vectors are summed to obtain a joint feature representation. This joint representation is projected to the prediction space <math id="S4.p7.1.m1.1" class="ltx_Math" alttext="P\in\mathbb{R}^{3000}" display="inline"><semantics id="S4.p7.1.m1.1a"><mrow id="S4.p7.1.m1.1.1" xref="S4.p7.1.m1.1.1.cmml"><mi id="S4.p7.1.m1.1.1.2" xref="S4.p7.1.m1.1.1.2.cmml">P</mi><mo id="S4.p7.1.m1.1.1.1" xref="S4.p7.1.m1.1.1.1.cmml">∈</mo><msup id="S4.p7.1.m1.1.1.3" xref="S4.p7.1.m1.1.1.3.cmml"><mi id="S4.p7.1.m1.1.1.3.2" xref="S4.p7.1.m1.1.1.3.2.cmml">ℝ</mi><mn id="S4.p7.1.m1.1.1.3.3" xref="S4.p7.1.m1.1.1.3.3.cmml">3000</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p7.1.m1.1b"><apply id="S4.p7.1.m1.1.1.cmml" xref="S4.p7.1.m1.1.1"><in id="S4.p7.1.m1.1.1.1.cmml" xref="S4.p7.1.m1.1.1.1"></in><ci id="S4.p7.1.m1.1.1.2.cmml" xref="S4.p7.1.m1.1.1.2">𝑃</ci><apply id="S4.p7.1.m1.1.1.3.cmml" xref="S4.p7.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.p7.1.m1.1.1.3.1.cmml" xref="S4.p7.1.m1.1.1.3">superscript</csymbol><ci id="S4.p7.1.m1.1.1.3.2.cmml" xref="S4.p7.1.m1.1.1.3.2">ℝ</ci><cn type="integer" id="S4.p7.1.m1.1.1.3.3.cmml" xref="S4.p7.1.m1.1.1.3.3">3000</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.1.m1.1c">P\in\mathbb{R}^{3000}</annotation></semantics></math> which is then passed through the answer prediction network to generate the final prediction. This can be expressed as: <math id="S4.p7.2.m2.1" class="ltx_Math" alttext="P^{\prime}=\omega_{P}(\omega_{\bm{q}}\bm{q}+\omega_{\bm{v}}\bm{v})" display="inline"><semantics id="S4.p7.2.m2.1a"><mrow id="S4.p7.2.m2.1.1" xref="S4.p7.2.m2.1.1.cmml"><msup id="S4.p7.2.m2.1.1.3" xref="S4.p7.2.m2.1.1.3.cmml"><mi id="S4.p7.2.m2.1.1.3.2" xref="S4.p7.2.m2.1.1.3.2.cmml">P</mi><mo id="S4.p7.2.m2.1.1.3.3" xref="S4.p7.2.m2.1.1.3.3.cmml">′</mo></msup><mo id="S4.p7.2.m2.1.1.2" xref="S4.p7.2.m2.1.1.2.cmml">=</mo><mrow id="S4.p7.2.m2.1.1.1" xref="S4.p7.2.m2.1.1.1.cmml"><msub id="S4.p7.2.m2.1.1.1.3" xref="S4.p7.2.m2.1.1.1.3.cmml"><mi id="S4.p7.2.m2.1.1.1.3.2" xref="S4.p7.2.m2.1.1.1.3.2.cmml">ω</mi><mi id="S4.p7.2.m2.1.1.1.3.3" xref="S4.p7.2.m2.1.1.1.3.3.cmml">P</mi></msub><mo lspace="0em" rspace="0em" id="S4.p7.2.m2.1.1.1.2" xref="S4.p7.2.m2.1.1.1.2.cmml">​</mo><mrow id="S4.p7.2.m2.1.1.1.1.1" xref="S4.p7.2.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.p7.2.m2.1.1.1.1.1.2" xref="S4.p7.2.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.p7.2.m2.1.1.1.1.1.1" xref="S4.p7.2.m2.1.1.1.1.1.1.cmml"><mrow id="S4.p7.2.m2.1.1.1.1.1.1.2" xref="S4.p7.2.m2.1.1.1.1.1.1.2.cmml"><msub id="S4.p7.2.m2.1.1.1.1.1.1.2.2" xref="S4.p7.2.m2.1.1.1.1.1.1.2.2.cmml"><mi id="S4.p7.2.m2.1.1.1.1.1.1.2.2.2" xref="S4.p7.2.m2.1.1.1.1.1.1.2.2.2.cmml">ω</mi><mi id="S4.p7.2.m2.1.1.1.1.1.1.2.2.3" xref="S4.p7.2.m2.1.1.1.1.1.1.2.2.3.cmml">𝒒</mi></msub><mo lspace="0em" rspace="0em" id="S4.p7.2.m2.1.1.1.1.1.1.2.1" xref="S4.p7.2.m2.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S4.p7.2.m2.1.1.1.1.1.1.2.3" xref="S4.p7.2.m2.1.1.1.1.1.1.2.3.cmml">𝒒</mi></mrow><mo id="S4.p7.2.m2.1.1.1.1.1.1.1" xref="S4.p7.2.m2.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.p7.2.m2.1.1.1.1.1.1.3" xref="S4.p7.2.m2.1.1.1.1.1.1.3.cmml"><msub id="S4.p7.2.m2.1.1.1.1.1.1.3.2" xref="S4.p7.2.m2.1.1.1.1.1.1.3.2.cmml"><mi id="S4.p7.2.m2.1.1.1.1.1.1.3.2.2" xref="S4.p7.2.m2.1.1.1.1.1.1.3.2.2.cmml">ω</mi><mi id="S4.p7.2.m2.1.1.1.1.1.1.3.2.3" xref="S4.p7.2.m2.1.1.1.1.1.1.3.2.3.cmml">𝒗</mi></msub><mo lspace="0em" rspace="0em" id="S4.p7.2.m2.1.1.1.1.1.1.3.1" xref="S4.p7.2.m2.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S4.p7.2.m2.1.1.1.1.1.1.3.3" xref="S4.p7.2.m2.1.1.1.1.1.1.3.3.cmml">𝒗</mi></mrow></mrow><mo stretchy="false" id="S4.p7.2.m2.1.1.1.1.1.3" xref="S4.p7.2.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p7.2.m2.1b"><apply id="S4.p7.2.m2.1.1.cmml" xref="S4.p7.2.m2.1.1"><eq id="S4.p7.2.m2.1.1.2.cmml" xref="S4.p7.2.m2.1.1.2"></eq><apply id="S4.p7.2.m2.1.1.3.cmml" xref="S4.p7.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.p7.2.m2.1.1.3.1.cmml" xref="S4.p7.2.m2.1.1.3">superscript</csymbol><ci id="S4.p7.2.m2.1.1.3.2.cmml" xref="S4.p7.2.m2.1.1.3.2">𝑃</ci><ci id="S4.p7.2.m2.1.1.3.3.cmml" xref="S4.p7.2.m2.1.1.3.3">′</ci></apply><apply id="S4.p7.2.m2.1.1.1.cmml" xref="S4.p7.2.m2.1.1.1"><times id="S4.p7.2.m2.1.1.1.2.cmml" xref="S4.p7.2.m2.1.1.1.2"></times><apply id="S4.p7.2.m2.1.1.1.3.cmml" xref="S4.p7.2.m2.1.1.1.3"><csymbol cd="ambiguous" id="S4.p7.2.m2.1.1.1.3.1.cmml" xref="S4.p7.2.m2.1.1.1.3">subscript</csymbol><ci id="S4.p7.2.m2.1.1.1.3.2.cmml" xref="S4.p7.2.m2.1.1.1.3.2">𝜔</ci><ci id="S4.p7.2.m2.1.1.1.3.3.cmml" xref="S4.p7.2.m2.1.1.1.3.3">𝑃</ci></apply><apply id="S4.p7.2.m2.1.1.1.1.1.1.cmml" xref="S4.p7.2.m2.1.1.1.1.1"><plus id="S4.p7.2.m2.1.1.1.1.1.1.1.cmml" xref="S4.p7.2.m2.1.1.1.1.1.1.1"></plus><apply id="S4.p7.2.m2.1.1.1.1.1.1.2.cmml" xref="S4.p7.2.m2.1.1.1.1.1.1.2"><times id="S4.p7.2.m2.1.1.1.1.1.1.2.1.cmml" xref="S4.p7.2.m2.1.1.1.1.1.1.2.1"></times><apply id="S4.p7.2.m2.1.1.1.1.1.1.2.2.cmml" xref="S4.p7.2.m2.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.p7.2.m2.1.1.1.1.1.1.2.2.1.cmml" xref="S4.p7.2.m2.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S4.p7.2.m2.1.1.1.1.1.1.2.2.2.cmml" xref="S4.p7.2.m2.1.1.1.1.1.1.2.2.2">𝜔</ci><ci id="S4.p7.2.m2.1.1.1.1.1.1.2.2.3.cmml" xref="S4.p7.2.m2.1.1.1.1.1.1.2.2.3">𝒒</ci></apply><ci id="S4.p7.2.m2.1.1.1.1.1.1.2.3.cmml" xref="S4.p7.2.m2.1.1.1.1.1.1.2.3">𝒒</ci></apply><apply id="S4.p7.2.m2.1.1.1.1.1.1.3.cmml" xref="S4.p7.2.m2.1.1.1.1.1.1.3"><times id="S4.p7.2.m2.1.1.1.1.1.1.3.1.cmml" xref="S4.p7.2.m2.1.1.1.1.1.1.3.1"></times><apply id="S4.p7.2.m2.1.1.1.1.1.1.3.2.cmml" xref="S4.p7.2.m2.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.p7.2.m2.1.1.1.1.1.1.3.2.1.cmml" xref="S4.p7.2.m2.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S4.p7.2.m2.1.1.1.1.1.1.3.2.2.cmml" xref="S4.p7.2.m2.1.1.1.1.1.1.3.2.2">𝜔</ci><ci id="S4.p7.2.m2.1.1.1.1.1.1.3.2.3.cmml" xref="S4.p7.2.m2.1.1.1.1.1.1.3.2.3">𝒗</ci></apply><ci id="S4.p7.2.m2.1.1.1.1.1.1.3.3.cmml" xref="S4.p7.2.m2.1.1.1.1.1.1.3.3">𝒗</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.2.m2.1c">P^{\prime}=\omega_{P}(\omega_{\bm{q}}\bm{q}+\omega_{\bm{v}}\bm{v})</annotation></semantics></math>
where <math id="S4.p7.3.m3.1" class="ltx_Math" alttext="\omega" display="inline"><semantics id="S4.p7.3.m3.1a"><mi id="S4.p7.3.m3.1.1" xref="S4.p7.3.m3.1.1.cmml">ω</mi><annotation-xml encoding="MathML-Content" id="S4.p7.3.m3.1b"><ci id="S4.p7.3.m3.1.1.cmml" xref="S4.p7.3.m3.1.1">𝜔</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.3.m3.1c">\omega</annotation></semantics></math> represents the fully connected layer weights used for projection.</p>
</div>
<div id="S4.p8" class="ltx_para ltx_noindent">
<p id="S4.p8.1" class="ltx_p"><span id="S4.p8.1.1" class="ltx_text ltx_font_italic">Concat-MLP:</span> The question and image features are concatenated and passed through a 3-layer MultiLayer Perceptron (MLP) with ReLU activation and dropout to combine the input features. The resulting vector is projected onto the prediction space for answer classification.</p>
</div>
<div id="S4.p9" class="ltx_para ltx_noindent">
<p id="S4.p9.1" class="ltx_p"><span id="S4.p9.1.1" class="ltx_text ltx_font_italic">Mutan:</span> The Mutan model learns a multimodal interaction between question and image using rank constrained Tucker tensor decomposition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. In this model, the visual and language features are decomposed into three matrices and a core tensor that is somewhat capable of modelling the fully parameterized interaction in the multimodal space.</p>
</div>
<div id="S4.p10" class="ltx_para ltx_noindent">
<p id="S4.p10.1" class="ltx_p"><span id="S4.p10.1.1" class="ltx_text ltx_font_italic">Block:</span> It employs block-term tensor decomposition following a super-diagonal fusion framework<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. This is the most computationally expensive model that we experiment with and achieves state-of-the-art performance. The complexity of a multimodal operation is inferred by calculating the number of trainable parameters from attended image features, the question embedding, and the answer prediction.</p>
</div>
<div id="S4.p11" class="ltx_para">
<p id="S4.p11.1" class="ltx_p"><span id="S4.p11.1.1" class="ltx_text ltx_font_bold">Ablation study:</span>
In Sec. <a href="#S4.SS1" title="IV-A Ablation on Different Multimodal Operations ‣ IV Experiments and Results ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a> we perform ablation to showcase the effectiveness of using complementary question-agnostic attention on VQA models employing different multimodal operation by evaluating on the VQAv2 Valset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and the TDIUC testset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Furthermore, in Sec. <a href="#S4.SS2" title="IV-B Inference with Global Representation ‣ IV Experiments and Results ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a> we show that without an explicit <span id="S4.p11.1.2" class="ltx_text ltx_font_italic">object map</span> during inference, our model can utilize image independent QAA features generated from a global representation of training examples.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Ablation on Different Multimodal Operations</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">Simplistic VQA models get a significant performance boost using complementary QAA features and perform on par with the state-of-the-art.</span></p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.8" class="ltx_p">In row (1) of Tab. <a href="#S4.T1" title="TABLE I ‣ IV Experiments and Results ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, we report that our baseline VQA model employing state-of-the-art Block fusion achieves <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="58.4" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">58.4</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><cn type="float" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">58.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">58.4</annotation></semantics></math> and <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="57.1" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mn id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">57.1</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><cn type="float" id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">57.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">57.1</annotation></semantics></math> accuracy, whereas with a linear-sum operation, the same model achieves accuracy of only <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="39.7" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mn id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">39.7</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><cn type="float" id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">39.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">39.7</annotation></semantics></math> and <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="38.2" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><mn id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">38.2</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><cn type="float" id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">38.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">38.2</annotation></semantics></math> on VQAv1 and VQAv2 validation sets, respectively. When the Linear model is trained with complementary QAA features, the accuracy increases to <math id="S4.SS1.p2.5.m5.1" class="ltx_Math" alttext="57.9" display="inline"><semantics id="S4.SS1.p2.5.m5.1a"><mn id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml">57.9</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.1b"><cn type="float" id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1">57.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.1c">57.9</annotation></semantics></math> and <math id="S4.SS1.p2.6.m6.1" class="ltx_Math" alttext="56.2" display="inline"><semantics id="S4.SS1.p2.6.m6.1a"><mn id="S4.SS1.p2.6.m6.1.1" xref="S4.SS1.p2.6.m6.1.1.cmml">56.2</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.6.m6.1b"><cn type="float" id="S4.SS1.p2.6.m6.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1">56.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.6.m6.1c">56.2</annotation></semantics></math> on the VQAv1 and VQAv2 datasets, respectively; performing very close to the state-of-the-art Block model (row (3)). This pattern also exists when these same models include the optional spatial attention module (comparing rows (4) and (6)). The simpler Linear model benefits from using complementary QAA features as it represents a subset of the spatial locations of the whole image that has object instances and encodes visual cues like count, location and attributes which are most important to predict the correct answer. The Linear model with only 8M trainable parameters and relatively simpler multimodal operation cannot learn to identify these visual cues on its own. Thus the performance boost while using complementary QAA feature is more significant (<math id="S4.SS1.p2.7.m7.1" class="ltx_Math" alttext="{\sim}18\uparrow" display="inline"><semantics id="S4.SS1.p2.7.m7.1a"><mrow id="S4.SS1.p2.7.m7.1.1" xref="S4.SS1.p2.7.m7.1.1.cmml"><mi id="S4.SS1.p2.7.m7.1.1.2" xref="S4.SS1.p2.7.m7.1.1.2.cmml"></mi><mo id="S4.SS1.p2.7.m7.1.1.3" xref="S4.SS1.p2.7.m7.1.1.3.cmml">∼</mo><mn id="S4.SS1.p2.7.m7.1.1.4" xref="S4.SS1.p2.7.m7.1.1.4.cmml">18</mn><mo stretchy="false" id="S4.SS1.p2.7.m7.1.1.5" xref="S4.SS1.p2.7.m7.1.1.5.cmml">↑</mo><mi id="S4.SS1.p2.7.m7.1.1.6" xref="S4.SS1.p2.7.m7.1.1.6.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.7.m7.1b"><apply id="S4.SS1.p2.7.m7.1.1.cmml" xref="S4.SS1.p2.7.m7.1.1"><and id="S4.SS1.p2.7.m7.1.1a.cmml" xref="S4.SS1.p2.7.m7.1.1"></and><apply id="S4.SS1.p2.7.m7.1.1b.cmml" xref="S4.SS1.p2.7.m7.1.1"><csymbol cd="latexml" id="S4.SS1.p2.7.m7.1.1.3.cmml" xref="S4.SS1.p2.7.m7.1.1.3">similar-to</csymbol><csymbol cd="latexml" id="S4.SS1.p2.7.m7.1.1.2.cmml" xref="S4.SS1.p2.7.m7.1.1.2">absent</csymbol><cn type="integer" id="S4.SS1.p2.7.m7.1.1.4.cmml" xref="S4.SS1.p2.7.m7.1.1.4">18</cn></apply><apply id="S4.SS1.p2.7.m7.1.1c.cmml" xref="S4.SS1.p2.7.m7.1.1"><ci id="S4.SS1.p2.7.m7.1.1.5.cmml" xref="S4.SS1.p2.7.m7.1.1.5">↑</ci><share href="#S4.SS1.p2.7.m7.1.1.4.cmml" id="S4.SS1.p2.7.m7.1.1d.cmml" xref="S4.SS1.p2.7.m7.1.1"></share><csymbol cd="latexml" id="S4.SS1.p2.7.m7.1.1.6.cmml" xref="S4.SS1.p2.7.m7.1.1.6">absent</csymbol></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.7.m7.1c">{\sim}18\uparrow</annotation></semantics></math> vs <math id="S4.SS1.p2.8.m8.1" class="ltx_Math" alttext="{\sim}0.5\uparrow" display="inline"><semantics id="S4.SS1.p2.8.m8.1a"><mrow id="S4.SS1.p2.8.m8.1.1" xref="S4.SS1.p2.8.m8.1.1.cmml"><mi id="S4.SS1.p2.8.m8.1.1.2" xref="S4.SS1.p2.8.m8.1.1.2.cmml"></mi><mo id="S4.SS1.p2.8.m8.1.1.3" xref="S4.SS1.p2.8.m8.1.1.3.cmml">∼</mo><mn id="S4.SS1.p2.8.m8.1.1.4" xref="S4.SS1.p2.8.m8.1.1.4.cmml">0.5</mn><mo stretchy="false" id="S4.SS1.p2.8.m8.1.1.5" xref="S4.SS1.p2.8.m8.1.1.5.cmml">↑</mo><mi id="S4.SS1.p2.8.m8.1.1.6" xref="S4.SS1.p2.8.m8.1.1.6.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.8.m8.1b"><apply id="S4.SS1.p2.8.m8.1.1.cmml" xref="S4.SS1.p2.8.m8.1.1"><and id="S4.SS1.p2.8.m8.1.1a.cmml" xref="S4.SS1.p2.8.m8.1.1"></and><apply id="S4.SS1.p2.8.m8.1.1b.cmml" xref="S4.SS1.p2.8.m8.1.1"><csymbol cd="latexml" id="S4.SS1.p2.8.m8.1.1.3.cmml" xref="S4.SS1.p2.8.m8.1.1.3">similar-to</csymbol><csymbol cd="latexml" id="S4.SS1.p2.8.m8.1.1.2.cmml" xref="S4.SS1.p2.8.m8.1.1.2">absent</csymbol><cn type="float" id="S4.SS1.p2.8.m8.1.1.4.cmml" xref="S4.SS1.p2.8.m8.1.1.4">0.5</cn></apply><apply id="S4.SS1.p2.8.m8.1.1c.cmml" xref="S4.SS1.p2.8.m8.1.1"><ci id="S4.SS1.p2.8.m8.1.1.5.cmml" xref="S4.SS1.p2.8.m8.1.1.5">↑</ci><share href="#S4.SS1.p2.8.m8.1.1.4.cmml" id="S4.SS1.p2.8.m8.1.1d.cmml" xref="S4.SS1.p2.8.m8.1.1"></share><csymbol cd="latexml" id="S4.SS1.p2.8.m8.1.1.6.cmml" xref="S4.SS1.p2.8.m8.1.1.6">absent</csymbol></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.8.m8.1c">{\sim}0.5\uparrow</annotation></semantics></math>) for VQA models employing a simplistic multimodal operation (<em id="S4.SS1.p2.8.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.p2.8.2" class="ltx_text"></span> Linear and Concat-MLP) compared to the models employing a more sophisticated fusion operation (<em id="S4.SS1.p2.8.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS1.p2.8.4" class="ltx_text"></span> Mutan<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and Block<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>). Since more complex multimodal operations learn salient visual cues by modeling the interaction between visual and semantic features through significantly more parameters; VQA models employing such complex operations benefit less from using complementary QAA features. Overall, it can be seen from Tab. <a href="#S4.T1" title="TABLE I ‣ IV Experiments and Results ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> that all variants of our VQA baseline employing different multimodal operations, with or without optional spatial attention, benefit from using complementary QAA features.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Comparison with state-of-the-art single (not ensemble) VQA models with our proposed QAA model, evaluated on VQAv2 Test-dev and Test-std dataset. The models in (1) are trained with spatial grid features and in (2) with Bottom-Up features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. With QAA, in both cases, our model outperform contemporary VQA models.</figcaption>
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:373.5pt;height:136.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-9.8pt,3.6pt) scale(0.95,0.95) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S4.T2.1.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.3.1" class="ltx_text" style="background-color:#F2F2F2;">Test-dev</span></th>
<th id="S4.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S4.T2.1.1.1.1.4.1" class="ltx_text" style="background-color:#F2F2F2;">Test-Standard</span></th>
</tr>
<tr id="S4.T2.1.1.2.2" class="ltx_tr">
<td id="S4.T2.1.1.2.2.1" class="ltx_td"></td>
<th id="S4.T2.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.1.1.2.2.2.1" class="ltx_text" style="background-color:#F2F2F2;">Model</span></th>
<th id="S4.T2.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.1.1.2.2.3.1" class="ltx_text" style="background-color:#F2F2F2;">All</span></th>
<th id="S4.T2.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.1.1.2.2.4.1" class="ltx_text" style="background-color:#F2F2F2;">All</span></th>
<th id="S4.T2.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.1.1.2.2.5.1" class="ltx_text" style="background-color:#F2F2F2;">Y/N</span></th>
<th id="S4.T2.1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.1.1.2.2.6.1" class="ltx_text" style="background-color:#F2F2F2;">Num.</span></th>
<th id="S4.T2.1.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.1.1.2.2.7.1" class="ltx_text" style="background-color:#F2F2F2;">Other</span></th>
</tr>
<tr id="S4.T2.1.1.3.3" class="ltx_tr">
<td id="S4.T2.1.1.3.3.1" class="ltx_td ltx_align_center" rowspan="3"><span id="S4.T2.1.1.3.3.1.1" class="ltx_text">(1)</span></td>
<td id="S4.T2.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">MCB<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</td>
<td id="S4.T2.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T2.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">62.3</td>
<td id="S4.T2.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">78.8</td>
<td id="S4.T2.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">38.3</td>
<td id="S4.T2.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">53.3</td>
</tr>
<tr id="S4.T2.1.1.4.4" class="ltx_tr">
<td id="S4.T2.1.1.4.4.1" class="ltx_td ltx_align_center">Mutan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</td>
<td id="S4.T2.1.1.4.4.2" class="ltx_td ltx_align_center">63.2</td>
<td id="S4.T2.1.1.4.4.3" class="ltx_td ltx_align_center">63.5</td>
<td id="S4.T2.1.1.4.4.4" class="ltx_td ltx_align_center">80.9</td>
<td id="S4.T2.1.1.4.4.5" class="ltx_td ltx_align_center">38.6</td>
<td id="S4.T2.1.1.4.4.6" class="ltx_td ltx_align_center">54.0</td>
</tr>
<tr id="S4.T2.1.1.5.5" class="ltx_tr">
<td id="S4.T2.1.1.5.5.1" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.5.5.1.1" class="ltx_text" style="background-color:#F5F5DB;">Ours(SG+QAA)</span></td>
<td id="S4.T2.1.1.5.5.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.5.5.2.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">64.7</span></td>
<td id="S4.T2.1.1.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.5.5.3.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">65.0</span></td>
<td id="S4.T2.1.1.5.5.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.5.5.4.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">81.8</span></td>
<td id="S4.T2.1.1.5.5.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.5.5.5.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">43.6</span></td>
<td id="S4.T2.1.1.5.5.6" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.5.5.6.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">55.4</span></td>
</tr>
<tr id="S4.T2.1.1.6.6" class="ltx_tr">
<td id="S4.T2.1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="3"><span id="S4.T2.1.1.6.6.1.1" class="ltx_text">(2)</span></td>
<td id="S4.T2.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_t">Up-Down<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</td>
<td id="S4.T2.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_t">65.3</td>
<td id="S4.T2.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t">65.7</td>
<td id="S4.T2.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_t">82.2</td>
<td id="S4.T2.1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_t">43.9</td>
<td id="S4.T2.1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_t">56.3</td>
</tr>
<tr id="S4.T2.1.1.7.7" class="ltx_tr">
<td id="S4.T2.1.1.7.7.1" class="ltx_td ltx_align_center">Block <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
<td id="S4.T2.1.1.7.7.2" class="ltx_td ltx_align_center">66.4</td>
<td id="S4.T2.1.1.7.7.3" class="ltx_td ltx_align_center">66.9</td>
<td id="S4.T2.1.1.7.7.4" class="ltx_td ltx_align_center">83.8</td>
<td id="S4.T2.1.1.7.7.5" class="ltx_td ltx_align_center">45.7</td>
<td id="S4.T2.1.1.7.7.6" class="ltx_td ltx_align_center">57.1</td>
</tr>
<tr id="S4.T2.1.1.8.8" class="ltx_tr">
<td id="S4.T2.1.1.8.8.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.1.8.8.1.1" class="ltx_text" style="background-color:#F5F5DB;">Ours(BU+QAA)</span></td>
<td id="S4.T2.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.1.8.8.2.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">66.7</span></td>
<td id="S4.T2.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.1.8.8.3.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">67.0</span></td>
<td id="S4.T2.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.1.8.8.4.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">83.8</span></td>
<td id="S4.T2.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.1.8.8.5.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">45.9</span></td>
<td id="S4.T2.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.1.8.8.6.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">57.1</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_italic">Complementary QAA features help answer rare questions more accurately.</span> In Tab. <a href="#S4.T3" title="TABLE III ‣ IV-A Ablation on Different Multimodal Operations ‣ IV Experiments and Results ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, we evaluate our baseline models with and without complementary QAA features on the TDIUC testset and compare it against other state-of-the-art models using spatial grid features.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">The baseline models reported in this table use the optional spatial attention module. We can see that the accuracy for the difficult question categories (<em id="S4.SS1.p4.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS1.p4.1.2" class="ltx_text"></span> Object Utility, Object Presence) increased when using QAA features, and this improvement is more prominent for models using Linear and Concat-MLP operations. Further, for all variants of the baseline model, both versions of Arithmetic and Harmonic MPT improved, and this improvement is more significant for Harmonic MPT and Harmonic N-MPT. This is particularly important as Harmonic MPTs is a more strict metric as it measures the ability of a model to have high scores across ‘<span id="S4.SS1.p4.1.3" class="ltx_text ltx_font_italic">all</span>’ question-types and it consequently puts an emphasis on lowest performing categories. In the last row of Tab. <a href="#S4.T3" title="TABLE III ‣ IV-A Ablation on Different Multimodal Operations ‣ IV Experiments and Results ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, we report the traditional VQA accuracy and observe that the Block variant of our(SG+QAA) model achieves higher accuracy than other state-of-the-art methods. Furthermore, the Concat-MLP model achieves almost same traditional VQA accuracy with or without QAA features (<math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="\sim 84.0" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mrow id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mi id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml"></mi><mo id="S4.SS1.p4.1.m1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.cmml">∼</mo><mn id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml">84.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2">absent</csymbol><cn type="float" id="S4.SS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3">84.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">\sim 84.0</annotation></semantics></math>). Interestingly, one can notice that, even with same VQA accuracy, our model achieves a significant boost in both versions of Arithmetic and Harmonic MPT. These findings support our hypothesis that the QAA features encode salient object-level information that helps consider high-level visual concepts when answering difficult questions.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Evaluation of our QAA models on the testset of TDIUC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> dataset and comparison with state-of-the-art methods. The first 12 rows report the unnormalized accuracy for each question-type. The Arithmetic MPT and Harmonic MPT are unnormalized averages, and Arithmetic N-MPT and Harmonic N-MPT are normalized averages of accuracy scores for all question type. The last row shows the simple VQA accuracy for all models. Using complementary QAA features, the models ability to answer rare questions increased significantly (<em id="S4.T3.23.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.T3.24.2" class="ltx_text"></span> higher Harmonic MPT and N-MPT) for all cases.</figcaption>
<div id="S4.T3.20" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:769.9pt;height:308.7pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-42.8pt,17.1pt) scale(0.9,0.9) ;">
<table id="S4.T3.20.20" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.20.20.21.1" class="ltx_tr">
<th id="S4.T3.20.20.21.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="S4.T3.20.20.21.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.20.20.21.1.2.1" class="ltx_text" style="background-color:#F2F2F2;">MCB</span></td>
<td id="S4.T3.20.20.21.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.20.20.21.1.3.1" class="ltx_text" style="background-color:#F2F2F2;">NMN</span></td>
<td id="S4.T3.20.20.21.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.20.20.21.1.4.1" class="ltx_text" style="background-color:#F2F2F2;">RAU</span></td>
<td id="S4.T3.20.20.21.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.20.20.21.1.5.1" class="ltx_text" style="background-color:#F2F2F2;">Linear</span></td>
<td id="S4.T3.20.20.21.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.20.20.21.1.6.1" class="ltx_text" style="background-color:#F2F2F2;">Ours</span></td>
<td id="S4.T3.20.20.21.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.20.20.21.1.7.1" class="ltx_text" style="background-color:#F2F2F2;">Concat</span></td>
<td id="S4.T3.20.20.21.1.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.20.20.21.1.8.1" class="ltx_text" style="background-color:#F2F2F2;">Ours</span></td>
<td id="S4.T3.20.20.21.1.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.20.20.21.1.9.1" class="ltx_text" style="background-color:#F2F2F2;">Mutan</span></td>
<td id="S4.T3.20.20.21.1.10" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.20.20.21.1.10.1" class="ltx_text" style="background-color:#F2F2F2;">Ours</span></td>
<td id="S4.T3.20.20.21.1.11" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.20.20.21.1.11.1" class="ltx_text" style="background-color:#F2F2F2;">Block</span></td>
<td id="S4.T3.20.20.21.1.12" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.20.20.21.1.12.1" class="ltx_text" style="background-color:#F2F2F2;">Ours</span></td>
</tr>
<tr id="S4.T3.20.20.22.2" class="ltx_tr">
<th id="S4.T3.20.20.22.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T3.20.20.22.2.2" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.20.20.22.2.2.1.1" class="ltx_text" style="background-color:#F2F2F2;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S4.T3.20.20.22.2.2.2.2" class="ltx_text" style="background-color:#F2F2F2;">]</span></cite></td>
<td id="S4.T3.20.20.22.2.3" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.20.20.22.2.3.1.1" class="ltx_text" style="background-color:#F2F2F2;">[</span><a href="#bib.bib25" title="" class="ltx_ref">25</a><span id="S4.T3.20.20.22.2.3.2.2" class="ltx_text" style="background-color:#F2F2F2;">]</span></cite></td>
<td id="S4.T3.20.20.22.2.4" class="ltx_td ltx_align_center"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.20.20.22.2.4.1.1" class="ltx_text" style="background-color:#F2F2F2;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S4.T3.20.20.22.2.4.2.2" class="ltx_text" style="background-color:#F2F2F2;">]</span></cite></td>
<td id="S4.T3.20.20.22.2.5" class="ltx_td"></td>
<td id="S4.T3.20.20.22.2.6" class="ltx_td ltx_align_center"><span id="S4.T3.20.20.22.2.6.1" class="ltx_text" style="background-color:#F2F2F2;">(SG+QAA)</span></td>
<td id="S4.T3.20.20.22.2.7" class="ltx_td ltx_align_center"><span id="S4.T3.20.20.22.2.7.1" class="ltx_text" style="background-color:#F2F2F2;">-MLP</span></td>
<td id="S4.T3.20.20.22.2.8" class="ltx_td ltx_align_center"><span id="S4.T3.20.20.22.2.8.1" class="ltx_text" style="background-color:#F2F2F2;">(SG+QAA)</span></td>
<td id="S4.T3.20.20.22.2.9" class="ltx_td"></td>
<td id="S4.T3.20.20.22.2.10" class="ltx_td ltx_align_center"><span id="S4.T3.20.20.22.2.10.1" class="ltx_text" style="background-color:#F2F2F2;">(SG+QAA)</span></td>
<td id="S4.T3.20.20.22.2.11" class="ltx_td"></td>
<td id="S4.T3.20.20.22.2.12" class="ltx_td ltx_align_center"><span id="S4.T3.20.20.22.2.12.1" class="ltx_text" style="background-color:#F2F2F2;">(SG+QAA)</span></td>
</tr>
<tr id="S4.T3.20.20.23.3" class="ltx_tr">
<th id="S4.T3.20.20.23.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Scene Recog.</th>
<td id="S4.T3.20.20.23.3.2" class="ltx_td ltx_align_center ltx_border_t">93.0</td>
<td id="S4.T3.20.20.23.3.3" class="ltx_td ltx_align_center ltx_border_t">91.9</td>
<td id="S4.T3.20.20.23.3.4" class="ltx_td ltx_align_center ltx_border_t">94.0</td>
<td id="S4.T3.20.20.23.3.5" class="ltx_td ltx_align_center ltx_border_t">50.9</td>
<td id="S4.T3.20.20.23.3.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.23.3.6.1" class="ltx_text" style="background-color:#F5F5DB;">93.1</span></td>
<td id="S4.T3.20.20.23.3.7" class="ltx_td ltx_align_center ltx_border_t">92.5</td>
<td id="S4.T3.20.20.23.3.8" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.23.3.8.1" class="ltx_text" style="background-color:#F5F5DB;">93.0</span></td>
<td id="S4.T3.20.20.23.3.9" class="ltx_td ltx_align_center ltx_border_t">92.2</td>
<td id="S4.T3.20.20.23.3.10" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.23.3.10.1" class="ltx_text" style="background-color:#F5F5DB;">92.4</span></td>
<td id="S4.T3.20.20.23.3.11" class="ltx_td ltx_align_center ltx_border_t">92.8</td>
<td id="S4.T3.20.20.23.3.12" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.23.3.12.1" class="ltx_text" style="background-color:#F5F5DB;">92.8</span></td>
</tr>
<tr id="S4.T3.20.20.24.4" class="ltx_tr">
<th id="S4.T3.20.20.24.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Sport Recog.</th>
<td id="S4.T3.20.20.24.4.2" class="ltx_td ltx_align_center">92.8</td>
<td id="S4.T3.20.20.24.4.3" class="ltx_td ltx_align_center">90.0</td>
<td id="S4.T3.20.20.24.4.4" class="ltx_td ltx_align_center">93.5</td>
<td id="S4.T3.20.20.24.4.5" class="ltx_td ltx_align_center">19.0</td>
<td id="S4.T3.20.20.24.4.6" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.24.4.6.1" class="ltx_text" style="background-color:#F5F5DB;">93.7</span></td>
<td id="S4.T3.20.20.24.4.7" class="ltx_td ltx_align_center">93.4</td>
<td id="S4.T3.20.20.24.4.8" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.24.4.8.1" class="ltx_text" style="background-color:#F5F5DB;">94.1</span></td>
<td id="S4.T3.20.20.24.4.9" class="ltx_td ltx_align_center">93.0</td>
<td id="S4.T3.20.20.24.4.10" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.24.4.10.1" class="ltx_text" style="background-color:#F5F5DB;">92.9</span></td>
<td id="S4.T3.20.20.24.4.11" class="ltx_td ltx_align_center">93.5</td>
<td id="S4.T3.20.20.24.4.12" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.24.4.12.1" class="ltx_text" style="background-color:#F5F5DB;">93.5</span></td>
</tr>
<tr id="S4.T3.20.20.25.5" class="ltx_tr">
<th id="S4.T3.20.20.25.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Color Attributes</th>
<td id="S4.T3.20.20.25.5.2" class="ltx_td ltx_align_center">68.5</td>
<td id="S4.T3.20.20.25.5.3" class="ltx_td ltx_align_center">54.9</td>
<td id="S4.T3.20.20.25.5.4" class="ltx_td ltx_align_center">66.9</td>
<td id="S4.T3.20.20.25.5.5" class="ltx_td ltx_align_center">55.7</td>
<td id="S4.T3.20.20.25.5.6" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.25.5.6.1" class="ltx_text" style="background-color:#F5F5DB;">67.1</span></td>
<td id="S4.T3.20.20.25.5.7" class="ltx_td ltx_align_center">65.4</td>
<td id="S4.T3.20.20.25.5.8" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.25.5.8.1" class="ltx_text" style="background-color:#F5F5DB;">68.2</span></td>
<td id="S4.T3.20.20.25.5.9" class="ltx_td ltx_align_center">66.3</td>
<td id="S4.T3.20.20.25.5.10" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.25.5.10.1" class="ltx_text" style="background-color:#F5F5DB;">66.2</span></td>
<td id="S4.T3.20.20.25.5.11" class="ltx_td ltx_align_center">68.6</td>
<td id="S4.T3.20.20.25.5.12" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.25.5.12.1" class="ltx_text" style="background-color:#F5F5DB;">64.5</span></td>
</tr>
<tr id="S4.T3.20.20.26.6" class="ltx_tr">
<th id="S4.T3.20.20.26.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Other Attributes</th>
<td id="S4.T3.20.20.26.6.2" class="ltx_td ltx_align_center">56.7</td>
<td id="S4.T3.20.20.26.6.3" class="ltx_td ltx_align_center">47.7</td>
<td id="S4.T3.20.20.26.6.4" class="ltx_td ltx_align_center">56.5</td>
<td id="S4.T3.20.20.26.6.5" class="ltx_td ltx_align_center">0.1</td>
<td id="S4.T3.20.20.26.6.6" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.26.6.6.1" class="ltx_text" style="background-color:#F5F5DB;">54.9</span></td>
<td id="S4.T3.20.20.26.6.7" class="ltx_td ltx_align_center">56.3</td>
<td id="S4.T3.20.20.26.6.8" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.26.6.8.1" class="ltx_text" style="background-color:#F5F5DB;">56.4</span></td>
<td id="S4.T3.20.20.26.6.9" class="ltx_td ltx_align_center">52.1</td>
<td id="S4.T3.20.20.26.6.10" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.26.6.10.1" class="ltx_text" style="background-color:#F5F5DB;">52.4</span></td>
<td id="S4.T3.20.20.26.6.11" class="ltx_td ltx_align_center">57.9</td>
<td id="S4.T3.20.20.26.6.12" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.26.6.12.1" class="ltx_text" style="background-color:#F5F5DB;">56.1</span></td>
</tr>
<tr id="S4.T3.20.20.27.7" class="ltx_tr">
<th id="S4.T3.20.20.27.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Activity Recog.</th>
<td id="S4.T3.20.20.27.7.2" class="ltx_td ltx_align_center">52.4</td>
<td id="S4.T3.20.20.27.7.3" class="ltx_td ltx_align_center">44.3</td>
<td id="S4.T3.20.20.27.7.4" class="ltx_td ltx_align_center">51.6</td>
<td id="S4.T3.20.20.27.7.5" class="ltx_td ltx_align_center">0.0</td>
<td id="S4.T3.20.20.27.7.6" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.27.7.6.1" class="ltx_text" style="background-color:#F5F5DB;">50.9</span></td>
<td id="S4.T3.20.20.27.7.7" class="ltx_td ltx_align_center">52.3</td>
<td id="S4.T3.20.20.27.7.8" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.27.7.8.1" class="ltx_text" style="background-color:#F5F5DB;">53.0</span></td>
<td id="S4.T3.20.20.27.7.9" class="ltx_td ltx_align_center">49.3</td>
<td id="S4.T3.20.20.27.7.10" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.27.7.10.1" class="ltx_text" style="background-color:#F5F5DB;">50.2</span></td>
<td id="S4.T3.20.20.27.7.11" class="ltx_td ltx_align_center">53.2</td>
<td id="S4.T3.20.20.27.7.12" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.27.7.12.1" class="ltx_text" style="background-color:#F5F5DB;">52.4</span></td>
</tr>
<tr id="S4.T3.20.20.28.8" class="ltx_tr">
<th id="S4.T3.20.20.28.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Pos. Reasoning</th>
<td id="S4.T3.20.20.28.8.2" class="ltx_td ltx_align_center">35.4</td>
<td id="S4.T3.20.20.28.8.3" class="ltx_td ltx_align_center">27.9</td>
<td id="S4.T3.20.20.28.8.4" class="ltx_td ltx_align_center">35.3</td>
<td id="S4.T3.20.20.28.8.5" class="ltx_td ltx_align_center">7.3</td>
<td id="S4.T3.20.20.28.8.6" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.28.8.6.1" class="ltx_text" style="background-color:#F5F5DB;">33.4</span></td>
<td id="S4.T3.20.20.28.8.7" class="ltx_td ltx_align_center">32.2</td>
<td id="S4.T3.20.20.28.8.8" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.28.8.8.1" class="ltx_text" style="background-color:#F5F5DB;">35.4</span></td>
<td id="S4.T3.20.20.28.8.9" class="ltx_td ltx_align_center">29.4</td>
<td id="S4.T3.20.20.28.8.10" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.28.8.10.1" class="ltx_text" style="background-color:#F5F5DB;">29.9</span></td>
<td id="S4.T3.20.20.28.8.11" class="ltx_td ltx_align_center">36.1</td>
<td id="S4.T3.20.20.28.8.12" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.28.8.12.1" class="ltx_text" style="background-color:#F5F5DB;">34.7</span></td>
</tr>
<tr id="S4.T3.20.20.29.9" class="ltx_tr">
<th id="S4.T3.20.20.29.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Sub-Obj Recog.</th>
<td id="S4.T3.20.20.29.9.2" class="ltx_td ltx_align_center">85.4</td>
<td id="S4.T3.20.20.29.9.3" class="ltx_td ltx_align_center">82.0</td>
<td id="S4.T3.20.20.29.9.4" class="ltx_td ltx_align_center">86.1</td>
<td id="S4.T3.20.20.29.9.5" class="ltx_td ltx_align_center">23.8</td>
<td id="S4.T3.20.20.29.9.6" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.29.9.6.1" class="ltx_text" style="background-color:#F5F5DB;">85.7</span></td>
<td id="S4.T3.20.20.29.9.7" class="ltx_td ltx_align_center">86.1</td>
<td id="S4.T3.20.20.29.9.8" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.29.9.8.1" class="ltx_text" style="background-color:#F5F5DB;">86.5</span></td>
<td id="S4.T3.20.20.29.9.9" class="ltx_td ltx_align_center">85.2</td>
<td id="S4.T3.20.20.29.9.10" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.29.9.10.1" class="ltx_text" style="background-color:#F5F5DB;">85.5</span></td>
<td id="S4.T3.20.20.29.9.11" class="ltx_td ltx_align_center">86.2</td>
<td id="S4.T3.20.20.29.9.12" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.29.9.12.1" class="ltx_text" style="background-color:#F5F5DB;">85.9</span></td>
</tr>
<tr id="S4.T3.20.20.30.10" class="ltx_tr">
<th id="S4.T3.20.20.30.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Absurd</th>
<td id="S4.T3.20.20.30.10.2" class="ltx_td ltx_align_center">84.8</td>
<td id="S4.T3.20.20.30.10.3" class="ltx_td ltx_align_center">87.5</td>
<td id="S4.T3.20.20.30.10.4" class="ltx_td ltx_align_center">96.0</td>
<td id="S4.T3.20.20.30.10.5" class="ltx_td ltx_align_center">90.3</td>
<td id="S4.T3.20.20.30.10.6" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.30.10.6.1" class="ltx_text" style="background-color:#F5F5DB;">88.2</span></td>
<td id="S4.T3.20.20.30.10.7" class="ltx_td ltx_align_center">92.4</td>
<td id="S4.T3.20.20.30.10.8" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.30.10.8.1" class="ltx_text" style="background-color:#F5F5DB;">92.4</span></td>
<td id="S4.T3.20.20.30.10.9" class="ltx_td ltx_align_center">90.0</td>
<td id="S4.T3.20.20.30.10.10" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.30.10.10.1" class="ltx_text" style="background-color:#F5F5DB;">89.1</span></td>
<td id="S4.T3.20.20.30.10.11" class="ltx_td ltx_align_center">90.7</td>
<td id="S4.T3.20.20.30.10.12" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.30.10.12.1" class="ltx_text" style="background-color:#F5F5DB;">92.1</span></td>
</tr>
<tr id="S4.T3.20.20.31.11" class="ltx_tr">
<th id="S4.T3.20.20.31.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Object Utility</th>
<td id="S4.T3.20.20.31.11.2" class="ltx_td ltx_align_center">35.0</td>
<td id="S4.T3.20.20.31.11.3" class="ltx_td ltx_align_center">25.1</td>
<td id="S4.T3.20.20.31.11.4" class="ltx_td ltx_align_center">31.6</td>
<td id="S4.T3.20.20.31.11.5" class="ltx_td ltx_align_center">15.2</td>
<td id="S4.T3.20.20.31.11.6" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.31.11.6.1" class="ltx_text" style="background-color:#F5F5DB;">29.3</span></td>
<td id="S4.T3.20.20.31.11.7" class="ltx_td ltx_align_center">26.2</td>
<td id="S4.T3.20.20.31.11.8" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.31.11.8.1" class="ltx_text" style="background-color:#F5F5DB;">35.7</span></td>
<td id="S4.T3.20.20.31.11.9" class="ltx_td ltx_align_center">27.4</td>
<td id="S4.T3.20.20.31.11.10" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.31.11.10.1" class="ltx_text" style="background-color:#F5F5DB;">30.4</span></td>
<td id="S4.T3.20.20.31.11.11" class="ltx_td ltx_align_center">34.5</td>
<td id="S4.T3.20.20.31.11.12" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.31.11.12.1" class="ltx_text" style="background-color:#F5F5DB;">37.4</span></td>
</tr>
<tr id="S4.T3.20.20.32.12" class="ltx_tr">
<th id="S4.T3.20.20.32.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Object Presence</th>
<td id="S4.T3.20.20.32.12.2" class="ltx_td ltx_align_center">93.6</td>
<td id="S4.T3.20.20.32.12.3" class="ltx_td ltx_align_center">92.5</td>
<td id="S4.T3.20.20.32.12.4" class="ltx_td ltx_align_center">94.4</td>
<td id="S4.T3.20.20.32.12.5" class="ltx_td ltx_align_center">93.5</td>
<td id="S4.T3.20.20.32.12.6" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.32.12.6.1" class="ltx_text" style="background-color:#F5F5DB;">94.3</span></td>
<td id="S4.T3.20.20.32.12.7" class="ltx_td ltx_align_center">94.3</td>
<td id="S4.T3.20.20.32.12.8" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.32.12.8.1" class="ltx_text" style="background-color:#F5F5DB;">94.4</span></td>
<td id="S4.T3.20.20.32.12.9" class="ltx_td ltx_align_center">93.8</td>
<td id="S4.T3.20.20.32.12.10" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.32.12.10.1" class="ltx_text" style="background-color:#F5F5DB;">93.9</span></td>
<td id="S4.T3.20.20.32.12.11" class="ltx_td ltx_align_center">94.1</td>
<td id="S4.T3.20.20.32.12.12" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.32.12.12.1" class="ltx_text" style="background-color:#F5F5DB;">94.2</span></td>
</tr>
<tr id="S4.T3.20.20.33.13" class="ltx_tr">
<th id="S4.T3.20.20.33.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Counting</th>
<td id="S4.T3.20.20.33.13.2" class="ltx_td ltx_align_center">51.0</td>
<td id="S4.T3.20.20.33.13.3" class="ltx_td ltx_align_center">49.2</td>
<td id="S4.T3.20.20.33.13.4" class="ltx_td ltx_align_center">48.4</td>
<td id="S4.T3.20.20.33.13.5" class="ltx_td ltx_align_center">50.1</td>
<td id="S4.T3.20.20.33.13.6" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.33.13.6.1" class="ltx_text" style="background-color:#F5F5DB;">51.2</span></td>
<td id="S4.T3.20.20.33.13.7" class="ltx_td ltx_align_center">53.0</td>
<td id="S4.T3.20.20.33.13.8" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.33.13.8.1" class="ltx_text" style="background-color:#F5F5DB;">52.6</span></td>
<td id="S4.T3.20.20.33.13.9" class="ltx_td ltx_align_center">51.2</td>
<td id="S4.T3.20.20.33.13.10" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.33.13.10.1" class="ltx_text" style="background-color:#F5F5DB;">50.4</span></td>
<td id="S4.T3.20.20.33.13.11" class="ltx_td ltx_align_center">51.1</td>
<td id="S4.T3.20.20.33.13.12" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.33.13.12.1" class="ltx_text" style="background-color:#F5F5DB;">51.2</span></td>
</tr>
<tr id="S4.T3.20.20.34.14" class="ltx_tr">
<th id="S4.T3.20.20.34.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Sentiment Undstd.</th>
<td id="S4.T3.20.20.34.14.2" class="ltx_td ltx_align_center">66.3</td>
<td id="S4.T3.20.20.34.14.3" class="ltx_td ltx_align_center">58.0</td>
<td id="S4.T3.20.20.34.14.4" class="ltx_td ltx_align_center">60.1</td>
<td id="S4.T3.20.20.34.14.5" class="ltx_td ltx_align_center">56.3</td>
<td id="S4.T3.20.20.34.14.6" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.34.14.6.1" class="ltx_text" style="background-color:#F5F5DB;">65.8</span></td>
<td id="S4.T3.20.20.34.14.7" class="ltx_td ltx_align_center">65.7</td>
<td id="S4.T3.20.20.34.14.8" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.34.14.8.1" class="ltx_text" style="background-color:#F5F5DB;">66.3</span></td>
<td id="S4.T3.20.20.34.14.9" class="ltx_td ltx_align_center">63.2</td>
<td id="S4.T3.20.20.34.14.10" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.34.14.10.1" class="ltx_text" style="background-color:#F5F5DB;">61.0</span></td>
<td id="S4.T3.20.20.34.14.11" class="ltx_td ltx_align_center">66.0</td>
<td id="S4.T3.20.20.34.14.12" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.34.14.12.1" class="ltx_text" style="background-color:#F5F5DB;">63.5</span></td>
</tr>
<tr id="S4.T3.4.4.4" class="ltx_tr">
<th id="S4.T3.4.4.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Arithmetic MPT</th>
<td id="S4.T3.4.4.4.6" class="ltx_td ltx_align_center ltx_border_t">67.9</td>
<td id="S4.T3.4.4.4.7" class="ltx_td ltx_align_center ltx_border_t">62.6</td>
<td id="S4.T3.4.4.4.8" class="ltx_td ltx_align_center ltx_border_t">67.8</td>
<td id="S4.T3.4.4.4.9" class="ltx_td ltx_align_center ltx_border_t">38.5</td>
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#F5F5DB;"><span id="S4.T3.1.1.1.1.1" class="ltx_text" style="background-color:#F5F5DB;">68.3(29.8<math id="S4.T3.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.1.1.1.1.1.m1.1a"><mo mathbackground="#F5F5DB" stretchy="false" id="S4.T3.1.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>)</span></td>
<td id="S4.T3.4.4.4.10" class="ltx_td ltx_align_center ltx_border_t">67.6</td>
<td id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#F5F5DB;"><span id="S4.T3.2.2.2.2.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">69.0<span id="S4.T3.2.2.2.2.1.1" class="ltx_text ltx_font_medium">(1.4<math id="S4.T3.2.2.2.2.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.2.2.2.2.1.1.m1.1a"><mo mathbackground="#F5F5DB" stretchy="false" id="S4.T3.2.2.2.2.1.1.m1.1.1" xref="S4.T3.2.2.2.2.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.2.1.1.m1.1b"><ci id="S4.T3.2.2.2.2.1.1.m1.1.1.cmml" xref="S4.T3.2.2.2.2.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.2.1.1.m1.1c">\uparrow</annotation></semantics></math>)</span></span></td>
<td id="S4.T3.4.4.4.11" class="ltx_td ltx_align_center ltx_border_t">66.2</td>
<td id="S4.T3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#F5F5DB;"><span id="S4.T3.3.3.3.3.1" class="ltx_text" style="background-color:#F5F5DB;">66.3(0.1<math id="S4.T3.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.3.3.3.3.1.m1.1a"><mo mathbackground="#F5F5DB" stretchy="false" id="S4.T3.3.3.3.3.1.m1.1.1" xref="S4.T3.3.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.3.1.m1.1b"><ci id="S4.T3.3.3.3.3.1.m1.1.1.cmml" xref="S4.T3.3.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math>)</span></td>
<td id="S4.T3.4.4.4.12" class="ltx_td ltx_align_center ltx_border_t">68.4</td>
<td id="S4.T3.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#F5F5DB;"><span id="S4.T3.4.4.4.4.1" class="ltx_text" style="background-color:#F5F5DB;">68.8(0.4<math id="S4.T3.4.4.4.4.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.4.4.4.4.1.m1.1a"><mo mathbackground="#F5F5DB" stretchy="false" id="S4.T3.4.4.4.4.1.m1.1.1" xref="S4.T3.4.4.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.4.1.m1.1b"><ci id="S4.T3.4.4.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.4.1.m1.1c">\uparrow</annotation></semantics></math>)</span></td>
</tr>
<tr id="S4.T3.8.8.8" class="ltx_tr">
<th id="S4.T3.8.8.8.5" class="ltx_td ltx_align_left ltx_th ltx_th_row">Harmonic MPT</th>
<td id="S4.T3.8.8.8.6" class="ltx_td ltx_align_center">60.5</td>
<td id="S4.T3.8.8.8.7" class="ltx_td ltx_align_center">51.9</td>
<td id="S4.T3.8.8.8.8" class="ltx_td ltx_align_center">59.0</td>
<td id="S4.T3.8.8.8.9" class="ltx_td ltx_align_center">0.0</td>
<td id="S4.T3.5.5.5.1" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.5.5.5.1.1" class="ltx_text" style="background-color:#F5F5DB;">58.1(58.1<math id="S4.T3.5.5.5.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.5.5.5.1.1.m1.1a"><mo mathbackground="#F5F5DB" stretchy="false" id="S4.T3.5.5.5.1.1.m1.1.1" xref="S4.T3.5.5.5.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.1.1.m1.1b"><ci id="S4.T3.5.5.5.1.1.m1.1.1.cmml" xref="S4.T3.5.5.5.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.1.1.m1.1c">\uparrow</annotation></semantics></math>)</span></td>
<td id="S4.T3.8.8.8.10" class="ltx_td ltx_align_center">57.3</td>
<td id="S4.T3.6.6.6.2" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.6.6.6.2.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">61.3<span id="S4.T3.6.6.6.2.1.1" class="ltx_text ltx_font_medium">(4.0<math id="S4.T3.6.6.6.2.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.6.6.6.2.1.1.m1.1a"><mo mathbackground="#F5F5DB" stretchy="false" id="S4.T3.6.6.6.2.1.1.m1.1.1" xref="S4.T3.6.6.6.2.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.2.1.1.m1.1b"><ci id="S4.T3.6.6.6.2.1.1.m1.1.1.cmml" xref="S4.T3.6.6.6.2.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.2.1.1.m1.1c">\uparrow</annotation></semantics></math>)</span></span></td>
<td id="S4.T3.8.8.8.11" class="ltx_td ltx_align_center">55.1</td>
<td id="S4.T3.7.7.7.3" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.7.7.7.3.1" class="ltx_text" style="background-color:#F5F5DB;">56.7(1.6<math id="S4.T3.7.7.7.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.7.7.7.3.1.m1.1a"><mo mathbackground="#F5F5DB" stretchy="false" id="S4.T3.7.7.7.3.1.m1.1.1" xref="S4.T3.7.7.7.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.7.7.7.3.1.m1.1b"><ci id="S4.T3.7.7.7.3.1.m1.1.1.cmml" xref="S4.T3.7.7.7.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.7.7.3.1.m1.1c">\uparrow</annotation></semantics></math>)</span></td>
<td id="S4.T3.8.8.8.12" class="ltx_td ltx_align_center">60.0</td>
<td id="S4.T3.8.8.8.4" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.8.8.8.4.1" class="ltx_text" style="background-color:#F5F5DB;">61.1(1.1<math id="S4.T3.8.8.8.4.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.8.8.8.4.1.m1.1a"><mo mathbackground="#F5F5DB" stretchy="false" id="S4.T3.8.8.8.4.1.m1.1.1" xref="S4.T3.8.8.8.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.8.8.8.4.1.m1.1b"><ci id="S4.T3.8.8.8.4.1.m1.1.1.cmml" xref="S4.T3.8.8.8.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.8.8.4.1.m1.1c">\uparrow</annotation></semantics></math>)</span></td>
</tr>
<tr id="S4.T3.12.12.12" class="ltx_tr">
<th id="S4.T3.12.12.12.5" class="ltx_td ltx_align_left ltx_th ltx_th_row">Arithmetic N-MPT</th>
<td id="S4.T3.12.12.12.6" class="ltx_td ltx_align_center">42.5</td>
<td id="S4.T3.12.12.12.7" class="ltx_td ltx_align_center">34.0</td>
<td id="S4.T3.12.12.12.8" class="ltx_td ltx_align_center">41.0</td>
<td id="S4.T3.12.12.12.9" class="ltx_td ltx_align_center ltx_border_t">29.8</td>
<td id="S4.T3.9.9.9.1" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#F5F5DB;"><span id="S4.T3.9.9.9.1.1" class="ltx_text" style="background-color:#F5F5DB;">54.1(34.3<math id="S4.T3.9.9.9.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.9.9.9.1.1.m1.1a"><mo mathbackground="#F5F5DB" stretchy="false" id="S4.T3.9.9.9.1.1.m1.1.1" xref="S4.T3.9.9.9.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.9.9.9.1.1.m1.1b"><ci id="S4.T3.9.9.9.1.1.m1.1.1.cmml" xref="S4.T3.9.9.9.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.9.9.1.1.m1.1c">\uparrow</annotation></semantics></math>)</span></td>
<td id="S4.T3.12.12.12.10" class="ltx_td ltx_align_center ltx_border_t">53.4</td>
<td id="S4.T3.10.10.10.2" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#F5F5DB;"><span id="S4.T3.10.10.10.2.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">56.4<span id="S4.T3.10.10.10.2.1.1" class="ltx_text ltx_font_medium">(3.0<math id="S4.T3.10.10.10.2.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.10.10.10.2.1.1.m1.1a"><mo mathbackground="#F5F5DB" stretchy="false" id="S4.T3.10.10.10.2.1.1.m1.1.1" xref="S4.T3.10.10.10.2.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.10.10.10.2.1.1.m1.1b"><ci id="S4.T3.10.10.10.2.1.1.m1.1.1.cmml" xref="S4.T3.10.10.10.2.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.10.10.10.2.1.1.m1.1c">\uparrow</annotation></semantics></math>)</span></span></td>
<td id="S4.T3.12.12.12.11" class="ltx_td ltx_align_center ltx_border_t">53.1</td>
<td id="S4.T3.11.11.11.3" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#F5F5DB;"><span id="S4.T3.11.11.11.3.1" class="ltx_text" style="background-color:#F5F5DB;">53.7(0.6<math id="S4.T3.11.11.11.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.11.11.11.3.1.m1.1a"><mo mathbackground="#F5F5DB" stretchy="false" id="S4.T3.11.11.11.3.1.m1.1.1" xref="S4.T3.11.11.11.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.11.11.11.3.1.m1.1b"><ci id="S4.T3.11.11.11.3.1.m1.1.1.cmml" xref="S4.T3.11.11.11.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.11.11.11.3.1.m1.1c">\uparrow</annotation></semantics></math>)</span></td>
<td id="S4.T3.12.12.12.12" class="ltx_td ltx_align_center ltx_border_t">54.7</td>
<td id="S4.T3.12.12.12.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#F5F5DB;"><span id="S4.T3.12.12.12.4.1" class="ltx_text" style="background-color:#F5F5DB;">55.9(1.2<math id="S4.T3.12.12.12.4.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.12.12.12.4.1.m1.1a"><mo mathbackground="#F5F5DB" stretchy="false" id="S4.T3.12.12.12.4.1.m1.1.1" xref="S4.T3.12.12.12.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.12.12.12.4.1.m1.1b"><ci id="S4.T3.12.12.12.4.1.m1.1.1.cmml" xref="S4.T3.12.12.12.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.12.12.12.4.1.m1.1c">\uparrow</annotation></semantics></math>)</span></td>
</tr>
<tr id="S4.T3.16.16.16" class="ltx_tr">
<th id="S4.T3.16.16.16.5" class="ltx_td ltx_align_left ltx_th ltx_th_row">Harmonic N-MPT</th>
<td id="S4.T3.16.16.16.6" class="ltx_td ltx_align_center">27.3</td>
<td id="S4.T3.16.16.16.7" class="ltx_td ltx_align_center">16.7</td>
<td id="S4.T3.16.16.16.8" class="ltx_td ltx_align_center">24.0</td>
<td id="S4.T3.16.16.16.9" class="ltx_td ltx_align_center">0.0</td>
<td id="S4.T3.13.13.13.1" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.13.13.13.1.1" class="ltx_text" style="background-color:#F5F5DB;">32.3(32.3<math id="S4.T3.13.13.13.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.13.13.13.1.1.m1.1a"><mo mathbackground="#F5F5DB" stretchy="false" id="S4.T3.13.13.13.1.1.m1.1.1" xref="S4.T3.13.13.13.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.13.13.13.1.1.m1.1b"><ci id="S4.T3.13.13.13.1.1.m1.1.1.cmml" xref="S4.T3.13.13.13.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.13.13.13.1.1.m1.1c">\uparrow</annotation></semantics></math>)</span></td>
<td id="S4.T3.16.16.16.10" class="ltx_td ltx_align_center">28.2</td>
<td id="S4.T3.14.14.14.2" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.14.14.14.2.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">38.8<span id="S4.T3.14.14.14.2.1.1" class="ltx_text ltx_font_medium">(3.0<math id="S4.T3.14.14.14.2.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.14.14.14.2.1.1.m1.1a"><mo mathbackground="#F5F5DB" stretchy="false" id="S4.T3.14.14.14.2.1.1.m1.1.1" xref="S4.T3.14.14.14.2.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.14.14.14.2.1.1.m1.1b"><ci id="S4.T3.14.14.14.2.1.1.m1.1.1.cmml" xref="S4.T3.14.14.14.2.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.14.14.14.2.1.1.m1.1c">\uparrow</annotation></semantics></math>)</span></span></td>
<td id="S4.T3.16.16.16.11" class="ltx_td ltx_align_center">32.3</td>
<td id="S4.T3.15.15.15.3" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.15.15.15.3.1" class="ltx_text" style="background-color:#F5F5DB;">32.8(0.6<math id="S4.T3.15.15.15.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.15.15.15.3.1.m1.1a"><mo mathbackground="#F5F5DB" stretchy="false" id="S4.T3.15.15.15.3.1.m1.1.1" xref="S4.T3.15.15.15.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.15.15.15.3.1.m1.1b"><ci id="S4.T3.15.15.15.3.1.m1.1.1.cmml" xref="S4.T3.15.15.15.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.15.15.15.3.1.m1.1c">\uparrow</annotation></semantics></math>)</span></td>
<td id="S4.T3.16.16.16.12" class="ltx_td ltx_align_center">34.1</td>
<td id="S4.T3.16.16.16.4" class="ltx_td ltx_align_center" style="background-color:#F5F5DB;"><span id="S4.T3.16.16.16.4.1" class="ltx_text" style="background-color:#F5F5DB;">38.2(1.2<math id="S4.T3.16.16.16.4.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.16.16.16.4.1.m1.1a"><mo mathbackground="#F5F5DB" stretchy="false" id="S4.T3.16.16.16.4.1.m1.1.1" xref="S4.T3.16.16.16.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.16.16.16.4.1.m1.1b"><ci id="S4.T3.16.16.16.4.1.m1.1.1.cmml" xref="S4.T3.16.16.16.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.16.16.16.4.1.m1.1c">\uparrow</annotation></semantics></math>)</span></td>
</tr>
<tr id="S4.T3.20.20.20" class="ltx_tr">
<th id="S4.T3.20.20.20.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Simple Accuracy</th>
<td id="S4.T3.20.20.20.6" class="ltx_td ltx_align_center ltx_border_bb">81.9</td>
<td id="S4.T3.20.20.20.7" class="ltx_td ltx_align_center ltx_border_bb">79.6</td>
<td id="S4.T3.20.20.20.8" class="ltx_td ltx_align_center ltx_border_bb">84.3</td>
<td id="S4.T3.20.20.20.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">72.9</td>
<td id="S4.T3.17.17.17.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="background-color:#F5F5DB;"><span id="S4.T3.17.17.17.1.1" class="ltx_text" style="background-color:#F5F5DB;">82.6(9.7<math id="S4.T3.17.17.17.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.17.17.17.1.1.m1.1a"><mo mathbackground="#F5F5DB" stretchy="false" id="S4.T3.17.17.17.1.1.m1.1.1" xref="S4.T3.17.17.17.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.17.17.17.1.1.m1.1b"><ci id="S4.T3.17.17.17.1.1.m1.1.1.cmml" xref="S4.T3.17.17.17.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.17.17.17.1.1.m1.1c">\uparrow</annotation></semantics></math>)</span></td>
<td id="S4.T3.20.20.20.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">83.9</td>
<td id="S4.T3.18.18.18.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="background-color:#F5F5DB;"><span id="S4.T3.18.18.18.2.1" class="ltx_text" style="background-color:#F5F5DB;">84.0(0.1<math id="S4.T3.18.18.18.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.18.18.18.2.1.m1.1a"><mo mathbackground="#F5F5DB" stretchy="false" id="S4.T3.18.18.18.2.1.m1.1.1" xref="S4.T3.18.18.18.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.18.18.18.2.1.m1.1b"><ci id="S4.T3.18.18.18.2.1.m1.1.1.cmml" xref="S4.T3.18.18.18.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.18.18.18.2.1.m1.1c">\uparrow</annotation></semantics></math>)</span></td>
<td id="S4.T3.20.20.20.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">82.5</td>
<td id="S4.T3.19.19.19.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="background-color:#F5F5DB;"><span id="S4.T3.19.19.19.3.1" class="ltx_text" style="background-color:#F5F5DB;">82.7(0.2<math id="S4.T3.19.19.19.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.19.19.19.3.1.m1.1a"><mo mathbackground="#F5F5DB" stretchy="false" id="S4.T3.19.19.19.3.1.m1.1.1" xref="S4.T3.19.19.19.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.19.19.19.3.1.m1.1b"><ci id="S4.T3.19.19.19.3.1.m1.1.1.cmml" xref="S4.T3.19.19.19.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.19.19.19.3.1.m1.1c">\uparrow</annotation></semantics></math>)</span></td>
<td id="S4.T3.20.20.20.12" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">84.5</td>
<td id="S4.T3.20.20.20.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="background-color:#F5F5DB;"><span id="S4.T3.20.20.20.4.1" class="ltx_text ltx_font_bold" style="background-color:#F5F5DB;">84.6<span id="S4.T3.20.20.20.4.1.1" class="ltx_text ltx_font_medium">(0.1<math id="S4.T3.20.20.20.4.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.20.20.20.4.1.1.m1.1a"><mo mathbackground="#F5F5DB" stretchy="false" id="S4.T3.20.20.20.4.1.1.m1.1.1" xref="S4.T3.20.20.20.4.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.20.20.20.4.1.1.m1.1b"><ci id="S4.T3.20.20.20.4.1.1.m1.1.1.cmml" xref="S4.T3.20.20.20.4.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.20.20.20.4.1.1.m1.1c">\uparrow</annotation></semantics></math>)</span></span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Inference with Global Representation</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.7" class="ltx_p">We further experiment with Image-Question-Agnostic Attention (IQAA) where the attention feature is generated without looking at the input question <span id="S4.SS2.p1.7.1" class="ltx_text ltx_font_italic">and</span> image. To do so, <span id="S4.SS2.p1.7.2" class="ltx_text ltx_font_bold">first</span>, we create a global representation of <span id="S4.SS2.p1.7.3" class="ltx_text ltx_font_italic">object maps</span> by counting object presence at each spatial grid location for all images in the dataset. In Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-B Inference with Global Representation ‣ IV Experiments and Results ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we show such a global representation from the count of object presence, <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{C}\in\mathbb{R}^{14\times 14}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">𝒞</mi><mo id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml"><mi id="S4.SS2.p1.1.m1.1.1.3.2" xref="S4.SS2.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS2.p1.1.m1.1.1.3.3" xref="S4.SS2.p1.1.m1.1.1.3.3.cmml"><mn id="S4.SS2.p1.1.m1.1.1.3.3.2" xref="S4.SS2.p1.1.m1.1.1.3.3.2.cmml">14</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.1.m1.1.1.3.3.1" xref="S4.SS2.p1.1.m1.1.1.3.3.1.cmml">×</mo><mn id="S4.SS2.p1.1.m1.1.1.3.3.3" xref="S4.SS2.p1.1.m1.1.1.3.3.3.cmml">14</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><in id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></in><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝒞</ci><apply id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.3.2.cmml" xref="S4.SS2.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S4.SS2.p1.1.m1.1.1.3.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3.3"><times id="S4.SS2.p1.1.m1.1.1.3.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.3.3.1"></times><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.3.2.cmml" xref="S4.SS2.p1.1.m1.1.1.3.3.2">14</cn><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.3.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3.3.3">14</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\mathcal{C}\in\mathbb{R}^{14\times 14}</annotation></semantics></math>, of VQA dataset training images (<em id="S4.SS2.p1.7.4" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS2.p1.7.5" class="ltx_text"></span> COCO trainset 2014 images) on a <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="14\times 14" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mn id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">14</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">14</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><times id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">14</cn><cn type="integer" id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">14</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">14\times 14</annotation></semantics></math> grid. We can see from this figure that most objects present in an image occupy the center grids. We leverage this centre bias to create fixed <span id="S4.SS2.p1.7.6" class="ltx_text ltx_font_italic">object maps</span>, that in turn is used to generate IQAA features. <span id="S4.SS2.p1.7.7" class="ltx_text ltx_font_bold">Second</span>, the count vector is min-max normalized between <math id="S4.SS2.p1.3.m3.2" class="ltx_Math" alttext="[0,1]" display="inline"><semantics id="S4.SS2.p1.3.m3.2a"><mrow id="S4.SS2.p1.3.m3.2.3.2" xref="S4.SS2.p1.3.m3.2.3.1.cmml"><mo stretchy="false" id="S4.SS2.p1.3.m3.2.3.2.1" xref="S4.SS2.p1.3.m3.2.3.1.cmml">[</mo><mn id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml">0</mn><mo id="S4.SS2.p1.3.m3.2.3.2.2" xref="S4.SS2.p1.3.m3.2.3.1.cmml">,</mo><mn id="S4.SS2.p1.3.m3.2.2" xref="S4.SS2.p1.3.m3.2.2.cmml">1</mn><mo stretchy="false" id="S4.SS2.p1.3.m3.2.3.2.3" xref="S4.SS2.p1.3.m3.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.2b"><interval closure="closed" id="S4.SS2.p1.3.m3.2.3.1.cmml" xref="S4.SS2.p1.3.m3.2.3.2"><cn type="integer" id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">0</cn><cn type="integer" id="S4.SS2.p1.3.m3.2.2.cmml" xref="S4.SS2.p1.3.m3.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.2c">[0,1]</annotation></semantics></math> (x-axis of Fig. <a href="#S4.F3" title="Figure 3 ‣ IV-B Inference with Global Representation ‣ IV Experiments and Results ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). The left y-axis shows the number of grid locations selected when applying different thresholds on the normalized count measures. It ranges from <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="191" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mn id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml">191</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><cn type="integer" id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">191</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">191</annotation></semantics></math> to <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="22" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><mn id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml">22</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><cn type="integer" id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1">22</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">22</annotation></semantics></math> grid locations when the threshold is varied between <math id="S4.SS2.p1.6.m6.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="S4.SS2.p1.6.m6.1a"><mn id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><cn type="float" id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">0.1</annotation></semantics></math> to <math id="S4.SS2.p1.7.m7.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="S4.SS2.p1.7.m7.1a"><mn id="S4.SS2.p1.7.m7.1.1" xref="S4.SS2.p1.7.m7.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.7.m7.1b"><cn type="float" id="S4.SS2.p1.7.m7.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.7.m7.1c">0.9</annotation></semantics></math>. <span id="S4.SS2.p1.7.8" class="ltx_text ltx_font_bold">Third</span>, we treat the selected grid location for a set threshold as fixed <span id="S4.SS2.p1.7.9" class="ltx_text ltx_font_italic">object maps</span> and apply fixed map on the input visual feature as discussed in Sec. <a href="#S3.SS1" title="III-A Question-Agnostic Attention ‣ III Method ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a> for generating IQAA features. These IQAA features can be used instead of QAA features in a similar fashion to train any VQA model.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/1908.03289/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="258" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>VQA accuracy (right y-axis) using complementary Image-Question-Agnostic Attention (IQAA) Features. IQAA feature is generated by selecting a global representation threshold (x-axis) and corresponding spatial grid locations (left y-axis).
</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/1908.03289/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="322" height="261" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Global Representation of <span id="S4.F4.4.1" class="ltx_text ltx_font_italic">object maps</span>: Count of object presence at each of the <math id="S4.F4.2.m1.1" class="ltx_Math" alttext="196~{}(14{\times}14)" display="inline"><semantics id="S4.F4.2.m1.1b"><mrow id="S4.F4.2.m1.1.1" xref="S4.F4.2.m1.1.1.cmml"><mn id="S4.F4.2.m1.1.1.3" xref="S4.F4.2.m1.1.1.3.cmml">196</mn><mo lspace="0.330em" rspace="0em" id="S4.F4.2.m1.1.1.2" xref="S4.F4.2.m1.1.1.2.cmml">​</mo><mrow id="S4.F4.2.m1.1.1.1.1" xref="S4.F4.2.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.F4.2.m1.1.1.1.1.2" xref="S4.F4.2.m1.1.1.1.1.1.cmml">(</mo><mrow id="S4.F4.2.m1.1.1.1.1.1" xref="S4.F4.2.m1.1.1.1.1.1.cmml"><mn id="S4.F4.2.m1.1.1.1.1.1.2" xref="S4.F4.2.m1.1.1.1.1.1.2.cmml">14</mn><mo lspace="0.222em" rspace="0.222em" id="S4.F4.2.m1.1.1.1.1.1.1" xref="S4.F4.2.m1.1.1.1.1.1.1.cmml">×</mo><mn id="S4.F4.2.m1.1.1.1.1.1.3" xref="S4.F4.2.m1.1.1.1.1.1.3.cmml">14</mn></mrow><mo stretchy="false" id="S4.F4.2.m1.1.1.1.1.3" xref="S4.F4.2.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.F4.2.m1.1c"><apply id="S4.F4.2.m1.1.1.cmml" xref="S4.F4.2.m1.1.1"><times id="S4.F4.2.m1.1.1.2.cmml" xref="S4.F4.2.m1.1.1.2"></times><cn type="integer" id="S4.F4.2.m1.1.1.3.cmml" xref="S4.F4.2.m1.1.1.3">196</cn><apply id="S4.F4.2.m1.1.1.1.1.1.cmml" xref="S4.F4.2.m1.1.1.1.1"><times id="S4.F4.2.m1.1.1.1.1.1.1.cmml" xref="S4.F4.2.m1.1.1.1.1.1.1"></times><cn type="integer" id="S4.F4.2.m1.1.1.1.1.1.2.cmml" xref="S4.F4.2.m1.1.1.1.1.1.2">14</cn><cn type="integer" id="S4.F4.2.m1.1.1.1.1.1.3.cmml" xref="S4.F4.2.m1.1.1.1.1.1.3">14</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.2.m1.1d">196~{}(14{\times}14)</annotation></semantics></math> spatial grid locations generated from the training images of VQA dataset.</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/1908.03289/assets/x5.png" id="S4.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="298" height="112" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Qualitative results on VQAv2 val-set to demonstrate the effectiveness of using complementary QAA. The learned spatial attention (<math id="S4.F5.4.m1.1" class="ltx_Math" alttext="2^{nd}" display="inline"><semantics id="S4.F5.4.m1.1b"><msup id="S4.F5.4.m1.1.1" xref="S4.F5.4.m1.1.1.cmml"><mn id="S4.F5.4.m1.1.1.2" xref="S4.F5.4.m1.1.1.2.cmml">2</mn><mrow id="S4.F5.4.m1.1.1.3" xref="S4.F5.4.m1.1.1.3.cmml"><mi id="S4.F5.4.m1.1.1.3.2" xref="S4.F5.4.m1.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.F5.4.m1.1.1.3.1" xref="S4.F5.4.m1.1.1.3.1.cmml">​</mo><mi id="S4.F5.4.m1.1.1.3.3" xref="S4.F5.4.m1.1.1.3.3.cmml">d</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.F5.4.m1.1c"><apply id="S4.F5.4.m1.1.1.cmml" xref="S4.F5.4.m1.1.1"><csymbol cd="ambiguous" id="S4.F5.4.m1.1.1.1.cmml" xref="S4.F5.4.m1.1.1">superscript</csymbol><cn type="integer" id="S4.F5.4.m1.1.1.2.cmml" xref="S4.F5.4.m1.1.1.2">2</cn><apply id="S4.F5.4.m1.1.1.3.cmml" xref="S4.F5.4.m1.1.1.3"><times id="S4.F5.4.m1.1.1.3.1.cmml" xref="S4.F5.4.m1.1.1.3.1"></times><ci id="S4.F5.4.m1.1.1.3.2.cmml" xref="S4.F5.4.m1.1.1.3.2">𝑛</ci><ci id="S4.F5.4.m1.1.1.3.3.cmml" xref="S4.F5.4.m1.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.4.m1.1d">2^{nd}</annotation></semantics></math> and <math id="S4.F5.5.m2.1" class="ltx_Math" alttext="5^{th}" display="inline"><semantics id="S4.F5.5.m2.1b"><msup id="S4.F5.5.m2.1.1" xref="S4.F5.5.m2.1.1.cmml"><mn id="S4.F5.5.m2.1.1.2" xref="S4.F5.5.m2.1.1.2.cmml">5</mn><mrow id="S4.F5.5.m2.1.1.3" xref="S4.F5.5.m2.1.1.3.cmml"><mi id="S4.F5.5.m2.1.1.3.2" xref="S4.F5.5.m2.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.F5.5.m2.1.1.3.1" xref="S4.F5.5.m2.1.1.3.1.cmml">​</mo><mi id="S4.F5.5.m2.1.1.3.3" xref="S4.F5.5.m2.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.F5.5.m2.1c"><apply id="S4.F5.5.m2.1.1.cmml" xref="S4.F5.5.m2.1.1"><csymbol cd="ambiguous" id="S4.F5.5.m2.1.1.1.cmml" xref="S4.F5.5.m2.1.1">superscript</csymbol><cn type="integer" id="S4.F5.5.m2.1.1.2.cmml" xref="S4.F5.5.m2.1.1.2">5</cn><apply id="S4.F5.5.m2.1.1.3.cmml" xref="S4.F5.5.m2.1.1.3"><times id="S4.F5.5.m2.1.1.3.1.cmml" xref="S4.F5.5.m2.1.1.3.1"></times><ci id="S4.F5.5.m2.1.1.3.2.cmml" xref="S4.F5.5.m2.1.1.3.2">𝑡</ci><ci id="S4.F5.5.m2.1.1.3.3.cmml" xref="S4.F5.5.m2.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.5.m2.1d">5^{th}</annotation></semantics></math> columns) focuses on regions with or without objects, but QAA map is focused on objects.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.5" class="ltx_p"><em id="S4.SS2.p2.5.1" class="ltx_emph ltx_font_italic">By only using complementary IQAA features VQA models achieve reasonable performance.</em> We report the VQA accuracy score on VQAv2 validation set using our Block baseline model without spatial attention on the right y-axis of Fig. <a href="#S4.F3" title="Figure 3 ‣ IV-B Inference with Global Representation ‣ IV Experiments and Results ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
When using IQAA features, the VQA accuracy ranges from <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="53" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mn id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">53</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><cn type="integer" id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">53</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">53</annotation></semantics></math> to <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="56" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mn id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">56</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><cn type="integer" id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">56</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">56</annotation></semantics></math> when the global representation threshold is varied between <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mn id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><cn type="float" id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">0.1</annotation></semantics></math> to <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mn id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><cn type="float" id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">0.9</annotation></semantics></math>. This means if one selects a fixed set of <math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="24" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><mn id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml">24</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><cn type="integer" id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1">24</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">24</annotation></semantics></math> spatial grid locations at the center of the image, and trains a state-of-the-art VQA model with visual features of only these grid locations; the model can still achieve VQA accuracy comparable to when it looks at the whole image.
A similar finding was reported by Judd <span id="S4.SS2.p2.5.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> where they show that humans tend to focus the object at the center when they take picture. Our finding further adds to that notion of <span id="S4.SS2.p2.5.3" class="ltx_text ltx_font_italic">Center Prior</span> by showing that humans also tend to ask questions about objects that are at the center of the image. By modeling the object presence prior in the dataset, one can achieve reasonable performance, without considering image specific object map.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Evaluation on the VQAv2 Testset</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We evaluate our model’s performance on the VQAv2 Test server and report accuracy for different question types on the Test-dev and Test-std dataset to compare with other contemporary state-of-the-art VQA models. For fair comparison, in Tab. <a href="#S4.T2" title="TABLE II ‣ IV-A Ablation on Different Multimodal Operations ‣ IV Experiments and Results ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> (1), we separate models that use spatial grid features (<em id="S4.SS3.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS3.p1.1.2" class="ltx_text"></span> visual features extracted by ResNet) and compare it with our SG+QAA model; in (2) the models that use Bottom-Up <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> features and we compare our BU+QAA model. For both cases, our question-agnostic models employ Block fusion to jointly embed image and question features with a spatial attention mechanism. From Tab. <a href="#S4.T2" title="TABLE II ‣ IV-A Ablation on Different Multimodal Operations ‣ IV Experiments and Results ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we can see that when the QAA features are used alongside spatial grid features, the gain is more, compared to when used with BU features. As the BU features are a collection of top object bounding box features generated using Faster-RCNN, it also offers some object-level information to the VQA model. Thus, when used in combination with BU features, the overall performance gain in relatively small. However, as the question-agnostic features encompass the <span id="S4.SS3.p1.1.3" class="ltx_text ltx_font_italic">Object Map</span> of an image, it somewhat encodes the global spatial relationship between object and count information of object instances; it provides accuracy gain when answering <span id="S4.SS3.p1.1.4" class="ltx_text ltx_font_italic">Number</span> (<em id="S4.SS3.p1.1.5" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS3.p1.1.6" class="ltx_text"></span> <span id="S4.SS3.p1.1.7" class="ltx_text ltx_font_italic">‘How many?’</span>) question (<math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="0.2\%\uparrow" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mrow id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2.2" xref="S4.SS3.p1.1.m1.1.1.2.2.cmml">0.2</mn><mo id="S4.SS3.p1.1.m1.1.1.2.1" xref="S4.SS3.p1.1.m1.1.1.2.1.cmml">%</mo></mrow><mo stretchy="false" id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">↑</mo><mi id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><ci id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1">↑</ci><apply id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2"><csymbol cd="latexml" id="S4.SS3.p1.1.m1.1.1.2.1.cmml" xref="S4.SS3.p1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="S4.SS3.p1.1.m1.1.1.2.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2.2">0.2</cn></apply><csymbol cd="latexml" id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">0.2\%\uparrow</annotation></semantics></math> in test-standard). Overall, if a parallel branch trained using question-agnostic features is added to an existing VQA model, accuracy of the model increases.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Qualitative Results</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We present qualitative results of our SG+QAA model with Block fusion in Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-B Inference with Global Representation ‣ IV Experiments and Results ‣ Question-Agnostic Attention for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> to showcase the effectiveness of having complementary question-agnostic features. In the second row, first example, the model is tasked with a count question, asking <span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_italic">‘How many animals are on the grass?’</span> The learned spatial attention map is scattered in different image locations whereas the question-agnostic feature localizes five object instances that help the model answer correctly. Altogether, from the qualitative results, we can deduce that learned and question-agnostic attention provides complementary information which can be leveraged by VQA models to be able to correctly answer intelligent questions.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we introduced Question-Agnostic Attention that can be used to augment existing VQA approaches. Rather than using computationally intensive methods to learn question-specific attention, our approach derives attention only from the image, based on the insight that questions generally relate to object instances. We use an object parsing model to automatically generate an <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">Object Map</span>, that has the same resolution as the feature map from a pre-existing classification network. The <span id="S5.p1.1.2" class="ltx_text ltx_font_italic">Object Map</span> is used to mask the convolutional feature map to generate question-agnostic attention features. When high-performing computationally-intensive VQA models are augmented with QAA, it improves their accuracy to be a new state-of-the-art. When simple linear models are augmented with QAA, they preform significantly better when answering question that require a higher level of visual reasoning (<em id="S5.p1.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.p1.1.4" class="ltx_text"></span> activity recognition), which a simplistic model cannot learn on its own. This capability provides the simplistic (low-complexity) models a significant boost that brings them close to state-of-the-art.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
H. Ben-Younes, R. Cadene, N. Thome, and M. Cord, “Block: Bilinear
Superdiagonal Fusion for Visual Question Answering and Visual
Relationship Detection,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">The Thirty-Third AAAI Conference on
Artificial Intelligence</em>, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick, and
D. Parikh, “Vqa: Visual question answering,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE International Conference on Computer Vision</em>, 2015, pp. 2425–2433.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and M. Rohrbach,
“Multimodal compact bilinear pooling for visual question answering and
visual grounding,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1606.01847</em>, 2016.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Z. Yang, X. He, J. Gao, L. Deng, and A. Smola, “Stacked attention networks for
image question answering,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition</em>, 2016, pp. 21–29.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang,
“Bottom-up and top-down attention for image captioning and visual question
answering,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
M. R. Farazi and S. Khan, “Reciprocal attention fusion for visual question
answering,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">The British Machine Vision Conference (BMVC)</em>,
September 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
P. Lu, H. Li, W. Zhang, J. Wang, and X. Wang, “Co-attending free-form regions
and detections with multi-modal multiplicative feature embedding for visual
question answering,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Thirty-Second AAAI Conference on Artificial
Intelligence</em>, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
H. Ben-Younes, R. Cadene, M. Cord, and N. Thome, “Mutan: Multimodal tucker
fusion for visual question answering,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
international conference on computer vision</em>, 2017, pp. 2612–2620.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. Lu, J. Yang, D. Batra, and D. Parikh, “Hierarchical question-image
co-attention for visual question answering,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Advances In Neural
Information Processing Systems</em>, 2016, pp. 289–297.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M. R. Farazi, S. H. Khan, and N. Barnes, “From known to the unknown:
Transferring knowledge to answer questions about novel visual and semantic
concepts,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Image and Vision Computing</em>, vol. 103, p. 103985, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition</em>, 2016, pp. 770–778.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Advances in neural
information processing systems</em>, 2015, pp. 91–99.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Y. Zhang, J. Hare, and A. Prügel-Bennett, “Learning to count objects in
natural images for visual question answering,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">International
Conference on Learning Representations</em>, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
K. J. Shih, S. Singh, and D. Hoiem, “Where to look: Focus regions for visual
question answering,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition</em>, 2016, pp. 4613–4621.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Das, H. Agrawal, L. Zitnick, D. Parikh, and D. Batra, “Human attention in
visual question answering: Do humans and deep networks look at the same
regions?” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Image Understanding</em>, vol. 163, pp.
90–100, 2017.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
T. Judd, K. Ehinger, F. Durand, and A. Torralba, “Learning to predict where
humans look,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2009 IEEE 12th international conference on computer
vision</em>.   IEEE, 2009, pp. 2106–2113.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A
large-scale hierarchical image database,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Conference on</em>.   IEEE, 2009, pp. 248–255.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J. Pennington, R. Socher, and C. Manning, “Glove: Global vectors for word
representation,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2014 conference on empirical
methods in natural language processing (EMNLP)</em>, 2014, pp. 1532–1543.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, and
S. Fidler, “Skip-thought vectors,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Advances in neural information
processing systems</em>, 2015, pp. 3294–3302.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh, “Making the v in
vqa matter: Elevating the role of image understanding in visual question
answering,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, July 2017.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>.   Springer, 2014, pp. 740–755.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
K. Kafle and C. Kanan, “An analysis of visual question answering algorithms,”
in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2017.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in
<em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>,
2017, pp. 2961–2969.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Z. Yu, J. Yu, C. Xiang, J. Fan, and D. Tao, “Beyond bilinear: Generalized
multimodal factorized high-order pooling for visual question answering,”
<em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>, 2018.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
J. Andreas, M. Rohrbach, T. Darrell, and D. Klein, “Neural module networks,”
in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition</em>, 2016, pp. 39–48.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
H. Noh and B. Han, “Training recurrent answering units with joint loss
minimization for vqa,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1606.03647</em>, 2016.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1908.03288" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1908.03289" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1908.03289">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1908.03289" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1908.03290" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar  7 23:44:51 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
