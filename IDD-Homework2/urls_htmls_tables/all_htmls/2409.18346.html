<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.18346] MultiClimate: Multimodal Stance Detection on Climate Change Videos</title><meta property="og:description" content="Climate change (CC) has attracted increasing attention in NLP in recent years. However, detecting the stance on CC in multimodal data is understudied and remains challenging due to a lack of reliable datasets. To impro…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="MultiClimate: Multimodal Stance Detection on Climate Change Videos">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="MultiClimate: Multimodal Stance Detection on Climate Change Videos">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.18346">

<!--Generated on Sat Oct  5 23:10:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">MultiClimate: Multimodal Stance Detection on Climate Change Videos</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id1.1.1" class="ltx_text ltx_font_bold">Jiawen Wang<sup id="id1.1.1.1" class="ltx_sup"><span id="id1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">♢,∗</span></sup></span>  <span id="id2.2.2" class="ltx_text ltx_font_bold">Longfei Zuo<sup id="id2.2.2.1" class="ltx_sup"><span id="id2.2.2.1.1" class="ltx_text ltx_font_medium ltx_font_italic">♢,∗</span></sup></span>  <span id="id3.3.3" class="ltx_text ltx_font_bold">Siyao Peng<sup id="id3.3.3.1" class="ltx_sup"><span id="id3.3.3.1.1" class="ltx_text ltx_font_medium ltx_font_italic">♢,†</span></sup></span>  <span id="id5.5.5" class="ltx_text ltx_font_bold">Barbara Plank<sup id="id5.5.5.1" class="ltx_sup"><span id="id5.5.5.1.1" class="ltx_text ltx_font_medium ltx_font_italic">♢,†</span></sup>
<br class="ltx_break"><sup id="id5.5.5.2" class="ltx_sup"><span id="id5.5.5.2.1" class="ltx_text ltx_font_medium">♢</span></sup></span>Center for Information and Language Processing
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> LMU Munich
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Germany
<br class="ltx_break"><sup id="id11.2.id1" class="ltx_sup">†</sup>MaiNLP &amp; MCML
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> LMU Munich
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Germany
<br class="ltx_break">{jiawen.wang, zuo.longfei}@campus.lmu.de  {siyao.peng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> b.plank}@lmu.de
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id10.4" class="ltx_p">Climate change (CC) has attracted increasing attention in NLP in recent years. However, detecting the stance on CC in multimodal data is understudied and remains challenging due to a lack of reliable datasets. To improve the understanding of public opinions and communication strategies, this paper presents MultiClimate, the first open-source manually-annotated stance detection dataset with <math id="id7.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="id7.1.m1.1a"><mn id="id7.1.m1.1.1" xref="id7.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="id7.1.m1.1b"><cn type="integer" id="id7.1.m1.1.1.cmml" xref="id7.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="id7.1.m1.1c">100</annotation></semantics></math> CC-related YouTube videos and <math id="id8.2.m2.2" class="ltx_Math" alttext="4,209" display="inline"><semantics id="id8.2.m2.2a"><mrow id="id8.2.m2.2.3.2" xref="id8.2.m2.2.3.1.cmml"><mn id="id8.2.m2.1.1" xref="id8.2.m2.1.1.cmml">4</mn><mo id="id8.2.m2.2.3.2.1" xref="id8.2.m2.2.3.1.cmml">,</mo><mn id="id8.2.m2.2.2" xref="id8.2.m2.2.2.cmml">209</mn></mrow><annotation-xml encoding="MathML-Content" id="id8.2.m2.2b"><list id="id8.2.m2.2.3.1.cmml" xref="id8.2.m2.2.3.2"><cn type="integer" id="id8.2.m2.1.1.cmml" xref="id8.2.m2.1.1">4</cn><cn type="integer" id="id8.2.m2.2.2.cmml" xref="id8.2.m2.2.2">209</cn></list></annotation-xml><annotation encoding="application/x-tex" id="id8.2.m2.2c">4,209</annotation></semantics></math> frame-transcript pairs. We deploy state-of-the-art vision and language models, as well as multimodal models for MultiClimate stance detection. Results show that text-only BERT significantly outperforms image-only ResNet50 and ViT. Combining both modalities achieves state-of-the-art, <math id="id9.3.m3.1" class="ltx_Math" alttext="0.747" display="inline"><semantics id="id9.3.m3.1a"><mn id="id9.3.m3.1.1" xref="id9.3.m3.1.1.cmml">0.747</mn><annotation-xml encoding="MathML-Content" id="id9.3.m3.1b"><cn type="float" id="id9.3.m3.1.1.cmml" xref="id9.3.m3.1.1">0.747</cn></annotation-xml><annotation encoding="application/x-tex" id="id9.3.m3.1c">0.747</annotation></semantics></math>/<math id="id10.4.m4.1" class="ltx_Math" alttext="0.749" display="inline"><semantics id="id10.4.m4.1a"><mn id="id10.4.m4.1.1" xref="id10.4.m4.1.1.cmml">0.749</mn><annotation-xml encoding="MathML-Content" id="id10.4.m4.1b"><cn type="float" id="id10.4.m4.1.1.cmml" xref="id10.4.m4.1.1">0.749</cn></annotation-xml><annotation encoding="application/x-tex" id="id10.4.m4.1c">0.749</annotation></semantics></math> in accuracy/F1. Our 100M-sized fusion models also beat CLIP and BLIP, as well as the much larger 9B-sized multimodal IDEFICS and text-only Llama3 and Gemma2, indicating that multimodal stance detection remains challenging for large language models. Our code, dataset, as well as supplementary materials, are available at <a target="_blank" href="https://github.com/werywjw/MultiClimate" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/werywjw/MultiClimate</a>.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_note_type">footnotetext: </span>Equal contributions.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">As climate change (CC) gains global attention, measuring human stance towards CC becomes increasingly important.
Numerous large language models (LLMs) and deep learning models have been developed to address these challenges. These models can help detect public opinions and assist stakeholders to improve decision-making, thus providing valuable insights into public perception regarding climate change <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a href="#bib.bib24" title="" class="ltx_ref">2024</a>); Meta AI (<a href="#bib.bib21" title="" class="ltx_ref">2024</a>); Dosovitskiy et al. (<a href="#bib.bib8" title="" class="ltx_ref">2021</a>); Alayrac et al. (<a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Stance detection is a task to determine whether authors of a document support, oppose, or take a neutral stance toward a specific target <cite class="ltx_cite ltx_citemacro_cite">Mohammad et al. (<a href="#bib.bib22" title="" class="ltx_ref">2016</a>); Hardalov et al. (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>); Weinzierl and Harabagiu (<a href="#bib.bib33" title="" class="ltx_ref">2023b</a>)</cite>.
It enhances information management by efficiently categorizing diverse opinions. Identifying varying public opinions helps promote societal understanding and communication, thus reducing conflicts and enhancing public discourse.
Previous research explored stance detection on climate change <cite class="ltx_cite ltx_citemacro_cite">Maynard and Bontcheva (<a href="#bib.bib19" title="" class="ltx_ref">2015</a>); Vaid et al. (<a href="#bib.bib30" title="" class="ltx_ref">2022</a>); Upadhyaya et al. (<a href="#bib.bib28" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib29" title="" class="ltx_ref">b</a>)</cite>, but focusing on text-only data.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2409.18346/assets/figs/WCCA-002.jpg" id="S1.F1.sf1.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="281" height="159" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2409.18346/assets/figs/WCCA-003.jpg" id="S1.F1.sf1.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="281" height="159" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span><span id="S1.F1.sf1.5.1" class="ltx_text ltx_ulem_uline">Anytime you hear the word</span> <span id="S1.F1.sf1.6.2" class="ltx_text ltx_font_typewriter">[0:05, left, <span id="S1.F1.sf1.6.2.1" class="ltx_text ltx_font_smallcaps">Neutral</span>]</span> <span id="S1.F1.sf1.7.3" class="ltx_text ltx_ulem_uline">wild you can bet it’s referring to something uncontrollable and unrestrained</span> <span id="S1.F1.sf1.8.4" class="ltx_text ltx_font_typewriter">[0:10, right, <span id="S1.F1.sf1.8.4.1" class="ltx_text ltx_font_smallcaps">Oppose</span>]</span>.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2409.18346/assets/figs/WCCA-024.jpg" id="S1.F1.sf2.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="281" height="159" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2409.18346/assets/figs/WCCA-025.jpg" id="S1.F1.sf2.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="281" height="159" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>But sit tight <span id="S1.F1.sf2.5.1" class="ltx_text ltx_ulem_uline">attribution research may crack the nut and science researchers in this emerging field are</span> <span id="S1.F1.sf2.6.2" class="ltx_text ltx_font_typewriter">[1:55, left, <span id="S1.F1.sf2.6.2.1" class="ltx_text ltx_font_smallcaps">Support</span>]</span> <span id="S1.F1.sf2.7.3" class="ltx_text ltx_ulem_uline">on the case</span> <span id="S1.F1.sf2.8.4" class="ltx_text ltx_font_typewriter">[2:00, right, <span id="S1.F1.sf2.8.4.1" class="ltx_text ltx_font_smallcaps">Neutral</span>]</span>.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>MultiClimate sample annotations with aligned video frames and transcript sentences.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Images and videos can shape public awareness of climate change by vividly illustrating its social impacts. Recently, multimodality gained significant traction for connecting CC to discourse <cite class="ltx_cite ltx_citemacro_citep">(Weinzierl and Harabagiu, <a href="#bib.bib33" title="" class="ltx_ref">2023b</a>)</cite>, as images and videos largely impact CC perceptions <cite class="ltx_cite ltx_citemacro_cite">Wang and Liu (<a href="#bib.bib31" title="" class="ltx_ref">2024</a>)</cite>.
Investigating stance through multimodality also becomes essential.
However, due to the lack of datasets, developing multimodal models for stance detection on climate change remains challenging, as previous studies mainly focused on language texts rather than the visual modality.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.2" class="ltx_p">To fill this gap, this paper proposes the first open-sourced dataset, MultiClimate, that integrates both visual and textual modalities, specifically <math id="S1.p4.1.m1.2" class="ltx_Math" alttext="4,209" display="inline"><semantics id="S1.p4.1.m1.2a"><mrow id="S1.p4.1.m1.2.3.2" xref="S1.p4.1.m1.2.3.1.cmml"><mn id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">4</mn><mo id="S1.p4.1.m1.2.3.2.1" xref="S1.p4.1.m1.2.3.1.cmml">,</mo><mn id="S1.p4.1.m1.2.2" xref="S1.p4.1.m1.2.2.cmml">209</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.2b"><list id="S1.p4.1.m1.2.3.1.cmml" xref="S1.p4.1.m1.2.3.2"><cn type="integer" id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1">4</cn><cn type="integer" id="S1.p4.1.m1.2.2.cmml" xref="S1.p4.1.m1.2.2">209</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.2c">4,209</annotation></semantics></math> image frames and transcripts, to label stances in <math id="S1.p4.2.m2.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S1.p4.2.m2.1a"><mn id="S1.p4.2.m2.1.1" xref="S1.p4.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S1.p4.2.m2.1b"><cn type="integer" id="S1.p4.2.m2.1.1.cmml" xref="S1.p4.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.2.m2.1c">100</annotation></semantics></math> CC-related YouTube videos
(§<a href="#S3" title="3 The MultiClimate Dataset ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).
§<a href="#S4" title="4 MultiClimate Stance Detection ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> evaluates text-only, image-only, and multimodal models on MultiClimate stance detection.
§<a href="#S5" title="5 Conclusion and Future Directions ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> concludes the paper and proposes future directions.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our results
show that the text-only BERT model outperforms image-only models on multimodal stance detection, and the best performance is achieved by fusing models from both modalities.
We further experiment with 9B-sized large language and multimodal models and illustrate that these larger models deliver unsatisfactory zero-shot results, much lower than our state-of-the-art (SOTA) fusion models.
Fine-tuning a large multimodal model brings about some improvements, but it is resource-heavy and encounters the Green NLP problems.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Multimodal Stance Detection.</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Stance detection has mainly concentrated on textual analysis <cite class="ltx_cite ltx_citemacro_cite">Küçük and Can (<a href="#bib.bib14" title="" class="ltx_ref">2021a</a>); Lan et al. (<a href="#bib.bib16" title="" class="ltx_ref">2024</a>)</cite>, with a significant focus on the stance expressed in social media platforms like Twitter <cite class="ltx_cite ltx_citemacro_cite">Taulé et al. (<a href="#bib.bib26" title="" class="ltx_ref">2018</a>); Conforti et al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>.
Yet, a recent trend arose that gradually includes images and videos in stance detection <cite class="ltx_cite ltx_citemacro_cite">Küçük and Can (<a href="#bib.bib15" title="" class="ltx_ref">2021b</a>); Carnot et al. (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p">Current multimodal stance detection datasets emphasize different aspects such as communication frames and trending topics like COVID-19 <cite class="ltx_cite ltx_citemacro_citep">(Taulé et al., <a href="#bib.bib26" title="" class="ltx_ref">2018</a>; Weinzierl and
Harabagiu, <a href="#bib.bib32" title="" class="ltx_ref">2023a</a>; Liang et al., <a href="#bib.bib18" title="" class="ltx_ref">2024</a>)</cite>.
These data rely solely on static images or extract the first frame from a video or GIF for visual input.
Despite these advancements, research on stance detection using visual input remains limited.
Our MultiClimate dataset addresses climate change topics by utilizing frames from full videos as visual input and transcripts as text, allowing for a more comprehensive information coverage.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Stance Detection on Climate Change.</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Stance detection on CC <cite class="ltx_cite ltx_citemacro_cite">Fraile-Hernandez and
Peñas (<a href="#bib.bib10" title="" class="ltx_ref">2024</a>)</cite> aims at determining whether a given document expresses a supportive, opposing, or neutral attitude toward whether CC is a real concern.
Recent CC stance detection studies particularly focused on social media texts <cite class="ltx_cite ltx_citemacro_cite">Vaid et al. (<a href="#bib.bib30" title="" class="ltx_ref">2022</a>)</cite>, especially Twitter <cite class="ltx_cite ltx_citemacro_cite">Conforti et al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>); Upadhyaya et al. (<a href="#bib.bib28" title="" class="ltx_ref">2023a</a>)</cite>.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p2.1" class="ltx_p">Regarding modeling, for instance, <cite class="ltx_cite ltx_citemacro_citet">Vaid et al. (<a href="#bib.bib30" title="" class="ltx_ref">2022</a>)</cite> offered Fast-Text <cite class="ltx_cite ltx_citemacro_cite">Bojanowski et al. (<a href="#bib.bib3" title="" class="ltx_ref">2017</a>)</cite> and BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite> variants for stance tasks.
<cite class="ltx_cite ltx_citemacro_citet">Upadhyaya et al. (<a href="#bib.bib28" title="" class="ltx_ref">2023a</a>)</cite> proposed a framework that utilizes emotion recognition and intensity prediction to discern different attitudes in tweets about climate change. CC stance detection is also applied to downstream tasks such as fake news detection <cite class="ltx_cite ltx_citemacro_cite">Mazid and Zarnaz (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>.
However, none of the models above handles multimodal inputs.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The MultiClimate Dataset</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We propose a new open-source MultiClimate dataset, the first climate change stance detection corpus on multimodal data.
To the best of our knowledge, Mendeley provides a closest dataset to ours <cite class="ltx_cite ltx_citemacro_cite">Aharonson (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite>.
However, it only provides a simple Excel file with <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="168" display="inline"><semantics id="S3.p1.1.m1.1a"><mn id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">168</mn><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><cn type="integer" id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">168</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">168</annotation></semantics></math> YouTube links and some basic statistics on each video.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://data.mendeley.com/datasets/j955mxnyyf/1" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://data.mendeley.com/datasets/j955mxnyyf/1</a></span></span></span>
Most of these videos are not under the Creative Commons license and none is annotated with stance labels.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.20" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:182.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(88.4pt,-37.1pt) scale(1.68778216716858,1.68778216716858) ;">
<table id="S3.T1.20.20" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.20.20.21.1" class="ltx_tr">
<th id="S3.T1.20.20.21.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S3.T1.20.20.21.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T1.20.20.21.1.2.1" class="ltx_text ltx_font_bold">Videos</span></th>
<th id="S3.T1.20.20.21.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S3.T1.20.20.21.1.3.1" class="ltx_text ltx_font_bold">Frame-Transcript Pairs</span></th>
</tr>
<tr id="S3.T1.20.20.22.2" class="ltx_tr">
<th id="S3.T1.20.20.22.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S3.T1.20.20.22.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_column"><span id="S3.T1.20.20.22.2.2.1" class="ltx_text ltx_font_smallcaps">Support</span></th>
<th id="S3.T1.20.20.22.2.3" class="ltx_td ltx_align_right ltx_th ltx_th_column"><span id="S3.T1.20.20.22.2.3.1" class="ltx_text ltx_font_smallcaps">Neutral</span></th>
<th id="S3.T1.20.20.22.2.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r"><span id="S3.T1.20.20.22.2.4.1" class="ltx_text ltx_font_smallcaps">Oppose</span></th>
<th id="S3.T1.20.20.22.2.5" class="ltx_td ltx_align_right ltx_th ltx_th_column"><span id="S3.T1.20.20.22.2.5.1" class="ltx_text ltx_font_bold">Total</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.5.5.5" class="ltx_tr">
<th id="S3.T1.5.5.5.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T1.5.5.5.6.1" class="ltx_text ltx_font_bold">Train</span></th>
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t"><math id="S3.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S3.T1.1.1.1.1.m1.1a"><mn id="S3.T1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><cn type="integer" id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">80</annotation></semantics></math></th>
<td id="S3.T1.2.2.2.2" class="ltx_td ltx_align_right ltx_border_t"><math id="S3.T1.2.2.2.2.m1.2" class="ltx_Math" alttext="1,449" display="inline"><semantics id="S3.T1.2.2.2.2.m1.2a"><mrow id="S3.T1.2.2.2.2.m1.2.3.2" xref="S3.T1.2.2.2.2.m1.2.3.1.cmml"><mn id="S3.T1.2.2.2.2.m1.1.1" xref="S3.T1.2.2.2.2.m1.1.1.cmml">1</mn><mo id="S3.T1.2.2.2.2.m1.2.3.2.1" xref="S3.T1.2.2.2.2.m1.2.3.1.cmml">,</mo><mn id="S3.T1.2.2.2.2.m1.2.2" xref="S3.T1.2.2.2.2.m1.2.2.cmml">449</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.m1.2b"><list id="S3.T1.2.2.2.2.m1.2.3.1.cmml" xref="S3.T1.2.2.2.2.m1.2.3.2"><cn type="integer" id="S3.T1.2.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.2.m1.1.1">1</cn><cn type="integer" id="S3.T1.2.2.2.2.m1.2.2.cmml" xref="S3.T1.2.2.2.2.m1.2.2">449</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.m1.2c">1,449</annotation></semantics></math></td>
<td id="S3.T1.3.3.3.3" class="ltx_td ltx_align_right ltx_border_t"><math id="S3.T1.3.3.3.3.m1.2" class="ltx_Math" alttext="1,036" display="inline"><semantics id="S3.T1.3.3.3.3.m1.2a"><mrow id="S3.T1.3.3.3.3.m1.2.3.2" xref="S3.T1.3.3.3.3.m1.2.3.1.cmml"><mn id="S3.T1.3.3.3.3.m1.1.1" xref="S3.T1.3.3.3.3.m1.1.1.cmml">1</mn><mo id="S3.T1.3.3.3.3.m1.2.3.2.1" xref="S3.T1.3.3.3.3.m1.2.3.1.cmml">,</mo><mn id="S3.T1.3.3.3.3.m1.2.2" xref="S3.T1.3.3.3.3.m1.2.2.cmml">036</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.m1.2b"><list id="S3.T1.3.3.3.3.m1.2.3.1.cmml" xref="S3.T1.3.3.3.3.m1.2.3.2"><cn type="integer" id="S3.T1.3.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.3.m1.1.1">1</cn><cn type="integer" id="S3.T1.3.3.3.3.m1.2.2.cmml" xref="S3.T1.3.3.3.3.m1.2.2">036</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.m1.2c">1,036</annotation></semantics></math></td>
<td id="S3.T1.4.4.4.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><math id="S3.T1.4.4.4.4.m1.1" class="ltx_Math" alttext="887" display="inline"><semantics id="S3.T1.4.4.4.4.m1.1a"><mn id="S3.T1.4.4.4.4.m1.1.1" xref="S3.T1.4.4.4.4.m1.1.1.cmml">887</mn><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.4.m1.1b"><cn type="integer" id="S3.T1.4.4.4.4.m1.1.1.cmml" xref="S3.T1.4.4.4.4.m1.1.1">887</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.4.m1.1c">887</annotation></semantics></math></td>
<td id="S3.T1.5.5.5.5" class="ltx_td ltx_align_right ltx_border_t"><math id="S3.T1.5.5.5.5.m1.2" class="ltx_Math" alttext="3,372" display="inline"><semantics id="S3.T1.5.5.5.5.m1.2a"><mrow id="S3.T1.5.5.5.5.m1.2.3.2" xref="S3.T1.5.5.5.5.m1.2.3.1.cmml"><mn id="S3.T1.5.5.5.5.m1.1.1" xref="S3.T1.5.5.5.5.m1.1.1.cmml">3</mn><mo id="S3.T1.5.5.5.5.m1.2.3.2.1" xref="S3.T1.5.5.5.5.m1.2.3.1.cmml">,</mo><mn id="S3.T1.5.5.5.5.m1.2.2" xref="S3.T1.5.5.5.5.m1.2.2.cmml">372</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.5.m1.2b"><list id="S3.T1.5.5.5.5.m1.2.3.1.cmml" xref="S3.T1.5.5.5.5.m1.2.3.2"><cn type="integer" id="S3.T1.5.5.5.5.m1.1.1.cmml" xref="S3.T1.5.5.5.5.m1.1.1">3</cn><cn type="integer" id="S3.T1.5.5.5.5.m1.2.2.cmml" xref="S3.T1.5.5.5.5.m1.2.2">372</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.5.m1.2c">3,372</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.10.10.10" class="ltx_tr">
<th id="S3.T1.10.10.10.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.10.10.10.6.1" class="ltx_text ltx_font_bold">Dev</span></th>
<th id="S3.T1.6.6.6.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r"><math id="S3.T1.6.6.6.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.T1.6.6.6.1.m1.1a"><mn id="S3.T1.6.6.6.1.m1.1.1" xref="S3.T1.6.6.6.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.1.m1.1b"><cn type="integer" id="S3.T1.6.6.6.1.m1.1.1.cmml" xref="S3.T1.6.6.6.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.1.m1.1c">10</annotation></semantics></math></th>
<td id="S3.T1.7.7.7.2" class="ltx_td ltx_align_right"><math id="S3.T1.7.7.7.2.m1.1" class="ltx_Math" alttext="204" display="inline"><semantics id="S3.T1.7.7.7.2.m1.1a"><mn id="S3.T1.7.7.7.2.m1.1.1" xref="S3.T1.7.7.7.2.m1.1.1.cmml">204</mn><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.7.2.m1.1b"><cn type="integer" id="S3.T1.7.7.7.2.m1.1.1.cmml" xref="S3.T1.7.7.7.2.m1.1.1">204</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.7.2.m1.1c">204</annotation></semantics></math></td>
<td id="S3.T1.8.8.8.3" class="ltx_td ltx_align_right"><math id="S3.T1.8.8.8.3.m1.1" class="ltx_Math" alttext="83" display="inline"><semantics id="S3.T1.8.8.8.3.m1.1a"><mn id="S3.T1.8.8.8.3.m1.1.1" xref="S3.T1.8.8.8.3.m1.1.1.cmml">83</mn><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.8.3.m1.1b"><cn type="integer" id="S3.T1.8.8.8.3.m1.1.1.cmml" xref="S3.T1.8.8.8.3.m1.1.1">83</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.8.3.m1.1c">83</annotation></semantics></math></td>
<td id="S3.T1.9.9.9.4" class="ltx_td ltx_align_right ltx_border_r"><math id="S3.T1.9.9.9.4.m1.1" class="ltx_Math" alttext="130" display="inline"><semantics id="S3.T1.9.9.9.4.m1.1a"><mn id="S3.T1.9.9.9.4.m1.1.1" xref="S3.T1.9.9.9.4.m1.1.1.cmml">130</mn><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.9.4.m1.1b"><cn type="integer" id="S3.T1.9.9.9.4.m1.1.1.cmml" xref="S3.T1.9.9.9.4.m1.1.1">130</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.9.4.m1.1c">130</annotation></semantics></math></td>
<td id="S3.T1.10.10.10.5" class="ltx_td ltx_align_right"><math id="S3.T1.10.10.10.5.m1.1" class="ltx_Math" alttext="417" display="inline"><semantics id="S3.T1.10.10.10.5.m1.1a"><mn id="S3.T1.10.10.10.5.m1.1.1" xref="S3.T1.10.10.10.5.m1.1.1.cmml">417</mn><annotation-xml encoding="MathML-Content" id="S3.T1.10.10.10.5.m1.1b"><cn type="integer" id="S3.T1.10.10.10.5.m1.1.1.cmml" xref="S3.T1.10.10.10.5.m1.1.1">417</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.10.10.5.m1.1c">417</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.15.15.15" class="ltx_tr">
<th id="S3.T1.15.15.15.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.15.15.15.6.1" class="ltx_text ltx_font_bold">Test</span></th>
<th id="S3.T1.11.11.11.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r"><math id="S3.T1.11.11.11.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.T1.11.11.11.1.m1.1a"><mn id="S3.T1.11.11.11.1.m1.1.1" xref="S3.T1.11.11.11.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.T1.11.11.11.1.m1.1b"><cn type="integer" id="S3.T1.11.11.11.1.m1.1.1.cmml" xref="S3.T1.11.11.11.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.11.11.1.m1.1c">10</annotation></semantics></math></th>
<td id="S3.T1.12.12.12.2" class="ltx_td ltx_align_right"><math id="S3.T1.12.12.12.2.m1.1" class="ltx_Math" alttext="194" display="inline"><semantics id="S3.T1.12.12.12.2.m1.1a"><mn id="S3.T1.12.12.12.2.m1.1.1" xref="S3.T1.12.12.12.2.m1.1.1.cmml">194</mn><annotation-xml encoding="MathML-Content" id="S3.T1.12.12.12.2.m1.1b"><cn type="integer" id="S3.T1.12.12.12.2.m1.1.1.cmml" xref="S3.T1.12.12.12.2.m1.1.1">194</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.12.12.2.m1.1c">194</annotation></semantics></math></td>
<td id="S3.T1.13.13.13.3" class="ltx_td ltx_align_right"><math id="S3.T1.13.13.13.3.m1.1" class="ltx_Math" alttext="73" display="inline"><semantics id="S3.T1.13.13.13.3.m1.1a"><mn id="S3.T1.13.13.13.3.m1.1.1" xref="S3.T1.13.13.13.3.m1.1.1.cmml">73</mn><annotation-xml encoding="MathML-Content" id="S3.T1.13.13.13.3.m1.1b"><cn type="integer" id="S3.T1.13.13.13.3.m1.1.1.cmml" xref="S3.T1.13.13.13.3.m1.1.1">73</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.13.13.13.3.m1.1c">73</annotation></semantics></math></td>
<td id="S3.T1.14.14.14.4" class="ltx_td ltx_align_right ltx_border_r"><math id="S3.T1.14.14.14.4.m1.1" class="ltx_Math" alttext="153" display="inline"><semantics id="S3.T1.14.14.14.4.m1.1a"><mn id="S3.T1.14.14.14.4.m1.1.1" xref="S3.T1.14.14.14.4.m1.1.1.cmml">153</mn><annotation-xml encoding="MathML-Content" id="S3.T1.14.14.14.4.m1.1b"><cn type="integer" id="S3.T1.14.14.14.4.m1.1.1.cmml" xref="S3.T1.14.14.14.4.m1.1.1">153</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.14.14.14.4.m1.1c">153</annotation></semantics></math></td>
<td id="S3.T1.15.15.15.5" class="ltx_td ltx_align_right"><math id="S3.T1.15.15.15.5.m1.1" class="ltx_Math" alttext="420" display="inline"><semantics id="S3.T1.15.15.15.5.m1.1a"><mn id="S3.T1.15.15.15.5.m1.1.1" xref="S3.T1.15.15.15.5.m1.1.1.cmml">420</mn><annotation-xml encoding="MathML-Content" id="S3.T1.15.15.15.5.m1.1b"><cn type="integer" id="S3.T1.15.15.15.5.m1.1.1.cmml" xref="S3.T1.15.15.15.5.m1.1.1">420</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.15.15.15.5.m1.1c">420</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.20.20.20" class="ltx_tr">
<th id="S3.T1.20.20.20.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t"><span id="S3.T1.20.20.20.6.1" class="ltx_text ltx_font_bold">Total</span></th>
<th id="S3.T1.16.16.16.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t"><math id="S3.T1.16.16.16.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.T1.16.16.16.1.m1.1a"><mn id="S3.T1.16.16.16.1.m1.1.1" xref="S3.T1.16.16.16.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.T1.16.16.16.1.m1.1b"><cn type="integer" id="S3.T1.16.16.16.1.m1.1.1.cmml" xref="S3.T1.16.16.16.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.16.16.16.1.m1.1c">100</annotation></semantics></math></th>
<td id="S3.T1.17.17.17.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><math id="S3.T1.17.17.17.2.m1.2" class="ltx_Math" alttext="1,847" display="inline"><semantics id="S3.T1.17.17.17.2.m1.2a"><mrow id="S3.T1.17.17.17.2.m1.2.3.2" xref="S3.T1.17.17.17.2.m1.2.3.1.cmml"><mn id="S3.T1.17.17.17.2.m1.1.1" xref="S3.T1.17.17.17.2.m1.1.1.cmml">1</mn><mo id="S3.T1.17.17.17.2.m1.2.3.2.1" xref="S3.T1.17.17.17.2.m1.2.3.1.cmml">,</mo><mn id="S3.T1.17.17.17.2.m1.2.2" xref="S3.T1.17.17.17.2.m1.2.2.cmml">847</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.17.17.17.2.m1.2b"><list id="S3.T1.17.17.17.2.m1.2.3.1.cmml" xref="S3.T1.17.17.17.2.m1.2.3.2"><cn type="integer" id="S3.T1.17.17.17.2.m1.1.1.cmml" xref="S3.T1.17.17.17.2.m1.1.1">1</cn><cn type="integer" id="S3.T1.17.17.17.2.m1.2.2.cmml" xref="S3.T1.17.17.17.2.m1.2.2">847</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.17.17.17.2.m1.2c">1,847</annotation></semantics></math></td>
<td id="S3.T1.18.18.18.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><math id="S3.T1.18.18.18.3.m1.2" class="ltx_Math" alttext="1,192" display="inline"><semantics id="S3.T1.18.18.18.3.m1.2a"><mrow id="S3.T1.18.18.18.3.m1.2.3.2" xref="S3.T1.18.18.18.3.m1.2.3.1.cmml"><mn id="S3.T1.18.18.18.3.m1.1.1" xref="S3.T1.18.18.18.3.m1.1.1.cmml">1</mn><mo id="S3.T1.18.18.18.3.m1.2.3.2.1" xref="S3.T1.18.18.18.3.m1.2.3.1.cmml">,</mo><mn id="S3.T1.18.18.18.3.m1.2.2" xref="S3.T1.18.18.18.3.m1.2.2.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.18.18.18.3.m1.2b"><list id="S3.T1.18.18.18.3.m1.2.3.1.cmml" xref="S3.T1.18.18.18.3.m1.2.3.2"><cn type="integer" id="S3.T1.18.18.18.3.m1.1.1.cmml" xref="S3.T1.18.18.18.3.m1.1.1">1</cn><cn type="integer" id="S3.T1.18.18.18.3.m1.2.2.cmml" xref="S3.T1.18.18.18.3.m1.2.2">192</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.18.18.18.3.m1.2c">1,192</annotation></semantics></math></td>
<td id="S3.T1.19.19.19.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t"><math id="S3.T1.19.19.19.4.m1.2" class="ltx_Math" alttext="1,170" display="inline"><semantics id="S3.T1.19.19.19.4.m1.2a"><mrow id="S3.T1.19.19.19.4.m1.2.3.2" xref="S3.T1.19.19.19.4.m1.2.3.1.cmml"><mn id="S3.T1.19.19.19.4.m1.1.1" xref="S3.T1.19.19.19.4.m1.1.1.cmml">1</mn><mo id="S3.T1.19.19.19.4.m1.2.3.2.1" xref="S3.T1.19.19.19.4.m1.2.3.1.cmml">,</mo><mn id="S3.T1.19.19.19.4.m1.2.2" xref="S3.T1.19.19.19.4.m1.2.2.cmml">170</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.19.19.19.4.m1.2b"><list id="S3.T1.19.19.19.4.m1.2.3.1.cmml" xref="S3.T1.19.19.19.4.m1.2.3.2"><cn type="integer" id="S3.T1.19.19.19.4.m1.1.1.cmml" xref="S3.T1.19.19.19.4.m1.1.1">1</cn><cn type="integer" id="S3.T1.19.19.19.4.m1.2.2.cmml" xref="S3.T1.19.19.19.4.m1.2.2">170</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.19.19.19.4.m1.2c">1,170</annotation></semantics></math></td>
<td id="S3.T1.20.20.20.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><math id="S3.T1.20.20.20.5.m1.2" class="ltx_Math" alttext="4,209" display="inline"><semantics id="S3.T1.20.20.20.5.m1.2a"><mrow id="S3.T1.20.20.20.5.m1.2.3.2" xref="S3.T1.20.20.20.5.m1.2.3.1.cmml"><mn id="S3.T1.20.20.20.5.m1.1.1" xref="S3.T1.20.20.20.5.m1.1.1.cmml">4</mn><mo id="S3.T1.20.20.20.5.m1.2.3.2.1" xref="S3.T1.20.20.20.5.m1.2.3.1.cmml">,</mo><mn id="S3.T1.20.20.20.5.m1.2.2" xref="S3.T1.20.20.20.5.m1.2.2.cmml">209</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.20.20.20.5.m1.2b"><list id="S3.T1.20.20.20.5.m1.2.3.1.cmml" xref="S3.T1.20.20.20.5.m1.2.3.2"><cn type="integer" id="S3.T1.20.20.20.5.m1.1.1.cmml" xref="S3.T1.20.20.20.5.m1.1.1">4</cn><cn type="integer" id="S3.T1.20.20.20.5.m1.2.2.cmml" xref="S3.T1.20.20.20.5.m1.2.2">209</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.20.20.20.5.m1.2c">4,209</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Overview of MultiClimate statistics.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.6" class="ltx_p">Our MultiClimate dataset consists of <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.p2.1.m1.1a"><mn id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><cn type="integer" id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">100</annotation></semantics></math> CC-related YouTube videos in English with <math id="S3.p2.2.m2.2" class="ltx_Math" alttext="4,209" display="inline"><semantics id="S3.p2.2.m2.2a"><mrow id="S3.p2.2.m2.2.3.2" xref="S3.p2.2.m2.2.3.1.cmml"><mn id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">4</mn><mo id="S3.p2.2.m2.2.3.2.1" xref="S3.p2.2.m2.2.3.1.cmml">,</mo><mn id="S3.p2.2.m2.2.2" xref="S3.p2.2.m2.2.2.cmml">209</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.2b"><list id="S3.p2.2.m2.2.3.1.cmml" xref="S3.p2.2.m2.2.3.2"><cn type="integer" id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">4</cn><cn type="integer" id="S3.p2.2.m2.2.2.cmml" xref="S3.p2.2.m2.2.2">209</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.2c">4,209</annotation></semantics></math> frame-transcript pairs.
To ensure a balanced distribution, we partition MultiClimate into <math id="S3.p2.3.m3.1" class="ltx_Math" alttext="80\%" display="inline"><semantics id="S3.p2.3.m3.1a"><mrow id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml"><mn id="S3.p2.3.m3.1.1.2" xref="S3.p2.3.m3.1.1.2.cmml">80</mn><mo id="S3.p2.3.m3.1.1.1" xref="S3.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><apply id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1"><csymbol cd="latexml" id="S3.p2.3.m3.1.1.1.cmml" xref="S3.p2.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S3.p2.3.m3.1.1.2.cmml" xref="S3.p2.3.m3.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">80\%</annotation></semantics></math> train, <math id="S3.p2.4.m4.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S3.p2.4.m4.1a"><mrow id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml"><mn id="S3.p2.4.m4.1.1.2" xref="S3.p2.4.m4.1.1.2.cmml">10</mn><mo id="S3.p2.4.m4.1.1.1" xref="S3.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><apply id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1"><csymbol cd="latexml" id="S3.p2.4.m4.1.1.1.cmml" xref="S3.p2.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S3.p2.4.m4.1.1.2.cmml" xref="S3.p2.4.m4.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">10\%</annotation></semantics></math> dev, and <math id="S3.p2.5.m5.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S3.p2.5.m5.1a"><mrow id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml"><mn id="S3.p2.5.m5.1.1.2" xref="S3.p2.5.m5.1.1.2.cmml">10</mn><mo id="S3.p2.5.m5.1.1.1" xref="S3.p2.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><apply id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1"><csymbol cd="latexml" id="S3.p2.5.m5.1.1.1.cmml" xref="S3.p2.5.m5.1.1.1">percent</csymbol><cn type="integer" id="S3.p2.5.m5.1.1.2.cmml" xref="S3.p2.5.m5.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">10\%</annotation></semantics></math> test sets, not only in terms of the number of videos but also the number of frame-transcript pairs.
<a href="#S3.T1" title="In 3 The MultiClimate Dataset ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a> provides statistics on MultiClimate frame-transcript pairs.
For a complete list of <math id="S3.p2.6.m6.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.p2.6.m6.1a"><mn id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><cn type="integer" id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">100</annotation></semantics></math> YouTube videos, see <a href="#A6" title="Appendix F Dataset Overview ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">F</span></a>.</p>
</div>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Video Collection.</h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">100 YouTube videos are collected by querying “climate change” and filtering the results by Creative Commons license. The videos are downloaded using the <span id="S3.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_typewriter">pytube<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span id="footnote2.1.1.1" class="ltx_text ltx_font_serif">2</span></span><a target="_blank" href="https://pytube.io/en/latest/" title="" class="ltx_ref ltx_url">https://pytube.io/en/latest/</a></span></span></span></span> and <span id="S3.SS0.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_typewriter">yt-dlp<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note"><span id="footnote3.1.1.1" class="ltx_text ltx_font_serif">3</span></span><a target="_blank" href="https://github.com/yt-dlp/yt-dlp" title="" class="ltx_ref ltx_url">https://github.com/yt-dlp/yt-dlp</a></span></span></span></span> APIs, while the transcripts are extracted using the <span id="S3.SS0.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_typewriter">youtube-transcript-api</span>.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://pypi.org/project/youtube-transcript-api/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pypi.org/project/youtube-transcript-api/</a></span></span></span></p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Frame Extraction and Alignment.</h4>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p1.1" class="ltx_p">To effectively use our video data, we extract the initial frame from each <math id="S3.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S3.SS0.SSS0.Px2.p1.1.m1.1a"><mn id="S3.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.1.m1.1b"><cn type="integer" id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.1.m1.1c">5</annotation></semantics></math>-second segment using <span id="S3.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_typewriter">FFmpeg</span> <cite class="ltx_cite ltx_citemacro_cite">Tomar (<a href="#bib.bib27" title="" class="ltx_ref">2006</a>)</cite>.
We then manually align each frame to its corresponding full sentence in the transcripts to form a frame-transcript pair.
Longer sentences can be aligned with
multiple time-stamped frames,<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>
MultiClimate has <math id="footnote5.m1.2" class="ltx_Math" alttext="1,902" display="inline"><semantics id="footnote5.m1.2b"><mrow id="footnote5.m1.2.3.2" xref="footnote5.m1.2.3.1.cmml"><mn id="footnote5.m1.1.1" xref="footnote5.m1.1.1.cmml">1</mn><mo id="footnote5.m1.2.3.2.1" xref="footnote5.m1.2.3.1.cmml">,</mo><mn id="footnote5.m1.2.2" xref="footnote5.m1.2.2.cmml">902</mn></mrow><annotation-xml encoding="MathML-Content" id="footnote5.m1.2c"><list id="footnote5.m1.2.3.1.cmml" xref="footnote5.m1.2.3.2"><cn type="integer" id="footnote5.m1.1.1.cmml" xref="footnote5.m1.1.1">1</cn><cn type="integer" id="footnote5.m1.2.2.cmml" xref="footnote5.m1.2.2">902</cn></list></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m1.2d">1,902</annotation></semantics></math> transcripts aligned to 1 frame, <math id="footnote5.m2.1" class="ltx_Math" alttext="656" display="inline"><semantics id="footnote5.m2.1b"><mn id="footnote5.m2.1.1" xref="footnote5.m2.1.1.cmml">656</mn><annotation-xml encoding="MathML-Content" id="footnote5.m2.1c"><cn type="integer" id="footnote5.m2.1.1.cmml" xref="footnote5.m2.1.1">656</cn></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m2.1d">656</annotation></semantics></math> to 2 frames,
<math id="footnote5.m3.1" class="ltx_Math" alttext="193" display="inline"><semantics id="footnote5.m3.1b"><mn id="footnote5.m3.1.1" xref="footnote5.m3.1.1.cmml">193</mn><annotation-xml encoding="MathML-Content" id="footnote5.m3.1c"><cn type="integer" id="footnote5.m3.1.1.cmml" xref="footnote5.m3.1.1">193</cn></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m3.1d">193</annotation></semantics></math> to 3,
<math id="footnote5.m4.1" class="ltx_Math" alttext="27" display="inline"><semantics id="footnote5.m4.1b"><mn id="footnote5.m4.1.1" xref="footnote5.m4.1.1.cmml">27</mn><annotation-xml encoding="MathML-Content" id="footnote5.m4.1c"><cn type="integer" id="footnote5.m4.1.1.cmml" xref="footnote5.m4.1.1">27</cn></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m4.1d">27</annotation></semantics></math> to 4,
<math id="footnote5.m5.1" class="ltx_Math" alttext="8" display="inline"><semantics id="footnote5.m5.1b"><mn id="footnote5.m5.1.1" xref="footnote5.m5.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="footnote5.m5.1c"><cn type="integer" id="footnote5.m5.1.1.cmml" xref="footnote5.m5.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m5.1d">8</annotation></semantics></math> to 5,
<math id="footnote5.m6.1" class="ltx_Math" alttext="3" display="inline"><semantics id="footnote5.m6.1b"><mn id="footnote5.m6.1.1" xref="footnote5.m6.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="footnote5.m6.1c"><cn type="integer" id="footnote5.m6.1.1.cmml" xref="footnote5.m6.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m6.1d">3</annotation></semantics></math> to 6,
<math id="footnote5.m7.1" class="ltx_Math" alttext="2" display="inline"><semantics id="footnote5.m7.1b"><mn id="footnote5.m7.1.1" xref="footnote5.m7.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="footnote5.m7.1c"><cn type="integer" id="footnote5.m7.1.1.cmml" xref="footnote5.m7.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m7.1d">2</annotation></semantics></math> to 7,
excluding <span id="footnote5.1" class="ltx_text ltx_font_typewriter">[Music]</span> or <span id="footnote5.2" class="ltx_text ltx_font_typewriter">None</span> transcripts. Details are provided in the repository <a target="_blank" href="https://github.com/werywjw/MultiClimate/tree/main/notebooks" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/werywjw/MultiClimate/tree/main/notebooks</a>.</span></span></span>
and we allow different stance labels for individual frame-transcript pairs, as shown in <a href="#S1.F1" title="In 1 Introduction ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Stance Annotation.</h4>

<div id="S3.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px3.p1.1" class="ltx_p">The first two authors of this paper manually annotated the stance expressed in each frame-transcript pair for <math id="S3.SS0.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.SS0.SSS0.Px3.p1.1.m1.1a"><mn id="S3.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p1.1.m1.1b"><cn type="integer" id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p1.1.m1.1c">100</annotation></semantics></math> videos.
We use trinary stance labels: <span id="S3.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_smallcaps">Support</span>, <span id="S3.SS0.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_smallcaps">Neutral</span>, and <span id="S3.SS0.SSS0.Px3.p1.1.3" class="ltx_text ltx_font_smallcaps">Oppose</span>.
<a href="#A1" title="Appendix A Annotation Guideline ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">A</span></a> details annotation guidelines, particularly label definitions, and examples.</p>
</div>
<div id="S3.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px3.p2.8" class="ltx_p">MultiClimate includes <math id="S3.SS0.SSS0.Px3.p2.1.m1.2" class="ltx_Math" alttext="1,847" display="inline"><semantics id="S3.SS0.SSS0.Px3.p2.1.m1.2a"><mrow id="S3.SS0.SSS0.Px3.p2.1.m1.2.3.2" xref="S3.SS0.SSS0.Px3.p2.1.m1.2.3.1.cmml"><mn id="S3.SS0.SSS0.Px3.p2.1.m1.1.1" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1.cmml">1</mn><mo id="S3.SS0.SSS0.Px3.p2.1.m1.2.3.2.1" xref="S3.SS0.SSS0.Px3.p2.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS0.SSS0.Px3.p2.1.m1.2.2" xref="S3.SS0.SSS0.Px3.p2.1.m1.2.2.cmml">847</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p2.1.m1.2b"><list id="S3.SS0.SSS0.Px3.p2.1.m1.2.3.1.cmml" xref="S3.SS0.SSS0.Px3.p2.1.m1.2.3.2"><cn type="integer" id="S3.SS0.SSS0.Px3.p2.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.1.m1.1.1">1</cn><cn type="integer" id="S3.SS0.SSS0.Px3.p2.1.m1.2.2.cmml" xref="S3.SS0.SSS0.Px3.p2.1.m1.2.2">847</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p2.1.m1.2c">1,847</annotation></semantics></math> <span id="S3.SS0.SSS0.Px3.p2.8.1" class="ltx_text ltx_font_smallcaps">Support</span>, <math id="S3.SS0.SSS0.Px3.p2.2.m2.2" class="ltx_Math" alttext="1,170" display="inline"><semantics id="S3.SS0.SSS0.Px3.p2.2.m2.2a"><mrow id="S3.SS0.SSS0.Px3.p2.2.m2.2.3.2" xref="S3.SS0.SSS0.Px3.p2.2.m2.2.3.1.cmml"><mn id="S3.SS0.SSS0.Px3.p2.2.m2.1.1" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1.cmml">1</mn><mo id="S3.SS0.SSS0.Px3.p2.2.m2.2.3.2.1" xref="S3.SS0.SSS0.Px3.p2.2.m2.2.3.1.cmml">,</mo><mn id="S3.SS0.SSS0.Px3.p2.2.m2.2.2" xref="S3.SS0.SSS0.Px3.p2.2.m2.2.2.cmml">170</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p2.2.m2.2b"><list id="S3.SS0.SSS0.Px3.p2.2.m2.2.3.1.cmml" xref="S3.SS0.SSS0.Px3.p2.2.m2.2.3.2"><cn type="integer" id="S3.SS0.SSS0.Px3.p2.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.2.m2.1.1">1</cn><cn type="integer" id="S3.SS0.SSS0.Px3.p2.2.m2.2.2.cmml" xref="S3.SS0.SSS0.Px3.p2.2.m2.2.2">170</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p2.2.m2.2c">1,170</annotation></semantics></math> <span id="S3.SS0.SSS0.Px3.p2.8.2" class="ltx_text ltx_font_smallcaps">Oppose</span>, and <math id="S3.SS0.SSS0.Px3.p2.3.m3.2" class="ltx_Math" alttext="1,192" display="inline"><semantics id="S3.SS0.SSS0.Px3.p2.3.m3.2a"><mrow id="S3.SS0.SSS0.Px3.p2.3.m3.2.3.2" xref="S3.SS0.SSS0.Px3.p2.3.m3.2.3.1.cmml"><mn id="S3.SS0.SSS0.Px3.p2.3.m3.1.1" xref="S3.SS0.SSS0.Px3.p2.3.m3.1.1.cmml">1</mn><mo id="S3.SS0.SSS0.Px3.p2.3.m3.2.3.2.1" xref="S3.SS0.SSS0.Px3.p2.3.m3.2.3.1.cmml">,</mo><mn id="S3.SS0.SSS0.Px3.p2.3.m3.2.2" xref="S3.SS0.SSS0.Px3.p2.3.m3.2.2.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p2.3.m3.2b"><list id="S3.SS0.SSS0.Px3.p2.3.m3.2.3.1.cmml" xref="S3.SS0.SSS0.Px3.p2.3.m3.2.3.2"><cn type="integer" id="S3.SS0.SSS0.Px3.p2.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.3.m3.1.1">1</cn><cn type="integer" id="S3.SS0.SSS0.Px3.p2.3.m3.2.2.cmml" xref="S3.SS0.SSS0.Px3.p2.3.m3.2.2">192</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p2.3.m3.2c">1,192</annotation></semantics></math> <span id="S3.SS0.SSS0.Px3.p2.8.3" class="ltx_text ltx_font_smallcaps">Neutral</span> frame-transcript pairs (<a href="#S3.T1" title="In 3 The MultiClimate Dataset ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>).
To assess the effectiveness of our annotation guideline and the quality of our dataset, all <math id="S3.SS0.SSS0.Px3.p2.4.m4.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS0.SSS0.Px3.p2.4.m4.1a"><mn id="S3.SS0.SSS0.Px3.p2.4.m4.1.1" xref="S3.SS0.SSS0.Px3.p2.4.m4.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p2.4.m4.1b"><cn type="integer" id="S3.SS0.SSS0.Px3.p2.4.m4.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.4.m4.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p2.4.m4.1c">10</annotation></semantics></math> videos in the test partition are double annotated.
Inter-annotator agreement (IAA) between the two annotators achieves <math id="S3.SS0.SSS0.Px3.p2.5.m5.1" class="ltx_Math" alttext="0.703" display="inline"><semantics id="S3.SS0.SSS0.Px3.p2.5.m5.1a"><mn id="S3.SS0.SSS0.Px3.p2.5.m5.1.1" xref="S3.SS0.SSS0.Px3.p2.5.m5.1.1.cmml">0.703</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p2.5.m5.1b"><cn type="float" id="S3.SS0.SSS0.Px3.p2.5.m5.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.5.m5.1.1">0.703</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p2.5.m5.1c">0.703</annotation></semantics></math> in Cohen’s kappa, <math id="S3.SS0.SSS0.Px3.p2.6.m6.1" class="ltx_Math" alttext="0.826" display="inline"><semantics id="S3.SS0.SSS0.Px3.p2.6.m6.1a"><mn id="S3.SS0.SSS0.Px3.p2.6.m6.1.1" xref="S3.SS0.SSS0.Px3.p2.6.m6.1.1.cmml">0.826</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p2.6.m6.1b"><cn type="float" id="S3.SS0.SSS0.Px3.p2.6.m6.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.6.m6.1.1">0.826</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p2.6.m6.1c">0.826</annotation></semantics></math> in accuracy, and <math id="S3.SS0.SSS0.Px3.p2.7.m7.1" class="ltx_Math" alttext="0.823" display="inline"><semantics id="S3.SS0.SSS0.Px3.p2.7.m7.1a"><mn id="S3.SS0.SSS0.Px3.p2.7.m7.1.1" xref="S3.SS0.SSS0.Px3.p2.7.m7.1.1.cmml">0.823</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p2.7.m7.1b"><cn type="float" id="S3.SS0.SSS0.Px3.p2.7.m7.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.7.m7.1.1">0.823</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p2.7.m7.1c">0.823</annotation></semantics></math> in weighted F1; see <a href="#A2" title="Appendix B IAA Details on 10 Test Videos ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">B</span></a> for IAA on <math id="S3.SS0.SSS0.Px3.p2.8.m8.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS0.SSS0.Px3.p2.8.m8.1a"><mn id="S3.SS0.SSS0.Px3.p2.8.m8.1.1" xref="S3.SS0.SSS0.Px3.p2.8.m8.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p2.8.m8.1b"><cn type="integer" id="S3.SS0.SSS0.Px3.p2.8.m8.1.1.cmml" xref="S3.SS0.SSS0.Px3.p2.8.m8.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p2.8.m8.1c">10</annotation></semantics></math> individual test videos.
Additionally, we aggregate stance labels from each video’s frame-transcript pairs by majority voting for future video-level analyses and experiments;
<a href="#A6" title="Appendix F Dataset Overview ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">F</span></a> includes these video stance labels.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>MultiClimate Stance Detection</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section evaluates SOTA text-only, image-only, and multimodal models
on MultiClimate stance detection.
We use both accuracy and weighted F1 scores since <span id="S4.p1.1.1" class="ltx_text ltx_font_smallcaps">Support/Neutral/Oppose</span> labels are unbalanced in the dataset. The models are run on the CPU, Google T4 GPU, and NVIDIA GeForce RTX 2080.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Models</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We conduct a comprehensive performance evaluation on several text-only, image-only, text-image-fusion, and multimodal models.
We leverage text-only BERT variants <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite> as earlier work showed their superior performance on tweet stance detection <cite class="ltx_cite ltx_citemacro_cite">Weinzierl and
Harabagiu (<a href="#bib.bib32" title="" class="ltx_ref">2023a</a>); Vaid et al. (<a href="#bib.bib30" title="" class="ltx_ref">2022</a>)</cite>.
Large language models (LLMs) are also included given their promising zero-shot classification performance <cite class="ltx_cite ltx_citemacro_cite">Dubey et al. (<a href="#bib.bib9" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Meanwhile, for image recognition and analysis, ResNet50 <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite> and ViT <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite> are capable of understanding and interpreting complex image data.
By combining aforementioned textual and visual models, we also deploy fusion models to investigate whether both modalities are essential for multimodal stance detection.
Moreover, models that are trained on cross-modal representations, CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite>,BLIP <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite>, and IDEFICS <cite class="ltx_cite ltx_citemacro_citep">(Alayrac et al., <a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite>, are also compared with the aforementioned unimodal and fusion models.</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Text-only Models.</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">We use BERT (Bidirectional Encoder Representations from Transformers, <span id="S4.SS1.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">bert-base-cased</span>, <cite class="ltx_cite ltx_citemacro_citet">Devlin et al. <a href="#bib.bib7" title="" class="ltx_ref">2019</a></cite>) for our textual stance detection given its effectiveness. Newly released LLMs, Llama3 (<span id="S4.SS1.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">meta-llama/Meta-Llama-3-8B</span>, <cite class="ltx_cite ltx_citemacro_citet">Dubey et al. <a href="#bib.bib9" title="" class="ltx_ref">2024</a>; Meta AI <a href="#bib.bib21" title="" class="ltx_ref">2024</a></cite>) and Gemma2-9B (<span id="S4.SS1.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_italic">google/gemma-2-9b</span>, <cite class="ltx_cite ltx_citemacro_citet">Clement and Warkentin <a href="#bib.bib5" title="" class="ltx_ref">2024</a></cite>) are also evaluated
on the Ollama <cite class="ltx_cite ltx_citemacro_cite">Morgan and Michael (<a href="#bib.bib23" title="" class="ltx_ref">2024</a>)</cite> platform
by giving the following zero-shot prompt:
<span id="S4.SS1.SSS0.Px1.p1.1.4" class="ltx_ERROR undefined">{lightbox}</span>
<span id="S4.SS1.SSS0.Px1.p1.1.5" class="ltx_text ltx_font_italic">Classify the stance of the following text towards climate change as: 0 (<span id="S4.SS1.SSS0.Px1.p1.1.5.1" class="ltx_text ltx_font_smallcaps">Neutral</span>), 1 (<span id="S4.SS1.SSS0.Px1.p1.1.5.2" class="ltx_text ltx_font_smallcaps">Support</span>), 2 (<span id="S4.SS1.SSS0.Px1.p1.1.5.3" class="ltx_text ltx_font_smallcaps">Oppose</span>): </span>{<span id="S4.SS1.SSS0.Px1.p1.1.6" class="ltx_text ltx_font_italic">transcript</span>}.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Image-only Models.</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">We also deploy two state-of-the-art image-only models, ResNet50 (Residual Network, <span id="S4.SS1.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">microsoft/resnet-50</span>, <cite class="ltx_cite ltx_citemacro_citet">He et al. <a href="#bib.bib12" title="" class="ltx_ref">2016</a></cite>) and
ViT (Vision Transformer, <span id="S4.SS1.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">google/vit-base-patch16-224</span>, <cite class="ltx_cite ltx_citemacro_citet">Dosovitskiy et al. <a href="#bib.bib8" title="" class="ltx_ref">2021</a></cite>).</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Multimodal Models.</h4>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p1.1" class="ltx_p">Our multimodal fusion models are built by concatenating BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite> with ViT <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite> or ResNet50 <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite> embeddings, as they are the smaller 100M-sized models (see <a href="#S4.T2" title="In 4.2 Results and Discussions ‣ 4 MultiClimate Stance Detection ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div id="S4.SS1.SSS0.Px3.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p2.1" class="ltx_p">CLIP (Contrastive Language-Image Pre-training, <span id="S4.SS1.SSS0.Px3.p2.1.1" class="ltx_text ltx_font_italic">openai/clip-vit-base-patch32</span>, <cite class="ltx_cite ltx_citemacro_citet">Radford et al. <a href="#bib.bib25" title="" class="ltx_ref">2021</a></cite>) and BLIP (Bootstrapping Language-Image Pre-training, <span id="S4.SS1.SSS0.Px3.p2.1.2" class="ltx_text ltx_font_italic">Salesforce/blip-image-captioning-base</span>, <cite class="ltx_cite ltx_citemacro_citet">Li et al. <a href="#bib.bib17" title="" class="ltx_ref">2022</a></cite>) are leveraged to associate images and text simultaneously, capturing richer, more nuanced information.
We also experiment with
IDEFICS (Image-aware Decoder Enhanced à la Flamingo with Interleaved Cross-attentionS,
<span id="S4.SS1.SSS0.Px3.p2.1.3" class="ltx_text ltx_font_italic">HuggingFaceM4/idefics-9b</span>, <cite class="ltx_cite ltx_citemacro_citet">Alayrac et al. <a href="#bib.bib2" title="" class="ltx_ref">2022</a></cite>),<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://huggingface.co/blog/idefics" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/blog/idefics</a></span></span></span> an open-source Multimodal Large Language Model (MLLM) by providing the following prompt template in zero-shot as well as fine-tuned settings.</p>
</div>
<div id="S4.SS1.SSS0.Px3.p3" class="ltx_para">
<span id="S4.SS1.SSS0.Px3.p3.1" class="ltx_ERROR undefined">{lightbox}</span>
<p id="S4.SS1.SSS0.Px3.p3.2" class="ltx_p"><span id="S4.SS1.SSS0.Px3.p3.2.1" class="ltx_text ltx_font_italic">Given the </span>{<span id="S4.SS1.SSS0.Px3.p3.2.2" class="ltx_text ltx_font_italic">frame</span>}<span id="S4.SS1.SSS0.Px3.p3.2.3" class="ltx_text ltx_font_italic"> and </span>{<span id="S4.SS1.SSS0.Px3.p3.2.4" class="ltx_text ltx_font_italic">transcripts</span>}<span id="S4.SS1.SSS0.Px3.p3.2.5" class="ltx_text ltx_font_italic">, what is the stance of this frame-transcript pair towards climate change? Choose one between 0 for <span id="S4.SS1.SSS0.Px3.p3.2.5.1" class="ltx_text ltx_font_smallcaps">Neutral</span>, 1 for <span id="S4.SS1.SSS0.Px3.p3.2.5.2" class="ltx_text ltx_font_smallcaps">Support</span>, and 2 for <span id="S4.SS1.SSS0.Px3.p3.2.5.3" class="ltx_text ltx_font_smallcaps">Oppose</span>.
</span></p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results and Discussions</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><a href="#S4.T2" title="In 4.2 Results and Discussions ‣ 4 MultiClimate Stance Detection ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a> presents the results of evaluating the effectiveness of individual modalities, multimodal models, and 9B-sized large models on MultiClimate.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.35" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:410.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(93.3pt,-88.4pt) scale(1.75558363012408,1.75558363012408) ;">
<table id="S4.T2.35.35" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.35.35.36.1" class="ltx_tr">
<td id="S4.T2.35.35.36.1.1" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt"><span id="S4.T2.35.35.36.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S4.T2.35.35.36.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.35.35.36.1.2.1" class="ltx_text ltx_font_smallcaps">Acc</span></td>
<td id="S4.T2.35.35.36.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.35.35.36.1.3.1" class="ltx_text ltx_font_smallcaps">F1</span></td>
<td id="S4.T2.35.35.36.1.4" class="ltx_td ltx_align_right ltx_border_tt"># Params</td>
</tr>
<tr id="S4.T2.3.3.3" class="ltx_tr">
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">BERT<sup id="S4.T2.1.1.1.1.1.1" class="ltx_sup"><span id="S4.T2.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">♣</span></sup></span></td>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.2.2.2.2.1" class="ltx_text ltx_markedasmath ltx_font_bold">0.705</span></td>
<td id="S4.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.3.3.3.3.1" class="ltx_text ltx_markedasmath ltx_font_bold">0.705</span></td>
<td id="S4.T2.3.3.3.4" class="ltx_td ltx_align_right ltx_border_t">110M</td>
</tr>
<tr id="S4.T2.6.6.6" class="ltx_tr">
<td id="S4.T2.4.4.4.1" class="ltx_td ltx_align_right ltx_border_r">
<span id="S4.T2.4.4.4.1.1" class="ltx_text ltx_font_bold">Llama3<sup id="S4.T2.4.4.4.1.1.1" class="ltx_sup"><span id="S4.T2.4.4.4.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">♣</span></sup></span> (zero-shot)</td>
<td id="S4.T2.5.5.5.2" class="ltx_td ltx_align_center"><math id="S4.T2.5.5.5.2.m1.1" class="ltx_Math" alttext="0.485" display="inline"><semantics id="S4.T2.5.5.5.2.m1.1a"><mn id="S4.T2.5.5.5.2.m1.1.1" xref="S4.T2.5.5.5.2.m1.1.1.cmml">0.485</mn><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.2.m1.1b"><cn type="float" id="S4.T2.5.5.5.2.m1.1.1.cmml" xref="S4.T2.5.5.5.2.m1.1.1">0.485</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.2.m1.1c">0.485</annotation></semantics></math></td>
<td id="S4.T2.6.6.6.3" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.6.6.6.3.m1.1" class="ltx_Math" alttext="0.451" display="inline"><semantics id="S4.T2.6.6.6.3.m1.1a"><mn id="S4.T2.6.6.6.3.m1.1.1" xref="S4.T2.6.6.6.3.m1.1.1.cmml">0.451</mn><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.3.m1.1b"><cn type="float" id="S4.T2.6.6.6.3.m1.1.1.cmml" xref="S4.T2.6.6.6.3.m1.1.1">0.451</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.3.m1.1c">0.451</annotation></semantics></math></td>
<td id="S4.T2.6.6.6.4" class="ltx_td ltx_align_right">8B</td>
</tr>
<tr id="S4.T2.9.9.9" class="ltx_tr">
<td id="S4.T2.7.7.7.1" class="ltx_td ltx_align_right ltx_border_r">
<span id="S4.T2.7.7.7.1.1" class="ltx_text ltx_font_bold">Gemma2<sup id="S4.T2.7.7.7.1.1.1" class="ltx_sup"><span id="S4.T2.7.7.7.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">♣</span></sup></span> (zero-shot)</td>
<td id="S4.T2.8.8.8.2" class="ltx_td ltx_align_center"><math id="S4.T2.8.8.8.2.m1.1" class="ltx_Math" alttext="0.461" display="inline"><semantics id="S4.T2.8.8.8.2.m1.1a"><mn id="S4.T2.8.8.8.2.m1.1.1" xref="S4.T2.8.8.8.2.m1.1.1.cmml">0.461</mn><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.8.2.m1.1b"><cn type="float" id="S4.T2.8.8.8.2.m1.1.1.cmml" xref="S4.T2.8.8.8.2.m1.1.1">0.461</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.8.2.m1.1c">0.461</annotation></semantics></math></td>
<td id="S4.T2.9.9.9.3" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.9.9.9.3.m1.1" class="ltx_Math" alttext="0.382" display="inline"><semantics id="S4.T2.9.9.9.3.m1.1a"><mn id="S4.T2.9.9.9.3.m1.1.1" xref="S4.T2.9.9.9.3.m1.1.1.cmml">0.382</mn><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.9.3.m1.1b"><cn type="float" id="S4.T2.9.9.9.3.m1.1.1.cmml" xref="S4.T2.9.9.9.3.m1.1.1">0.382</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.9.3.m1.1c">0.382</annotation></semantics></math></td>
<td id="S4.T2.9.9.9.4" class="ltx_td ltx_align_right">9B</td>
</tr>
<tr id="S4.T2.12.12.12" class="ltx_tr">
<td id="S4.T2.10.10.10.1" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T2.10.10.10.1.1" class="ltx_text ltx_font_bold">ResNet50<sup id="S4.T2.10.10.10.1.1.1" class="ltx_sup"><span id="S4.T2.10.10.10.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">♠</span></sup></span></td>
<td id="S4.T2.11.11.11.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.11.11.11.2.m1.1" class="ltx_Math" alttext="0.424" display="inline"><semantics id="S4.T2.11.11.11.2.m1.1a"><mn id="S4.T2.11.11.11.2.m1.1.1" xref="S4.T2.11.11.11.2.m1.1.1.cmml">0.424</mn><annotation-xml encoding="MathML-Content" id="S4.T2.11.11.11.2.m1.1b"><cn type="float" id="S4.T2.11.11.11.2.m1.1.1.cmml" xref="S4.T2.11.11.11.2.m1.1.1">0.424</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.11.11.2.m1.1c">0.424</annotation></semantics></math></td>
<td id="S4.T2.12.12.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.12.12.12.3.m1.1" class="ltx_Math" alttext="0.399" display="inline"><semantics id="S4.T2.12.12.12.3.m1.1a"><mn id="S4.T2.12.12.12.3.m1.1.1" xref="S4.T2.12.12.12.3.m1.1.1.cmml">0.399</mn><annotation-xml encoding="MathML-Content" id="S4.T2.12.12.12.3.m1.1b"><cn type="float" id="S4.T2.12.12.12.3.m1.1.1.cmml" xref="S4.T2.12.12.12.3.m1.1.1">0.399</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.12.12.3.m1.1c">0.399</annotation></semantics></math></td>
<td id="S4.T2.12.12.12.4" class="ltx_td ltx_align_right ltx_border_t">25.6M</td>
</tr>
<tr id="S4.T2.15.15.15" class="ltx_tr">
<td id="S4.T2.13.13.13.1" class="ltx_td ltx_align_right ltx_border_r"><span id="S4.T2.13.13.13.1.1" class="ltx_text ltx_font_bold">ViT<sup id="S4.T2.13.13.13.1.1.1" class="ltx_sup"><span id="S4.T2.13.13.13.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">♠</span></sup></span></td>
<td id="S4.T2.14.14.14.2" class="ltx_td ltx_align_center"><span id="S4.T2.14.14.14.2.1" class="ltx_text ltx_markedasmath ltx_font_bold">0.460</span></td>
<td id="S4.T2.15.15.15.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.15.15.15.3.1" class="ltx_text ltx_markedasmath ltx_font_bold">0.462</span></td>
<td id="S4.T2.15.15.15.4" class="ltx_td ltx_align_right">86.6M</td>
</tr>
<tr id="S4.T2.18.18.18" class="ltx_tr">
<td id="S4.T2.16.16.16.1" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T2.16.16.16.1.1" class="ltx_text ltx_font_bold">BERT + ResNet50<sup id="S4.T2.16.16.16.1.1.1" class="ltx_sup"><span id="S4.T2.16.16.16.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">⋆</span></sup></span></td>
<td id="S4.T2.17.17.17.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.17.17.17.2.m1.1" class="ltx_Math" alttext="0.717" display="inline"><semantics id="S4.T2.17.17.17.2.m1.1a"><mn id="S4.T2.17.17.17.2.m1.1.1" xref="S4.T2.17.17.17.2.m1.1.1.cmml">0.717</mn><annotation-xml encoding="MathML-Content" id="S4.T2.17.17.17.2.m1.1b"><cn type="float" id="S4.T2.17.17.17.2.m1.1.1.cmml" xref="S4.T2.17.17.17.2.m1.1.1">0.717</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.17.17.17.2.m1.1c">0.717</annotation></semantics></math></td>
<td id="S4.T2.18.18.18.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S4.T2.18.18.18.3.m1.1" class="ltx_Math" alttext="0.714" display="inline"><semantics id="S4.T2.18.18.18.3.m1.1a"><mn id="S4.T2.18.18.18.3.m1.1.1" xref="S4.T2.18.18.18.3.m1.1.1.cmml">0.714</mn><annotation-xml encoding="MathML-Content" id="S4.T2.18.18.18.3.m1.1b"><cn type="float" id="S4.T2.18.18.18.3.m1.1.1.cmml" xref="S4.T2.18.18.18.3.m1.1.1">0.714</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.18.18.18.3.m1.1c">0.714</annotation></semantics></math></td>
<td id="S4.T2.18.18.18.4" class="ltx_td ltx_align_right ltx_border_t">111.7M</td>
</tr>
<tr id="S4.T2.21.21.21" class="ltx_tr">
<td id="S4.T2.19.19.19.1" class="ltx_td ltx_align_right ltx_border_r"><span id="S4.T2.19.19.19.1.1" class="ltx_text ltx_font_bold">BERT + ViT<sup id="S4.T2.19.19.19.1.1.1" class="ltx_sup"><span id="S4.T2.19.19.19.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">⋆</span></sup></span></td>
<td id="S4.T2.20.20.20.2" class="ltx_td ltx_align_center"><span id="S4.T2.20.20.20.2.1" class="ltx_text ltx_markedasmath ltx_font_bold">0.747</span></td>
<td id="S4.T2.21.21.21.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T2.21.21.21.3.1" class="ltx_text ltx_markedasmath ltx_font_bold">0.749</span></td>
<td id="S4.T2.21.21.21.4" class="ltx_td ltx_align_right">196.8M</td>
</tr>
<tr id="S4.T2.24.24.24" class="ltx_tr">
<td id="S4.T2.22.22.22.1" class="ltx_td ltx_align_right ltx_border_r"><span id="S4.T2.22.22.22.1.1" class="ltx_text ltx_font_bold">CLIP<sup id="S4.T2.22.22.22.1.1.1" class="ltx_sup"><span id="S4.T2.22.22.22.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">⋆</span></sup></span></td>
<td id="S4.T2.23.23.23.2" class="ltx_td ltx_align_center"><math id="S4.T2.23.23.23.2.m1.1" class="ltx_Math" alttext="0.431" display="inline"><semantics id="S4.T2.23.23.23.2.m1.1a"><mn id="S4.T2.23.23.23.2.m1.1.1" xref="S4.T2.23.23.23.2.m1.1.1.cmml">0.431</mn><annotation-xml encoding="MathML-Content" id="S4.T2.23.23.23.2.m1.1b"><cn type="float" id="S4.T2.23.23.23.2.m1.1.1.cmml" xref="S4.T2.23.23.23.2.m1.1.1">0.431</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.23.23.23.2.m1.1c">0.431</annotation></semantics></math></td>
<td id="S4.T2.24.24.24.3" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.24.24.24.3.m1.1" class="ltx_Math" alttext="0.298" display="inline"><semantics id="S4.T2.24.24.24.3.m1.1a"><mn id="S4.T2.24.24.24.3.m1.1.1" xref="S4.T2.24.24.24.3.m1.1.1.cmml">0.298</mn><annotation-xml encoding="MathML-Content" id="S4.T2.24.24.24.3.m1.1b"><cn type="float" id="S4.T2.24.24.24.3.m1.1.1.cmml" xref="S4.T2.24.24.24.3.m1.1.1">0.298</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.24.24.24.3.m1.1c">0.298</annotation></semantics></math></td>
<td id="S4.T2.24.24.24.4" class="ltx_td ltx_align_right">151.3M</td>
</tr>
<tr id="S4.T2.27.27.27" class="ltx_tr">
<td id="S4.T2.25.25.25.1" class="ltx_td ltx_align_right ltx_border_r"><span id="S4.T2.25.25.25.1.1" class="ltx_text ltx_font_bold">BLIP<sup id="S4.T2.25.25.25.1.1.1" class="ltx_sup"><span id="S4.T2.25.25.25.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">⋆</span></sup></span></td>
<td id="S4.T2.26.26.26.2" class="ltx_td ltx_align_center"><math id="S4.T2.26.26.26.2.m1.1" class="ltx_Math" alttext="0.462" display="inline"><semantics id="S4.T2.26.26.26.2.m1.1a"><mn id="S4.T2.26.26.26.2.m1.1.1" xref="S4.T2.26.26.26.2.m1.1.1.cmml">0.462</mn><annotation-xml encoding="MathML-Content" id="S4.T2.26.26.26.2.m1.1b"><cn type="float" id="S4.T2.26.26.26.2.m1.1.1.cmml" xref="S4.T2.26.26.26.2.m1.1.1">0.462</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.26.26.26.2.m1.1c">0.462</annotation></semantics></math></td>
<td id="S4.T2.27.27.27.3" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.27.27.27.3.m1.1" class="ltx_Math" alttext="0.292" display="inline"><semantics id="S4.T2.27.27.27.3.m1.1a"><mn id="S4.T2.27.27.27.3.m1.1.1" xref="S4.T2.27.27.27.3.m1.1.1.cmml">0.292</mn><annotation-xml encoding="MathML-Content" id="S4.T2.27.27.27.3.m1.1b"><cn type="float" id="S4.T2.27.27.27.3.m1.1.1.cmml" xref="S4.T2.27.27.27.3.m1.1.1">0.292</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.27.27.27.3.m1.1c">0.292</annotation></semantics></math></td>
<td id="S4.T2.27.27.27.4" class="ltx_td ltx_align_right">470M</td>
</tr>
<tr id="S4.T2.30.30.30" class="ltx_tr">
<td id="S4.T2.28.28.28.1" class="ltx_td ltx_align_right ltx_border_r">
<span id="S4.T2.28.28.28.1.1" class="ltx_text ltx_font_bold">IDEFICS<sup id="S4.T2.28.28.28.1.1.1" class="ltx_sup"><span id="S4.T2.28.28.28.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">⋆</span></sup></span> (zero-shot)</td>
<td id="S4.T2.29.29.29.2" class="ltx_td ltx_align_center"><math id="S4.T2.29.29.29.2.m1.1" class="ltx_Math" alttext="0.347" display="inline"><semantics id="S4.T2.29.29.29.2.m1.1a"><mn id="S4.T2.29.29.29.2.m1.1.1" xref="S4.T2.29.29.29.2.m1.1.1.cmml">0.347</mn><annotation-xml encoding="MathML-Content" id="S4.T2.29.29.29.2.m1.1b"><cn type="float" id="S4.T2.29.29.29.2.m1.1.1.cmml" xref="S4.T2.29.29.29.2.m1.1.1">0.347</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.29.29.29.2.m1.1c">0.347</annotation></semantics></math></td>
<td id="S4.T2.30.30.30.3" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.30.30.30.3.m1.1" class="ltx_Math" alttext="0.270" display="inline"><semantics id="S4.T2.30.30.30.3.m1.1a"><mn id="S4.T2.30.30.30.3.m1.1.1" xref="S4.T2.30.30.30.3.m1.1.1.cmml">0.270</mn><annotation-xml encoding="MathML-Content" id="S4.T2.30.30.30.3.m1.1b"><cn type="float" id="S4.T2.30.30.30.3.m1.1.1.cmml" xref="S4.T2.30.30.30.3.m1.1.1">0.270</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.30.30.30.3.m1.1c">0.270</annotation></semantics></math></td>
<td id="S4.T2.30.30.30.4" class="ltx_td ltx_align_right">9B</td>
</tr>
<tr id="S4.T2.33.33.33" class="ltx_tr">
<td id="S4.T2.31.31.31.1" class="ltx_td ltx_align_right ltx_border_r">
<span id="S4.T2.31.31.31.1.1" class="ltx_text ltx_font_bold">IDEFICS<sup id="S4.T2.31.31.31.1.1.1" class="ltx_sup"><span id="S4.T2.31.31.31.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">⋆</span></sup></span> (fine-tuned)</td>
<td id="S4.T2.32.32.32.2" class="ltx_td ltx_align_center"><math id="S4.T2.32.32.32.2.m1.1" class="ltx_Math" alttext="0.600" display="inline"><semantics id="S4.T2.32.32.32.2.m1.1a"><mn id="S4.T2.32.32.32.2.m1.1.1" xref="S4.T2.32.32.32.2.m1.1.1.cmml">0.600</mn><annotation-xml encoding="MathML-Content" id="S4.T2.32.32.32.2.m1.1b"><cn type="float" id="S4.T2.32.32.32.2.m1.1.1.cmml" xref="S4.T2.32.32.32.2.m1.1.1">0.600</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.32.32.32.2.m1.1c">0.600</annotation></semantics></math></td>
<td id="S4.T2.33.33.33.3" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T2.33.33.33.3.m1.1" class="ltx_Math" alttext="0.591" display="inline"><semantics id="S4.T2.33.33.33.3.m1.1a"><mn id="S4.T2.33.33.33.3.m1.1.1" xref="S4.T2.33.33.33.3.m1.1.1.cmml">0.591</mn><annotation-xml encoding="MathML-Content" id="S4.T2.33.33.33.3.m1.1b"><cn type="float" id="S4.T2.33.33.33.3.m1.1.1.cmml" xref="S4.T2.33.33.33.3.m1.1.1">0.591</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.33.33.33.3.m1.1c">0.591</annotation></semantics></math></td>
<td id="S4.T2.33.33.33.4" class="ltx_td ltx_align_right">9B</td>
</tr>
<tr id="S4.T2.35.35.35" class="ltx_tr">
<td id="S4.T2.35.35.35.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T2.35.35.35.3.1" class="ltx_text ltx_font_smallcaps">Human</span></td>
<td id="S4.T2.34.34.34.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.34.34.34.1.1" class="ltx_text ltx_markedasmath ltx_font_bold">0.826</span></td>
<td id="S4.T2.35.35.35.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T2.35.35.35.2.1" class="ltx_text ltx_markedasmath ltx_font_bold">0.823</span></td>
<td id="S4.T2.35.35.35.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">-</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Text-only<sup id="S4.T2.45.1" class="ltx_sup"><span id="S4.T2.45.1.1" class="ltx_text ltx_font_italic">♣</span></sup>, image-only<sup id="S4.T2.46.2" class="ltx_sup"><span id="S4.T2.46.2.1" class="ltx_text ltx_font_italic">♠</span></sup>, and multimodal<sup id="S4.T2.47.3" class="ltx_sup"><span id="S4.T2.47.3.1" class="ltx_text ltx_font_italic">⋆</span></sup> model results on the MultiClimate test set.</figcaption>
</figure>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">BERT results are outstanding.</h4>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.2" class="ltx_p">The text-only BERT model achieves the best performance among single-modal models, notably surpassing the zero-shot LLMs.
Furthermore, the multimodal fusion model BERT + ViT achieves state-of-the-art, <math id="S4.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="0.747" display="inline"><semantics id="S4.SS2.SSS0.Px1.p1.1.m1.1a"><mn id="S4.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">0.747</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.1.m1.1b"><cn type="float" id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1">0.747</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.1.m1.1c">0.747</annotation></semantics></math> in accuracy and <math id="S4.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="0.749" display="inline"><semantics id="S4.SS2.SSS0.Px1.p1.2.m2.1a"><mn id="S4.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.cmml">0.749</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.2.m2.1b"><cn type="float" id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1">0.749</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.2.m2.1c">0.749</annotation></semantics></math> in F1 score. Generally speaking, BERT + ResNet50/ViT fusion models outperform CLIP, BLIP, and IDEFICS, as textual features are crucial to our CC stance detection, and transcripts in YouTube videos benefit the already well-performing BERT model.</p>
</div>
<div id="S4.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p2.2" class="ltx_p">CLIP performs the worst in accuracy among trained multimodal models.
One hypothesis is that the maximum sequence length has an impact on the results,
that is, the maximum sequence length of CLIP is <math id="S4.SS2.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="77" display="inline"><semantics id="S4.SS2.SSS0.Px1.p2.1.m1.1a"><mn id="S4.SS2.SSS0.Px1.p2.1.m1.1.1" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1.cmml">77</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p2.1.m1.1b"><cn type="integer" id="S4.SS2.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px1.p2.1.m1.1.1">77</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p2.1.m1.1c">77</annotation></semantics></math> tokens for text inputs, much shorter than BERT, and leads to declined performance due to a lack of information.
For instance, the “MACC” video includes one sentence with <math id="S4.SS2.SSS0.Px1.p2.2.m2.1" class="ltx_Math" alttext="82" display="inline"><semantics id="S4.SS2.SSS0.Px1.p2.2.m2.1a"><mn id="S4.SS2.SSS0.Px1.p2.2.m2.1.1" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1.cmml">82</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p2.2.m2.1b"><cn type="integer" id="S4.SS2.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px1.p2.2.m2.1.1">82</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p2.2.m2.1c">82</annotation></semantics></math> tokens, exceeding CLIP’s limit (77 tokens) but not BERT’s.
Notably, fine-tuned BLIP shows similar poor performance, in particular low weighted F1 score, which can be attributed to the misclassification of minority classes such as <span id="S4.SS2.SSS0.Px1.p2.2.1" class="ltx_text ltx_font_smallcaps">Neutral</span> and <span id="S4.SS2.SSS0.Px1.p2.2.2" class="ltx_text ltx_font_smallcaps">Oppose</span>.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Textual and visual information compensate each other.</h4>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p1.1" class="ltx_p"><a href="#S4.T2" title="In 4.2 Results and Discussions ‣ 4 MultiClimate Stance Detection ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a> shows text-only models overall perform considerably better than image-only models,
indicating that specific language contextual understanding can outperform approaches that are trained for detailed image classification.
However, if both visual and textual information are concatenated, we can achieve the optimal result.
We note that even though transcripts generally contain richer linguistic information, it is not guaranteed that visual and textual information are both meaningful in every frame-transcript instance.
For example, video segments with no speech but only music playing in the background are transcribed as <span id="S4.SS2.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_typewriter">[Music]</span> in YouTube.
Text-dependent models can barely capture stances from text data in such cases, while the image can reveal additional information, and hence visual-informed models predict labels that align with human annotations; see the last <math id="S4.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S4.SS2.SSS0.Px2.p1.1.m1.1a"><mn id="S4.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px2.p1.1.m1.1b"><cn type="integer" id="S4.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px2.p1.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px2.p1.1.m1.1c">6</annotation></semantics></math> frame-transcript pairs of “AMCC” in <a href="#A5" title="Appendix E Sample Responses Generated from Llama3 and Gemma2 ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Text-only Llama3 wins in zero-shot.</h4>

<div id="S4.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px3.p1.2" class="ltx_p">We also observe that Llama3 performs better than single-modal Gemma2 and multimodal IDEFICS within zero-shot,
with a marginally increase of <math id="S4.SS2.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="0.138" display="inline"><semantics id="S4.SS2.SSS0.Px3.p1.1.m1.1a"><mn id="S4.SS2.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px3.p1.1.m1.1.1.cmml">0.138</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px3.p1.1.m1.1b"><cn type="float" id="S4.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px3.p1.1.m1.1.1">0.138</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px3.p1.1.m1.1c">0.138</annotation></semantics></math> in accuracy and <math id="S4.SS2.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="0.181" display="inline"><semantics id="S4.SS2.SSS0.Px3.p1.2.m2.1a"><mn id="S4.SS2.SSS0.Px3.p1.2.m2.1.1" xref="S4.SS2.SSS0.Px3.p1.2.m2.1.1.cmml">0.181</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px3.p1.2.m2.1b"><cn type="float" id="S4.SS2.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px3.p1.2.m2.1.1">0.181</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px3.p1.2.m2.1c">0.181</annotation></semantics></math> in F1 score compared with 9B-sized IDEFICS.
Since Llama3 has been trained on more extensive text and speech data from various domains,
it can better process longer sequences of text <cite class="ltx_cite ltx_citemacro_cite">Dubey et al. (<a href="#bib.bib9" title="" class="ltx_ref">2024</a>)</cite>.
Meanwhile, we hypothesize that Llama3 is better at handling noisy data, which is common in the transcripts as many videos feature colloquial speech (e.g., in “ACCFP”), thereby increasing the noise level.
The results, alike the earlier BERT superior performances, suggest that the textual part is vital in CC stance detection due to its more explicit narrative and clearer directionality, and leveraging SOTA LLMs can significantly improve performance.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Zero-shot IDEFICS is biased toward <span id="S4.SS2.SSS0.Px4.1.1" class="ltx_text ltx_font_smallcaps">Support</span>.</h4>

<div id="S4.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px4.p1.11" class="ltx_p">We also observe a tendency for the zero-shot IDEFICS model to predominantly predict <span id="S4.SS2.SSS0.Px4.p1.11.1" class="ltx_text ltx_font_smallcaps">Support</span> labels, less often <span id="S4.SS2.SSS0.Px4.p1.11.2" class="ltx_text ltx_font_smallcaps">Neutral</span>, and rarely <span id="S4.SS2.SSS0.Px4.p1.11.3" class="ltx_text ltx_font_smallcaps">Oppose</span>.
This bias impacts the model’s performance negatively on videos where the majority of gold labels are <span id="S4.SS2.SSS0.Px4.p1.11.4" class="ltx_text ltx_font_smallcaps">Oppose</span> or <span id="S4.SS2.SSS0.Px4.p1.11.5" class="ltx_text ltx_font_smallcaps">Neutral</span>.
This accounts for the poor performance observed in videos like “CCUIM” (Acc/F1: <math id="S4.SS2.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="0.167/0.111" display="inline"><semantics id="S4.SS2.SSS0.Px4.p1.1.m1.1a"><mrow id="S4.SS2.SSS0.Px4.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.cmml"><mn id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.2" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.2.cmml">0.167</mn><mo id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.cmml">/</mo><mn id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.3" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.3.cmml">0.111</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px4.p1.1.m1.1b"><apply id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1"><divide id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1"></divide><cn type="float" id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.2">0.167</cn><cn type="float" id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.3">0.111</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px4.p1.1.m1.1c">0.167/0.111</annotation></semantics></math>; <math id="S4.SS2.SSS0.Px4.p1.2.m2.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S4.SS2.SSS0.Px4.p1.2.m2.1a"><mn id="S4.SS2.SSS0.Px4.p1.2.m2.1.1" xref="S4.SS2.SSS0.Px4.p1.2.m2.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px4.p1.2.m2.1b"><cn type="integer" id="S4.SS2.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.2.m2.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px4.p1.2.m2.1c">7</annotation></semantics></math> <span id="S4.SS2.SSS0.Px4.p1.11.6" class="ltx_text ltx_font_smallcaps">Support</span>, <math id="S4.SS2.SSS0.Px4.p1.3.m3.1" class="ltx_Math" alttext="21" display="inline"><semantics id="S4.SS2.SSS0.Px4.p1.3.m3.1a"><mn id="S4.SS2.SSS0.Px4.p1.3.m3.1.1" xref="S4.SS2.SSS0.Px4.p1.3.m3.1.1.cmml">21</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px4.p1.3.m3.1b"><cn type="integer" id="S4.SS2.SSS0.Px4.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.3.m3.1.1">21</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px4.p1.3.m3.1c">21</annotation></semantics></math> <span id="S4.SS2.SSS0.Px4.p1.11.7" class="ltx_text ltx_font_smallcaps">Neutral</span> and 20 <span id="S4.SS2.SSS0.Px4.p1.11.8" class="ltx_text ltx_font_smallcaps">Oppose</span>) and “EWCC” (Acc/F1: <math id="S4.SS2.SSS0.Px4.p1.4.m4.1" class="ltx_Math" alttext="0.163/0.109" display="inline"><semantics id="S4.SS2.SSS0.Px4.p1.4.m4.1a"><mrow id="S4.SS2.SSS0.Px4.p1.4.m4.1.1" xref="S4.SS2.SSS0.Px4.p1.4.m4.1.1.cmml"><mn id="S4.SS2.SSS0.Px4.p1.4.m4.1.1.2" xref="S4.SS2.SSS0.Px4.p1.4.m4.1.1.2.cmml">0.163</mn><mo id="S4.SS2.SSS0.Px4.p1.4.m4.1.1.1" xref="S4.SS2.SSS0.Px4.p1.4.m4.1.1.1.cmml">/</mo><mn id="S4.SS2.SSS0.Px4.p1.4.m4.1.1.3" xref="S4.SS2.SSS0.Px4.p1.4.m4.1.1.3.cmml">0.109</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px4.p1.4.m4.1b"><apply id="S4.SS2.SSS0.Px4.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.4.m4.1.1"><divide id="S4.SS2.SSS0.Px4.p1.4.m4.1.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.4.m4.1.1.1"></divide><cn type="float" id="S4.SS2.SSS0.Px4.p1.4.m4.1.1.2.cmml" xref="S4.SS2.SSS0.Px4.p1.4.m4.1.1.2">0.163</cn><cn type="float" id="S4.SS2.SSS0.Px4.p1.4.m4.1.1.3.cmml" xref="S4.SS2.SSS0.Px4.p1.4.m4.1.1.3">0.109</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px4.p1.4.m4.1c">0.163/0.109</annotation></semantics></math>; <math id="S4.SS2.SSS0.Px4.p1.5.m5.1" class="ltx_Math" alttext="9" display="inline"><semantics id="S4.SS2.SSS0.Px4.p1.5.m5.1a"><mn id="S4.SS2.SSS0.Px4.p1.5.m5.1.1" xref="S4.SS2.SSS0.Px4.p1.5.m5.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px4.p1.5.m5.1b"><cn type="integer" id="S4.SS2.SSS0.Px4.p1.5.m5.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.5.m5.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px4.p1.5.m5.1c">9</annotation></semantics></math> <span id="S4.SS2.SSS0.Px4.p1.11.9" class="ltx_text ltx_font_smallcaps">Support</span>, <math id="S4.SS2.SSS0.Px4.p1.6.m6.1" class="ltx_Math" alttext="11" display="inline"><semantics id="S4.SS2.SSS0.Px4.p1.6.m6.1a"><mn id="S4.SS2.SSS0.Px4.p1.6.m6.1.1" xref="S4.SS2.SSS0.Px4.p1.6.m6.1.1.cmml">11</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px4.p1.6.m6.1b"><cn type="integer" id="S4.SS2.SSS0.Px4.p1.6.m6.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.6.m6.1.1">11</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px4.p1.6.m6.1c">11</annotation></semantics></math> <span id="S4.SS2.SSS0.Px4.p1.11.10" class="ltx_text ltx_font_smallcaps">Neutral</span> and <math id="S4.SS2.SSS0.Px4.p1.7.m7.1" class="ltx_Math" alttext="29" display="inline"><semantics id="S4.SS2.SSS0.Px4.p1.7.m7.1a"><mn id="S4.SS2.SSS0.Px4.p1.7.m7.1.1" xref="S4.SS2.SSS0.Px4.p1.7.m7.1.1.cmml">29</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px4.p1.7.m7.1b"><cn type="integer" id="S4.SS2.SSS0.Px4.p1.7.m7.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.7.m7.1.1">29</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px4.p1.7.m7.1c">29</annotation></semantics></math> <span id="S4.SS2.SSS0.Px4.p1.11.11" class="ltx_text ltx_font_smallcaps">Oppose</span>).
In contrast, the human inter-annotator Acc/F1 scores on these two test documents are not low: <math id="S4.SS2.SSS0.Px4.p1.8.m8.1" class="ltx_Math" alttext="0.771" display="inline"><semantics id="S4.SS2.SSS0.Px4.p1.8.m8.1a"><mn id="S4.SS2.SSS0.Px4.p1.8.m8.1.1" xref="S4.SS2.SSS0.Px4.p1.8.m8.1.1.cmml">0.771</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px4.p1.8.m8.1b"><cn type="float" id="S4.SS2.SSS0.Px4.p1.8.m8.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.8.m8.1.1">0.771</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px4.p1.8.m8.1c">0.771</annotation></semantics></math>/<math id="S4.SS2.SSS0.Px4.p1.9.m9.1" class="ltx_Math" alttext="0.773" display="inline"><semantics id="S4.SS2.SSS0.Px4.p1.9.m9.1a"><mn id="S4.SS2.SSS0.Px4.p1.9.m9.1.1" xref="S4.SS2.SSS0.Px4.p1.9.m9.1.1.cmml">0.773</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px4.p1.9.m9.1b"><cn type="float" id="S4.SS2.SSS0.Px4.p1.9.m9.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.9.m9.1.1">0.773</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px4.p1.9.m9.1c">0.773</annotation></semantics></math> on “CCUIM” and <math id="S4.SS2.SSS0.Px4.p1.10.m10.1" class="ltx_Math" alttext="0.816" display="inline"><semantics id="S4.SS2.SSS0.Px4.p1.10.m10.1a"><mn id="S4.SS2.SSS0.Px4.p1.10.m10.1.1" xref="S4.SS2.SSS0.Px4.p1.10.m10.1.1.cmml">0.816</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px4.p1.10.m10.1b"><cn type="float" id="S4.SS2.SSS0.Px4.p1.10.m10.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.10.m10.1.1">0.816</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px4.p1.10.m10.1c">0.816</annotation></semantics></math>/<math id="S4.SS2.SSS0.Px4.p1.11.m11.1" class="ltx_Math" alttext="0.811" display="inline"><semantics id="S4.SS2.SSS0.Px4.p1.11.m11.1a"><mn id="S4.SS2.SSS0.Px4.p1.11.m11.1.1" xref="S4.SS2.SSS0.Px4.p1.11.m11.1.1.cmml">0.811</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px4.p1.11.m11.1b"><cn type="float" id="S4.SS2.SSS0.Px4.p1.11.m11.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.11.m11.1.1">0.811</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px4.p1.11.m11.1c">0.811</annotation></semantics></math> on “EWCC,” illustrating that such bias is not present during human annotation.</p>
</div>
<div id="S4.SS2.SSS0.Px4.p2" class="ltx_para">
<p id="S4.SS2.SSS0.Px4.p2.1" class="ltx_p">The underlying reason for this annotation bias can stem from the mix of “stance” and “sentiment”.
During manual annotation, instances that are negatively framed are often categorized as <span id="S4.SS2.SSS0.Px4.p2.1.1" class="ltx_text ltx_font_smallcaps">Oppose</span>.
However, the model classifies based on the expressed “stance” towards CC.
Given that our video selection was filtered under “climate change”, a <span id="S4.SS2.SSS0.Px4.p2.1.2" class="ltx_text ltx_font_smallcaps">Support</span> stance predominates.
Additionally, the model occasionally conflates frame information,
whereas annotations are based on both the frame and paired transcripts. Consequently, frames opposing climate change are undervalued in the classification of the model,
leading to a pronounced bias towards <span id="S4.SS2.SSS0.Px4.p2.1.3" class="ltx_text ltx_font_smallcaps">Neutral</span> and <span id="S4.SS2.SSS0.Px4.p2.1.4" class="ltx_text ltx_font_smallcaps">Support</span> categories.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Fine-tuned IDEFICS reduces stance bias.</h4>

<div id="S4.SS2.SSS0.Px5.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px5.p1.1" class="ltx_p">To mitigate biases present in the zero-shot IDEFICS model,
we fine-tune IDEFICS using LoRA (Low-Rank Adaptation, <cite class="ltx_cite ltx_citemacro_citet">Hu et al. <a href="#bib.bib13" title="" class="ltx_ref">2022</a></cite>)
on 80/10 MultiClimate train/dev videos before evaluation;
see <a href="#A4" title="Appendix D Hyperparameters for Fine-tuning ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">D</span></a> for a list of hyperparameters.</p>
</div>
<div id="S4.SS2.SSS0.Px5.p2" class="ltx_para">
<p id="S4.SS2.SSS0.Px5.p2.6" class="ltx_p">Fine-tuning results in a significant increase in model performance. The accuracy increases from <math id="S4.SS2.SSS0.Px5.p2.1.m1.1" class="ltx_Math" alttext="0.347" display="inline"><semantics id="S4.SS2.SSS0.Px5.p2.1.m1.1a"><mn id="S4.SS2.SSS0.Px5.p2.1.m1.1.1" xref="S4.SS2.SSS0.Px5.p2.1.m1.1.1.cmml">0.347</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px5.p2.1.m1.1b"><cn type="float" id="S4.SS2.SSS0.Px5.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px5.p2.1.m1.1.1">0.347</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px5.p2.1.m1.1c">0.347</annotation></semantics></math> to <math id="S4.SS2.SSS0.Px5.p2.2.m2.1" class="ltx_Math" alttext="0.600" display="inline"><semantics id="S4.SS2.SSS0.Px5.p2.2.m2.1a"><mn id="S4.SS2.SSS0.Px5.p2.2.m2.1.1" xref="S4.SS2.SSS0.Px5.p2.2.m2.1.1.cmml">0.600</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px5.p2.2.m2.1b"><cn type="float" id="S4.SS2.SSS0.Px5.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px5.p2.2.m2.1.1">0.600</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px5.p2.2.m2.1c">0.600</annotation></semantics></math>, and
the F1 score improves from <math id="S4.SS2.SSS0.Px5.p2.3.m3.1" class="ltx_Math" alttext="0.270" display="inline"><semantics id="S4.SS2.SSS0.Px5.p2.3.m3.1a"><mn id="S4.SS2.SSS0.Px5.p2.3.m3.1.1" xref="S4.SS2.SSS0.Px5.p2.3.m3.1.1.cmml">0.270</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px5.p2.3.m3.1b"><cn type="float" id="S4.SS2.SSS0.Px5.p2.3.m3.1.1.cmml" xref="S4.SS2.SSS0.Px5.p2.3.m3.1.1">0.270</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px5.p2.3.m3.1c">0.270</annotation></semantics></math> to <math id="S4.SS2.SSS0.Px5.p2.4.m4.1" class="ltx_Math" alttext="0.591" display="inline"><semantics id="S4.SS2.SSS0.Px5.p2.4.m4.1a"><mn id="S4.SS2.SSS0.Px5.p2.4.m4.1.1" xref="S4.SS2.SSS0.Px5.p2.4.m4.1.1.cmml">0.591</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px5.p2.4.m4.1b"><cn type="float" id="S4.SS2.SSS0.Px5.p2.4.m4.1.1.cmml" xref="S4.SS2.SSS0.Px5.p2.4.m4.1.1">0.591</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px5.p2.4.m4.1c">0.591</annotation></semantics></math>. These enhancements suggest that the model has developed a stronger capability for CC stance detection.
Besides the higher Acc/F1, after examining the predictions made by the model before and after the fine-tuning process (see <a href="#A3" title="Appendix C IDEFICS Confusion Matrix of Predictions Before and After Fine-tuning ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">C</span></a>),
we observe a marked increase in the number of instances classified as <span id="S4.SS2.SSS0.Px5.p2.6.1" class="ltx_text ltx_font_smallcaps">Oppose</span>, particularly the percentage of correctly predicted gold <span id="S4.SS2.SSS0.Px5.p2.6.2" class="ltx_text ltx_font_smallcaps">Oppose</span> labels increased from <math id="S4.SS2.SSS0.Px5.p2.5.m5.1" class="ltx_Math" alttext="0.00\%" display="inline"><semantics id="S4.SS2.SSS0.Px5.p2.5.m5.1a"><mrow id="S4.SS2.SSS0.Px5.p2.5.m5.1.1" xref="S4.SS2.SSS0.Px5.p2.5.m5.1.1.cmml"><mn id="S4.SS2.SSS0.Px5.p2.5.m5.1.1.2" xref="S4.SS2.SSS0.Px5.p2.5.m5.1.1.2.cmml">0.00</mn><mo id="S4.SS2.SSS0.Px5.p2.5.m5.1.1.1" xref="S4.SS2.SSS0.Px5.p2.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px5.p2.5.m5.1b"><apply id="S4.SS2.SSS0.Px5.p2.5.m5.1.1.cmml" xref="S4.SS2.SSS0.Px5.p2.5.m5.1.1"><csymbol cd="latexml" id="S4.SS2.SSS0.Px5.p2.5.m5.1.1.1.cmml" xref="S4.SS2.SSS0.Px5.p2.5.m5.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.SSS0.Px5.p2.5.m5.1.1.2.cmml" xref="S4.SS2.SSS0.Px5.p2.5.m5.1.1.2">0.00</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px5.p2.5.m5.1c">0.00\%</annotation></semantics></math> to <math id="S4.SS2.SSS0.Px5.p2.6.m6.1" class="ltx_Math" alttext="83.01\%" display="inline"><semantics id="S4.SS2.SSS0.Px5.p2.6.m6.1a"><mrow id="S4.SS2.SSS0.Px5.p2.6.m6.1.1" xref="S4.SS2.SSS0.Px5.p2.6.m6.1.1.cmml"><mn id="S4.SS2.SSS0.Px5.p2.6.m6.1.1.2" xref="S4.SS2.SSS0.Px5.p2.6.m6.1.1.2.cmml">83.01</mn><mo id="S4.SS2.SSS0.Px5.p2.6.m6.1.1.1" xref="S4.SS2.SSS0.Px5.p2.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px5.p2.6.m6.1b"><apply id="S4.SS2.SSS0.Px5.p2.6.m6.1.1.cmml" xref="S4.SS2.SSS0.Px5.p2.6.m6.1.1"><csymbol cd="latexml" id="S4.SS2.SSS0.Px5.p2.6.m6.1.1.1.cmml" xref="S4.SS2.SSS0.Px5.p2.6.m6.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.SSS0.Px5.p2.6.m6.1.1.2.cmml" xref="S4.SS2.SSS0.Px5.p2.6.m6.1.1.2">83.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px5.p2.6.m6.1c">83.01\%</annotation></semantics></math>, indicating a better performance and less model bias.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Future Directions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This paper curates MultiClimate, a novel multimodal stance detection dataset to support video research on climate change.
Our results show that BERT-fusion models can achieve considerably higher performance than large multimodal models. Textual information is vital on MultiClimate,
while the visual modality can compensate for the drawbacks of language models, and the best is achieved when combining image and text information.
Similarly, SOTA LLMs beat large multimodal models in zero-shot.
Our work provides a foundation for multimodal stance detection in CC.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">We plan to extend our dataset with more videos and annotations
and further investigate the interactions between visual and textual features, as well as the informativeness of each modality in manual stance labeling.
We are also interested in expanding to audio and video modalities, as well as unimodal models using transfer learning techniques, to improve performance and explain stance detection in CC. We hope our work fosters the positive social impact of CC stance detection in the NLP field.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">While our study makes valuable contributions by enriching the multimodal climate change dataset and exploring the performance of different modalities in stance detection tasks, it is important to recognize several inherent limitations.
First, the dataset annotation was conducted by a limited number of annotators, which may introduce personal biases into the data.
Second, large language models are not fine-tuned due to limited computational resources.
Third, our study focuses solely on transcripts and frames. Incorporating audio and video modalities can enhance the understanding of speaker’s emotions and intentions, and potentially further improve stance detection.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">We thank anonymous reviewers for their constructive feedback on this work.
This research project is in parts funded by Deutschlandstipendium, AWS (Amazon Web Service).
This work also partially belongs to the KLIMA-MEMES project funded by the Bavarian Research Institute for Digital Transformation (bidt), an institute of the Bavarian Academy of Sciences and Humanities.
The authors are responsible for the content of this publication.
</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aharonson (2023)</span>
<span class="ltx_bibblock">
Vered Aharonson. 2023.

</span>
<span class="ltx_bibblock">Data of: Engaging for change-investigating youtube videos and users
comments.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Mendeley Data, V1, doi: 10.17632/j955mxnyyf.1</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al. (2022)</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds,
Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina
Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy
Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo
Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://papers.nips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html" title="" class="ltx_ref ltx_href">Flamingo: a visual language model for few-shot learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 35: Annual
Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New
Orleans, LA, USA, November 28 - December 9, 2022</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bojanowski et al. (2017)</span>
<span class="ltx_bibblock">
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00051" title="" class="ltx_ref ltx_href">Enriching word vectors
with subword information</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
5:135–146.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carnot et al. (2023)</span>
<span class="ltx_bibblock">
Miriam Louise Carnot, Lorenz Heinemann, Jan Braker, Tobias Schreieder, Johannes
Kiesel, Maik Fröbe, Martin Potthast, and Benno Stein. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3539618.3591917" title="" class="ltx_ref ltx_href">On stance detection
in image retrieval for argumentation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 46th International ACM SIGIR
Conference on Research and Development in Information Retrieval, SIGIR
2023, Taipei, Taiwan, July 23-27, 2023</em>, pages 2562–2571. ACM.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clement and Warkentin (2024)</span>
<span class="ltx_bibblock">
Farabet Clement and Tris Warkentin. 2024.

</span>
<span class="ltx_bibblock">Gemma 2 is now available to researchers and developers.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Google Deepmind</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conforti et al. (2020)</span>
<span class="ltx_bibblock">
Costanza Conforti, Jakob Berndt, Mohammad Taher Pilehvar, Chryssi Giannitsarou,
Flavio Toxvaerd, and Nigel Collier. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/2020.ACL-MAIN.157" title="" class="ltx_ref ltx_href">Will-they-won’t-they: A very large dataset for stance detection on
twitter</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics, ACL 2020, Online, July 5-10, 2020</em>, pages
1715–1724. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N19-1423" title="" class="ltx_ref ltx_href">BERT: Pre-training of
deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186,
Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2021)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=YicbFdNTTy" title="" class="ltx_ref ltx_href">An image is worth
16x16 words: Transformers for image recognition at scale</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et al. (2024)</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad
Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan,
et al. 2024.

</span>
<span class="ltx_bibblock">The llama 3 herd of models.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2407.21783</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fraile-Hernandez and
Peñas (2024)</span>
<span class="ltx_bibblock">
Jesus M. Fraile-Hernandez and Anselmo Peñas. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2024.case-1.10" title="" class="ltx_ref ltx_href">HAMiSoN-generative at ClimateActivism 2024: Stance detection using
generative large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 7th Workshop on Challenges and
Applications of Automated Extraction of Socio-political Events from Text
(CASE 2024)</em>, pages 79–84, St. Julians, Malta. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hardalov et al. (2022)</span>
<span class="ltx_bibblock">
Momchil Hardalov, Arnav Arora, Preslav Nakov, and Isabelle Augenstein. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.findings-naacl.94" title="" class="ltx_ref ltx_href">A survey
on stance detection for mis- and disinformation identification</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
NAACL 2022</em>, pages 1259–1277, Seattle, United States. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2016.90" title="" class="ltx_ref ltx_href">Deep residual learning
for image recognition</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016</em>, pages
770–778. IEEE Computer Society.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2022)</span>
<span class="ltx_bibblock">
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
Shean Wang, Lu Wang, and Weizhu Chen. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="" class="ltx_ref ltx_href">Lora: Low-rank
adaptation of large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>.
OpenReview.net.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Küçük and Can (2021a)</span>
<span class="ltx_bibblock">
Dilek Küçük and Fazli Can. 2021a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3369026" title="" class="ltx_ref ltx_href">Stance detection: A
survey</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">ACM Comput. Surv.</em>, 53(1):12:1–12:37.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Küçük and Can (2021b)</span>
<span class="ltx_bibblock">
Dilek Küçük and Fazli Can. 2021b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3404835.3462815" title="" class="ltx_ref ltx_href">Stance detection:
Concepts, approaches, resources, and outstanding issues</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">SIGIR ’21: The 44th International ACM SIGIR Conference
on Research and Development in Information Retrieval, Virtual Event, Canada,
July 11-15, 2021</em>, pages 2673–2676. ACM.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lan et al. (2024)</span>
<span class="ltx_bibblock">
Xiaochong Lan, Chen Gao, Depeng Jin, and Yong Li. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1609/ICWSM.V18I1.31360" title="" class="ltx_ref ltx_href">Stance detection
with collaborative role-infused llm-based agents</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Eighteenth International AAAI
Conference on Web and Social Media, ICWSM 2024, Buffalo, New York, USA,
June 3-6, 2024</em>, pages 891–903. AAAI Press.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v162/li22n.html" title="" class="ltx_ref ltx_href">BLIP:
bootstrapping language-image pre-training for unified vision-language
understanding and generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning, ICML 2022,
17-23 July 2022, Baltimore, Maryland, USA</em>, volume 162 of <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">Proceedings
of Machine Learning Research</em>, pages 12888–12900. PMLR.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2024)</span>
<span class="ltx_bibblock">
Bin Liang, Ang Li, Jingqian Zhao, Lin Gui, Min Yang, Yue Yu, Kam-Fai Wong, and
Ruifeng Xu. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2024.findings-acl.736" title="" class="ltx_ref ltx_href">Multi-modal stance detection: New datasets and model</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics
ACL 2024</em>, pages 12373–12387, Bangkok, Thailand and virtual meeting.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maynard and Bontcheva (2015)</span>
<span class="ltx_bibblock">
Diana Maynard and Kalina Bontcheva. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://iai-uiserv1.iai.fzk.de/ictensure/site?mod=litdb&amp;subject=art&amp;pid=X75B21A7D&amp;action=detail" title="" class="ltx_ref ltx_href">Understanding climate change tweets: an open source toolkit for social media
analysis</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">29th International Conference on Environmental Informatics,
EnviroInfo 2015 / 3rd International Conference on Information and
Communication Technology for Sustainability, ICT4S 2015, Copenhagen,
Denmark, September 7-9, 2015, Part 1: Conference Proceedings</em>, pages
242–250. Atlantis Press.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mazid and Zarnaz (2022)</span>
<span class="ltx_bibblock">
Md Abdullah Al Mazid and Zaima Zarnaz. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3542954.3542995" title="" class="ltx_ref ltx_href">Climate change myths
detection using dynamically weighted ensemble based stance classifier</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">ICCA 2022: 2nd International Conference on Computing
Advancements, Dhaka, Bangladesh, March 10 - 12, 2022</em>, pages 277–283. ACM.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meta AI (2024)</span>
<span class="ltx_bibblock">
Meta AI. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://ai.meta.com/blog/meta-llama-3/" title="" class="ltx_ref ltx_href">Introducing meta
llama 3: The most capable openly available llm to date</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Meta AI</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohammad et al. (2016)</span>
<span class="ltx_bibblock">
Saif Mohammad, Svetlana Kiritchenko, Parinaz Sobhani, Xiaodan Zhu, and Colin
Cherry. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/S16-1003" title="" class="ltx_ref ltx_href">SemEval-2016 task
6: Detecting stance in tweets</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 10th International Workshop on Semantic
Evaluation (SemEval-2016)</em>, pages 31–41, San Diego, California.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morgan and Michael (2024)</span>
<span class="ltx_bibblock">
Jeffrey Morgan and Michael. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://ollama.com" title="" class="ltx_ref ltx_href">Get up and running with large language
models.</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Ollama</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024)</span>
<span class="ltx_bibblock">
OpenAI. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openai.com/index/hello-gpt-4o/" title="" class="ltx_ref ltx_href">Hello gpt-4o</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">OpenAI Blog</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
Gretchen Krueger, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://proceedings.mlr.press/v139/radford21a.html" title="" class="ltx_ref ltx_href">Learning
transferable visual models from natural language supervision</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 38th International Conference on Machine
Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, volume 139 of
<em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 8748–8763. PMLR.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taulé et al. (2018)</span>
<span class="ltx_bibblock">
Mariona Taulé, Francisco M. Rangel Pardo, M. Antònia Martí,
and Paolo Rosso. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://ceur-ws.org/Vol-2150/overview-Multistance18.pdf" title="" class="ltx_ref ltx_href">Overview of the task on multimodal stance detection in tweets on catalan
#1oct referendum</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Third Workshop on Evaluation of Human
Language Technologies for Iberian Languages (IberEval 2018) co-located with
34th Conference of the Spanish Society for Natural Language Processing
(SEPLN 2018), Sevilla, Spain, September 18th, 2018</em>, volume 2150 of
<em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic">CEUR Workshop Proceedings</em>, pages 149–166. CEUR-WS.org.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tomar (2006)</span>
<span class="ltx_bibblock">
Suramya Tomar. 2006.

</span>
<span class="ltx_bibblock">Converting video formats with ffmpeg.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Linux Journal</em>, 2006(146):10.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Upadhyaya et al. (2023a)</span>
<span class="ltx_bibblock">
Apoorva Upadhyaya, Marco Fisichella, and Wolfgang Nejdl. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.24963/IJCAI.2023/693" title="" class="ltx_ref ltx_href">Intensity-valued
emotions help stance detection of climate change twitter data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Thirty-Second International Joint
Conference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023,
Macao, SAR, China</em>, pages 6246–6254. ijcai.org.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Upadhyaya et al. (2023b)</span>
<span class="ltx_bibblock">
Apoorva Upadhyaya, Marco Fisichella, and Wolfgang Nejdl. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1016/J.IPM.2023.103325" title="" class="ltx_ref ltx_href">Towards sentiment
and temporal aided stance detection of climate change tweets</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Inf. Process. Manag.</em>, 60(4):103325.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaid et al. (2022)</span>
<span class="ltx_bibblock">
Roopal Vaid, Kartikey Pant, and Manish Shrivastava. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-srw.35" title="" class="ltx_ref ltx_href">Towards
fine-grained classification of climate change related social media text</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics: Student Research Workshop</em>, pages 434–443,
Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Liu (2024)</span>
<span class="ltx_bibblock">
Jiayu Wang and Jinyan Liu. 2024.

</span>
<span class="ltx_bibblock">The visual stories in addressing climate change that a petroleum
business tells and lives by: A multimodal critical ecolinguistic perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Energy Research &amp; Social Science</em>, 111:103483.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weinzierl and
Harabagiu (2023a)</span>
<span class="ltx_bibblock">
Maxwell Weinzierl and Sanda Harabagiu. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.emnlp-main.776" title="" class="ltx_ref ltx_href">Identification of multimodal stance towards frames of communication</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in
Natural Language Processing</em>, pages 12597–12609, Singapore. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weinzierl and Harabagiu (2023b)</span>
<span class="ltx_bibblock">
Maxwell A. Weinzierl and Sanda M. Harabagiu. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/V1/2023.EMNLP-MAIN.776" title="" class="ltx_ref ltx_href">Identification of multimodal stance towards frames of communication</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>,
pages 12597–12609. Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Annotation Guideline</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Annotators are tasked with determining the stance on climate change within each frame-transcript pair provided. This involves assessing both the visual and textual elements of each pair and following the below instructions.</p>
</div>
<section id="A1.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Annotation Order.</h4>

<div id="A1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px1.p1.1" class="ltx_p">Annotators annotate at the frame-transcript level for each pair within a single video, deliberately avoiding sentence context.</p>
</div>
</section>
<section id="A1.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Text and Frame Consideration.</h4>

<div id="A1.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px2.p1.1" class="ltx_p">Annotators evaluate both the text and the accompanying image frame. In cases of conflict between text and image, prioritize the element that evokes stronger emotions related to the stance.</p>
</div>
</section>
<section id="A1.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data Storage.</h4>

<div id="A1.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px3.p1.1" class="ltx_p">The annotated dataset is provided in the following formats on the GitHub repository.<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://github.com/werywjw/MultiClimate/tree/main/dataset" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/werywjw/MultiClimate/tree/main/dataset</a></span></span></span>
As shown in <a href="#A1.F2" title="In Data Storage. ‣ Appendix A Annotation Guideline ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>, each sentence transcript is presented in a <span id="A1.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_typewriter">CSV</span> file with the column label ‘text’, and the corresponding frame is provided as a <span id="A1.SS0.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_typewriter">JPEG</span> file. The sentences and frames are in the same order.</p>
</div>
<figure id="A1.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="A1.F2.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:433.6pt;">
<img src="/html/2409.18346/assets/x1.png" id="A1.F2.1.g1" class="ltx_graphics ltx_img_square" width="438" height="417" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="A1.F2.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" style="width:433.6pt;">
<img src="/html/2409.18346/assets/x2.png" id="A1.F2.2.g1" class="ltx_graphics ltx_img_square" width="438" height="417" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Annotator materials in “ACCFP”: CSV file (left) and frames (right) provided.</figcaption>
</figure>
</section>
<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Stance Definitions and Examples</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">Each frame-transcript pair must be annotated with one of the following stance values regarding climate change: <span id="A1.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">Support</span>, <span id="A1.SS1.p1.1.2" class="ltx_text ltx_font_smallcaps">Neutral</span>, or <span id="A1.SS1.p1.1.3" class="ltx_text ltx_font_smallcaps">Oppose</span>.</p>
</div>
<section id="A1.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Support.</h4>

<div id="A1.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="A1.SS1.SSS0.Px1.p1.1" class="ltx_p">The frame-transcript pair accepts, agrees with, and/or promotes climate change-related topics or actions. Presented below are several sample categories that exemplify support arguments. Additional categories may also exist.</p>
</div>
<div id="A1.SS1.SSS0.Px1.p2" class="ltx_para">
<ul id="A1.I1" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.2" class="ltx_p"><span id="A1.I1.i1.p1.2.1" class="ltx_text ltx_font_italic">Action Promotion</span>: “Goal 13 of the sustainable development goals climate action.” <span id="A1.I1.i1.p1.2.2" class="ltx_text ltx_font_typewriter">WISE-002</span></p>
<img src="/html/2409.18346/assets/figs/WISE-002.jpg" id="A1.I1.i1.p1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="303" alt="[Uncaptioned image]">
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.2" class="ltx_p"><span id="A1.I1.i2.p1.2.1" class="ltx_text ltx_font_italic">Encouragement</span>: “It’s up to us to preserve these natural wonders and maintain the balance on earth.”
<span id="A1.I1.i2.p1.2.2" class="ltx_text ltx_font_typewriter">HCCAE-029</span></p>
<img src="/html/2409.18346/assets/figs/HCCAE-029.jpg" id="A1.I1.i2.p1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="303" alt="[Uncaptioned image]">
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.2" class="ltx_p"><span id="A1.I1.i3.p1.2.1" class="ltx_text ltx_font_italic">Achievements</span>: “It could prevent as much as 174 million tons of carbon from getting released.”
<span id="A1.I1.i3.p1.2.2" class="ltx_text ltx_font_typewriter">DACC-019</span></p>
<img src="/html/2409.18346/assets/figs/DACC-019.jpg" id="A1.I1.i3.p1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="303" alt="[Uncaptioned image]">
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p id="A1.I1.i4.p1.2" class="ltx_p"><span id="A1.I1.i4.p1.2.1" class="ltx_text ltx_font_italic">Solutions</span>: “By eating more vegetables and less meat you not only get to enjoy the wonderful variety of fresh produce, but you also help the planet.”
<span id="A1.I1.i4.p1.2.2" class="ltx_text ltx_font_typewriter">CCTA-010</span></p>
<img src="/html/2409.18346/assets/figs/CCTA-010.jpg" id="A1.I1.i4.p1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="303" alt="[Uncaptioned image]">
</div>
</li>
</ul>
</div>
</section>
<section id="A1.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Neutral.</h4>

<div id="A1.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="A1.SS1.SSS0.Px2.p1.1" class="ltx_p">The pair neither supports nor opposes climate change topics or related actions. Presented below are several categories that exemplify neutral arguments. Additional categories may also exist.</p>
<ul id="A1.I2" class="ltx_itemize">
<li id="A1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i1.p1" class="ltx_para">
<p id="A1.I2.i1.p1.2" class="ltx_p"><span id="A1.I2.i1.p1.2.1" class="ltx_text ltx_font_italic">Unrelated Context</span>: “People think that economists don’t agree about anything.”
<span id="A1.I2.i1.p1.2.2" class="ltx_text ltx_font_typewriter">RHTCC-025</span></p>
<img src="/html/2409.18346/assets/figs/RHTCC-025.jpg" id="A1.I2.i1.p1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="303" alt="[Uncaptioned image]">
</div>
</li>
<li id="A1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i2.p1" class="ltx_para">
<p id="A1.I2.i2.p1.2" class="ltx_p"><span id="A1.I2.i2.p1.2.1" class="ltx_text ltx_font_italic">General Information</span>: “An ice core is a continuous section of ice drilled into a glacier or an ice sheet.”
<span id="A1.I2.i2.p1.2.2" class="ltx_text ltx_font_typewriter">SDDA-007</span></p>
<img src="/html/2409.18346/assets/figs/SDDA-007.jpg" id="A1.I2.i2.p1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="303" alt="[Uncaptioned image]">
</div>
</li>
<li id="A1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i3.p1" class="ltx_para">
<p id="A1.I2.i3.p1.2" class="ltx_p"><span id="A1.I2.i3.p1.2.1" class="ltx_text ltx_font_italic">Interrogative</span>: “Why have so few thought leaders made it their signature issue?”
<span id="A1.I2.i3.p1.2.2" class="ltx_text ltx_font_typewriter">CCIS-008</span></p>
<img src="/html/2409.18346/assets/figs/CCIS-008.jpg" id="A1.I2.i3.p1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="303" alt="[Uncaptioned image]">
</div>
</li>
</ul>
</div>
</section>
<section id="A1.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Oppose.</h4>

<div id="A1.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="A1.SS1.SSS0.Px3.p1.1" class="ltx_p">The pair expresses negative sentiments or criticism towards climate change or its related aspects. Presented below are several sample categories that exemplify opposing arguments. Additional categories may also exist.</p>
<ul id="A1.I3" class="ltx_itemize">
<li id="A1.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i1.p1" class="ltx_para">
<p id="A1.I3.i1.p1.2" class="ltx_p"><span id="A1.I3.i1.p1.2.1" class="ltx_text ltx_font_italic">Negative Consequences</span>: “Either everyone is leaving, or everyone is killing each other it all comes down to whether there’s enough rain.”
<span id="A1.I3.i1.p1.2.2" class="ltx_text ltx_font_typewriter">TIOCC-005</span></p>
<img src="/html/2409.18346/assets/figs/TIOCC-005.jpg" id="A1.I3.i1.p1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="303" alt="[Uncaptioned image]">
</div>
</li>
<li id="A1.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i2.p1" class="ltx_para">
<p id="A1.I3.i2.p1.2" class="ltx_p"><span id="A1.I3.i2.p1.2.1" class="ltx_text ltx_font_italic">Adverse Effects</span>: “And this is the negative effects the diet is having on the ecosystem and the adverse effects on human health.”
<span id="A1.I3.i2.p1.2.2" class="ltx_text ltx_font_typewriter">DACC-047</span></p>
<img src="/html/2409.18346/assets/figs/DACC-047.jpg" id="A1.I3.i2.p1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="303" alt="[Uncaptioned image]">
</div>
</li>
</ul>
</div>
</section>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>IAA Details on 10 Test Videos</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">We also assessed the accuracy and weighted F1 score on the <math id="A2.p1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="A2.p1.1.m1.1a"><mn id="A2.p1.1.m1.1.1" xref="A2.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="A2.p1.1.m1.1b"><cn type="integer" id="A2.p1.1.m1.1.1.cmml" xref="A2.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.1.m1.1c">10</annotation></semantics></math> test video documents in <a href="#A2.T3" title="In Appendix B IAA Details on 10 Test Videos ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="A2.T3" class="ltx_table">
<div id="A2.T3.31" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:325.2pt;height:365.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(74.5pt,-83.6pt) scale(1.84477781692,1.84477781692) ;">
<table id="A2.T3.31.31" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T3.1.1.1" class="ltx_tr">
<th id="A2.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A2.T3.1.1.1.2.1" class="ltx_text ltx_font_smallcaps">Video</span></th>
<th id="A2.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="A2.T3.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">Cohen’s</span> <math id="A2.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\kappa" display="inline"><semantics id="A2.T3.1.1.1.1.m1.1a"><mi id="A2.T3.1.1.1.1.m1.1.1" xref="A2.T3.1.1.1.1.m1.1.1.cmml">κ</mi><annotation-xml encoding="MathML-Content" id="A2.T3.1.1.1.1.m1.1b"><ci id="A2.T3.1.1.1.1.m1.1.1.cmml" xref="A2.T3.1.1.1.1.m1.1.1">𝜅</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.1.1.1.1.m1.1c">\kappa</annotation></semantics></math>
</th>
<th id="A2.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A2.T3.1.1.1.3.1" class="ltx_text ltx_font_smallcaps">Acc</span></th>
<th id="A2.T3.1.1.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A2.T3.1.1.1.4.1" class="ltx_text ltx_font_smallcaps">F1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T3.4.4.4" class="ltx_tr">
<th id="A2.T3.4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">ACCFP</th>
<td id="A2.T3.2.2.2.1" class="ltx_td ltx_align_center ltx_border_t"><math id="A2.T3.2.2.2.1.m1.1" class="ltx_Math" alttext="0.698" display="inline"><semantics id="A2.T3.2.2.2.1.m1.1a"><mn id="A2.T3.2.2.2.1.m1.1.1" xref="A2.T3.2.2.2.1.m1.1.1.cmml">0.698</mn><annotation-xml encoding="MathML-Content" id="A2.T3.2.2.2.1.m1.1b"><cn type="float" id="A2.T3.2.2.2.1.m1.1.1.cmml" xref="A2.T3.2.2.2.1.m1.1.1">0.698</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.2.2.2.1.m1.1c">0.698</annotation></semantics></math></td>
<td id="A2.T3.3.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><math id="A2.T3.3.3.3.2.m1.1" class="ltx_Math" alttext="0.867" display="inline"><semantics id="A2.T3.3.3.3.2.m1.1a"><mn id="A2.T3.3.3.3.2.m1.1.1" xref="A2.T3.3.3.3.2.m1.1.1.cmml">0.867</mn><annotation-xml encoding="MathML-Content" id="A2.T3.3.3.3.2.m1.1b"><cn type="float" id="A2.T3.3.3.3.2.m1.1.1.cmml" xref="A2.T3.3.3.3.2.m1.1.1">0.867</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.3.3.3.2.m1.1c">0.867</annotation></semantics></math></td>
<td id="A2.T3.4.4.4.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><math id="A2.T3.4.4.4.3.m1.1" class="ltx_Math" alttext="0.873" display="inline"><semantics id="A2.T3.4.4.4.3.m1.1a"><mn id="A2.T3.4.4.4.3.m1.1.1" xref="A2.T3.4.4.4.3.m1.1.1.cmml">0.873</mn><annotation-xml encoding="MathML-Content" id="A2.T3.4.4.4.3.m1.1b"><cn type="float" id="A2.T3.4.4.4.3.m1.1.1.cmml" xref="A2.T3.4.4.4.3.m1.1.1">0.873</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.4.4.4.3.m1.1c">0.873</annotation></semantics></math></td>
</tr>
<tr id="A2.T3.7.7.7" class="ltx_tr">
<th id="A2.T3.7.7.7.4" class="ltx_td ltx_align_center ltx_th ltx_th_row">CCAH</th>
<td id="A2.T3.5.5.5.1" class="ltx_td ltx_align_center"><math id="A2.T3.5.5.5.1.m1.1" class="ltx_Math" alttext="0.778" display="inline"><semantics id="A2.T3.5.5.5.1.m1.1a"><mn id="A2.T3.5.5.5.1.m1.1.1" xref="A2.T3.5.5.5.1.m1.1.1.cmml">0.778</mn><annotation-xml encoding="MathML-Content" id="A2.T3.5.5.5.1.m1.1b"><cn type="float" id="A2.T3.5.5.5.1.m1.1.1.cmml" xref="A2.T3.5.5.5.1.m1.1.1">0.778</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.5.5.5.1.m1.1c">0.778</annotation></semantics></math></td>
<td id="A2.T3.6.6.6.2" class="ltx_td ltx_align_center"><math id="A2.T3.6.6.6.2.m1.1" class="ltx_Math" alttext="0.867" display="inline"><semantics id="A2.T3.6.6.6.2.m1.1a"><mn id="A2.T3.6.6.6.2.m1.1.1" xref="A2.T3.6.6.6.2.m1.1.1.cmml">0.867</mn><annotation-xml encoding="MathML-Content" id="A2.T3.6.6.6.2.m1.1b"><cn type="float" id="A2.T3.6.6.6.2.m1.1.1.cmml" xref="A2.T3.6.6.6.2.m1.1.1">0.867</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.6.6.6.2.m1.1c">0.867</annotation></semantics></math></td>
<td id="A2.T3.7.7.7.3" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A2.T3.7.7.7.3.m1.1" class="ltx_Math" alttext="0.851" display="inline"><semantics id="A2.T3.7.7.7.3.m1.1a"><mn id="A2.T3.7.7.7.3.m1.1.1" xref="A2.T3.7.7.7.3.m1.1.1.cmml">0.851</mn><annotation-xml encoding="MathML-Content" id="A2.T3.7.7.7.3.m1.1b"><cn type="float" id="A2.T3.7.7.7.3.m1.1.1.cmml" xref="A2.T3.7.7.7.3.m1.1.1">0.851</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.7.7.7.3.m1.1c">0.851</annotation></semantics></math></td>
</tr>
<tr id="A2.T3.10.10.10" class="ltx_tr">
<th id="A2.T3.10.10.10.4" class="ltx_td ltx_align_center ltx_th ltx_th_row">CCSAD</th>
<td id="A2.T3.8.8.8.1" class="ltx_td ltx_align_center"><math id="A2.T3.8.8.8.1.m1.1" class="ltx_Math" alttext="0.408" display="inline"><semantics id="A2.T3.8.8.8.1.m1.1a"><mn id="A2.T3.8.8.8.1.m1.1.1" xref="A2.T3.8.8.8.1.m1.1.1.cmml">0.408</mn><annotation-xml encoding="MathML-Content" id="A2.T3.8.8.8.1.m1.1b"><cn type="float" id="A2.T3.8.8.8.1.m1.1.1.cmml" xref="A2.T3.8.8.8.1.m1.1.1">0.408</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.8.8.8.1.m1.1c">0.408</annotation></semantics></math></td>
<td id="A2.T3.9.9.9.2" class="ltx_td ltx_align_center"><math id="A2.T3.9.9.9.2.m1.1" class="ltx_Math" alttext="0.644" display="inline"><semantics id="A2.T3.9.9.9.2.m1.1a"><mn id="A2.T3.9.9.9.2.m1.1.1" xref="A2.T3.9.9.9.2.m1.1.1.cmml">0.644</mn><annotation-xml encoding="MathML-Content" id="A2.T3.9.9.9.2.m1.1b"><cn type="float" id="A2.T3.9.9.9.2.m1.1.1.cmml" xref="A2.T3.9.9.9.2.m1.1.1">0.644</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.9.9.9.2.m1.1c">0.644</annotation></semantics></math></td>
<td id="A2.T3.10.10.10.3" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A2.T3.10.10.10.3.m1.1" class="ltx_Math" alttext="0.675" display="inline"><semantics id="A2.T3.10.10.10.3.m1.1a"><mn id="A2.T3.10.10.10.3.m1.1.1" xref="A2.T3.10.10.10.3.m1.1.1.cmml">0.675</mn><annotation-xml encoding="MathML-Content" id="A2.T3.10.10.10.3.m1.1b"><cn type="float" id="A2.T3.10.10.10.3.m1.1.1.cmml" xref="A2.T3.10.10.10.3.m1.1.1">0.675</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.10.10.10.3.m1.1c">0.675</annotation></semantics></math></td>
</tr>
<tr id="A2.T3.13.13.13" class="ltx_tr">
<th id="A2.T3.13.13.13.4" class="ltx_td ltx_align_center ltx_th ltx_th_row">CCUIM</th>
<td id="A2.T3.11.11.11.1" class="ltx_td ltx_align_center"><math id="A2.T3.11.11.11.1.m1.1" class="ltx_Math" alttext="0.633" display="inline"><semantics id="A2.T3.11.11.11.1.m1.1a"><mn id="A2.T3.11.11.11.1.m1.1.1" xref="A2.T3.11.11.11.1.m1.1.1.cmml">0.633</mn><annotation-xml encoding="MathML-Content" id="A2.T3.11.11.11.1.m1.1b"><cn type="float" id="A2.T3.11.11.11.1.m1.1.1.cmml" xref="A2.T3.11.11.11.1.m1.1.1">0.633</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.11.11.11.1.m1.1c">0.633</annotation></semantics></math></td>
<td id="A2.T3.12.12.12.2" class="ltx_td ltx_align_center"><math id="A2.T3.12.12.12.2.m1.1" class="ltx_Math" alttext="0.771" display="inline"><semantics id="A2.T3.12.12.12.2.m1.1a"><mn id="A2.T3.12.12.12.2.m1.1.1" xref="A2.T3.12.12.12.2.m1.1.1.cmml">0.771</mn><annotation-xml encoding="MathML-Content" id="A2.T3.12.12.12.2.m1.1b"><cn type="float" id="A2.T3.12.12.12.2.m1.1.1.cmml" xref="A2.T3.12.12.12.2.m1.1.1">0.771</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.12.12.12.2.m1.1c">0.771</annotation></semantics></math></td>
<td id="A2.T3.13.13.13.3" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A2.T3.13.13.13.3.m1.1" class="ltx_Math" alttext="0.773" display="inline"><semantics id="A2.T3.13.13.13.3.m1.1a"><mn id="A2.T3.13.13.13.3.m1.1.1" xref="A2.T3.13.13.13.3.m1.1.1.cmml">0.773</mn><annotation-xml encoding="MathML-Content" id="A2.T3.13.13.13.3.m1.1b"><cn type="float" id="A2.T3.13.13.13.3.m1.1.1.cmml" xref="A2.T3.13.13.13.3.m1.1.1">0.773</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.13.13.13.3.m1.1c">0.773</annotation></semantics></math></td>
</tr>
<tr id="A2.T3.16.16.16" class="ltx_tr">
<th id="A2.T3.16.16.16.4" class="ltx_td ltx_align_center ltx_th ltx_th_row">EIB</th>
<td id="A2.T3.14.14.14.1" class="ltx_td ltx_align_center"><math id="A2.T3.14.14.14.1.m1.1" class="ltx_Math" alttext="0.647" display="inline"><semantics id="A2.T3.14.14.14.1.m1.1a"><mn id="A2.T3.14.14.14.1.m1.1.1" xref="A2.T3.14.14.14.1.m1.1.1.cmml">0.647</mn><annotation-xml encoding="MathML-Content" id="A2.T3.14.14.14.1.m1.1b"><cn type="float" id="A2.T3.14.14.14.1.m1.1.1.cmml" xref="A2.T3.14.14.14.1.m1.1.1">0.647</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.14.14.14.1.m1.1c">0.647</annotation></semantics></math></td>
<td id="A2.T3.15.15.15.2" class="ltx_td ltx_align_center"><math id="A2.T3.15.15.15.2.m1.1" class="ltx_Math" alttext="0.822" display="inline"><semantics id="A2.T3.15.15.15.2.m1.1a"><mn id="A2.T3.15.15.15.2.m1.1.1" xref="A2.T3.15.15.15.2.m1.1.1.cmml">0.822</mn><annotation-xml encoding="MathML-Content" id="A2.T3.15.15.15.2.m1.1b"><cn type="float" id="A2.T3.15.15.15.2.m1.1.1.cmml" xref="A2.T3.15.15.15.2.m1.1.1">0.822</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.15.15.15.2.m1.1c">0.822</annotation></semantics></math></td>
<td id="A2.T3.16.16.16.3" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A2.T3.16.16.16.3.m1.1" class="ltx_Math" alttext="0.809" display="inline"><semantics id="A2.T3.16.16.16.3.m1.1a"><mn id="A2.T3.16.16.16.3.m1.1.1" xref="A2.T3.16.16.16.3.m1.1.1.cmml">0.809</mn><annotation-xml encoding="MathML-Content" id="A2.T3.16.16.16.3.m1.1b"><cn type="float" id="A2.T3.16.16.16.3.m1.1.1.cmml" xref="A2.T3.16.16.16.3.m1.1.1">0.809</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.16.16.16.3.m1.1c">0.809</annotation></semantics></math></td>
</tr>
<tr id="A2.T3.19.19.19" class="ltx_tr">
<th id="A2.T3.19.19.19.4" class="ltx_td ltx_align_center ltx_th ltx_th_row">EWCC</th>
<td id="A2.T3.17.17.17.1" class="ltx_td ltx_align_center"><math id="A2.T3.17.17.17.1.m1.1" class="ltx_Math" alttext="0.690" display="inline"><semantics id="A2.T3.17.17.17.1.m1.1a"><mn id="A2.T3.17.17.17.1.m1.1.1" xref="A2.T3.17.17.17.1.m1.1.1.cmml">0.690</mn><annotation-xml encoding="MathML-Content" id="A2.T3.17.17.17.1.m1.1b"><cn type="float" id="A2.T3.17.17.17.1.m1.1.1.cmml" xref="A2.T3.17.17.17.1.m1.1.1">0.690</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.17.17.17.1.m1.1c">0.690</annotation></semantics></math></td>
<td id="A2.T3.18.18.18.2" class="ltx_td ltx_align_center"><math id="A2.T3.18.18.18.2.m1.1" class="ltx_Math" alttext="0.816" display="inline"><semantics id="A2.T3.18.18.18.2.m1.1a"><mn id="A2.T3.18.18.18.2.m1.1.1" xref="A2.T3.18.18.18.2.m1.1.1.cmml">0.816</mn><annotation-xml encoding="MathML-Content" id="A2.T3.18.18.18.2.m1.1b"><cn type="float" id="A2.T3.18.18.18.2.m1.1.1.cmml" xref="A2.T3.18.18.18.2.m1.1.1">0.816</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.18.18.18.2.m1.1c">0.816</annotation></semantics></math></td>
<td id="A2.T3.19.19.19.3" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A2.T3.19.19.19.3.m1.1" class="ltx_Math" alttext="0.811" display="inline"><semantics id="A2.T3.19.19.19.3.m1.1a"><mn id="A2.T3.19.19.19.3.m1.1.1" xref="A2.T3.19.19.19.3.m1.1.1.cmml">0.811</mn><annotation-xml encoding="MathML-Content" id="A2.T3.19.19.19.3.m1.1b"><cn type="float" id="A2.T3.19.19.19.3.m1.1.1.cmml" xref="A2.T3.19.19.19.3.m1.1.1">0.811</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.19.19.19.3.m1.1c">0.811</annotation></semantics></math></td>
</tr>
<tr id="A2.T3.22.22.22" class="ltx_tr">
<th id="A2.T3.22.22.22.4" class="ltx_td ltx_align_center ltx_th ltx_th_row">GGCC</th>
<td id="A2.T3.20.20.20.1" class="ltx_td ltx_align_center"><math id="A2.T3.20.20.20.1.m1.1" class="ltx_Math" alttext="0.736" display="inline"><semantics id="A2.T3.20.20.20.1.m1.1a"><mn id="A2.T3.20.20.20.1.m1.1.1" xref="A2.T3.20.20.20.1.m1.1.1.cmml">0.736</mn><annotation-xml encoding="MathML-Content" id="A2.T3.20.20.20.1.m1.1b"><cn type="float" id="A2.T3.20.20.20.1.m1.1.1.cmml" xref="A2.T3.20.20.20.1.m1.1.1">0.736</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.20.20.20.1.m1.1c">0.736</annotation></semantics></math></td>
<td id="A2.T3.21.21.21.2" class="ltx_td ltx_align_center"><math id="A2.T3.21.21.21.2.m1.1" class="ltx_Math" alttext="0.827" display="inline"><semantics id="A2.T3.21.21.21.2.m1.1a"><mn id="A2.T3.21.21.21.2.m1.1.1" xref="A2.T3.21.21.21.2.m1.1.1.cmml">0.827</mn><annotation-xml encoding="MathML-Content" id="A2.T3.21.21.21.2.m1.1b"><cn type="float" id="A2.T3.21.21.21.2.m1.1.1.cmml" xref="A2.T3.21.21.21.2.m1.1.1">0.827</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.21.21.21.2.m1.1c">0.827</annotation></semantics></math></td>
<td id="A2.T3.22.22.22.3" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A2.T3.22.22.22.3.m1.1" class="ltx_Math" alttext="0.814" display="inline"><semantics id="A2.T3.22.22.22.3.m1.1a"><mn id="A2.T3.22.22.22.3.m1.1.1" xref="A2.T3.22.22.22.3.m1.1.1.cmml">0.814</mn><annotation-xml encoding="MathML-Content" id="A2.T3.22.22.22.3.m1.1b"><cn type="float" id="A2.T3.22.22.22.3.m1.1.1.cmml" xref="A2.T3.22.22.22.3.m1.1.1">0.814</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.22.22.22.3.m1.1c">0.814</annotation></semantics></math></td>
</tr>
<tr id="A2.T3.25.25.25" class="ltx_tr">
<th id="A2.T3.25.25.25.4" class="ltx_td ltx_align_center ltx_th ltx_th_row">SCCC</th>
<td id="A2.T3.23.23.23.1" class="ltx_td ltx_align_center"><math id="A2.T3.23.23.23.1.m1.1" class="ltx_Math" alttext="0.724" display="inline"><semantics id="A2.T3.23.23.23.1.m1.1a"><mn id="A2.T3.23.23.23.1.m1.1.1" xref="A2.T3.23.23.23.1.m1.1.1.cmml">0.724</mn><annotation-xml encoding="MathML-Content" id="A2.T3.23.23.23.1.m1.1b"><cn type="float" id="A2.T3.23.23.23.1.m1.1.1.cmml" xref="A2.T3.23.23.23.1.m1.1.1">0.724</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.23.23.23.1.m1.1c">0.724</annotation></semantics></math></td>
<td id="A2.T3.24.24.24.2" class="ltx_td ltx_align_center"><math id="A2.T3.24.24.24.2.m1.1" class="ltx_Math" alttext="0.824" display="inline"><semantics id="A2.T3.24.24.24.2.m1.1a"><mn id="A2.T3.24.24.24.2.m1.1.1" xref="A2.T3.24.24.24.2.m1.1.1.cmml">0.824</mn><annotation-xml encoding="MathML-Content" id="A2.T3.24.24.24.2.m1.1b"><cn type="float" id="A2.T3.24.24.24.2.m1.1.1.cmml" xref="A2.T3.24.24.24.2.m1.1.1">0.824</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.24.24.24.2.m1.1c">0.824</annotation></semantics></math></td>
<td id="A2.T3.25.25.25.3" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A2.T3.25.25.25.3.m1.1" class="ltx_Math" alttext="0.806" display="inline"><semantics id="A2.T3.25.25.25.3.m1.1a"><mn id="A2.T3.25.25.25.3.m1.1.1" xref="A2.T3.25.25.25.3.m1.1.1.cmml">0.806</mn><annotation-xml encoding="MathML-Content" id="A2.T3.25.25.25.3.m1.1b"><cn type="float" id="A2.T3.25.25.25.3.m1.1.1.cmml" xref="A2.T3.25.25.25.3.m1.1.1">0.806</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.25.25.25.3.m1.1c">0.806</annotation></semantics></math></td>
</tr>
<tr id="A2.T3.28.28.28" class="ltx_tr">
<th id="A2.T3.28.28.28.4" class="ltx_td ltx_align_center ltx_th ltx_th_row">TICC</th>
<td id="A2.T3.26.26.26.1" class="ltx_td ltx_align_center"><math id="A2.T3.26.26.26.1.m1.1" class="ltx_Math" alttext="0.872" display="inline"><semantics id="A2.T3.26.26.26.1.m1.1a"><mn id="A2.T3.26.26.26.1.m1.1.1" xref="A2.T3.26.26.26.1.m1.1.1.cmml">0.872</mn><annotation-xml encoding="MathML-Content" id="A2.T3.26.26.26.1.m1.1b"><cn type="float" id="A2.T3.26.26.26.1.m1.1.1.cmml" xref="A2.T3.26.26.26.1.m1.1.1">0.872</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.26.26.26.1.m1.1c">0.872</annotation></semantics></math></td>
<td id="A2.T3.27.27.27.2" class="ltx_td ltx_align_center"><math id="A2.T3.27.27.27.2.m1.1" class="ltx_Math" alttext="0.936" display="inline"><semantics id="A2.T3.27.27.27.2.m1.1a"><mn id="A2.T3.27.27.27.2.m1.1.1" xref="A2.T3.27.27.27.2.m1.1.1.cmml">0.936</mn><annotation-xml encoding="MathML-Content" id="A2.T3.27.27.27.2.m1.1b"><cn type="float" id="A2.T3.27.27.27.2.m1.1.1.cmml" xref="A2.T3.27.27.27.2.m1.1.1">0.936</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.27.27.27.2.m1.1c">0.936</annotation></semantics></math></td>
<td id="A2.T3.28.28.28.3" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A2.T3.28.28.28.3.m1.1" class="ltx_Math" alttext="0.929" display="inline"><semantics id="A2.T3.28.28.28.3.m1.1a"><mn id="A2.T3.28.28.28.3.m1.1.1" xref="A2.T3.28.28.28.3.m1.1.1.cmml">0.929</mn><annotation-xml encoding="MathML-Content" id="A2.T3.28.28.28.3.m1.1b"><cn type="float" id="A2.T3.28.28.28.3.m1.1.1.cmml" xref="A2.T3.28.28.28.3.m1.1.1">0.929</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.28.28.28.3.m1.1c">0.929</annotation></semantics></math></td>
</tr>
<tr id="A2.T3.31.31.31" class="ltx_tr">
<th id="A2.T3.31.31.31.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">WICC</th>
<td id="A2.T3.29.29.29.1" class="ltx_td ltx_align_center ltx_border_bb"><math id="A2.T3.29.29.29.1.m1.1" class="ltx_Math" alttext="0.838" display="inline"><semantics id="A2.T3.29.29.29.1.m1.1a"><mn id="A2.T3.29.29.29.1.m1.1.1" xref="A2.T3.29.29.29.1.m1.1.1.cmml">0.838</mn><annotation-xml encoding="MathML-Content" id="A2.T3.29.29.29.1.m1.1b"><cn type="float" id="A2.T3.29.29.29.1.m1.1.1.cmml" xref="A2.T3.29.29.29.1.m1.1.1">0.838</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.29.29.29.1.m1.1c">0.838</annotation></semantics></math></td>
<td id="A2.T3.30.30.30.2" class="ltx_td ltx_align_center ltx_border_bb"><math id="A2.T3.30.30.30.2.m1.1" class="ltx_Math" alttext="0.900" display="inline"><semantics id="A2.T3.30.30.30.2.m1.1a"><mn id="A2.T3.30.30.30.2.m1.1.1" xref="A2.T3.30.30.30.2.m1.1.1.cmml">0.900</mn><annotation-xml encoding="MathML-Content" id="A2.T3.30.30.30.2.m1.1b"><cn type="float" id="A2.T3.30.30.30.2.m1.1.1.cmml" xref="A2.T3.30.30.30.2.m1.1.1">0.900</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.30.30.30.2.m1.1c">0.900</annotation></semantics></math></td>
<td id="A2.T3.31.31.31.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><math id="A2.T3.31.31.31.3.m1.1" class="ltx_Math" alttext="0.890" display="inline"><semantics id="A2.T3.31.31.31.3.m1.1a"><mn id="A2.T3.31.31.31.3.m1.1.1" xref="A2.T3.31.31.31.3.m1.1.1.cmml">0.890</mn><annotation-xml encoding="MathML-Content" id="A2.T3.31.31.31.3.m1.1b"><cn type="float" id="A2.T3.31.31.31.3.m1.1.1.cmml" xref="A2.T3.31.31.31.3.m1.1.1">0.890</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.31.31.31.3.m1.1c">0.890</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Overview of IAA (Cohen’s kappa), accuracy, and weighted F1 score on 10 MultiClimate test videos.</figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>IDEFICS Confusion Matrix of Predictions Before and After Fine-tuning</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">The confusion matrices (predictions in %) before and after fine-tuning IDEFICS are shown in <a href="#A3.F3" title="In Appendix C IDEFICS Confusion Matrix of Predictions Before and After Fine-tuning ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#A3.F4" title="In Appendix C IDEFICS Confusion Matrix of Predictions Before and After Fine-tuning ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a> respectively.</p>
</div>
<figure id="A3.F3" class="ltx_figure">
<div id="A3.F3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:328.3pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-11.8pt,9.4pt) scale(0.945878674410633,0.945878674410633) ;"><img src="/html/2409.18346/assets/x3.png" id="A3.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="368" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Confusion matrix of predictions before fine-tuning IDEFICS.</figcaption>
</figure>
<figure id="A3.F4" class="ltx_figure">
<div id="A3.F4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:328.3pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-11.8pt,9.4pt) scale(0.945878674410633,0.945878674410633) ;"><img src="/html/2409.18346/assets/x4.png" id="A3.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="368" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Confusion matrix of predictions after fine-tuning IDEFICS.</figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Hyperparameters for Fine-tuning</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p"><a href="#A4.T4" title="In Appendix D Hyperparameters for Fine-tuning ‣ MultiClimate: Multimodal Stance Detection on Climate Change Videos" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a> provides our explored hyperparameter space for all models in fine-tuning, helping in the reproducibility of our experimental results.</p>
</div>
<figure id="A4.T4" class="ltx_table">
<div id="A4.T4.27" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:420.6pt;height:969pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(58.0pt,-133.5pt) scale(1.38037339737266,1.38037339737266) ;">
<table id="A4.T4.27.27" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.T4.27.27.28.1" class="ltx_tr">
<th id="A4.T4.27.27.28.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="A4.T4.27.27.28.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="A4.T4.27.27.28.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="A4.T4.27.27.28.1.2.1" class="ltx_text ltx_font_smallcaps">Hyperparameter</span></th>
<th id="A4.T4.27.27.28.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A4.T4.27.27.28.1.3.1" class="ltx_text ltx_font_smallcaps">Value</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.T4.1.1.1" class="ltx_tr">
<td id="A4.T4.1.1.1.2" class="ltx_td ltx_align_left ltx_border_t" rowspan="5"><span id="A4.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">BERT</span></td>
<td id="A4.T4.1.1.1.3" class="ltx_td ltx_align_right ltx_border_t">Epochs</td>
<td id="A4.T4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><math id="A4.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="A4.T4.1.1.1.1.m1.1a"><mn id="A4.T4.1.1.1.1.m1.1.1" xref="A4.T4.1.1.1.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="A4.T4.1.1.1.1.m1.1b"><cn type="integer" id="A4.T4.1.1.1.1.m1.1.1.cmml" xref="A4.T4.1.1.1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.1.1.1.1.m1.1c">3</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.27.27.29.1" class="ltx_tr">
<td id="A4.T4.27.27.29.1.1" class="ltx_td ltx_align_right">Learning rate</td>
<td id="A4.T4.27.27.29.1.2" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A4.T4.27.27.29.1.2.1" class="ltx_text ltx_font_typewriter">2e-4</span></td>
</tr>
<tr id="A4.T4.27.27.30.2" class="ltx_tr">
<td id="A4.T4.27.27.30.2.1" class="ltx_td ltx_align_right">Weight decay</td>
<td id="A4.T4.27.27.30.2.2" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A4.T4.27.27.30.2.2.1" class="ltx_text ltx_font_typewriter">1e-2</span></td>
</tr>
<tr id="A4.T4.2.2.2" class="ltx_tr">
<td id="A4.T4.2.2.2.2" class="ltx_td ltx_align_right">Per device train batch size</td>
<td id="A4.T4.2.2.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A4.T4.2.2.2.1.m1.1" class="ltx_Math" alttext="16" display="inline"><semantics id="A4.T4.2.2.2.1.m1.1a"><mn id="A4.T4.2.2.2.1.m1.1.1" xref="A4.T4.2.2.2.1.m1.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="A4.T4.2.2.2.1.m1.1b"><cn type="integer" id="A4.T4.2.2.2.1.m1.1.1.cmml" xref="A4.T4.2.2.2.1.m1.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.2.2.2.1.m1.1c">16</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.3.3.3" class="ltx_tr">
<td id="A4.T4.3.3.3.2" class="ltx_td ltx_align_right">Per device eval batch size</td>
<td id="A4.T4.3.3.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A4.T4.3.3.3.1.m1.1" class="ltx_Math" alttext="16" display="inline"><semantics id="A4.T4.3.3.3.1.m1.1a"><mn id="A4.T4.3.3.3.1.m1.1.1" xref="A4.T4.3.3.3.1.m1.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="A4.T4.3.3.3.1.m1.1b"><cn type="integer" id="A4.T4.3.3.3.1.m1.1.1.cmml" xref="A4.T4.3.3.3.1.m1.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.3.3.3.1.m1.1c">16</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.4.4.4" class="ltx_tr">
<td id="A4.T4.4.4.4.2" class="ltx_td ltx_align_left ltx_border_t" rowspan="4"><span id="A4.T4.4.4.4.2.1" class="ltx_text ltx_font_bold">ResNet50</span></td>
<td id="A4.T4.4.4.4.3" class="ltx_td ltx_align_right ltx_border_t">Epochs</td>
<td id="A4.T4.4.4.4.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><math id="A4.T4.4.4.4.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="A4.T4.4.4.4.1.m1.1a"><mn id="A4.T4.4.4.4.1.m1.1.1" xref="A4.T4.4.4.4.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="A4.T4.4.4.4.1.m1.1b"><cn type="integer" id="A4.T4.4.4.4.1.m1.1.1.cmml" xref="A4.T4.4.4.4.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.4.4.4.1.m1.1c">3</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.27.27.31.3" class="ltx_tr">
<td id="A4.T4.27.27.31.3.1" class="ltx_td ltx_align_right">Learning rate</td>
<td id="A4.T4.27.27.31.3.2" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A4.T4.27.27.31.3.2.1" class="ltx_text ltx_font_typewriter">3e-4</span></td>
</tr>
<tr id="A4.T4.5.5.5" class="ltx_tr">
<td id="A4.T4.5.5.5.2" class="ltx_td ltx_align_right">Train batch size</td>
<td id="A4.T4.5.5.5.1" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A4.T4.5.5.5.1.m1.1" class="ltx_Math" alttext="32" display="inline"><semantics id="A4.T4.5.5.5.1.m1.1a"><mn id="A4.T4.5.5.5.1.m1.1.1" xref="A4.T4.5.5.5.1.m1.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="A4.T4.5.5.5.1.m1.1b"><cn type="integer" id="A4.T4.5.5.5.1.m1.1.1.cmml" xref="A4.T4.5.5.5.1.m1.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.5.5.5.1.m1.1c">32</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.6.6.6" class="ltx_tr">
<td id="A4.T4.6.6.6.2" class="ltx_td ltx_align_right">Eval batch size</td>
<td id="A4.T4.6.6.6.1" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A4.T4.6.6.6.1.m1.1" class="ltx_Math" alttext="32" display="inline"><semantics id="A4.T4.6.6.6.1.m1.1a"><mn id="A4.T4.6.6.6.1.m1.1.1" xref="A4.T4.6.6.6.1.m1.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="A4.T4.6.6.6.1.m1.1b"><cn type="integer" id="A4.T4.6.6.6.1.m1.1.1.cmml" xref="A4.T4.6.6.6.1.m1.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.6.6.6.1.m1.1c">32</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.7.7.7" class="ltx_tr">
<td id="A4.T4.7.7.7.2" class="ltx_td ltx_align_left ltx_border_t" rowspan="5"><span id="A4.T4.7.7.7.2.1" class="ltx_text ltx_font_bold">ViT</span></td>
<td id="A4.T4.7.7.7.3" class="ltx_td ltx_align_right ltx_border_t">Epochs</td>
<td id="A4.T4.7.7.7.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><math id="A4.T4.7.7.7.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="A4.T4.7.7.7.1.m1.1a"><mn id="A4.T4.7.7.7.1.m1.1.1" xref="A4.T4.7.7.7.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="A4.T4.7.7.7.1.m1.1b"><cn type="integer" id="A4.T4.7.7.7.1.m1.1.1.cmml" xref="A4.T4.7.7.7.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.7.7.7.1.m1.1c">3</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.27.27.32.4" class="ltx_tr">
<td id="A4.T4.27.27.32.4.1" class="ltx_td ltx_align_right">Learning rate</td>
<td id="A4.T4.27.27.32.4.2" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A4.T4.27.27.32.4.2.1" class="ltx_text ltx_font_typewriter">3e-5</span></td>
</tr>
<tr id="A4.T4.27.27.33.5" class="ltx_tr">
<td id="A4.T4.27.27.33.5.1" class="ltx_td ltx_align_right">Optimizer</td>
<td id="A4.T4.27.27.33.5.2" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A4.T4.27.27.33.5.2.1" class="ltx_text ltx_font_typewriter">AdamW</span></td>
</tr>
<tr id="A4.T4.8.8.8" class="ltx_tr">
<td id="A4.T4.8.8.8.2" class="ltx_td ltx_align_right">Train batch size</td>
<td id="A4.T4.8.8.8.1" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A4.T4.8.8.8.1.m1.1" class="ltx_Math" alttext="32" display="inline"><semantics id="A4.T4.8.8.8.1.m1.1a"><mn id="A4.T4.8.8.8.1.m1.1.1" xref="A4.T4.8.8.8.1.m1.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="A4.T4.8.8.8.1.m1.1b"><cn type="integer" id="A4.T4.8.8.8.1.m1.1.1.cmml" xref="A4.T4.8.8.8.1.m1.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.8.8.8.1.m1.1c">32</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.9.9.9" class="ltx_tr">
<td id="A4.T4.9.9.9.2" class="ltx_td ltx_align_right">Eval batch size</td>
<td id="A4.T4.9.9.9.1" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A4.T4.9.9.9.1.m1.1" class="ltx_Math" alttext="32" display="inline"><semantics id="A4.T4.9.9.9.1.m1.1a"><mn id="A4.T4.9.9.9.1.m1.1.1" xref="A4.T4.9.9.9.1.m1.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="A4.T4.9.9.9.1.m1.1b"><cn type="integer" id="A4.T4.9.9.9.1.m1.1.1.cmml" xref="A4.T4.9.9.9.1.m1.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.9.9.9.1.m1.1c">32</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.10.10.10" class="ltx_tr">
<td id="A4.T4.10.10.10.2" class="ltx_td ltx_align_left ltx_border_t" rowspan="4"><span id="A4.T4.10.10.10.2.1" class="ltx_text ltx_font_bold">BERT + ResNet50/ViT</span></td>
<td id="A4.T4.10.10.10.3" class="ltx_td ltx_align_right ltx_border_t">Epochs</td>
<td id="A4.T4.10.10.10.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><math id="A4.T4.10.10.10.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="A4.T4.10.10.10.1.m1.1a"><mn id="A4.T4.10.10.10.1.m1.1.1" xref="A4.T4.10.10.10.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="A4.T4.10.10.10.1.m1.1b"><cn type="integer" id="A4.T4.10.10.10.1.m1.1.1.cmml" xref="A4.T4.10.10.10.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.10.10.10.1.m1.1c">3</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.27.27.34.6" class="ltx_tr">
<td id="A4.T4.27.27.34.6.1" class="ltx_td ltx_align_right">Learning rate</td>
<td id="A4.T4.27.27.34.6.2" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A4.T4.27.27.34.6.2.1" class="ltx_text ltx_font_typewriter">2e-5</span></td>
</tr>
<tr id="A4.T4.11.11.11" class="ltx_tr">
<td id="A4.T4.11.11.11.2" class="ltx_td ltx_align_right">Train batch size</td>
<td id="A4.T4.11.11.11.1" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A4.T4.11.11.11.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="A4.T4.11.11.11.1.m1.1a"><mn id="A4.T4.11.11.11.1.m1.1.1" xref="A4.T4.11.11.11.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A4.T4.11.11.11.1.m1.1b"><cn type="integer" id="A4.T4.11.11.11.1.m1.1.1.cmml" xref="A4.T4.11.11.11.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.11.11.11.1.m1.1c">4</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.12.12.12" class="ltx_tr">
<td id="A4.T4.12.12.12.2" class="ltx_td ltx_align_right">Eval batch size</td>
<td id="A4.T4.12.12.12.1" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A4.T4.12.12.12.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="A4.T4.12.12.12.1.m1.1a"><mn id="A4.T4.12.12.12.1.m1.1.1" xref="A4.T4.12.12.12.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A4.T4.12.12.12.1.m1.1b"><cn type="integer" id="A4.T4.12.12.12.1.m1.1.1.cmml" xref="A4.T4.12.12.12.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.12.12.12.1.m1.1c">4</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.13.13.13" class="ltx_tr">
<td id="A4.T4.13.13.13.2" class="ltx_td ltx_align_left ltx_border_t" rowspan="4"><span id="A4.T4.13.13.13.2.1" class="ltx_text ltx_font_bold">CLIP</span></td>
<td id="A4.T4.13.13.13.3" class="ltx_td ltx_align_right ltx_border_t">Epochs</td>
<td id="A4.T4.13.13.13.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><math id="A4.T4.13.13.13.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="A4.T4.13.13.13.1.m1.1a"><mn id="A4.T4.13.13.13.1.m1.1.1" xref="A4.T4.13.13.13.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="A4.T4.13.13.13.1.m1.1b"><cn type="integer" id="A4.T4.13.13.13.1.m1.1.1.cmml" xref="A4.T4.13.13.13.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.13.13.13.1.m1.1c">3</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.27.27.35.7" class="ltx_tr">
<td id="A4.T4.27.27.35.7.1" class="ltx_td ltx_align_right">Learning rate</td>
<td id="A4.T4.27.27.35.7.2" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A4.T4.27.27.35.7.2.1" class="ltx_text ltx_font_typewriter">2e-5</span></td>
</tr>
<tr id="A4.T4.14.14.14" class="ltx_tr">
<td id="A4.T4.14.14.14.2" class="ltx_td ltx_align_right">Train batch size</td>
<td id="A4.T4.14.14.14.1" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A4.T4.14.14.14.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="A4.T4.14.14.14.1.m1.1a"><mn id="A4.T4.14.14.14.1.m1.1.1" xref="A4.T4.14.14.14.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A4.T4.14.14.14.1.m1.1b"><cn type="integer" id="A4.T4.14.14.14.1.m1.1.1.cmml" xref="A4.T4.14.14.14.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.14.14.14.1.m1.1c">4</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.15.15.15" class="ltx_tr">
<td id="A4.T4.15.15.15.2" class="ltx_td ltx_align_right">Eval batch size</td>
<td id="A4.T4.15.15.15.1" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A4.T4.15.15.15.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="A4.T4.15.15.15.1.m1.1a"><mn id="A4.T4.15.15.15.1.m1.1.1" xref="A4.T4.15.15.15.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A4.T4.15.15.15.1.m1.1b"><cn type="integer" id="A4.T4.15.15.15.1.m1.1.1.cmml" xref="A4.T4.15.15.15.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.15.15.15.1.m1.1c">4</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.16.16.16" class="ltx_tr">
<td id="A4.T4.16.16.16.2" class="ltx_td ltx_align_left ltx_border_t" rowspan="5"><span id="A4.T4.16.16.16.2.1" class="ltx_text ltx_font_bold">BLIP</span></td>
<td id="A4.T4.16.16.16.3" class="ltx_td ltx_align_right ltx_border_t">Epochs</td>
<td id="A4.T4.16.16.16.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><math id="A4.T4.16.16.16.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="A4.T4.16.16.16.1.m1.1a"><mn id="A4.T4.16.16.16.1.m1.1.1" xref="A4.T4.16.16.16.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="A4.T4.16.16.16.1.m1.1b"><cn type="integer" id="A4.T4.16.16.16.1.m1.1.1.cmml" xref="A4.T4.16.16.16.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.16.16.16.1.m1.1c">3</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.27.27.36.8" class="ltx_tr">
<td id="A4.T4.27.27.36.8.1" class="ltx_td ltx_align_right">Learning rate</td>
<td id="A4.T4.27.27.36.8.2" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A4.T4.27.27.36.8.2.1" class="ltx_text ltx_font_typewriter">1e-5</span></td>
</tr>
<tr id="A4.T4.27.27.37.9" class="ltx_tr">
<td id="A4.T4.27.27.37.9.1" class="ltx_td ltx_align_right">Weight decay</td>
<td id="A4.T4.27.27.37.9.2" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A4.T4.27.27.37.9.2.1" class="ltx_text ltx_font_typewriter">1e-4</span></td>
</tr>
<tr id="A4.T4.17.17.17" class="ltx_tr">
<td id="A4.T4.17.17.17.2" class="ltx_td ltx_align_right">Train batch size</td>
<td id="A4.T4.17.17.17.1" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A4.T4.17.17.17.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="A4.T4.17.17.17.1.m1.1a"><mn id="A4.T4.17.17.17.1.m1.1.1" xref="A4.T4.17.17.17.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A4.T4.17.17.17.1.m1.1b"><cn type="integer" id="A4.T4.17.17.17.1.m1.1.1.cmml" xref="A4.T4.17.17.17.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.17.17.17.1.m1.1c">4</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.18.18.18" class="ltx_tr">
<td id="A4.T4.18.18.18.2" class="ltx_td ltx_align_right">Eval batch size</td>
<td id="A4.T4.18.18.18.1" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A4.T4.18.18.18.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="A4.T4.18.18.18.1.m1.1a"><mn id="A4.T4.18.18.18.1.m1.1.1" xref="A4.T4.18.18.18.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A4.T4.18.18.18.1.m1.1b"><cn type="integer" id="A4.T4.18.18.18.1.m1.1.1.cmml" xref="A4.T4.18.18.18.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.18.18.18.1.m1.1c">4</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.19.19.19" class="ltx_tr">
<td id="A4.T4.19.19.19.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="11"><span id="A4.T4.19.19.19.2.1" class="ltx_text ltx_font_bold">IDEFICS</span></td>
<td id="A4.T4.19.19.19.3" class="ltx_td ltx_align_right ltx_border_t">Epochs</td>
<td id="A4.T4.19.19.19.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><math id="A4.T4.19.19.19.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="A4.T4.19.19.19.1.m1.1a"><mn id="A4.T4.19.19.19.1.m1.1.1" xref="A4.T4.19.19.19.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A4.T4.19.19.19.1.m1.1b"><cn type="integer" id="A4.T4.19.19.19.1.m1.1.1.cmml" xref="A4.T4.19.19.19.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.19.19.19.1.m1.1c">4</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.27.27.38.10" class="ltx_tr">
<td id="A4.T4.27.27.38.10.1" class="ltx_td ltx_align_right">Learning rate</td>
<td id="A4.T4.27.27.38.10.2" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A4.T4.27.27.38.10.2.1" class="ltx_text ltx_font_typewriter">2e-4</span></td>
</tr>
<tr id="A4.T4.20.20.20" class="ltx_tr">
<td id="A4.T4.20.20.20.2" class="ltx_td ltx_align_right">Per device train batch size</td>
<td id="A4.T4.20.20.20.1" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A4.T4.20.20.20.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="A4.T4.20.20.20.1.m1.1a"><mn id="A4.T4.20.20.20.1.m1.1.1" xref="A4.T4.20.20.20.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A4.T4.20.20.20.1.m1.1b"><cn type="integer" id="A4.T4.20.20.20.1.m1.1.1.cmml" xref="A4.T4.20.20.20.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.20.20.20.1.m1.1c">4</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.21.21.21" class="ltx_tr">
<td id="A4.T4.21.21.21.2" class="ltx_td ltx_align_right">Per device eval batch size</td>
<td id="A4.T4.21.21.21.1" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A4.T4.21.21.21.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="A4.T4.21.21.21.1.m1.1a"><mn id="A4.T4.21.21.21.1.m1.1.1" xref="A4.T4.21.21.21.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A4.T4.21.21.21.1.m1.1b"><cn type="integer" id="A4.T4.21.21.21.1.m1.1.1.cmml" xref="A4.T4.21.21.21.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.21.21.21.1.m1.1c">4</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.22.22.22" class="ltx_tr">
<td id="A4.T4.22.22.22.2" class="ltx_td ltx_align_right">Gradient accumulation steps</td>
<td id="A4.T4.22.22.22.1" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A4.T4.22.22.22.1.m1.1" class="ltx_Math" alttext="8" display="inline"><semantics id="A4.T4.22.22.22.1.m1.1a"><mn id="A4.T4.22.22.22.1.m1.1.1" xref="A4.T4.22.22.22.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="A4.T4.22.22.22.1.m1.1b"><cn type="integer" id="A4.T4.22.22.22.1.m1.1.1.cmml" xref="A4.T4.22.22.22.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.22.22.22.1.m1.1c">8</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.23.23.23" class="ltx_tr">
<td id="A4.T4.23.23.23.2" class="ltx_td ltx_align_right">Lora alpha</td>
<td id="A4.T4.23.23.23.1" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A4.T4.23.23.23.1.m1.1" class="ltx_Math" alttext="32" display="inline"><semantics id="A4.T4.23.23.23.1.m1.1a"><mn id="A4.T4.23.23.23.1.m1.1.1" xref="A4.T4.23.23.23.1.m1.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="A4.T4.23.23.23.1.m1.1b"><cn type="integer" id="A4.T4.23.23.23.1.m1.1.1.cmml" xref="A4.T4.23.23.23.1.m1.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.23.23.23.1.m1.1c">32</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.27.27.39.11" class="ltx_tr">
<td id="A4.T4.27.27.39.11.1" class="ltx_td ltx_align_right">Lora dropout</td>
<td id="A4.T4.27.27.39.11.2" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A4.T4.27.27.39.11.2.1" class="ltx_text ltx_font_typewriter">5e-2</span></td>
</tr>
<tr id="A4.T4.24.24.24" class="ltx_tr">
<td id="A4.T4.24.24.24.2" class="ltx_td ltx_align_right">Eval steps</td>
<td id="A4.T4.24.24.24.1" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A4.T4.24.24.24.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="A4.T4.24.24.24.1.m1.1a"><mn id="A4.T4.24.24.24.1.m1.1.1" xref="A4.T4.24.24.24.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="A4.T4.24.24.24.1.m1.1b"><cn type="integer" id="A4.T4.24.24.24.1.m1.1.1.cmml" xref="A4.T4.24.24.24.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.24.24.24.1.m1.1c">100</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.25.25.25" class="ltx_tr">
<td id="A4.T4.25.25.25.2" class="ltx_td ltx_align_right">Save step</td>
<td id="A4.T4.25.25.25.1" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A4.T4.25.25.25.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="A4.T4.25.25.25.1.m1.1a"><mn id="A4.T4.25.25.25.1.m1.1.1" xref="A4.T4.25.25.25.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="A4.T4.25.25.25.1.m1.1b"><cn type="integer" id="A4.T4.25.25.25.1.m1.1.1.cmml" xref="A4.T4.25.25.25.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.25.25.25.1.m1.1c">100</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.26.26.26" class="ltx_tr">
<td id="A4.T4.26.26.26.2" class="ltx_td ltx_align_right">Logging steps</td>
<td id="A4.T4.26.26.26.1" class="ltx_td ltx_nopad_r ltx_align_center"><math id="A4.T4.26.26.26.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="A4.T4.26.26.26.1.m1.1a"><mn id="A4.T4.26.26.26.1.m1.1.1" xref="A4.T4.26.26.26.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="A4.T4.26.26.26.1.m1.1b"><cn type="integer" id="A4.T4.26.26.26.1.m1.1.1.cmml" xref="A4.T4.26.26.26.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.26.26.26.1.m1.1c">20</annotation></semantics></math></td>
</tr>
<tr id="A4.T4.27.27.27" class="ltx_tr">
<td id="A4.T4.27.27.27.2" class="ltx_td ltx_align_right ltx_border_bb">Max steps</td>
<td id="A4.T4.27.27.27.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><math id="A4.T4.27.27.27.1.m1.1" class="ltx_Math" alttext="400" display="inline"><semantics id="A4.T4.27.27.27.1.m1.1a"><mn id="A4.T4.27.27.27.1.m1.1.1" xref="A4.T4.27.27.27.1.m1.1.1.cmml">400</mn><annotation-xml encoding="MathML-Content" id="A4.T4.27.27.27.1.m1.1b"><cn type="integer" id="A4.T4.27.27.27.1.m1.1.1.cmml" xref="A4.T4.27.27.27.1.m1.1.1">400</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.27.27.27.1.m1.1c">400</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Explored hyperparameter space for fine-tuning used in our experiment.</figcaption>
</figure>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Sample Responses Generated from Llama3 and Gemma2</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p">Since BERT, image-only, and multimodal models are fine-tuned on our dataset, we provide exemplary responses generated by the other two text-only state-of-the-arts, Llama3 and Gemma2, in zero-shot setups, which may offer some insights for further study.
Human annotations and original transcripts are given below.</p>
</div>
<section id="A5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span><span id="A5.SS1.3.1" class="ltx_text ltx_font_smallcaps">Support</span> Example</h3>

<div id="A5.SS1.2" class="ltx_logical-block">
<div id="A5.SS1.2.p1" class="ltx_para">
<p id="A5.SS1.2.p1.1" class="ltx_p ltx_align_center">“<span id="A5.SS1.2.p1.1.1" class="ltx_text ltx_font_italic">Together we can address the causes and impacts of climate change to ensure that our spectacular ocean and great lakes continue to thrive while supporting our economies and values.</span>”
<span id="A5.SS1.2.p1.1.2" class="ltx_text ltx_font_typewriter">EIB-040</span></p>
<img src="/html/2409.18346/assets/figs/EIB-040.jpg" id="A5.SS1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="303" alt="[Uncaptioned image]">
</div>
</div>
<section id="A5.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Llama3 Response</h4>

<div id="A5.SS1.SSS0.Px1.p1" class="ltx_para">
<span id="A5.SS1.SSS0.Px1.p1.1" class="ltx_ERROR undefined">{lightbox}</span>
<p id="A5.SS1.SSS0.Px1.p1.2" class="ltx_p"><span id="A5.SS1.SSS0.Px1.p1.2.1" class="ltx_text ltx_font_typewriter">‘I would classify the stance of this text towards climate change as a 1 (<span id="A5.SS1.SSS0.Px1.p1.2.1.1" class="ltx_text ltx_font_smallcaps">Support</span>). The text explicitly mentions addressing the "causes" and "impacts" of climate change, indicating an acceptance of the reality and need to take action on climate change. Additionally, it frames taking action on climate change as a positive step to ensure the continued health of natural systems and support economic and societal values.’</span></p>
</div>
</section>
<section id="A5.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Gemma2 Response</h4>

<div id="A5.SS1.SSS0.Px2.p1" class="ltx_para">
<span id="A5.SS1.SSS0.Px2.p1.1" class="ltx_ERROR undefined">{lightbox}</span>
<p id="A5.SS1.SSS0.Px2.p1.2" class="ltx_p"><span id="A5.SS1.SSS0.Px2.p1.2.1" class="ltx_text ltx_font_typewriter">‘The stance of this text towards climate change is <span id="A5.SS1.SSS0.Px2.p1.2.1.1" class="ltx_text ltx_font_bold">1 (<span id="A5.SS1.SSS0.Px2.p1.2.1.1.1" class="ltx_text ltx_font_smallcaps">Support</span>)</span>. 
<br class="ltx_break">Here’s why:
<br class="ltx_break">* <span id="A5.SS1.SSS0.Px2.p1.2.1.2" class="ltx_text ltx_font_bold">"Address the causes and impacts of climate change" directly indicates a willingness to take action on climate change.
<br class="ltx_break">* Phrasing like "ensure that our spectacular ocean and great lakes continue to thrive"</span> highlights the negative consequences of inaction and emphasizes the importance of protecting natural resources.
<br class="ltx_break">The text uses positive and collaborative language, suggesting support for addressing climate change.’</span></p>
</div>
</section>
</section>
<section id="A5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.2 </span><span id="A5.SS2.3.1" class="ltx_text ltx_font_smallcaps">Neutral</span> Example</h3>

<div id="A5.SS2.2" class="ltx_logical-block">
<div id="A5.SS2.2.p1" class="ltx_para">
<p id="A5.SS2.2.p1.1" class="ltx_p ltx_align_center">“<span id="A5.SS2.2.p1.1.1" class="ltx_text ltx_font_italic">[Music]</span>”
<span id="A5.SS2.2.p1.1.2" class="ltx_text ltx_font_typewriter">AMCC-072</span></p>
<img src="/html/2409.18346/assets/figs/AMCC-072.jpg" id="A5.SS2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="303" alt="[Uncaptioned image]">
</div>
</div>
<section id="A5.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Llama3 Response</h4>

<div id="A5.SS2.SSS0.Px1.p1" class="ltx_para">
<span id="A5.SS2.SSS0.Px1.p1.1" class="ltx_ERROR undefined">{lightbox}</span>
<p id="A5.SS2.SSS0.Px1.p1.2" class="ltx_p"><span id="A5.SS2.SSS0.Px1.p1.2.1" class="ltx_text ltx_font_typewriter">‘‘I’m happy to help! However, I must point out that the provided text appears to be a single musical note, which doesn’t contain any information or opinion about climate change. Therefore, it’s impossible for me to classify its stance towards climate change. 
<br class="ltx_break">If you meant to provide a different text, please feel free to share it, and I’ll be happy to help with the classification!’’</span></p>
</div>
</section>
<section id="A5.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Gemma2 Response</h4>

<div id="A5.SS2.SSS0.Px2.p1" class="ltx_para">
<span id="A5.SS2.SSS0.Px2.p1.1" class="ltx_ERROR undefined">{lightbox}</span>
<p id="A5.SS2.SSS0.Px2.p1.2" class="ltx_p"><span id="A5.SS2.SSS0.Px2.p1.2.1" class="ltx_text ltx_font_typewriter">‘The provided text "[Music]" does not express a stance on climate change. 
<br class="ltx_break">Therefore, the classification is <span id="A5.SS2.SSS0.Px2.p1.2.1.1" class="ltx_text ltx_font_bold">0 (<span id="A5.SS2.SSS0.Px2.p1.2.1.1.1" class="ltx_text ltx_font_smallcaps">Neutral</span>)</span>.’</span></p>
</div>
</section>
</section>
<section id="A5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.3 </span><span id="A5.SS3.3.1" class="ltx_text ltx_font_smallcaps">Oppose</span> Example</h3>

<div id="A5.SS3.2" class="ltx_logical-block">
<div id="A5.SS3.2.p1" class="ltx_para">
<p id="A5.SS3.2.p1.1" class="ltx_p ltx_align_center">“<span id="A5.SS3.2.p1.1.1" class="ltx_text ltx_font_italic">Injuries, property damage – even homelessness and loss of loved ones.</span>”
<span id="A5.SS3.2.p1.1.2" class="ltx_text ltx_font_typewriter">CCAH-010</span></p>
<img src="/html/2409.18346/assets/figs/CCAH-010.jpg" id="A5.SS3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="303" alt="[Uncaptioned image]">
</div>
</div>
<section id="A5.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Llama3 Response</h4>

<div id="A5.SS3.SSS0.Px1.p1" class="ltx_para">
<span id="A5.SS3.SSS0.Px1.p1.1" class="ltx_ERROR undefined">{lightbox}</span>
<p id="A5.SS3.SSS0.Px1.p1.2" class="ltx_p"><span id="A5.SS3.SSS0.Px1.p1.2.1" class="ltx_text ltx_font_typewriter">‘I would classify the stance of this text towards climate change as 2 (oppose). The text is listing potential consequences of climate change, such as injuries, property damage, homelessness, and loss of loved ones, which implies that the author views climate change as a negative and harmful phenomenon.’</span></p>
</div>
</section>
<section id="A5.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Gemma2 Responses</h4>

<div id="A5.SS3.SSS0.Px2.p1" class="ltx_para">
<span id="A5.SS3.SSS0.Px2.p1.1" class="ltx_ERROR undefined">{lightbox}</span>
<p id="A5.SS3.SSS0.Px2.p1.2" class="ltx_p"><span id="A5.SS3.SSS0.Px2.p1.2.1" class="ltx_text ltx_font_typewriter">‘The stance expressed in "Injuries, property damage – even homelessness and loss of loved ones" is strongly suggestive of <span id="A5.SS3.SSS0.Px2.p1.2.1.1" class="ltx_text ltx_font_bold">1 (<span id="A5.SS3.SSS0.Px2.p1.2.1.1.1" class="ltx_text ltx_font_smallcaps">Support</span>)</span> for addressing climate change. 
<br class="ltx_break">Here’s why:</span></p>
<ul id="A5.I1" class="ltx_itemize">
<li id="A5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A5.I1.i1.p1" class="ltx_para">
<p id="A5.I1.i1.p1.1" class="ltx_p"><span id="A5.I1.i1.p1.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">Negative Consequences:</span><span id="A5.I1.i1.p1.1.2" class="ltx_text ltx_font_typewriter"> The text lists severe negative consequences that are widely associated with the effects of climate change.</span></p>
</div>
</li>
<li id="A5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A5.I1.i2.p1" class="ltx_para">
<p id="A5.I1.i2.p1.1" class="ltx_p"><span id="A5.I1.i2.p1.1.1" class="ltx_text ltx_font_typewriter ltx_font_bold">Emotional Impact:</span><span id="A5.I1.i2.p1.1.2" class="ltx_text ltx_font_typewriter"> Phrases like "loss of loved ones" evoke a strong emotional response, aiming to highlight the human cost of inaction on climate change.</span></p>
</div>
</li>
</ul>
<p id="A5.SS3.SSS0.Px2.p1.3" class="ltx_p"><span id="A5.SS3.SSS0.Px2.p1.3.1" class="ltx_text ltx_font_typewriter">The language implies a concern for these potential outcomes and suggests a need for action to prevent or mitigate them.’</span></p>
</div>
</section>
</section>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Dataset Overview</h2>

<figure id="A6.T5" class="ltx_table">
<div id="A6.T5.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:388.1pt;height:908.5pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-195.8pt,458.2pt) scale(0.497694360670742,0.497694360670742) ;">
<table id="A6.T5.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A6.T5.3.1.1.1" class="ltx_tr">
<th id="A6.T5.3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A6.T5.3.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">#</span></th>
<th id="A6.T5.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A6.T5.3.1.1.1.2.1" class="ltx_text ltx_font_smallcaps">YouTube-ID</span></th>
<th id="A6.T5.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A6.T5.3.1.1.1.3.1" class="ltx_text ltx_font_smallcaps">Title</span></th>
<th id="A6.T5.3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A6.T5.3.1.1.1.4.1" class="ltx_text ltx_font_smallcaps">Abbreviation</span></th>
<th id="A6.T5.3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A6.T5.3.1.1.1.5.1" class="ltx_text ltx_font_smallcaps"># Pairs</span></th>
<th id="A6.T5.3.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A6.T5.3.1.1.1.6.1" class="ltx_text ltx_font_smallcaps">Split</span></th>
<th id="A6.T5.3.1.1.1.7" class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="A6.T5.3.1.1.1.7.1" class="ltx_text ltx_font_smallcaps">VideoStance</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A6.T5.3.1.2.1" class="ltx_tr">
<td id="A6.T5.3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="A6.T5.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><a target="_blank" href="https://www.youtube.com/watch?v=brN2_YNAEK8" title="" class="ltx_ref ltx_href">brN2_YNAEK8</a></td>
<td id="A6.T5.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">About the Committee on Climate Change</td>
<td id="A6.T5.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">ACCC</td>
<td id="A6.T5.3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">28</td>
<td id="A6.T5.3.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">Train</td>
<td id="A6.T5.3.1.2.1.7" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t"><span id="A6.T5.3.1.2.1.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.3.2" class="ltx_tr">
<td id="A6.T5.3.1.3.2.1" class="ltx_td ltx_align_center">2</td>
<td id="A6.T5.3.1.3.2.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=NTP5a1M2Z6I" title="" class="ltx_ref ltx_href">NTP5a1M2Z6I</a></td>
<td id="A6.T5.3.1.3.2.3" class="ltx_td ltx_align_center">Addressing Climate Change through Fiscal Policies in Indonesia</td>
<td id="A6.T5.3.1.3.2.4" class="ltx_td ltx_align_center">ACCFP</td>
<td id="A6.T5.3.1.3.2.5" class="ltx_td ltx_align_center">42</td>
<td id="A6.T5.3.1.3.2.6" class="ltx_td ltx_align_center">Test</td>
<td id="A6.T5.3.1.3.2.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.3.2.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.4.3" class="ltx_tr">
<td id="A6.T5.3.1.4.3.1" class="ltx_td ltx_align_center">3</td>
<td id="A6.T5.3.1.4.3.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=t-MiN_0kYhM" title="" class="ltx_ref ltx_href">t-MiN_0kYhM</a></td>
<td id="A6.T5.3.1.4.3.3" class="ltx_td ltx_align_center">Analysing impacts of air quality policies on health and climate change | Denise Mauzerall</td>
<td id="A6.T5.3.1.4.3.4" class="ltx_td ltx_align_center">AIAQ</td>
<td id="A6.T5.3.1.4.3.5" class="ltx_td ltx_align_center">59</td>
<td id="A6.T5.3.1.4.3.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.4.3.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.4.3.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.5.4" class="ltx_tr">
<td id="A6.T5.3.1.5.4.1" class="ltx_td ltx_align_center">4</td>
<td id="A6.T5.3.1.5.4.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=BQr593iCEn8" title="" class="ltx_ref ltx_href">BQr593iCEn8</a></td>
<td id="A6.T5.3.1.5.4.3" class="ltx_td ltx_align_center">AI and digital twins: Tools to tackle climate change</td>
<td id="A6.T5.3.1.5.4.4" class="ltx_td ltx_align_center">AIDT</td>
<td id="A6.T5.3.1.5.4.5" class="ltx_td ltx_align_center">22</td>
<td id="A6.T5.3.1.5.4.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.5.4.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.5.4.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.6.5" class="ltx_tr">
<td id="A6.T5.3.1.6.5.1" class="ltx_td ltx_align_center">5</td>
<td id="A6.T5.3.1.6.5.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=T4CTgXYI2kA" title="" class="ltx_ref ltx_href">T4CTgXYI2kA</a></td>
<td id="A6.T5.3.1.6.5.3" class="ltx_td ltx_align_center">1-Arctic Methane and Climate Change</td>
<td id="A6.T5.3.1.6.5.4" class="ltx_td ltx_align_center">AMCC</td>
<td id="A6.T5.3.1.6.5.5" class="ltx_td ltx_align_center">74</td>
<td id="A6.T5.3.1.6.5.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.6.5.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.6.5.7.1" class="ltx_text ltx_font_smallcaps">Neutral</span></td>
</tr>
<tr id="A6.T5.3.1.7.6" class="ltx_tr">
<td id="A6.T5.3.1.7.6.1" class="ltx_td ltx_align_center">6</td>
<td id="A6.T5.3.1.7.6.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=xajNochi7tM" title="" class="ltx_ref ltx_href">xajNochi7tM</a></td>
<td id="A6.T5.3.1.7.6.3" class="ltx_td ltx_align_center">Big data and climate change</td>
<td id="A6.T5.3.1.7.6.4" class="ltx_td ltx_align_center">BDCC</td>
<td id="A6.T5.3.1.7.6.5" class="ltx_td ltx_align_center">33</td>
<td id="A6.T5.3.1.7.6.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.7.6.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.7.6.7.1" class="ltx_text ltx_font_smallcaps">Neutral</span></td>
</tr>
<tr id="A6.T5.3.1.8.7" class="ltx_tr">
<td id="A6.T5.3.1.8.7.1" class="ltx_td ltx_align_center">7</td>
<td id="A6.T5.3.1.8.7.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=eTqKLJ_o9yQ" title="" class="ltx_ref ltx_href">eTqKLJ_o9yQ</a></td>
<td id="A6.T5.3.1.8.7.3" class="ltx_td ltx_align_center">Bitcoin Energy Consumption &amp; Climate Change: Does Bitcoin Use Too Much Energy? [2021]</td>
<td id="A6.T5.3.1.8.7.4" class="ltx_td ltx_align_center">BECCC</td>
<td id="A6.T5.3.1.8.7.5" class="ltx_td ltx_align_center">41</td>
<td id="A6.T5.3.1.8.7.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.8.7.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.8.7.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.9.8" class="ltx_tr">
<td id="A6.T5.3.1.9.8.1" class="ltx_td ltx_align_center">8</td>
<td id="A6.T5.3.1.9.8.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=iKzdIzN4q2Q" title="" class="ltx_ref ltx_href">iKzdIzN4q2Q</a></td>
<td id="A6.T5.3.1.9.8.3" class="ltx_td ltx_align_center">Belize: Women in Fisheries Forum 5: Gender and Climate Change - Understanding the Link</td>
<td id="A6.T5.3.1.9.8.4" class="ltx_td ltx_align_center">BWFF</td>
<td id="A6.T5.3.1.9.8.5" class="ltx_td ltx_align_center">79</td>
<td id="A6.T5.3.1.9.8.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.9.8.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.9.8.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.10.9" class="ltx_tr">
<td id="A6.T5.3.1.10.9.1" class="ltx_td ltx_align_center">9</td>
<td id="A6.T5.3.1.10.9.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=s4ly6o-VT90" title="" class="ltx_ref ltx_href">s4ly6o-VT90</a></td>
<td id="A6.T5.3.1.10.9.3" class="ltx_td ltx_align_center">Connections between air quality and climate - English - Sept. 2021</td>
<td id="A6.T5.3.1.10.9.4" class="ltx_td ltx_align_center">CBAQC</td>
<td id="A6.T5.3.1.10.9.5" class="ltx_td ltx_align_center">35</td>
<td id="A6.T5.3.1.10.9.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.10.9.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.10.9.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.11.10" class="ltx_tr">
<td id="A6.T5.3.1.11.10.1" class="ltx_td ltx_align_center">10</td>
<td id="A6.T5.3.1.11.10.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=FGs2QQWFqyQ" title="" class="ltx_ref ltx_href">FGs2QQWFqyQ</a></td>
<td id="A6.T5.3.1.11.10.3" class="ltx_td ltx_align_center">The Crucial Connection: Climate Change and Health | Kaiser Permanente</td>
<td id="A6.T5.3.1.11.10.4" class="ltx_td ltx_align_center">CCAH</td>
<td id="A6.T5.3.1.11.10.5" class="ltx_td ltx_align_center">30</td>
<td id="A6.T5.3.1.11.10.6" class="ltx_td ltx_align_center">Test</td>
<td id="A6.T5.3.1.11.10.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.11.10.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.12.11" class="ltx_tr">
<td id="A6.T5.3.1.12.11.1" class="ltx_td ltx_align_center">11</td>
<td id="A6.T5.3.1.12.11.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=1tGWJ-NkcGU" title="" class="ltx_ref ltx_href">1tGWJ-NkcGU</a></td>
<td id="A6.T5.3.1.12.11.3" class="ltx_td ltx_align_center">Climate change, biodiversity and nutrition - Helping local heroes tell their stories</td>
<td id="A6.T5.3.1.12.11.4" class="ltx_td ltx_align_center">CCBN</td>
<td id="A6.T5.3.1.12.11.5" class="ltx_td ltx_align_center">29</td>
<td id="A6.T5.3.1.12.11.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.12.11.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.12.11.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.13.12" class="ltx_tr">
<td id="A6.T5.3.1.13.12.1" class="ltx_td ltx_align_center">12</td>
<td id="A6.T5.3.1.13.12.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=lAop3wreUek" title="" class="ltx_ref ltx_href">lAop3wreUek</a></td>
<td id="A6.T5.3.1.13.12.3" class="ltx_td ltx_align_center">Climate change, biodiversity and nutrition nexus</td>
<td id="A6.T5.3.1.13.12.4" class="ltx_td ltx_align_center">CCBNN</td>
<td id="A6.T5.3.1.13.12.5" class="ltx_td ltx_align_center">15</td>
<td id="A6.T5.3.1.13.12.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.13.12.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.13.12.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.14.13" class="ltx_tr">
<td id="A6.T5.3.1.14.13.1" class="ltx_td ltx_align_center">13</td>
<td id="A6.T5.3.1.14.13.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=4VXSrQospVY" title="" class="ltx_ref ltx_href">4VXSrQospVY</a></td>
<td id="A6.T5.3.1.14.13.3" class="ltx_td ltx_align_center">Can climate change and biodiversity loss be tackled together?</td>
<td id="A6.T5.3.1.14.13.4" class="ltx_td ltx_align_center">CCCBL</td>
<td id="A6.T5.3.1.14.13.5" class="ltx_td ltx_align_center">24</td>
<td id="A6.T5.3.1.14.13.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.14.13.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.14.13.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.15.14" class="ltx_tr">
<td id="A6.T5.3.1.15.14.1" class="ltx_td ltx_align_center">14</td>
<td id="A6.T5.3.1.15.14.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=DRXQ9ixPbD8" title="" class="ltx_ref ltx_href">DRXQ9ixPbD8</a></td>
<td id="A6.T5.3.1.15.14.3" class="ltx_td ltx_align_center">Combating climate change in the Pacific</td>
<td id="A6.T5.3.1.15.14.4" class="ltx_td ltx_align_center">CCCP</td>
<td id="A6.T5.3.1.15.14.5" class="ltx_td ltx_align_center">28</td>
<td id="A6.T5.3.1.15.14.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.15.14.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.15.14.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.16.15" class="ltx_tr">
<td id="A6.T5.3.1.16.15.1" class="ltx_td ltx_align_center">15</td>
<td id="A6.T5.3.1.16.15.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=TdxNG8L4JCM" title="" class="ltx_ref ltx_href">TdxNG8L4JCM</a></td>
<td id="A6.T5.3.1.16.15.3" class="ltx_td ltx_align_center">Climate Change and Conflict in Somalia</td>
<td id="A6.T5.3.1.16.15.4" class="ltx_td ltx_align_center">CCCS</td>
<td id="A6.T5.3.1.16.15.5" class="ltx_td ltx_align_center">58</td>
<td id="A6.T5.3.1.16.15.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.16.15.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.16.15.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.17.16" class="ltx_tr">
<td id="A6.T5.3.1.17.16.1" class="ltx_td ltx_align_center">16</td>
<td id="A6.T5.3.1.17.16.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=8fbrnAAg7VM" title="" class="ltx_ref ltx_href">8fbrnAAg7VM</a></td>
<td id="A6.T5.3.1.17.16.3" class="ltx_td ltx_align_center">Climate change and development</td>
<td id="A6.T5.3.1.17.16.4" class="ltx_td ltx_align_center">CCD</td>
<td id="A6.T5.3.1.17.16.5" class="ltx_td ltx_align_center">106</td>
<td id="A6.T5.3.1.17.16.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.17.16.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.17.16.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.18.17" class="ltx_tr">
<td id="A6.T5.3.1.18.17.1" class="ltx_td ltx_align_center">17</td>
<td id="A6.T5.3.1.18.17.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=nXOB8YPyc04" title="" class="ltx_ref ltx_href">nXOB8YPyc04</a></td>
<td id="A6.T5.3.1.18.17.3" class="ltx_td ltx_align_center">Climate Change and Food Supply</td>
<td id="A6.T5.3.1.18.17.4" class="ltx_td ltx_align_center">CCFS</td>
<td id="A6.T5.3.1.18.17.5" class="ltx_td ltx_align_center">43</td>
<td id="A6.T5.3.1.18.17.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.18.17.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.18.17.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.19.18" class="ltx_tr">
<td id="A6.T5.3.1.19.18.1" class="ltx_td ltx_align_center">18</td>
<td id="A6.T5.3.1.19.18.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=MNdF-eVRWX4" title="" class="ltx_ref ltx_href">MNdF-eVRWX4</a></td>
<td id="A6.T5.3.1.19.18.3" class="ltx_td ltx_align_center">Climate Change Fuelling Wilder Weather</td>
<td id="A6.T5.3.1.19.18.4" class="ltx_td ltx_align_center">CCFWW</td>
<td id="A6.T5.3.1.19.18.5" class="ltx_td ltx_align_center">27</td>
<td id="A6.T5.3.1.19.18.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.19.18.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.19.18.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.20.19" class="ltx_tr">
<td id="A6.T5.3.1.20.19.1" class="ltx_td ltx_align_center">19</td>
<td id="A6.T5.3.1.20.19.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=v24wT16OU2w" title="" class="ltx_ref ltx_href">v24wT16OU2w</a></td>
<td id="A6.T5.3.1.20.19.3" class="ltx_td ltx_align_center">Climate Change, Global Food Security, and the U.S. Food System</td>
<td id="A6.T5.3.1.20.19.4" class="ltx_td ltx_align_center">CCGFS</td>
<td id="A6.T5.3.1.20.19.5" class="ltx_td ltx_align_center">74</td>
<td id="A6.T5.3.1.20.19.6" class="ltx_td ltx_align_center">Dev</td>
<td id="A6.T5.3.1.20.19.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.20.19.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.21.20" class="ltx_tr">
<td id="A6.T5.3.1.21.20.1" class="ltx_td ltx_align_center">20</td>
<td id="A6.T5.3.1.21.20.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=CA8iTY7iMCk" title="" class="ltx_ref ltx_href">CA8iTY7iMCk</a></td>
<td id="A6.T5.3.1.21.20.3" class="ltx_td ltx_align_center">Climate Change and our Health (ADB Insight Full Episode)</td>
<td id="A6.T5.3.1.21.20.4" class="ltx_td ltx_align_center">CCH</td>
<td id="A6.T5.3.1.21.20.5" class="ltx_td ltx_align_center">118</td>
<td id="A6.T5.3.1.21.20.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.21.20.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.21.20.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.22.21" class="ltx_tr">
<td id="A6.T5.3.1.22.21.1" class="ltx_td ltx_align_center">21</td>
<td id="A6.T5.3.1.22.21.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=mPE7D0wRYoU" title="" class="ltx_ref ltx_href">mPE7D0wRYoU</a></td>
<td id="A6.T5.3.1.22.21.3" class="ltx_td ltx_align_center">Climate Change: Health Equity Stories from The Colorado Trust (English subtitles)</td>
<td id="A6.T5.3.1.22.21.4" class="ltx_td ltx_align_center">CCHES</td>
<td id="A6.T5.3.1.22.21.5" class="ltx_td ltx_align_center">85</td>
<td id="A6.T5.3.1.22.21.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.22.21.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.22.21.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.23.22" class="ltx_tr">
<td id="A6.T5.3.1.23.22.1" class="ltx_td ltx_align_center">22</td>
<td id="A6.T5.3.1.23.22.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=KxBAiad3Xto" title="" class="ltx_ref ltx_href">KxBAiad3Xto</a></td>
<td id="A6.T5.3.1.23.22.3" class="ltx_td ltx_align_center">Climate change in the Australian Alps</td>
<td id="A6.T5.3.1.23.22.4" class="ltx_td ltx_align_center">CCIAA</td>
<td id="A6.T5.3.1.23.22.5" class="ltx_td ltx_align_center">43</td>
<td id="A6.T5.3.1.23.22.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.23.22.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.23.22.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.24.23" class="ltx_tr">
<td id="A6.T5.3.1.24.23.1" class="ltx_td ltx_align_center">23</td>
<td id="A6.T5.3.1.24.23.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=yeih2v4P25A" title="" class="ltx_ref ltx_href">yeih2v4P25A</a></td>
<td id="A6.T5.3.1.24.23.3" class="ltx_td ltx_align_center">Climate Change: It’s About Health | Kaiser Permanente</td>
<td id="A6.T5.3.1.24.23.4" class="ltx_td ltx_align_center">CCIAH</td>
<td id="A6.T5.3.1.24.23.5" class="ltx_td ltx_align_center">37</td>
<td id="A6.T5.3.1.24.23.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.24.23.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.24.23.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.25.24" class="ltx_tr">
<td id="A6.T5.3.1.25.24.1" class="ltx_td ltx_align_center">24</td>
<td id="A6.T5.3.1.25.24.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=m95K7LClIC4" title="" class="ltx_ref ltx_href">m95K7LClIC4</a></td>
<td id="A6.T5.3.1.25.24.3" class="ltx_td ltx_align_center">350.org - Climate Change Is About Power</td>
<td id="A6.T5.3.1.25.24.4" class="ltx_td ltx_align_center">CCIAP</td>
<td id="A6.T5.3.1.25.24.5" class="ltx_td ltx_align_center">28</td>
<td id="A6.T5.3.1.25.24.6" class="ltx_td ltx_align_center">Dev</td>
<td id="A6.T5.3.1.25.24.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.25.24.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.26.25" class="ltx_tr">
<td id="A6.T5.3.1.26.25.1" class="ltx_td ltx_align_center">25</td>
<td id="A6.T5.3.1.26.25.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=b919Fb-P3N8" title="" class="ltx_ref ltx_href">b919Fb-P3N8</a></td>
<td id="A6.T5.3.1.26.25.3" class="ltx_td ltx_align_center">Climate Change Impacts for Canadian Directors</td>
<td id="A6.T5.3.1.26.25.4" class="ltx_td ltx_align_center">CCICD</td>
<td id="A6.T5.3.1.26.25.5" class="ltx_td ltx_align_center">33</td>
<td id="A6.T5.3.1.26.25.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.26.25.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.26.25.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.27.26" class="ltx_tr">
<td id="A6.T5.3.1.27.26.1" class="ltx_td ltx_align_center">26</td>
<td id="A6.T5.3.1.27.26.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=A7ktYbVwr90" title="" class="ltx_ref ltx_href">A7ktYbVwr90</a></td>
<td id="A6.T5.3.1.27.26.3" class="ltx_td ltx_align_center">Climate change is simple: David Roberts at TEDxTheEvergreenStateCollege</td>
<td id="A6.T5.3.1.27.26.4" class="ltx_td ltx_align_center">CCIS</td>
<td id="A6.T5.3.1.27.26.5" class="ltx_td ltx_align_center">213</td>
<td id="A6.T5.3.1.27.26.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.27.26.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.27.26.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.28.27" class="ltx_tr">
<td id="A6.T5.3.1.28.27.1" class="ltx_td ltx_align_center">27</td>
<td id="A6.T5.3.1.28.27.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=yVvVk2zNSbo" title="" class="ltx_ref ltx_href">yVvVk2zNSbo</a></td>
<td id="A6.T5.3.1.28.27.3" class="ltx_td ltx_align_center">Untold stories of climate change loss and damage in the LDCs: Sierra Leone</td>
<td id="A6.T5.3.1.28.27.4" class="ltx_td ltx_align_center">CCISL</td>
<td id="A6.T5.3.1.28.27.5" class="ltx_td ltx_align_center">26</td>
<td id="A6.T5.3.1.28.27.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.28.27.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.28.27.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.29.28" class="ltx_tr">
<td id="A6.T5.3.1.29.28.1" class="ltx_td ltx_align_center">28</td>
<td id="A6.T5.3.1.29.28.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=lNBP7aRskVE" title="" class="ltx_ref ltx_href">lNBP7aRskVE</a></td>
<td id="A6.T5.3.1.29.28.3" class="ltx_td ltx_align_center">Climate Change: Mitigate or Adapt</td>
<td id="A6.T5.3.1.29.28.4" class="ltx_td ltx_align_center">CCMA</td>
<td id="A6.T5.3.1.29.28.5" class="ltx_td ltx_align_center">46</td>
<td id="A6.T5.3.1.29.28.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.29.28.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.29.28.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.30.29" class="ltx_tr">
<td id="A6.T5.3.1.30.29.1" class="ltx_td ltx_align_center">29</td>
<td id="A6.T5.3.1.30.29.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=5DVa8xBgToc" title="" class="ltx_ref ltx_href">5DVa8xBgToc</a></td>
<td id="A6.T5.3.1.30.29.3" class="ltx_td ltx_align_center">Climate Change for South African Directors</td>
<td id="A6.T5.3.1.30.29.4" class="ltx_td ltx_align_center">CCSAD</td>
<td id="A6.T5.3.1.30.29.5" class="ltx_td ltx_align_center">59</td>
<td id="A6.T5.3.1.30.29.6" class="ltx_td ltx_align_center">Test</td>
<td id="A6.T5.3.1.30.29.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.30.29.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.31.30" class="ltx_tr">
<td id="A6.T5.3.1.31.30.1" class="ltx_td ltx_align_center">30</td>
<td id="A6.T5.3.1.31.30.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=GeksVaAnMzc" title="" class="ltx_ref ltx_href">GeksVaAnMzc</a></td>
<td id="A6.T5.3.1.31.30.3" class="ltx_td ltx_align_center">Climate Change or Social Change: The Role of Blockchain</td>
<td id="A6.T5.3.1.31.30.4" class="ltx_td ltx_align_center">CCSC</td>
<td id="A6.T5.3.1.31.30.5" class="ltx_td ltx_align_center">295</td>
<td id="A6.T5.3.1.31.30.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.31.30.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.31.30.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.32.31" class="ltx_tr">
<td id="A6.T5.3.1.32.31.1" class="ltx_td ltx_align_center">31</td>
<td id="A6.T5.3.1.32.31.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=zI9h-HTBHO8" title="" class="ltx_ref ltx_href">zI9h-HTBHO8</a></td>
<td id="A6.T5.3.1.32.31.3" class="ltx_td ltx_align_center">Climate Change: Take Action</td>
<td id="A6.T5.3.1.32.31.4" class="ltx_td ltx_align_center">CCTA</td>
<td id="A6.T5.3.1.32.31.5" class="ltx_td ltx_align_center">22</td>
<td id="A6.T5.3.1.32.31.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.32.31.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.32.31.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.33.32" class="ltx_tr">
<td id="A6.T5.3.1.33.32.1" class="ltx_td ltx_align_center">32</td>
<td id="A6.T5.3.1.33.32.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=fN-ZnY61_C8" title="" class="ltx_ref ltx_href">fN-ZnY61_C8</a></td>
<td id="A6.T5.3.1.33.32.3" class="ltx_td ltx_align_center">Climate Change: The Philippines</td>
<td id="A6.T5.3.1.33.32.4" class="ltx_td ltx_align_center">CCTP</td>
<td id="A6.T5.3.1.33.32.5" class="ltx_td ltx_align_center">23</td>
<td id="A6.T5.3.1.33.32.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.33.32.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.33.32.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.34.33" class="ltx_tr">
<td id="A6.T5.3.1.34.33.1" class="ltx_td ltx_align_center">33</td>
<td id="A6.T5.3.1.34.33.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=Vve6zge_RsA" title="" class="ltx_ref ltx_href">Vve6zge_RsA</a></td>
<td id="A6.T5.3.1.34.33.3" class="ltx_td ltx_align_center">Climate change unlikely to increase malaria burden in West Africa</td>
<td id="A6.T5.3.1.34.33.4" class="ltx_td ltx_align_center">CCUIM</td>
<td id="A6.T5.3.1.34.33.5" class="ltx_td ltx_align_center">48</td>
<td id="A6.T5.3.1.34.33.6" class="ltx_td ltx_align_center">Test</td>
<td id="A6.T5.3.1.34.33.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.34.33.7.1" class="ltx_text ltx_font_smallcaps">Neutral</span></td>
</tr>
<tr id="A6.T5.3.1.35.34" class="ltx_tr">
<td id="A6.T5.3.1.35.34.1" class="ltx_td ltx_align_center">34</td>
<td id="A6.T5.3.1.35.34.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=RZ-N5KwBaVc" title="" class="ltx_ref ltx_href">RZ-N5KwBaVc</a></td>
<td id="A6.T5.3.1.35.34.3" class="ltx_td ltx_align_center">Climate Change is Water Change</td>
<td id="A6.T5.3.1.35.34.4" class="ltx_td ltx_align_center">CCWC</td>
<td id="A6.T5.3.1.35.34.5" class="ltx_td ltx_align_center">12</td>
<td id="A6.T5.3.1.35.34.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.35.34.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.35.34.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.36.35" class="ltx_tr">
<td id="A6.T5.3.1.36.35.1" class="ltx_td ltx_align_center">35</td>
<td id="A6.T5.3.1.36.35.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=MnTm89dSHhA" title="" class="ltx_ref ltx_href">MnTm89dSHhA</a></td>
<td id="A6.T5.3.1.36.35.3" class="ltx_td ltx_align_center">Climate Change and Water Quality</td>
<td id="A6.T5.3.1.36.35.4" class="ltx_td ltx_align_center">CCWQ</td>
<td id="A6.T5.3.1.36.35.5" class="ltx_td ltx_align_center">15</td>
<td id="A6.T5.3.1.36.35.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.36.35.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.36.35.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.37.36" class="ltx_tr">
<td id="A6.T5.3.1.37.36.1" class="ltx_td ltx_align_center">36</td>
<td id="A6.T5.3.1.37.36.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=OL8a1YEhk_o" title="" class="ltx_ref ltx_href">OL8a1YEhk_o</a></td>
<td id="A6.T5.3.1.37.36.3" class="ltx_td ltx_align_center">Honest Government Ad | Climate Emergency &amp; School Strikes</td>
<td id="A6.T5.3.1.37.36.4" class="ltx_td ltx_align_center">CESS</td>
<td id="A6.T5.3.1.37.36.5" class="ltx_td ltx_align_center">29</td>
<td id="A6.T5.3.1.37.36.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.37.36.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.37.36.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.38.37" class="ltx_tr">
<td id="A6.T5.3.1.38.37.1" class="ltx_td ltx_align_center">37</td>
<td id="A6.T5.3.1.38.37.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=b7LiW66cSM4" title="" class="ltx_ref ltx_href">b7LiW66cSM4</a></td>
<td id="A6.T5.3.1.38.37.3" class="ltx_td ltx_align_center">How should Coronavirus influence the fight against Climate Change?</td>
<td id="A6.T5.3.1.38.37.4" class="ltx_td ltx_align_center">CICC</td>
<td id="A6.T5.3.1.38.37.5" class="ltx_td ltx_align_center">29</td>
<td id="A6.T5.3.1.38.37.6" class="ltx_td ltx_align_center">Dev</td>
<td id="A6.T5.3.1.38.37.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.38.37.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.39.38" class="ltx_tr">
<td id="A6.T5.3.1.39.38.1" class="ltx_td ltx_align_center">38</td>
<td id="A6.T5.3.1.39.38.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=vD0lx_b8jNM" title="" class="ltx_ref ltx_href">vD0lx_b8jNM</a></td>
<td id="A6.T5.3.1.39.38.3" class="ltx_td ltx_align_center">COP28: Nuclear Science and Technology for Climate Change Adaptation</td>
<td id="A6.T5.3.1.39.38.4" class="ltx_td ltx_align_center">COP</td>
<td id="A6.T5.3.1.39.38.5" class="ltx_td ltx_align_center">66</td>
<td id="A6.T5.3.1.39.38.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.39.38.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.39.38.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.40.39" class="ltx_tr">
<td id="A6.T5.3.1.40.39.1" class="ltx_td ltx_align_center">39</td>
<td id="A6.T5.3.1.40.39.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=MeFbo0z0xi4" title="" class="ltx_ref ltx_href">MeFbo0z0xi4</a></td>
<td id="A6.T5.3.1.40.39.3" class="ltx_td ltx_align_center">Crop production - Climate change affects biosphere - Earth Hazards - meriSTEM</td>
<td id="A6.T5.3.1.40.39.4" class="ltx_td ltx_align_center">CPCC</td>
<td id="A6.T5.3.1.40.39.5" class="ltx_td ltx_align_center">19</td>
<td id="A6.T5.3.1.40.39.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.40.39.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.40.39.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.41.40" class="ltx_tr">
<td id="A6.T5.3.1.41.40.1" class="ltx_td ltx_align_center">40</td>
<td id="A6.T5.3.1.41.40.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=FhyUbeDVM3k" title="" class="ltx_ref ltx_href">FhyUbeDVM3k</a></td>
<td id="A6.T5.3.1.41.40.3" class="ltx_td ltx_align_center">Capturing and transforming CO2 to mitigate climate change</td>
<td id="A6.T5.3.1.41.40.4" class="ltx_td ltx_align_center">CTCM</td>
<td id="A6.T5.3.1.41.40.5" class="ltx_td ltx_align_center">12</td>
<td id="A6.T5.3.1.41.40.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.41.40.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.41.40.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.42.41" class="ltx_tr">
<td id="A6.T5.3.1.42.41.1" class="ltx_td ltx_align_center">41</td>
<td id="A6.T5.3.1.42.41.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=Gu5NKLxqTak" title="" class="ltx_ref ltx_href">Gu5NKLxqTak</a></td>
<td id="A6.T5.3.1.42.41.3" class="ltx_td ltx_align_center">Diet and Climate Change: Cooking Up a Storm</td>
<td id="A6.T5.3.1.42.41.4" class="ltx_td ltx_align_center">DACC</td>
<td id="A6.T5.3.1.42.41.5" class="ltx_td ltx_align_center">77</td>
<td id="A6.T5.3.1.42.41.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.42.41.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.42.41.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.43.42" class="ltx_tr">
<td id="A6.T5.3.1.43.42.1" class="ltx_td ltx_align_center">42</td>
<td id="A6.T5.3.1.43.42.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=OfYGx-N_gB0" title="" class="ltx_ref ltx_href">OfYGx-N_gB0</a></td>
<td id="A6.T5.3.1.43.42.3" class="ltx_td ltx_align_center">Deforestation and Climate Change</td>
<td id="A6.T5.3.1.43.42.4" class="ltx_td ltx_align_center">DFCC</td>
<td id="A6.T5.3.1.43.42.5" class="ltx_td ltx_align_center">64</td>
<td id="A6.T5.3.1.43.42.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.43.42.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.43.42.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.44.43" class="ltx_tr">
<td id="A6.T5.3.1.44.43.1" class="ltx_td ltx_align_center">43</td>
<td id="A6.T5.3.1.44.43.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=CG3pN7qQqZI" title="" class="ltx_ref ltx_href">CG3pN7qQqZI</a></td>
<td id="A6.T5.3.1.44.43.3" class="ltx_td ltx_align_center">This Is How Denmark Protects Its Cities Against Climate Change</td>
<td id="A6.T5.3.1.44.43.4" class="ltx_td ltx_align_center">DPIC</td>
<td id="A6.T5.3.1.44.43.5" class="ltx_td ltx_align_center">29</td>
<td id="A6.T5.3.1.44.43.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.44.43.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.44.43.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.45.44" class="ltx_tr">
<td id="A6.T5.3.1.45.44.1" class="ltx_td ltx_align_center">44</td>
<td id="A6.T5.3.1.45.44.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=Ry-ei9Bu8UI" title="" class="ltx_ref ltx_href">Ry-ei9Bu8UI</a></td>
<td id="A6.T5.3.1.45.44.3" class="ltx_td ltx_align_center">Developing tools for equality in climate change planning in Tanzania</td>
<td id="A6.T5.3.1.45.44.4" class="ltx_td ltx_align_center">DTECC</td>
<td id="A6.T5.3.1.45.44.5" class="ltx_td ltx_align_center">51</td>
<td id="A6.T5.3.1.45.44.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.45.44.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.45.44.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.46.45" class="ltx_tr">
<td id="A6.T5.3.1.46.45.1" class="ltx_td ltx_align_center">45</td>
<td id="A6.T5.3.1.46.45.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=M17pm2iPT_c" title="" class="ltx_ref ltx_href">M17pm2iPT_c</a></td>
<td id="A6.T5.3.1.46.45.3" class="ltx_td ltx_align_center">Effects Of Climate Change In MN Discussed At Seminar</td>
<td id="A6.T5.3.1.46.45.4" class="ltx_td ltx_align_center">ECCDS</td>
<td id="A6.T5.3.1.46.45.5" class="ltx_td ltx_align_center">16</td>
<td id="A6.T5.3.1.46.45.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.46.45.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.46.45.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.47.46" class="ltx_tr">
<td id="A6.T5.3.1.47.46.1" class="ltx_td ltx_align_center">46</td>
<td id="A6.T5.3.1.47.46.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=HzL9hUOh_K4" title="" class="ltx_ref ltx_href">HzL9hUOh_K4</a></td>
<td id="A6.T5.3.1.47.46.3" class="ltx_td ltx_align_center">Ecosystems at risk from Climate Change</td>
<td id="A6.T5.3.1.47.46.4" class="ltx_td ltx_align_center">EFCC</td>
<td id="A6.T5.3.1.47.46.5" class="ltx_td ltx_align_center">34</td>
<td id="A6.T5.3.1.47.46.6" class="ltx_td ltx_align_center">Dev</td>
<td id="A6.T5.3.1.47.46.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.47.46.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.48.47" class="ltx_tr">
<td id="A6.T5.3.1.48.47.1" class="ltx_td ltx_align_center">47</td>
<td id="A6.T5.3.1.48.47.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=kjTAWBPPez0" title="" class="ltx_ref ltx_href">kjTAWBPPez0</a></td>
<td id="A6.T5.3.1.48.47.3" class="ltx_td ltx_align_center">Earth Is Blue: Climate Change in your Sanctuaries</td>
<td id="A6.T5.3.1.48.47.4" class="ltx_td ltx_align_center">EIB</td>
<td id="A6.T5.3.1.48.47.5" class="ltx_td ltx_align_center">45</td>
<td id="A6.T5.3.1.48.47.6" class="ltx_td ltx_align_center">Test</td>
<td id="A6.T5.3.1.48.47.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.48.47.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.49.48" class="ltx_tr">
<td id="A6.T5.3.1.49.48.1" class="ltx_td ltx_align_center">48</td>
<td id="A6.T5.3.1.49.48.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=Qmxg97Ae9Wg" title="" class="ltx_ref ltx_href">Qmxg97Ae9Wg</a></td>
<td id="A6.T5.3.1.49.48.3" class="ltx_td ltx_align_center">Extreme Weather and Climate Change, EarthNow</td>
<td id="A6.T5.3.1.49.48.4" class="ltx_td ltx_align_center">EWCC</td>
<td id="A6.T5.3.1.49.48.5" class="ltx_td ltx_align_center">49</td>
<td id="A6.T5.3.1.49.48.6" class="ltx_td ltx_align_center">Test</td>
<td id="A6.T5.3.1.49.48.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.49.48.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.50.49" class="ltx_tr">
<td id="A6.T5.3.1.50.49.1" class="ltx_td ltx_align_center">49</td>
<td id="A6.T5.3.1.50.49.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=Wrb4b28dgcU" title="" class="ltx_ref ltx_href">Wrb4b28dgcU</a></td>
<td id="A6.T5.3.1.50.49.3" class="ltx_td ltx_align_center">Forests and Climate Change</td>
<td id="A6.T5.3.1.50.49.4" class="ltx_td ltx_align_center">FCC</td>
<td id="A6.T5.3.1.50.49.5" class="ltx_td ltx_align_center">13</td>
<td id="A6.T5.3.1.50.49.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.50.49.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.50.49.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.51.50" class="ltx_tr">
<td id="A6.T5.3.1.51.50.1" class="ltx_td ltx_align_center">50</td>
<td id="A6.T5.3.1.51.50.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=9DaUn0geq4U" title="" class="ltx_ref ltx_href">9DaUn0geq4U</a></td>
<td id="A6.T5.3.1.51.50.3" class="ltx_td ltx_align_center">Fiji: Standing tall against climate change threats</td>
<td id="A6.T5.3.1.51.50.4" class="ltx_td ltx_align_center">FIJI</td>
<td id="A6.T5.3.1.51.50.5" class="ltx_td ltx_align_center">32</td>
<td id="A6.T5.3.1.51.50.6" class="ltx_td ltx_align_center">Dev</td>
<td id="A6.T5.3.1.51.50.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.51.50.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.52.51" class="ltx_tr">
<td id="A6.T5.3.1.52.51.1" class="ltx_td ltx_align_center">51</td>
<td id="A6.T5.3.1.52.51.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=ison6lQozDU" title="" class="ltx_ref ltx_href">ison6lQozDU</a></td>
<td id="A6.T5.3.1.52.51.3" class="ltx_td ltx_align_center">Food loss and waste are among the main causes of climate change</td>
<td id="A6.T5.3.1.52.51.4" class="ltx_td ltx_align_center">FLW</td>
<td id="A6.T5.3.1.52.51.5" class="ltx_td ltx_align_center">14</td>
<td id="A6.T5.3.1.52.51.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.52.51.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.52.51.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.53.52" class="ltx_tr">
<td id="A6.T5.3.1.53.52.1" class="ltx_td ltx_align_center">52</td>
<td id="A6.T5.3.1.53.52.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=M9wSP16P9xM" title="" class="ltx_ref ltx_href">M9wSP16P9xM</a></td>
<td id="A6.T5.3.1.53.52.3" class="ltx_td ltx_align_center">+Forest, together against climate change</td>
<td id="A6.T5.3.1.53.52.4" class="ltx_td ltx_align_center">FTACC</td>
<td id="A6.T5.3.1.53.52.5" class="ltx_td ltx_align_center">26</td>
<td id="A6.T5.3.1.53.52.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.53.52.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.53.52.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.54.53" class="ltx_tr">
<td id="A6.T5.3.1.54.53.1" class="ltx_td ltx_align_center">53</td>
<td id="A6.T5.3.1.54.53.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=epZ9Rw-i8Mo" title="" class="ltx_ref ltx_href">epZ9Rw-i8Mo</a></td>
<td id="A6.T5.3.1.54.53.3" class="ltx_td ltx_align_center">Greenland’s glaciers and Climate Change, Danish Broadcasting Corporation - Denmark</td>
<td id="A6.T5.3.1.54.53.4" class="ltx_td ltx_align_center">GGCC</td>
<td id="A6.T5.3.1.54.53.5" class="ltx_td ltx_align_center">52</td>
<td id="A6.T5.3.1.54.53.6" class="ltx_td ltx_align_center">Test</td>
<td id="A6.T5.3.1.54.53.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.54.53.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.55.54" class="ltx_tr">
<td id="A6.T5.3.1.55.54.1" class="ltx_td ltx_align_center">54</td>
<td id="A6.T5.3.1.55.54.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=S9Z_h1_LQ0o" title="" class="ltx_ref ltx_href">S9Z_h1_LQ0o</a></td>
<td id="A6.T5.3.1.55.54.3" class="ltx_td ltx_align_center">How Climate Change Affects Biodiversity</td>
<td id="A6.T5.3.1.55.54.4" class="ltx_td ltx_align_center">HCCAB</td>
<td id="A6.T5.3.1.55.54.5" class="ltx_td ltx_align_center">25</td>
<td id="A6.T5.3.1.55.54.6" class="ltx_td ltx_align_center">Dev</td>
<td id="A6.T5.3.1.55.54.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.55.54.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.56.55" class="ltx_tr">
<td id="A6.T5.3.1.56.55.1" class="ltx_td ltx_align_center">55</td>
<td id="A6.T5.3.1.56.55.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=me14ikumMZE" title="" class="ltx_ref ltx_href">me14ikumMZE</a></td>
<td id="A6.T5.3.1.56.55.3" class="ltx_td ltx_align_center">How Climate Change Affects the Ecosystem</td>
<td id="A6.T5.3.1.56.55.4" class="ltx_td ltx_align_center">HCCAE</td>
<td id="A6.T5.3.1.56.55.5" class="ltx_td ltx_align_center">31</td>
<td id="A6.T5.3.1.56.55.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.56.55.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.56.55.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.57.56" class="ltx_tr">
<td id="A6.T5.3.1.57.56.1" class="ltx_td ltx_align_center">56</td>
<td id="A6.T5.3.1.57.56.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=CGoNpwN0mrs" title="" class="ltx_ref ltx_href">CGoNpwN0mrs</a></td>
<td id="A6.T5.3.1.57.56.3" class="ltx_td ltx_align_center">How Climate Change /Actually/ Works…in 4 Minutes</td>
<td id="A6.T5.3.1.57.56.4" class="ltx_td ltx_align_center">HCCAW</td>
<td id="A6.T5.3.1.57.56.5" class="ltx_td ltx_align_center">48</td>
<td id="A6.T5.3.1.57.56.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.57.56.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.57.56.7.1" class="ltx_text ltx_font_smallcaps">Neutral</span></td>
</tr>
<tr id="A6.T5.3.1.58.57" class="ltx_tr">
<td id="A6.T5.3.1.58.57.1" class="ltx_td ltx_align_center">57</td>
<td id="A6.T5.3.1.58.57.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=KTA5onaECFE" title="" class="ltx_ref ltx_href">KTA5onaECFE</a></td>
<td id="A6.T5.3.1.58.57.3" class="ltx_td ltx_align_center">How climate change influences geopolitics - Interview with Francesco Femia</td>
<td id="A6.T5.3.1.58.57.4" class="ltx_td ltx_align_center">HCCIG</td>
<td id="A6.T5.3.1.58.57.5" class="ltx_td ltx_align_center">14</td>
<td id="A6.T5.3.1.58.57.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.58.57.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.58.57.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.59.58" class="ltx_tr">
<td id="A6.T5.3.1.59.58.1" class="ltx_td ltx_align_center">58</td>
<td id="A6.T5.3.1.59.58.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=RFsxDqQWjhk" title="" class="ltx_ref ltx_href">RFsxDqQWjhk</a></td>
<td id="A6.T5.3.1.59.58.3" class="ltx_td ltx_align_center">How COVID-19 is impacting air pollution and climate change</td>
<td id="A6.T5.3.1.59.58.4" class="ltx_td ltx_align_center">HCI</td>
<td id="A6.T5.3.1.59.58.5" class="ltx_td ltx_align_center">16</td>
<td id="A6.T5.3.1.59.58.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.59.58.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.59.58.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.60.59" class="ltx_tr">
<td id="A6.T5.3.1.60.59.1" class="ltx_td ltx_align_center">59</td>
<td id="A6.T5.3.1.60.59.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=ivN1QIvdBUI" title="" class="ltx_ref ltx_href">ivN1QIvdBUI</a></td>
<td id="A6.T5.3.1.60.59.3" class="ltx_td ltx_align_center">How do we change our attitude towards climate change? Christiana Figueres</td>
<td id="A6.T5.3.1.60.59.4" class="ltx_td ltx_align_center">HDWC</td>
<td id="A6.T5.3.1.60.59.5" class="ltx_td ltx_align_center">26</td>
<td id="A6.T5.3.1.60.59.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.60.59.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.60.59.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.61.60" class="ltx_tr">
<td id="A6.T5.3.1.61.60.1" class="ltx_td ltx_align_center">60</td>
<td id="A6.T5.3.1.61.60.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=jDueuwB3Tcs" title="" class="ltx_ref ltx_href">jDueuwB3Tcs</a></td>
<td id="A6.T5.3.1.61.60.3" class="ltx_td ltx_align_center">Human Health, Vector-Borne Diseases, and Climate Change</td>
<td id="A6.T5.3.1.61.60.4" class="ltx_td ltx_align_center">HHVBD</td>
<td id="A6.T5.3.1.61.60.5" class="ltx_td ltx_align_center">98</td>
<td id="A6.T5.3.1.61.60.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.61.60.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.61.60.7.1" class="ltx_text ltx_font_smallcaps">Neutral</span></td>
</tr>
<tr id="A6.T5.3.1.62.61" class="ltx_tr">
<td id="A6.T5.3.1.62.61.1" class="ltx_td ltx_align_center">61</td>
<td id="A6.T5.3.1.62.61.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=e_8upuAySOI" title="" class="ltx_ref ltx_href">e_8upuAySOI</a></td>
<td id="A6.T5.3.1.62.61.3" class="ltx_td ltx_align_center">Human Rights Day 2012: Climate Change and Human Rights</td>
<td id="A6.T5.3.1.62.61.4" class="ltx_td ltx_align_center">HRDCC</td>
<td id="A6.T5.3.1.62.61.5" class="ltx_td ltx_align_center">42</td>
<td id="A6.T5.3.1.62.61.6" class="ltx_td ltx_align_center">Dev</td>
<td id="A6.T5.3.1.62.61.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.62.61.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.63.62" class="ltx_tr">
<td id="A6.T5.3.1.63.62.1" class="ltx_td ltx_align_center">62</td>
<td id="A6.T5.3.1.63.62.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=vsbcasoudtM" title="" class="ltx_ref ltx_href">vsbcasoudtM</a></td>
<td id="A6.T5.3.1.63.62.3" class="ltx_td ltx_align_center">How are scientists helping whale-watchers adapt to climate change?</td>
<td id="A6.T5.3.1.63.62.4" class="ltx_td ltx_align_center">HSHWA</td>
<td id="A6.T5.3.1.63.62.5" class="ltx_td ltx_align_center">18</td>
<td id="A6.T5.3.1.63.62.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.63.62.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.63.62.7.1" class="ltx_text ltx_font_smallcaps">Neutral</span></td>
</tr>
<tr id="A6.T5.3.1.64.63" class="ltx_tr">
<td id="A6.T5.3.1.64.63.1" class="ltx_td ltx_align_center">63</td>
<td id="A6.T5.3.1.64.63.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=mc1qAnGGGCE" title="" class="ltx_ref ltx_href">mc1qAnGGGCE</a></td>
<td id="A6.T5.3.1.64.63.3" class="ltx_td ltx_align_center">How Solar Panels Work To Reduce Climate Change</td>
<td id="A6.T5.3.1.64.63.4" class="ltx_td ltx_align_center">HSPW</td>
<td id="A6.T5.3.1.64.63.5" class="ltx_td ltx_align_center">28</td>
<td id="A6.T5.3.1.64.63.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.64.63.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.64.63.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.65.64" class="ltx_tr">
<td id="A6.T5.3.1.65.64.1" class="ltx_td ltx_align_center">64</td>
<td id="A6.T5.3.1.65.64.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=RYsZ0NdHKyc" title="" class="ltx_ref ltx_href">RYsZ0NdHKyc</a></td>
<td id="A6.T5.3.1.65.64.3" class="ltx_td ltx_align_center">How the US National Security Policy Incorporates Climate Change - Interview with Alice Hill</td>
<td id="A6.T5.3.1.65.64.4" class="ltx_td ltx_align_center">HUSNS</td>
<td id="A6.T5.3.1.65.64.5" class="ltx_td ltx_align_center">39</td>
<td id="A6.T5.3.1.65.64.6" class="ltx_td ltx_align_center">Dev</td>
<td id="A6.T5.3.1.65.64.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.65.64.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.66.65" class="ltx_tr">
<td id="A6.T5.3.1.66.65.1" class="ltx_td ltx_align_center">65</td>
<td id="A6.T5.3.1.66.65.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=OSA944ShtmE" title="" class="ltx_ref ltx_href">OSA944ShtmE</a></td>
<td id="A6.T5.3.1.66.65.3" class="ltx_td ltx_align_center">IMRF: Statement from IOM’s Migration, Environment, Climate Change, and Risk Reduction Division</td>
<td id="A6.T5.3.1.66.65.4" class="ltx_td ltx_align_center">IMRF</td>
<td id="A6.T5.3.1.66.65.5" class="ltx_td ltx_align_center">16</td>
<td id="A6.T5.3.1.66.65.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.66.65.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.66.65.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.67.66" class="ltx_tr">
<td id="A6.T5.3.1.67.66.1" class="ltx_td ltx_align_center">66</td>
<td id="A6.T5.3.1.67.66.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=OKPzj-l7gp0" title="" class="ltx_ref ltx_href">OKPzj-l7gp0</a></td>
<td id="A6.T5.3.1.67.66.3" class="ltx_td ltx_align_center">INCAS: Monitoring for Climate Change</td>
<td id="A6.T5.3.1.67.66.4" class="ltx_td ltx_align_center">INCAS</td>
<td id="A6.T5.3.1.67.66.5" class="ltx_td ltx_align_center">45</td>
<td id="A6.T5.3.1.67.66.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.67.66.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.67.66.7.1" class="ltx_text ltx_font_smallcaps">Neutral</span></td>
</tr>
<tr id="A6.T5.3.1.68.67" class="ltx_tr">
<td id="A6.T5.3.1.68.67.1" class="ltx_td ltx_align_center">67</td>
<td id="A6.T5.3.1.68.67.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=ks7rCR7-mF0" title="" class="ltx_ref ltx_href">ks7rCR7-mF0</a></td>
<td id="A6.T5.3.1.68.67.3" class="ltx_td ltx_align_center">Migration and Climate Change in the Global Compact for Migration</td>
<td id="A6.T5.3.1.68.67.4" class="ltx_td ltx_align_center">MACC</td>
<td id="A6.T5.3.1.68.67.5" class="ltx_td ltx_align_center">62</td>
<td id="A6.T5.3.1.68.67.6" class="ltx_td ltx_align_center">Dev</td>
<td id="A6.T5.3.1.68.67.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.68.67.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.69.68" class="ltx_tr">
<td id="A6.T5.3.1.69.68.1" class="ltx_td ltx_align_center">68</td>
<td id="A6.T5.3.1.69.68.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=ko4cUnzoPic" title="" class="ltx_ref ltx_href">ko4cUnzoPic</a></td>
<td id="A6.T5.3.1.69.68.3" class="ltx_td ltx_align_center">Science Action: What’s the unique role of methane in climate change?</td>
<td id="A6.T5.3.1.69.68.4" class="ltx_td ltx_align_center">MICC</td>
<td id="A6.T5.3.1.69.68.5" class="ltx_td ltx_align_center">53</td>
<td id="A6.T5.3.1.69.68.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.69.68.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.69.68.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.70.69" class="ltx_tr">
<td id="A6.T5.3.1.70.69.1" class="ltx_td ltx_align_center">69</td>
<td id="A6.T5.3.1.70.69.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=vB3_49ULzf0" title="" class="ltx_ref ltx_href">vB3_49ULzf0</a></td>
<td id="A6.T5.3.1.70.69.3" class="ltx_td ltx_align_center">NASA’s Climate Advisor Discusses Climate Change</td>
<td id="A6.T5.3.1.70.69.4" class="ltx_td ltx_align_center">NASA</td>
<td id="A6.T5.3.1.70.69.5" class="ltx_td ltx_align_center">20</td>
<td id="A6.T5.3.1.70.69.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.70.69.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.70.69.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.71.70" class="ltx_tr">
<td id="A6.T5.3.1.71.70.1" class="ltx_td ltx_align_center">70</td>
<td id="A6.T5.3.1.71.70.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=EwHtHsBeRIA" title="" class="ltx_ref ltx_href">EwHtHsBeRIA</a></td>
<td id="A6.T5.3.1.71.70.3" class="ltx_td ltx_align_center">Overview of the C-ROADS Climate Change Policy Simulator</td>
<td id="A6.T5.3.1.71.70.4" class="ltx_td ltx_align_center">OCCC</td>
<td id="A6.T5.3.1.71.70.5" class="ltx_td ltx_align_center">46</td>
<td id="A6.T5.3.1.71.70.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.71.70.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.71.70.7.1" class="ltx_text ltx_font_smallcaps">Neutral</span></td>
</tr>
<tr id="A6.T5.3.1.72.71" class="ltx_tr">
<td id="A6.T5.3.1.72.71.1" class="ltx_td ltx_align_center">71</td>
<td id="A6.T5.3.1.72.71.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=CUdBaExvHy4" title="" class="ltx_ref ltx_href">CUdBaExvHy4</a></td>
<td id="A6.T5.3.1.72.71.3" class="ltx_td ltx_align_center">President Clinton On the Cost of Climate Change</td>
<td id="A6.T5.3.1.72.71.4" class="ltx_td ltx_align_center">PCOCC</td>
<td id="A6.T5.3.1.72.71.5" class="ltx_td ltx_align_center">22</td>
<td id="A6.T5.3.1.72.71.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.72.71.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.72.71.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.73.72" class="ltx_tr">
<td id="A6.T5.3.1.73.72.1" class="ltx_td ltx_align_center">72</td>
<td id="A6.T5.3.1.73.72.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=A6uRlax7AuE" title="" class="ltx_ref ltx_href">A6uRlax7AuE</a></td>
<td id="A6.T5.3.1.73.72.3" class="ltx_td ltx_align_center">Preserve the Wonder - Climate change action</td>
<td id="A6.T5.3.1.73.72.4" class="ltx_td ltx_align_center">PWCCA</td>
<td id="A6.T5.3.1.73.72.5" class="ltx_td ltx_align_center">26</td>
<td id="A6.T5.3.1.73.72.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.73.72.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.73.72.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.74.73" class="ltx_tr">
<td id="A6.T5.3.1.74.73.1" class="ltx_td ltx_align_center">73</td>
<td id="A6.T5.3.1.74.73.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=yTo3zmn3u84" title="" class="ltx_ref ltx_href">yTo3zmn3u84</a></td>
<td id="A6.T5.3.1.74.73.3" class="ltx_td ltx_align_center">Removing atmospheric greenhouse gases to prevent dangerous climate change</td>
<td id="A6.T5.3.1.74.73.4" class="ltx_td ltx_align_center">RAGG</td>
<td id="A6.T5.3.1.74.73.5" class="ltx_td ltx_align_center">37</td>
<td id="A6.T5.3.1.74.73.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.74.73.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.74.73.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.75.74" class="ltx_tr">
<td id="A6.T5.3.1.75.74.1" class="ltx_td ltx_align_center">74</td>
<td id="A6.T5.3.1.75.74.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=_P31w8E_5Zc" title="" class="ltx_ref ltx_href">_P31w8E_5Zc</a></td>
<td id="A6.T5.3.1.75.74.3" class="ltx_td ltx_align_center">Regenerative Agriculture: A Solution to Climate Change</td>
<td id="A6.T5.3.1.75.74.4" class="ltx_td ltx_align_center">RASCC</td>
<td id="A6.T5.3.1.75.74.5" class="ltx_td ltx_align_center">22</td>
<td id="A6.T5.3.1.75.74.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.75.74.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.75.74.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.76.75" class="ltx_tr">
<td id="A6.T5.3.1.76.75.1" class="ltx_td ltx_align_center">75</td>
<td id="A6.T5.3.1.76.75.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=Btqy4-H03dE" title="" class="ltx_ref ltx_href">Btqy4-H03dE</a></td>
<td id="A6.T5.3.1.76.75.3" class="ltx_td ltx_align_center">Ron Cohen on “BEACO2N” Climate Change Sensors</td>
<td id="A6.T5.3.1.76.75.4" class="ltx_td ltx_align_center">RCCCS</td>
<td id="A6.T5.3.1.76.75.5" class="ltx_td ltx_align_center">9</td>
<td id="A6.T5.3.1.76.75.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.76.75.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.76.75.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.77.76" class="ltx_tr">
<td id="A6.T5.3.1.77.76.1" class="ltx_td ltx_align_center">76</td>
<td id="A6.T5.3.1.77.76.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=fCo77PW2G6Y" title="" class="ltx_ref ltx_href">fCo77PW2G6Y</a></td>
<td id="A6.T5.3.1.77.76.3" class="ltx_td ltx_align_center">Reframing climate change science to include indigenous and local knowledge - Dr Tero Mustonen</td>
<td id="A6.T5.3.1.77.76.4" class="ltx_td ltx_align_center">RCCS</td>
<td id="A6.T5.3.1.77.76.5" class="ltx_td ltx_align_center">39</td>
<td id="A6.T5.3.1.77.76.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.77.76.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.77.76.7.1" class="ltx_text ltx_font_smallcaps">Neutral</span></td>
</tr>
<tr id="A6.T5.3.1.78.77" class="ltx_tr">
<td id="A6.T5.3.1.78.77.1" class="ltx_td ltx_align_center">77</td>
<td id="A6.T5.3.1.78.77.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=s3ViLeAG6_U" title="" class="ltx_ref ltx_href">s3ViLeAG6_U</a></td>
<td id="A6.T5.3.1.78.77.3" class="ltx_td ltx_align_center">Richard H Thaler on climate change</td>
<td id="A6.T5.3.1.78.77.4" class="ltx_td ltx_align_center">RHTCC</td>
<td id="A6.T5.3.1.78.77.5" class="ltx_td ltx_align_center">77</td>
<td id="A6.T5.3.1.78.77.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.78.77.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.78.77.7.1" class="ltx_text ltx_font_smallcaps">Neutral</span></td>
</tr>
<tr id="A6.T5.3.1.79.78" class="ltx_tr">
<td id="A6.T5.3.1.79.78.1" class="ltx_td ltx_align_center">78</td>
<td id="A6.T5.3.1.79.78.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=G6JfQwonm78" title="" class="ltx_ref ltx_href">G6JfQwonm78</a></td>
<td id="A6.T5.3.1.79.78.3" class="ltx_td ltx_align_center">Rare plant - Snakeshead Fritillary - defies climate change</td>
<td id="A6.T5.3.1.79.78.4" class="ltx_td ltx_align_center">RPDCC</td>
<td id="A6.T5.3.1.79.78.5" class="ltx_td ltx_align_center">30</td>
<td id="A6.T5.3.1.79.78.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.79.78.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.79.78.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.80.79" class="ltx_tr">
<td id="A6.T5.3.1.80.79.1" class="ltx_td ltx_align_center">79</td>
<td id="A6.T5.3.1.80.79.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=K4dpmfzEASo" title="" class="ltx_ref ltx_href">K4dpmfzEASo</a></td>
<td id="A6.T5.3.1.80.79.3" class="ltx_td ltx_align_center">Strengthening Africa’s pastoral food systems transformation in the face of climate change</td>
<td id="A6.T5.3.1.80.79.4" class="ltx_td ltx_align_center">SAPFS</td>
<td id="A6.T5.3.1.80.79.5" class="ltx_td ltx_align_center">52</td>
<td id="A6.T5.3.1.80.79.6" class="ltx_td ltx_align_center">Dev</td>
<td id="A6.T5.3.1.80.79.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.80.79.7.1" class="ltx_text ltx_font_smallcaps">Neutral</span></td>
</tr>
<tr id="A6.T5.3.1.81.80" class="ltx_tr">
<td id="A6.T5.3.1.81.80.1" class="ltx_td ltx_align_center">80</td>
<td id="A6.T5.3.1.81.80.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=N2l_AjZGVQo" title="" class="ltx_ref ltx_href">N2l_AjZGVQo</a></td>
<td id="A6.T5.3.1.81.80.3" class="ltx_td ltx_align_center">System change NOT Climate change: Can we leverage the digital age to get there? - D2̂S Agenda</td>
<td id="A6.T5.3.1.81.80.4" class="ltx_td ltx_align_center">SCCC</td>
<td id="A6.T5.3.1.81.80.5" class="ltx_td ltx_align_center">34</td>
<td id="A6.T5.3.1.81.80.6" class="ltx_td ltx_align_center">Test</td>
<td id="A6.T5.3.1.81.80.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.81.80.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.82.81" class="ltx_tr">
<td id="A6.T5.3.1.82.81.1" class="ltx_td ltx_align_center">81</td>
<td id="A6.T5.3.1.82.81.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=se-BRvZuu7k" title="" class="ltx_ref ltx_href">se-BRvZuu7k</a></td>
<td id="A6.T5.3.1.82.81.3" class="ltx_td ltx_align_center">Scientists drill deep in Antarctic ice for clues to climate change</td>
<td id="A6.T5.3.1.82.81.4" class="ltx_td ltx_align_center">SDDA</td>
<td id="A6.T5.3.1.82.81.5" class="ltx_td ltx_align_center">32</td>
<td id="A6.T5.3.1.82.81.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.82.81.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.82.81.7.1" class="ltx_text ltx_font_smallcaps">Neutral</span></td>
</tr>
<tr id="A6.T5.3.1.83.82" class="ltx_tr">
<td id="A6.T5.3.1.83.82.1" class="ltx_td ltx_align_center">82</td>
<td id="A6.T5.3.1.83.82.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=ZsbSI8UrPYA" title="" class="ltx_ref ltx_href">ZsbSI8UrPYA</a></td>
<td id="A6.T5.3.1.83.82.3" class="ltx_td ltx_align_center">Saint Lucia and Climate Change Adaptation (English)</td>
<td id="A6.T5.3.1.83.82.4" class="ltx_td ltx_align_center">SLCCA</td>
<td id="A6.T5.3.1.83.82.5" class="ltx_td ltx_align_center">16</td>
<td id="A6.T5.3.1.83.82.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.83.82.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.83.82.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.84.83" class="ltx_tr">
<td id="A6.T5.3.1.84.83.1" class="ltx_td ltx_align_center">83</td>
<td id="A6.T5.3.1.84.83.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=id4DZ0NiKk4" title="" class="ltx_ref ltx_href">id4DZ0NiKk4</a></td>
<td id="A6.T5.3.1.84.83.3" class="ltx_td ltx_align_center">Stanford Students Tackle Climate Change</td>
<td id="A6.T5.3.1.84.83.4" class="ltx_td ltx_align_center">SSTCC</td>
<td id="A6.T5.3.1.84.83.5" class="ltx_td ltx_align_center">24</td>
<td id="A6.T5.3.1.84.83.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.84.83.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.84.83.7.1" class="ltx_text ltx_font_smallcaps">Neutral</span></td>
</tr>
<tr id="A6.T5.3.1.85.84" class="ltx_tr">
<td id="A6.T5.3.1.85.84.1" class="ltx_td ltx_align_center">84</td>
<td id="A6.T5.3.1.85.84.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=pCraV8ahpYo" title="" class="ltx_ref ltx_href">pCraV8ahpYo</a></td>
<td id="A6.T5.3.1.85.84.3" class="ltx_td ltx_align_center">The connections between climate change and mental health</td>
<td id="A6.T5.3.1.85.84.4" class="ltx_td ltx_align_center">TCBCC</td>
<td id="A6.T5.3.1.85.84.5" class="ltx_td ltx_align_center">22</td>
<td id="A6.T5.3.1.85.84.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.85.84.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.85.84.7.1" class="ltx_text ltx_font_smallcaps">Neutral</span></td>
</tr>
<tr id="A6.T5.3.1.86.85" class="ltx_tr">
<td id="A6.T5.3.1.86.85.1" class="ltx_td ltx_align_center">85</td>
<td id="A6.T5.3.1.86.85.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=tqavP5lotNo" title="" class="ltx_ref ltx_href">tqavP5lotNo</a></td>
<td id="A6.T5.3.1.86.85.3" class="ltx_td ltx_align_center">Transforming our Economy to Combat Climate Change</td>
<td id="A6.T5.3.1.86.85.4" class="ltx_td ltx_align_center">TECCC</td>
<td id="A6.T5.3.1.86.85.5" class="ltx_td ltx_align_center">28</td>
<td id="A6.T5.3.1.86.85.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.86.85.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.86.85.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.87.86" class="ltx_tr">
<td id="A6.T5.3.1.87.86.1" class="ltx_td ltx_align_center">86</td>
<td id="A6.T5.3.1.87.86.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=L555lOp_0pQ" title="" class="ltx_ref ltx_href">L555lOp_0pQ</a></td>
<td id="A6.T5.3.1.87.86.3" class="ltx_td ltx_align_center">Trade, Investment, and Climate Change in Asia and the Pacific</td>
<td id="A6.T5.3.1.87.86.4" class="ltx_td ltx_align_center">TICC</td>
<td id="A6.T5.3.1.87.86.5" class="ltx_td ltx_align_center">31</td>
<td id="A6.T5.3.1.87.86.6" class="ltx_td ltx_align_center">Test</td>
<td id="A6.T5.3.1.87.86.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.87.86.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.88.87" class="ltx_tr">
<td id="A6.T5.3.1.88.87.1" class="ltx_td ltx_align_center">87</td>
<td id="A6.T5.3.1.88.87.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=xWYwSgvZh38" title="" class="ltx_ref ltx_href">xWYwSgvZh38</a></td>
<td id="A6.T5.3.1.88.87.3" class="ltx_td ltx_align_center">The inequalities of climate change - ICRC</td>
<td id="A6.T5.3.1.88.87.4" class="ltx_td ltx_align_center">TIOCC</td>
<td id="A6.T5.3.1.88.87.5" class="ltx_td ltx_align_center">13</td>
<td id="A6.T5.3.1.88.87.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.88.87.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.88.87.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.89.88" class="ltx_tr">
<td id="A6.T5.3.1.89.88.1" class="ltx_td ltx_align_center">88</td>
<td id="A6.T5.3.1.89.88.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=jsZ2_WFtlDU" title="" class="ltx_ref ltx_href">jsZ2_WFtlDU</a></td>
<td id="A6.T5.3.1.89.88.3" class="ltx_td ltx_align_center">Things in Your Home that are Linked to Climate Change</td>
<td id="A6.T5.3.1.89.88.4" class="ltx_td ltx_align_center">TIYH</td>
<td id="A6.T5.3.1.89.88.5" class="ltx_td ltx_align_center">22</td>
<td id="A6.T5.3.1.89.88.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.89.88.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.89.88.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.90.89" class="ltx_tr">
<td id="A6.T5.3.1.90.89.1" class="ltx_td ltx_align_center">89</td>
<td id="A6.T5.3.1.90.89.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=763lGy43spk" title="" class="ltx_ref ltx_href">763lGy43spk</a></td>
<td id="A6.T5.3.1.90.89.3" class="ltx_td ltx_align_center">The technology fighting climate change</td>
<td id="A6.T5.3.1.90.89.4" class="ltx_td ltx_align_center">TTFCC</td>
<td id="A6.T5.3.1.90.89.5" class="ltx_td ltx_align_center">42</td>
<td id="A6.T5.3.1.90.89.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.90.89.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.90.89.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.91.90" class="ltx_tr">
<td id="A6.T5.3.1.91.90.1" class="ltx_td ltx_align_center">90</td>
<td id="A6.T5.3.1.91.90.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=5KtGg-Lvxso" title="" class="ltx_ref ltx_href">5KtGg-Lvxso</a></td>
<td id="A6.T5.3.1.91.90.3" class="ltx_td ltx_align_center">To understand climate change, understand these three numbers.</td>
<td id="A6.T5.3.1.91.90.4" class="ltx_td ltx_align_center">TUCC</td>
<td id="A6.T5.3.1.91.90.5" class="ltx_td ltx_align_center">38</td>
<td id="A6.T5.3.1.91.90.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.91.90.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.91.90.7.1" class="ltx_text ltx_font_smallcaps">Neutral</span></td>
</tr>
<tr id="A6.T5.3.1.92.91" class="ltx_tr">
<td id="A6.T5.3.1.92.91.1" class="ltx_td ltx_align_center">91</td>
<td id="A6.T5.3.1.92.91.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=SDxmlvGiV9k" title="" class="ltx_ref ltx_href">SDxmlvGiV9k</a></td>
<td id="A6.T5.3.1.92.91.3" class="ltx_td ltx_align_center">UK Climate Change Risk Assessment 2017 - Urgent priorities for the UK</td>
<td id="A6.T5.3.1.92.91.4" class="ltx_td ltx_align_center">UKCC</td>
<td id="A6.T5.3.1.92.91.5" class="ltx_td ltx_align_center">31</td>
<td id="A6.T5.3.1.92.91.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.92.91.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.92.91.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.93.92" class="ltx_tr">
<td id="A6.T5.3.1.93.92.1" class="ltx_td ltx_align_center">92</td>
<td id="A6.T5.3.1.93.92.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=eIcWgCjTHWM" title="" class="ltx_ref ltx_href">eIcWgCjTHWM</a></td>
<td id="A6.T5.3.1.93.92.3" class="ltx_td ltx_align_center">Voices from Vanuatu: Climate Change Impacts and Human Mobility</td>
<td id="A6.T5.3.1.93.92.4" class="ltx_td ltx_align_center">VFVCC</td>
<td id="A6.T5.3.1.93.92.5" class="ltx_td ltx_align_center">67</td>
<td id="A6.T5.3.1.93.92.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.93.92.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.93.92.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.94.93" class="ltx_tr">
<td id="A6.T5.3.1.94.93.1" class="ltx_td ltx_align_center">93</td>
<td id="A6.T5.3.1.94.93.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=ii9mx391VVk" title="" class="ltx_ref ltx_href">ii9mx391VVk</a></td>
<td id="A6.T5.3.1.94.93.3" class="ltx_td ltx_align_center">View from the Pacific: ‘Climate change is real’</td>
<td id="A6.T5.3.1.94.93.4" class="ltx_td ltx_align_center">VPCC</td>
<td id="A6.T5.3.1.94.93.5" class="ltx_td ltx_align_center">17</td>
<td id="A6.T5.3.1.94.93.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.94.93.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.94.93.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.95.94" class="ltx_tr">
<td id="A6.T5.3.1.95.94.1" class="ltx_td ltx_align_center">94</td>
<td id="A6.T5.3.1.95.94.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=_IVDYaQDNCg" title="" class="ltx_ref ltx_href">_IVDYaQDNCg</a></td>
<td id="A6.T5.3.1.95.94.3" class="ltx_td ltx_align_center">Wildfires and Climate Change Attribution: It’s Complicated!</td>
<td id="A6.T5.3.1.95.94.4" class="ltx_td ltx_align_center">WCCA</td>
<td id="A6.T5.3.1.95.94.5" class="ltx_td ltx_align_center">25</td>
<td id="A6.T5.3.1.95.94.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.95.94.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.95.94.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.96.95" class="ltx_tr">
<td id="A6.T5.3.1.96.95.1" class="ltx_td ltx_align_center">95</td>
<td id="A6.T5.3.1.96.95.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=TM_6C9szLOI" title="" class="ltx_ref ltx_href">TM_6C9szLOI</a></td>
<td id="A6.T5.3.1.96.95.3" class="ltx_td ltx_align_center">Why focus on human security when working on climate change adaptation?</td>
<td id="A6.T5.3.1.96.95.4" class="ltx_td ltx_align_center">WFHSW</td>
<td id="A6.T5.3.1.96.95.5" class="ltx_td ltx_align_center">21</td>
<td id="A6.T5.3.1.96.95.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.96.95.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.96.95.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.97.96" class="ltx_tr">
<td id="A6.T5.3.1.97.96.1" class="ltx_td ltx_align_center">96</td>
<td id="A6.T5.3.1.97.96.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=MPiFBW0NnWY" title="" class="ltx_ref ltx_href">MPiFBW0NnWY</a></td>
<td id="A6.T5.3.1.97.96.3" class="ltx_td ltx_align_center">What is Climate Change?</td>
<td id="A6.T5.3.1.97.96.4" class="ltx_td ltx_align_center">WICC</td>
<td id="A6.T5.3.1.97.96.5" class="ltx_td ltx_align_center">30</td>
<td id="A6.T5.3.1.97.96.6" class="ltx_td ltx_align_center">Test</td>
<td id="A6.T5.3.1.97.96.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.97.96.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.98.97" class="ltx_tr">
<td id="A6.T5.3.1.98.97.1" class="ltx_td ltx_align_center">97</td>
<td id="A6.T5.3.1.98.97.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=mgBYo6eG80U" title="" class="ltx_ref ltx_href">mgBYo6eG80U</a></td>
<td id="A6.T5.3.1.98.97.3" class="ltx_td ltx_align_center">What is climate change? | Earth Hazards | meriSTEM</td>
<td id="A6.T5.3.1.98.97.4" class="ltx_td ltx_align_center">WICCE</td>
<td id="A6.T5.3.1.98.97.5" class="ltx_td ltx_align_center">32</td>
<td id="A6.T5.3.1.98.97.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.98.97.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.98.97.7.1" class="ltx_text ltx_font_smallcaps">Oppose</span></td>
</tr>
<tr id="A6.T5.3.1.99.98" class="ltx_tr">
<td id="A6.T5.3.1.99.98.1" class="ltx_td ltx_align_center">98</td>
<td id="A6.T5.3.1.99.98.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=iXvyExAzQ58" title="" class="ltx_ref ltx_href">iXvyExAzQ58</a></td>
<td id="A6.T5.3.1.99.98.3" class="ltx_td ltx_align_center">What is the Impact of Solar Energy and Solar Panels on Climate Change?</td>
<td id="A6.T5.3.1.99.98.4" class="ltx_td ltx_align_center">WISE</td>
<td id="A6.T5.3.1.99.98.5" class="ltx_td ltx_align_center">25</td>
<td id="A6.T5.3.1.99.98.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.99.98.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.99.98.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.100.99" class="ltx_tr">
<td id="A6.T5.3.1.100.99.1" class="ltx_td ltx_align_center">99</td>
<td id="A6.T5.3.1.100.99.2" class="ltx_td ltx_align_center"><a target="_blank" href="https://www.youtube.com/watch?v=iFmoMhVb6iw" title="" class="ltx_ref ltx_href">iFmoMhVb6iw</a></td>
<td id="A6.T5.3.1.100.99.3" class="ltx_td ltx_align_center">Cuomo: Walk the Talk on Climate Change</td>
<td id="A6.T5.3.1.100.99.4" class="ltx_td ltx_align_center">WTCC</td>
<td id="A6.T5.3.1.100.99.5" class="ltx_td ltx_align_center">29</td>
<td id="A6.T5.3.1.100.99.6" class="ltx_td ltx_align_center">Train</td>
<td id="A6.T5.3.1.100.99.7" class="ltx_td ltx_nopad_r ltx_align_right"><span id="A6.T5.3.1.100.99.7.1" class="ltx_text ltx_font_smallcaps">Support</span></td>
</tr>
<tr id="A6.T5.3.1.101.100" class="ltx_tr">
<td id="A6.T5.3.1.101.100.1" class="ltx_td ltx_align_center ltx_border_bb">100</td>
<td id="A6.T5.3.1.101.100.2" class="ltx_td ltx_align_center ltx_border_bb"><a target="_blank" href="https://www.youtube.com/watch?v=6ObqydfPGLI" title="" class="ltx_ref ltx_href">6ObqydfPGLI</a></td>
<td id="A6.T5.3.1.101.100.3" class="ltx_td ltx_align_center ltx_border_bb">Yale Professor Tony Leiserowitz Discusses American Perceptions of Climate Change</td>
<td id="A6.T5.3.1.101.100.4" class="ltx_td ltx_align_center ltx_border_bb">YPTL</td>
<td id="A6.T5.3.1.101.100.5" class="ltx_td ltx_align_center ltx_border_bb">82</td>
<td id="A6.T5.3.1.101.100.6" class="ltx_td ltx_align_center ltx_border_bb">Train</td>
<td id="A6.T5.3.1.101.100.7" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb"><span id="A6.T5.3.1.101.100.7.1" class="ltx_text ltx_font_smallcaps">Neutral</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>List of <math id="A6.T5.2.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="A6.T5.2.m1.1b"><mn id="A6.T5.2.m1.1.1" xref="A6.T5.2.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="A6.T5.2.m1.1c"><cn type="integer" id="A6.T5.2.m1.1.1.cmml" xref="A6.T5.2.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.T5.2.m1.1d">100</annotation></semantics></math> Youtube videos on the MultiClimate dataset.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.18344" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.18346" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.18346">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.18346" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.18347" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 23:10:25 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
