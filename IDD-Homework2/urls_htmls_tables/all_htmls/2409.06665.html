<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Data Collection-free Masked Video Modeling</title>
<!--Generated on Fri Sep  6 06:19:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Self-supervised Learning Masked Video Modeling Action Recognition Pseudo-motion Videos" lang="en" name="keywords"/>
<base href="/html/2409.06665v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S1" title="In Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S2" title="In Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S3" title="In Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Proposed Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S3.SS1" title="In 3 Proposed Method ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Overview of Our Self-supervised Framework</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S3.SS2" title="In 3 Proposed Method ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Pseudo Motion Generator (PMG)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S3.SS3" title="In 3 Proposed Method ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Combination of Our Framework with Synthetic Images</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4" title="In Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS1" title="In 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Ablation Studies</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS1.SSS1" title="In 4.1 Ablation Studies ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>The effect of image augmentations.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS1.SSS2" title="In 4.1 Ablation Studies ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>The combination of image augmentations.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS1.SSS3" title="In 4.1 Ablation Studies ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>The efficacy of video-level augmentations.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS2" title="In 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Transferability of Our Framework</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS2.SSS1" title="In 4.2 Transferability of Our Framework ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Transferability from other video datasets.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS2.SSS2" title="In 4.2 Transferability of Our Framework ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Transferability from real image datasets.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS2.SSS3" title="In 4.2 Transferability of Our Framework ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Transferability from synthetic images.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS3" title="In 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Effect of the Number of Epochs, Data, and Categories in Image Datasets for Pre-training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS4" title="In 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>PMG as Video Augmentation on Pre-training</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS4.SSS1" title="In 4.4 PMG as Video Augmentation on Pre-training ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span>Pre-training with both real videos and pseudo-motion videos.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS4.SSS2" title="In 4.4 PMG as Video Augmentation on Pre-training ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.2 </span>Can we combine image datasets with video datasets to train our framework?</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS5" title="In 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Comparison to Existing Methods</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS5.SSS1" title="In 4.5 Comparison to Existing Methods ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.1 </span>Comparison to methods using HMDB51, UCF101 and Diving48.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS5.SSS2" title="In 4.5 Comparison to Existing Methods ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.2 </span>Comparison on SynAPT benchmark.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS5.SSS3" title="In 4.5 Comparison to Existing Methods ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.3 </span>Results on K400.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS6" title="In 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Performance When the Number Data for Fine-tuning is Limited</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS7" title="In 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7 </span>What Does VideoMAE Learn from Pre-training with Videos?</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S5" title="In Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A1" title="In Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.A </span>Details on Video Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A2" title="In Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.B </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A3" title="In Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.C </span>Pseudo-code of Pseudo Motion Generator (PMG)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A4" title="In Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.D </span>Parameters of Image Augmentations in PMG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A5" title="In Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.E </span>Examples of Pseudo-motion Videos</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A6" title="In Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.F </span>Quantitative Results of Our Framework</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A7" title="In Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.G </span>Failure Cases</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A8" title="In Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">0.H </span>Linear Probing</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>
LY Corporation
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id1.1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{yuchi.ishikawa, masayoshi.kondo}@lycorp.co.jp</span></span></span> </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Keio University
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id2.1"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>aoki@elec.keio.ac.jp</span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">
Data Collection-free Masked Video Modeling 
<br class="ltx_break"/>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yuchi Ishikawa<span class="ltx_ERROR undefined" id="id1.1.id1">\orcidlink</span>0000-0002-9485-6840
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Masayoshi Kondo<span class="ltx_ERROR undefined" id="id2.1.id1">\orcidlink</span>0009-0003-6485-2480
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yoshimitsu Aoki<span class="ltx_ERROR undefined" id="id3.1.id1">\orcidlink</span>0000-0001-7361-0027
</span><span class="ltx_author_notes">22</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">Pre-training video transformers generally requires a large amount of data,
presenting significant challenges in terms of data collection costs
and concerns related to privacy, licensing, and inherent biases.
Synthesizing data is one of the promising ways to solve these issues,
yet pre-training solely on synthetic data has its own challenges.
In this paper, we introduce an effective self-supervised learning
framework for videos that leverages readily available and less costly
static images.
Specifically, we define the Pseudo Motion Generator (PMG) module
that recursively applies image transformations to generate pseudo-motion
videos from images.
These pseudo-motion videos are then leveraged in masked video modeling.
Our approach is applicable to synthetic images as well, thus entirely
freeing video pre-training from data collection costs and other concerns
in real data.
Through experiments in action recognition tasks, we demonstrate that
this framework allows effective learning of spatio-temporal features through
pseudo-motion videos,
significantly improving over existing methods which also use static images
and partially outperforming those using both real and synthetic videos.
These results uncover fragments
of what video transformers learn through masked video modeling.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Self-supervised Learning Masked Video Modeling Action Recognition Pseudo-motion Videos
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Pre-training video transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib10" title="">10</a>]</cite>
generally requires a large amount of labeled data.
Although self-supervised learning enables pre-training of video transformers
without labels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib19" title="">19</a>]</cite>,
it still demands substantial volumes of video data.
This highlights various issues related to real video data including the following:</p>
</div>
<div class="ltx_para" id="S1.p2">
<dl class="ltx_description" id="S1.I1">
<dt class="ltx_item" id="S1.I1.i1"></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">High cost of data collection.</span>
Video data, compared to audio, text, and images, is massive in size.
Therefore, downloading, storing, and pre-processing videos
is extremely costly.
Furthermore, the following issues related to licenses, privacy, and bias arise during data collection.</p>
</div>
</dd>
<dt class="ltx_item" id="S1.I1.i2"></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Copyright and license infringement.</span>
Video data may have been collected without permission,
potentially infringing on licenses and copyrights.
For example, some datasets are gathered from video-sharing sites
like YouTube, in which videos are licensed by default with a
Standard YouTube license<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.youtube.com/t/terms</span></span></span>,
which prohibits the download of content.</p>
</div>
</dd>
<dt class="ltx_item" id="S1.I1.i3"></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Privacy issues.</span>
Video data often contains Personally Identifiable Information (PII)
including faces, which raises significant privacy concerns.</p>
</div>
</dd>
<dt class="ltx_item" id="S1.I1.i4"></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i4.p1.1.1">Bias and ethical issues.</span>
Large-scale datasets may unintentionally include biases
leading to ethical issues related to
nationality, gender, age, and more <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib78" title="">78</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib72" title="">72</a>]</cite>,
which can impact the fairness and inclusiveness of model outcomes.
Some works have also reported that video recognition models
might have context and object biases,
failing to recognize actions accurately <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib42" title="">42</a>]</cite>.</p>
</div>
</dd>
<dt class="ltx_item" id="S1.I1.i5"></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i5.p1.1.1">Data access issues.</span>
Possibly due to the above issues,
some datasets like
IG-Curated/Uncurated dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib21" title="">21</a>]</cite>
and CREATE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib83" title="">83</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib45" title="">45</a>]</cite>
are only made available to certain research groups.
This limitation restricts other researchers from replicating
or further developing these works, thereby impeding scientific progress.</p>
</div>
</dd>
</dl>
</div>
<figure class="ltx_table" id="S1.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S1.T1.21.2.1" style="font-size:113%;">Table 1</span>: </span><span class="ltx_text" id="S1.T1.2.1" style="font-size:113%;">
<span class="ltx_text ltx_font_bold" id="S1.T1.2.1.2">Comparison of each data source and their issues when conducting pre-training.</span>
While real videos enhance model performance, they have concerns related to
collection cost, privacy and licenses.
Synthetic videos <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib84" title="">84</a>]</cite> and
pseudo motions by MoSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib30" title="">30</a>]</cite> partially resolve these issues, but they
rely on the CNN architecture and its inherent inductive bias,
thus failing to accurately train ViT.
Note that in VPN, additional real data is required for optimal performance, therefore
the asterisked issues (<span class="ltx_text" id="S1.T1.2.1.1" style="position:relative; bottom:1.0pt;"><math alttext="\ast" class="ltx_Math" display="inline" id="S1.T1.2.1.1.m1.1"><semantics id="S1.T1.2.1.1.m1.1b"><mo id="S1.T1.2.1.1.m1.1.1" mathsize="89%" xref="S1.T1.2.1.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S1.T1.2.1.1.m1.1c"><ci id="S1.T1.2.1.1.m1.1.1.cmml" xref="S1.T1.2.1.1.m1.1.1">∗</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.2.1.1.m1.1d">\ast</annotation><annotation encoding="application/x-llamapun" id="S1.T1.2.1.1.m1.1e">∗</annotation></semantics></math></span>) are not resolved.
Our proposed framework is free of these issues
by generating pseudo-motion videos from synthetic images.
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S1.T1.8">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.8.7.1">
<td class="ltx_td" id="S1.T1.8.7.1.1"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S1.T1.8.7.1.2">
<span class="ltx_text" id="S1.T1.8.7.1.2.1" style="font-size:80%;">Real Video </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.T1.8.7.1.2.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib64" title="">64</a><span class="ltx_text" id="S1.T1.8.7.1.2.3.2" style="font-size:80%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S1.T1.8.7.1.3">
<span class="ltx_text" id="S1.T1.8.7.1.3.1" style="font-size:80%;">VPN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.T1.8.7.1.3.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib33" title="">33</a><span class="ltx_text" id="S1.T1.8.7.1.3.3.2" style="font-size:80%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S1.T1.8.7.1.4">
<span class="ltx_text" id="S1.T1.8.7.1.4.1" style="font-size:80%;">MoSI </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.T1.8.7.1.4.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib30" title="">30</a><span class="ltx_text" id="S1.T1.8.7.1.4.3.2" style="font-size:80%;">]</span></cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S1.T1.8.7.1.5"><span class="ltx_text" id="S1.T1.8.7.1.5.1" style="font-size:80%;">Ours</span></th>
</tr>
<tr class="ltx_tr" id="S1.T1.6.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T1.6.4.5"><span class="ltx_text" id="S1.T1.6.4.5.1" style="font-size:80%;">example</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T1.3.1.1"><span class="ltx_text" id="S1.T1.3.1.1.1" style="font-size:80%;position:relative; bottom:-11.4pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="66" id="S1.T1.3.1.1.1.g1" src="x1.png" width="166"/></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T1.4.2.2"><span class="ltx_text" id="S1.T1.4.2.2.1" style="font-size:80%;position:relative; bottom:-11.4pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="66" id="S1.T1.4.2.2.1.g1" src="x2.png" width="166"/></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T1.5.3.3"><span class="ltx_text" id="S1.T1.5.3.3.1" style="font-size:80%;position:relative; bottom:-11.4pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="66" id="S1.T1.5.3.3.1.g1" src="x3.png" width="166"/></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T1.6.4.4"><span class="ltx_text" id="S1.T1.6.4.4.1" style="font-size:80%;position:relative; bottom:-11.4pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="66" id="S1.T1.6.4.4.1.g1" src="x4.png" width="166"/></span></th>
</tr>
<tr class="ltx_tr" id="S1.T1.8.8.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.8.8.2.1"><span class="ltx_text" id="S1.T1.8.8.2.1.1" style="font-size:80%;">Acc. @UCF101</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.8.8.2.2"><span class="ltx_text" id="S1.T1.8.8.2.2.1" style="font-size:80%;">96.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.8.8.2.3"><span class="ltx_text" id="S1.T1.8.8.2.3.1" style="font-size:80%;">89.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.8.8.2.4"><span class="ltx_text" id="S1.T1.8.8.2.4.1" style="font-size:80%;">82.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.8.8.2.5"><span class="ltx_text" id="S1.T1.8.8.2.5.1" style="font-size:80%;">89.4</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.7.5">
<td class="ltx_td ltx_align_center" id="S1.T1.7.5.2"><span class="ltx_text" id="S1.T1.7.5.2.1" style="font-size:80%;">collection cost</span></td>
<td class="ltx_td" id="S1.T1.7.5.3"></td>
<td class="ltx_td ltx_align_center" id="S1.T1.7.5.1"><math alttext="\checkmark^{\ast}" class="ltx_Math" display="inline" id="S1.T1.7.5.1.m1.1"><semantics id="S1.T1.7.5.1.m1.1a"><msup id="S1.T1.7.5.1.m1.1.1" xref="S1.T1.7.5.1.m1.1.1.cmml"><mi id="S1.T1.7.5.1.m1.1.1.2" mathsize="80%" mathvariant="normal" xref="S1.T1.7.5.1.m1.1.1.2.cmml">✓</mi><mo id="S1.T1.7.5.1.m1.1.1.3" mathsize="80%" xref="S1.T1.7.5.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S1.T1.7.5.1.m1.1b"><apply id="S1.T1.7.5.1.m1.1.1.cmml" xref="S1.T1.7.5.1.m1.1.1"><csymbol cd="ambiguous" id="S1.T1.7.5.1.m1.1.1.1.cmml" xref="S1.T1.7.5.1.m1.1.1">superscript</csymbol><ci id="S1.T1.7.5.1.m1.1.1.2.cmml" xref="S1.T1.7.5.1.m1.1.1.2">✓</ci><ci id="S1.T1.7.5.1.m1.1.1.3.cmml" xref="S1.T1.7.5.1.m1.1.1.3">∗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.7.5.1.m1.1c">\checkmark^{\ast}</annotation><annotation encoding="application/x-llamapun" id="S1.T1.7.5.1.m1.1d">✓ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S1.T1.7.5.4"><span class="ltx_text" id="S1.T1.7.5.4.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.7.5.5"><span class="ltx_text" id="S1.T1.7.5.5.1" style="font-size:80%;">✓</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.8.6">
<td class="ltx_td ltx_align_center" id="S1.T1.8.6.2"><span class="ltx_text" id="S1.T1.8.6.2.1" style="font-size:80%;">privacy/license</span></td>
<td class="ltx_td" id="S1.T1.8.6.3"></td>
<td class="ltx_td ltx_align_center" id="S1.T1.8.6.1"><math alttext="\checkmark^{\ast}" class="ltx_Math" display="inline" id="S1.T1.8.6.1.m1.1"><semantics id="S1.T1.8.6.1.m1.1a"><msup id="S1.T1.8.6.1.m1.1.1" xref="S1.T1.8.6.1.m1.1.1.cmml"><mi id="S1.T1.8.6.1.m1.1.1.2" mathsize="80%" mathvariant="normal" xref="S1.T1.8.6.1.m1.1.1.2.cmml">✓</mi><mo id="S1.T1.8.6.1.m1.1.1.3" mathsize="80%" xref="S1.T1.8.6.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S1.T1.8.6.1.m1.1b"><apply id="S1.T1.8.6.1.m1.1.1.cmml" xref="S1.T1.8.6.1.m1.1.1"><csymbol cd="ambiguous" id="S1.T1.8.6.1.m1.1.1.1.cmml" xref="S1.T1.8.6.1.m1.1.1">superscript</csymbol><ci id="S1.T1.8.6.1.m1.1.1.2.cmml" xref="S1.T1.8.6.1.m1.1.1.2">✓</ci><ci id="S1.T1.8.6.1.m1.1.1.3.cmml" xref="S1.T1.8.6.1.m1.1.1.3">∗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.8.6.1.m1.1c">\checkmark^{\ast}</annotation><annotation encoding="application/x-llamapun" id="S1.T1.8.6.1.m1.1d">✓ start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S1.T1.8.6.4"><span class="ltx_text" id="S1.T1.8.6.4.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.8.6.5"><span class="ltx_text" id="S1.T1.8.6.5.1" style="font-size:80%;">✓</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.8.9.3">
<td class="ltx_td ltx_align_center ltx_border_b" id="S1.T1.8.9.3.1"><span class="ltx_text" id="S1.T1.8.9.3.1.1" style="font-size:80%;">training ViT</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S1.T1.8.9.3.2"><span class="ltx_text" id="S1.T1.8.9.3.2.1" style="font-size:80%;">✓</span></td>
<td class="ltx_td ltx_border_b" id="S1.T1.8.9.3.3"></td>
<td class="ltx_td ltx_border_b" id="S1.T1.8.9.3.4"></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S1.T1.8.9.3.5"><span class="ltx_text" id="S1.T1.8.9.3.5.1" style="font-size:80%;">✓</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In image recognition, to address these concerns
and eliminate the costs associated with data collection,
some researchers have proposed pre-training methods
using synthetic images as an alternative to those using real images.
While some works have synthesized images
from mathematical formulas <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib50" title="">50</a>]</cite>,
others have utilized structured noise <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib8" title="">8</a>]</cite> or
OpenGL fragment shaders <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib7" title="">7</a>]</cite>.
These methods have achieved comparable results
to pre-training on real image datasets
like ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib17" title="">17</a>]</cite> and JFT-300M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib59" title="">59</a>]</cite>,
emphasizing the importance of data diversity.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">However,
pre-training using synthetic videos still presents significant challenges.
Few works address this, including the Video Perlin Noise (VPN) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib33" title="">33</a>]</cite>
generated from Perlin Noise <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib54" title="">54</a>]</cite>,
and SynAPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib84" title="">84</a>]</cite>.
However,
they still require real video datasets such as Kinetics400 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib36" title="">36</a>]</cite>.
This diverges from our goal of reducing data collection costs
and minimizing issues related to real data.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">An alternative approach involves generating pseudo-motions from static images.
Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib30" title="">30</a>]</cite> proposed a self-supervised learning
framework named Unmasked MoSI,
designed to make models learn spatio-temporal features
through the classification of pseudo motions.
This can be promising
because it only requires static images
and can be combined with datasets
with protected
privacy and liberal licenses, like PASS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib4" title="">4</a>]</cite>.
However, this method is specialized on CNN architectures
and cannot generalize to transformer-based architectures,
which are the current state-of-the-art models.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In this paper, to mitigate video collection costs
and address concerns regarding privacy, bias, and licenses,
we propose a self-supervised learning framework for video transformers using synthetic images
(Table <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">1</span></a>).
Our framework includes a Pseudo Motion Generator (PMG) module
that recursively applies image transformations to static images,
generating videos with diverse pseudo-motion.
These videos are then used for masked video modeling.
Through experiments, by using videos generated from the PMG module, we examine that video
transformers can learn transferable and robust video features which are not limited to a single domain.
To the best of our knowledge,
we are the first to pre-train video transformers
exclusively using synthetic images.
Our contributions are threefold;</p>
</div>
<div class="ltx_para" id="S1.p7">
<ol class="ltx_enumerate" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p" id="S1.I2.i1.p1.1">We introduce
a self-supervised learning framework for videos
that uses single images to reduce data collection costs compared to videos.
Our framework includes a Pseudo Motion Generator (PMG) module,
which generates a wide variety of pseudo-motion videos.
These pseudo-motion videos are utilized for self-supervised masked video modeling.
Notably, PMG can also be used for video augmentation when pre-training with real videos.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p" id="S1.I2.i2.p1.1">We demonstrate that synthetic images can be used for our framework to
still effectively pre-train video transformers,
completely eliminating the need for real videos or images.
This mitigates privacy, bias, and licensing concerns.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I2.i3.p1">
<p class="ltx_p" id="S1.I2.i3.p1.1">Through experiments in action recognition tasks,
we demonstrate that our proposed framework significantly improves over existing works using static images,
and also partially surpasses existing pre-training methods
using both real and synthetic videos.
These experimental findings reveal pieces of
what video transformers learn through masked video modeling.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Self-supervised Learning for Videos.</span>
Videos require significantly more effort than images and text
for annotation.
Therefore, more interest is invested in self-supervised
learning methods which do not require labeled data.
While earlier works leverage pretext tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib79" title="">79</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib67" title="">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib76" title="">76</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib37" title="">37</a>]</cite>,
recent advancements have introduced
contrastive learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib51" title="">51</a>]</cite>
and masked video modeling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib74" title="">74</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib77" title="">77</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib68" title="">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib71" title="">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib60" title="">60</a>]</cite>,
which offers more robust representation learning without explicit labeling.
Notably, VideoMAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib64" title="">64</a>]</cite>
has emerged as a leading method due to its simplicity and efficacy,
learning video representations by simply reconstructing masked regions.
Some works, however, point out that VideoMAE predominantly
learns low-level features
such as shapes.
This tendency may limit its ability to capture
high-level semantic features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib56" title="">56</a>]</cite>.
Nonetheless, the emphasis on low-level features suggests that
VideoMAE does not specialize in domain-specific features,
leading to its high transferability across various domains.
We aim to capitalize on this characteristic
to train video transformers with static images.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Large-scale Datasets in Computer Vision. </span>
Though self-supervised learning eliminates the need for annotation,
it still demands large volumes of data.
The growth of computer vision has relied on massive datasets
like ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib17" title="">17</a>]</cite>
and LAION-5B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib55" title="">55</a>]</cite>.
However, these resources are fraught with privacy, bias, and licensing issues.
Furthermore, access to datasets like JFT-300M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib59" title="">59</a>]</cite>,
Instagram-3.5B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib46" title="">46</a>]</cite>,
IG-Curated/Uncurated <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib21" title="">21</a>]</cite>,
and CREATE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib83" title="">83</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib45" title="">45</a>]</cite>
is restricted to certain research groups.
These issues underscore the urgent need for accessible data sources
which are free from bias and privacy violation.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Video data exacerbates these challenges with its higher collection costs,
privacy risks and biases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib42" title="">42</a>]</cite>.
Some popular datasets such as Kinetics400 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib36" title="">36</a>]</cite>,
HowTo100M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib47" title="">47</a>]</cite>, YouTube-8M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib1" title="">1</a>]</cite>,
and ActivityNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib12" title="">12</a>]</cite>,
are collected from YouTube and may encounter copyright and license restrictions.
On the other hand, our self-supervised framework
requires only synthetic images which are free from these challenges.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p" id="S2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.p4.1.1">Learning from Synthetic Data. </span>
In response to these challenges,
there is a growing interest in synthetic data,
which bypasses many of the issues existent when using real-world data.
Some research has focused on synthesizing realistic data
 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib75" title="">75</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib48" title="">48</a>]</cite>,
while others have proposed systematic synthesis of data
from noise <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib8" title="">8</a>]</cite> or mathematical formulas
 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib49" title="">49</a>]</cite>.
These works have proven that not only realism
but also diversity in synthetic data is crucial for effectively
training models.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">Few attempts are made to train action recognition models using synthetic data.
For example, the GATA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib26" title="">26</a>]</cite>,
collected from a video game,
is proposed for human motion representation learning.
However, this dataset is not allowed for commercial use,
and the rights of game companies have not been considered.
Another example is the Video Perlin Noise (VPN)
dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib33" title="">33</a>]</cite>,
which is generated from Perlin Noise <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib54" title="">54</a>]</cite>.
This dataset is proposed to initialize model weights before pre-training.
Zhong et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib84" title="">84</a>]</cite> propose a pre-training method
with both No-Human Kinetics (NH-Kinetics) and SynAPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib38" title="">38</a>]</cite>.
While these approaches contribute to model performance,
they still require pre-training on real videos.
Additionally,
ElderSim <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib31" title="">31</a>]</cite>,
PHAV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib16" title="">16</a>]</cite>,
and SURREAL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib66" title="">66</a>]</cite>,
which are included in SynAPT,
are not allowed for commercial use.
As an alternative,
Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib30" title="">30</a>]</cite> have proposed MoSI,
which pre-trains models with pseudo-motion videos generated from static images.
In terms of collection cost, requiring only static images for pre-training is favorable.
However, because MoSI’s synthesized videos lack diversity,
they fail to pre-train video
transformers (See <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.T10" title="In 4.4.1 Pre-training with both real videos and pseudo-motion videos. ‣ 4.4 PMG as Video Augmentation on Pre-training ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">10</span></a>).</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">Overall, existing works have shown the capability
to pre-train video recognition models using synthetic or pseudo-motion videos.
However, they either specialize on CNN architectures or still require the
use of real video data.
In contrast, our method generates a diversity of pseudo-motion videos
from synthetic images,
which can effectively pre-train video transformers.
Moreover, our approach is completely agnostic of the issues
associated with video data collection, privacy, and bias.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Method</h2>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="265" id="S3.F1.g1" src="x5.png" width="747"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.4.1.1" style="font-size:113%;">Figure 1</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F1.5.2" style="font-size:113%;">Overview of our proposed framework.</span></figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overview of Our Self-supervised Framework</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To reduce the collection cost of video data,
we propose
a self-supervised framework using pseudo-motion videos generated from static images.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S3.F1" title="Figure 1 ‣ 3 Proposed Method ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">1</span></a> shows the overview of our framework.
We first generate pseudo-motion videos from static images by Pseudo Motion Generator (PMG).
Then, we utilize these videos to train VideoMAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib19" title="">19</a>]</cite>.
VideoMAE is a powerful self-supervised learning framework
and can learn spatio-temporal features effectively
by reconstructing masked video regions from their complementaries.
Some works point out that VideoMAE has a tendency
to learn low-level features such as edges,
thus failing to achieve high-level alignment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib56" title="">56</a>]</cite>.
Conversely, VideoMAE does not obtain domain-specific features,
leading to high transferability.
We focus on and leverage this characteristic to train video transformers with pseudo-motion videos.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Pseudo Motion Generator (PMG)</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.6">As mentioned, VideoMAE learns low-level features such as edges in a video.
Especially, we hypothesize that it focuses on the correspondence of patches
between frames.
Therefore, we assume that to train VideoMAE effectively,
patches between frames in videos should be trackable.
To generate such a pseudo-motion video <math alttext="V\in\mathbb{R}^{C\times T\times H\times W}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">V</mi><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.3.2" xref="S3.SS2.p1.1.m1.1.1.3.3.2.cmml">C</mi><mo id="S3.SS2.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p1.1.m1.1.1.3.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.3.cmml">T</mi><mo id="S3.SS2.p1.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p1.1.m1.1.1.3.3.4" xref="S3.SS2.p1.1.m1.1.1.3.3.4.cmml">H</mi><mo id="S3.SS2.p1.1.m1.1.1.3.3.1b" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p1.1.m1.1.1.3.3.5" xref="S3.SS2.p1.1.m1.1.1.3.3.5.cmml">W</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><in id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></in><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝑉</ci><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3"><times id="S3.SS2.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS2.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.2">𝐶</ci><ci id="S3.SS2.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.3">𝑇</ci><ci id="S3.SS2.p1.1.m1.1.1.3.3.4.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.4">𝐻</ci><ci id="S3.SS2.p1.1.m1.1.1.3.3.5.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3.5">𝑊</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">V\in\mathbb{R}^{C\times T\times H\times W}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_V ∈ blackboard_R start_POSTSUPERSCRIPT italic_C × italic_T × italic_H × italic_W end_POSTSUPERSCRIPT</annotation></semantics></math>
<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>
<math alttext="T" class="ltx_Math" display="inline" id="footnote2.m1.1"><semantics id="footnote2.m1.1b"><mi id="footnote2.m1.1.1" xref="footnote2.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="footnote2.m1.1c"><ci id="footnote2.m1.1.1.cmml" xref="footnote2.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m1.1d">T</annotation><annotation encoding="application/x-llamapun" id="footnote2.m1.1e">italic_T</annotation></semantics></math> is the number of frames in a video.
<math alttext="H" class="ltx_Math" display="inline" id="footnote2.m2.1"><semantics id="footnote2.m2.1b"><mi id="footnote2.m2.1.1" xref="footnote2.m2.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="footnote2.m2.1c"><ci id="footnote2.m2.1.1.cmml" xref="footnote2.m2.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m2.1d">H</annotation><annotation encoding="application/x-llamapun" id="footnote2.m2.1e">italic_H</annotation></semantics></math> and <math alttext="W" class="ltx_Math" display="inline" id="footnote2.m3.1"><semantics id="footnote2.m3.1b"><mi id="footnote2.m3.1.1" xref="footnote2.m3.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="footnote2.m3.1c"><ci id="footnote2.m3.1.1.cmml" xref="footnote2.m3.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m3.1d">W</annotation><annotation encoding="application/x-llamapun" id="footnote2.m3.1e">italic_W</annotation></semantics></math> are the width and the height of each video frame.
<math alttext="C" class="ltx_Math" display="inline" id="footnote2.m4.1"><semantics id="footnote2.m4.1b"><mi id="footnote2.m4.1.1" xref="footnote2.m4.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="footnote2.m4.1c"><ci id="footnote2.m4.1.1.cmml" xref="footnote2.m4.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m4.1d">C</annotation><annotation encoding="application/x-llamapun" id="footnote2.m4.1e">italic_C</annotation></semantics></math> is the number of channels.
</span></span></span>,
we propose a simple module, namely Pseudo Motion Generator (PMG).
The algorithm of PMG is as follows:
First, PMG randomly selects an image transformation <math alttext="f" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">f</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_f</annotation></semantics></math>
from a predefined set <math alttext="\phi" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">italic_ϕ</annotation></semantics></math>
and determines its intensity parameter <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">italic_θ</annotation></semantics></math>.
Then, PMG takes as input a static image <math alttext="I_{1}\in\mathbb{R}^{C\times H\times W}" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5.1"><semantics id="S3.SS2.p1.5.m5.1a"><mrow id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><msub id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml"><mi id="S3.SS2.p1.5.m5.1.1.2.2" xref="S3.SS2.p1.5.m5.1.1.2.2.cmml">I</mi><mn id="S3.SS2.p1.5.m5.1.1.2.3" xref="S3.SS2.p1.5.m5.1.1.2.3.cmml">1</mn></msub><mo id="S3.SS2.p1.5.m5.1.1.1" xref="S3.SS2.p1.5.m5.1.1.1.cmml">∈</mo><msup id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml"><mi id="S3.SS2.p1.5.m5.1.1.3.2" xref="S3.SS2.p1.5.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p1.5.m5.1.1.3.3" xref="S3.SS2.p1.5.m5.1.1.3.3.cmml"><mi id="S3.SS2.p1.5.m5.1.1.3.3.2" xref="S3.SS2.p1.5.m5.1.1.3.3.2.cmml">C</mi><mo id="S3.SS2.p1.5.m5.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.5.m5.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p1.5.m5.1.1.3.3.3" xref="S3.SS2.p1.5.m5.1.1.3.3.3.cmml">H</mi><mo id="S3.SS2.p1.5.m5.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p1.5.m5.1.1.3.3.1.cmml">×</mo><mi id="S3.SS2.p1.5.m5.1.1.3.3.4" xref="S3.SS2.p1.5.m5.1.1.3.3.4.cmml">W</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><in id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1.1"></in><apply id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.2.1.cmml" xref="S3.SS2.p1.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.5.m5.1.1.2.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2.2">𝐼</ci><cn id="S3.SS2.p1.5.m5.1.1.2.3.cmml" type="integer" xref="S3.SS2.p1.5.m5.1.1.2.3">1</cn></apply><apply id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.3.1.cmml" xref="S3.SS2.p1.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.5.m5.1.1.3.2.cmml" xref="S3.SS2.p1.5.m5.1.1.3.2">ℝ</ci><apply id="S3.SS2.p1.5.m5.1.1.3.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3.3"><times id="S3.SS2.p1.5.m5.1.1.3.3.1.cmml" xref="S3.SS2.p1.5.m5.1.1.3.3.1"></times><ci id="S3.SS2.p1.5.m5.1.1.3.3.2.cmml" xref="S3.SS2.p1.5.m5.1.1.3.3.2">𝐶</ci><ci id="S3.SS2.p1.5.m5.1.1.3.3.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3.3.3">𝐻</ci><ci id="S3.SS2.p1.5.m5.1.1.3.3.4.cmml" xref="S3.SS2.p1.5.m5.1.1.3.3.4">𝑊</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">I_{1}\in\mathbb{R}^{C\times H\times W}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.5.m5.1d">italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_C × italic_H × italic_W end_POSTSUPERSCRIPT</annotation></semantics></math>
and recursively applies image transformation to <math alttext="I_{1}" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6.1"><semantics id="S3.SS2.p1.6.m6.1a"><msub id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><mi id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml">I</mi><mn id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2">𝐼</ci><cn id="S3.SS2.p1.6.m6.1.1.3.cmml" type="integer" xref="S3.SS2.p1.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">I_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.6.m6.1d">italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="I_{i+1}=f_{\theta}(I_{i})\quad\text{for}\quad i=1,...,T-1" class="ltx_Math" display="block" id="S3.E1.m1.5"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5.2" xref="S3.E1.m1.5.5.3.cmml"><mrow id="S3.E1.m1.4.4.1.1" xref="S3.E1.m1.4.4.1.1.cmml"><msub id="S3.E1.m1.4.4.1.1.3" xref="S3.E1.m1.4.4.1.1.3.cmml"><mi id="S3.E1.m1.4.4.1.1.3.2" xref="S3.E1.m1.4.4.1.1.3.2.cmml">I</mi><mrow id="S3.E1.m1.4.4.1.1.3.3" xref="S3.E1.m1.4.4.1.1.3.3.cmml"><mi id="S3.E1.m1.4.4.1.1.3.3.2" xref="S3.E1.m1.4.4.1.1.3.3.2.cmml">i</mi><mo id="S3.E1.m1.4.4.1.1.3.3.1" xref="S3.E1.m1.4.4.1.1.3.3.1.cmml">+</mo><mn id="S3.E1.m1.4.4.1.1.3.3.3" xref="S3.E1.m1.4.4.1.1.3.3.3.cmml">1</mn></mrow></msub><mo id="S3.E1.m1.4.4.1.1.2" xref="S3.E1.m1.4.4.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.4.4.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.2.cmml"><mrow id="S3.E1.m1.4.4.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.cmml"><msub id="S3.E1.m1.4.4.1.1.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.1.3.2" xref="S3.E1.m1.4.4.1.1.1.1.1.3.2.cmml">f</mi><mi id="S3.E1.m1.4.4.1.1.1.1.1.3.3" xref="S3.E1.m1.4.4.1.1.1.1.1.3.3.cmml">θ</mi></msub><mo id="S3.E1.m1.4.4.1.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.4.4.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.2.cmml">I</mi><mi id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.4.4.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mspace id="S3.E1.m1.4.4.1.1.1.1.2" width="1em" xref="S3.E1.m1.4.4.1.1.1.2.cmml"></mspace><mtext id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3a.cmml">for</mtext></mrow></mrow><mspace id="S3.E1.m1.5.5.2.3" width="1em" xref="S3.E1.m1.5.5.3a.cmml"></mspace><mrow id="S3.E1.m1.5.5.2.2" xref="S3.E1.m1.5.5.2.2.cmml"><mi id="S3.E1.m1.5.5.2.2.3" xref="S3.E1.m1.5.5.2.2.3.cmml">i</mi><mo id="S3.E1.m1.5.5.2.2.2" xref="S3.E1.m1.5.5.2.2.2.cmml">=</mo><mrow id="S3.E1.m1.5.5.2.2.1.1" xref="S3.E1.m1.5.5.2.2.1.2.cmml"><mn id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">1</mn><mo id="S3.E1.m1.5.5.2.2.1.1.2" xref="S3.E1.m1.5.5.2.2.1.2.cmml">,</mo><mi id="S3.E1.m1.2.2" mathvariant="normal" xref="S3.E1.m1.2.2.cmml">…</mi><mo id="S3.E1.m1.5.5.2.2.1.1.3" xref="S3.E1.m1.5.5.2.2.1.2.cmml">,</mo><mrow id="S3.E1.m1.5.5.2.2.1.1.1" xref="S3.E1.m1.5.5.2.2.1.1.1.cmml"><mi id="S3.E1.m1.5.5.2.2.1.1.1.2" xref="S3.E1.m1.5.5.2.2.1.1.1.2.cmml">T</mi><mo id="S3.E1.m1.5.5.2.2.1.1.1.1" xref="S3.E1.m1.5.5.2.2.1.1.1.1.cmml">−</mo><mn id="S3.E1.m1.5.5.2.2.1.1.1.3" xref="S3.E1.m1.5.5.2.2.1.1.1.3.cmml">1</mn></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.3.cmml" xref="S3.E1.m1.5.5.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.3a.cmml" xref="S3.E1.m1.5.5.2.3">formulae-sequence</csymbol><apply id="S3.E1.m1.4.4.1.1.cmml" xref="S3.E1.m1.4.4.1.1"><eq id="S3.E1.m1.4.4.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.2"></eq><apply id="S3.E1.m1.4.4.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.3.1.cmml" xref="S3.E1.m1.4.4.1.1.3">subscript</csymbol><ci id="S3.E1.m1.4.4.1.1.3.2.cmml" xref="S3.E1.m1.4.4.1.1.3.2">𝐼</ci><apply id="S3.E1.m1.4.4.1.1.3.3.cmml" xref="S3.E1.m1.4.4.1.1.3.3"><plus id="S3.E1.m1.4.4.1.1.3.3.1.cmml" xref="S3.E1.m1.4.4.1.1.3.3.1"></plus><ci id="S3.E1.m1.4.4.1.1.3.3.2.cmml" xref="S3.E1.m1.4.4.1.1.3.3.2">𝑖</ci><cn id="S3.E1.m1.4.4.1.1.3.3.3.cmml" type="integer" xref="S3.E1.m1.4.4.1.1.3.3.3">1</cn></apply></apply><list id="S3.E1.m1.4.4.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1"><apply id="S3.E1.m1.4.4.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1"><times id="S3.E1.m1.4.4.1.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2"></times><apply id="S3.E1.m1.4.4.1.1.1.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.4.4.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.3.2">𝑓</ci><ci id="S3.E1.m1.4.4.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.3.3">𝜃</ci></apply><apply id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.2">𝐼</ci><ci id="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><ci id="S3.E1.m1.3.3a.cmml" xref="S3.E1.m1.3.3"><mtext id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">for</mtext></ci></list></apply><apply id="S3.E1.m1.5.5.2.2.cmml" xref="S3.E1.m1.5.5.2.2"><eq id="S3.E1.m1.5.5.2.2.2.cmml" xref="S3.E1.m1.5.5.2.2.2"></eq><ci id="S3.E1.m1.5.5.2.2.3.cmml" xref="S3.E1.m1.5.5.2.2.3">𝑖</ci><list id="S3.E1.m1.5.5.2.2.1.2.cmml" xref="S3.E1.m1.5.5.2.2.1.1"><cn id="S3.E1.m1.1.1.cmml" type="integer" xref="S3.E1.m1.1.1">1</cn><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">…</ci><apply id="S3.E1.m1.5.5.2.2.1.1.1.cmml" xref="S3.E1.m1.5.5.2.2.1.1.1"><minus id="S3.E1.m1.5.5.2.2.1.1.1.1.cmml" xref="S3.E1.m1.5.5.2.2.1.1.1.1"></minus><ci id="S3.E1.m1.5.5.2.2.1.1.1.2.cmml" xref="S3.E1.m1.5.5.2.2.1.1.1.2">𝑇</ci><cn id="S3.E1.m1.5.5.2.2.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.5.5.2.2.1.1.1.3">1</cn></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">I_{i+1}=f_{\theta}(I_{i})\quad\text{for}\quad i=1,...,T-1</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.5d">italic_I start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) for italic_i = 1 , … , italic_T - 1</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.3">Finally, by concatenating the images from <math alttext="I_{1}" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><msub id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">I</mi><mn id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝐼</ci><cn id="S3.SS2.p3.1.m1.1.1.3.cmml" type="integer" xref="S3.SS2.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">I_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> to <math alttext="I_{T}" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><msub id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">I</mi><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">𝐼</ci><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">I_{T}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">italic_I start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math>
in the temporal dimension, a pseudo-motion video <math alttext="V" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.1"><semantics id="S3.SS2.p3.3.m3.1a"><mi id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><ci id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.1d">italic_V</annotation></semantics></math> is generated.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="V=[I_{1};I_{2};...;I_{T}]" class="ltx_Math" display="block" id="S3.E2.m1.4"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml"><mi id="S3.E2.m1.4.4.5" xref="S3.E2.m1.4.4.5.cmml">V</mi><mo id="S3.E2.m1.4.4.4" xref="S3.E2.m1.4.4.4.cmml">=</mo><mrow id="S3.E2.m1.4.4.3.3" xref="S3.E2.m1.4.4.3.4.cmml"><mo id="S3.E2.m1.4.4.3.3.4" stretchy="false" xref="S3.E2.m1.4.4.3.4.cmml">[</mo><msub id="S3.E2.m1.2.2.1.1.1" xref="S3.E2.m1.2.2.1.1.1.cmml"><mi id="S3.E2.m1.2.2.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.2.cmml">I</mi><mn id="S3.E2.m1.2.2.1.1.1.3" xref="S3.E2.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.E2.m1.4.4.3.3.5" xref="S3.E2.m1.4.4.3.4.cmml">;</mo><msub id="S3.E2.m1.3.3.2.2.2" xref="S3.E2.m1.3.3.2.2.2.cmml"><mi id="S3.E2.m1.3.3.2.2.2.2" xref="S3.E2.m1.3.3.2.2.2.2.cmml">I</mi><mn id="S3.E2.m1.3.3.2.2.2.3" xref="S3.E2.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.E2.m1.4.4.3.3.6" xref="S3.E2.m1.4.4.3.4.cmml">;</mo><mi id="S3.E2.m1.1.1" mathvariant="normal" xref="S3.E2.m1.1.1.cmml">…</mi><mo id="S3.E2.m1.4.4.3.3.7" xref="S3.E2.m1.4.4.3.4.cmml">;</mo><msub id="S3.E2.m1.4.4.3.3.3" xref="S3.E2.m1.4.4.3.3.3.cmml"><mi id="S3.E2.m1.4.4.3.3.3.2" xref="S3.E2.m1.4.4.3.3.3.2.cmml">I</mi><mi id="S3.E2.m1.4.4.3.3.3.3" xref="S3.E2.m1.4.4.3.3.3.3.cmml">T</mi></msub><mo id="S3.E2.m1.4.4.3.3.8" stretchy="false" xref="S3.E2.m1.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4"><eq id="S3.E2.m1.4.4.4.cmml" xref="S3.E2.m1.4.4.4"></eq><ci id="S3.E2.m1.4.4.5.cmml" xref="S3.E2.m1.4.4.5">𝑉</ci><list id="S3.E2.m1.4.4.3.4.cmml" xref="S3.E2.m1.4.4.3.3"><apply id="S3.E2.m1.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.E2.m1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.2">𝐼</ci><cn id="S3.E2.m1.2.2.1.1.1.3.cmml" type="integer" xref="S3.E2.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.E2.m1.3.3.2.2.2.cmml" xref="S3.E2.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.2.2.2.1.cmml" xref="S3.E2.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.E2.m1.3.3.2.2.2.2.cmml" xref="S3.E2.m1.3.3.2.2.2.2">𝐼</ci><cn id="S3.E2.m1.3.3.2.2.2.3.cmml" type="integer" xref="S3.E2.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">…</ci><apply id="S3.E2.m1.4.4.3.3.3.cmml" xref="S3.E2.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.3.3.3.1.cmml" xref="S3.E2.m1.4.4.3.3.3">subscript</csymbol><ci id="S3.E2.m1.4.4.3.3.3.2.cmml" xref="S3.E2.m1.4.4.3.3.3.2">𝐼</ci><ci id="S3.E2.m1.4.4.3.3.3.3.cmml" xref="S3.E2.m1.4.4.3.3.3.3">𝑇</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">V=[I_{1};I_{2};...;I_{T}]</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.4d">italic_V = [ italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; italic_I start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ; … ; italic_I start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">For clarity, we also provide pseudo-code for PMG in the supplementary material.</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.1">As candidates for image transformation,
we consider the following 8 image transformations.
(See <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS1.SSS1" title="4.1.1 The effect of image augmentations. ‣ 4.1 Ablation Studies ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.1.1</span></a> for the effect of each transformation)</p>
</div>
<div class="ltx_para" id="S3.SS2.p7">
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Identity: </span> Return an input image as is. We regard this as a baseline.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Sliding Window: </span>
Cut a window from a static image and move it randomly.
Note that this is similar to Unmasked MoSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib30" title="">30</a>]</cite>,
but our method does not limit the window’s movement to only four directions.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Zoom-in/out:</span>
Cut a window from an input image and enlarge or reduce the size of the window.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i4.p1.1.1">Fade-in/out: </span>
An input image gradually becomes visible or invisible.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i5.p1.1.1">Affine Transformation</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S3.I1.i6.p1">
<p class="ltx_p" id="S3.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i6.p1.1.1">Perspective Transformation</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="S3.I1.i7.p1">
<p class="ltx_p" id="S3.I1.i7.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i7.p1.1.1">Color Jitter</span>: Randomly change the brightness, contrast, saturation, and hue of an input image.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span>
<div class="ltx_para" id="S3.I1.i8.p1">
<p class="ltx_p" id="S3.I1.i8.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i8.p1.1.1">CutMix</span>: Generate an image using CutMix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib80" title="">80</a>]</cite> from two images and move a small area of the image in the manner of Sliding Window.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p8">
<p class="ltx_p" id="S3.SS2.p8.1">From these candidates, through experimentation,
we identify the optimal set of image transformations <math alttext="\phi" class="ltx_Math" display="inline" id="S3.SS2.p8.1.m1.1"><semantics id="S3.SS2.p8.1.m1.1a"><mi id="S3.SS2.p8.1.m1.1.1" xref="S3.SS2.p8.1.m1.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.1.m1.1b"><ci id="S3.SS2.p8.1.m1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.1.m1.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.1.m1.1d">italic_ϕ</annotation></semantics></math>
(See <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS1.SSS2" title="4.1.2 The combination of image augmentations. ‣ 4.1 Ablation Studies ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.1.2</span></a>).
Furthermore, to prevent overfitting to specific types of pseudo-motion videos,
we apply mixup <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib82" title="">82</a>]</cite> to each frame of the generated pseudo-motion videos.
This approach significantly enhances the diversity
in motion and appearance of the pseudo-motion videos,
facilitating more efficient learning by VideoMAE.
In the supplementary material, we describe the parameters for each image augmentation.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="230" id="S3.F2.g1" src="x6.png" width="747"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.9.1.1" style="font-size:113%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.10.2" style="font-size:113%;">
<span class="ltx_text ltx_font_bold" id="S3.F2.10.2.1">Examples of pseudo-motion videos.</span>
Images are sampled from PASS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib4" title="">4</a>]</cite>.
For more examples of pseudo-motion videos,
see the supplementary material.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p9">
<p class="ltx_p" id="S3.SS2.p9.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S3.F2" title="Figure 2 ‣ 3.2 Pseudo Motion Generator (PMG) ‣ 3 Proposed Method ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">2</span></a> presents
examples of pseudo-motion videos generated by PMG.
Although the motions in these videos differ from real videos,
they exhibit a wide range of motion and appearance patterns.
Moreover, the clear correspondence of patches between frames
makes these pseudo-motion videos particularly well-suited for VideoMAE,
because it focuses on capturing low-level features
rather than high-level semantic features.
Notably, when pre-training VideoMAE using real videos,
we can use pseudo-motion videos generated from a frame within the videos
as a powerful form of data augmentation
(we call this PMG Aug).
We demonstrate the effect of PMG Aug through experiments
(Refer to <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS4" title="4.4 PMG as Video Augmentation on Pre-training ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.4</span></a>).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Combination of Our Framework with Synthetic Images</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Our framework enables the pre-training of video transformers using single images,
which are more accessible than real videos.
Additionally, our framework is applicable to synthetic images,
further reducing data collection costs and minimizing privacy
and other concerns associated with the use of real-world data.
We use the following synthetic image datasets for this purpose:
(i) FractalDB, generated based on fractal geometry <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib35" title="">35</a>]</cite>,
(ii) Visual Atom, created using sine waves <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib62" title="">62</a>]</cite>,
(iii) Shaders1k, produced through OpenGL fragment shaders <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib7" title="">7</a>]</cite>.
These datasets encompass a large volume and wide variety of images,
and have demonstrated to be as effective as real image datasets in the image recognition task.
By combining these synthetic images with our PMG module,
we can generate a wide variety of pseudo-motion videos,
enabling video transformers to learn effective spatio-temporal representations
as they would using real videos.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.p1.1.1">Datasets.</span>
To evaluate the effectiveness of our framework,
we pre-train on pseudo-motion videos before fine-tuning and evaluating on various
action recognition datasets.
Following the SynAPT benchmark,
we use six action recognition datasets for fine-tuning and evaluation;
UCF101 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib57" title="">57</a>]</cite>,
HMDB51 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib39" title="">39</a>]</cite>,
MiniSSV2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib13" title="">13</a>]</cite> (a subset of Something-Something V2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib25" title="">25</a>]</cite>),
Diving48 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib43" title="">43</a>]</cite>,
IkeaFA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib65" title="">65</a>]</cite>,
and UAV-Human (UAV-H) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib41" title="">41</a>]</cite>.
This benchmark is used to assess the transferability of our framework.
Additionally, we use Kinetics400 (K400) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib36" title="">36</a>]</cite>.
As an evaluation metric, we report the top-1 accuracy.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">For pre-training, we adopt randomly sampled images from the following large-scale
image datasets:
ImageNet-1k (IN-1k) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib17" title="">17</a>]</cite>,
PASS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib4" title="">4</a>]</cite>,
FractalDB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib35" title="">35</a>]</cite>,
Shaders1k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib7" title="">7</a>]</cite>,
and Visual Atom <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib62" title="">62</a>]</cite>.
If the datasets have category annotations,
we sampled images so that the number of images of each category is the same.
Additionally, for comparison with  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib30" title="">30</a>]</cite>,
we randomly sample one frame of
a video and use it as an input image for generating pseudo-motion videos.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.p3.1.1">Implementation Details.</span>
We conducted our experiments using 8 A100 GPUs.
Our training settings were mostly aligned with VideoMAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib64" title="">64</a>]</cite>,
with a mask ratio of 0.75 and the number of epochs set to 2,000
unless otherwise noted
(See the supplementary material for details).
We used videos with 16 square frames (224 pixels in width).
For the model architecture,
we adopted a vanilla ViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib18" title="">18</a>]</cite> as the backbone,
specifically the ViT-Base variant.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Ablation Studies</h3>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S4.T3.fig1" style="width:195.1pt;">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.T3.fig1.3.1.1" style="font-size:129%;">Table 2</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T3.fig1.4.2" style="font-size:114%;">Comparison of different image augmentations.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.fig1.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.fig1.5.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.fig1.5.1.1.1"><span class="ltx_text" id="S4.T3.fig1.5.1.1.1.1" style="font-size:70%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.fig1.5.1.1.2"><span class="ltx_text" id="S4.T3.fig1.5.1.1.2.1" style="font-size:70%;">UCF101</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.fig1.5.1.1.3"><span class="ltx_text" id="S4.T3.fig1.5.1.1.3.1" style="font-size:70%;">HMDB51</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.fig1.5.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.fig1.5.2.1.1"><span class="ltx_text" id="S4.T3.fig1.5.2.1.1.1" style="font-size:70%;">Baseline (Identity)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.fig1.5.2.1.2"><span class="ltx_text" id="S4.T3.fig1.5.2.1.2.1" style="font-size:70%;">72.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.fig1.5.2.1.3"><span class="ltx_text" id="S4.T3.fig1.5.2.1.3.1" style="font-size:70%;">35.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig1.5.3.2">
<td class="ltx_td ltx_align_center" id="S4.T3.fig1.5.3.2.1"><span class="ltx_text" id="S4.T3.fig1.5.3.2.1.1" style="font-size:70%;">Sliding Window</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig1.5.3.2.2"><span class="ltx_text" id="S4.T3.fig1.5.3.2.2.1" style="font-size:70%;">75.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig1.5.3.2.3"><span class="ltx_text" id="S4.T3.fig1.5.3.2.3.1" style="font-size:70%;">40.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig1.5.4.3">
<td class="ltx_td ltx_align_center" id="S4.T3.fig1.5.4.3.1"><span class="ltx_text" id="S4.T3.fig1.5.4.3.1.1" style="font-size:70%;">Zoom-in/out</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig1.5.4.3.2"><span class="ltx_text" id="S4.T3.fig1.5.4.3.2.1" style="font-size:70%;">81.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig1.5.4.3.3"><span class="ltx_text" id="S4.T3.fig1.5.4.3.3.1" style="font-size:70%;">44.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig1.5.5.4">
<td class="ltx_td ltx_align_center" id="S4.T3.fig1.5.5.4.1"><span class="ltx_text" id="S4.T3.fig1.5.5.4.1.1" style="font-size:70%;">Fade-in/out</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig1.5.5.4.2"><span class="ltx_text" id="S4.T3.fig1.5.5.4.2.1" style="font-size:70%;">76.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig1.5.5.4.3"><span class="ltx_text" id="S4.T3.fig1.5.5.4.3.1" style="font-size:70%;">34.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig1.5.6.5">
<td class="ltx_td ltx_align_center" id="S4.T3.fig1.5.6.5.1"><span class="ltx_text" id="S4.T3.fig1.5.6.5.1.1" style="font-size:70%;">Affine</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig1.5.6.5.2"><span class="ltx_text" id="S4.T3.fig1.5.6.5.2.1" style="font-size:70%;">80.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig1.5.6.5.3"><span class="ltx_text" id="S4.T3.fig1.5.6.5.3.1" style="font-size:70%;">43.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig1.5.7.6">
<td class="ltx_td ltx_align_center" id="S4.T3.fig1.5.7.6.1"><span class="ltx_text" id="S4.T3.fig1.5.7.6.1.1" style="font-size:70%;">Perspective</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig1.5.7.6.2"><span class="ltx_text ltx_font_bold" id="S4.T3.fig1.5.7.6.2.1" style="font-size:70%;">82.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig1.5.7.6.3"><span class="ltx_text ltx_font_bold" id="S4.T3.fig1.5.7.6.3.1" style="font-size:70%;">45.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig1.5.8.7">
<td class="ltx_td ltx_align_center" id="S4.T3.fig1.5.8.7.1"><span class="ltx_text" id="S4.T3.fig1.5.8.7.1.1" style="font-size:70%;">Color Jitter</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig1.5.8.7.2"><span class="ltx_text" id="S4.T3.fig1.5.8.7.2.1" style="font-size:70%;">76.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig1.5.8.7.3"><span class="ltx_text" id="S4.T3.fig1.5.8.7.3.1" style="font-size:70%;">38.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig1.5.9.8">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.fig1.5.9.8.1"><span class="ltx_text" id="S4.T3.fig1.5.9.8.1.1" style="font-size:70%;">CutMix</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.fig1.5.9.8.2"><span class="ltx_text" id="S4.T3.fig1.5.9.8.2.1" style="font-size:70%;">76.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.fig1.5.9.8.3"><span class="ltx_text" id="S4.T3.fig1.5.9.8.3.1" style="font-size:70%;">45.1</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S4.T3.fig2" style="width:195.1pt;">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.T3.fig2.3.1.1" style="font-size:129%;">Table 3</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T3.fig2.4.2" style="font-size:114%;">Combination of image augmentations for PMG.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.fig2.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.fig2.5.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.fig2.5.1.1.1"><span class="ltx_text" id="S4.T3.fig2.5.1.1.1.1" style="font-size:70%;">Zoom-in/out</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.fig2.5.1.1.2"><span class="ltx_text" id="S4.T3.fig2.5.1.1.2.1" style="font-size:70%;">Affine</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.fig2.5.1.1.3"><span class="ltx_text" id="S4.T3.fig2.5.1.1.3.1" style="font-size:70%;">Perspective</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.fig2.5.1.1.4"><span class="ltx_text" id="S4.T3.fig2.5.1.1.4.1" style="font-size:70%;">CutMix</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.fig2.5.1.1.5"><span class="ltx_text" id="S4.T3.fig2.5.1.1.5.1" style="font-size:70%;">HMDB51</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig2.5.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.fig2.5.2.2.1"><span class="ltx_text" id="S4.T3.fig2.5.2.2.1.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.fig2.5.2.2.2"><span class="ltx_text" id="S4.T3.fig2.5.2.2.2.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_border_t" id="S4.T3.fig2.5.2.2.3"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.fig2.5.2.2.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.fig2.5.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T3.fig2.5.2.2.5.1" style="font-size:70%;">51.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig2.5.3.3">
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.3.3.1"><span class="ltx_text" id="S4.T3.fig2.5.3.3.1.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td" id="S4.T3.fig2.5.3.3.2"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.3.3.3"><span class="ltx_text" id="S4.T3.fig2.5.3.3.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td" id="S4.T3.fig2.5.3.3.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.3.3.5"><span class="ltx_text" id="S4.T3.fig2.5.3.3.5.1" style="font-size:70%;">45.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig2.5.4.4">
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.4.4.1"><span class="ltx_text" id="S4.T3.fig2.5.4.4.1.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td" id="S4.T3.fig2.5.4.4.2"></td>
<td class="ltx_td" id="S4.T3.fig2.5.4.4.3"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.4.4.4"><span class="ltx_text" id="S4.T3.fig2.5.4.4.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.4.4.5"><span class="ltx_text" id="S4.T3.fig2.5.4.4.5.1" style="font-size:70%;">41.4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig2.5.5.5">
<td class="ltx_td" id="S4.T3.fig2.5.5.5.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.5.5.2"><span class="ltx_text" id="S4.T3.fig2.5.5.5.2.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.5.5.3"><span class="ltx_text" id="S4.T3.fig2.5.5.5.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td" id="S4.T3.fig2.5.5.5.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.5.5.5"><span class="ltx_text" id="S4.T3.fig2.5.5.5.5.1" style="font-size:70%;">50.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig2.5.6.6">
<td class="ltx_td" id="S4.T3.fig2.5.6.6.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.6.6.2"><span class="ltx_text" id="S4.T3.fig2.5.6.6.2.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td" id="S4.T3.fig2.5.6.6.3"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.6.6.4"><span class="ltx_text" id="S4.T3.fig2.5.6.6.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.6.6.5"><span class="ltx_text" id="S4.T3.fig2.5.6.6.5.1" style="font-size:70%;">47.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig2.5.7.7">
<td class="ltx_td" id="S4.T3.fig2.5.7.7.1"></td>
<td class="ltx_td" id="S4.T3.fig2.5.7.7.2"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.7.7.3"><span class="ltx_text" id="S4.T3.fig2.5.7.7.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.7.7.4"><span class="ltx_text" id="S4.T3.fig2.5.7.7.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.7.7.5"><span class="ltx_text" id="S4.T3.fig2.5.7.7.5.1" style="font-size:70%;">44.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig2.5.8.8">
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.8.8.1"><span class="ltx_text" id="S4.T3.fig2.5.8.8.1.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.8.8.2"><span class="ltx_text" id="S4.T3.fig2.5.8.8.2.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.8.8.3"><span class="ltx_text" id="S4.T3.fig2.5.8.8.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td" id="S4.T3.fig2.5.8.8.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.8.8.5"><span class="ltx_text" id="S4.T3.fig2.5.8.8.5.1" style="font-size:70%;">49.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig2.5.9.9">
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.9.9.1"><span class="ltx_text" id="S4.T3.fig2.5.9.9.1.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.9.9.2"><span class="ltx_text" id="S4.T3.fig2.5.9.9.2.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td" id="S4.T3.fig2.5.9.9.3"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.9.9.4"><span class="ltx_text" id="S4.T3.fig2.5.9.9.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.9.9.5"><span class="ltx_text" id="S4.T3.fig2.5.9.9.5.1" style="font-size:70%;">49.4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig2.5.10.10">
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.10.10.1"><span class="ltx_text" id="S4.T3.fig2.5.10.10.1.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td" id="S4.T3.fig2.5.10.10.2"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.10.10.3"><span class="ltx_text" id="S4.T3.fig2.5.10.10.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.10.10.4"><span class="ltx_text" id="S4.T3.fig2.5.10.10.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.10.10.5"><span class="ltx_text" id="S4.T3.fig2.5.10.10.5.1" style="font-size:70%;">47.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig2.5.11.11">
<td class="ltx_td" id="S4.T3.fig2.5.11.11.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.11.11.2"><span class="ltx_text" id="S4.T3.fig2.5.11.11.2.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.11.11.3"><span class="ltx_text" id="S4.T3.fig2.5.11.11.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.11.11.4"><span class="ltx_text" id="S4.T3.fig2.5.11.11.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.fig2.5.11.11.5"><span class="ltx_text" id="S4.T3.fig2.5.11.11.5.1" style="font-size:70%;">42.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.fig2.5.12.12">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.fig2.5.12.12.1"><span class="ltx_text" id="S4.T3.fig2.5.12.12.1.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.fig2.5.12.12.2"><span class="ltx_text" id="S4.T3.fig2.5.12.12.2.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.fig2.5.12.12.3"><span class="ltx_text" id="S4.T3.fig2.5.12.12.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.fig2.5.12.12.4"><span class="ltx_text" id="S4.T3.fig2.5.12.12.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.fig2.5.12.12.5"><span class="ltx_text" id="S4.T3.fig2.5.12.12.5.1" style="font-size:70%;">47.9</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>The effect of image augmentations.</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">First, we investigated the contribution of each image transformation
on VideoMAE pre-training.
We used HMDB51 and UCF101 for generating pseudo-motion videos for pre-training,
then used videos from the respective datasets to fine-tune the model.
<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.T3" title="In 4.1 Ablation Studies ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a> reports the results when applying only a single variation
of image augmentation.
Videos generated by the Identity transformation serve as a baseline
because they do not contain any motion.
Compared to this baseline,
videos generated with Sliding Window, Zoom-in/out, Affine Transformation,
Perspective Transformation, and CutMix
improve the model’s accuracy over the baseline.
Pseudo-motion videos generated with these transformations have corresponding patches between
frames, meaning that patches in one frame might slightly move but would still exist in the
subsequent frame.
Therefore, this supports our hypothesis that this characteristic aids the VideoMAE when learning
spatio-temporal features.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1">While videos generated with Fade-in/out and Color Jitter marginally improved performance
on UCF101, they did not do as well on HMDB51,
which is a motion-sensitive dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib43" title="">43</a>]</cite>.
This suggests that videos made with these transformations are beneficial for capturing
spatial features but do not aid in capturing motion features.
Next, we experimentally determine the optimal set of image transformations <math alttext="\phi" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p2.1.m1.1"><semantics id="S4.SS1.SSS1.p2.1.m1.1a"><mi id="S4.SS1.SSS1.p2.1.m1.1.1" xref="S4.SS1.SSS1.p2.1.m1.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.1.m1.1b"><ci id="S4.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.1.m1.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p2.1.m1.1d">italic_ϕ</annotation></semantics></math>
from Sliding Window, Zoom-in/out, Affine Transformation,
Perspective Transformation, and CutMix.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>The combination of image augmentations.</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.T3" title="In 4.1 Ablation Studies ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a> compares the performance on HMDB51
when models are pre-trained with various combinations of image transformations.
It is observed that combining multiple image transformations improves the model’s performance.
This indicates that the model can effectively learn as long as there is sufficient diversity,
even if the motion patterns in pseudo-motion videos differ from those in real videos.
However, combining more image transformations did not necessarily yield better results.
In particular, in most cases where we applied CutMix, the accuracy decreased.
We hypothesize that this is due to the non-continuous nature of CutMix videos.
From this point on, we will use Zoom-in/out and Affine Transformation
as the set of image transformations <math alttext="\phi" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.1.m1.1"><semantics id="S4.SS1.SSS2.p1.1.m1.1a"><mi id="S4.SS1.SSS2.p1.1.m1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.1.m1.1b"><ci id="S4.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.1.m1.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.1.m1.1d">italic_ϕ</annotation></semantics></math>.
Further discussion on the failure cases of pre-training with these pseudo-motion videos
is provided in the supplementary material.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>The efficacy of video-level augmentations.</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">To further enhance the diversity of videos,
we applied video-level augmentation to the generated pseudo-motion videos.
We examined two methods: Mixup <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib82" title="">82</a>]</cite> and VideoMix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib81" title="">81</a>]</cite>.
 <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.T5" title="In 4.1.3 The efficacy of video-level augmentations. ‣ 4.1 Ablation Studies ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5</span></a> demonstrates that video-level augmentation, especially Mixup,
significantly contributes to performance improvement.
This is because both video augmentations diversify pseudo-motion videos,
resulting in better performance.
Pre-training with VideoMix results in lower accuracy compared to Mixup
because the videos generated by VideoMix have non-continuous regions
like CutMix, as discussed in <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS1.SSS2" title="4.1.2 The combination of image augmentations. ‣ 4.1 Ablation Studies ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.1.2</span></a>,
From here, we will utilize Mixup in our experiments.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S4.T5.fig1" style="width:212.5pt;">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.T5.fig1.5.1.1" style="font-size:129%;">Table 4</span>: </span><span class="ltx_text" id="S4.T5.fig1.6.2" style="font-size:129%;">
<span class="ltx_text ltx_font_bold" id="S4.T5.fig1.6.2.1">Effects of video-level augmentation.</span>
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T5.fig1.7">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.fig1.7.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T5.fig1.7.1.1.1"><span class="ltx_text" id="S4.T5.fig1.7.1.1.1.1" style="font-size:70%;">Video Augmentaiton</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T5.fig1.7.1.1.2"><span class="ltx_text" id="S4.T5.fig1.7.1.1.2.1" style="font-size:70%;">Dataset</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.fig1.7.2.2">
<td class="ltx_td ltx_align_center" id="S4.T5.fig1.7.2.2.1"><span class="ltx_text" id="S4.T5.fig1.7.2.2.1.1" style="font-size:70%;">Mixup</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.fig1.7.2.2.2"><span class="ltx_text" id="S4.T5.fig1.7.2.2.2.1" style="font-size:70%;">VideoMix</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.fig1.7.2.2.3"><span class="ltx_text" id="S4.T5.fig1.7.2.2.3.1" style="font-size:70%;">HMDB51</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.fig1.7.2.2.4"><span class="ltx_text" id="S4.T5.fig1.7.2.2.4.1" style="font-size:70%;">UCF101</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.fig1.7.3.3">
<td class="ltx_td ltx_border_t" id="S4.T5.fig1.7.3.3.1"></td>
<td class="ltx_td ltx_border_t" id="S4.T5.fig1.7.3.3.2"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.fig1.7.3.3.3"><span class="ltx_text" id="S4.T5.fig1.7.3.3.3.1" style="font-size:70%;">51.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.fig1.7.3.3.4"><span class="ltx_text" id="S4.T5.fig1.7.3.3.4.1" style="font-size:70%;">83.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.fig1.7.4.4">
<td class="ltx_td ltx_align_center" id="S4.T5.fig1.7.4.4.1"><span class="ltx_text" id="S4.T5.fig1.7.4.4.1.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td" id="S4.T5.fig1.7.4.4.2"></td>
<td class="ltx_td ltx_align_center" id="S4.T5.fig1.7.4.4.3"><span class="ltx_text ltx_font_bold" id="S4.T5.fig1.7.4.4.3.1" style="font-size:70%;">55.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.fig1.7.4.4.4"><span class="ltx_text ltx_font_bold" id="S4.T5.fig1.7.4.4.4.1" style="font-size:70%;">87.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.fig1.7.5.5">
<td class="ltx_td ltx_border_bb" id="S4.T5.fig1.7.5.5.1"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.fig1.7.5.5.2"><span class="ltx_text" id="S4.T5.fig1.7.5.5.2.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.fig1.7.5.5.3"><span class="ltx_text" id="S4.T5.fig1.7.5.5.3.1" style="font-size:70%;">53.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.fig1.7.5.5.4"><span class="ltx_text" id="S4.T5.fig1.7.5.5.4.1" style="font-size:70%;">85.2</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S4.T5.fig2" style="width:212.5pt;">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.T5.fig2.3.1.1" style="font-size:129%;">Table 5</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T5.fig2.4.2" style="font-size:129%;">Transferability from other video datasets.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.fig2.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.fig2.5.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.fig2.5.1.1.1"><span class="ltx_text" id="S4.T5.fig2.5.1.1.1.1" style="font-size:70%;">Pre-training</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.fig2.5.1.1.2"><span class="ltx_text" id="S4.T5.fig2.5.1.1.2.1" style="font-size:70%;">Fine-tuning</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.fig2.5.1.1.3"><span class="ltx_text" id="S4.T5.fig2.5.1.1.3.1" style="font-size:70%;">Top1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.fig2.5.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.fig2.5.2.1.1"><span class="ltx_text" id="S4.T5.fig2.5.2.1.1.1" style="font-size:70%;">HMDB51</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.fig2.5.2.1.2" rowspan="2"><span class="ltx_text" id="S4.T5.fig2.5.2.1.2.1" style="font-size:70%;">HMDB51</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.fig2.5.2.1.3"><span class="ltx_text" id="S4.T5.fig2.5.2.1.3.1" style="font-size:70%;">55.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.fig2.5.3.2">
<td class="ltx_td ltx_align_center" id="S4.T5.fig2.5.3.2.1"><span class="ltx_text" id="S4.T5.fig2.5.3.2.1.1" style="font-size:70%;">UCF101</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.fig2.5.3.2.2"><span class="ltx_text" id="S4.T5.fig2.5.3.2.2.1" style="font-size:70%;">56.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.fig2.5.4.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.fig2.5.4.3.1"><span class="ltx_text" id="S4.T5.fig2.5.4.3.1.1" style="font-size:70%;">UCF101</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.fig2.5.4.3.2" rowspan="2"><span class="ltx_text" id="S4.T5.fig2.5.4.3.2.1" style="font-size:70%;">UCF101</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.fig2.5.4.3.3"><span class="ltx_text" id="S4.T5.fig2.5.4.3.3.1" style="font-size:70%;">87.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.fig2.5.5.4">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.fig2.5.5.4.1"><span class="ltx_text" id="S4.T5.fig2.5.5.4.1.1" style="font-size:70%;">HMDB51</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.fig2.5.5.4.2"><span class="ltx_text" id="S4.T5.fig2.5.5.4.2.1" style="font-size:70%;">85.5</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Transferability of Our Framework</h3>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Transferability from other video datasets.</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.2">To verify the transferability of our framework,
we conducted experiments by pre-training models with pseudo-motion videos generated from
frames in HMDB51 and then fine-tuning on
UCF101 (hereafter, we refer to this as HMDB51 <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.1.m1.1"><semantics id="S4.SS2.SSS1.p1.1.m1.1a"><mo id="S4.SS2.SSS1.p1.1.m1.1.1" stretchy="false" xref="S4.SS2.SSS1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.1.m1.1b"><ci id="S4.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.1.m1.1d">→</annotation></semantics></math> UCF101),
and then vice versa (UCF101 <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.2.m2.1"><semantics id="S4.SS2.SSS1.p1.2.m2.1a"><mo id="S4.SS2.SSS1.p1.2.m2.1.1" stretchy="false" xref="S4.SS2.SSS1.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.2.m2.1b"><ci id="S4.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.2.m2.1d">→</annotation></semantics></math> HMDB51).
<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.T5" title="In 4.1.3 The efficacy of video-level augmentations. ‣ 4.1 Ablation Studies ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5</span></a> shows the results.
Comparing the accuracy when pre-training on different datasets, the difference
is marginal.
This suggests that our framework learns robust features that are not
domain-specific.
Furthermore, this appeals that our framework can effectively pre-train
models even when using image datasets instead, such as ImageNet and PASS.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Transferability from real image datasets.</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">In our previous experiments,
we used samples with similar visual cues between pre-training and fine-tuning,
namely the semantic information including objects and people.
To further assess the transferability of our framework,
we conducted pre-training on the ImageNet-1k and PASS,
which are in different domains compared to the fine-tuning datasets (UCF101 and HMDB51).
As detailed in <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.T7" title="In 4.2.3 Transferability from synthetic images. ‣ 4.2 Transferability of Our Framework ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">7</span></a>,
pre-training using ImageNet and PASS
achieved comparable performance to when pre-training with the same
datasets that are used when fine-tuning.
Note that PASS does not include any human images.
Therefore, the semantic information within pre-training datasets are not
a must for effective pre-training of VideoMAE.
Moreover, increasing the number of images scaled the performance.
These experimental results suggest that for VideoMAE,
the diversity of the data is more crucial
than domain-specific information like human motion or visual cues.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Transferability from synthetic images.</h4>
<figure class="ltx_table" id="S4.T7">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S4.T7.fig1" style="width:208.1pt;">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.T7.fig1.4.1.1" style="font-size:129%;">Table 6</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T7.fig1.5.2" style="font-size:129%;">Pre-training with ImageNet and PASS.<span class="ltx_text ltx_font_medium" id="S4.T7.fig1.5.2.1">
The term ’FT data’ indicates that the
datasets used for pre-training are identical to those used in fine-tuning.
</span></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T7.fig1.6">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T7.fig1.6.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T7.fig1.6.1.1.1"><span class="ltx_text" id="S4.T7.fig1.6.1.1.1.1" style="font-size:70%;">Pre-training</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T7.fig1.6.1.1.2"><span class="ltx_text" id="S4.T7.fig1.6.1.1.2.1" style="font-size:70%;">Downstream task</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.fig1.6.2.2">
<td class="ltx_td ltx_align_center" id="S4.T7.fig1.6.2.2.1"><span class="ltx_text" id="S4.T7.fig1.6.2.2.1.1" style="font-size:70%;">Dataset</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.fig1.6.2.2.2"><span class="ltx_text" id="S4.T7.fig1.6.2.2.2.1" style="font-size:70%;">#data</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.fig1.6.2.2.3"><span class="ltx_text" id="S4.T7.fig1.6.2.2.3.1" style="font-size:70%;">UCF101</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.fig1.6.2.2.4"><span class="ltx_text" id="S4.T7.fig1.6.2.2.4.1" style="font-size:70%;">HMDB51</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.fig1.6.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig1.6.3.3.1"><span class="ltx_text" id="S4.T7.fig1.6.3.3.1.1" style="font-size:70%;">FT data</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig1.6.3.3.2"><span class="ltx_text" id="S4.T7.fig1.6.3.3.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig1.6.3.3.3"><span class="ltx_text" id="S4.T7.fig1.6.3.3.3.1" style="font-size:70%;">87.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig1.6.3.3.4"><span class="ltx_text" id="S4.T7.fig1.6.3.3.4.1" style="font-size:70%;">55.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.fig1.6.4.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig1.6.4.4.1"><span class="ltx_text" id="S4.T7.fig1.6.4.4.1.1" style="font-size:70%;">ImageNet</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig1.6.4.4.2"><span class="ltx_text" id="S4.T7.fig1.6.4.4.2.1" style="font-size:70%;">10,000</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig1.6.4.4.3"><span class="ltx_text" id="S4.T7.fig1.6.4.4.3.1" style="font-size:70%;">87.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig1.6.4.4.4"><span class="ltx_text" id="S4.T7.fig1.6.4.4.4.1" style="font-size:70%;">58.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.fig1.6.5.5">
<td class="ltx_td ltx_align_center" id="S4.T7.fig1.6.5.5.1"><span class="ltx_text" id="S4.T7.fig1.6.5.5.1.1" style="font-size:70%;">ImageNet</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.fig1.6.5.5.2"><span class="ltx_text" id="S4.T7.fig1.6.5.5.2.1" style="font-size:70%;">100,000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.fig1.6.5.5.3"><span class="ltx_text" id="S4.T7.fig1.6.5.5.3.1" style="font-size:70%;">89.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.fig1.6.5.5.4"><span class="ltx_text" id="S4.T7.fig1.6.5.5.4.1" style="font-size:70%;">59.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.fig1.6.6.6">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig1.6.6.6.1"><span class="ltx_text" id="S4.T7.fig1.6.6.6.1.1" style="font-size:70%;">PASS</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig1.6.6.6.2"><span class="ltx_text" id="S4.T7.fig1.6.6.6.2.1" style="font-size:70%;">10,000</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig1.6.6.6.3"><span class="ltx_text" id="S4.T7.fig1.6.6.6.3.1" style="font-size:70%;">87.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig1.6.6.6.4"><span class="ltx_text" id="S4.T7.fig1.6.6.6.4.1" style="font-size:70%;">58.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.fig1.6.7.7">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.fig1.6.7.7.1"><span class="ltx_text" id="S4.T7.fig1.6.7.7.1.1" style="font-size:70%;">PASS</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.fig1.6.7.7.2"><span class="ltx_text" id="S4.T7.fig1.6.7.7.2.1" style="font-size:70%;">100,000</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.fig1.6.7.7.3"><span class="ltx_text ltx_font_bold" id="S4.T7.fig1.6.7.7.3.1" style="font-size:70%;">89.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.fig1.6.7.7.4"><span class="ltx_text ltx_font_bold" id="S4.T7.fig1.6.7.7.4.1" style="font-size:70%;">60.0</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S4.T7.fig2" style="width:212.5pt;">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.T7.fig2.3.1.1" style="font-size:129%;">Table 7</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T7.fig2.4.2" style="font-size:129%;">Pre-training on synthetic image datasets.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T7.fig2.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T7.fig2.5.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T7.fig2.5.1.1.1"><span class="ltx_text" id="S4.T7.fig2.5.1.1.1.1" style="font-size:70%;">Pre-training Setting</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T7.fig2.5.1.1.2"><span class="ltx_text" id="S4.T7.fig2.5.1.1.2.1" style="font-size:70%;">Downstream task</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.fig2.5.2.2">
<td class="ltx_td ltx_align_center" id="S4.T7.fig2.5.2.2.1"><span class="ltx_text" id="S4.T7.fig2.5.2.2.1.1" style="font-size:70%;">Dataset</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.fig2.5.2.2.2"><span class="ltx_text" id="S4.T7.fig2.5.2.2.2.1" style="font-size:70%;">#Data</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.fig2.5.2.2.3"><span class="ltx_text" id="S4.T7.fig2.5.2.2.3.1" style="font-size:70%;">UCF101</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.fig2.5.2.2.4"><span class="ltx_text" id="S4.T7.fig2.5.2.2.4.1" style="font-size:70%;">HMDB51</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.fig2.5.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig2.5.3.3.1"><span class="ltx_text" id="S4.T7.fig2.5.3.3.1.1" style="font-size:70%;">FT data</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig2.5.3.3.2"><span class="ltx_text" id="S4.T7.fig2.5.3.3.2.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig2.5.3.3.3"><span class="ltx_text" id="S4.T7.fig2.5.3.3.3.1" style="font-size:70%;">87.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig2.5.3.3.4"><span class="ltx_text" id="S4.T7.fig2.5.3.3.4.1" style="font-size:70%;">55.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.fig2.5.4.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig2.5.4.4.1" rowspan="2"><span class="ltx_text" id="S4.T7.fig2.5.4.4.1.1" style="font-size:70%;">FractalDB</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig2.5.4.4.2"><span class="ltx_text" id="S4.T7.fig2.5.4.4.2.1" style="font-size:70%;">10,000</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig2.5.4.4.3"><span class="ltx_text" id="S4.T7.fig2.5.4.4.3.1" style="font-size:70%;">77.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig2.5.4.4.4"><span class="ltx_text" id="S4.T7.fig2.5.4.4.4.1" style="font-size:70%;">42.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.fig2.5.5.5">
<td class="ltx_td ltx_align_center" id="S4.T7.fig2.5.5.5.1"><span class="ltx_text" id="S4.T7.fig2.5.5.5.1.1" style="font-size:70%;">100,000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.fig2.5.5.5.2"><span class="ltx_text" id="S4.T7.fig2.5.5.5.2.1" style="font-size:70%;">78.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.fig2.5.5.5.3"><span class="ltx_text" id="S4.T7.fig2.5.5.5.3.1" style="font-size:70%;">41.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.fig2.5.6.6">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig2.5.6.6.1" rowspan="2"><span class="ltx_text" id="S4.T7.fig2.5.6.6.1.1" style="font-size:70%;">Shaders1k</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig2.5.6.6.2"><span class="ltx_text" id="S4.T7.fig2.5.6.6.2.1" style="font-size:70%;">10,000</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig2.5.6.6.3"><span class="ltx_text" id="S4.T7.fig2.5.6.6.3.1" style="font-size:70%;">88.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig2.5.6.6.4"><span class="ltx_text" id="S4.T7.fig2.5.6.6.4.1" style="font-size:70%;">57.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.fig2.5.7.7">
<td class="ltx_td ltx_align_center" id="S4.T7.fig2.5.7.7.1"><span class="ltx_text" id="S4.T7.fig2.5.7.7.1.1" style="font-size:70%;">100,000</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.fig2.5.7.7.2"><span class="ltx_text ltx_font_bold" id="S4.T7.fig2.5.7.7.2.1" style="font-size:70%;">89.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.fig2.5.7.7.3"><span class="ltx_text ltx_font_bold" id="S4.T7.fig2.5.7.7.3.1" style="font-size:70%;">59.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.fig2.5.8.8">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T7.fig2.5.8.8.1" rowspan="2"><span class="ltx_text" id="S4.T7.fig2.5.8.8.1.1" style="font-size:70%;">Visual Atom</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig2.5.8.8.2"><span class="ltx_text" id="S4.T7.fig2.5.8.8.2.1" style="font-size:70%;">10,000</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig2.5.8.8.3"><span class="ltx_text" id="S4.T7.fig2.5.8.8.3.1" style="font-size:70%;">83.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.fig2.5.8.8.4"><span class="ltx_text" id="S4.T7.fig2.5.8.8.4.1" style="font-size:70%;">48.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.fig2.5.9.9">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.fig2.5.9.9.1"><span class="ltx_text" id="S4.T7.fig2.5.9.9.1.1" style="font-size:70%;">100,000</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.fig2.5.9.9.2"><span class="ltx_text" id="S4.T7.fig2.5.9.9.2.1" style="font-size:70%;">82.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.fig2.5.9.9.3"><span class="ltx_text" id="S4.T7.fig2.5.9.9.3.1" style="font-size:70%;">48.2</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">We then pre-trained on synthetic image datasets using our framework
to verify that spatio-temporal features can be effectively learnt
from synthetic images, which present completely different visual cues
compared to our target action recognition datasets.
For synthetic image datasets, we used FractalDB, Shaders1k, and Visual Atom.
Herein, we used 10k/100k images sampled from each dataset.
<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.T7" title="In 4.2.3 Transferability from synthetic images. ‣ 4.2 Transferability of Our Framework ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">7</span></a> shows
the performance on UCF101 and HMDB51
when pre-training on diverse synthetic datasets,
including FractalDB, Shaders1k, and VisualAtom.
Note that pre-training with Shaders1k achieved
comparable results to pre-training with real images, where
pre-training with FractalDB and Visual Atom lead to subpar performance.
This denotes that the model struggles to correlate patches between
frames of pseudo-motion videos generated from FractalDB and Visual Atom, thus
failing to capture robust low-level features.
On the other hand, images in Shaders1k have distinctive patches that can be
correlated before and after transformations, which supports the model when
capturing low-level features.
This indicates that our framework can successfully replace the need for real
data when pre-training the model, as long as synthetic videos have patches that
can be tracked between frames.
Thus, when using our framework, challenges related to real datasets such as privacy
and license infringement are nonexistent.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Effect of the Number of Epochs, Data, and Categories in Image Datasets for Pre-training</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A7.F10" title="In Appendix 0.G Failure Cases ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">10</span></a> shows the relationship between the number of epochs
of pre-training and accuracy on HMDB51.
For generating videos for pre-training, we used 10k images from Shaders1k and a frame from
each of the 3k videos in HMDB51.
In both datasets, the model performance improved over epochs and
the difference of accuracy gradually decreased.
Because our PMG allows for the generation of diverse videos,
even if we have a small amount of data for pre-training,
it is possible to improve performance by increasing the number of iterations.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.F3.sf2" title="In Figure 3 ‣ 4.3 Effect of the Number of Epochs, Data, and Categories in Image Datasets for Pre-training ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3(b)</span></a> presents the accuracy transition
when the number of pre-training samples is varied among <math alttext="\{1k,5k,10k,50k,100k\}" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.5"><semantics id="S4.SS3.p2.1.m1.5a"><mrow id="S4.SS3.p2.1.m1.5.5.5" xref="S4.SS3.p2.1.m1.5.5.6.cmml"><mo id="S4.SS3.p2.1.m1.5.5.5.6" stretchy="false" xref="S4.SS3.p2.1.m1.5.5.6.cmml">{</mo><mrow id="S4.SS3.p2.1.m1.1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.1.cmml"><mn id="S4.SS3.p2.1.m1.1.1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.1.1.2.cmml">1</mn><mo id="S4.SS3.p2.1.m1.1.1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S4.SS3.p2.1.m1.1.1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.1.1.3.cmml">k</mi></mrow><mo id="S4.SS3.p2.1.m1.5.5.5.7" xref="S4.SS3.p2.1.m1.5.5.6.cmml">,</mo><mrow id="S4.SS3.p2.1.m1.2.2.2.2" xref="S4.SS3.p2.1.m1.2.2.2.2.cmml"><mn id="S4.SS3.p2.1.m1.2.2.2.2.2" xref="S4.SS3.p2.1.m1.2.2.2.2.2.cmml">5</mn><mo id="S4.SS3.p2.1.m1.2.2.2.2.1" xref="S4.SS3.p2.1.m1.2.2.2.2.1.cmml">⁢</mo><mi id="S4.SS3.p2.1.m1.2.2.2.2.3" xref="S4.SS3.p2.1.m1.2.2.2.2.3.cmml">k</mi></mrow><mo id="S4.SS3.p2.1.m1.5.5.5.8" xref="S4.SS3.p2.1.m1.5.5.6.cmml">,</mo><mrow id="S4.SS3.p2.1.m1.3.3.3.3" xref="S4.SS3.p2.1.m1.3.3.3.3.cmml"><mn id="S4.SS3.p2.1.m1.3.3.3.3.2" xref="S4.SS3.p2.1.m1.3.3.3.3.2.cmml">10</mn><mo id="S4.SS3.p2.1.m1.3.3.3.3.1" xref="S4.SS3.p2.1.m1.3.3.3.3.1.cmml">⁢</mo><mi id="S4.SS3.p2.1.m1.3.3.3.3.3" xref="S4.SS3.p2.1.m1.3.3.3.3.3.cmml">k</mi></mrow><mo id="S4.SS3.p2.1.m1.5.5.5.9" xref="S4.SS3.p2.1.m1.5.5.6.cmml">,</mo><mrow id="S4.SS3.p2.1.m1.4.4.4.4" xref="S4.SS3.p2.1.m1.4.4.4.4.cmml"><mn id="S4.SS3.p2.1.m1.4.4.4.4.2" xref="S4.SS3.p2.1.m1.4.4.4.4.2.cmml">50</mn><mo id="S4.SS3.p2.1.m1.4.4.4.4.1" xref="S4.SS3.p2.1.m1.4.4.4.4.1.cmml">⁢</mo><mi id="S4.SS3.p2.1.m1.4.4.4.4.3" xref="S4.SS3.p2.1.m1.4.4.4.4.3.cmml">k</mi></mrow><mo id="S4.SS3.p2.1.m1.5.5.5.10" xref="S4.SS3.p2.1.m1.5.5.6.cmml">,</mo><mrow id="S4.SS3.p2.1.m1.5.5.5.5" xref="S4.SS3.p2.1.m1.5.5.5.5.cmml"><mn id="S4.SS3.p2.1.m1.5.5.5.5.2" xref="S4.SS3.p2.1.m1.5.5.5.5.2.cmml">100</mn><mo id="S4.SS3.p2.1.m1.5.5.5.5.1" xref="S4.SS3.p2.1.m1.5.5.5.5.1.cmml">⁢</mo><mi id="S4.SS3.p2.1.m1.5.5.5.5.3" xref="S4.SS3.p2.1.m1.5.5.5.5.3.cmml">k</mi></mrow><mo id="S4.SS3.p2.1.m1.5.5.5.11" stretchy="false" xref="S4.SS3.p2.1.m1.5.5.6.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.5b"><set id="S4.SS3.p2.1.m1.5.5.6.cmml" xref="S4.SS3.p2.1.m1.5.5.5"><apply id="S4.SS3.p2.1.m1.1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1.1"><times id="S4.SS3.p2.1.m1.1.1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1.1.1"></times><cn id="S4.SS3.p2.1.m1.1.1.1.1.2.cmml" type="integer" xref="S4.SS3.p2.1.m1.1.1.1.1.2">1</cn><ci id="S4.SS3.p2.1.m1.1.1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.1.1.3">𝑘</ci></apply><apply id="S4.SS3.p2.1.m1.2.2.2.2.cmml" xref="S4.SS3.p2.1.m1.2.2.2.2"><times id="S4.SS3.p2.1.m1.2.2.2.2.1.cmml" xref="S4.SS3.p2.1.m1.2.2.2.2.1"></times><cn id="S4.SS3.p2.1.m1.2.2.2.2.2.cmml" type="integer" xref="S4.SS3.p2.1.m1.2.2.2.2.2">5</cn><ci id="S4.SS3.p2.1.m1.2.2.2.2.3.cmml" xref="S4.SS3.p2.1.m1.2.2.2.2.3">𝑘</ci></apply><apply id="S4.SS3.p2.1.m1.3.3.3.3.cmml" xref="S4.SS3.p2.1.m1.3.3.3.3"><times id="S4.SS3.p2.1.m1.3.3.3.3.1.cmml" xref="S4.SS3.p2.1.m1.3.3.3.3.1"></times><cn id="S4.SS3.p2.1.m1.3.3.3.3.2.cmml" type="integer" xref="S4.SS3.p2.1.m1.3.3.3.3.2">10</cn><ci id="S4.SS3.p2.1.m1.3.3.3.3.3.cmml" xref="S4.SS3.p2.1.m1.3.3.3.3.3">𝑘</ci></apply><apply id="S4.SS3.p2.1.m1.4.4.4.4.cmml" xref="S4.SS3.p2.1.m1.4.4.4.4"><times id="S4.SS3.p2.1.m1.4.4.4.4.1.cmml" xref="S4.SS3.p2.1.m1.4.4.4.4.1"></times><cn id="S4.SS3.p2.1.m1.4.4.4.4.2.cmml" type="integer" xref="S4.SS3.p2.1.m1.4.4.4.4.2">50</cn><ci id="S4.SS3.p2.1.m1.4.4.4.4.3.cmml" xref="S4.SS3.p2.1.m1.4.4.4.4.3">𝑘</ci></apply><apply id="S4.SS3.p2.1.m1.5.5.5.5.cmml" xref="S4.SS3.p2.1.m1.5.5.5.5"><times id="S4.SS3.p2.1.m1.5.5.5.5.1.cmml" xref="S4.SS3.p2.1.m1.5.5.5.5.1"></times><cn id="S4.SS3.p2.1.m1.5.5.5.5.2.cmml" type="integer" xref="S4.SS3.p2.1.m1.5.5.5.5.2">100</cn><ci id="S4.SS3.p2.1.m1.5.5.5.5.3.cmml" xref="S4.SS3.p2.1.m1.5.5.5.5.3">𝑘</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.5c">\{1k,5k,10k,50k,100k\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.5d">{ 1 italic_k , 5 italic_k , 10 italic_k , 50 italic_k , 100 italic_k }</annotation></semantics></math>.
Our framework shows improvement as the number of data increased.
Because we use only a small subset from PASS and Shaders1k,
there is potential for more substantial performance improvement
when generating from all images.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">Based on the results of the previous experiment,
we hypothesized that
performance can be further enhanced
by increasing the diversity of samples in pre-training image datasets.
<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A7.F10" title="In Appendix 0.G Failure Cases ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">10</span></a> shows the relationship between
the number of categories in the pre-training datasets we use
and the classification performance on HMDB51.
We set the number of training samples to 10k images, using the IN-1k and Shaders1k datasets.
For IN-1k, the accuracy seems to saturate after raising the diversity to more than 50 classes.
For Shaders1k, the accuracy was almost the same
even when the number of categories increased.
This suggests our framework scales with having more data samples,
but does not require semantic diversity within the samples.
These results support the fact that VideoMAE learns low-level features like the
correspondence of patches between frames, rather than semantic information like categories of objects
displayed.</p>
</div>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="593" id="S4.F3.sf1.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.sf1.4.1.1" style="font-size:113%;">(a)</span> </span><span class="ltx_text" id="S4.F3.sf1.5.2" style="font-size:113%;">Pre-training epochs vs accuracy on HMDB51</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="593" id="S4.F3.sf2.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.sf2.4.1.1" style="font-size:113%;">(b)</span> </span><span class="ltx_text" id="S4.F3.sf2.5.2" style="font-size:113%;">The number of data vs accuracy on HMDB51</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="593" id="S4.F3.sf3.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.sf3.4.1.1" style="font-size:113%;">(c)</span> </span><span class="ltx_text" id="S4.F3.sf3.5.2" style="font-size:113%;">The number of categories vs accuracy on HMDB51</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.4.1.1" style="font-size:113%;">Figure 3</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F3.5.2" style="font-size:113%;">Effect of the number of epochs, data, categories.</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T9">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S4.T9.4" style="width:203.8pt;">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.T9.4.14.2.1" style="font-size:129%;">Table 8</span>: </span><span class="ltx_text" id="S4.T9.2.2.1" style="font-size:129%;">
<span class="ltx_text ltx_font_bold" id="S4.T9.2.2.1.1">Effectiveness of our PMG as a video augmentation method on HMDB51 and UCF101.</span>
<sup class="ltx_sup" id="S4.T9.2.2.1.2">∗</sup> results from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib64" title="">64</a>]</cite>.
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T9.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T9.4.4.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" colspan="2" id="S4.T9.4.4.3.1.1"><span class="ltx_text" id="S4.T9.4.4.3.1.1.1" style="font-size:70%;">Pre-training data</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="2" id="S4.T9.4.4.3.1.2"><span class="ltx_text" id="S4.T9.4.4.3.1.2.1" style="font-size:70%;">Downstream tasks</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T9.4.4.4.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T9.4.4.4.1.1"><span class="ltx_text" id="S4.T9.4.4.4.1.1.1" style="font-size:70%;"> real video</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T9.4.4.4.1.2"><span class="ltx_text" id="S4.T9.4.4.4.1.2.1" style="font-size:70%;"> pseudo motion</span></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T9.4.4.4.1.3"><span class="ltx_text" id="S4.T9.4.4.4.1.3.1" style="font-size:70%;">HMDB51</span></th>
<td class="ltx_td ltx_align_center" id="S4.T9.4.4.4.1.4"><span class="ltx_text" id="S4.T9.4.4.4.1.4.1" style="font-size:70%;">UCF101</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T9.4.4.2.3"><span class="ltx_text" id="S4.T9.4.4.2.3.1" style="font-size:70%;">✓</span></th>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T9.4.4.2.4"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T9.3.3.1.1">
<span class="ltx_text" id="S4.T9.3.3.1.1.1" style="font-size:70%;">62.6</span><sup class="ltx_sup" id="S4.T9.3.3.1.1.2"><span class="ltx_text" id="S4.T9.3.3.1.1.2.1" style="font-size:70%;">∗</span></sup>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.4.4.2.2">
<span class="ltx_text" id="S4.T9.4.4.2.2.1" style="font-size:70%;">91.3</span><sup class="ltx_sup" id="S4.T9.4.4.2.2.2"><span class="ltx_text" id="S4.T9.4.4.2.2.2.1" style="font-size:70%;">∗</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.4.5.2">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T9.4.4.5.2.1"></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T9.4.4.5.2.2"><span class="ltx_text" id="S4.T9.4.4.5.2.2.1" style="font-size:70%;">✓</span></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T9.4.4.5.2.3"><span class="ltx_text" id="S4.T9.4.4.5.2.3.1" style="font-size:70%;">55.9</span></th>
<td class="ltx_td ltx_align_center" id="S4.T9.4.4.5.2.4"><span class="ltx_text" id="S4.T9.4.4.5.2.4.1" style="font-size:70%;">87.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.4.6.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T9.4.4.6.3.1"><span class="ltx_text" id="S4.T9.4.4.6.3.1.1" style="font-size:70%;">✓</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T9.4.4.6.3.2"><span class="ltx_text" id="S4.T9.4.4.6.3.2.1" style="font-size:70%;">✓</span></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T9.4.4.6.3.3"><span class="ltx_text ltx_font_bold" id="S4.T9.4.4.6.3.3.1" style="font-size:70%;">64.6</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T9.4.4.6.3.4"><span class="ltx_text ltx_font_bold" id="S4.T9.4.4.6.3.4.1" style="font-size:70%;">92.2</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S4.T9.9" style="width:225.5pt;">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.T9.9.12.2.1" style="font-size:129%;">Table 9</span>: </span><span class="ltx_text" id="S4.T9.6.2.1" style="font-size:129%;">
<span class="ltx_text ltx_font_bold" id="S4.T9.6.2.1.1">Comparison of each combination with real videos and pseudo-motion videos.</span>
<sup class="ltx_sup" id="S4.T9.6.2.1.2">∗</sup> Sources of PMG Aug.
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T9.9.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T9.9.5.4.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="4" id="S4.T9.9.5.4.1.1"><span class="ltx_text" id="S4.T9.9.5.4.1.1.1" style="font-size:70%;">Pre-training data</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T9.9.5.4.1.2" rowspan="2"><span class="ltx_text" id="S4.T9.9.5.4.1.2.1" style="font-size:70%;"> HMDB51</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.9.5.3">
<td class="ltx_td ltx_align_center" id="S4.T9.9.5.3.4"><span class="ltx_text" id="S4.T9.9.5.3.4.1" style="font-size:70%;">Videos</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.7.3.1.1">
<span class="ltx_text" id="S4.T9.7.3.1.1.1" style="font-size:70%;"> frames</span><sup class="ltx_sup" id="S4.T9.7.3.1.1.2"><span class="ltx_text" id="S4.T9.7.3.1.1.2.1" style="font-size:70%;">∗</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S4.T9.8.4.2.2">
<span class="ltx_text" id="S4.T9.8.4.2.2.1" style="font-size:70%;"> PASS</span><sup class="ltx_sup" id="S4.T9.8.4.2.2.2"><span class="ltx_text" id="S4.T9.8.4.2.2.2.1" style="font-size:70%;">∗</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T9.9.5.3.3">
<span class="ltx_text" id="S4.T9.9.5.3.3.1" style="font-size:70%;"> Shaders1k</span><sup class="ltx_sup" id="S4.T9.9.5.3.3.2"><span class="ltx_text" id="S4.T9.9.5.3.3.2.1" style="font-size:70%;">∗</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S4.T9.9.5.5.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.9.5.5.2.1"><span class="ltx_text" id="S4.T9.9.5.5.2.1.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_border_t" id="S4.T9.9.5.5.2.2"></td>
<td class="ltx_td ltx_border_t" id="S4.T9.9.5.5.2.3"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T9.9.5.5.2.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.9.5.5.2.5"><span class="ltx_text" id="S4.T9.9.5.5.2.5.1" style="font-size:70%;">62.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.9.5.6.3">
<td class="ltx_td ltx_align_center" id="S4.T9.9.5.6.3.1"><span class="ltx_text" id="S4.T9.9.5.6.3.1.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.9.5.6.3.2"><span class="ltx_text" id="S4.T9.9.5.6.3.2.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td" id="S4.T9.9.5.6.3.3"></td>
<td class="ltx_td ltx_border_r" id="S4.T9.9.5.6.3.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T9.9.5.6.3.5"><span class="ltx_text" id="S4.T9.9.5.6.3.5.1" style="font-size:70%;">64.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.9.5.7.4">
<td class="ltx_td ltx_align_center" id="S4.T9.9.5.7.4.1"><span class="ltx_text" id="S4.T9.9.5.7.4.1.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.9.5.7.4.2"><span class="ltx_text" id="S4.T9.9.5.7.4.2.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.9.5.7.4.3"><span class="ltx_text" id="S4.T9.9.5.7.4.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_border_r" id="S4.T9.9.5.7.4.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T9.9.5.7.4.5"><span class="ltx_text ltx_font_bold" id="S4.T9.9.5.7.4.5.1" style="font-size:70%;">68.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.9.5.8.5">
<td class="ltx_td ltx_align_center" id="S4.T9.9.5.8.5.1"><span class="ltx_text" id="S4.T9.9.5.8.5.1.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.9.5.8.5.2"><span class="ltx_text" id="S4.T9.9.5.8.5.2.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td" id="S4.T9.9.5.8.5.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T9.9.5.8.5.4"><span class="ltx_text" id="S4.T9.9.5.8.5.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.9.5.8.5.5"><span class="ltx_text" id="S4.T9.9.5.8.5.5.1" style="font-size:70%;">67.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.9.5.9.6">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T9.9.5.9.6.1"><span class="ltx_text" id="S4.T9.9.5.9.6.1.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T9.9.5.9.6.2"><span class="ltx_text" id="S4.T9.9.5.9.6.2.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T9.9.5.9.6.3"><span class="ltx_text" id="S4.T9.9.5.9.6.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T9.9.5.9.6.4"><span class="ltx_text" id="S4.T9.9.5.9.6.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T9.9.5.9.6.5"><span class="ltx_text" id="S4.T9.9.5.9.6.5.1" style="font-size:70%;">67.9</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>PMG as Video Augmentation on Pre-training</h3>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Pre-training with both real videos and pseudo-motion videos.</h4>
<div class="ltx_para" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.1">As we have verified so far,
our framework enables efficient pre-training with static images.
This suggests that our proposed PMG can be also used
as a data augmentation method during pre-training.
<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.T9" title="In 4.3 Effect of the Number of Epochs, Data, and Categories in Image Datasets for Pre-training ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">9</span></a> compares the performance
when pre-training solely with real videos
and when pre-training with both real and pseudo-motion videos.
Notably, PMG Aug boosted model accuracy by up to 2%.
This suggests that synthetic motion, despite its differences to real video motion,
unintuitively contributes to the model’s performance by increasing diversity.</p>
</div>
<figure class="ltx_table" id="S4.T10">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T10.50.5.1" style="font-size:129%;">Table 10</span>: </span><span class="ltx_text" id="S4.T10.8.4" style="font-size:129%;">
<span class="ltx_text ltx_font_bold" id="S4.T10.8.4.1">Comparison with existing methods on HMDB51, UCF101, and Diving48.</span>
RV = Real Videos, SV = Synthetic Videos, RI = Real Images, SI = Synthetic Images,
SP = Supervised Pre-training,
FT data = Fine-tuning data.
<sup class="ltx_sup" id="S4.T10.8.4.2"><span class="ltx_text ltx_font_italic" id="S4.T10.8.4.2.1">†</span></sup> Results in our replicated experiments.
<sup class="ltx_sup" id="S4.T10.8.4.3"><span class="ltx_text ltx_font_italic" id="S4.T10.8.4.3.1">‡</span></sup> Reported in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib38" title="">38</a>]</cite>.
<sup class="ltx_sup" id="S4.T10.8.4.4">∗</sup> Herein, we refer to a combination of
ElderSim <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib31" title="">31</a>]</cite>,
SURREACT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib66" title="">66</a>]</cite>,
and PHAV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib58" title="">58</a>]</cite>
as SynAPT, as proposed in  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib38" title="">38</a>]</cite>.
<sup class="ltx_sup" id="S4.T10.8.4.5">§</sup> we report only the number of videos in SynAPT.
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T10.21">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T10.21.14.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T10.21.14.1.1" rowspan="2"><span class="ltx_text" id="S4.T10.21.14.1.1.1" style="font-size:70%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="3" id="S4.T10.21.14.1.2"><span class="ltx_text" id="S4.T10.21.14.1.2.1" style="font-size:70%;">Pre-training Setting</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S4.T10.21.14.1.3"><span class="ltx_text" id="S4.T10.21.14.1.3.1" style="font-size:70%;">Downstream Tasks</span></th>
</tr>
<tr class="ltx_tr" id="S4.T10.21.15.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T10.21.15.2.1"><span class="ltx_text" id="S4.T10.21.15.2.1.1" style="font-size:70%;">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T10.21.15.2.2"><span class="ltx_text" id="S4.T10.21.15.2.2.1" style="font-size:70%;">Data Source</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T10.21.15.2.3"><span class="ltx_text" id="S4.T10.21.15.2.3.1" style="font-size:70%;">#Data</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T10.21.15.2.4"><span class="ltx_text" id="S4.T10.21.15.2.4.1" style="font-size:70%;">UCF101</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T10.21.15.2.5"><span class="ltx_text" id="S4.T10.21.15.2.5.1" style="font-size:70%;">HMDB51</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T10.21.15.2.6"><span class="ltx_text" id="S4.T10.21.15.2.6.1" style="font-size:70%;">Diving48</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T10.9.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T10.9.1.2"><span class="ltx_text" id="S4.T10.9.1.2.1" style="font-size:70%;">from scratch (ViT-B)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.9.1.3"><span class="ltx_text" id="S4.T10.9.1.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.9.1.4"><span class="ltx_text" id="S4.T10.9.1.4.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.9.1.5"><span class="ltx_text" id="S4.T10.9.1.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.9.1.6"><span class="ltx_text" id="S4.T10.9.1.6.1" style="font-size:70%;">51.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.9.1.7"><span class="ltx_text" id="S4.T10.9.1.7.1" style="font-size:70%;">18.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.9.1.1">
<span class="ltx_text" id="S4.T10.9.1.1.1" style="font-size:70%;">17.9</span><sup class="ltx_sup" id="S4.T10.9.1.1.2"><span class="ltx_text ltx_font_italic" id="S4.T10.9.1.1.2.1" style="font-size:70%;">†</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S4.T10.21.16.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.21.16.1.1">
<span class="ltx_text" id="S4.T10.21.16.1.1.1" style="font-size:70%;">MoCo v3 (ViT-B) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T10.21.16.1.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib13" title="">13</a><span class="ltx_text" id="S4.T10.21.16.1.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T10.21.16.1.2"><span class="ltx_text" id="S4.T10.21.16.1.2.1" style="font-size:70%;">FT data</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.16.1.3"><span class="ltx_text" id="S4.T10.21.16.1.3.1" style="font-size:70%;">RV</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.21.16.1.4"><span class="ltx_text" id="S4.T10.21.16.1.4.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.16.1.5"><span class="ltx_text" id="S4.T10.21.16.1.5.1" style="font-size:70%;">81.7</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.16.1.6"><span class="ltx_text" id="S4.T10.21.16.1.6.1" style="font-size:70%;">39.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.16.1.7"><span class="ltx_text" id="S4.T10.21.16.1.7.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.10.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.10.2.2">
<span class="ltx_text" id="S4.T10.10.2.2.1" style="font-size:70%;">VideoMAE (ViT-B) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T10.10.2.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib64" title="">64</a><span class="ltx_text" id="S4.T10.10.2.2.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T10.10.2.3"><span class="ltx_text" id="S4.T10.10.2.3.1" style="font-size:70%;">FT data</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.10.2.4"><span class="ltx_text" id="S4.T10.10.2.4.1" style="font-size:70%;">RV</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.10.2.5"><span class="ltx_text" id="S4.T10.10.2.5.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.10.2.6"><span class="ltx_text" id="S4.T10.10.2.6.1" style="font-size:70%;">91.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.10.2.7"><span class="ltx_text" id="S4.T10.10.2.7.1" style="font-size:70%;">62.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.10.2.1">
<span class="ltx_text" id="S4.T10.10.2.1.1" style="font-size:70%;">79.3</span><sup class="ltx_sup" id="S4.T10.10.2.1.2"><span class="ltx_text ltx_font_italic" id="S4.T10.10.2.1.2.1" style="font-size:70%;">†</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S4.T10.21.17.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.21.17.2.1">
<span class="ltx_text" id="S4.T10.21.17.2.1.1" style="font-size:70%;">VideoMAE (ViT-B) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T10.21.17.2.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib64" title="">64</a><span class="ltx_text" id="S4.T10.21.17.2.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T10.21.17.2.2"><span class="ltx_text" id="S4.T10.21.17.2.2.1" style="font-size:70%;">Kinetics400</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.17.2.3"><span class="ltx_text" id="S4.T10.21.17.2.3.1" style="font-size:70%;">RV</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.21.17.2.4"><span class="ltx_text" id="S4.T10.21.17.2.4.1" style="font-size:70%;">260k</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.17.2.5"><span class="ltx_text" id="S4.T10.21.17.2.5.1" style="font-size:70%;">96.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.17.2.6"><span class="ltx_text" id="S4.T10.21.17.2.6.1" style="font-size:70%;">73.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.17.2.7"><span class="ltx_text" id="S4.T10.21.17.2.7.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.11.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.11.3.1">
<span class="ltx_text" id="S4.T10.11.3.1.1" style="font-size:70%;">VideoMAE (ViT-B)</span><sup class="ltx_sup" id="S4.T10.11.3.1.2"><span class="ltx_text ltx_font_italic" id="S4.T10.11.3.1.2.1" style="font-size:70%;">†</span></sup>
</th>
<td class="ltx_td ltx_align_center" id="S4.T10.11.3.2">
<span class="ltx_text" id="S4.T10.11.3.2.1" style="font-size:70%;">VPN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T10.11.3.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib33" title="">33</a><span class="ltx_text" id="S4.T10.11.3.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T10.11.3.3"><span class="ltx_text" id="S4.T10.11.3.3.1" style="font-size:70%;">RV</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.11.3.4"><span class="ltx_text" id="S4.T10.11.3.4.1" style="font-size:70%;">10k</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.11.3.5"><span class="ltx_text" id="S4.T10.11.3.5.1" style="font-size:70%;">64.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.11.3.6"><span class="ltx_text" id="S4.T10.11.3.6.1" style="font-size:70%;">30.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.11.3.7"><span class="ltx_text" id="S4.T10.11.3.7.1" style="font-size:70%;">17.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.21.18.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.21.18.3.1">
<span class="ltx_text" id="S4.T10.21.18.3.1.1" style="font-size:70%;">3D-ResNet50 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T10.21.18.3.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib27" title="">27</a><span class="ltx_text" id="S4.T10.21.18.3.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T10.21.18.3.2">
<span class="ltx_text" id="S4.T10.21.18.3.2.1" style="font-size:70%;">VPN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T10.21.18.3.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib33" title="">33</a><span class="ltx_text" id="S4.T10.21.18.3.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.18.3.3"><span class="ltx_text" id="S4.T10.21.18.3.3.1" style="font-size:70%;">SV</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.21.18.3.4"><span class="ltx_text" id="S4.T10.21.18.3.4.1" style="font-size:70%;">28k</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.18.3.5"><span class="ltx_text" id="S4.T10.21.18.3.5.1" style="font-size:70%;">49.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.18.3.6"><span class="ltx_text" id="S4.T10.21.18.3.6.1" style="font-size:70%;">23.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.18.3.7"><span class="ltx_text" id="S4.T10.21.18.3.7.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.12.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.12.4.2">
<span class="ltx_text" id="S4.T10.12.4.2.1" style="font-size:70%;">3D-ResNet50 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T10.12.4.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib27" title="">27</a><span class="ltx_text" id="S4.T10.12.4.2.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T10.12.4.1">
<span class="ltx_text" id="S4.T10.12.4.1.1" style="font-size:70%;">VPN</span><math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T10.12.4.1.m1.1"><semantics id="S4.T10.12.4.1.m1.1a"><mo id="S4.T10.12.4.1.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T10.12.4.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T10.12.4.1.m1.1b"><ci id="S4.T10.12.4.1.m1.1.1.cmml" xref="S4.T10.12.4.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T10.12.4.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T10.12.4.1.m1.1d">→</annotation></semantics></math><span class="ltx_text" id="S4.T10.12.4.1.2" style="font-size:70%;">Kinetics400</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T10.12.4.3"><span class="ltx_text" id="S4.T10.12.4.3.1" style="font-size:70%;">RV + SV</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.12.4.4"><span class="ltx_text" id="S4.T10.12.4.4.1" style="font-size:70%;">280k</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.12.4.5"><span class="ltx_text" id="S4.T10.12.4.5.1" style="font-size:70%;">89.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.12.4.6"><span class="ltx_text" id="S4.T10.12.4.6.1" style="font-size:70%;">61.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.12.4.7"><span class="ltx_text" id="S4.T10.12.4.7.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.16.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.13.5.1">
<span class="ltx_text" id="S4.T10.13.5.1.1" style="font-size:70%;">TimeSformer </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T10.13.5.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib10" title="">10</a><span class="ltx_text" id="S4.T10.13.5.1.3.2" style="font-size:70%;">]</span></cite><sup class="ltx_sup" id="S4.T10.13.5.1.4"><span class="ltx_text ltx_font_italic" id="S4.T10.13.5.1.4.1" style="font-size:70%;">‡</span></sup>
</th>
<td class="ltx_td ltx_align_center" id="S4.T10.15.7.3">
<span class="ltx_text" id="S4.T10.15.7.3.1" style="font-size:70%;">IN-21k</span><math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T10.14.6.2.m1.1"><semantics id="S4.T10.14.6.2.m1.1a"><mo id="S4.T10.14.6.2.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T10.14.6.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T10.14.6.2.m1.1b"><ci id="S4.T10.14.6.2.m1.1.1.cmml" xref="S4.T10.14.6.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T10.14.6.2.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T10.14.6.2.m1.1d">→</annotation></semantics></math><span class="ltx_text" id="S4.T10.15.7.3.2" style="font-size:70%;">SynAPT</span><sup class="ltx_sup" id="S4.T10.15.7.3.3"><span class="ltx_text" id="S4.T10.15.7.3.3.1" style="font-size:70%;">∗</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S4.T10.16.8.5"><span class="ltx_text" id="S4.T10.16.8.5.1" style="font-size:70%;">RI + SV</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.16.8.4">
<span class="ltx_text" id="S4.T10.16.8.4.1" style="font-size:70%;">150k</span><sup class="ltx_sup" id="S4.T10.16.8.4.2"><span class="ltx_text" id="S4.T10.16.8.4.2.1" style="font-size:70%;">§</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S4.T10.16.8.6"><span class="ltx_text" id="S4.T10.16.8.6.1" style="font-size:70%;">89.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.16.8.7"><span class="ltx_text" id="S4.T10.16.8.7.1" style="font-size:70%;">54.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.16.8.8"><span class="ltx_text" id="S4.T10.16.8.8.1" style="font-size:70%;">44.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.17.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.17.9.2">
<span class="ltx_text" id="S4.T10.17.9.2.1" style="font-size:70%;">PPMA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T10.17.9.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib84" title="">84</a><span class="ltx_text" id="S4.T10.17.9.2.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T10.17.9.1">
<span class="ltx_text" id="S4.T10.17.9.1.1" style="font-size:70%;">NH-Kinetics+SynAPT</span><sup class="ltx_sup" id="S4.T10.17.9.1.2"><span class="ltx_text" id="S4.T10.17.9.1.2.1" style="font-size:70%;">∗</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S4.T10.17.9.3"><span class="ltx_text" id="S4.T10.17.9.3.1" style="font-size:70%;">RV + SV</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.17.9.4"><span class="ltx_text" id="S4.T10.17.9.4.1" style="font-size:70%;">300k</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.17.9.5"><span class="ltx_text" id="S4.T10.17.9.5.1" style="font-size:70%;">92.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.17.9.6"><span class="ltx_text" id="S4.T10.17.9.6.1" style="font-size:70%;">71.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.17.9.7"><span class="ltx_text" id="S4.T10.17.9.7.1" style="font-size:70%;">64.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.21.19.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S4.T10.21.19.4.1">
<span class="ltx_text" id="S4.T10.21.19.4.1.1" style="font-size:70%;">MoSI (R-2D3D) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T10.21.19.4.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib30" title="">30</a><span class="ltx_text" id="S4.T10.21.19.4.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T10.21.19.4.2"><span class="ltx_text" id="S4.T10.21.19.4.2.1" style="font-size:70%;">FT data</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T10.21.19.4.3"><span class="ltx_text" id="S4.T10.21.19.4.3.1" style="font-size:70%;">RI</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T10.21.19.4.4"><span class="ltx_text" id="S4.T10.21.19.4.4.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T10.21.19.4.5"><span class="ltx_text" id="S4.T10.21.19.4.5.1" style="font-size:70%;">71.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T10.21.19.4.6"><span class="ltx_text" id="S4.T10.21.19.4.6.1" style="font-size:70%;">47.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T10.21.19.4.7"><span class="ltx_text" id="S4.T10.21.19.4.7.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.21.20.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.21.20.5.1">
<span class="ltx_text" id="S4.T10.21.20.5.1.1" style="font-size:70%;">MoSI (R(2+1)D) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T10.21.20.5.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib30" title="">30</a><span class="ltx_text" id="S4.T10.21.20.5.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T10.21.20.5.2"><span class="ltx_text" id="S4.T10.21.20.5.2.1" style="font-size:70%;">FT data</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.20.5.3"><span class="ltx_text" id="S4.T10.21.20.5.3.1" style="font-size:70%;">RI</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.21.20.5.4"><span class="ltx_text" id="S4.T10.21.20.5.4.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.20.5.5"><span class="ltx_text" id="S4.T10.21.20.5.5.1" style="font-size:70%;">82.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.20.5.6"><span class="ltx_text" id="S4.T10.21.20.5.6.1" style="font-size:70%;">51.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.20.5.7"><span class="ltx_text" id="S4.T10.21.20.5.7.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.18.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.18.10.1">
<span class="ltx_text" id="S4.T10.18.10.1.1" style="font-size:70%;">MoSI (ViT-B)</span><sup class="ltx_sup" id="S4.T10.18.10.1.2"><span class="ltx_text ltx_font_italic" id="S4.T10.18.10.1.2.1" style="font-size:70%;">†</span></sup>
</th>
<td class="ltx_td ltx_align_center" id="S4.T10.18.10.2"><span class="ltx_text" id="S4.T10.18.10.2.1" style="font-size:70%;">FT data</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.18.10.3"><span class="ltx_text" id="S4.T10.18.10.3.1" style="font-size:70%;">RI</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.18.10.4"><span class="ltx_text" id="S4.T10.18.10.4.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.18.10.5"><span class="ltx_text" id="S4.T10.18.10.5.1" style="font-size:70%;">48.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.18.10.6"><span class="ltx_text" id="S4.T10.18.10.6.1" style="font-size:70%;">27.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.18.10.7"><span class="ltx_text" id="S4.T10.18.10.7.1" style="font-size:70%;">14.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.19.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.19.11.1">
<span class="ltx_text" id="S4.T10.19.11.1.1" style="font-size:70%;">SP (ViT-B)</span><sup class="ltx_sup" id="S4.T10.19.11.1.2"><span class="ltx_text ltx_font_italic" id="S4.T10.19.11.1.2.1" style="font-size:70%;">†</span></sup>
</th>
<td class="ltx_td ltx_align_center" id="S4.T10.19.11.2"><span class="ltx_text" id="S4.T10.19.11.2.1" style="font-size:70%;">IN-21k</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.19.11.3"><span class="ltx_text" id="S4.T10.19.11.3.1" style="font-size:70%;">RI</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.19.11.4"><span class="ltx_text" id="S4.T10.19.11.4.1" style="font-size:70%;">14M</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.19.11.5"><span class="ltx_text" id="S4.T10.19.11.5.1" style="font-size:70%;">71.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.19.11.6"><span class="ltx_text" id="S4.T10.19.11.6.1" style="font-size:70%;">34.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.19.11.7"><span class="ltx_text" id="S4.T10.19.11.7.1" style="font-size:70%;">34.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.20.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.20.12.1">
<span class="ltx_text" id="S4.T10.20.12.1.1" style="font-size:70%;">SP (ViT-B)</span><sup class="ltx_sup" id="S4.T10.20.12.1.2"><span class="ltx_text ltx_font_italic" id="S4.T10.20.12.1.2.1" style="font-size:70%;">†</span></sup>
</th>
<td class="ltx_td ltx_align_center" id="S4.T10.20.12.2">
<span class="ltx_text" id="S4.T10.20.12.2.1" style="font-size:70%;">ExFractalDB-21k </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T10.20.12.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib34" title="">34</a><span class="ltx_text" id="S4.T10.20.12.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T10.20.12.3"><span class="ltx_text" id="S4.T10.20.12.3.1" style="font-size:70%;">SI</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.20.12.4"><span class="ltx_text" id="S4.T10.20.12.4.1" style="font-size:70%;">21M</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.20.12.5"><span class="ltx_text" id="S4.T10.20.12.5.1" style="font-size:70%;">61.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.20.12.6"><span class="ltx_text" id="S4.T10.20.12.6.1" style="font-size:70%;">20.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.20.12.7"><span class="ltx_text" id="S4.T10.20.12.7.1" style="font-size:70%;">28.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.21.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.21.13.1">
<span class="ltx_text" id="S4.T10.21.13.1.1" style="font-size:70%;">SP (ViT-B)</span><sup class="ltx_sup" id="S4.T10.21.13.1.2"><span class="ltx_text ltx_font_italic" id="S4.T10.21.13.1.2.1" style="font-size:70%;">†</span></sup>
</th>
<td class="ltx_td ltx_align_center" id="S4.T10.21.13.2"><span class="ltx_text" id="S4.T10.21.13.2.1" style="font-size:70%;">VisualAtom-21k</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.13.3"><span class="ltx_text" id="S4.T10.21.13.3.1" style="font-size:70%;">SI</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.21.13.4"><span class="ltx_text" id="S4.T10.21.13.4.1" style="font-size:70%;">21M</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.13.5"><span class="ltx_text" id="S4.T10.21.13.5.1" style="font-size:70%;">58.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.13.6"><span class="ltx_text" id="S4.T10.21.13.6.1" style="font-size:70%;">20.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.13.7"><span class="ltx_text" id="S4.T10.21.13.7.1" style="font-size:70%;">21.4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.21.21.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T10.21.21.6.1" rowspan="3"><span class="ltx_text" id="S4.T10.21.21.6.1.1" style="font-size:70%;">Ours (ViT-B)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.21.21.6.2"><span class="ltx_text" id="S4.T10.21.21.6.2.1" style="font-size:70%;">frames from FT data</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.21.21.6.3"><span class="ltx_text" id="S4.T10.21.21.6.3.1" style="font-size:70%;">RI</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T10.21.21.6.4"><span class="ltx_text" id="S4.T10.21.21.6.4.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.21.21.6.5"><span class="ltx_text" id="S4.T10.21.21.6.5.1" style="font-size:70%;">87.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.21.21.6.6"><span class="ltx_text" id="S4.T10.21.21.6.6.1" style="font-size:70%;">55.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.21.21.6.7"><span class="ltx_text" id="S4.T10.21.21.6.7.1" style="font-size:70%;">68.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.21.22.7">
<td class="ltx_td ltx_align_center" id="S4.T10.21.22.7.1"><span class="ltx_text" id="S4.T10.21.22.7.1.1" style="font-size:70%;">PASS</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.22.7.2"><span class="ltx_text" id="S4.T10.21.22.7.2.1" style="font-size:70%;">RI</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T10.21.22.7.3"><span class="ltx_text" id="S4.T10.21.22.7.3.1" style="font-size:70%;">100k</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.22.7.4"><span class="ltx_text" id="S4.T10.21.22.7.4.1" style="font-size:70%;">89.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.22.7.5"><span class="ltx_text ltx_font_bold" id="S4.T10.21.22.7.5.1" style="font-size:70%;">60.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.22.7.6"><span class="ltx_text" id="S4.T10.21.22.7.6.1" style="font-size:70%;">69.2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.21.23.8">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T10.21.23.8.1"><span class="ltx_text" id="S4.T10.21.23.8.1.1" style="font-size:70%;">Shaders1k</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T10.21.23.8.2"><span class="ltx_text" id="S4.T10.21.23.8.2.1" style="font-size:70%;">SI</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T10.21.23.8.3"><span class="ltx_text" id="S4.T10.21.23.8.3.1" style="font-size:70%;">100k</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T10.21.23.8.4"><span class="ltx_text ltx_font_bold" id="S4.T10.21.23.8.4.1" style="font-size:70%;">89.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T10.21.23.8.5"><span class="ltx_text" id="S4.T10.21.23.8.5.1" style="font-size:70%;">59.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T10.21.23.8.6"><span class="ltx_text ltx_font_bold" id="S4.T10.21.23.8.6.1" style="font-size:70%;">72.3</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Can we combine image datasets with video datasets to train our framework?</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p" id="S4.SS4.SSS2.p1.1">Next, we use image datasets as well as sources of PMG Aug during pre-training.
For the image datasets (PASS and Shaders1k), we randomly sampled 10k images as input.
<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.T9" title="In 4.3 Effect of the Number of Epochs, Data, and Categories in Image Datasets for Pre-training ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">9</span></a> compares
the performance of the models pre-trained on HMDB51,
PASS, and Shaders1k.
The results show that using both image and video datasets
improved the model’s performance.
Particularly, the combination of HMDB51 and PASS
enhanced the accuracy on HMDB51 by 5.4%
compared to pre-training with only real videos.
This indicates that using PMG Aug resolves the problem of insufficient data quantity
during VideoMAE pre-training.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Comparison to Existing Methods</h3>
<section class="ltx_subsubsection" id="S4.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.1 </span>Comparison to methods using HMDB51, UCF101 and Diving48.</h4>
<div class="ltx_para" id="S4.SS5.SSS1.p1">
<p class="ltx_p" id="S4.SS5.SSS1.p1.1">The upper part of <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.T10" title="In 4.4.1 Pre-training with both real videos and pseudo-motion videos. ‣ 4.4 PMG as Video Augmentation on Pre-training ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">10</span></a> presents
the performance of existing works which pre-train using the
HMDB51, UCF101, and Diving48 datasets.
Existing methods like 3D-ResNet with VPN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib33" title="">33</a>]</cite>,
TimeSformer with SynAPT, and PPMA
have improved model performance compared to training the model from scratch.
However, they still require real data, causing issues as mentioned.
In contrast, our framework,
despite using fewer samples which are also synthetic,
achieves comparable performance on UCF101
and better performance on Diving48.</p>
</div>
<div class="ltx_para" id="S4.SS5.SSS1.p2">
<p class="ltx_p" id="S4.SS5.SSS1.p2.1">We also compared with pre-training methods which only use static images
(the lower part of <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.T10" title="In 4.4.1 Pre-training with both real videos and pseudo-motion videos. ‣ 4.4 PMG as Video Augmentation on Pre-training ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">10</span></a>).
MoSI works on CNN-based architectures,
but it fails to pre-train a ViT model because of the lack of diversity in generated
videos.
Supervised pre-training (SP) on IN-21k, ExFractalDB-21k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib34" title="">34</a>]</cite>
and VisualAtom-21k
slightly improves the performance in comparison with ’from scratch’.
However, our framework significantly surpasses that performance in both settings,
when using real images and when using synthetic images.</p>
</div>
<div class="ltx_para" id="S4.SS5.SSS1.p3">
<p class="ltx_p" id="S4.SS5.SSS1.p3.1">Note that VideoMAE pre-trained with VPN has low accuracy on downstream classification tasks,
which suggests that VPN does not work well with VideoMAE when learning spatio-temporal features.
We consider this is because VPN videos have temporal continuity,
but do not possess clear correspondence of patches between frames
(e.g. edges are ambiguous, and regions suddenly disappear or appear).
We believe this characteristic is key for effective VideoMAE pre-training.
In <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.SS7" title="4.7 What Does VideoMAE Learn from Pre-training with Videos? ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.7</span></a>,
we further experiment to support this hypothesis.</p>
</div>
<figure class="ltx_table" id="S4.T11">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T11.14.2.1" style="font-size:129%;">Table 11</span>: </span><span class="ltx_text" id="S4.T11.2.1" style="font-size:129%;">
<span class="ltx_text ltx_font_bold" id="S4.T11.2.1.1">Results on SynAPT benchmark.</span>
<sup class="ltx_sup" id="S4.T11.2.1.2"><span class="ltx_text ltx_font_italic" id="S4.T11.2.1.2.1">†</span></sup> Results reported in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib38" title="">38</a>]</cite>.
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T11.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T11.3.2.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T11.3.2.1.1" rowspan="2"><span class="ltx_text" id="S4.T11.3.2.1.1.1" style="font-size:70%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T11.3.2.1.2"><span class="ltx_text" id="S4.T11.3.2.1.2.1" style="font-size:70%;">Pre-training</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="S4.T11.3.2.1.3"><span class="ltx_text" id="S4.T11.3.2.1.3.1" style="font-size:70%;">Downstream Tasks</span></td>
</tr>
<tr class="ltx_tr" id="S4.T11.3.3.2">
<td class="ltx_td ltx_align_center" id="S4.T11.3.3.2.1"><span class="ltx_text" id="S4.T11.3.3.2.1.1" style="font-size:70%;">Dataset</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.3.3.2.2"><span class="ltx_text" id="S4.T11.3.3.2.2.1" style="font-size:70%;">#data</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T11.3.3.2.3"><span class="ltx_text" id="S4.T11.3.3.2.3.1" style="font-size:70%;"> labels</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.3.3.2.4"><span class="ltx_text" id="S4.T11.3.3.2.4.1" style="font-size:70%;">UCF101</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.3.3.2.5"><span class="ltx_text" id="S4.T11.3.3.2.5.1" style="font-size:70%;">HMDB51</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.3.3.2.6"><span class="ltx_text" id="S4.T11.3.3.2.6.1" style="font-size:70%;">MiniSSV2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.3.3.2.7"><span class="ltx_text" id="S4.T11.3.3.2.7.1" style="font-size:70%;">Diving48</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.3.3.2.8"><span class="ltx_text" id="S4.T11.3.3.2.8.1" style="font-size:70%;">IkeaFA</span></td>
<td class="ltx_td ltx_align_center" id="S4.T11.3.3.2.9"><span class="ltx_text" id="S4.T11.3.3.2.9.1" style="font-size:70%;">UAV-H</span></td>
</tr>
<tr class="ltx_tr" id="S4.T11.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.1.1">
<span class="ltx_text" id="S4.T11.3.1.1.1" style="font-size:70%;">TimeSformer</span><sup class="ltx_sup" id="S4.T11.3.1.1.2"><span class="ltx_text ltx_font_italic" id="S4.T11.3.1.1.2.1" style="font-size:70%;">†</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.1.2">
<table class="ltx_tabular ltx_align_middle" id="S4.T11.3.1.2.1">
<tr class="ltx_tr" id="S4.T11.3.1.2.1.1">
<td class="ltx_td ltx_align_center" id="S4.T11.3.1.2.1.1.1"><span class="ltx_text" id="S4.T11.3.1.2.1.1.1.1" style="font-size:70%;">IN-21k</span></td>
</tr>
<tr class="ltx_tr" id="S4.T11.3.1.2.1.2">
<td class="ltx_td ltx_align_center" id="S4.T11.3.1.2.1.2.1"><span class="ltx_text" id="S4.T11.3.1.2.1.2.1.1" style="font-size:70%;">+Synthetic</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.1.3"><span class="ltx_text" id="S4.T11.3.1.3.1" style="font-size:70%;">150k</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.3.1.4"><span class="ltx_text" id="S4.T11.3.1.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.1.5"><span class="ltx_text" id="S4.T11.3.1.5.1" style="font-size:70%;">89.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.1.6"><span class="ltx_text" id="S4.T11.3.1.6.1" style="font-size:70%;">54.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.1.7"><span class="ltx_text" id="S4.T11.3.1.7.1" style="font-size:70%;">51.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.1.8"><span class="ltx_text" id="S4.T11.3.1.8.1" style="font-size:70%;">44.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.1.9"><span class="ltx_text" id="S4.T11.3.1.9.1" style="font-size:70%;">63.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.1.10"><span class="ltx_text" id="S4.T11.3.1.10.1" style="font-size:70%;">25.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T11.3.4.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.4.3.1">
<span class="ltx_text" id="S4.T11.3.4.3.1.1" style="font-size:70%;">PPMA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T11.3.4.3.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib84" title="">84</a><span class="ltx_text" id="S4.T11.3.4.3.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.4.3.2">
<table class="ltx_tabular ltx_align_middle" id="S4.T11.3.4.3.2.1">
<tr class="ltx_tr" id="S4.T11.3.4.3.2.1.1">
<td class="ltx_td ltx_align_center" id="S4.T11.3.4.3.2.1.1.1"><span class="ltx_text" id="S4.T11.3.4.3.2.1.1.1.1" style="font-size:70%;">NH-Kinetics</span></td>
</tr>
<tr class="ltx_tr" id="S4.T11.3.4.3.2.1.2">
<td class="ltx_td ltx_align_center" id="S4.T11.3.4.3.2.1.2.1"><span class="ltx_text" id="S4.T11.3.4.3.2.1.2.1.1" style="font-size:70%;">+Synthetic</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.4.3.3"><span class="ltx_text" id="S4.T11.3.4.3.3.1" style="font-size:70%;">300k</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T11.3.4.3.4"><span class="ltx_text" id="S4.T11.3.4.3.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.4.3.5"><span class="ltx_text ltx_font_bold" id="S4.T11.3.4.3.5.1" style="font-size:70%;">92.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.4.3.6"><span class="ltx_text ltx_font_bold" id="S4.T11.3.4.3.6.1" style="font-size:70%;">71.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.4.3.7"><span class="ltx_text" id="S4.T11.3.4.3.7.1" style="font-size:70%;">67.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.4.3.8"><span class="ltx_text" id="S4.T11.3.4.3.8.1" style="font-size:70%;">64.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.4.3.9"><span class="ltx_text ltx_font_bold" id="S4.T11.3.4.3.9.1" style="font-size:70%;">67.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.4.3.10"><span class="ltx_text" id="S4.T11.3.4.3.10.1" style="font-size:70%;">38.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T11.3.5.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.5.4.1"><span class="ltx_text" id="S4.T11.3.5.4.1.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.5.4.2"><span class="ltx_text" id="S4.T11.3.5.4.2.1" style="font-size:70%;">no extra data</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.5.4.3"><span class="ltx_text" id="S4.T11.3.5.4.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T11.3.5.4.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.5.4.5"><span class="ltx_text" id="S4.T11.3.5.4.5.1" style="font-size:70%;">87.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.5.4.6"><span class="ltx_text" id="S4.T11.3.5.4.6.1" style="font-size:70%;">55.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.5.4.7"><span class="ltx_text ltx_font_bold" id="S4.T11.3.5.4.7.1" style="font-size:70%;">69.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.5.4.8"><span class="ltx_text" id="S4.T11.3.5.4.8.1" style="font-size:70%;">68.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.5.4.9"><span class="ltx_text" id="S4.T11.3.5.4.9.1" style="font-size:70%;">61.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T11.3.5.4.10"><span class="ltx_text" id="S4.T11.3.5.4.10.1" style="font-size:70%;">36.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T11.3.6.5">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T11.3.6.5.1"><span class="ltx_text" id="S4.T11.3.6.5.1.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T11.3.6.5.2"><span class="ltx_text" id="S4.T11.3.6.5.2.1" style="font-size:70%;">Shaders1k</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T11.3.6.5.3"><span class="ltx_text" id="S4.T11.3.6.5.3.1" style="font-size:70%;">100k</span></td>
<td class="ltx_td ltx_border_bb ltx_border_r ltx_border_t" id="S4.T11.3.6.5.4"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T11.3.6.5.5"><span class="ltx_text" id="S4.T11.3.6.5.5.1" style="font-size:70%;">89.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T11.3.6.5.6"><span class="ltx_text" id="S4.T11.3.6.5.6.1" style="font-size:70%;">59.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T11.3.6.5.7"><span class="ltx_text" id="S4.T11.3.6.5.7.1" style="font-size:70%;">68.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T11.3.6.5.8"><span class="ltx_text ltx_font_bold" id="S4.T11.3.6.5.8.1" style="font-size:70%;">72.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T11.3.6.5.9"><span class="ltx_text" id="S4.T11.3.6.5.9.1" style="font-size:70%;">60.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T11.3.6.5.10"><span class="ltx_text ltx_font_bold" id="S4.T11.3.6.5.10.1" style="font-size:70%;">40.0</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T13">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S4.T13.4" style="width:225.5pt;">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.T13.4.14.2.1" style="font-size:129%;">Table 12</span>: </span><span class="ltx_text" id="S4.T13.2.2.1" style="font-size:129%;">
<span class="ltx_text ltx_font_bold" id="S4.T13.2.2.1.1">Results on K400.</span>
<sup class="ltx_sup" id="S4.T13.2.2.1.2"><span class="ltx_text ltx_font_italic" id="S4.T13.2.2.1.2.1">†</span></sup> Results from  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib64" title="">64</a>]</cite>.
We use 100k images from Shaders1k.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T13.4.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T13.4.4.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T13.4.4.3.1.1" rowspan="2"><span class="ltx_text" id="S4.T13.4.4.3.1.1.1" style="font-size:70%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T13.4.4.3.1.2"><span class="ltx_text" id="S4.T13.4.4.3.1.2.1" style="font-size:70%;">Pre-training data</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T13.4.4.3.1.3"><span class="ltx_text" id="S4.T13.4.4.3.1.3.1" style="font-size:70%;">Kinetics400</span></th>
</tr>
<tr class="ltx_tr" id="S4.T13.4.4.4.2">
<td class="ltx_td ltx_align_center" id="S4.T13.4.4.4.2.1"><span class="ltx_text" id="S4.T13.4.4.4.2.1.1" style="font-size:70%;">Data</span></td>
<td class="ltx_td ltx_align_center" id="S4.T13.4.4.4.2.2"><span class="ltx_text" id="S4.T13.4.4.4.2.2.1" style="font-size:70%;">Acc@1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T13.4.4.4.2.3"><span class="ltx_text" id="S4.T13.4.4.4.2.3.1" style="font-size:70%;">Acc@5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T13.3.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T13.3.3.1.1">
<span class="ltx_text" id="S4.T13.3.3.1.1.1" style="font-size:70%;">from scratch</span><sup class="ltx_sup" id="S4.T13.3.3.1.1.2"><span class="ltx_text ltx_font_italic" id="S4.T13.3.3.1.1.2.1" style="font-size:70%;">†</span></sup>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T13.3.3.1.2"><span class="ltx_text" id="S4.T13.3.3.1.2.1" style="font-size:70%;">-</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T13.3.3.1.3"><span class="ltx_text" id="S4.T13.3.3.1.3.1" style="font-size:70%;">68.8</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T13.3.3.1.4"><span class="ltx_text" id="S4.T13.3.3.1.4.1" style="font-size:70%;">-</span></th>
</tr>
<tr class="ltx_tr" id="S4.T13.4.4.2">
<td class="ltx_td ltx_align_center" id="S4.T13.4.4.2.1">
<span class="ltx_text" id="S4.T13.4.4.2.1.1" style="font-size:70%;">VideoMAE</span><sup class="ltx_sup" id="S4.T13.4.4.2.1.2"><span class="ltx_text ltx_font_italic" id="S4.T13.4.4.2.1.2.1" style="font-size:70%;">†</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="S4.T13.4.4.2.2"><span class="ltx_text" id="S4.T13.4.4.2.2.1" style="font-size:70%;">K400</span></td>
<td class="ltx_td ltx_align_center" id="S4.T13.4.4.2.3"><span class="ltx_text ltx_font_bold" id="S4.T13.4.4.2.3.1" style="font-size:70%;">81.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T13.4.4.2.4"><span class="ltx_text ltx_font_bold" id="S4.T13.4.4.2.4.1" style="font-size:70%;">95.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T13.4.4.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_bb ltx_border_t" id="S4.T13.4.4.5.3.1" rowspan="2"><span class="ltx_text" id="S4.T13.4.4.5.3.1.1" style="font-size:70%;">Ours</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T13.4.4.5.3.2"><span class="ltx_text" id="S4.T13.4.4.5.3.2.1" style="font-size:70%;">frames from K400</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T13.4.4.5.3.3"><span class="ltx_text" id="S4.T13.4.4.5.3.3.1" style="font-size:70%;">74.8</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T13.4.4.5.3.4"><span class="ltx_text" id="S4.T13.4.4.5.3.4.1" style="font-size:70%;">92.0</span></th>
</tr>
<tr class="ltx_tr" id="S4.T13.4.4.6.4">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T13.4.4.6.4.1"><span class="ltx_text" id="S4.T13.4.4.6.4.1.1" style="font-size:70%;">Shaders1k</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T13.4.4.6.4.2"><span class="ltx_text" id="S4.T13.4.4.6.4.2.1" style="font-size:70%;">74.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T13.4.4.6.4.3"><span class="ltx_text" id="S4.T13.4.4.6.4.3.1" style="font-size:70%;">91.9</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S4.T13.fig1" style="width:203.8pt;">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.T13.fig1.3.1.1" style="font-size:129%;">Table 13</span>: </span><span class="ltx_text ltx_font_bold" id="S4.T13.fig1.4.2" style="font-size:129%;">Comparison of accuracy on HMDB51 and UCF101 when using subsets grouped by frame difference.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T13.fig1.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T13.fig1.5.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T13.fig1.5.1.1.1"><span class="ltx_text" id="S4.T13.fig1.5.1.1.1.1" style="font-size:70%;">Frame difference</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T13.fig1.5.1.1.2"><span class="ltx_text" id="S4.T13.fig1.5.1.1.2.1" style="font-size:70%;">HMDB51</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T13.fig1.5.1.1.3"><span class="ltx_text" id="S4.T13.fig1.5.1.1.3.1" style="font-size:70%;">UCF101</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T13.fig1.5.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T13.fig1.5.2.1.1"><span class="ltx_text" id="S4.T13.fig1.5.2.1.1.1" style="font-size:70%;">(i) Large</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T13.fig1.5.2.1.2"><span class="ltx_text" id="S4.T13.fig1.5.2.1.2.1" style="font-size:70%;">32.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T13.fig1.5.2.1.3"><span class="ltx_text" id="S4.T13.fig1.5.2.1.3.1" style="font-size:70%;">68.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T13.fig1.5.3.2">
<td class="ltx_td ltx_align_center" id="S4.T13.fig1.5.3.2.1"><span class="ltx_text" id="S4.T13.fig1.5.3.2.1.1" style="font-size:70%;">(ii) Medium</span></td>
<td class="ltx_td ltx_align_center" id="S4.T13.fig1.5.3.2.2"><span class="ltx_text ltx_font_bold" id="S4.T13.fig1.5.3.2.2.1" style="font-size:70%;">33.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T13.fig1.5.3.2.3"><span class="ltx_text ltx_font_bold" id="S4.T13.fig1.5.3.2.3.1" style="font-size:70%;">71.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T13.fig1.5.4.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T13.fig1.5.4.3.1"><span class="ltx_text" id="S4.T13.fig1.5.4.3.1.1" style="font-size:70%;">(iii) Small</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T13.fig1.5.4.3.2"><span class="ltx_text" id="S4.T13.fig1.5.4.3.2.1" style="font-size:70%;">32.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T13.fig1.5.4.3.3"><span class="ltx_text" id="S4.T13.fig1.5.4.3.3.1" style="font-size:70%;">69.3</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.2 </span>Comparison on SynAPT benchmark.</h4>
<div class="ltx_para" id="S4.SS5.SSS2.p1">
<p class="ltx_p" id="S4.SS5.SSS2.p1.1">Following the SynAPT benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib38" title="">38</a>]</cite>,
we evaluate using the following six datasets:
UCF101, HMDB51, MiniSSV2, Diving48, IkeaFA, and UAV-H.
<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.T11" title="In 4.5.1 Comparison to methods using HMDB51, UCF101 and Diving48. ‣ 4.5 Comparison to Existing Methods ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">11</span></a> presents the results.
Using only synthetic images,
our proposed framework partially surpasses some of the results of existing works utilizing
real videos and action labels.
Our framework is inferior to PPMA on UCF101, HMDB51, and IkeaFA.
This is because these datasets have less data than others.
PPMA leverages the 150 action labels in the video datasets for pre-training,
therefore having the advantage of learning action features from a small number of videos
during fine-tuning.
On the other hand, our framework, not having these labels beforehand, struggled to learn
meaningful features with fewer labeled data.
However, our framework shows better performance
on less biased datasets like MiniSSV2, Diving48, and UAV-H.
This suggests that scene and object biases are mitigated when using our generated synthetic
videos.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS5.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.3 </span>Results on K400.</h4>
<div class="ltx_para" id="S4.SS5.SSS3.p1">
<p class="ltx_p" id="S4.SS5.SSS3.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.T13" title="In 4.5.1 Comparison to methods using HMDB51, UCF101 and Diving48. ‣ 4.5 Comparison to Existing Methods ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">13</span></a> shows the comparison of our framework with VideoMAE on K400.
Although our framework outperforms the model ’from scratch’,
it falls short of the performance of VideoMAE with real videos.
This shortfall is attributed to the limited diversity of pseudo-motion videos
generated by PMG,
especially when compared to the vast variety found in large-scale datasets.
We understand our shortcoming here, but increasing the diversity of generated videos
may close this gap.</p>
</div>
<figure class="ltx_figure" id="S4.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="340" id="S4.F4.sf1.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.sf1.4.1.1" style="font-size:129%;">(a)</span> </span><span class="ltx_text" id="S4.F4.sf1.5.2" style="font-size:129%;">Accuracy transition on HMDB51</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="340" id="S4.F4.sf2.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.sf2.4.1.1" style="font-size:129%;">(b)</span> </span><span class="ltx_text" id="S4.F4.sf2.5.2" style="font-size:129%;">Accuracy transition on UCF101</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.4.1.1" style="font-size:129%;">Figure 4</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F4.5.2" style="font-size:129%;">Performance when the number of video data for finetuning is limited.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Performance When the Number Data for Fine-tuning is Limited</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">In previous experiments,
the full set of video datasets for fine-tuning was available.
Under these conditions, pre-training with all the videos for fine-tuning
yielded better performance than our framework.
However, for video datasets, there is often a limited amount of training samples to fine-tune with.
To assess the effectiveness of our framework in such cases,
we sampled <math alttext="\{1,5,10,25,50\}" class="ltx_Math" display="inline" id="S4.SS6.p1.1.m1.5"><semantics id="S4.SS6.p1.1.m1.5a"><mrow id="S4.SS6.p1.1.m1.5.6.2" xref="S4.SS6.p1.1.m1.5.6.1.cmml"><mo id="S4.SS6.p1.1.m1.5.6.2.1" stretchy="false" xref="S4.SS6.p1.1.m1.5.6.1.cmml">{</mo><mn id="S4.SS6.p1.1.m1.1.1" xref="S4.SS6.p1.1.m1.1.1.cmml">1</mn><mo id="S4.SS6.p1.1.m1.5.6.2.2" xref="S4.SS6.p1.1.m1.5.6.1.cmml">,</mo><mn id="S4.SS6.p1.1.m1.2.2" xref="S4.SS6.p1.1.m1.2.2.cmml">5</mn><mo id="S4.SS6.p1.1.m1.5.6.2.3" xref="S4.SS6.p1.1.m1.5.6.1.cmml">,</mo><mn id="S4.SS6.p1.1.m1.3.3" xref="S4.SS6.p1.1.m1.3.3.cmml">10</mn><mo id="S4.SS6.p1.1.m1.5.6.2.4" xref="S4.SS6.p1.1.m1.5.6.1.cmml">,</mo><mn id="S4.SS6.p1.1.m1.4.4" xref="S4.SS6.p1.1.m1.4.4.cmml">25</mn><mo id="S4.SS6.p1.1.m1.5.6.2.5" xref="S4.SS6.p1.1.m1.5.6.1.cmml">,</mo><mn id="S4.SS6.p1.1.m1.5.5" xref="S4.SS6.p1.1.m1.5.5.cmml">50</mn><mo id="S4.SS6.p1.1.m1.5.6.2.6" stretchy="false" xref="S4.SS6.p1.1.m1.5.6.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.1.m1.5b"><set id="S4.SS6.p1.1.m1.5.6.1.cmml" xref="S4.SS6.p1.1.m1.5.6.2"><cn id="S4.SS6.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS6.p1.1.m1.1.1">1</cn><cn id="S4.SS6.p1.1.m1.2.2.cmml" type="integer" xref="S4.SS6.p1.1.m1.2.2">5</cn><cn id="S4.SS6.p1.1.m1.3.3.cmml" type="integer" xref="S4.SS6.p1.1.m1.3.3">10</cn><cn id="S4.SS6.p1.1.m1.4.4.cmml" type="integer" xref="S4.SS6.p1.1.m1.4.4">25</cn><cn id="S4.SS6.p1.1.m1.5.5.cmml" type="integer" xref="S4.SS6.p1.1.m1.5.5">50</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.1.m1.5c">\{1,5,10,25,50\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p1.1.m1.5d">{ 1 , 5 , 10 , 25 , 50 }</annotation></semantics></math> videos per category from HMDB51 and UCF101, respectively,
and compared the performance of our framework with VideoMAE using real videos.
<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.F4" title="In 4.5.3 Results on K400. ‣ 4.5 Comparison to Existing Methods ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a> presents the results.
The model pre-trained by our framework
shows higher performance compared to the model pre-trained by VideoMAE using real data.
This underscores the efficacy of our framework where the available data is limited.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>What Does VideoMAE Learn from Pre-training with Videos?</h3>
<div class="ltx_para" id="S4.SS7.p1">
<p class="ltx_p" id="S4.SS7.p1.1">Finally, to support our hypothesis that
VideoMAE learns the correspondence of patches between frames,
we conducted a simple experiment.
Here, we assume that a larger frame difference in a video makes
it difficult to capture this correspondence,
for instance, due to extreme camera motion.
Based on this, we made three subsets from HMDB51 and UCF101 depending on the frame difference;
(i) videos having the top 50% average frame difference
(ii) videos ranging from the 25th to the 75th percentile in
average frame difference,
(iii) videos having the bottom 50% average frame difference.
We then use each of these subsets for pre-training, then fine-tune on the full set.
The results are shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#S4.T13" title="In 4.5.1 Comparison to methods using HMDB51, UCF101 and Diving48. ‣ 4.5 Comparison to Existing Methods ‣ 4 Experiments ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">13</span></a>.
Models that are pre-trained on (i) and (iii)
performed worse than those pre-trained on (ii).
This lends support to our hypothesis regarding what VideoMAE learns.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we introduced a self-supervised framework
for pre-training video transformers solely with synthetic images.
Our framework eliminates the costs associated with collecting video data
and addresses concerns related to privacy, licensing, and biases inherent in real data.
Our experiments have demonstrated that our framework
not only outperforms existing pre-training methods with static images
but also partially outperforms existing works with synthetic videos.
Further analysis unveiled segments of what masked autoencoders learn from videos.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Limitations</span>
Our framework is inferior to pre-training with large-scale datasets like K400.
We consider this to be due to a lack of fine-grained motion patterns
compared to real videos.
Our framework depends on hand-crafted image transformations and
applies them to images globally,
pseudo-motion videos do not have flexible motion patterns.
Additionally, our framework does not learn high-level semantic features,
because we utilized VideoMAE’s focus on capturing low-level features.
Therefore, it is challenging to extend our framework to
other tasks like video-text retrieval,
without additional training or extra labeled data.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B., Vijayanarasimhan, S.: Youtube-8m: A large-scale video classification benchmark. arXiv preprint arXiv:1609.08675 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Ahsan, U., Madhok, R., Essa, I.: Video jigsaw: Unsupervised learning of spatiotemporal context for video action recognition. In: 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). pp. 179–189. IEEE (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Arnab, A., Dehghani, M., Heigold, G., Sun, C., Lučić, M., Schmid, C.: Vivit: A video vision transformer. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 6836–6846 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Asano, Y.M., Rupprecht, C., Zisserman, A., Vedaldi, A.: Pass: An imagenet replacement for self-supervised pretraining without humans. arXiv preprint arXiv:2109.13228 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Bandara, W.G.C., Patel, N., Gholami, A., Nikkhah, M., Agrawal, M., Patel, V.M.: Adamae: Adaptive masking for efficient spatiotemporal learning with masked autoencoders. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 14507–14517 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Bao, H., Dong, L., Piao, S., Wei, F.: Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Baradad, M., Chen, R., Wulff, J., Wang, T., Feris, R., Torralba, A., Isola, P.: Procedural image programs for representation learning. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib7.1.1">35</span>, 6450–6462 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Baradad Jurjo, M., Wulff, J., Wang, T., Isola, P., Torralba, A.: Learning to see by looking at noise. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib8.1.1">34</span>, 2556–2569 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Benaim, S., Ephrat, A., Lang, O., Mosseri, I., Freeman, W.T., Rubinstein, M., Irani, M., Dekel, T.: Speednet: Learning the speediness in videos. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9922–9931 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Bertasius, G., Wang, H., Torresani, L.: Is space-time attention all you need for video understanding? In: ICML. vol. 2, p. 4 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Buolamwini, J., Gebru, T.: Gender shades: Intersectional accuracy disparities in commercial gender classification. In: Conference on fairness, accountability and transparency. pp. 77–91. PMLR (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Caba Heilbron, F., Escorcia, V., Ghanem, B., Carlos Niebles, J.: Activitynet: A large-scale video benchmark for human activity understanding. In: Proceedings of the ieee conference on computer vision and pattern recognition. pp. 961–970 (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Chen, X., Xie, S., He, K.: An empirical study of training self-supervised vision transformers. arXiv preprint arXiv:2104.02057 <span class="ltx_text ltx_font_bold" id="bib.bib13.1.1">2</span>(5),  6 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Choi, J., Gao, C., Messou, J.C., Huang, J.B.: Why can’t i dance in the mall? learning to mitigate scene bias in action recognition. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib14.1.1">32</span> (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automated data augmentation with a reduced search space. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. pp. 702–703 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
De Souza, C., Gaidon, A., Cabon, Y., López, A.M.: Procedural generation of videos to train deep action recognition networks. corr. arXiv preprint arXiv:1612.00881 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp. 248–255. Ieee (2009)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Feichtenhofer, C., Fan, H., Li, Y., He, K.: Masked autoencoders as spatiotemporal learners. arXiv preprint arXiv:2205.09113 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Feichtenhofer, C., Fan, H., Malik, J., He, K.: Slowfast networks for video recognition. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 6202–6211 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Feichtenhofer, C., Fan, H., Xiong, B., Girshick, R., He, K.: A large-scale study on unsupervised spatiotemporal representation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3299–3309 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Fernando, B., Bilen, H., Gavves, E., Gould, S.: Self-supervised video representation learning with odd-one-out networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3636–3645 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Fischer, P., Dosovitskiy, A., Ilg, E., Häusser, P., Hazırbaş, C., Golkov, V., Van der Smagt, P., Cremers, D., Brox, T.: Flownet: Learning optical flow with convolutional networks. arXiv preprint arXiv:1504.06852 (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Ghadiyaram, D., Tran, D., Mahajan, D.: Large-scale weakly-supervised pre-training for video action recognition. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 12046–12055 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Goyal, R., Ebrahimi Kahou, S., Michalski, V., Materzynska, J., Westphal, S., Kim, H., Haenel, V., Fruend, I., Yianilos, P., Mueller-Freitag, M., et al.: The" something something" video database for learning and evaluating visual common sense. In: Proceedings of the IEEE international conference on computer vision. pp. 5842–5850 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Guo, X., Wu, W., Wang, D., Su, J., Su, H., Gan, W., Huang, J., Yang, Q.: Learning video representations of human motion from synthetic data. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 20197–20207 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Hara, K., Kataoka, H., Satoh, Y.: Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet? In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. pp. 6546–6555 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Hoffer, E., Ben-Nun, T., Hubara, I., Giladi, N., Hoefler, T., Soudry, D.: Augment your batch: Improving generalization through instance repetition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8129–8138 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Huang, G., Sun, Y., Liu, Z., Sedra, D., Weinberger, K.Q.: Deep networks with stochastic depth. In: Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14. pp. 646–661. Springer (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Huang, Z., Zhang, S., Jiang, J., Tang, M., Jin, R., Ang, M.H.: Self-supervised motion learning from static images. In: Proceedings of the ieee/cvf conference on computer vision and pattern recognition. pp. 1276–1285 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Hwang, H., Jang, C., Park, G., Cho, J., Kim, I.J.: Eldersim: A synthetic data generation platform for human action recognition in eldercare applications. IEEE Access (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Jenni, S., Meishvili, G., Favaro, P.: Video representation learning by recognizing temporal transformations. In: European Conference on Computer Vision. pp. 425–442. Springer (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Kataoka, H., Hara, K., Hayashi, R., Yamagata, E., Inoue, N.: Spatiotemporal initialization for 3d cnns with generated motion patterns. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 1279–1288 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Kataoka, H., Hayamizu, R., Yamada, R., Nakashima, K., Takashima, S., Zhang, X., Martinez-Noriega, E.J., Inoue, N., Yokota, R.: Replacing labeled real-image datasets with auto-generated contours. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 21232–21241 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Kataoka, H., Okayasu, K., Matsumoto, A., Yamagata, E., Yamada, R., Inoue, N., Nakamura, A., Satoh, Y.: Pre-training without natural images. In: Proceedings of the Asian Conference on Computer Vision (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al.: The kinetics human action video dataset. arXiv preprint arXiv:1705.06950 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Kim, D., Cho, D., Kweon, I.S.: Self-supervised video representation learning with space-time cubic puzzles. In: Proceedings of the AAAI conference on artificial intelligence. vol. 33, pp. 8545–8552 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Kim, Y.w., Mishra, S., Jin, S., Panda, R., Kuehne, H., Karlinsky, L., Saligrama, V., Saenko, K., Oliva, A., Feris, R.: How transferable are video representations based on synthetic data? Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib38.1.1">35</span>, 35710–35723 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., Serre, T.: Hmdb: a large video database for human motion recognition. In: 2011 International conference on computer vision. pp. 2556–2563. IEEE (2011)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Li, K., Wang, Y., Li, Y., Wang, Y., He, Y., Wang, L., Qiao, Y.: Unmasked teacher: Towards training-efficient video foundation models. arXiv preprint arXiv:2303.16058 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Li, T., Liu, J., Zhang, W., Ni, Y., Wang, W., Li, Z.: Uav-human: A large benchmark for human behavior understanding with unmanned aerial vehicles. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 16266–16275 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Li, Y., Vasconcelos, N.: Repair: Removing representation bias by dataset resampling. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 9572–9581 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Li, Y., Li, Y., Vasconcelos, N.: Resound: Towards action recognition without representation bias. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 513–528 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Ma, Z., Zhang, Z., Chen, Y., Qi, Z., Luo, Y., Li, Z., Yuan, C., Li, B., Qie, X., Shan, Y., et al.: Order-prompted tag sequence generation for video tagging. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 15681–15690 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri, M., Li, Y., Bharambe, A., Van Der Maaten, L.: Exploring the limits of weakly supervised pretraining. In: Proceedings of the European conference on computer vision (ECCV). pp. 181–196 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Miech, A., Zhukov, D., Alayrac, J.B., Tapaswi, M., Laptev, I., Sivic, J.: Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 2630–2640 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Mu, J., Qiu, W., Hager, G.D., Yuille, A.L.: Learning from synthetic animals. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12386–12395 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Nakamura, R., Kataoka, H., Takashima, S., Noriega, E.J.M., Yokota, R., Inoue, N.: Pre-training vision transformers with very limited synthesized images. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 20360–20369 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Nakashima, K., Kataoka, H., Matsumoto, A., Iwata, K., Inoue, N., Satoh, Y.: Can vision transformers learn without natural images? In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 36, pp. 1990–1998 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Pan, T., Song, Y., Yang, T., Jiang, W., Liu, W.: Videomoco: Contrastive video representation learning with temporally adversarial examples. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 11205–11214 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems <span class="ltx_text ltx_font_bold" id="bib.bib52.1.1">32</span> (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Perlin, K.: An image synthesizer. ACM Siggraph Computer Graphics <span class="ltx_text ltx_font_bold" id="bib.bib53.1.1">19</span>(3), 287–296 (1985)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Perlin, K.: Improving noise. In: Proceedings of the 29th annual conference on Computer graphics and interactive techniques. pp. 681–682 (2002)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al.: Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems <span class="ltx_text ltx_font_bold" id="bib.bib55.1.1">35</span>, 25278–25294 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Shu, F., Chen, B., Liao, Y., Xiao, S., Sun, W., Li, X., Zhu, Y., Wang, J., Liu, S.: Masked contrastive pre-training for efficient video-text retrieval. arXiv preprint arXiv:2212.00986 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Soomro, K., Zamir, A.R., Shah, M.: Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402 (2012)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Roberto de Souza, C., Gaidon, A., Cabon, Y., Manuel Lopez, A.: Procedural generation of videos to train deep action recognition networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 4757–4767 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Sun, C., Shrivastava, A., Singh, S., Gupta, A.: Revisiting unreasonable effectiveness of data in deep learning era. In: Proceedings of the IEEE international conference on computer vision. pp. 843–852 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Sun, X., Chen, P., Chen, L., Li, C., Li, T.H., Tan, M., Gan, C.: Masked motion encoding for self-supervised video representation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2235–2245 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the inception architecture for computer vision. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2818–2826 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Takashima, S., Hayamizu, R., Inoue, N., Kataoka, H., Yokota, R.: Visual atoms: Pre-training vision transformers with sinusoidal waves. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18579–18588 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Teed, Z., Deng, J.: Raft: Recurrent all-pairs field transforms for optical flow. In: Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16. pp. 402–419. Springer (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Tong, Z., Song, Y., Wang, J., Wang, L.: Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. arXiv preprint arXiv:2203.12602 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Toyer, S., Cherian, A., Han, T., Gould, S.: Human pose forecasting via deep markov models. In: 2017 International Conference on Digital Image Computing: Techniques and Applications (DICTA). pp. 1–8. IEEE (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Varol, G., Laptev, I., Schmid, C., Zisserman, A.: Synthetic humans for action recognition from unseen viewpoints. International Journal of Computer Vision <span class="ltx_text ltx_font_bold" id="bib.bib66.1.1">129</span>(7), 2264–2287 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Wang, J., Jiao, J., Liu, Y.H.: Self-supervised video representation learning by pace prediction. In: Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVII 16. pp. 504–521. Springer (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Wang, L., Huang, B., Zhao, Z., Tong, Z., He, Y., Wang, Y., Wang, Y., Qiao, Y.: Videomae v2: Scaling video masked autoencoders with dual masking. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 14549–14560 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Van Gool, L.: Temporal segment networks for action recognition in videos. IEEE transactions on pattern analysis and machine intelligence <span class="ltx_text ltx_font_bold" id="bib.bib69.1.1">41</span>(11), 2740–2755 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Wang, R., Chen, D., Wu, Z., Chen, Y., Dai, X., Liu, M., Jiang, Y.G., Zhou, L., Yuan, L.: Bevt: Bert pretraining of video transformers. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 14733–14743 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Wang, R., Chen, D., Wu, Z., Chen, Y., Dai, X., Liu, M., Yuan, L., Jiang, Y.G.: Masked video distillation: Rethinking masked feature modeling for self-supervised video representation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6312–6322 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Wang, T., Zhao, J., Yatskar, M., Chang, K.W., Ordonez, V.: Balanced datasets are not enough: Estimating and mitigating gender bias in deep image representations. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 5310–5319 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 7794–7803 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Wei, C., Fan, H., Xie, S., Wu, C.Y., Yuille, A., Feichtenhofer, C.: Masked feature prediction for self-supervised visual pre-training. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 14668–14678 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Wu, Z., Wang, Z., Wang, Z., Jin, H.: Towards privacy-preserving visual recognition via adversarial training: A pilot study. In: Proceedings of the European conference on computer vision (ECCV). pp. 606–624 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Xu, D., Xiao, J., Zhao, Z., Shao, J., Xie, D., Zhuang, Y.: Self-supervised spatiotemporal learning via video clip order prediction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10334–10343 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Yang, H., Huang, D., Wen, B., Wu, J., Yao, H., Jiang, Y., Zhu, X., Yuan, Z.: Self-supervised video representation learning with motion-aware masked autoencoders. arXiv preprint arXiv:2210.04154 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Yang, K., Qinami, K., Fei-Fei, L., Deng, J., Russakovsky, O.: Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the imagenet hierarchy. In: Proceedings of the 2020 conference on fairness, accountability, and transparency. pp. 547–558 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Yao, Y., Liu, C., Luo, D., Zhou, Y., Ye, Q.: Video playback rate perception for self-supervised spatio-temporal representation learning. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 6548–6557 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y.: Cutmix: Regularization strategy to train strong classifiers with localizable features. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 6023–6032 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Yun, S., Oh, S.J., Heo, B., Han, D., Kim, J.: Videomix: Rethinking data augmentation for video classification. arXiv preprint arXiv:2012.03457 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Zhang, Z., Chen, Y., Ma, Z., Qi, Z., Yuan, C., Li, B., Shan, Y., Hu, W.: Create: A benchmark for chinese short video retrieval and title generation. arXiv preprint arXiv:2203.16763 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Zhong, H., Mishra, S., Kim, D., Jin, S., Panda, R., Kuehne, H., Karlinsky, L., Saligrama, V., Oliva, A., Feris, R.: Learning human action recognition representations without real humans. arXiv preprint arXiv:2311.06231 (2023)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="Pt0.Ax1">
<h2 class="ltx_title ltx_title_appendix">Overview of Supplementary Material</h2>
<div class="ltx_para" id="Pt0.Ax1.p1">
<p class="ltx_p" id="Pt0.Ax1.p1.1">In this supplementary material,
we provide more details on our framework
and analyses of our experiments
with respect to the following points:</p>
</div>
<div class="ltx_para" id="Pt0.Ax1.p2">
<ul class="ltx_itemize" id="Pt0.Ax1.I1">
<li class="ltx_item" id="Pt0.Ax1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Pt0.Ax1.I1.i1.p1">
<p class="ltx_p" id="Pt0.Ax1.I1.i1.p1.1">Details on video datasets (<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A1" title="Appendix 0.A Details on Video Datasets ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">0.A</span></a>)</p>
</div>
</li>
<li class="ltx_item" id="Pt0.Ax1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Pt0.Ax1.I1.i2.p1">
<p class="ltx_p" id="Pt0.Ax1.I1.i2.p1.1">Implementation details (<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A2" title="Appendix 0.B Implementation Details ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">0.B</span></a>)</p>
</div>
</li>
<li class="ltx_item" id="Pt0.Ax1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Pt0.Ax1.I1.i3.p1">
<p class="ltx_p" id="Pt0.Ax1.I1.i3.p1.1">Pseudo-code of Pseudo Motion Generator (PMG) (<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A3" title="Appendix 0.C Pseudo-code of Pseudo Motion Generator (PMG) ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">0.C</span></a>)</p>
</div>
</li>
<li class="ltx_item" id="Pt0.Ax1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Pt0.Ax1.I1.i4.p1">
<p class="ltx_p" id="Pt0.Ax1.I1.i4.p1.1">Parameters of image augmentations in PMG (<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A4" title="Appendix 0.D Parameters of Image Augmentations in PMG ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">0.D</span></a>)</p>
</div>
</li>
<li class="ltx_item" id="Pt0.Ax1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Pt0.Ax1.I1.i5.p1">
<p class="ltx_p" id="Pt0.Ax1.I1.i5.p1.1">Examples of pseudo-motion videos generated by PMG (<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A5" title="Appendix 0.E Examples of Pseudo-motion Videos ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">0.E</span></a>)</p>
</div>
</li>
<li class="ltx_item" id="Pt0.Ax1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Pt0.Ax1.I1.i6.p1">
<p class="ltx_p" id="Pt0.Ax1.I1.i6.p1.1">Quantitative results of our framework (<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A6" title="Appendix 0.F Quantitative Results of Our Framework ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">0.F</span></a>)</p>
</div>
</li>
<li class="ltx_item" id="Pt0.Ax1.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Pt0.Ax1.I1.i7.p1">
<p class="ltx_p" id="Pt0.Ax1.I1.i7.p1.1">Failure cases (<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A7" title="Appendix 0.G Failure Cases ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">0.G</span></a>)</p>
</div>
</li>
<li class="ltx_item" id="Pt0.Ax1.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Pt0.Ax1.I1.i8.p1">
<p class="ltx_p" id="Pt0.Ax1.I1.i8.p1.1">Linear probing (<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A8" title="Appendix 0.H Linear Probing ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">0.H</span></a>)</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_appendix" id="Pt0.A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.A </span>Details on Video Datasets</h2>
<div class="ltx_para" id="Pt0.A1.p1">
<p class="ltx_p" id="Pt0.A1.p1.1">In our experiments,
we use seven datasets to evaluate the effectiveness of our framework;
UCF101 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib57" title="">57</a>]</cite>,
HMDB51 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib39" title="">39</a>]</cite>,
MiniSSV2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib13" title="">13</a>]</cite>,
Diving48 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib43" title="">43</a>]</cite>,
IkeaFA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib65" title="">65</a>]</cite>,
UAV-Human (UAV-H) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib41" title="">41</a>]</cite>,
and Kinetics400 (K400) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib36" title="">36</a>]</cite>.
The first six datasets are included in the SynAPT benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib38" title="">38</a>]</cite>
We conducted our experiments following its setup.
Herein, we provide an overview of the datasets used in this study.</p>
</div>
<div class="ltx_para ltx_noindent" id="Pt0.A1.p2">
<p class="ltx_p" id="Pt0.A1.p2.1"><span class="ltx_text ltx_font_bold" id="Pt0.A1.p2.1.1">UCF101 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib57" title="">57</a>]</cite>:</span>
This dataset features approximately 13,000 videos classified into 101 categories of actions.
These categories are segmented into five groups:
(i) Human-object Interaction (e.g., Juggling Balls),
(ii) Body-Motion Only (e.g., Push Ups),
(iii) Human-Human Interaction (e.g., Head Massage),
(iv) Playing Musical Instruments (e.g., Drumming),
and (v) Sports (e.g., Archery).</p>
</div>
<div class="ltx_para ltx_noindent" id="Pt0.A1.p3">
<p class="ltx_p" id="Pt0.A1.p3.1"><span class="ltx_text ltx_font_bold" id="Pt0.A1.p3.1.1">HMDB51 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib39" title="">39</a>]</cite>:</span>
Comprising roughly 6,000 video clips sourced from both movies and YouTube,
this dataset is annotated across 51 action categories.
These categories encompass five action types:
(i) general facial actions (e.g. smile),
(ii) facial actions with object manipulation (e.g. eat),
(iii) general body movements (e.g. jump),
(iv) body movements with object interaction (e.g. kick ball),
(v) body movements for human interaction (e.g. punch).</p>
</div>
<div class="ltx_para ltx_noindent" id="Pt0.A1.p4">
<p class="ltx_p" id="Pt0.A1.p4.1"><span class="ltx_text ltx_font_bold" id="Pt0.A1.p4.1.1">MiniSSV2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib13" title="">13</a>]</cite>:</span>
MiniSSV2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib38" title="">38</a>]</cite> is a subset of Something-Something V2 (SSV2) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib25" title="">25</a>]</cite>,
which encompasses over 220,000 video clips with 174 action classes.
MiniSSV2 contains just half of the original action categories,
with 87 randomly selected labels.
The total number of videos is approximately 93,000 videos.
Actions in this dataset are basic interactions with everyday objects,
defined via caption templates like "Moving something up" or "Covering something with something".</p>
</div>
<div class="ltx_para ltx_noindent" id="Pt0.A1.p5">
<p class="ltx_p" id="Pt0.A1.p5.1"><span class="ltx_text ltx_font_bold" id="Pt0.A1.p5.1.1">Diving48 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib43" title="">43</a>]</cite>:</span>
Dedicated to competitive diving,
this dataset consists of about 18,000 videos categorized into 48 distinct types of diving actions.
All videos in Diving48 exhibit consistent background and object characteristics.
Therefore, this dataset is often used to evaluate how the models capture motion information.</p>
</div>
<div class="ltx_para ltx_noindent" id="Pt0.A1.p6">
<p class="ltx_p" id="Pt0.A1.p6.1"><span class="ltx_text ltx_font_bold" id="Pt0.A1.p6.1.1">IkeaFA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib65" title="">65</a>]</cite>:</span>
Ikea Furniture Assembly (IkeaFA) offers 111 video clips,
each lasting between 2 to 4 minutes, accumulating roughly 480,000 frames.
This dataset consists of videos captured by GoPro cameras showcasing furniture assembly tasks,
all recorded against a uniform background by 14 individuals.
IkeaFA categorizes these assembly actions into 12 classes.</p>
</div>
<div class="ltx_para ltx_noindent" id="Pt0.A1.p7">
<p class="ltx_p" id="Pt0.A1.p7.1"><span class="ltx_text ltx_font_bold" id="Pt0.A1.p7.1.1">UAV-Human (UAV-H) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib41" title="">41</a>]</cite>:</span>
This dataset is gathered through the lens of an Unmanned Aerial Vehicle,
offering a unique perspective through its collection of video footage.
This dataset features a variety of recording types, including fisheye and night-vision videos.
In our study, we use videos captured by standard RGB cameras.
This subset includes 22,476 videos having 155 different action categories.</p>
</div>
<div class="ltx_para ltx_noindent" id="Pt0.A1.p8">
<p class="ltx_p" id="Pt0.A1.p8.1"><span class="ltx_text ltx_font_bold" id="Pt0.A1.p8.1.1">Kinetics400 (K400) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib36" title="">36</a>]</cite>:</span>
This large-scale dataset includes around 300,000 video clips,
each labeled with one of 400 actions.
The Kinetics400 videos are all sourced from YouTube and last about 10 seconds each.</p>
</div>
<figure class="ltx_table" id="Pt0.A1.T14">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Pt0.A1.T14.7.1.1" style="font-size:129%;">Table 14</span>: </span><span class="ltx_text" id="Pt0.A1.T14.8.2" style="font-size:129%;">
<span class="ltx_text ltx_font_bold" id="Pt0.A1.T14.8.2.1">Pre-training setting for each dataset.</span>
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Pt0.A1.T14.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Pt0.A1.T14.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="Pt0.A1.T14.1.2.1.1"><span class="ltx_text" id="Pt0.A1.T14.1.2.1.1.1" style="font-size:70%;">configuration</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A1.T14.1.2.1.2"><span class="ltx_text" id="Pt0.A1.T14.1.2.1.2.1" style="font-size:70%;">Kinetics400</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A1.T14.1.2.1.3"><span class="ltx_text" id="Pt0.A1.T14.1.2.1.3.1" style="font-size:70%;">MiniSSV2</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A1.T14.1.2.1.4"><span class="ltx_text" id="Pt0.A1.T14.1.2.1.4.1" style="font-size:70%;">Other Datasets</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T14.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Pt0.A1.T14.1.3.2.1"><span class="ltx_text" id="Pt0.A1.T14.1.3.2.1.1" style="font-size:70%;">optimizer</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="Pt0.A1.T14.1.3.2.2">
<span class="ltx_text" id="Pt0.A1.T14.1.3.2.2.1" style="font-size:70%;">AdamW </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A1.T14.1.3.2.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib44" title="">44</a><span class="ltx_text" id="Pt0.A1.T14.1.3.2.2.3.2" style="font-size:70%;">]</span></cite>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T14.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T14.1.4.3.1"><span class="ltx_text" id="Pt0.A1.T14.1.4.3.1.1" style="font-size:70%;">learning rate</span></th>
<td class="ltx_td ltx_align_center" colspan="3" id="Pt0.A1.T14.1.4.3.2"><span class="ltx_text" id="Pt0.A1.T14.1.4.3.2.1" style="font-size:70%;">1e-3</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T14.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T14.1.5.4.1"><span class="ltx_text" id="Pt0.A1.T14.1.5.4.1.1" style="font-size:70%;">weight decay</span></th>
<td class="ltx_td ltx_align_center" colspan="3" id="Pt0.A1.T14.1.5.4.2"><span class="ltx_text" id="Pt0.A1.T14.1.5.4.2.1" style="font-size:70%;">0.05</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T14.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T14.1.1.2"><span class="ltx_text" id="Pt0.A1.T14.1.1.2.1" style="font-size:70%;">optimizer momentum</span></th>
<td class="ltx_td ltx_align_center" colspan="3" id="Pt0.A1.T14.1.1.1">
<span class="ltx_text" id="Pt0.A1.T14.1.1.1.1" style="font-size:70%;"></span><math alttext="\beta_{1}=0.9,\beta_{2}=0.95" class="ltx_Math" display="inline" id="Pt0.A1.T14.1.1.1.m1.2"><semantics id="Pt0.A1.T14.1.1.1.m1.2a"><mrow id="Pt0.A1.T14.1.1.1.m1.2.2.2" xref="Pt0.A1.T14.1.1.1.m1.2.2.3.cmml"><mrow id="Pt0.A1.T14.1.1.1.m1.1.1.1.1" xref="Pt0.A1.T14.1.1.1.m1.1.1.1.1.cmml"><msub id="Pt0.A1.T14.1.1.1.m1.1.1.1.1.2" xref="Pt0.A1.T14.1.1.1.m1.1.1.1.1.2.cmml"><mi id="Pt0.A1.T14.1.1.1.m1.1.1.1.1.2.2" mathsize="70%" xref="Pt0.A1.T14.1.1.1.m1.1.1.1.1.2.2.cmml">β</mi><mn id="Pt0.A1.T14.1.1.1.m1.1.1.1.1.2.3" mathsize="70%" xref="Pt0.A1.T14.1.1.1.m1.1.1.1.1.2.3.cmml">1</mn></msub><mo id="Pt0.A1.T14.1.1.1.m1.1.1.1.1.1" mathsize="70%" xref="Pt0.A1.T14.1.1.1.m1.1.1.1.1.1.cmml">=</mo><mn id="Pt0.A1.T14.1.1.1.m1.1.1.1.1.3" mathsize="70%" xref="Pt0.A1.T14.1.1.1.m1.1.1.1.1.3.cmml">0.9</mn></mrow><mo id="Pt0.A1.T14.1.1.1.m1.2.2.2.3" mathsize="70%" xref="Pt0.A1.T14.1.1.1.m1.2.2.3a.cmml">,</mo><mrow id="Pt0.A1.T14.1.1.1.m1.2.2.2.2" xref="Pt0.A1.T14.1.1.1.m1.2.2.2.2.cmml"><msub id="Pt0.A1.T14.1.1.1.m1.2.2.2.2.2" xref="Pt0.A1.T14.1.1.1.m1.2.2.2.2.2.cmml"><mi id="Pt0.A1.T14.1.1.1.m1.2.2.2.2.2.2" mathsize="70%" xref="Pt0.A1.T14.1.1.1.m1.2.2.2.2.2.2.cmml">β</mi><mn id="Pt0.A1.T14.1.1.1.m1.2.2.2.2.2.3" mathsize="70%" xref="Pt0.A1.T14.1.1.1.m1.2.2.2.2.2.3.cmml">2</mn></msub><mo id="Pt0.A1.T14.1.1.1.m1.2.2.2.2.1" mathsize="70%" xref="Pt0.A1.T14.1.1.1.m1.2.2.2.2.1.cmml">=</mo><mn id="Pt0.A1.T14.1.1.1.m1.2.2.2.2.3" mathsize="70%" xref="Pt0.A1.T14.1.1.1.m1.2.2.2.2.3.cmml">0.95</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A1.T14.1.1.1.m1.2b"><apply id="Pt0.A1.T14.1.1.1.m1.2.2.3.cmml" xref="Pt0.A1.T14.1.1.1.m1.2.2.2"><csymbol cd="ambiguous" id="Pt0.A1.T14.1.1.1.m1.2.2.3a.cmml" xref="Pt0.A1.T14.1.1.1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="Pt0.A1.T14.1.1.1.m1.1.1.1.1.cmml" xref="Pt0.A1.T14.1.1.1.m1.1.1.1.1"><eq id="Pt0.A1.T14.1.1.1.m1.1.1.1.1.1.cmml" xref="Pt0.A1.T14.1.1.1.m1.1.1.1.1.1"></eq><apply id="Pt0.A1.T14.1.1.1.m1.1.1.1.1.2.cmml" xref="Pt0.A1.T14.1.1.1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="Pt0.A1.T14.1.1.1.m1.1.1.1.1.2.1.cmml" xref="Pt0.A1.T14.1.1.1.m1.1.1.1.1.2">subscript</csymbol><ci id="Pt0.A1.T14.1.1.1.m1.1.1.1.1.2.2.cmml" xref="Pt0.A1.T14.1.1.1.m1.1.1.1.1.2.2">𝛽</ci><cn id="Pt0.A1.T14.1.1.1.m1.1.1.1.1.2.3.cmml" type="integer" xref="Pt0.A1.T14.1.1.1.m1.1.1.1.1.2.3">1</cn></apply><cn id="Pt0.A1.T14.1.1.1.m1.1.1.1.1.3.cmml" type="float" xref="Pt0.A1.T14.1.1.1.m1.1.1.1.1.3">0.9</cn></apply><apply id="Pt0.A1.T14.1.1.1.m1.2.2.2.2.cmml" xref="Pt0.A1.T14.1.1.1.m1.2.2.2.2"><eq id="Pt0.A1.T14.1.1.1.m1.2.2.2.2.1.cmml" xref="Pt0.A1.T14.1.1.1.m1.2.2.2.2.1"></eq><apply id="Pt0.A1.T14.1.1.1.m1.2.2.2.2.2.cmml" xref="Pt0.A1.T14.1.1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="Pt0.A1.T14.1.1.1.m1.2.2.2.2.2.1.cmml" xref="Pt0.A1.T14.1.1.1.m1.2.2.2.2.2">subscript</csymbol><ci id="Pt0.A1.T14.1.1.1.m1.2.2.2.2.2.2.cmml" xref="Pt0.A1.T14.1.1.1.m1.2.2.2.2.2.2">𝛽</ci><cn id="Pt0.A1.T14.1.1.1.m1.2.2.2.2.2.3.cmml" type="integer" xref="Pt0.A1.T14.1.1.1.m1.2.2.2.2.2.3">2</cn></apply><cn id="Pt0.A1.T14.1.1.1.m1.2.2.2.2.3.cmml" type="float" xref="Pt0.A1.T14.1.1.1.m1.2.2.2.2.3">0.95</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.T14.1.1.1.m1.2c">\beta_{1}=0.9,\beta_{2}=0.95</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.T14.1.1.1.m1.2d">italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9 , italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.95</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T14.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T14.1.6.5.1"><span class="ltx_text" id="Pt0.A1.T14.1.6.5.1.1" style="font-size:70%;">mask ratio</span></th>
<td class="ltx_td ltx_align_center" colspan="3" id="Pt0.A1.T14.1.6.5.2"><span class="ltx_text" id="Pt0.A1.T14.1.6.5.2.1" style="font-size:70%;">0.75</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T14.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T14.1.7.6.1"><span class="ltx_text" id="Pt0.A1.T14.1.7.6.1.1" style="font-size:70%;">batch size</span></th>
<td class="ltx_td ltx_align_center" colspan="3" id="Pt0.A1.T14.1.7.6.2"><span class="ltx_text" id="Pt0.A1.T14.1.7.6.2.1" style="font-size:70%;">256</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T14.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T14.1.8.7.1"><span class="ltx_text" id="Pt0.A1.T14.1.8.7.1.1" style="font-size:70%;">batch size</span></th>
<td class="ltx_td ltx_align_center" colspan="3" id="Pt0.A1.T14.1.8.7.2"><span class="ltx_text" id="Pt0.A1.T14.1.8.7.2.1" style="font-size:70%;">256</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T14.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T14.1.9.8.1"><span class="ltx_text" id="Pt0.A1.T14.1.9.8.1.1" style="font-size:70%;">learning rate schedule</span></th>
<td class="ltx_td ltx_align_center" colspan="3" id="Pt0.A1.T14.1.9.8.2"><span class="ltx_text" id="Pt0.A1.T14.1.9.8.2.1" style="font-size:70%;">cosine decay</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T14.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T14.1.10.9.1"><span class="ltx_text" id="Pt0.A1.T14.1.10.9.1.1" style="font-size:70%;">warmup epochs</span></th>
<td class="ltx_td ltx_align_center" colspan="3" id="Pt0.A1.T14.1.10.9.2"><span class="ltx_text" id="Pt0.A1.T14.1.10.9.2.1" style="font-size:70%;">40</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T14.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T14.1.11.10.1"><span class="ltx_text" id="Pt0.A1.T14.1.11.10.1.1" style="font-size:70%;">epochs</span></th>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T14.1.11.10.2"><span class="ltx_text" id="Pt0.A1.T14.1.11.10.2.1" style="font-size:70%;">800</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T14.1.11.10.3"><span class="ltx_text" id="Pt0.A1.T14.1.11.10.3.1" style="font-size:70%;">2000</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T14.1.11.10.4"><span class="ltx_text" id="Pt0.A1.T14.1.11.10.4.1" style="font-size:70%;">2000</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T14.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="Pt0.A1.T14.1.12.11.1"><span class="ltx_text" id="Pt0.A1.T14.1.12.11.1.1" style="font-size:70%;">flip augmentation</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Pt0.A1.T14.1.12.11.2"><span class="ltx_text" id="Pt0.A1.T14.1.12.11.2.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Pt0.A1.T14.1.12.11.3"><span class="ltx_text" id="Pt0.A1.T14.1.12.11.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Pt0.A1.T14.1.12.11.4"><span class="ltx_text" id="Pt0.A1.T14.1.12.11.4.1" style="font-size:70%;">✓</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="Pt0.A1.T15">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Pt0.A1.T15.7.1.1" style="font-size:129%;">Table 15</span>: </span><span class="ltx_text" id="Pt0.A1.T15.8.2" style="font-size:129%;">
<span class="ltx_text ltx_font_bold" id="Pt0.A1.T15.8.2.1">Fine-tuning setting for each dataset.</span>
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Pt0.A1.T15.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Pt0.A1.T15.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="Pt0.A1.T15.1.2.1.1"><span class="ltx_text" id="Pt0.A1.T15.1.2.1.1.1" style="font-size:70%;">configuration</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A1.T15.1.2.1.2"><span class="ltx_text" id="Pt0.A1.T15.1.2.1.2.1" style="font-size:70%;">Kinetics400</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A1.T15.1.2.1.3"><span class="ltx_text" id="Pt0.A1.T15.1.2.1.3.1" style="font-size:70%;">MiniSSV2</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A1.T15.1.2.1.4"><span class="ltx_text" id="Pt0.A1.T15.1.2.1.4.1" style="font-size:70%;">Other Datasets</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T15.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="Pt0.A1.T15.1.3.2.1"><span class="ltx_text" id="Pt0.A1.T15.1.3.2.1.1" style="font-size:70%;">optimizer</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="Pt0.A1.T15.1.3.2.2">
<span class="ltx_text" id="Pt0.A1.T15.1.3.2.2.1" style="font-size:70%;">AdamW </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A1.T15.1.3.2.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib44" title="">44</a><span class="ltx_text" id="Pt0.A1.T15.1.3.2.2.3.2" style="font-size:70%;">]</span></cite>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T15.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T15.1.4.3.1"><span class="ltx_text" id="Pt0.A1.T15.1.4.3.1.1" style="font-size:70%;">learning rate</span></th>
<td class="ltx_td ltx_align_center" colspan="3" id="Pt0.A1.T15.1.4.3.2"><span class="ltx_text" id="Pt0.A1.T15.1.4.3.2.1" style="font-size:70%;">1e-3</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T15.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T15.1.5.4.1"><span class="ltx_text" id="Pt0.A1.T15.1.5.4.1.1" style="font-size:70%;">weight decay</span></th>
<td class="ltx_td ltx_align_center" colspan="3" id="Pt0.A1.T15.1.5.4.2"><span class="ltx_text" id="Pt0.A1.T15.1.5.4.2.1" style="font-size:70%;">0.05</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T15.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T15.1.1.2"><span class="ltx_text" id="Pt0.A1.T15.1.1.2.1" style="font-size:70%;">optimizer momentum</span></th>
<td class="ltx_td ltx_align_center" colspan="3" id="Pt0.A1.T15.1.1.1">
<span class="ltx_text" id="Pt0.A1.T15.1.1.1.1" style="font-size:70%;"></span><math alttext="\beta_{1}=0.9,\beta_{2}=0.999" class="ltx_Math" display="inline" id="Pt0.A1.T15.1.1.1.m1.2"><semantics id="Pt0.A1.T15.1.1.1.m1.2a"><mrow id="Pt0.A1.T15.1.1.1.m1.2.2.2" xref="Pt0.A1.T15.1.1.1.m1.2.2.3.cmml"><mrow id="Pt0.A1.T15.1.1.1.m1.1.1.1.1" xref="Pt0.A1.T15.1.1.1.m1.1.1.1.1.cmml"><msub id="Pt0.A1.T15.1.1.1.m1.1.1.1.1.2" xref="Pt0.A1.T15.1.1.1.m1.1.1.1.1.2.cmml"><mi id="Pt0.A1.T15.1.1.1.m1.1.1.1.1.2.2" mathsize="70%" xref="Pt0.A1.T15.1.1.1.m1.1.1.1.1.2.2.cmml">β</mi><mn id="Pt0.A1.T15.1.1.1.m1.1.1.1.1.2.3" mathsize="70%" xref="Pt0.A1.T15.1.1.1.m1.1.1.1.1.2.3.cmml">1</mn></msub><mo id="Pt0.A1.T15.1.1.1.m1.1.1.1.1.1" mathsize="70%" xref="Pt0.A1.T15.1.1.1.m1.1.1.1.1.1.cmml">=</mo><mn id="Pt0.A1.T15.1.1.1.m1.1.1.1.1.3" mathsize="70%" xref="Pt0.A1.T15.1.1.1.m1.1.1.1.1.3.cmml">0.9</mn></mrow><mo id="Pt0.A1.T15.1.1.1.m1.2.2.2.3" mathsize="70%" xref="Pt0.A1.T15.1.1.1.m1.2.2.3a.cmml">,</mo><mrow id="Pt0.A1.T15.1.1.1.m1.2.2.2.2" xref="Pt0.A1.T15.1.1.1.m1.2.2.2.2.cmml"><msub id="Pt0.A1.T15.1.1.1.m1.2.2.2.2.2" xref="Pt0.A1.T15.1.1.1.m1.2.2.2.2.2.cmml"><mi id="Pt0.A1.T15.1.1.1.m1.2.2.2.2.2.2" mathsize="70%" xref="Pt0.A1.T15.1.1.1.m1.2.2.2.2.2.2.cmml">β</mi><mn id="Pt0.A1.T15.1.1.1.m1.2.2.2.2.2.3" mathsize="70%" xref="Pt0.A1.T15.1.1.1.m1.2.2.2.2.2.3.cmml">2</mn></msub><mo id="Pt0.A1.T15.1.1.1.m1.2.2.2.2.1" mathsize="70%" xref="Pt0.A1.T15.1.1.1.m1.2.2.2.2.1.cmml">=</mo><mn id="Pt0.A1.T15.1.1.1.m1.2.2.2.2.3" mathsize="70%" xref="Pt0.A1.T15.1.1.1.m1.2.2.2.2.3.cmml">0.999</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A1.T15.1.1.1.m1.2b"><apply id="Pt0.A1.T15.1.1.1.m1.2.2.3.cmml" xref="Pt0.A1.T15.1.1.1.m1.2.2.2"><csymbol cd="ambiguous" id="Pt0.A1.T15.1.1.1.m1.2.2.3a.cmml" xref="Pt0.A1.T15.1.1.1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="Pt0.A1.T15.1.1.1.m1.1.1.1.1.cmml" xref="Pt0.A1.T15.1.1.1.m1.1.1.1.1"><eq id="Pt0.A1.T15.1.1.1.m1.1.1.1.1.1.cmml" xref="Pt0.A1.T15.1.1.1.m1.1.1.1.1.1"></eq><apply id="Pt0.A1.T15.1.1.1.m1.1.1.1.1.2.cmml" xref="Pt0.A1.T15.1.1.1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="Pt0.A1.T15.1.1.1.m1.1.1.1.1.2.1.cmml" xref="Pt0.A1.T15.1.1.1.m1.1.1.1.1.2">subscript</csymbol><ci id="Pt0.A1.T15.1.1.1.m1.1.1.1.1.2.2.cmml" xref="Pt0.A1.T15.1.1.1.m1.1.1.1.1.2.2">𝛽</ci><cn id="Pt0.A1.T15.1.1.1.m1.1.1.1.1.2.3.cmml" type="integer" xref="Pt0.A1.T15.1.1.1.m1.1.1.1.1.2.3">1</cn></apply><cn id="Pt0.A1.T15.1.1.1.m1.1.1.1.1.3.cmml" type="float" xref="Pt0.A1.T15.1.1.1.m1.1.1.1.1.3">0.9</cn></apply><apply id="Pt0.A1.T15.1.1.1.m1.2.2.2.2.cmml" xref="Pt0.A1.T15.1.1.1.m1.2.2.2.2"><eq id="Pt0.A1.T15.1.1.1.m1.2.2.2.2.1.cmml" xref="Pt0.A1.T15.1.1.1.m1.2.2.2.2.1"></eq><apply id="Pt0.A1.T15.1.1.1.m1.2.2.2.2.2.cmml" xref="Pt0.A1.T15.1.1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="Pt0.A1.T15.1.1.1.m1.2.2.2.2.2.1.cmml" xref="Pt0.A1.T15.1.1.1.m1.2.2.2.2.2">subscript</csymbol><ci id="Pt0.A1.T15.1.1.1.m1.2.2.2.2.2.2.cmml" xref="Pt0.A1.T15.1.1.1.m1.2.2.2.2.2.2">𝛽</ci><cn id="Pt0.A1.T15.1.1.1.m1.2.2.2.2.2.3.cmml" type="integer" xref="Pt0.A1.T15.1.1.1.m1.2.2.2.2.2.3">2</cn></apply><cn id="Pt0.A1.T15.1.1.1.m1.2.2.2.2.3.cmml" type="float" xref="Pt0.A1.T15.1.1.1.m1.2.2.2.2.3">0.999</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A1.T15.1.1.1.m1.2c">\beta_{1}=0.9,\beta_{2}=0.999</annotation><annotation encoding="application/x-llamapun" id="Pt0.A1.T15.1.1.1.m1.2d">italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9 , italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.999</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T15.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T15.1.6.5.1"><span class="ltx_text" id="Pt0.A1.T15.1.6.5.1.1" style="font-size:70%;">batch size</span></th>
<td class="ltx_td ltx_align_center" colspan="3" id="Pt0.A1.T15.1.6.5.2"><span class="ltx_text" id="Pt0.A1.T15.1.6.5.2.1" style="font-size:70%;">128</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T15.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T15.1.7.6.1"><span class="ltx_text" id="Pt0.A1.T15.1.7.6.1.1" style="font-size:70%;">learning rate schedule</span></th>
<td class="ltx_td ltx_align_center" colspan="3" id="Pt0.A1.T15.1.7.6.2"><span class="ltx_text" id="Pt0.A1.T15.1.7.6.2.1" style="font-size:70%;">cosine decay</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T15.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T15.1.8.7.1"><span class="ltx_text" id="Pt0.A1.T15.1.8.7.1.1" style="font-size:70%;">warmup epochs</span></th>
<td class="ltx_td ltx_align_center" colspan="3" id="Pt0.A1.T15.1.8.7.2"><span class="ltx_text" id="Pt0.A1.T15.1.8.7.2.1" style="font-size:70%;">5</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T15.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T15.1.9.8.1"><span class="ltx_text" id="Pt0.A1.T15.1.9.8.1.1" style="font-size:70%;">epochs</span></th>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T15.1.9.8.2"><span class="ltx_text" id="Pt0.A1.T15.1.9.8.2.1" style="font-size:70%;">50</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T15.1.9.8.3"><span class="ltx_text" id="Pt0.A1.T15.1.9.8.3.1" style="font-size:70%;">100</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T15.1.9.8.4"><span class="ltx_text" id="Pt0.A1.T15.1.9.8.4.1" style="font-size:70%;">100</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T15.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T15.1.10.9.1">
<span class="ltx_text" id="Pt0.A1.T15.1.10.9.1.1" style="font-size:70%;">repeated augmentation </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A1.T15.1.10.9.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib28" title="">28</a><span class="ltx_text" id="Pt0.A1.T15.1.10.9.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" colspan="3" id="Pt0.A1.T15.1.10.9.2"><span class="ltx_text" id="Pt0.A1.T15.1.10.9.2.1" style="font-size:70%;">2</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T15.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T15.1.11.10.1"><span class="ltx_text" id="Pt0.A1.T15.1.11.10.1.1" style="font-size:70%;">flip augmentation</span></th>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T15.1.11.10.2"><span class="ltx_text" id="Pt0.A1.T15.1.11.10.2.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T15.1.11.10.3"><span class="ltx_text" id="Pt0.A1.T15.1.11.10.3.1" style="font-size:70%;">-</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T15.1.11.10.4"><span class="ltx_text" id="Pt0.A1.T15.1.11.10.4.1" style="font-size:70%;">✓</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T15.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T15.1.12.11.1">
<span class="ltx_text" id="Pt0.A1.T15.1.12.11.1.1" style="font-size:70%;">RandAug </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A1.T15.1.12.11.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib15" title="">15</a><span class="ltx_text" id="Pt0.A1.T15.1.12.11.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" colspan="3" id="Pt0.A1.T15.1.12.11.2"><span class="ltx_text" id="Pt0.A1.T15.1.12.11.2.1" style="font-size:70%;">(9, 0.5)</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T15.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T15.1.13.12.1">
<span class="ltx_text" id="Pt0.A1.T15.1.13.12.1.1" style="font-size:70%;">label smoothing </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A1.T15.1.13.12.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib61" title="">61</a><span class="ltx_text" id="Pt0.A1.T15.1.13.12.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" colspan="3" id="Pt0.A1.T15.1.13.12.2"><span class="ltx_text" id="Pt0.A1.T15.1.13.12.2.1" style="font-size:70%;">0.1</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T15.1.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T15.1.14.13.1">
<span class="ltx_text" id="Pt0.A1.T15.1.14.13.1.1" style="font-size:70%;">mixup </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A1.T15.1.14.13.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib82" title="">82</a><span class="ltx_text" id="Pt0.A1.T15.1.14.13.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" colspan="3" id="Pt0.A1.T15.1.14.13.2"><span class="ltx_text" id="Pt0.A1.T15.1.14.13.2.1" style="font-size:70%;">0.8</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T15.1.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T15.1.15.14.1">
<span class="ltx_text" id="Pt0.A1.T15.1.15.14.1.1" style="font-size:70%;">cutmix </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A1.T15.1.15.14.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib80" title="">80</a><span class="ltx_text" id="Pt0.A1.T15.1.15.14.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" colspan="3" id="Pt0.A1.T15.1.15.14.2"><span class="ltx_text" id="Pt0.A1.T15.1.15.14.2.1" style="font-size:70%;">1.0</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T15.1.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T15.1.16.15.1">
<span class="ltx_text" id="Pt0.A1.T15.1.16.15.1.1" style="font-size:70%;">drop path </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A1.T15.1.16.15.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib29" title="">29</a><span class="ltx_text" id="Pt0.A1.T15.1.16.15.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T15.1.16.15.2"><span class="ltx_text" id="Pt0.A1.T15.1.16.15.2.1" style="font-size:70%;">0.1</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T15.1.16.15.3"><span class="ltx_text" id="Pt0.A1.T15.1.16.15.3.1" style="font-size:70%;">0.1</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T15.1.16.15.4"><span class="ltx_text" id="Pt0.A1.T15.1.16.15.4.1" style="font-size:70%;">0.2</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T15.1.17.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T15.1.17.16.1"><span class="ltx_text" id="Pt0.A1.T15.1.17.16.1.1" style="font-size:70%;">dropout</span></th>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T15.1.17.16.2"><span class="ltx_text" id="Pt0.A1.T15.1.17.16.2.1" style="font-size:70%;">0.0</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T15.1.17.16.3"><span class="ltx_text" id="Pt0.A1.T15.1.17.16.3.1" style="font-size:70%;">0.0</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A1.T15.1.17.16.4"><span class="ltx_text" id="Pt0.A1.T15.1.17.16.4.1" style="font-size:70%;">0.5</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T15.1.18.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="Pt0.A1.T15.1.18.17.1">
<span class="ltx_text" id="Pt0.A1.T15.1.18.17.1.1" style="font-size:70%;">layer-wise lr decay </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A1.T15.1.18.17.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib6" title="">6</a><span class="ltx_text" id="Pt0.A1.T15.1.18.17.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" colspan="3" id="Pt0.A1.T15.1.18.17.2"><span class="ltx_text" id="Pt0.A1.T15.1.18.17.2.1" style="font-size:70%;">0.75</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A1.T15.1.19.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="Pt0.A1.T15.1.19.18.1"><span class="ltx_text" id="Pt0.A1.T15.1.19.18.1.1" style="font-size:70%;">sampling</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Pt0.A1.T15.1.19.18.2">
<span class="ltx_text" id="Pt0.A1.T15.1.19.18.2.1" style="font-size:70%;">dense sampling </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A1.T15.1.19.18.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib73" title="">73</a><span class="ltx_text" id="Pt0.A1.T15.1.19.18.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Pt0.A1.T15.1.19.18.3">
<span class="ltx_text" id="Pt0.A1.T15.1.19.18.3.1" style="font-size:70%;">uniform sampling </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A1.T15.1.19.18.3.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib69" title="">69</a><span class="ltx_text" id="Pt0.A1.T15.1.19.18.3.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Pt0.A1.T15.1.19.18.4"><span class="ltx_text" id="Pt0.A1.T15.1.19.18.4.1" style="font-size:70%;">dense sampling</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_appendix" id="Pt0.A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.B </span>Implementation Details</h2>
<div class="ltx_para" id="Pt0.A2.p1">
<p class="ltx_p" id="Pt0.A2.p1.1">We conducted the experiments with 8 A100 GPUs for both pre-training and fine-tuning,
mostly following the settings in VideoMAE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib64" title="">64</a>]</cite>.
The settings for pre-training are detailed in <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A1.T14" title="In Appendix 0.A Details on Video Datasets ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">14</span></a>
and those for fine-tuning are described in <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A1.T15" title="In Appendix 0.A Details on Video Datasets ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">15</span></a>.
We used PyTorch <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib52" title="">52</a>]</cite> to implement our framework.</p>
</div>
</section>
<section class="ltx_appendix" id="Pt0.A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.C </span>Pseudo-code of Pseudo Motion Generator (PMG)</h2>
<div class="ltx_para" id="Pt0.A3.p1">
<p class="ltx_p" id="Pt0.A3.p1.1">While the algorithm of our Pseudo Motion Generator (PMG) is detailed in the main paper,
we offer Python pseudo-code for PMG in <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A4.F5" title="In Appendix 0.D Parameters of Image Augmentations in PMG ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a> for more clarity.</p>
</div>
</section>
<section class="ltx_appendix" id="Pt0.A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.D </span>Parameters of Image Augmentations in PMG</h2>
<div class="ltx_para" id="Pt0.A4.p1">
<p class="ltx_p" id="Pt0.A4.p1.1">Since it is difficult to find the optimal parameters for each image augmentation in our framework,
we implement each augmentation with a predefined range of parameters as follows:</p>
</div>
<div class="ltx_para" id="Pt0.A4.p2">
<ul class="ltx_itemize" id="Pt0.A4.I1">
<li class="ltx_item" id="Pt0.A4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Pt0.A4.I1.i1.p1">
<p class="ltx_p" id="Pt0.A4.I1.i1.p1.2"><span class="ltx_text ltx_font_bold" id="Pt0.A4.I1.i1.p1.2.1">Sliding Window: </span>
Cut a <math alttext="112\times 112" class="ltx_Math" display="inline" id="Pt0.A4.I1.i1.p1.1.m1.1"><semantics id="Pt0.A4.I1.i1.p1.1.m1.1a"><mrow id="Pt0.A4.I1.i1.p1.1.m1.1.1" xref="Pt0.A4.I1.i1.p1.1.m1.1.1.cmml"><mn id="Pt0.A4.I1.i1.p1.1.m1.1.1.2" xref="Pt0.A4.I1.i1.p1.1.m1.1.1.2.cmml">112</mn><mo id="Pt0.A4.I1.i1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="Pt0.A4.I1.i1.p1.1.m1.1.1.1.cmml">×</mo><mn id="Pt0.A4.I1.i1.p1.1.m1.1.1.3" xref="Pt0.A4.I1.i1.p1.1.m1.1.1.3.cmml">112</mn></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A4.I1.i1.p1.1.m1.1b"><apply id="Pt0.A4.I1.i1.p1.1.m1.1.1.cmml" xref="Pt0.A4.I1.i1.p1.1.m1.1.1"><times id="Pt0.A4.I1.i1.p1.1.m1.1.1.1.cmml" xref="Pt0.A4.I1.i1.p1.1.m1.1.1.1"></times><cn id="Pt0.A4.I1.i1.p1.1.m1.1.1.2.cmml" type="integer" xref="Pt0.A4.I1.i1.p1.1.m1.1.1.2">112</cn><cn id="Pt0.A4.I1.i1.p1.1.m1.1.1.3.cmml" type="integer" xref="Pt0.A4.I1.i1.p1.1.m1.1.1.3">112</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A4.I1.i1.p1.1.m1.1c">112\times 112</annotation><annotation encoding="application/x-llamapun" id="Pt0.A4.I1.i1.p1.1.m1.1d">112 × 112</annotation></semantics></math> window from a <math alttext="224\times 224" class="ltx_Math" display="inline" id="Pt0.A4.I1.i1.p1.2.m2.1"><semantics id="Pt0.A4.I1.i1.p1.2.m2.1a"><mrow id="Pt0.A4.I1.i1.p1.2.m2.1.1" xref="Pt0.A4.I1.i1.p1.2.m2.1.1.cmml"><mn id="Pt0.A4.I1.i1.p1.2.m2.1.1.2" xref="Pt0.A4.I1.i1.p1.2.m2.1.1.2.cmml">224</mn><mo id="Pt0.A4.I1.i1.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="Pt0.A4.I1.i1.p1.2.m2.1.1.1.cmml">×</mo><mn id="Pt0.A4.I1.i1.p1.2.m2.1.1.3" xref="Pt0.A4.I1.i1.p1.2.m2.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A4.I1.i1.p1.2.m2.1b"><apply id="Pt0.A4.I1.i1.p1.2.m2.1.1.cmml" xref="Pt0.A4.I1.i1.p1.2.m2.1.1"><times id="Pt0.A4.I1.i1.p1.2.m2.1.1.1.cmml" xref="Pt0.A4.I1.i1.p1.2.m2.1.1.1"></times><cn id="Pt0.A4.I1.i1.p1.2.m2.1.1.2.cmml" type="integer" xref="Pt0.A4.I1.i1.p1.2.m2.1.1.2">224</cn><cn id="Pt0.A4.I1.i1.p1.2.m2.1.1.3.cmml" type="integer" xref="Pt0.A4.I1.i1.p1.2.m2.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A4.I1.i1.p1.2.m2.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="Pt0.A4.I1.i1.p1.2.m2.1d">224 × 224</annotation></semantics></math> image and move it randomly.</p>
</div>
</li>
<li class="ltx_item" id="Pt0.A4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Pt0.A4.I1.i2.p1">
<p class="ltx_p" id="Pt0.A4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="Pt0.A4.I1.i2.p1.1.1">Zoom-in/out:</span>
For Zoom-out, randomly set a window from a <math alttext="224\times 224" class="ltx_Math" display="inline" id="Pt0.A4.I1.i2.p1.1.m1.1"><semantics id="Pt0.A4.I1.i2.p1.1.m1.1a"><mrow id="Pt0.A4.I1.i2.p1.1.m1.1.1" xref="Pt0.A4.I1.i2.p1.1.m1.1.1.cmml"><mn id="Pt0.A4.I1.i2.p1.1.m1.1.1.2" xref="Pt0.A4.I1.i2.p1.1.m1.1.1.2.cmml">224</mn><mo id="Pt0.A4.I1.i2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="Pt0.A4.I1.i2.p1.1.m1.1.1.1.cmml">×</mo><mn id="Pt0.A4.I1.i2.p1.1.m1.1.1.3" xref="Pt0.A4.I1.i2.p1.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A4.I1.i2.p1.1.m1.1b"><apply id="Pt0.A4.I1.i2.p1.1.m1.1.1.cmml" xref="Pt0.A4.I1.i2.p1.1.m1.1.1"><times id="Pt0.A4.I1.i2.p1.1.m1.1.1.1.cmml" xref="Pt0.A4.I1.i2.p1.1.m1.1.1.1"></times><cn id="Pt0.A4.I1.i2.p1.1.m1.1.1.2.cmml" type="integer" xref="Pt0.A4.I1.i2.p1.1.m1.1.1.2">224</cn><cn id="Pt0.A4.I1.i2.p1.1.m1.1.1.3.cmml" type="integer" xref="Pt0.A4.I1.i2.p1.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A4.I1.i2.p1.1.m1.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="Pt0.A4.I1.i2.p1.1.m1.1d">224 × 224</annotation></semantics></math> image
within the size range of [0.2, 0.45], then gradually enlarge the window
until it reaches a random size between [0.55, 0.95].
For Zoom-in, reverse the process for pseudo-motion videos generated by Zoom-out.
We randomly choose between Zoom-in and Zoom-out with a 50% probability.</p>
</div>
</li>
<li class="ltx_item" id="Pt0.A4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Pt0.A4.I1.i3.p1">
<p class="ltx_p" id="Pt0.A4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="Pt0.A4.I1.i3.p1.1.1">Fade-in/out: </span>
For Fade-out, make an input image gradually become completely invisible.
For Fade-in, reverse the process of pseudo-motion videos generated by Zoom-in.
We randomly choose between Fade-in and Fade-out with a 50% probability.</p>
</div>
</li>
<li class="ltx_item" id="Pt0.A4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Pt0.A4.I1.i4.p1">
<p class="ltx_p" id="Pt0.A4.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="Pt0.A4.I1.i4.p1.1.1">Affine Transformation</span>
We use the AffineTransformation class provided in PyTorch <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib52" title="">52</a>]</cite>.
The rotation angle in degrees is randomly selected between -15 and 15.
The translation is randomly selected between [-0.01, 0.01] for both horizontal and vertical directions.
The scale value is randomly selected between [0.9999, 1.0001].
The shear angle value in degrees is randomly selected between -1 and 1.</p>
</div>
</li>
<li class="ltx_item" id="Pt0.A4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Pt0.A4.I1.i5.p1">
<p class="ltx_p" id="Pt0.A4.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="Pt0.A4.I1.i5.p1.1.1">Perspective Transformation</span>
We use the PerspectiveTransformation class provided in PyTorch.
The scale of distortion is set to 0.05.</p>
</div>
</li>
<li class="ltx_item" id="Pt0.A4.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Pt0.A4.I1.i6.p1">
<p class="ltx_p" id="Pt0.A4.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="Pt0.A4.I1.i6.p1.1.1">Color Jitter</span>:
We use the ColorJitter class provided in PyTorch.
We set the range of brightness as [0.0, 0.2], that of contrast as [0, 0.3],
that of saturation [0, 0.2], that of hue [0.0, 0.1].</p>
</div>
</li>
<li class="ltx_item" id="Pt0.A4.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Pt0.A4.I1.i7.p1">
<p class="ltx_p" id="Pt0.A4.I1.i7.p1.2"><span class="ltx_text ltx_font_bold" id="Pt0.A4.I1.i7.p1.2.1">CutMix</span>:
As in Sliding Window, we cut a <math alttext="112\times 112" class="ltx_Math" display="inline" id="Pt0.A4.I1.i7.p1.1.m1.1"><semantics id="Pt0.A4.I1.i7.p1.1.m1.1a"><mrow id="Pt0.A4.I1.i7.p1.1.m1.1.1" xref="Pt0.A4.I1.i7.p1.1.m1.1.1.cmml"><mn id="Pt0.A4.I1.i7.p1.1.m1.1.1.2" xref="Pt0.A4.I1.i7.p1.1.m1.1.1.2.cmml">112</mn><mo id="Pt0.A4.I1.i7.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="Pt0.A4.I1.i7.p1.1.m1.1.1.1.cmml">×</mo><mn id="Pt0.A4.I1.i7.p1.1.m1.1.1.3" xref="Pt0.A4.I1.i7.p1.1.m1.1.1.3.cmml">112</mn></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A4.I1.i7.p1.1.m1.1b"><apply id="Pt0.A4.I1.i7.p1.1.m1.1.1.cmml" xref="Pt0.A4.I1.i7.p1.1.m1.1.1"><times id="Pt0.A4.I1.i7.p1.1.m1.1.1.1.cmml" xref="Pt0.A4.I1.i7.p1.1.m1.1.1.1"></times><cn id="Pt0.A4.I1.i7.p1.1.m1.1.1.2.cmml" type="integer" xref="Pt0.A4.I1.i7.p1.1.m1.1.1.2">112</cn><cn id="Pt0.A4.I1.i7.p1.1.m1.1.1.3.cmml" type="integer" xref="Pt0.A4.I1.i7.p1.1.m1.1.1.3">112</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A4.I1.i7.p1.1.m1.1c">112\times 112</annotation><annotation encoding="application/x-llamapun" id="Pt0.A4.I1.i7.p1.1.m1.1d">112 × 112</annotation></semantics></math> window from an image and
paste it to another <math alttext="224\times 224" class="ltx_Math" display="inline" id="Pt0.A4.I1.i7.p1.2.m2.1"><semantics id="Pt0.A4.I1.i7.p1.2.m2.1a"><mrow id="Pt0.A4.I1.i7.p1.2.m2.1.1" xref="Pt0.A4.I1.i7.p1.2.m2.1.1.cmml"><mn id="Pt0.A4.I1.i7.p1.2.m2.1.1.2" xref="Pt0.A4.I1.i7.p1.2.m2.1.1.2.cmml">224</mn><mo id="Pt0.A4.I1.i7.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="Pt0.A4.I1.i7.p1.2.m2.1.1.1.cmml">×</mo><mn id="Pt0.A4.I1.i7.p1.2.m2.1.1.3" xref="Pt0.A4.I1.i7.p1.2.m2.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="Pt0.A4.I1.i7.p1.2.m2.1b"><apply id="Pt0.A4.I1.i7.p1.2.m2.1.1.cmml" xref="Pt0.A4.I1.i7.p1.2.m2.1.1"><times id="Pt0.A4.I1.i7.p1.2.m2.1.1.1.cmml" xref="Pt0.A4.I1.i7.p1.2.m2.1.1.1"></times><cn id="Pt0.A4.I1.i7.p1.2.m2.1.1.2.cmml" type="integer" xref="Pt0.A4.I1.i7.p1.2.m2.1.1.2">224</cn><cn id="Pt0.A4.I1.i7.p1.2.m2.1.1.3.cmml" type="integer" xref="Pt0.A4.I1.i7.p1.2.m2.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Pt0.A4.I1.i7.p1.2.m2.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="Pt0.A4.I1.i7.p1.2.m2.1d">224 × 224</annotation></semantics></math> image,
then move the window randomly.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Pt0.A4.p3">
<p class="ltx_p" id="Pt0.A4.p3.1">We understand that these predefined parameters are not optimal and there is room for further consideration.
We plan to conduct exhaustive experiments and develop a framework that does not rely on hand-crafted augmentations.</p>
</div>
<figure class="ltx_figure" id="Pt0.A4.F5">
<div class="ltx_listing ltx_lst_language_Python ltx_lst_numbers_left ltx_lstlisting ltx_align_center ltx_listing" id="Pt0.A4.F5.2" style="background-color:#F2F2EB;">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,aW1wb3J0IHJhbmRvbQoKdHJhbnNmb3JtX2xpc3QgPSBbCiAgICAiSWRlbnRpdHkiLCAiU2xpZGluZyBXaW5kb3ciLCAiWm9vbS1pbiIsICJab29tLW91dCIsCiAgICAiRmFkZS1pbiIsICJGYWRlLW91dCIsICJBZmZpbmUgVHJhbnNmb3JtYXRpb24iLAogICAgIlBlcnNwZWN0aXZlIFRyYW5zZm9ybWF0aW9uIiwgIkNvbG9yIEppdHRlciIsICJDdXRNaXgiLApdCgpkZWYgZ2VuZXJhdGVfcHNldWRvX21vdGlvbihpbWFnZSwgVCk6CiAgICAiIiJQc2V1ZG8gVmlkZW8gR2VuZXJhdG9yLgoKICAgIEFyZ3M6CiAgICAgICAgaW1hZ2U6IElucHV0IGltYWdlLgogICAgICAgIFQ6IFRoZSBudW1iZXIgb2YgZnJhbWVzIGluIGEgdmlkZW8uCiAgICAiIiIKICAgIHRyYW5zZm9ybSA9IHJhbmRvbS5jaG9pY2UodHJhbnNmb3JtX2xpc3QpCiAgICBwYXJhbXMgPSB0cmFuc2Zvcm0uZ2V0X3JhbmRvbV9wYXJhbWV0ZXJzKCkKCiAgICB2aWRlbyA9IFtpbWFnZV0KICAgIHByZXZpb3VzX2ZyYW1lID0gaW1hZ2UKICAgIGZvciBfIGluIHJhbmdlKFQgLSAxKToKICAgICAgICB0cmFuc2Zvcm1lZF9mcmFtZSA9IHRyYW5zZm9ybShwcmV2aW91c19mcmFtZSwgcGFyYW1zKQogICAgICAgIHZpZGVvLmFwcGVuZCh0cmFuc2Zvcm1lZF9mcmFtZSkKICAgICAgICBwcmV2aW91c19mcmFtZSA9IHRyYW5zZm9ybWVkX2ZyYW1lCgogICAgcmV0dXJuIHZpZGVv">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx1">
<span class="ltx_tag ltx_tag_listingline">1</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx1.1" style="font-size:70%;color:#FF00FF;">import</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.2" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.3" style="font-size:70%;">random</span>
</div>
<div class="ltx_listingline" id="lstnumberx2">
<span class="ltx_tag ltx_tag_listingline">2</span>
</div>
<div class="ltx_listingline" id="lstnumberx3">
<span class="ltx_tag ltx_tag_listingline">3</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.1" style="font-size:70%;">transform_list</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.2" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.3" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.4" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.5" style="font-size:70%;">[</span>
</div>
<div class="ltx_listingline" id="lstnumberx4">
<span class="ltx_tag ltx_tag_listingline">4</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.1" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx4.2" style="font-size:70%;color:#9400D1;">"Identity"</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.3" style="font-size:70%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.4" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx4.5" style="font-size:70%;color:#9400D1;">"Sliding<span class="ltx_text ltx_lst_space" id="lstnumberx4.5.1"> </span>Window"</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.6" style="font-size:70%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.7" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx4.8" style="font-size:70%;color:#9400D1;">"Zoom-in"</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.9" style="font-size:70%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.10" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx4.11" style="font-size:70%;color:#9400D1;">"Zoom-out"</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.12" style="font-size:70%;">,</span>
</div>
<div class="ltx_listingline" id="lstnumberx5">
<span class="ltx_tag ltx_tag_listingline">5</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.1" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx5.2" style="font-size:70%;color:#9400D1;">"Fade-in"</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.3" style="font-size:70%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.4" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx5.5" style="font-size:70%;color:#9400D1;">"Fade-out"</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.6" style="font-size:70%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.7" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx5.8" style="font-size:70%;color:#9400D1;">"Affine<span class="ltx_text ltx_lst_space" id="lstnumberx5.8.1"> </span>Transformation"</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.9" style="font-size:70%;">,</span>
</div>
<div class="ltx_listingline" id="lstnumberx6">
<span class="ltx_tag ltx_tag_listingline">6</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.1" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx6.2" style="font-size:70%;color:#9400D1;">"Perspective<span class="ltx_text ltx_lst_space" id="lstnumberx6.2.1"> </span>Transformation"</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.3" style="font-size:70%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.4" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx6.5" style="font-size:70%;color:#9400D1;">"Color<span class="ltx_text ltx_lst_space" id="lstnumberx6.5.1"> </span>Jitter"</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.6" style="font-size:70%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.7" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx6.8" style="font-size:70%;color:#9400D1;">"CutMix"</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.9" style="font-size:70%;">,</span>
</div>
<div class="ltx_listingline" id="lstnumberx7">
<span class="ltx_tag ltx_tag_listingline">7</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.1" style="font-size:70%;">]</span>
</div>
<div class="ltx_listingline" id="lstnumberx8">
<span class="ltx_tag ltx_tag_listingline">8</span>
</div>
<div class="ltx_listingline" id="lstnumberx9">
<span class="ltx_tag ltx_tag_listingline">9</span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx9.1" style="font-size:70%;color:#FF00FF;">def</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.2" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.3" style="font-size:70%;">generate_pseudo_motion</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.4" style="font-size:70%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.5" style="font-size:70%;">image</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.6" style="font-size:70%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.7" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.8" style="font-size:70%;">T</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.9" style="font-size:70%;">):</span>
</div>
<div class="ltx_listingline" id="lstnumberx10">
<span class="ltx_tag ltx_tag_listingline">10</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.1" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx10.2" style="font-size:70%;color:#9400D1;">""</span><span class="ltx_text ltx_lst_string ltx_font_typewriter" id="lstnumberx10.3" style="font-size:70%;color:#9400D1;">"Pseudo<span class="ltx_text ltx_lst_space" id="lstnumberx10.3.1"> </span>Video<span class="ltx_text ltx_lst_space" id="lstnumberx10.3.2"> </span>Generator.</span>
</div>
<div class="ltx_listingline" id="lstnumberx11">
<span class="ltx_tag ltx_tag_listingline">11</span>
</div>
<div class="ltx_listingline" id="lstnumberx12">
<span class="ltx_tag ltx_tag_listingline">12</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.1" style="font-size:70%;color:#9400D1;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.2" style="font-size:70%;color:#9400D1;">Args:</span>
</div>
<div class="ltx_listingline" id="lstnumberx13">
<span class="ltx_tag ltx_tag_listingline">13</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.1" style="font-size:70%;color:#9400D1;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.2" style="font-size:70%;color:#9400D1;">image:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.3" style="font-size:70%;color:#9400D1;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.4" style="font-size:70%;color:#9400D1;">Input</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx13.5" style="font-size:70%;color:#9400D1;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.6" style="font-size:70%;color:#9400D1;">image.</span>
</div>
<div class="ltx_listingline" id="lstnumberx14">
<span class="ltx_tag ltx_tag_listingline">14</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.1" style="font-size:70%;color:#9400D1;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.2" style="font-size:70%;color:#9400D1;">T:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.3" style="font-size:70%;color:#9400D1;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.4" style="font-size:70%;color:#9400D1;">The</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.5" style="font-size:70%;color:#9400D1;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.6" style="font-size:70%;color:#9400D1;">number</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.7" style="font-size:70%;color:#9400D1;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.8" style="font-size:70%;color:#9400D1;">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.9" style="font-size:70%;color:#9400D1;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.10" style="font-size:70%;color:#9400D1;">frames</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.11" style="font-size:70%;color:#9400D1;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.12" style="font-size:70%;color:#9400D1;">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.13" style="font-size:70%;color:#9400D1;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.14" style="font-size:70%;color:#9400D1;">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.15" style="font-size:70%;color:#9400D1;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.16" style="font-size:70%;color:#9400D1;">video.</span>
</div>
<div class="ltx_listingline" id="lstnumberx15">
<span class="ltx_tag ltx_tag_listingline">15</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.1" style="font-size:70%;color:#9400D1;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.2" style="font-size:70%;color:#9400D1;">"</span><span class="ltx_text ltx_lst_string" id="lstnumberx15.3"></span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.4" style="font-size:70%;color:#9400D1;">""</span>
</div>
<div class="ltx_listingline" id="lstnumberx16">
<span class="ltx_tag ltx_tag_listingline">16</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.1" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.2" style="font-size:70%;">transform</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.3" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.4" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.5" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.6" style="font-size:70%;">random</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.7" style="font-size:70%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.8" style="font-size:70%;">choice</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.9" style="font-size:70%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.10" style="font-size:70%;">transform_list</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.11" style="font-size:70%;">)</span>
</div>
<div class="ltx_listingline" id="lstnumberx17">
<span class="ltx_tag ltx_tag_listingline">17</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.1" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.2" style="font-size:70%;">params</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.3" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.4" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.5" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.6" style="font-size:70%;">transform</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.7" style="font-size:70%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.8" style="font-size:70%;">get_random_parameters</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.9" style="font-size:70%;">()</span>
</div>
<div class="ltx_listingline" id="lstnumberx18">
<span class="ltx_tag ltx_tag_listingline">18</span>
</div>
<div class="ltx_listingline" id="lstnumberx19">
<span class="ltx_tag ltx_tag_listingline">19</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.1" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.2" style="font-size:70%;">video</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.3" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.4" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.5" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.6" style="font-size:70%;">[</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.7" style="font-size:70%;">image</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.8" style="font-size:70%;">]</span>
</div>
<div class="ltx_listingline" id="lstnumberx20">
<span class="ltx_tag ltx_tag_listingline">20</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.1" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.2" style="font-size:70%;">previous_frame</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.3" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.4" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.5" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.6" style="font-size:70%;">image</span>
</div>
<div class="ltx_listingline" id="lstnumberx21">
<span class="ltx_tag ltx_tag_listingline">21</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.1" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx21.2" style="font-size:70%;color:#FF00FF;">for</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.3" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.4" style="font-size:70%;">_</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.5" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx21.6" style="font-size:70%;color:#FF00FF;">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.7" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter" id="lstnumberx21.8" style="font-size:70%;color:#FF00FF;">range</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.9" style="font-size:70%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.10" style="font-size:70%;">T</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.11" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.12" style="font-size:70%;">-</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.13" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.14" style="font-size:70%;">1):</span>
</div>
<div class="ltx_listingline" id="lstnumberx22">
<span class="ltx_tag ltx_tag_listingline">22</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx22.1" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx22.2" style="font-size:70%;">transformed_frame</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx22.3" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx22.4" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx22.5" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx22.6" style="font-size:70%;">transform</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx22.7" style="font-size:70%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx22.8" style="font-size:70%;">previous_frame</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx22.9" style="font-size:70%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx22.10" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx22.11" style="font-size:70%;">params</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx22.12" style="font-size:70%;">)</span>
</div>
<div class="ltx_listingline" id="lstnumberx23">
<span class="ltx_tag ltx_tag_listingline">23</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.1" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.2" style="font-size:70%;">video</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.3" style="font-size:70%;">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.4" style="font-size:70%;">append</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.5" style="font-size:70%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.6" style="font-size:70%;">transformed_frame</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.7" style="font-size:70%;">)</span>
</div>
<div class="ltx_listingline" id="lstnumberx24">
<span class="ltx_tag ltx_tag_listingline">24</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.1" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.2" style="font-size:70%;">previous_frame</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.3" style="font-size:70%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.4" style="font-size:70%;">=</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.5" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.6" style="font-size:70%;">transformed_frame</span>
</div>
<div class="ltx_listingline" id="lstnumberx25">
<span class="ltx_tag ltx_tag_listingline">25</span>
</div>
<div class="ltx_listingline" id="lstnumberx26">
<span class="ltx_tag ltx_tag_listingline">26</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.1" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_keyword ltx_font_typewriter" id="lstnumberx26.2" style="font-size:70%;color:#FF00FF;">return</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx26.3" style="font-size:70%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.4" style="font-size:70%;">video</span>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A4.F5.4.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text ltx_font_bold" id="Pt0.A4.F5.5.2" style="font-size:90%;">Python pseudo-code for Pseudo Motion Generator (PMG).</span></figcaption>
</figure>
<figure class="ltx_figure" id="Pt0.A4.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="Pt0.A4.F6.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="341" id="Pt0.A4.F6.sf1.g1" src="x12.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A4.F6.sf1.4.1.1" style="font-size:129%;">(a)</span> </span><span class="ltx_text" id="Pt0.A4.F6.sf1.5.2" style="font-size:129%;">FractalDB</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="Pt0.A4.F6.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="334" id="Pt0.A4.F6.sf2.g1" src="x13.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A4.F6.sf2.4.1.1" style="font-size:129%;">(b)</span> </span><span class="ltx_text" id="Pt0.A4.F6.sf2.5.2" style="font-size:129%;">Shaders1k</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="Pt0.A4.F6.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="333" id="Pt0.A4.F6.sf3.g1" src="x14.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A4.F6.sf3.4.1.1" style="font-size:129%;">(c)</span> </span><span class="ltx_text" id="Pt0.A4.F6.sf3.5.2" style="font-size:129%;">VisualAtom</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A4.F6.4.1.1" style="font-size:129%;">Figure 6</span>: </span><span class="ltx_text ltx_font_bold" id="Pt0.A4.F6.5.2" style="font-size:129%;">Examples of pseudo-motion videos generated from synthetic image datasets.</span></figcaption>
</figure>
</section>
<section class="ltx_appendix" id="Pt0.A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.E </span>Examples of Pseudo-motion Videos</h2>
<div class="ltx_para" id="Pt0.A5.p1">
<p class="ltx_p" id="Pt0.A5.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A4.F6" title="In Appendix 0.D Parameters of Image Augmentations in PMG ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a> shows the examples of pseudo-motion videos
generated from three synthetic image datasets;
FractalDB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib35" title="">35</a>]</cite>,
Shaders1k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib7" title="">7</a>]</cite>,
and Visual Atom <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib62" title="">62</a>]</cite>.
Although the appearance and motions in these videos differ from real videos,
they exhibit a wide range of motion and appearance patterns.
This variety enables VideoMAE to learn effectively.
Specifically, pre-training with pseudo-motion videos generated from Shaders1k
improves the model’s performance compared to pre-training with those from the other sources.
This improvement is attributed to the videos from Shaders1k having a clear correspondence
of patches between frames, which suits for VideoMAE.</p>
</div>
<figure class="ltx_figure" id="Pt0.A5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="910" id="Pt0.A5.F7.g1" src="x15.png" width="567"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A5.F7.6.1.1" style="font-size:113%;">Figure 7</span>: </span><span class="ltx_text" id="Pt0.A5.F7.7.2" style="font-size:113%;">
<span class="ltx_text ltx_font_bold" id="Pt0.A5.F7.7.2.1">Visualization of outputs and loss heatmaps for VideoMAE on HMDB51.</span>
The mask ratio is set as 75%.
Loss heatmaps are normalized per frame.
</span></figcaption>
</figure>
<figure class="ltx_figure" id="Pt0.A5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="910" id="Pt0.A5.F8.g1" src="x16.png" width="565"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A5.F8.6.1.1" style="font-size:113%;">Figure 8</span>: </span><span class="ltx_text" id="Pt0.A5.F8.7.2" style="font-size:113%;">
<span class="ltx_text ltx_font_bold" id="Pt0.A5.F8.7.2.1">Visualization of outputs and loss heatmaps for VideoMAE on UCF101.</span>
The mask ratio is set as 75%.
Loss heatmaps are normalized per frame.
</span></figcaption>
</figure>
</section>
<section class="ltx_appendix" id="Pt0.A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.F </span>Quantitative Results of Our Framework</h2>
<div class="ltx_para" id="Pt0.A6.p1">
<p class="ltx_p" id="Pt0.A6.p1.1">To verify that VideoMAE successfully learns the reconstruction task,
we visualized its output results on HMDB51 and UCF101.
We compared the outputs of three models:
(i) VideoMAE trained on real videos from each dataset,
(ii) VideoMAE trained on pseudo-motion videos generated from frames on each video dataset,
and (iii) VideoMAE trained on pseudo-motion videos from Shaders1k.
<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A5.F7" title="In Appendix 0.E Examples of Pseudo-motion Videos ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A5.F8" title="In Appendix 0.E Examples of Pseudo-motion Videos ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a>
shows the results for HMDB51 and UCF101, respectively.
The inputs for these models were sampled from the test set,
which was not used for pre-training.
Despite not being trained on real videos,
VideoMAE trained on Shaders1k manages to achieve a reasonable level of accuracy
in reconstructing real videos.
This suggests that the method can roughly capture
the complex motion and shape characteristics of the real world.</p>
</div>
<div class="ltx_para" id="Pt0.A6.p2">
<p class="ltx_p" id="Pt0.A6.p2.1">However, compared to VideoMAE trained on real videos,
VideoMAE trained on pseudo-motion videos
struggles with the reconstruction of finer details.
This issue likely arises because our PMG applies image transformations globally,
hindering its ability to learn fine-grained motions.
Consequently, our framework exhibits lower performance
in classifying certain fine-grained actions,
compared to VideoMAE trained on real videos
(See <a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A7" title="Appendix 0.G Failure Cases ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">0.G</span></a>).</p>
</div>
</section>
<section class="ltx_appendix" id="Pt0.A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.G </span>Failure Cases</h2>
<figure class="ltx_figure" id="Pt0.A7.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="920" id="Pt0.A7.F9.g1" src="x17.png" width="368"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A7.F9.3.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text ltx_font_bold" id="Pt0.A7.F9.4.2" style="font-size:90%;">Comparison of accuracy per class for each model on HMDB51.</span></figcaption>
</figure>
<figure class="ltx_figure" id="Pt0.A7.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="Pt0.A7.F10.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="2075" id="Pt0.A7.F10.1.g1" src="x18.png" width="830"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="Pt0.A7.F10.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="2075" id="Pt0.A7.F10.2.g1" src="x19.png" width="830"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="Pt0.A7.F10.6.1.1" style="font-size:113%;">Figure 10</span>: </span><span class="ltx_text ltx_font_bold" id="Pt0.A7.F10.7.2" style="font-size:113%;">Comparison of accuracy per class for each model on UCF101.</span></figcaption>
</figure>
<div class="ltx_para" id="Pt0.A7.p1">
<p class="ltx_p" id="Pt0.A7.p1.1">We further analyzed the failure cases of our framework
compared to VideoMAE when trained with real videos.
For this analysis, we evaluated three models:
(i) VideoMAE trained with pseudo-motion videos by Identity (no-motion videos),
(ii) VideoMAE trained with pseudo-motion videos by Affine Transformation and Zoom-in/out combined with Mixup.
(iii) VideoMAE trained with real videos.</p>
</div>
<div class="ltx_para" id="Pt0.A7.p2">
<p class="ltx_p" id="Pt0.A7.p2.1"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A7.F9" title="In Appendix 0.G Failure Cases ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">9</span></a> presents the accuracy per class on HMDB51.
Between model (i) and (ii),
model (ii) demonstrated improved performance of actions such as ’cartwheel’, ’sit’, and ’stand’,
which rely on motion information for recognition.
However, in the comparison between model (ii) and (iii),
we found that model (ii) struggled to classify actions like ’kiss’, ’push’, ’shake hands’, and ’wave’,
which involve more subtle and fine-grained motion.</p>
</div>
<div class="ltx_para" id="Pt0.A7.p3">
<p class="ltx_p" id="Pt0.A7.p3.1"><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A7.F10" title="In Appendix 0.G Failure Cases ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">10</span></a> shows the accuracy per class on UCF101.
As in the patterns observed in HMDB51,
model (ii) improved the performance in classes like
’BodyWeightSquats’, ’CleanAndJerk’, ’JumpRope’ and ’YoYo’,
where videos lack object and background cues.
Additionally, model (ii) successfully differentiated between action classes involving similar objects,
for instance, ’BasketballDunk’ versus ’Basketball’, and ’HammerThrow’ versus ’Hammering’.
However, in comparison between model (ii) and (iii),
we found it was difficult for model (ii) to recognize more fine-grained actions such as
’Handstand Walking’, ’Nunchucks’, ’PullUps’, and ’WallPushups’.</p>
</div>
<div class="ltx_para" id="Pt0.A7.p4">
<p class="ltx_p" id="Pt0.A7.p4.1">Our framework struggles to capture fine-grained motion information.
since our PMG applies hand-crafted image transformations globally.
Consequently, the model trained by our framework has difficulty recognizing
fine-grained actions, representing one of the limitations of our framework.
Addressing this issue will be a priority for our future work.</p>
</div>
</section>
<section class="ltx_appendix" id="Pt0.A8">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix 0.H </span>Linear Probing</h2>
<figure class="ltx_table" id="Pt0.A8.T16">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Pt0.A8.T16.14.2.1" style="font-size:129%;">Table 16</span>: </span><span class="ltx_text" id="Pt0.A8.T16.2.1" style="font-size:129%;">
<span class="ltx_text ltx_font_bold" id="Pt0.A8.T16.2.1.1">Results on SynAPT benchmark in the linear probing setting.</span>
<sup class="ltx_sup" id="Pt0.A8.T16.2.1.2"><span class="ltx_text ltx_font_italic" id="Pt0.A8.T16.2.1.2.1">†</span></sup> Results reported in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib38" title="">38</a>]</cite>.
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="Pt0.A8.T16.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Pt0.A8.T16.3.2.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="Pt0.A8.T16.3.2.1.1" rowspan="2"><span class="ltx_text" id="Pt0.A8.T16.3.2.1.1.1" style="font-size:70%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="Pt0.A8.T16.3.2.1.2"><span class="ltx_text" id="Pt0.A8.T16.3.2.1.2.1" style="font-size:70%;">Pre-training</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="Pt0.A8.T16.3.2.1.3"><span class="ltx_text" id="Pt0.A8.T16.3.2.1.3.1" style="font-size:70%;">Downstream Tasks</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A8.T16.3.3.2">
<td class="ltx_td ltx_align_center" id="Pt0.A8.T16.3.3.2.1"><span class="ltx_text" id="Pt0.A8.T16.3.3.2.1.1" style="font-size:70%;">Dataset</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A8.T16.3.3.2.2"><span class="ltx_text" id="Pt0.A8.T16.3.3.2.2.1" style="font-size:70%;">#data</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Pt0.A8.T16.3.3.2.3"><span class="ltx_text" id="Pt0.A8.T16.3.3.2.3.1" style="font-size:70%;"> labels</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A8.T16.3.3.2.4"><span class="ltx_text" id="Pt0.A8.T16.3.3.2.4.1" style="font-size:70%;">UCF101</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A8.T16.3.3.2.5"><span class="ltx_text" id="Pt0.A8.T16.3.3.2.5.1" style="font-size:70%;">HMDB51</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A8.T16.3.3.2.6"><span class="ltx_text" id="Pt0.A8.T16.3.3.2.6.1" style="font-size:70%;">MiniSSV2</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A8.T16.3.3.2.7"><span class="ltx_text" id="Pt0.A8.T16.3.3.2.7.1" style="font-size:70%;">Diving48</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A8.T16.3.3.2.8"><span class="ltx_text" id="Pt0.A8.T16.3.3.2.8.1" style="font-size:70%;">IkeaFA</span></td>
<td class="ltx_td ltx_align_center" id="Pt0.A8.T16.3.3.2.9"><span class="ltx_text" id="Pt0.A8.T16.3.3.2.9.1" style="font-size:70%;">UAV-H</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A8.T16.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A8.T16.3.1.1">
<span class="ltx_text" id="Pt0.A8.T16.3.1.1.1" style="font-size:70%;">TimeSformer</span><sup class="ltx_sup" id="Pt0.A8.T16.3.1.1.2"><span class="ltx_text ltx_font_italic" id="Pt0.A8.T16.3.1.1.2.1" style="font-size:70%;">†</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A8.T16.3.1.2">
<table class="ltx_tabular ltx_align_middle" id="Pt0.A8.T16.3.1.2.1">
<tr class="ltx_tr" id="Pt0.A8.T16.3.1.2.1.1">
<td class="ltx_td ltx_align_center" id="Pt0.A8.T16.3.1.2.1.1.1"><span class="ltx_text" id="Pt0.A8.T16.3.1.2.1.1.1.1" style="font-size:70%;">IN-21k</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A8.T16.3.1.2.1.2">
<td class="ltx_td ltx_align_center" id="Pt0.A8.T16.3.1.2.1.2.1"><span class="ltx_text" id="Pt0.A8.T16.3.1.2.1.2.1.1" style="font-size:70%;">+Synthetic</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A8.T16.3.1.3"><span class="ltx_text" id="Pt0.A8.T16.3.1.3.1" style="font-size:70%;">150k</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Pt0.A8.T16.3.1.4"><span class="ltx_text" id="Pt0.A8.T16.3.1.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A8.T16.3.1.5"><span class="ltx_text" id="Pt0.A8.T16.3.1.5.1" style="font-size:70%;">82.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A8.T16.3.1.6"><span class="ltx_text" id="Pt0.A8.T16.3.1.6.1" style="font-size:70%;">49.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A8.T16.3.1.7"><span class="ltx_text" id="Pt0.A8.T16.3.1.7.1" style="font-size:70%;">21.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A8.T16.3.1.8"><span class="ltx_text" id="Pt0.A8.T16.3.1.8.1" style="font-size:70%;">19.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A8.T16.3.1.9"><span class="ltx_text" id="Pt0.A8.T16.3.1.9.1" style="font-size:70%;">45.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A8.T16.3.1.10"><span class="ltx_text" id="Pt0.A8.T16.3.1.10.1" style="font-size:70%;">13.8</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A8.T16.3.4.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A8.T16.3.4.3.1">
<span class="ltx_text" id="Pt0.A8.T16.3.4.3.1.1" style="font-size:70%;">PPMA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="Pt0.A8.T16.3.4.3.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib84" title="">84</a><span class="ltx_text" id="Pt0.A8.T16.3.4.3.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A8.T16.3.4.3.2">
<table class="ltx_tabular ltx_align_middle" id="Pt0.A8.T16.3.4.3.2.1">
<tr class="ltx_tr" id="Pt0.A8.T16.3.4.3.2.1.1">
<td class="ltx_td ltx_align_center" id="Pt0.A8.T16.3.4.3.2.1.1.1"><span class="ltx_text" id="Pt0.A8.T16.3.4.3.2.1.1.1.1" style="font-size:70%;">NH-Kinetics</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A8.T16.3.4.3.2.1.2">
<td class="ltx_td ltx_align_center" id="Pt0.A8.T16.3.4.3.2.1.2.1"><span class="ltx_text" id="Pt0.A8.T16.3.4.3.2.1.2.1.1" style="font-size:70%;">+Synthetic</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A8.T16.3.4.3.3"><span class="ltx_text" id="Pt0.A8.T16.3.4.3.3.1" style="font-size:70%;">300k</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Pt0.A8.T16.3.4.3.4"><span class="ltx_text" id="Pt0.A8.T16.3.4.3.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A8.T16.3.4.3.5"><span class="ltx_text" id="Pt0.A8.T16.3.4.3.5.1" style="font-size:70%;">88.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A8.T16.3.4.3.6"><span class="ltx_text" id="Pt0.A8.T16.3.4.3.6.1" style="font-size:70%;">64.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A8.T16.3.4.3.7"><span class="ltx_text" id="Pt0.A8.T16.3.4.3.7.1" style="font-size:70%;">34.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A8.T16.3.4.3.8"><span class="ltx_text" id="Pt0.A8.T16.3.4.3.8.1" style="font-size:70%;">21.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A8.T16.3.4.3.9"><span class="ltx_text" id="Pt0.A8.T16.3.4.3.9.1" style="font-size:70%;">57.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Pt0.A8.T16.3.4.3.10"><span class="ltx_text" id="Pt0.A8.T16.3.4.3.10.1" style="font-size:70%;">19.3</span></td>
</tr>
<tr class="ltx_tr" id="Pt0.A8.T16.3.5.4">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Pt0.A8.T16.3.5.4.1"><span class="ltx_text" id="Pt0.A8.T16.3.5.4.1.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Pt0.A8.T16.3.5.4.2"><span class="ltx_text" id="Pt0.A8.T16.3.5.4.2.1" style="font-size:70%;">Shaders1k</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Pt0.A8.T16.3.5.4.3"><span class="ltx_text" id="Pt0.A8.T16.3.5.4.3.1" style="font-size:70%;">100k</span></td>
<td class="ltx_td ltx_border_bb ltx_border_r ltx_border_t" id="Pt0.A8.T16.3.5.4.4"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Pt0.A8.T16.3.5.4.5"><span class="ltx_text" id="Pt0.A8.T16.3.5.4.5.1" style="font-size:70%;">42.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Pt0.A8.T16.3.5.4.6"><span class="ltx_text" id="Pt0.A8.T16.3.5.4.6.1" style="font-size:70%;">28.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Pt0.A8.T16.3.5.4.7"><span class="ltx_text" id="Pt0.A8.T16.3.5.4.7.1" style="font-size:70%;">10.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Pt0.A8.T16.3.5.4.8"><span class="ltx_text" id="Pt0.A8.T16.3.5.4.8.1" style="font-size:70%;">6.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Pt0.A8.T16.3.5.4.9"><span class="ltx_text" id="Pt0.A8.T16.3.5.4.9.1" style="font-size:70%;">33.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="Pt0.A8.T16.3.5.4.10"><span class="ltx_text" id="Pt0.A8.T16.3.5.4.10.1" style="font-size:70%;">1.1</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="Pt0.A8.p1">
<p class="ltx_p" id="Pt0.A8.p1.1">Another limitation of our framework is that
our framework does not learn high-level semantic features,
because our framework focuses on low-level features and
does not utilize labels during pre-training, unlike PPMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#bib.bib84" title="">84</a>]</cite>.
This limitation leads to lower performance in the linear probing settings,
where the weights of the encoder are frozen while only the linear layer is trained (<a class="ltx_ref" href="https://arxiv.org/html/2409.06665v1#Pt0.A8.T16" title="In Appendix 0.H Linear Probing ‣ Data Collection-free Masked Video Modeling"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">16</span></a>).
Moreover, it is challenging to extend our framework
to other tasks like video-text retrieval and video captioning,
without additional training or extra labeled data.
We will also tackle this issue in future work.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep  6 06:19:03 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
