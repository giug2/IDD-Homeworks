<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1911.04559] Privacy is What We Care About: Experimental Investigation of Federated Learning on Edge Devices</title><meta property="og:description" content="Federated Learning enables training of a general model through edge devices without sending raw data to the cloud. Hence, this approach is attractive for digital health applications, where data is sourced through edge â€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Privacy is What We Care About: Experimental Investigation of Federated Learning on Edge Devices">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Privacy is What We Care About: Experimental Investigation of Federated Learning on Edge Devices">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1911.04559">

<!--Generated on Sat Mar 16 17:28:15 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="federated learning,  privacy preserving,  edge computing">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Privacy is What We Care About: Experimental Investigation of Federated Learning on Edge Devices</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anirban Das
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.1.id1" class="ltx_text ltx_affiliation_institution">Rensselaer Polytechnic Institute
<br class="ltx_break">Troy, New York, USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:dasa2@rpi.edu">dasa2@rpi.edu</a>
</span></span></span>
<span class="ltx_author_before">Â andÂ </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Thomas Brunschwiler
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_affiliation_institution">IBM Research Zurich
<br class="ltx_break">RÃ¼schlikon, Switzerland</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:tbr@zurich.ibm.com">tbr@zurich.ibm.com</a>
</span></span></span>
</div>
<div class="ltx_dates">(2019)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id1.1" class="ltx_p"><span id="id1.1.1" class="ltx_text ltx_font_italic">Federated Learning</span> enables training of a general model through edge devices without sending raw data to the cloud. Hence, this approach is attractive for digital health applications, where data is sourced through edge devices and users care about privacy.
Here, we report on the feasibility to train deep neural networks on the Raspberry Pi4s as edge devices. A CNN, a LSTM and a MLP were successfully trained on the MNIST data-set. Further, federated learning is demonstrated experimentally on IID and non-IID samples in a parametric study, to benchmark the model convergence. The weight updates from the workers are shared with the cloud to train the general model through federated learning. With the CNN and the non-IID samples a test-accuracy of up to 85% could be achieved within a training time of 2 minutes, while exchanging less than <math id="id1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="id1.1.m1.1a"><mn id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><cn type="integer" id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">10</annotation></semantics></math> MB data per device.
In addition, we discuss federated learning from an use-case standpoint, elaborating on privacy risks and labeling requirements for the application of emotion detection from sound. Based on the experimental findings, we discuss possible research directions to improve model and system performance. Finally, we provide best practices for a practitioner, considering the implementation of federated learning.</p>
</div>
<div class="ltx_keywords">federated learning, privacy preserving, edge computing
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">copyright: </span>acmcopyright</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Security and privacy</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Computing methodologiesÂ Cooperation and coordination</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Computing methodologiesÂ Neural networks</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journalyear: </span>2019</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">conference: </span>First International Workshop on Challenges in Artificial Intelligence and Machine Learning ; November 10â€“13, 2019; New York, NY, USA</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_booktitle"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">booktitle: </span>First International Workshop on Challenges in Artificial Intelligence and Machine Learning (AIChallengeIoTâ€™19), November 10â€“13, 2019, New York, NY, USA</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_price"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">price: </span>15.00</span></span></span><span id="id9" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">doi: </span>10.1145/3363347.3363365</span></span></span><span id="id10" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">isbn: </span>978-1-4503-7013-4/19/11</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recently, the availability of low-cost and performant edge devices has resulted in a trend of pushing intelligent services towards the edge of the network. Moreover, these devices have resulted in an exponential increase of data collected at the network edgeÂ <cite class="ltx_cite ltx_citemacro_citep">(Middleton etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite>. The availability of such smart devices and data facilitates the deployment of advanced deep learning algorithms in proximity of the consumer, in effect doing edge computing.
In general, large data sets need to be accessible during model training to yield the required accuracy of deep neural networks. Accordingly, recorded data sets from the edge devices are sent to the cloud to perform the model training on the consolidated data. The trained models are then deployed to the edge devices for inference. For personal sensitive data (e.g. audio and video recordings) this approach suffers from user acceptance.
User expectations and regulations (e.g. GDPR) drive increasing levels of data security and privacy of personal sensitive data, also for machine learning applications <cite class="ltx_cite ltx_citemacro_citep">(Shokri and
Shmatikov, <a href="#bib.bib7" title="" class="ltx_ref">2015</a>)</cite>.Thus, we explore a distributed machine learning approach named <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">Federated Learning</span> to respond to the trend. Federated learning allows training models with data-sets stored on individual clients. The locally optimized model weights are then aggregated centrally to train a general global model.
By design, some of the privacy concerns can be alleviated as the raw data never leaves the premises of the patient or hospital. Moreover, we can add additional layers of e.g. secure multi-party communication, homomorphic encryption or differential privacy <cite class="ltx_cite ltx_citemacro_citep">(Abadi etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2016</a>; Yang
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite> to increase the level of security further.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Why federated learning?</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Federated Learning:</span> The term <span id="S2.p1.1.2" class="ltx_text ltx_font_italic">Federated Learning</span> was first proposed by <cite class="ltx_cite ltx_citemacro_citep">(McMahan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2016</a>)</cite>, with the algorithm <span id="S2.p1.1.3" class="ltx_text ltx_font_italic">FedAvg</span>. Here, training happens in multiple <span id="S2.p1.1.4" class="ltx_text ltx_font_italic">rounds</span> of communication. The central server first sends the current global model to the clients/workers (we use the term client and worker interchangeably).
The set of clients then perform local optimization and communicate the learned model weights to the central server. The central server learns a global shared model by methodically aggregating the updates from the clients. Preferably, the training on the clients is performed over several batches to minimize network traffic.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In many cases, data owners have constraints on directly sharing their raw data for model training on the cloud due to either privacy, security or legal constraints. However, they could benefit from a cooperative learning process to train the global model. Federated learning can be appropriate in this context. Only the weights of the model representing a statistical summary of many raw data samples are sent to the cloud, resulting in an inherently privacy preserving approach.
Further, for federated supervised learning to work, we need the presence of labelled data at the edge. Labels can be obtained from various avenues such as the user self reports or from reports by non-colluding experts during training. In case of unsupervised learning, however, no labels are required.
</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Data Distribution</span>: The availability of independent and identical distributed (IID) and non-IID data for a worker depends on the use-case. In health-care applications, a single patient, with a rather personalized data distribution is the source of data per edge device, resulting in non-IID training. However, in scenarios of predictive maintenance of machinery, with identical device types as data source, the data can be assumed to be IID. Thus, we explore the importance of data distribution types an the training performance.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1911.04559/assets/x1.png" id="S2.F1.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="341" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.F1.sf1.3.2" class="ltx_text" style="font-size:90%;">CNN</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1911.04559/assets/x2.png" id="S2.F1.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="341" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.F1.sf2.3.2" class="ltx_text" style="font-size:90%;">LSTM</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>. </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">Forward and backward propagation latency versus batch size</span></figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Learning at the edge</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">For federated learning, execution of substantial machine learning (training) workloads needs to be feasible on resource constrained edge devices.
In order to explore the feasibility of training, we consider the newly introduced Raspberry Pi 4 devices (with 1.5 GHz 4 core processor and 4GB RAM) as the edge device. Further, we assume the edge devices are connected to steady energy sources. For our experiments, we do not deploy sensors, but store raw data as files on the workers (i.e. image files from the MNIST data-set).
We consider 3 network models for the feasibility study:
</p>
</div>
<div id="S3.p2" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.3" class="ltx_p"><span id="S3.I1.i1.p1.3.1" class="ltx_text" style="font-size:80%;">CNN: Conv2d(16) </span><math id="S3.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.I1.i1.p1.1.m1.1a"><mo mathsize="80%" stretchy="false" id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><ci id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">\rightarrow</annotation></semantics></math><span id="S3.I1.i1.p1.3.2" class="ltx_text" style="font-size:80%;"> Conv2d(32) </span><math id="S3.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.I1.i1.p1.2.m2.1a"><mo mathsize="80%" stretchy="false" id="S3.I1.i1.p1.2.m2.1.1" xref="S3.I1.i1.p1.2.m2.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.2.m2.1b"><ci id="S3.I1.i1.p1.2.m2.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.2.m2.1c">\rightarrow</annotation></semantics></math><span id="S3.I1.i1.p1.3.3" class="ltx_text" style="font-size:80%;"> Maxpool(2) </span><math id="S3.I1.i1.p1.3.m3.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.I1.i1.p1.3.m3.1a"><mo mathsize="80%" stretchy="false" id="S3.I1.i1.p1.3.m3.1.1" xref="S3.I1.i1.p1.3.m3.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.3.m3.1b"><ci id="S3.I1.i1.p1.3.m3.1.1.cmml" xref="S3.I1.i1.p1.3.m3.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.3.m3.1c">\rightarrow</annotation></semantics></math><span id="S3.I1.i1.p1.3.4" class="ltx_text" style="font-size:80%;"> FC(10) ; </span><span id="S3.I1.i1.p1.3.5" class="ltx_text ltx_font_italic" style="font-size:80%;">(47K params)</span><span id="S3.I1.i1.p1.3.6" class="ltx_text" style="font-size:80%;"></span></p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.2" class="ltx_p"><span id="S3.I1.i2.p1.2.1" class="ltx_text" style="font-size:80%;">LSTM: LSTM(128) </span><math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><mo mathsize="80%" stretchy="false" id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><ci id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">\rightarrow</annotation></semantics></math><span id="S3.I1.i2.p1.2.2" class="ltx_text" style="font-size:80%;"> LSTM(64) </span><math id="S3.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.I1.i2.p1.2.m2.1a"><mo mathsize="80%" stretchy="false" id="S3.I1.i2.p1.2.m2.1.1" xref="S3.I1.i2.p1.2.m2.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.1b"><ci id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.1c">\rightarrow</annotation></semantics></math><span id="S3.I1.i2.p1.2.3" class="ltx_text" style="font-size:80%;"> FC(8) ; </span><span id="S3.I1.i2.p1.2.4" class="ltx_text ltx_font_italic" style="font-size:80%;">(641K params)</span><span id="S3.I1.i2.p1.2.5" class="ltx_text" style="font-size:80%;"></span></p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.2" class="ltx_p"><span id="S3.I1.i3.p1.2.1" class="ltx_text" style="font-size:80%;">MLP: FC(512) </span><math id="S3.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.I1.i3.p1.1.m1.1a"><mo mathsize="80%" stretchy="false" id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><ci id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">\rightarrow</annotation></semantics></math><span id="S3.I1.i3.p1.2.2" class="ltx_text" style="font-size:80%;"> FC(256) </span><math id="S3.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.I1.i3.p1.2.m2.1a"><mo mathsize="80%" stretchy="false" id="S3.I1.i3.p1.2.m2.1.1" xref="S3.I1.i3.p1.2.m2.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.2.m2.1b"><ci id="S3.I1.i3.p1.2.m2.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.2.m2.1c">\rightarrow</annotation></semantics></math><span id="S3.I1.i3.p1.2.3" class="ltx_text" style="font-size:80%;"> FC(10) ; </span><span id="S3.I1.i3.p1.2.4" class="ltx_text ltx_font_italic" style="font-size:80%;">(1.7M params)</span><span id="S3.I1.i3.p1.2.5" class="ltx_text" style="font-size:80%;"></span></p>
</div>
</li>
</ol>
<p id="S3.p2.7" class="ltx_p">In Fig.Â <a href="#S2.F1" title="Figure 1 â€£ 2. Why federated learning? â€£ Privacy is What We Care About: Experimental Investigation of Federated Learning on Edge Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and Fig.Â <a href="#S3.F2" title="Figure 2 â€£ 3. Learning at the edge â€£ Privacy is What We Care About: Experimental Investigation of Federated Learning on Edge Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> we show the latency for forward and backward propagation during training vs. the increase in batch size. The input to the CNN and the MLP are random matrices of size (<math id="S3.p2.1.m1.1" class="ltx_Math" alttext="1\times 28\times 28" display="inline"><semantics id="S3.p2.1.m1.1a"><mrow id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml"><mn id="S3.p2.1.m1.1.1.2" xref="S3.p2.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.p2.1.m1.1.1.1" xref="S3.p2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.p2.1.m1.1.1.3" xref="S3.p2.1.m1.1.1.3.cmml">28</mn><mo lspace="0.222em" rspace="0.222em" id="S3.p2.1.m1.1.1.1a" xref="S3.p2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.p2.1.m1.1.1.4" xref="S3.p2.1.m1.1.1.4.cmml">28</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"><times id="S3.p2.1.m1.1.1.1.cmml" xref="S3.p2.1.m1.1.1.1"></times><cn type="integer" id="S3.p2.1.m1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.2">1</cn><cn type="integer" id="S3.p2.1.m1.1.1.3.cmml" xref="S3.p2.1.m1.1.1.3">28</cn><cn type="integer" id="S3.p2.1.m1.1.1.4.cmml" xref="S3.p2.1.m1.1.1.4">28</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">1\times 28\times 28</annotation></semantics></math>) and (<math id="S3.p2.2.m2.1" class="ltx_Math" alttext="1\times 3*32*32" display="inline"><semantics id="S3.p2.2.m2.1a"><mrow id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml"><mrow id="S3.p2.2.m2.1.1.2" xref="S3.p2.2.m2.1.1.2.cmml"><mn id="S3.p2.2.m2.1.1.2.2" xref="S3.p2.2.m2.1.1.2.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.p2.2.m2.1.1.2.1" xref="S3.p2.2.m2.1.1.2.1.cmml">Ã—</mo><mn id="S3.p2.2.m2.1.1.2.3" xref="S3.p2.2.m2.1.1.2.3.cmml">3</mn></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.p2.2.m2.1.1.1" xref="S3.p2.2.m2.1.1.1.cmml">âˆ—</mo><mn id="S3.p2.2.m2.1.1.3" xref="S3.p2.2.m2.1.1.3.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S3.p2.2.m2.1.1.1a" xref="S3.p2.2.m2.1.1.1.cmml">âˆ—</mo><mn id="S3.p2.2.m2.1.1.4" xref="S3.p2.2.m2.1.1.4.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"><times id="S3.p2.2.m2.1.1.1.cmml" xref="S3.p2.2.m2.1.1.1"></times><apply id="S3.p2.2.m2.1.1.2.cmml" xref="S3.p2.2.m2.1.1.2"><times id="S3.p2.2.m2.1.1.2.1.cmml" xref="S3.p2.2.m2.1.1.2.1"></times><cn type="integer" id="S3.p2.2.m2.1.1.2.2.cmml" xref="S3.p2.2.m2.1.1.2.2">1</cn><cn type="integer" id="S3.p2.2.m2.1.1.2.3.cmml" xref="S3.p2.2.m2.1.1.2.3">3</cn></apply><cn type="integer" id="S3.p2.2.m2.1.1.3.cmml" xref="S3.p2.2.m2.1.1.3">32</cn><cn type="integer" id="S3.p2.2.m2.1.1.4.cmml" xref="S3.p2.2.m2.1.1.4">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">1\times 3*32*32</annotation></semantics></math>) respectively to simulate the dimensionality of common data-sets of MNIST and CIFAR. The input to the LSTM is (<math id="S3.p2.3.m3.1" class="ltx_Math" alttext="8\times 1024" display="inline"><semantics id="S3.p2.3.m3.1a"><mrow id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml"><mn id="S3.p2.3.m3.1.1.2" xref="S3.p2.3.m3.1.1.2.cmml">8</mn><mo lspace="0.222em" rspace="0.222em" id="S3.p2.3.m3.1.1.1" xref="S3.p2.3.m3.1.1.1.cmml">Ã—</mo><mn id="S3.p2.3.m3.1.1.3" xref="S3.p2.3.m3.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><apply id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1"><times id="S3.p2.3.m3.1.1.1.cmml" xref="S3.p2.3.m3.1.1.1"></times><cn type="integer" id="S3.p2.3.m3.1.1.2.cmml" xref="S3.p2.3.m3.1.1.2">8</cn><cn type="integer" id="S3.p2.3.m3.1.1.3.cmml" xref="S3.p2.3.m3.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">8\times 1024</annotation></semantics></math>) with 8 timesteps and 1024 dimensions to simulate an input embedding from SoundNet, which we aim to use for Speech Emotion Recognition (discussed in Sec.Â <a href="#S5.SS2" title="5.2. Usecase study â€£ 5. Usecase and Recommendations â€£ Privacy is What We Care About: Experimental Investigation of Federated Learning on Edge Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>). We measure the actual CPU time spent in forward and backward propagation using PyTorchâ€™s â€˜autograd profilerâ€™ and present the mean results over 50 runs per batch size.
<br class="ltx_break"><span id="S3.p2.7.1" class="ltx_text ltx_font_bold">Results and Discussion:</span>
We observe that for all 3 cases, the back propagation is substantially slower than the forward pass. For the CNN we observe a linear increase with the same factor in CPU time vs. batch size. The latency for processing a batch size of 32 is <math id="S3.p2.4.m4.1" class="ltx_Math" alttext="&lt;600" display="inline"><semantics id="S3.p2.4.m4.1a"><mrow id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml"><mi id="S3.p2.4.m4.1.1.2" xref="S3.p2.4.m4.1.1.2.cmml"></mi><mo id="S3.p2.4.m4.1.1.1" xref="S3.p2.4.m4.1.1.1.cmml">&lt;</mo><mn id="S3.p2.4.m4.1.1.3" xref="S3.p2.4.m4.1.1.3.cmml">600</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><apply id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1"><lt id="S3.p2.4.m4.1.1.1.cmml" xref="S3.p2.4.m4.1.1.1"></lt><csymbol cd="latexml" id="S3.p2.4.m4.1.1.2.cmml" xref="S3.p2.4.m4.1.1.2">absent</csymbol><cn type="integer" id="S3.p2.4.m4.1.1.3.cmml" xref="S3.p2.4.m4.1.1.3">600</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">&lt;600</annotation></semantics></math>ms. Further, we observe that the variance of both latencies increase with batch size in case of the CNN. For the LSTM, the forward propagation scales linearly with batch size. The difference between forward and back propagation is much larger and back propagation latency has a weak linear trend. In our experiments, we went up to moderate batch-size of 40, yet the Raspberry Pi4 was still able to support the load with an average delay of <math id="S3.p2.5.m5.1" class="ltx_Math" alttext="&lt;1.4~{}s" display="inline"><semantics id="S3.p2.5.m5.1a"><mrow id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml"><mi id="S3.p2.5.m5.1.1.2" xref="S3.p2.5.m5.1.1.2.cmml"></mi><mo id="S3.p2.5.m5.1.1.1" xref="S3.p2.5.m5.1.1.1.cmml">&lt;</mo><mrow id="S3.p2.5.m5.1.1.3" xref="S3.p2.5.m5.1.1.3.cmml"><mn id="S3.p2.5.m5.1.1.3.2" xref="S3.p2.5.m5.1.1.3.2.cmml">1.4</mn><mo lspace="0.330em" rspace="0em" id="S3.p2.5.m5.1.1.3.1" xref="S3.p2.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S3.p2.5.m5.1.1.3.3" xref="S3.p2.5.m5.1.1.3.3.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><apply id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1"><lt id="S3.p2.5.m5.1.1.1.cmml" xref="S3.p2.5.m5.1.1.1"></lt><csymbol cd="latexml" id="S3.p2.5.m5.1.1.2.cmml" xref="S3.p2.5.m5.1.1.2">absent</csymbol><apply id="S3.p2.5.m5.1.1.3.cmml" xref="S3.p2.5.m5.1.1.3"><times id="S3.p2.5.m5.1.1.3.1.cmml" xref="S3.p2.5.m5.1.1.3.1"></times><cn type="float" id="S3.p2.5.m5.1.1.3.2.cmml" xref="S3.p2.5.m5.1.1.3.2">1.4</cn><ci id="S3.p2.5.m5.1.1.3.3.cmml" xref="S3.p2.5.m5.1.1.3.3">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">&lt;1.4~{}s</annotation></semantics></math>. In older Raspberry Pi3 we started getting â€˜seg-faultsâ€˜ from batch size 8 with the same model prompting training impossible. The fact that we can easily go over batch size 40 in Raspberry Pi4 without memory error is already a huge improvement.
For the MLP, we observe that the forward and backward propagation time for batch sizes up-to 32 does not vary much and stays below <math id="S3.p2.6.m6.1" class="ltx_Math" alttext="\approx 250ms" display="inline"><semantics id="S3.p2.6.m6.1a"><mrow id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml"><mi id="S3.p2.6.m6.1.1.2" xref="S3.p2.6.m6.1.1.2.cmml"></mi><mo id="S3.p2.6.m6.1.1.1" xref="S3.p2.6.m6.1.1.1.cmml">â‰ˆ</mo><mrow id="S3.p2.6.m6.1.1.3" xref="S3.p2.6.m6.1.1.3.cmml"><mn id="S3.p2.6.m6.1.1.3.2" xref="S3.p2.6.m6.1.1.3.2.cmml">250</mn><mo lspace="0em" rspace="0em" id="S3.p2.6.m6.1.1.3.1" xref="S3.p2.6.m6.1.1.3.1.cmml">â€‹</mo><mi id="S3.p2.6.m6.1.1.3.3" xref="S3.p2.6.m6.1.1.3.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.p2.6.m6.1.1.3.1a" xref="S3.p2.6.m6.1.1.3.1.cmml">â€‹</mo><mi id="S3.p2.6.m6.1.1.3.4" xref="S3.p2.6.m6.1.1.3.4.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><apply id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1"><approx id="S3.p2.6.m6.1.1.1.cmml" xref="S3.p2.6.m6.1.1.1"></approx><csymbol cd="latexml" id="S3.p2.6.m6.1.1.2.cmml" xref="S3.p2.6.m6.1.1.2">absent</csymbol><apply id="S3.p2.6.m6.1.1.3.cmml" xref="S3.p2.6.m6.1.1.3"><times id="S3.p2.6.m6.1.1.3.1.cmml" xref="S3.p2.6.m6.1.1.3.1"></times><cn type="integer" id="S3.p2.6.m6.1.1.3.2.cmml" xref="S3.p2.6.m6.1.1.3.2">250</cn><ci id="S3.p2.6.m6.1.1.3.3.cmml" xref="S3.p2.6.m6.1.1.3.3">ğ‘š</ci><ci id="S3.p2.6.m6.1.1.3.4.cmml" xref="S3.p2.6.m6.1.1.3.4">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">\approx 250ms</annotation></semantics></math> for back propagation, and <math id="S3.p2.7.m7.1" class="ltx_Math" alttext="\approx 60ms" display="inline"><semantics id="S3.p2.7.m7.1a"><mrow id="S3.p2.7.m7.1.1" xref="S3.p2.7.m7.1.1.cmml"><mi id="S3.p2.7.m7.1.1.2" xref="S3.p2.7.m7.1.1.2.cmml"></mi><mo id="S3.p2.7.m7.1.1.1" xref="S3.p2.7.m7.1.1.1.cmml">â‰ˆ</mo><mrow id="S3.p2.7.m7.1.1.3" xref="S3.p2.7.m7.1.1.3.cmml"><mn id="S3.p2.7.m7.1.1.3.2" xref="S3.p2.7.m7.1.1.3.2.cmml">60</mn><mo lspace="0em" rspace="0em" id="S3.p2.7.m7.1.1.3.1" xref="S3.p2.7.m7.1.1.3.1.cmml">â€‹</mo><mi id="S3.p2.7.m7.1.1.3.3" xref="S3.p2.7.m7.1.1.3.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.p2.7.m7.1.1.3.1a" xref="S3.p2.7.m7.1.1.3.1.cmml">â€‹</mo><mi id="S3.p2.7.m7.1.1.3.4" xref="S3.p2.7.m7.1.1.3.4.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.7.m7.1b"><apply id="S3.p2.7.m7.1.1.cmml" xref="S3.p2.7.m7.1.1"><approx id="S3.p2.7.m7.1.1.1.cmml" xref="S3.p2.7.m7.1.1.1"></approx><csymbol cd="latexml" id="S3.p2.7.m7.1.1.2.cmml" xref="S3.p2.7.m7.1.1.2">absent</csymbol><apply id="S3.p2.7.m7.1.1.3.cmml" xref="S3.p2.7.m7.1.1.3"><times id="S3.p2.7.m7.1.1.3.1.cmml" xref="S3.p2.7.m7.1.1.3.1"></times><cn type="integer" id="S3.p2.7.m7.1.1.3.2.cmml" xref="S3.p2.7.m7.1.1.3.2">60</cn><ci id="S3.p2.7.m7.1.1.3.3.cmml" xref="S3.p2.7.m7.1.1.3.3">ğ‘š</ci><ci id="S3.p2.7.m7.1.1.3.4.cmml" xref="S3.p2.7.m7.1.1.3.4">ğ‘ </ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.7.m7.1c">\approx 60ms</annotation></semantics></math> for forward propagation with some jitters. For larger batch size of up to 512 we see both the latencies go up linearly, with low variance. This suggests that up-to a certain problem size, the bottleneck is most likely in the framework and not in the compute performance of the hardware. Also, the iterations in the MLP are much faster than the CNN or RNN inspite of higher number of parameters. We note that compute time not only depends on the number of parameters or operations, but also on other factors such as framework implementation, memory access patterns, caching effects, special mathematical operation overheads, compiler optimization etc. If there are time constraints for training, MLP network could therefore be a good choice given it performs well in the use-case.</p>
</div>
<figure id="S3.F2" class="ltx_figure ltx_align_floatright"><img src="/html/1911.04559/assets/x3.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="341" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>. </span><span id="S3.F2.4.2" class="ltx_text" style="font-size:90%;">Forward and back propagation latency versus batch size in MLP</span></figcaption>
</figure>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">In conclusion, with the new Raspberry Pi4 we can train small to moderate networks with reasonable batch sizes. Moreover, the latency to train each iteration over such batch sizes is also very low and can be done in several hundred milliseconds to a few seconds. The hardware improvement over the previous generation Raspberry Pi3 has mainly contributed to its usefulness for <span id="S3.p3.1.1" class="ltx_text ltx_font_italic">training</span> deep networks.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1911.04559/assets/x4.png" id="S3.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="342" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">IID</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1911.04559/assets/x5.png" id="S3.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="342" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">Non-IID</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1911.04559/assets/x6.png" id="S3.F3.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="341" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.sf3.4.2.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S3.F3.sf3.2.1" class="ltx_text" style="font-size:90%;">Non-IID Convergence vs. <math id="S3.F3.sf3.2.1.m1.1" class="ltx_Math" alttext="\mathbf{E}" display="inline"><semantics id="S3.F3.sf3.2.1.m1.1b"><mi id="S3.F3.sf3.2.1.m1.1.1" xref="S3.F3.sf3.2.1.m1.1.1.cmml">ğ„</mi><annotation-xml encoding="MathML-Content" id="S3.F3.sf3.2.1.m1.1c"><ci id="S3.F3.sf3.2.1.m1.1.1.cmml" xref="S3.F3.sf3.2.1.m1.1.1">ğ„</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.sf3.2.1.m1.1d">\mathbf{E}</annotation></semantics></math></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>. </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">Convergence vs. number of local batches per round</span></figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Federated Learning Performance</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.5" class="ltx_p">In this section we study the performance of federated learning on resource constrained edge devices.
We use an internally developed federated learning framework for our experiments. Experiments with PySyft<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/OpenMined/PySyft</span></span></span> and comparative bench-marking is a future goal.
Hardware-wise, we created a test-bed consisting of 5 Raspberry Pi 4s. Each device is connected to the central server, a Macbook Pro
via a 300 Mbits wireless router. We have used PyTorch v1.1.0 and all code developed in Python v3.7.
Experiments are performed using the <span id="S4.p1.5.1" class="ltx_text ltx_font_italic">FedAvg</span> algorithm.
In the experiment
we used all 5 clients (<math id="S4.p1.1.m1.1" class="ltx_Math" alttext="K=5" display="inline"><semantics id="S4.p1.1.m1.1a"><mrow id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">K</mi><mo id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><eq id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1"></eq><ci id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">ğ¾</ci><cn type="integer" id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">K=5</annotation></semantics></math>) for each round. We define <span id="S4.p1.5.2" class="ltx_text ltx_font_italic">round</span> as one iteration of the whole <span id="S4.p1.5.3" class="ltx_text ltx_font_italic">FedAvg</span> process.
Further, iterating through all local data points per round is difficult for resource constrained workers. For ease of performance characterization, we re-define <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{E}" display="inline"><semantics id="S4.p1.2.m2.1a"><mi id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">ğ„</mi><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><ci id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">ğ„</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">\mathbf{E}</annotation></semantics></math> to denote the number of local batches of fixed size each worker trains in each round and <math id="S4.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{B}" display="inline"><semantics id="S4.p1.3.m3.1a"><mi id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><ci id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">\mathbf{B}</annotation></semantics></math> as the batch size. The magnitude of hyperparameters <math id="S4.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{E}" display="inline"><semantics id="S4.p1.4.m4.1a"><mi id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml">ğ„</mi><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.1b"><ci id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1">ğ„</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.1c">\mathbf{E}</annotation></semantics></math> and <math id="S4.p1.5.m5.1" class="ltx_Math" alttext="\mathbf{B}" display="inline"><semantics id="S4.p1.5.m5.1a"><mi id="S4.p1.5.m5.1.1" xref="S4.p1.5.m5.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S4.p1.5.m5.1b"><ci id="S4.p1.5.m5.1.1.cmml" xref="S4.p1.5.m5.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.5.m5.1c">\mathbf{B}</annotation></semantics></math>, determines the amount of local computation and thereby training performance.
We use the standard image classification task on the MNIST data-set as the federated learning task, to keep our study comparable to the literature <cite class="ltx_cite ltx_citemacro_citep">(Abadi etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2016</a>; McMahan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2016</a>; Caldas etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2018</a>; Geyer
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.2" class="ltx_p">In the experiment, we studied the compute vs. communication latency of individual workers and the convergence behaviour of federated learning using the CNN from Sec.Â <a href="#S3" title="3. Learning at the edge â€£ Privacy is What We Care About: Experimental Investigation of Federated Learning on Edge Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
Furthermore, <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{E}" display="inline"><semantics id="S4.p2.1.m1.1a"><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">ğ„</mi><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">ğ„</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">\mathbf{E}</annotation></semantics></math>, the number of batches on which training is executed on a client per round was varied, while keeping the batch size fixed at <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{B}" display="inline"><semantics id="S4.p2.2.m2.1a"><mi id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><ci id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">\mathbf{B}</annotation></semantics></math>=16. It should be noticed, for IID distribution, the data is distributed randomly among 5 devices, for the non-IID distribution the 5 workers each contains non overlapping data points corresponding to 2 digits (ie. [0,1], [2,3], â€¦). The accuracy test is done on the central server device on the test set. <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>
has shown that synchronous gradient updates converge faster than synchronous federated learning. Hence we use synchronous gradient updates.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.9" class="ltx_p">For both, the IID and the non-IID case, we run 5 experiments each with <math id="S4.p3.1.m1.4" class="ltx_Math" alttext="\mathbf{E}\in\{10,20,30,40\}" display="inline"><semantics id="S4.p3.1.m1.4a"><mrow id="S4.p3.1.m1.4.5" xref="S4.p3.1.m1.4.5.cmml"><mi id="S4.p3.1.m1.4.5.2" xref="S4.p3.1.m1.4.5.2.cmml">ğ„</mi><mo id="S4.p3.1.m1.4.5.1" xref="S4.p3.1.m1.4.5.1.cmml">âˆˆ</mo><mrow id="S4.p3.1.m1.4.5.3.2" xref="S4.p3.1.m1.4.5.3.1.cmml"><mo stretchy="false" id="S4.p3.1.m1.4.5.3.2.1" xref="S4.p3.1.m1.4.5.3.1.cmml">{</mo><mn id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">10</mn><mo id="S4.p3.1.m1.4.5.3.2.2" xref="S4.p3.1.m1.4.5.3.1.cmml">,</mo><mn id="S4.p3.1.m1.2.2" xref="S4.p3.1.m1.2.2.cmml">20</mn><mo id="S4.p3.1.m1.4.5.3.2.3" xref="S4.p3.1.m1.4.5.3.1.cmml">,</mo><mn id="S4.p3.1.m1.3.3" xref="S4.p3.1.m1.3.3.cmml">30</mn><mo id="S4.p3.1.m1.4.5.3.2.4" xref="S4.p3.1.m1.4.5.3.1.cmml">,</mo><mn id="S4.p3.1.m1.4.4" xref="S4.p3.1.m1.4.4.cmml">40</mn><mo stretchy="false" id="S4.p3.1.m1.4.5.3.2.5" xref="S4.p3.1.m1.4.5.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.4b"><apply id="S4.p3.1.m1.4.5.cmml" xref="S4.p3.1.m1.4.5"><in id="S4.p3.1.m1.4.5.1.cmml" xref="S4.p3.1.m1.4.5.1"></in><ci id="S4.p3.1.m1.4.5.2.cmml" xref="S4.p3.1.m1.4.5.2">ğ„</ci><set id="S4.p3.1.m1.4.5.3.1.cmml" xref="S4.p3.1.m1.4.5.3.2"><cn type="integer" id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">10</cn><cn type="integer" id="S4.p3.1.m1.2.2.cmml" xref="S4.p3.1.m1.2.2">20</cn><cn type="integer" id="S4.p3.1.m1.3.3.cmml" xref="S4.p3.1.m1.3.3">30</cn><cn type="integer" id="S4.p3.1.m1.4.4.cmml" xref="S4.p3.1.m1.4.4">40</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.4c">\mathbf{E}\in\{10,20,30,40\}</annotation></semantics></math>. For each experiment we record the <span id="S4.p3.9.1" class="ltx_text ltx_font_italic">round</span> and corresponding time when the system <span id="S4.p3.9.2" class="ltx_text ltx_font_italic">first</span> achieves 95% and 85% test accuracy for the IID and for the non-IID data-sets, respectively. In Fig.Â <a href="#S3.F3.sf1" title="In Figure 3 â€£ 3. Learning at the edge â€£ Privacy is What We Care About: Experimental Investigation of Federated Learning on Edge Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a>,<a href="#S3.F3.sf2" title="In Figure 3 â€£ 3. Learning at the edge â€£ Privacy is What We Care About: Experimental Investigation of Federated Learning on Edge Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a> the bars represent the average wall clock time, the error bars represent the <span id="S4.p3.9.3" class="ltx_text ltx_font_italic">min</span> and <span id="S4.p3.9.4" class="ltx_text ltx_font_italic">max</span> time observed among 5 runs for each value of <math id="S4.p3.2.m2.1" class="ltx_Math" alttext="\mathbf{E}" display="inline"><semantics id="S4.p3.2.m2.1a"><mi id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">ğ„</mi><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><ci id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1">ğ„</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">\mathbf{E}</annotation></semantics></math> (measured at the central server). We also plot the average round (quantized to closest integer) and min and max rounds needed in the 5 runs for each value of <math id="S4.p3.3.m3.1" class="ltx_Math" alttext="\mathbf{E}" display="inline"><semantics id="S4.p3.3.m3.1a"><mi id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml">ğ„</mi><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><ci id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1">ğ„</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">\mathbf{E}</annotation></semantics></math>. Further, the wall clock time of each device spent on compute, communication or on idle time is recorded, besides the data transmission between the workers and the server using <span id="S4.p3.9.5" class="ltx_text ltx_font_typewriter">pyshark<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span id="footnote2.1.1.1" class="ltx_text ltx_font_serif">2</span></span><span id="footnote2.5" class="ltx_text ltx_font_serif">https://github.com/KimiNewt/pyshark</span></span></span></span></span>. 
<br class="ltx_break"><span id="S4.p3.9.6" class="ltx_text ltx_font_bold">Results and Discussion:</span>
We observe that training on IID data converges to 95% test-accuracy in as low as 6 rounds of federated averaging at <math id="S4.p3.4.m4.1" class="ltx_Math" alttext="\mathbf{E}" display="inline"><semantics id="S4.p3.4.m4.1a"><mi id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml">ğ„</mi><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.1b"><ci id="S4.p3.4.m4.1.1.cmml" xref="S4.p3.4.m4.1.1">ğ„</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.1c">\mathbf{E}</annotation></semantics></math>=40. As expected, with increasing <math id="S4.p3.5.m5.1" class="ltx_Math" alttext="\mathbf{E}" display="inline"><semantics id="S4.p3.5.m5.1a"><mi id="S4.p3.5.m5.1.1" xref="S4.p3.5.m5.1.1.cmml">ğ„</mi><annotation-xml encoding="MathML-Content" id="S4.p3.5.m5.1b"><ci id="S4.p3.5.m5.1.1.cmml" xref="S4.p3.5.m5.1.1">ğ„</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.5.m5.1c">\mathbf{E}</annotation></semantics></math> the number of rounds needed for convergence is dropping rapidly. But once the local computation reaches a certain level <math id="S4.p3.6.m6.1" class="ltx_Math" alttext="\mathbf{E}" display="inline"><semantics id="S4.p3.6.m6.1a"><mi id="S4.p3.6.m6.1.1" xref="S4.p3.6.m6.1.1.cmml">ğ„</mi><annotation-xml encoding="MathML-Content" id="S4.p3.6.m6.1b"><ci id="S4.p3.6.m6.1.1.cmml" xref="S4.p3.6.m6.1.1">ğ„</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.6.m6.1c">\mathbf{E}</annotation></semantics></math><math id="S4.p3.7.m7.1" class="ltx_Math" alttext="=" display="inline"><semantics id="S4.p3.7.m7.1a"><mo id="S4.p3.7.m7.1.1" xref="S4.p3.7.m7.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S4.p3.7.m7.1b"><eq id="S4.p3.7.m7.1.1.cmml" xref="S4.p3.7.m7.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.7.m7.1c">=</annotation></semantics></math>30 or <math id="S4.p3.8.m8.1" class="ltx_Math" alttext="\mathbf{E}" display="inline"><semantics id="S4.p3.8.m8.1a"><mi id="S4.p3.8.m8.1.1" xref="S4.p3.8.m8.1.1.cmml">ğ„</mi><annotation-xml encoding="MathML-Content" id="S4.p3.8.m8.1b"><ci id="S4.p3.8.m8.1.1.cmml" xref="S4.p3.8.m8.1.1">ğ„</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.8.m8.1c">\mathbf{E}</annotation></semantics></math><math id="S4.p3.9.m9.1" class="ltx_Math" alttext="=" display="inline"><semantics id="S4.p3.9.m9.1a"><mo id="S4.p3.9.m9.1.1" xref="S4.p3.9.m9.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S4.p3.9.m9.1b"><eq id="S4.p3.9.m9.1.1.cmml" xref="S4.p3.9.m9.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.9.m9.1c">=</annotation></semantics></math>40, the local models generalize themselves enough, so further training does not improve the model accuracy.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/1911.04559/assets/x7.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="323" height="117" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>. </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Avg. compute time at all clients</span></figcaption>
</figure>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.3" class="ltx_p">For the non-IID case, we observe that the number of rounds to reach 85% accuracy does only decrease moderately with increalocal training per round i.e. increase of <math id="S4.p4.1.m1.1" class="ltx_Math" alttext="\mathbf{E}" display="inline"><semantics id="S4.p4.1.m1.1a"><mi id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml">ğ„</mi><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><ci id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1">ğ„</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">\mathbf{E}</annotation></semantics></math>. In the non-IID case the individual models on clients can diverge substantially by increasing the number of times the local model is updated before the global averaging. Hence, the system needs considerable more time to converge when we increase <math id="S4.p4.2.m2.1" class="ltx_Math" alttext="\mathbf{E}" display="inline"><semantics id="S4.p4.2.m2.1a"><mi id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml">ğ„</mi><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><ci id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1">ğ„</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">\mathbf{E}</annotation></semantics></math>. In general, the actual wall clock time for non-IID is larger than that in IID. However, we still observe good performance (even with limited local computation) with an accuracy of 85% within 2 minutes considering <math id="S4.p4.3.m3.1" class="ltx_Math" alttext="\mathbf{E}" display="inline"><semantics id="S4.p4.3.m3.1a"><mi id="S4.p4.3.m3.1.1" xref="S4.p4.3.m3.1.1.cmml">ğ„</mi><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><ci id="S4.p4.3.m3.1.1.cmml" xref="S4.p4.3.m3.1.1">ğ„</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">\mathbf{E}</annotation></semantics></math>=10 and in 25 rounds. In Fig.<a href="#S3.F3.sf3" title="In Figure 3 â€£ 3. Learning at the edge â€£ Privacy is What We Care About: Experimental Investigation of Federated Learning on Edge Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(c)</span></a> we observe good convergence, where we plot the accuracy and its variance per round averaged over 5 runs.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.3" class="ltx_p">It has to be mentioned, that the variability in performance of the Raspberry Pis is large. We pick an example run from the non-IID experiments (<math id="S4.p5.1.m1.1" class="ltx_Math" alttext="\mathbf{E}" display="inline"><semantics id="S4.p5.1.m1.1a"><mi id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml">ğ„</mi><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><ci id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1">ğ„</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">\mathbf{E}</annotation></semantics></math>=40, <math id="S4.p5.2.m2.1" class="ltx_Math" alttext="\mathbf{B}" display="inline"><semantics id="S4.p5.2.m2.1a"><mi id="S4.p5.2.m2.1.1" xref="S4.p5.2.m2.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S4.p5.2.m2.1b"><ci id="S4.p5.2.m2.1.1.cmml" xref="S4.p5.2.m2.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.2.m2.1c">\mathbf{B}</annotation></semantics></math>=16) to demonstrate this variability for a fixed computation size, however, with varying data points in batches. The average compute time across 100 rounds is depicted in Fig.Â <a href="#S4.F4" title="Figure 4 â€£ 4. Federated Learning Performance â€£ Privacy is What We Care About: Experimental Investigation of Federated Learning on Edge Devices" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> for individual clients.
On average, worker 2 seems to be the straggler in this run. Since we have synchronous rounds of communication, the server waits till it receives model updates from each of the devices. Overall, the maximum average compute time per round of training in a client here is <math id="S4.p5.3.m3.1" class="ltx_Math" alttext="\approx" display="inline"><semantics id="S4.p5.3.m3.1a"><mo id="S4.p5.3.m3.1.1" xref="S4.p5.3.m3.1.1.cmml">â‰ˆ</mo><annotation-xml encoding="MathML-Content" id="S4.p5.3.m3.1b"><approx id="S4.p5.3.m3.1.1.cmml" xref="S4.p5.3.m3.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.3.m3.1c">\approx</annotation></semantics></math> 15s.
On investigating the different components of latency of the slowest worker 2 we further observed that the average compute time on a device is an order of magnitude larger than the time the device spends to send or receive the model weights, rendering the former the bottleneck in the cycle.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.3" class="ltx_p">We find that each worker exchanges <math id="S4.p6.1.m1.1" class="ltx_Math" alttext="\approx 400~{}KB" display="inline"><semantics id="S4.p6.1.m1.1a"><mrow id="S4.p6.1.m1.1.1" xref="S4.p6.1.m1.1.1.cmml"><mi id="S4.p6.1.m1.1.1.2" xref="S4.p6.1.m1.1.1.2.cmml"></mi><mo id="S4.p6.1.m1.1.1.1" xref="S4.p6.1.m1.1.1.1.cmml">â‰ˆ</mo><mrow id="S4.p6.1.m1.1.1.3" xref="S4.p6.1.m1.1.1.3.cmml"><mn id="S4.p6.1.m1.1.1.3.2" xref="S4.p6.1.m1.1.1.3.2.cmml">400</mn><mo lspace="0.330em" rspace="0em" id="S4.p6.1.m1.1.1.3.1" xref="S4.p6.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.p6.1.m1.1.1.3.3" xref="S4.p6.1.m1.1.1.3.3.cmml">K</mi><mo lspace="0em" rspace="0em" id="S4.p6.1.m1.1.1.3.1a" xref="S4.p6.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.p6.1.m1.1.1.3.4" xref="S4.p6.1.m1.1.1.3.4.cmml">B</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.1b"><apply id="S4.p6.1.m1.1.1.cmml" xref="S4.p6.1.m1.1.1"><approx id="S4.p6.1.m1.1.1.1.cmml" xref="S4.p6.1.m1.1.1.1"></approx><csymbol cd="latexml" id="S4.p6.1.m1.1.1.2.cmml" xref="S4.p6.1.m1.1.1.2">absent</csymbol><apply id="S4.p6.1.m1.1.1.3.cmml" xref="S4.p6.1.m1.1.1.3"><times id="S4.p6.1.m1.1.1.3.1.cmml" xref="S4.p6.1.m1.1.1.3.1"></times><cn type="integer" id="S4.p6.1.m1.1.1.3.2.cmml" xref="S4.p6.1.m1.1.1.3.2">400</cn><ci id="S4.p6.1.m1.1.1.3.3.cmml" xref="S4.p6.1.m1.1.1.3.3">ğ¾</ci><ci id="S4.p6.1.m1.1.1.3.4.cmml" xref="S4.p6.1.m1.1.1.3.4">ğµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.1c">\approx 400~{}KB</annotation></semantics></math> back and forth with the server in each round. This is once for downloading the global model weights and sending the updated weights for each round, respectively. For test set convergence to 85% in the non-IID case, the total data transmission between the server and each worker is <math id="S4.p6.2.m2.1" class="ltx_Math" alttext="\approx 10~{}MB" display="inline"><semantics id="S4.p6.2.m2.1a"><mrow id="S4.p6.2.m2.1.1" xref="S4.p6.2.m2.1.1.cmml"><mi id="S4.p6.2.m2.1.1.2" xref="S4.p6.2.m2.1.1.2.cmml"></mi><mo id="S4.p6.2.m2.1.1.1" xref="S4.p6.2.m2.1.1.1.cmml">â‰ˆ</mo><mrow id="S4.p6.2.m2.1.1.3" xref="S4.p6.2.m2.1.1.3.cmml"><mn id="S4.p6.2.m2.1.1.3.2" xref="S4.p6.2.m2.1.1.3.2.cmml">10</mn><mo lspace="0.330em" rspace="0em" id="S4.p6.2.m2.1.1.3.1" xref="S4.p6.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S4.p6.2.m2.1.1.3.3" xref="S4.p6.2.m2.1.1.3.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.p6.2.m2.1.1.3.1a" xref="S4.p6.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S4.p6.2.m2.1.1.3.4" xref="S4.p6.2.m2.1.1.3.4.cmml">B</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.2.m2.1b"><apply id="S4.p6.2.m2.1.1.cmml" xref="S4.p6.2.m2.1.1"><approx id="S4.p6.2.m2.1.1.1.cmml" xref="S4.p6.2.m2.1.1.1"></approx><csymbol cd="latexml" id="S4.p6.2.m2.1.1.2.cmml" xref="S4.p6.2.m2.1.1.2">absent</csymbol><apply id="S4.p6.2.m2.1.1.3.cmml" xref="S4.p6.2.m2.1.1.3"><times id="S4.p6.2.m2.1.1.3.1.cmml" xref="S4.p6.2.m2.1.1.3.1"></times><cn type="integer" id="S4.p6.2.m2.1.1.3.2.cmml" xref="S4.p6.2.m2.1.1.3.2">10</cn><ci id="S4.p6.2.m2.1.1.3.3.cmml" xref="S4.p6.2.m2.1.1.3.3">ğ‘€</ci><ci id="S4.p6.2.m2.1.1.3.4.cmml" xref="S4.p6.2.m2.1.1.3.4">ğµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.2.m2.1c">\approx 10~{}MB</annotation></semantics></math> for <math id="S4.p6.3.m3.1" class="ltx_Math" alttext="\mathbf{E}" display="inline"><semantics id="S4.p6.3.m3.1a"><mi id="S4.p6.3.m3.1.1" xref="S4.p6.3.m3.1.1.cmml">ğ„</mi><annotation-xml encoding="MathML-Content" id="S4.p6.3.m3.1b"><ci id="S4.p6.3.m3.1.1.cmml" xref="S4.p6.3.m3.1.1">ğ„</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.3.m3.1c">\mathbf{E}</annotation></semantics></math>=10 up-to round 25, which is quite low, but has potential to result in substantial traffic with more devices.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Usecase and Recommendations</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Possible attack and data loss scenarios</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">When data is sent to the cloud for training, adversarial attacks on the central server may compromise information of all clients, resulting in a massive security breach scenario, affecting millions of users. Vanilla federated learning prevents leakage of raw data from the cloud, by transferring only statistical aggregates, i.e. the model weights to a central server. Thus, data would only leak from individual devices, compromising only one or few users. However, addition of techniques such as differential privacy could improve the robustness against adversarial attacks even further.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Usecase study</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">One interesting federated learning use-case is emotion detection from audio signals. This could be useful
in elderly care homes for detecting depression or general mood of the elderly to take precautionary health measures and their improving quality-of-life. With permission of the elderly, a small edge device can be installed in the room to intermittently record utterance level audio from the person.
Now, due to the sensitive nature of the information which was captured, in accordance with the GDPR and other privacy laws, it is imperative that the processing and training is taken place on premises.
For labelling, either the elderly could be asked to fill out a self-report or the concerned nurse or doctor could perform the labeling of the emotion of the recorded audio clips each day.
Here, we could make use of federated learning, where each Raspberry Pi in the room of each elderly acts as a worker device. The first tier of privacy concern is alleviated as the raw data is not transmitted to the cloud. Further, if weights are intercepted, it can be very hard to reconstruct an audio signal from compromised model weights and thus, the motivation to do so depends on the cost benefit for the adversary. Additionally, differential privacy techniques such as gradient clipping and addition of Laplace or Gaussian noise to the weights transmitted as done by authors in <cite class="ltx_cite ltx_citemacro_citep">(Geyer
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2017</a>)</cite> could be considered.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Challenges and opportunities</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p"><span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_bold">Communication Structure</span>: Traditionally, federated learning is hierarchical with edge nodes and one central â€˜cloudâ€™ server. We can envision using fog computing for hierarchical gradient aggregation and have aggregate models corresponding to different geo-spatial scale at different hierarchies instead of a single central server. However, ensuring security and privacy will be challenging.
<br class="ltx_break"><span id="S5.SS3.p1.1.2" class="ltx_text ltx_font_bold">Hardware Limitations</span>: Sometimes the learning task can be very hard and resource hungry. A good example is emotion detection from audio, which needs advanced deep networks such as FCN, TCN etc., and may be infeasible to run on resource constrained edge devices. Here, we could train a general model on the cloud using the full network architecture and later fine tune it by only training the last few layers using federated learning at the edge. 
<br class="ltx_break"><span id="S5.SS3.p1.1.3" class="ltx_text ltx_font_bold">Stragglers</span>: For synchronous gradient updates, the straggler nodes contribute substantially to the overall latency. One solution can be to accept <span id="S5.SS3.p1.1.4" class="ltx_text ltx_font_italic">approximate</span> gradient updates from workers as proposed by authors in <cite class="ltx_cite ltx_citemacro_citep">(Smith
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite>. Another solution can be to piggyback the compute and resource utilization with client updates.
Based on this, a predictive performance model for the workers can be formed and dynamically adjust to the amount of local computation by controlling <math id="S5.SS3.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{E}" display="inline"><semantics id="S5.SS3.p1.1.m1.1a"><mi id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml">ğ„</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><ci id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">ğ„</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">\mathbf{E}</annotation></semantics></math> and batch size. 
<br class="ltx_break"><span id="S5.SS3.p1.1.5" class="ltx_text ltx_font_bold">Personalization</span>: The voice pattern and emotional state of each person is different, thus, the generated data will be highly non-IID. Hence, it may be effective to maintain a personalized model locally, alongside the global model. For this purpose, we can perform transfer learning at the edge by transferring the frozen global model to the edge and retrain a fully connected layer or an SVM on local data using the global model as feature extractor. Federated multitask learning proposed in <cite class="ltx_cite ltx_citemacro_citep">(Smith
etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite> could also be used for personalization.
<span id="S5.SS3.p1.1.6" class="ltx_text ltx_font_bold">Framework Limitations</span>: So far, there is a dearth of frameworks to seamlessly deploy federated learning workloads. PySyft and Tensorflow Federated are the prominent leaders, but their deployability and scalability are limited. A possible way may be to use Kubernetes cluster as the central hub where nodes may join as workers. It could also be interesting to integrate federated learning with edge computing frameworks like AWS Greengrass, Azure IoT Edge, KubeEdge etc. to deploy worker logic.
<br class="ltx_break"><span id="S5.SS3.p1.1.7" class="ltx_text ltx_font_bold">Benchmarking</span>: As more frameworks emerge, it is important to evaluate those systems with standardized workloads and representative use cases on real testbeds to understand trade-offs in performance and decide future bottlenecks. So far, a large part of the literature has internal implementations of federated learning framework due to reasons mentioned above. <cite class="ltx_cite ltx_citemacro_citep">(Caldas etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite> provides an excellent benchmark suite, but we need more such studies to tackle the breadth of application requirements.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We briefly studied the feasibility of federated learning on resource constrained edge devices. We demonstrated the capability of current devices to train deep neural networks and presented the performance numbers on MNIST data-set trainings. For practitioners considering health-care applications with vital signs, audio or video data (data with limited damage) we recommend a) RISK: federated learning should be applied to limit data leaks to individual clients (edge devices) rather than millions of data samples from the cloud, b) TRAINING: for model training speed, consider low number of batch per round in case of non-IID data, but keep a minimum batch size to ensure enough learning from the data. c) DATA TRANSFER: In the IID case, the number of batch per round should be increased to minimize data transfer and according cost (especially if communication over channels inferior to WiFi are used). Federated learning on edge devices is an interesting new field and we hope to see more exciting research in this area.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
We want to thank Supriyo Chakraborty, Dean Steuer, Shalisha Witherspoon from IBM Research (Yorktown) for providing us support and insights in using the federated learning framework.

</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadi etÂ al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Martin Abadi, Andy Chu,
Ian Goodfellow, HÂ Brendan McMahan,
Ilya Mironov, Kunal Talwar, and
Li Zhang. 2016.

</span>
<span class="ltx_bibblock">Deep learning with differential privacy. In
<em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">Proc. ACM SIGSAC Conf. on Computer and
Communications Security</em>. ACM,
308â€“318.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldas etÂ al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Sebastian Caldas, Peter
Wu, Tian Li, Jakub KoneÄná»³,
HÂ Brendan McMahan, Virginia Smith, and
Ameet Talwalkar. 2018.

</span>
<span class="ltx_bibblock">Leaf: A benchmark for federated settings.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.01097</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geyer
etÂ al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
RobinÂ C Geyer, Tassilo
Klein, and Moin Nabi. 2017.

</span>
<span class="ltx_bibblock">Differentially private federated learning: A client
level perspective.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1712.07557</em>
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan etÂ al<span id="bib.bib5.3.3.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
HÂ Brendan McMahan, Eider
Moore, Daniel Ramage, Seth Hampson,
etÂ al<span id="bib.bib5.4.1" class="ltx_text">.</span> 2016.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks
from decentralized data.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.5.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1602.05629</em>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Middleton etÂ al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Peter Middleton, Tracy
Tsai, Masatsune Yamaji, Anurag Gupta,
and Denise Rueb. 2017.

</span>
<span class="ltx_bibblock">Forecast: Internet of Things - Endpoints and
Associated Services, Worldwide, 2017, Gartner.

</span>
<span class="ltx_bibblock">(2017).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">[Online; Accessed August 2019].

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shokri and
Shmatikov (2015)</span>
<span class="ltx_bibblock">
Reza Shokri and Vitaly
Shmatikov. 2015.

</span>
<span class="ltx_bibblock">Privacy-preserving deep learning. In
<em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proc. ACM SIGSAC Conf. on Computer and
Communications Security</em>. ACM,
1310â€“1321.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith
etÂ al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Virginia Smith, Chao-Kai
Chiang, Maziar Sanjabi, and Ameet
Talwalkar. 2017.

</span>
<span class="ltx_bibblock">Federated Multi-task Learning. In
<em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">Proc. Int. Conf. on Neural Information Processing
Systems</em> <em id="bib.bib8.4.2" class="ltx_emph ltx_font_italic">(NIPSâ€™17)</em>. Curran
Associates Inc., USA, 4427â€“4437.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
S. Wang, T. Tuor,
T. Salonidis, K.Â K. Leung,
C. Makaya, T. He, and
K. Chan. 2019.

</span>
<span class="ltx_bibblock">Adaptive Federated Learning in Resource Constrained
Edge Computing Systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">IEEE J. Sel. Areas Commun.</em>
37, 6 (June
2019), 1205â€“1221.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang
etÂ al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu,
Tianjian Chen, and Yongxin Tong.
2019.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and
applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">ACM Trans. Intel. Syst. Tec.</em>
10, 2 (2019),
12.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1911.04558" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1911.04559" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1911.04559">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1911.04559" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1911.04560" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar 16 17:28:15 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
