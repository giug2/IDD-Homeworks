<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2104.03042] 1 Introduction</title><meta property="og:description" content="Federated Learning (FL) allows edge devices to collaboratively learn a shared prediction model while keeping their training data on the device, thereby decoupling the ability to do machine learning from the need to sto…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="1 Introduction">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="1 Introduction">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2104.03042">

<!--Generated on Sat Mar  9 12:15:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">marginparsep has been altered.
<br class="ltx_break">topmargin has been altered.
<br class="ltx_break">marginparwidth has been altered.
<br class="ltx_break">marginparpush has been altered.
<br class="ltx_break"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">The page layout violates the ICML style.</span>
Please do not change the page layout, or include packages like geometry,
savetrees, or fullpage, which change it for you.
We’re not able to reliably undo arbitrary changes to the style. Please remove
the offending package(s), or layout-changing commands and try again.</p>
</div>
<div id="p3" class="ltx_para ltx_align_center">
<p id="p3.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:2.0pt;background:black;display:inline-block;"> </span></p>
</div>
<div id="p4" class="ltx_para ltx_align_center">
<p id="p4.1" class="ltx_p"><span id="p4.1.1" class="ltx_text ltx_font_smallcaps">On-device federated learning with Flower</span></p>
</div>
<div id="p5" class="ltx_para ltx_align_center">
<p id="p5.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span></p>
</div>
<div id="p6" class="ltx_para">
<p id="p6.1" class="ltx_p ltx_align_center"><span id="p6.1.1" class="ltx_text ltx_font_bold">Anonymous Authors</span><sup id="p6.1.2" class="ltx_sup">1 </sup></p>
</div>
<div id="p7" class="ltx_para ltx_noindent">
<br class="ltx_break">
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Federated Learning (FL) allows edge devices to collaboratively learn a shared prediction model while keeping their training data on the device, thereby decoupling the ability to do machine learning from the need to store data in the cloud. Despite the algorithmic advancements in FL, the support for on-device training of FL algorithms on edge devices remains poor. In this paper, we present an exploration of on-device FL on various smartphones and embedded devices using the Flower framework. We also evaluate the system costs of on-device FL and discuss how this quantification could be used to design more efficient FL algorithms.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span>
<sup id="footnotex1.1" class="ltx_sup">1</sup>Anonymous Institution, Anonymous City, Anonymous Region, Anonymous Country.
Correspondence to: Anonymous Author &lt;anon.email@domain.com&gt;.
 
<br class="ltx_break">Preliminary work. Under review by the
Machine Learning and Systems (MLSys) Conference. Do not distribute.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">We have seen remarkable progress in enabling the execution of deep learning models on mobile and embedded devices to infer user contexts and behaviors <cite class="ltx_cite ltx_citemacro_cite">Warden &amp; Situnayake (<a href="#bib.bib22" title="" class="ltx_ref">2019</a>); Fromm et al. (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>); Chowdhery et al. (<a href="#bib.bib7" title="" class="ltx_ref">2019</a>); Malekzadeh et al. (<a href="#bib.bib16" title="" class="ltx_ref">2019</a>); Lee et al. (<a href="#bib.bib13" title="" class="ltx_ref">2019</a>)</cite>. This has been powered by the increasing computational abilities of edge devices as well as novel software optimizations to enable cloud-scale models to run on resource-constrained devices. However, when it comes to the training of these edge-focused models, a working assumption has been that the models will be trained centrally in the cloud, using training data aggregated from several users.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib17" title="" class="ltx_ref">2017</a>)</cite> aims to enable distributed edge devices (or users) to collaboratively <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">train</em> a shared prediction model while keeping their personal data private. At a high level, this is achieved by repeating three basic steps: i) local parameters update to a shared prediction model on each edge device, ii) sending the local parameter updates to a central server for aggregation, and iii) receiving the aggregated model back for the next round of local updates.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">A major bottleneck to FL research is the paucity of frameworks that support federated training of workloads on mobile and embedded devices. While several frameworks including Tensorflow Federated <cite class="ltx_cite ltx_citemacro_cite">Google (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>); Abadi et al. (<a href="#bib.bib1" title="" class="ltx_ref">2016</a>)</cite> and LEAF <cite class="ltx_cite ltx_citemacro_cite">Caldas et al. (<a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite> enable simulation of FL clients, they cannot be used to understand the training dynamics and compute the system costs of FL on edge devices. Edge devices exhibit significant heterogeneity in their software stack, compute capabilities, and network bandwidth. All these system-related factors, in combination with the choice of the FL clients and parameter aggregation algorithms, can impact the accuracy and training time of models trained in a federated setting.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">In this paper, we present our exploration of on-device training of FL workloads on Android smartphones and Nvidia Jetson series embedded devices, using the Flower framework <cite class="ltx_cite ltx_citemacro_cite">Beutel et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>. Flower offers a stable implementation of the core components of an FL system, and provides higher-level abstractions to enable researchers to experiment and implement new ideas on top of a reliable stack. We first demonstrate how we use the language-, platform- and ML framework-agnostic capabilities of Flower to support on-device training of FL workloads on edge devices with heterogeneous hardware and software stacks. We then deploy these FL clients on various embedded devices as well as on Android smartphones hosted in the Amazon AWS Device Farm (<a target="_blank" href="https://aws.amazon.com/device-farm/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aws.amazon.com/device-farm/</a>). Finally, we present an evaluation to compute various system-related metrics of FL and highlight how this quantification could lead to the design of more efficient FL algorithms.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">McMahan et al. <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib17" title="" class="ltx_ref">2017</a>)</cite> introduced the basic federated averaging (FedAvg) algorithm and evaluated it in terms of communication efficiency. The optimization of distributed training with and without federated concepts has been covered from many angles <cite class="ltx_cite ltx_citemacro_cite">Jia et al. (<a href="#bib.bib12" title="" class="ltx_ref">2018</a>); Chahal et al. (<a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>. Bonawitz et al. <cite class="ltx_cite ltx_citemacro_citeyearpar">(<a href="#bib.bib3" title="" class="ltx_ref">2019</a>)</cite> detail the design of a large-scale Google-internal FL system. TFF <cite class="ltx_cite ltx_citemacro_cite">Google (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>, PySyft <cite class="ltx_cite ltx_citemacro_cite">Ryffel et al. (<a href="#bib.bib20" title="" class="ltx_ref">2018</a>)</cite>, LEAF <cite class="ltx_cite ltx_citemacro_cite">Caldas et al. (<a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite>, FedML <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite> are other open-source frameworks that support research and experimentation of FL workloads.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p">Given their relative hardware limitations, most of the works involving machine learning on mobile devices were originally aimed at adapting existing models to specific latency and storage constraints. For this purpose, optimized versions of TensorFlow and PyTorch were developed <cite class="ltx_cite ltx_citemacro_cite">David et al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>); Paszke et al. (<a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite> and can now be considered mainstream. Federated Learning, on the other hand, shifts the training burden from server to the client, which in turn creates the need for developing the adequate supporting back-ends (e.g. back-propagation) for low-power hardware as well. Recent works towards this goal include using low-precision training <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite>, controlled updates of biases to reduce memory <cite class="ltx_cite ltx_citemacro_cite">Cai et al. (<a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>, and early-exit models that provide a trade-off between accuracy and compute <cite class="ltx_cite ltx_citemacro_cite">Leontiadis et al. (<a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>. In time, as hardware capabilities in edge devices improve, we expect to see these advances being deployed by mainstream frameworks.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Primer on Flower</h2>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2104.03042/assets/diagrams/flower-core-framework-architecture.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="323" height="161" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Flower framework architecture.
<br class="ltx_break"></figcaption>
</figure>
<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">Flower is a novel client-agnostic federated learning framework. One of the underlying design goals of Flower is to enable integrating with an inherently heterogeneous and ever-evolving edge device landscape. There are multiple dimensions of on-device heterogeneity, amongst them are operating systems, machine learning frameworks, programming languages, connectivity, and hardware accelerators.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p">The Flower core framework, shown in Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Primer on Flower" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, implements the infrastructure to run these heterogeneous workloads at scale. On the server side, there are three major components involved: the FL loop, the RPC server, and a (user customizable) <em id="S3.p2.1.1" class="ltx_emph ltx_font_italic">Strategy</em>. Strategy here refers to the federated averaging algorithms (e.g., FedAvg) used for aggregating the model parameters across clients. Clients connect to the RPC server which is responsible for monitoring these connections and for sending and receiving <em id="S3.p2.1.2" class="ltx_emph ltx_font_italic">Flower Protocol</em> messages. The FL loop is at the heart of the FL process: it orchestrates the learning process and ensures that progress is made. It does not, however, make decisions about <em id="S3.p2.1.3" class="ltx_emph ltx_font_italic">how</em> to proceed, those decisions are delegated to the currently configured <em id="S3.p2.1.4" class="ltx_emph ltx_font_italic">Strategy</em> implementation.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p">A distinctive property of this architecture is a server which is unaware of the nature of connected clients. This allows to train models across heterogeneous client platforms and implementations, including workloads comprised of different client-side ML frameworks. Furthermore, on-device training and evaluation can be implemented in different programming languages, a property especially important for research on mobile and embedded platforms. These platforms often do not support Python, but rely on specific languages (Java on Android, Swift on iOS) for idiomatic development, or native C/C++ for some embedded devices. Flower achieves a fully language-agnostic interface by offering protocol-level integration. The <em id="S3.p3.1.1" class="ltx_emph ltx_font_italic">Flower Protocol</em> defines core server-side messages such as <em id="S3.p3.1.2" class="ltx_emph ltx_font_italic">fit</em> and <em id="S3.p3.1.3" class="ltx_emph ltx_font_italic">evaluate</em>, which include the (serialized) global model parameters and expect return messages from the client that return either updated model parameters (or gradients) or evaluation results. Each message contains additional user-customizable metadata that allows the server to control on-device hyper-parameters, for example, the number of on-device training epochs. Due to space constraints, we refer the reader to <cite class="ltx_cite ltx_citemacro_cite">Beutel et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite> and <a target="_blank" href="https://flower.dev/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://flower.dev/</a> for more details on Flower.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Flower Clients and on-device training</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">In this section, we describe two instances of on-device federated learning with Flower. First, we present how Flower clients can be developed in Java and deployed on Android phones in the AWS Device Farm for federated model training. Next, we discuss the implementation of Flower clients in Python and their deployment on heterogeneous embedded devices such as Nvidia Jetson series and Raspberry Pi.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2104.03042/assets/diagrams/android_arch.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="236" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Flower Android client architecture.
<br class="ltx_break"></figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Java Flower Clients for Android Smartphones</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">By design, Flower is language-agnostic and can work with any ML framework on the FL client, which maximizes its ability to federate existing training pipelines. However, it also means Flower inherits the limitations of these frameworks that currently offer limited support for on-device training on Android devices.
</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p">In the absence of a full-fledged model training library for Android, we employ the TensorFlow Lite (TFLite) Model Personalization support on Android to perform on-device federated learning. More specifically, as shown in Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Flower Clients and on-device training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we first obtain a pre-trained <em id="S4.SS1.p2.1.1" class="ltx_emph ltx_font_italic">Base Model</em> (e.g., MobileNetV2 without its top layers). The parameters of this model are frozen (i.e., not updated during training), and it is only used as a feature extractor. Next, we define a <em id="S4.SS1.p2.1.2" class="ltx_emph ltx_font_italic">Head Model</em> which corresponds to the task-specific classifier that we want to train using federated learning. The input to the <em id="S4.SS1.p2.1.3" class="ltx_emph ltx_font_italic">Head Model</em> are the features extracted from the <em id="S4.SS1.p2.1.4" class="ltx_emph ltx_font_italic">Base Model</em>, and its weights are randomly initialized. Finally, we use the <span id="S4.SS1.p2.1.5" class="ltx_text ltx_font_smallcaps">TFLite Transfer Convertor</span> to port the Base and Head models to TFLite and package them inside an Android application for on-device training.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p">Inside the Android application, <span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_smallcaps">Flower Client</span> is a class which coordinates with the TFLite Model Personalization libraries and implements the three core methods required for federated training with Flower, namely (i) get_weights (.), which gets the current weights of the <em id="S4.SS1.p3.1.2" class="ltx_emph ltx_font_italic">Head Model</em> to support server-side aggregation requests, (ii) fit (.), which updates the parameters of the Head Model through local training and (iii) evaluate (.), which computes test loss on the local dataset and communicates it to the server.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.1" class="ltx_p">The <span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_smallcaps">Flower Client</span> spawns a background thread and sets up bi-directional streaming RPC with the Flower server using the <span id="S4.SS1.p4.1.2" class="ltx_text ltx_font_smallcaps">StreamObserver</span> class. Upon receiving messages from the Flower server, the background thread calls the appropriate TFLite methods to update the parameters of the <em id="S4.SS1.p4.1.3" class="ltx_emph ltx_font_italic">Head Model</em> using the local training data (e.g., by optimizing a cross-entropy loss for classification tasks). The updated parameters are sent to the Flower Server where they are aggregated across all clients, before being sent back to the clients for the next round of training.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Python Flower Clients on Embedded Devices</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">Contrary to Android, on-device training on Python-enabled embedded devices can be performed using platform-specific ML frameworks (e.g., TensorFlow, PyTorch). However, one of the open challenges to deploy on-device training workloads arises due to the heterogeneity of computing resources present in such devices. For example, NVIDIA Jetson devices provide GPU acceleration, while Raspberry Pi is limited to CPU-only workloads. Implementing different versions of an FL client for each target platform can quickly become unpractical in real-world scenarios.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p">Flower overcomes this challenge by providing a platform-agnostic way of writing FL clients. Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Python Flower Clients on Embedded Devices ‣ 4 Flower Clients and on-device training" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the design of Flower Clients for a NVIDIA Jetson TX2 and a Raspberry Pi device. Despite the differences in these two embedded platforms, we can use the same code to develop Flower Clients, and run them inside a platform-specific Docker image, enabling the client code to access platform-specific resources (e.g. GPU) when available.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p">More importantly, this platform-agnostic capability of Flower can be combined with its language-agnostic capabilities (i.e., support for Python, Java, C++ clients) to enable on-device training of federated workloads in highly-heterogeneous client setups, e.g., when the FL clients include Android phones, Android watches, Raspberry Pis, ARM micro-controllers, Nvidia Jetson devices etc.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2104.03042/assets/diagrams/embedded_devices.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="161" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Flower clients can easily be deployed to heterogeneous devices by leveraging existing container-based frameworks (e.g. Docker) that interface with the host’s hardware.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluation</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">Our evaluation focuses on quantifying the system costs associated with running FL on various edge devices. In doing so, we also explore how such quantification could help FL developers to design novel algorithms that effectively trade-off between system costs and FL accuracy.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Android phones used from the AWS Device Farm</figcaption>
<table id="S5.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_tt"><span id="S5.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Device Name</span></th>
<th id="S5.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S5.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Type</span></th>
<th id="S5.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S5.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">OS Version</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.1.2.1" class="ltx_tr">
<td id="S5.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Google Pixel 4</td>
<td id="S5.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Phone</td>
<td id="S5.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10</td>
</tr>
<tr id="S5.T1.1.3.2" class="ltx_tr">
<td id="S5.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Google Pixel 3</td>
<td id="S5.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">Phone</td>
<td id="S5.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">10</td>
</tr>
<tr id="S5.T1.1.4.3" class="ltx_tr">
<td id="S5.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Google Pixel 2</td>
<td id="S5.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">Phone</td>
<td id="S5.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">9</td>
</tr>
<tr id="S5.T1.1.5.4" class="ltx_tr">
<td id="S5.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Samsung Galaxy Tab S6</td>
<td id="S5.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">Tablet</td>
<td id="S5.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">9</td>
</tr>
<tr id="S5.T1.1.6.5" class="ltx_tr">
<td id="S5.T1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_l ltx_border_r">Samsung Galaxy Tab S4</td>
<td id="S5.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Tablet</td>
<td id="S5.T1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">8.1.0</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Datasets.</span> Two datasets are used in our evaluation, namely <em id="S5.p2.1.2" class="ltx_emph ltx_font_italic">CIFAR-10</em> and <em id="S5.p2.1.3" class="ltx_emph ltx_font_italic">Office-31</em> <cite class="ltx_cite ltx_citemacro_cite">Office31 (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>, both of which are examples of object recognition datasets.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">Deployment Setup.</span> We run the Flower Server configured with the <span id="S5.p3.1.2" class="ltx_text ltx_font_typewriter">FedAvg</span> strategy and host it on a cloud virtual machine. Two sets of edge devices are used in the evaluation: Android smartphones and Nvidia Jetson TX2 accelerators. To scale our experiments to a reasonably large number of Android clients with different OS versions, we deploy Flower Clients on the Amazon AWS Device Farm, which enables testing applications on real Android devices accessed through AWS. Table <a href="#S5.T1" title="Table 1 ‣ 5 Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> list the smartphones from AWS Device Farm used in our evaluation. Nvidia Jetson TX2 devices support full-fledged PyTorch – this means we could successfully port existing PyTorch training pipelines to implement FL clients on them.</p>
</div>
<figure id="S5.T2" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Flower supports implementation of FL clients on any device that has on-device training support. Here we show various FL experiments on Android and Nvidia Jetson devices.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.T2.st1" class="ltx_table ltx_figure_panel">
<table id="S5.T2.st1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.st1.3.1.1" class="ltx_tr">
<th id="S5.T2.st1.3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">
<table id="S5.T2.st1.3.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.st1.3.1.1.1.1.1" class="ltx_tr">
<td id="S5.T2.st1.3.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.st1.3.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Local</span></td>
</tr>
<tr id="S5.T2.st1.3.1.1.1.1.2" class="ltx_tr">
<td id="S5.T2.st1.3.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.st1.3.1.1.1.1.2.1.1" class="ltx_text ltx_font_bold">Epochs (E)</span></td>
</tr>
</table>
</th>
<th id="S5.T2.st1.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.st1.3.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></th>
<th id="S5.T2.st1.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S5.T2.st1.3.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.st1.3.1.1.3.1.1" class="ltx_tr">
<td id="S5.T2.st1.3.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.st1.3.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">Convergence</span></td>
</tr>
<tr id="S5.T2.st1.3.1.1.3.1.2" class="ltx_tr">
<td id="S5.T2.st1.3.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.st1.3.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">Time (mins)</span></td>
</tr>
</table>
</th>
<th id="S5.T2.st1.3.1.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S5.T2.st1.3.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.st1.3.1.1.4.1.1" class="ltx_tr">
<td id="S5.T2.st1.3.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.st1.3.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold">Energy</span></td>
</tr>
<tr id="S5.T2.st1.3.1.1.4.1.2" class="ltx_tr">
<td id="S5.T2.st1.3.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.st1.3.1.1.4.1.2.1.1" class="ltx_text ltx_font_bold">Consumed (kJ)</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.st1.3.2.1" class="ltx_tr">
<th id="S5.T2.st1.3.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">1</th>
<td id="S5.T2.st1.3.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.48</td>
<td id="S5.T2.st1.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t">17.63</td>
<td id="S5.T2.st1.3.2.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">10.21</td>
</tr>
<tr id="S5.T2.st1.3.3.2" class="ltx_tr">
<th id="S5.T2.st1.3.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">5</th>
<td id="S5.T2.st1.3.3.2.2" class="ltx_td ltx_align_center">0.64</td>
<td id="S5.T2.st1.3.3.2.3" class="ltx_td ltx_align_center">36.83</td>
<td id="S5.T2.st1.3.3.2.4" class="ltx_td ltx_nopad_r ltx_align_center">50.54</td>
</tr>
<tr id="S5.T2.st1.3.4.3" class="ltx_tr">
<th id="S5.T2.st1.3.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">10</th>
<td id="S5.T2.st1.3.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">0.67</td>
<td id="S5.T2.st1.3.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">80.32</td>
<td id="S5.T2.st1.3.4.3.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">100.95</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">(a) </span>Performance metrics with Nvidia Jetson TX2 as we vary the number of local epochs. Number of clients <math id="S5.T2.st1.2.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S5.T2.st1.2.m1.1b"><mi id="S5.T2.st1.2.m1.1.1" xref="S5.T2.st1.2.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S5.T2.st1.2.m1.1c"><ci id="S5.T2.st1.2.m1.1.1.cmml" xref="S5.T2.st1.2.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.st1.2.m1.1d">C</annotation></semantics></math> is set to 10 and the model is trained for 40 rounds.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.T2.st2" class="ltx_table ltx_figure_panel">
<table id="S5.T2.st2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.st2.3.1.1" class="ltx_tr">
<th id="S5.T2.st2.3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">
<table id="S5.T2.st2.3.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.st2.3.1.1.1.1.1" class="ltx_tr">
<td id="S5.T2.st2.3.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.st2.3.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Number</span></td>
</tr>
<tr id="S5.T2.st2.3.1.1.1.1.2" class="ltx_tr">
<td id="S5.T2.st2.3.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.st2.3.1.1.1.1.2.1.1" class="ltx_text ltx_font_bold">of Clients (C)</span></td>
</tr>
</table>
</th>
<td id="S5.T2.st2.3.1.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S5.T2.st2.3.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
<td id="S5.T2.st2.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S5.T2.st2.3.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.st2.3.1.1.3.1.1" class="ltx_tr">
<td id="S5.T2.st2.3.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.st2.3.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">Convergence</span></td>
</tr>
<tr id="S5.T2.st2.3.1.1.3.1.2" class="ltx_tr">
<td id="S5.T2.st2.3.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.st2.3.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">Time (mins)</span></td>
</tr>
</table>
</td>
<td id="S5.T2.st2.3.1.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">
<table id="S5.T2.st2.3.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.st2.3.1.1.4.1.1" class="ltx_tr">
<td id="S5.T2.st2.3.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.st2.3.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold">Energy</span></td>
</tr>
<tr id="S5.T2.st2.3.1.1.4.1.2" class="ltx_tr">
<td id="S5.T2.st2.3.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.st2.3.1.1.4.1.2.1.1" class="ltx_text ltx_font_bold">Consumed (kJ)</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T2.st2.3.2.2" class="ltx_tr">
<th id="S5.T2.st2.3.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">4</th>
<td id="S5.T2.st2.3.2.2.2" class="ltx_td ltx_align_center ltx_border_t">0.84</td>
<td id="S5.T2.st2.3.2.2.3" class="ltx_td ltx_align_center ltx_border_t">30.7</td>
<td id="S5.T2.st2.3.2.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">10.4</td>
</tr>
<tr id="S5.T2.st2.3.3.3" class="ltx_tr">
<th id="S5.T2.st2.3.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">7</th>
<td id="S5.T2.st2.3.3.3.2" class="ltx_td ltx_align_center">0.85</td>
<td id="S5.T2.st2.3.3.3.3" class="ltx_td ltx_align_center">31.3</td>
<td id="S5.T2.st2.3.3.3.4" class="ltx_td ltx_nopad_r ltx_align_center">19.72</td>
</tr>
<tr id="S5.T2.st2.3.4.4" class="ltx_tr">
<th id="S5.T2.st2.3.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">10</th>
<td id="S5.T2.st2.3.4.4.2" class="ltx_td ltx_align_center ltx_border_bb">0.87</td>
<td id="S5.T2.st2.3.4.4.3" class="ltx_td ltx_align_center ltx_border_bb">31.8</td>
<td id="S5.T2.st2.3.4.4.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">28.0</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">(b) </span>Performance metrics with Android clients as we vary the number of clients. Local epochs <math id="S5.T2.st2.2.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S5.T2.st2.2.m1.1b"><mi id="S5.T2.st2.2.m1.1.1" xref="S5.T2.st2.2.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S5.T2.st2.2.m1.1c"><ci id="S5.T2.st2.2.m1.1.1.cmml" xref="S5.T2.st2.2.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.st2.2.m1.1d">E</annotation></semantics></math> is fixed to 5 in this experiment and the model is trained for 20 rounds.</figcaption>
</figure>
</div>
</div>
</figure>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.3" class="ltx_p"><span id="S5.p4.3.1" class="ltx_text ltx_font_bold">System Costs of FL.</span> In Table <a href="#S5.T2.st2" title="In Table 2 ‣ 5 Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>, we present various performance metrics obtained on Nvidia TX2 and Android devices. First, we train a ResNet-18 model on the CIFAR-10 dataset on 10 Nvidia TX2 clients. In Table <a href="#S5.T2.st1" title="In Table 2 ‣ 5 Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a>, we vary the number of local training epochs (<math id="S5.p4.1.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S5.p4.1.m1.1a"><mi id="S5.p4.1.m1.1.1" xref="S5.p4.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S5.p4.1.m1.1b"><ci id="S5.p4.1.m1.1.1.cmml" xref="S5.p4.1.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.1.m1.1c">E</annotation></semantics></math>) performed on each client in a round of FL. Our results show that choosing a higher <math id="S5.p4.2.m2.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S5.p4.2.m2.1a"><mi id="S5.p4.2.m2.1.1" xref="S5.p4.2.m2.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S5.p4.2.m2.1b"><ci id="S5.p4.2.m2.1.1.cmml" xref="S5.p4.2.m2.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.2.m2.1c">E</annotation></semantics></math> results in better FL accuracy, however it also comes at the expense of significant increase in total training time and overall energy consumption across the clients. While the accuracy metrics in Table <a href="#S5.T2.st1" title="In Table 2 ‣ 5 Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a> could have been obtained in a simulated setup, quantifying the training time and energy costs on real clients would not have been possible without a real on-device deployment. As reducing the energy and carbon footprint of training ML models is a major challenge for the community, Flower can assist researchers in choosing an optimal value of <math id="S5.p4.3.m3.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S5.p4.3.m3.1a"><mi id="S5.p4.3.m3.1.1" xref="S5.p4.3.m3.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S5.p4.3.m3.1b"><ci id="S5.p4.3.m3.1.1.cmml" xref="S5.p4.3.m3.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.3.m3.1c">E</annotation></semantics></math> to obtain the best trade-off between accuracy and energy consumption.</p>
</div>
<div id="S5.p5" class="ltx_para ltx_noindent">
<p id="S5.p5.2" class="ltx_p">Next, we train a 2-layer DNN classifier (<em id="S5.p5.2.1" class="ltx_emph ltx_font_italic">Head Model</em>) on top of a pre-trained MobileNetV2 <em id="S5.p5.2.2" class="ltx_emph ltx_font_italic">Base Model</em> on Android clients for the Office-31 dataset. In Table <a href="#S5.T2.st2" title="In Table 2 ‣ 5 Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>, we vary the number of Android clients (<math id="S5.p5.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S5.p5.1.m1.1a"><mi id="S5.p5.1.m1.1.1" xref="S5.p5.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S5.p5.1.m1.1b"><ci id="S5.p5.1.m1.1.1.cmml" xref="S5.p5.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.1.m1.1c">C</annotation></semantics></math>) participating in FL, while keeping the local training epochs (<math id="S5.p5.2.m2.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S5.p5.2.m2.1a"><mi id="S5.p5.2.m2.1.1" xref="S5.p5.2.m2.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S5.p5.2.m2.1b"><ci id="S5.p5.2.m2.1.1.cmml" xref="S5.p5.2.m2.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.2.m2.1c">E</annotation></semantics></math>) on each client fixed to 5. We observe that by increasing the number of clients, we can train a more accurate object recognition model. Intuitively, as more clients participate in the training, the model gets exposed to more diverse training examples, thereby increasing its generalizability to unseen test samples. However, this accuracy gain comes at the expense of high energy consumption – the more clients we use, the higher the total energy consumption of FL. Again, based on this analysis obtained using Flower, researchers can choose an appropriate number of clients to find a balance between accuracy and energy consumption.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Effect of computational heterogeneity on FL training times. Using Flower, we can compute a hardware-specific cutoff <math id="S5.T3.3.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S5.T3.3.m1.1b"><mi id="S5.T3.3.m1.1.1" xref="S5.T3.3.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S5.T3.3.m1.1c"><ci id="S5.T3.3.m1.1.1.cmml" xref="S5.T3.3.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.m1.1d">\tau</annotation></semantics></math> (in minutes) for each processor, and find a balance between FL accuracy and training time. <math id="S5.T3.4.m2.1" class="ltx_Math" alttext="\tau=0" display="inline"><semantics id="S5.T3.4.m2.1b"><mrow id="S5.T3.4.m2.1.1" xref="S5.T3.4.m2.1.1.cmml"><mi id="S5.T3.4.m2.1.1.2" xref="S5.T3.4.m2.1.1.2.cmml">τ</mi><mo id="S5.T3.4.m2.1.1.1" xref="S5.T3.4.m2.1.1.1.cmml">=</mo><mn id="S5.T3.4.m2.1.1.3" xref="S5.T3.4.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.4.m2.1c"><apply id="S5.T3.4.m2.1.1.cmml" xref="S5.T3.4.m2.1.1"><eq id="S5.T3.4.m2.1.1.1.cmml" xref="S5.T3.4.m2.1.1.1"></eq><ci id="S5.T3.4.m2.1.1.2.cmml" xref="S5.T3.4.m2.1.1.2">𝜏</ci><cn type="integer" id="S5.T3.4.m2.1.1.3.cmml" xref="S5.T3.4.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.m2.1d">\tau=0</annotation></semantics></math> indicates no cutoff time.</figcaption>
<table id="S5.T3.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.8.4" class="ltx_tr">
<td id="S5.T3.8.4.5" class="ltx_td ltx_border_tt"></td>
<th id="S5.T3.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">
<table id="S5.T3.5.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.5.1.1.1.2" class="ltx_tr">
<td id="S5.T3.5.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T3.5.1.1.1.2.1.1" class="ltx_text ltx_font_bold">GPU</span></td>
</tr>
<tr id="S5.T3.5.1.1.1.1" class="ltx_tr">
<td id="S5.T3.5.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">
<span id="S5.T3.5.1.1.1.1.1.1" class="ltx_text ltx_font_bold">(</span><math id="S5.T3.5.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S5.T3.5.1.1.1.1.1.m1.1a"><mi id="S5.T3.5.1.1.1.1.1.m1.1.1" xref="S5.T3.5.1.1.1.1.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S5.T3.5.1.1.1.1.1.m1.1b"><ci id="S5.T3.5.1.1.1.1.1.m1.1.1.cmml" xref="S5.T3.5.1.1.1.1.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.1.1.1.1.1.m1.1c">\tau</annotation></semantics></math><span id="S5.T3.5.1.1.1.1.1.2" class="ltx_text ltx_font_bold"> = 0)</span>
</td>
</tr>
</table>
</th>
<th id="S5.T3.6.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S5.T3.6.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.6.2.2.1.2" class="ltx_tr">
<td id="S5.T3.6.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T3.6.2.2.1.2.1.1" class="ltx_text ltx_font_bold">CPU</span></td>
</tr>
<tr id="S5.T3.6.2.2.1.1" class="ltx_tr">
<td id="S5.T3.6.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">
<span id="S5.T3.6.2.2.1.1.1.1" class="ltx_text ltx_font_bold">(</span><math id="S5.T3.6.2.2.1.1.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S5.T3.6.2.2.1.1.1.m1.1a"><mi id="S5.T3.6.2.2.1.1.1.m1.1.1" xref="S5.T3.6.2.2.1.1.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S5.T3.6.2.2.1.1.1.m1.1b"><ci id="S5.T3.6.2.2.1.1.1.m1.1.1.cmml" xref="S5.T3.6.2.2.1.1.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.2.2.1.1.1.m1.1c">\tau</annotation></semantics></math><span id="S5.T3.6.2.2.1.1.1.2" class="ltx_text ltx_font_bold"> = 0)</span>
</td>
</tr>
</table>
</th>
<th id="S5.T3.7.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">
<table id="S5.T3.7.3.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.7.3.3.1.2" class="ltx_tr">
<td id="S5.T3.7.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T3.7.3.3.1.2.1.1" class="ltx_text ltx_font_bold">CPU</span></td>
</tr>
<tr id="S5.T3.7.3.3.1.1" class="ltx_tr">
<td id="S5.T3.7.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">
<span id="S5.T3.7.3.3.1.1.1.1" class="ltx_text ltx_font_bold">(</span><math id="S5.T3.7.3.3.1.1.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S5.T3.7.3.3.1.1.1.m1.1a"><mi id="S5.T3.7.3.3.1.1.1.m1.1.1" xref="S5.T3.7.3.3.1.1.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S5.T3.7.3.3.1.1.1.m1.1b"><ci id="S5.T3.7.3.3.1.1.1.m1.1.1.cmml" xref="S5.T3.7.3.3.1.1.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.7.3.3.1.1.1.m1.1c">\tau</annotation></semantics></math><span id="S5.T3.7.3.3.1.1.1.2" class="ltx_text ltx_font_bold"> = 2.23)</span>
</td>
</tr>
</table>
</th>
<th id="S5.T3.8.4.4" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt">
<table id="S5.T3.8.4.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.8.4.4.1.2" class="ltx_tr">
<td id="S5.T3.8.4.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S5.T3.8.4.4.1.2.1.1" class="ltx_text ltx_font_bold">CPU</span></td>
</tr>
<tr id="S5.T3.8.4.4.1.1" class="ltx_tr">
<td id="S5.T3.8.4.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">
<span id="S5.T3.8.4.4.1.1.1.1" class="ltx_text ltx_font_bold">(</span><math id="S5.T3.8.4.4.1.1.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S5.T3.8.4.4.1.1.1.m1.1a"><mi id="S5.T3.8.4.4.1.1.1.m1.1.1" xref="S5.T3.8.4.4.1.1.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S5.T3.8.4.4.1.1.1.m1.1b"><ci id="S5.T3.8.4.4.1.1.1.m1.1.1.cmml" xref="S5.T3.8.4.4.1.1.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.8.4.4.1.1.1.m1.1c">\tau</annotation></semantics></math><span id="S5.T3.8.4.4.1.1.1.2" class="ltx_text ltx_font_bold"> = 1.99)</span>
</td>
</tr>
</table>
</th>
</tr>
<tr id="S5.T3.8.5.1" class="ltx_tr">
<td id="S5.T3.8.5.1.1" class="ltx_td ltx_align_center ltx_border_t">Accuracy</td>
<td id="S5.T3.8.5.1.2" class="ltx_td ltx_align_center ltx_border_t">0.67</td>
<td id="S5.T3.8.5.1.3" class="ltx_td ltx_align_center ltx_border_t">0.67</td>
<td id="S5.T3.8.5.1.4" class="ltx_td ltx_align_left ltx_border_t">0.66</td>
<td id="S5.T3.8.5.1.5" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">0.63</td>
</tr>
<tr id="S5.T3.8.6.2" class="ltx_tr">
<td id="S5.T3.8.6.2.1" class="ltx_td ltx_align_center ltx_border_bb">
<table id="S5.T3.8.6.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.8.6.2.1.1.1" class="ltx_tr">
<td id="S5.T3.8.6.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Training</td>
</tr>
<tr id="S5.T3.8.6.2.1.1.2" class="ltx_tr">
<td id="S5.T3.8.6.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">time (mins)</td>
</tr>
</table>
</td>
<td id="S5.T3.8.6.2.2" class="ltx_td ltx_align_center ltx_border_bb">80.32</td>
<td id="S5.T3.8.6.2.3" class="ltx_td ltx_align_center ltx_border_bb">
<table id="S5.T3.8.6.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.8.6.2.3.1.1" class="ltx_tr">
<td id="S5.T3.8.6.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">102</td>
</tr>
<tr id="S5.T3.8.6.2.3.1.2" class="ltx_tr">
<td id="S5.T3.8.6.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(1.27x)</td>
</tr>
</table>
</td>
<td id="S5.T3.8.6.2.4" class="ltx_td ltx_align_left ltx_border_bb">
<table id="S5.T3.8.6.2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.8.6.2.4.1.1" class="ltx_tr">
<td id="S5.T3.8.6.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">89.15</td>
</tr>
<tr id="S5.T3.8.6.2.4.1.2" class="ltx_tr">
<td id="S5.T3.8.6.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">(1.11x)</td>
</tr>
</table>
</td>
<td id="S5.T3.8.6.2.5" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb">
<table id="S5.T3.8.6.2.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.8.6.2.5.1.1" class="ltx_tr">
<td id="S5.T3.8.6.2.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">80.34</td>
</tr>
<tr id="S5.T3.8.6.2.5.1.2" class="ltx_tr">
<td id="S5.T3.8.6.2.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">(1.0x)</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p6" class="ltx_para ltx_noindent">
<p id="S5.p6.1" class="ltx_p"><span id="S5.p6.1.1" class="ltx_text ltx_font_bold">Computational Heterogeneity across Clients.</span> FL clients could have vastly different computational capabilities. While newer smartphones are now equipped with GPUs, other devices may have a much less powerful processor. How does this computational heterogeneity impact FL?</p>
</div>
<div id="S5.p7" class="ltx_para ltx_noindent">
<p id="S5.p7.2" class="ltx_p">For this experiment, we use Nvidia Jetson TX2 as the client device, which has one Pascal GPU and six CPU cores. We repeat the experiment shown in Table <a href="#S5.T2.st1" title="In Table 2 ‣ 5 Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a>, but instead of using the embedded GPU for training, we train the ResNet-18 model on a CPU. In Table <a href="#S5.T3" title="Table 3 ‣ 5 Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we show that CPU training with local epochs <math id="S5.p7.1.m1.1" class="ltx_Math" alttext="E=10" display="inline"><semantics id="S5.p7.1.m1.1a"><mrow id="S5.p7.1.m1.1.1" xref="S5.p7.1.m1.1.1.cmml"><mi id="S5.p7.1.m1.1.1.2" xref="S5.p7.1.m1.1.1.2.cmml">E</mi><mo id="S5.p7.1.m1.1.1.1" xref="S5.p7.1.m1.1.1.1.cmml">=</mo><mn id="S5.p7.1.m1.1.1.3" xref="S5.p7.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p7.1.m1.1b"><apply id="S5.p7.1.m1.1.1.cmml" xref="S5.p7.1.m1.1.1"><eq id="S5.p7.1.m1.1.1.1.cmml" xref="S5.p7.1.m1.1.1.1"></eq><ci id="S5.p7.1.m1.1.1.2.cmml" xref="S5.p7.1.m1.1.1.2">𝐸</ci><cn type="integer" id="S5.p7.1.m1.1.1.3.cmml" xref="S5.p7.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p7.1.m1.1c">E=10</annotation></semantics></math> would take <math id="S5.p7.2.m2.1" class="ltx_math_unparsed" alttext="1.27\times" display="inline"><semantics id="S5.p7.2.m2.1a"><mrow id="S5.p7.2.m2.1b"><mn id="S5.p7.2.m2.1.1">1.27</mn><mo lspace="0.222em" id="S5.p7.2.m2.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S5.p7.2.m2.1c">1.27\times</annotation></semantics></math> more time to obtain the same accuracy as the GPU training. This implies that even a single client device with low compute resources (e.g., a CPU) can become a bottleneck and significantly increase the FL training time.</p>
</div>
<div id="S5.p8" class="ltx_para ltx_noindent">
<p id="S5.p8.3" class="ltx_p">Once we obtain this quantification of computational heterogeneity, we can design better federated optimization algorithms. As an example, we implement a modified version of FedAvg where each client device is assigned a cutoff time (<math id="S5.p8.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S5.p8.1.m1.1a"><mi id="S5.p8.1.m1.1.1" xref="S5.p8.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S5.p8.1.m1.1b"><ci id="S5.p8.1.m1.1.1.cmml" xref="S5.p8.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p8.1.m1.1c">\tau</annotation></semantics></math>) after which it must send its model parameters to the server, irrespective of whether it has finished its local epochs or not. This strategy has parallels with the FedProx algorithm <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib15" title="" class="ltx_ref">2018</a>)</cite> which also accepts partial results from clients. However, the key advantage of using Flower is that we can compute and assign a processor-specific cutoff time for each client. For example, on average it takes 1.99 minutes to complete an FL round on the TX2 GPU. If we set the same time as a cutoff for CPU clients (<math id="S5.p8.2.m2.1" class="ltx_Math" alttext="\tau=1.99" display="inline"><semantics id="S5.p8.2.m2.1a"><mrow id="S5.p8.2.m2.1.1" xref="S5.p8.2.m2.1.1.cmml"><mi id="S5.p8.2.m2.1.1.2" xref="S5.p8.2.m2.1.1.2.cmml">τ</mi><mo id="S5.p8.2.m2.1.1.1" xref="S5.p8.2.m2.1.1.1.cmml">=</mo><mn id="S5.p8.2.m2.1.1.3" xref="S5.p8.2.m2.1.1.3.cmml">1.99</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p8.2.m2.1b"><apply id="S5.p8.2.m2.1.1.cmml" xref="S5.p8.2.m2.1.1"><eq id="S5.p8.2.m2.1.1.1.cmml" xref="S5.p8.2.m2.1.1.1"></eq><ci id="S5.p8.2.m2.1.1.2.cmml" xref="S5.p8.2.m2.1.1.2">𝜏</ci><cn type="float" id="S5.p8.2.m2.1.1.3.cmml" xref="S5.p8.2.m2.1.1.3">1.99</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p8.2.m2.1c">\tau=1.99</annotation></semantics></math> mins) as shown in Table <a href="#S5.T3" title="Table 3 ‣ 5 Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we obtain the same convergence time as GPU, at the expense of 3% accuracy drop. With <math id="S5.p8.3.m3.1" class="ltx_Math" alttext="\tau=2.23" display="inline"><semantics id="S5.p8.3.m3.1a"><mrow id="S5.p8.3.m3.1.1" xref="S5.p8.3.m3.1.1.cmml"><mi id="S5.p8.3.m3.1.1.2" xref="S5.p8.3.m3.1.1.2.cmml">τ</mi><mo id="S5.p8.3.m3.1.1.1" xref="S5.p8.3.m3.1.1.1.cmml">=</mo><mn id="S5.p8.3.m3.1.1.3" xref="S5.p8.3.m3.1.1.3.cmml">2.23</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p8.3.m3.1b"><apply id="S5.p8.3.m3.1.1.cmml" xref="S5.p8.3.m3.1.1"><eq id="S5.p8.3.m3.1.1.1.cmml" xref="S5.p8.3.m3.1.1.1"></eq><ci id="S5.p8.3.m3.1.1.2.cmml" xref="S5.p8.3.m3.1.1.2">𝜏</ci><cn type="float" id="S5.p8.3.m3.1.1.3.cmml" xref="S5.p8.3.m3.1.1.3">2.23</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p8.3.m3.1c">\tau=2.23</annotation></semantics></math>, a better balance between accuracy and training time could be obtained on a CPU.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">We presented our exploration of federated training of models on mobile and embedded devices by leveraging the language-, framework- and platform-agnostic capabilities of the Flower framework. We also presented early results quantifying the system costs of FL on the edge and its implications for the design of more efficient FL algorithms. Although our work is early stage, we hope it will trigger discussions at the workshop related to on-device training for FL and beyond.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadi et al. (2016)</span>
<span class="ltx_bibblock">
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
Ghemawat, S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R.,
Moore, S., Murray, D. G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P.,
Wicke, M., Yu, Y., and Zheng, X.

</span>
<span class="ltx_bibblock">Tensorflow: A system for large-scale machine learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">12th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 16)</em>, pp.  265–283, 2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beutel et al. (2020)</span>
<span class="ltx_bibblock">
Beutel, D. J., Topal, T., Mathur, A., Qiu, X., Parcollet, T., de Gusmão, P.
P. B., and Lane, N. D.

</span>
<span class="ltx_bibblock">Flower: A friendly federated learning research framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2007.14390, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2007.14390" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2007.14390</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al. (2019)</span>
<span class="ltx_bibblock">
Bonawitz, K., Eichner, H., Grieskamp, W., Huba, D., Ingerman, A., Ivanov, V.,
Kiddon, C. M., Konečný, J., Mazzocchi, S., McMahan, B., Overveldt, T. V.,
Petrou, D., Ramage, D., and Roselander, J.

</span>
<span class="ltx_bibblock">Towards federated learning at scale: System design.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">SysML 2019</em>, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al. (2020)</span>
<span class="ltx_bibblock">
Cai, H., Gan, C., Zhu, L., and Han, S.

</span>
<span class="ltx_bibblock">Tinytl: Reduce memory, not parameters for efficient on-device
learning.

</span>
<span class="ltx_bibblock">In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin,
H. (eds.), <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volume 33, pp.  11285–11297. Curran Associates, Inc., 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldas et al. (2018)</span>
<span class="ltx_bibblock">
Caldas, S., Wu, P., Li, T., Konecný, J., McMahan, H. B., Smith, V., and
Talwalkar, A.

</span>
<span class="ltx_bibblock">LEAF: A benchmark for federated settings.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1812.01097, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1812.01097" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1812.01097</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chahal et al. (2018)</span>
<span class="ltx_bibblock">
Chahal, K. S., Grover, M. S., and Dey, K.

</span>
<span class="ltx_bibblock">A hitchhiker’s guide on distributed training of deep neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1810.11787, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1810.11787" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1810.11787</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et al. (2019)</span>
<span class="ltx_bibblock">
Chowdhery, A., Warden, P., Shlens, J., Howard, A., and Rhodes, R.

</span>
<span class="ltx_bibblock">Visual wake words dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1906.05721, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1906.05721" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1906.05721</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">David et al. (2020)</span>
<span class="ltx_bibblock">
David, R., Duke, J., Jain, A., Reddi, V. J., Jeffries, N., Li, J., Kreeger, N.,
Nappier, I., Natraj, M., Regev, S., et al.

</span>
<span class="ltx_bibblock">Tensorflow lite micro: Embedded machine learning on tinyml systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.08678</em>, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fromm et al. (2018)</span>
<span class="ltx_bibblock">
Fromm, J., Patel, S., and Philipose, M.

</span>
<span class="ltx_bibblock">Heterogeneous bitwidth binarization in convolutional neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 32nd International Conference on Neural
Information Processing Systems</em>, NIPS’18, pp.  4010–4019, Red Hook, NY,
USA, 2018. Curran Associates Inc.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google (2020)</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Tensorflow federated: Machine learning on decentralized data.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.tensorflow.org/federated" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tensorflow.org/federated</a>, 2020.

</span>
<span class="ltx_bibblock">accessed 25-Mar-20.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2020)</span>
<span class="ltx_bibblock">
He, C., Li, S., So, J., Zhang, M., Wang, H., Wang, X., Vepakomma, P., Singh,
A., Qiu, H., Shen, L., Zhao, P., Kang, Y., Liu, Y., Raskar, R., Yang, Q.,
Annavaram, M., and Avestimehr, S.

</span>
<span class="ltx_bibblock">Fedml: A research library and benchmark for federated machine
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.13518</em>, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al. (2018)</span>
<span class="ltx_bibblock">
Jia, X., Song, S., He, W., Wang, Y., Rong, H., Zhou, F., Xie, L., Guo, Z.,
Yang, Y., Yu, L., Chen, T., Hu, G., Shi, S., and Chu, X.

</span>
<span class="ltx_bibblock">Highly scalable deep learning training system with mixed-precision:
Training imagenet in four minutes.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1807.11205, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1807.11205" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1807.11205</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2019)</span>
<span class="ltx_bibblock">
Lee, T., Lin, Z., Pushp, S., Li, C., Liu, Y., Lee, Y., Xu, F., Xu, C., Zhang,
L., and Song, J.

</span>
<span class="ltx_bibblock">Occlumency: Privacy-preserving remote deep-learning inference using
sgx.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">The 25th Annual International Conference on Mobile Computing
and Networking</em>, MobiCom ’19, New York, NY, USA, 2019. Association for
Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450361699.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3300061.3345447</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3300061.3345447" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3300061.3345447</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leontiadis et al. (2021)</span>
<span class="ltx_bibblock">
Leontiadis, I., Laskaridis, S., Venieris, S. I., and Lane, N. D.

</span>
<span class="ltx_bibblock">It’s always personal: Using early exits for efficient on-device cnn
personalisation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 22nd International Workshop on Mobile
Computing Systems and Applications</em>, HotMobile ’21, pp.  15–21, New York,
NY, USA, 2021. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450383233.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3446382.3448359</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3446382.3448359" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3446382.3448359</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2018)</span>
<span class="ltx_bibblock">
Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.06127</em>, 2018.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malekzadeh et al. (2019)</span>
<span class="ltx_bibblock">
Malekzadeh, M., Athanasakis, D., Haddadi, H., and Livshits, B.

</span>
<span class="ltx_bibblock">Privacy-preserving bandits, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2017)</span>
<span class="ltx_bibblock">
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In Singh, A. and Zhu, X. J. (eds.), <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th
International Conference on Artificial Intelligence and Statistics, AISTATS
2017, 20-22 April 2017, Fort Lauderdale, FL, USA</em>, volume 54 of
<em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pp.  1273–1282. PMLR,
2017.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://proceedings.mlr.press/v54/mcmahan17a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://proceedings.mlr.press/v54/mcmahan17a.html</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Office31 (2020)</span>
<span class="ltx_bibblock">
Office31.

</span>
<span class="ltx_bibblock">Office 31 dataset.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://people.eecs.berkeley.edu/~jhoffman/domainadapt/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://people.eecs.berkeley.edu/~jhoffman/domainadapt/</a>, 2020.

</span>
<span class="ltx_bibblock">accessed 10-Oct-20.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke et al. (2019)</span>
<span class="ltx_bibblock">
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
Bai, J., and Chintala, S.

</span>
<span class="ltx_bibblock">Pytorch: An imperative style, high-performance deep learning library.

</span>
<span class="ltx_bibblock">In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., and Garnett, R. (eds.), <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems 32</em>, pp.  8026–8037. Curran Associates,
Inc., 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ryffel et al. (2018)</span>
<span class="ltx_bibblock">
Ryffel, T., Trask, A., Dahl, M., Wagner, B., Mancuso, J., Rueckert, D., and
Passerat-Palmbach, J.

</span>
<span class="ltx_bibblock">A generic framework for privacy preserving deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1811.04017, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1811.04017" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1811.04017</a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2020)</span>
<span class="ltx_bibblock">
Sun, X., Wang, N., Chen, C.-Y., Ni, J., Agrawal, A., Cui, X., Venkataramani,
S., El Maghraoui, K., Srinivasan, V. V., and Gopalakrishnan, K.

</span>
<span class="ltx_bibblock">Ultra-low precision 4-bit training of deep neural networks.

</span>
<span class="ltx_bibblock">In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin,
H. (eds.), <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volume 33, pp.  1796–1807. Curran Associates, Inc., 2020.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Warden &amp; Situnayake (2019)</span>
<span class="ltx_bibblock">
Warden, P. and Situnayake, D.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Tinyml: Machine learning with tensorflow lite on arduino and
ultra-low-power microcontrollers</em>.

</span>
<span class="ltx_bibblock">” O’Reilly Media, Inc.”, 2019.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:creator" content="Anonymous Authors"></div>
<div class="ltx_rdf" about="" property="dcterms:subject"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="mlsys 2020"></div>
<div class="ltx_rdf" about="" property="dcterms:title" content="On-device federated learning with Flower"></div>

<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2104.03041" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2104.03042" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2104.03042">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2104.03042" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2104.03043" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  9 12:15:31 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
