<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding</title>
<!--Generated on Mon May 27 07:52:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2311.02851v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S1" title="In Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S2" title="In Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S2.SS1" title="In 2 Related Work ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Large Language Models on Machine Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S2.SS2" title="In 2 Related Work ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Accelerate Generation for Large Language Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3" title="In Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Preliminary Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.SS1" title="In 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.SS1.SSS0.Px1" title="In 3.1 Setup ‚Ä£ 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title">Commercial NMT Systems &amp; MT-oriented LLMs.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.SS1.SSS0.Px2" title="In 3.1 Setup ‚Ä£ 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title">Automated MT Metrics.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.SS2" title="In 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Analyses on WMT22 test sets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.SS2.SSS0.Px1" title="In 3.2 Analyses on WMT22 test sets ‚Ä£ 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title">Main Results.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.SS2.SSS0.Px2" title="In 3.2 Analyses on WMT22 test sets ‚Ä£ 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title">Off-target Rates.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.SS2.SSS0.Px3" title="In 3.2 Analyses on WMT22 test sets ‚Ä£ 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title">Unaligned Source/Target Words.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.SS3" title="In 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Analyses on Web Crawl test sets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.SS3.SSS0.Px1" title="In 3.3 Analyses on Web Crawl test sets ‚Ä£ 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title">Main Results.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.SS3.SSS0.Px2" title="In 3.3 Analyses on Web Crawl test sets ‚Ä£ 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title">Case Study.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.SS4" title="In 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S4" title="In Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Cooperative Decoding</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S4.SS0.SSS0.Px1" title="In 4 Cooperative Decoding ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title">Step1: Draft Generation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S4.SS0.SSS0.Px2" title="In 4 Cooperative Decoding ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title">Step2: Verification.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S4.SS0.SSS0.Px3" title="In 4 Cooperative Decoding ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title">Step3: Re-decoding.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S5" title="In Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S5.SS1" title="In 5 Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Main Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S5.SS1.SSS0.Px1" title="In 5.1 Main Results ‚Ä£ 5 Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title">Effect of different values of <math alttext="k" class="ltx_Math" display="inline"><semantics><mi>k</mi><annotation-xml encoding="MathML-Content"><ci>ùëò</ci></annotation-xml><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math>.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S5.SS1.SSS0.Px2" title="In 5.1 Main Results ‚Ä£ 5 Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title">CoDec vs. Hybrid Threshold.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S5.SS2" title="In 5 Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Human Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S5.SS3" title="In 5 Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Terminology Translation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S6" title="In Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S7" title="In Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#A1" title="In Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>WMT22 test sets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#A2" title="In Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Unaligned Source/Target Words.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#A3" title="In Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>WebCrawl test sets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#A4" title="In Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Terminology Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#A5" title="In Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Different from traditional NMT with additional language models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#A6" title="In Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>About speedup</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document" lang="en">
<h1 class="ltx_title ltx_title_document">Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiali Zeng, ¬†Fandong Meng, ¬†Yongjing Yin, ¬†Jie Zhou 
<br class="ltx_break"/>Pattern Recognition Center, WeChat AI, Tencent Inc 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">{lemonzeng,fandongmeng,yongjingyin,withtomzhou}@tencent.com</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1"><span class="ltx_text" id="id2.id1.1">Contemporary translation engines based on the encoder-decoder framework have made significant strides in development.
However, the emergence of Large Language Models (LLMs) has disrupted their position by presenting the potential for achieving superior translation quality.
To uncover <span class="ltx_text ltx_font_italic" id="id2.id1.1.1">the circumstances in which LLMs excel and explore how their strengths can be harnessed to enhance translation quality</span>,
we first conduct a comprehensive analysis to assess the strengths and limitations of various commercial NMT systems and MT-oriented LLMs.
Our findings indicate that neither NMT nor MT-oriented LLMs alone can effectively address all the translation issues, but MT-oriented LLMs show promise as a complementary solution to NMT systems.
Building upon these insights, we
propose <span class="ltx_text ltx_font_bold" id="id2.id1.1.2">Co</span>operative <span class="ltx_text ltx_font_bold" id="id2.id1.1.3">Dec</span>oding (CoDec), which treats NMT systems as a pretranslation model and MT-oriented LLMs as a supplemental solution to handle complex scenarios beyond the capability of NMT alone.
Experimental results on the WMT22 test sets and a newly collected test set WebCrawl demonstrate the effectiveness and efficiency of CoDec, highlighting its potential as a robust solution for combining NMT systems with MT-oriented LLMs in the field of machine translation.</span></p>
</div>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">{CJK}</span>
<p class="ltx_p" id="p1.2">UTF8gbsn</p>
</div>
<div class="ltx_para ltx_noindent" id="p2">
<div class="ltx_block ltx_align_bottom" id="p2.1">
<p class="ltx_p" id="p2.1.1"><span class="ltx_text ltx_font_bold" id="p2.1.1.1">Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p2.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p2.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p2.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p2.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p2.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p2.1.2.1.1.1.1.1.1">Jiali Zeng, ¬†Fandong Meng, ¬†Yongjing Yin, ¬†Jie Zhou</span></span></span>
<span class="ltx_tr" id="p2.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p2.1.2.1.1.2.2.1">Pattern Recognition Center, WeChat AI, Tencent Inc</span></span>
<span class="ltx_tr" id="p2.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p2.1.2.1.1.3.3.1"><span class="ltx_text ltx_font_typewriter" id="p2.1.2.1.1.3.3.1.1">{lemonzeng,fandongmeng,yongjingyin,withtomzhou}@tencent.com</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="494" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
<span class="ltx_text ltx_font_bold" id="S1.F1.2.1">Comparison between the Hybrid Threshold and CoDec frameworks</span>.
CoDec is more efficient than Hybrid Threshold as it eliminates the need for an extra quality evaluation module and autoregressive generation of the whole translation using MT-oriented LLM.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Over the years, the encoder-decoder framework has established Neural Machine Translation (NMT) models as the prevailing standard, achieving impressive translation quality through extensive training on large-scale and high-quality parallel data <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib38" title="">2017</a>; Freitag and Firat, <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib10" title="">2020</a>; Fan et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib8" title="">2021</a>)</cite>.
Commercial machine translation engines, e.g., Google Translate, are proficient in addressing the majority of translation requirements.
More recently, with the emergence of generative large language models (LLMs), the position of traditional NMT models has been challenged <cite class="ltx_cite ltx_citemacro_citep">(Brown et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib3" title="">2020</a>; OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib25" title="">2023</a>)</cite>.
While commercial LLMs like OpenAI‚Äôs GPT-4 currently perform well in translation <cite class="ltx_cite ltx_citemacro_citep">(Hendy et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib14" title="">2023</a>; Zhu et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib45" title="">2023</a>; Lin et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib22" title="">2022</a>; Agrawal et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib1" title="">2022</a>)</cite>, they are constrained by their interface nature, thereby limiting further customization and improvement due to privacy concerns in industrial applications.
A more promising approach involves fine-tuning relatively smaller LLMs (i.e., fewer than 13B parameters) to create LLMs specifically tailored for MT <cite class="ltx_cite ltx_citemacro_citep">(Zeng et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib42" title="">2023</a>; Zhang et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib44" title="">2023</a>; Jiao et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib17" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this context,
this study aims to investigate the following research questions:
<span class="ltx_text ltx_font_italic" id="S1.p2.1.1">In which scenarios do MT-oriented LLMs demonstrate superior performance to conventional NMT models, and how can we leverage the strengths of the two paradigms to enhance translation quality?</span></p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To begin, we conduct a comprehensive analysis
into the characteristics of translations generated by commercial NMT systems and MT-oriented LLMs.
Our findings reveal that commercial NMT systems excel at producing adequate translations in specific domains or languages.
Conversely, MT-oriented LLMs demonstrate proficiency in generating authentic-sounding translations and handling infrequent words that are not effectively processed by NMT systems.
In summary, MT-oriented LLMs can serve as valuable fallback systems in cases where the output of commercial NMT systems is unsatisfactory.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To complement NMT with MT-oriented LLMs,
<cite class="ltx_cite ltx_citemacro_citet">Hendy et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib14" title="">2023</a>)</cite> introduced the Hybrid Threshold approach (Figure <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">1</span></a>(a)), which employs the NMT system as the primary translation system.
When the translation fails to meet the quality threshold determined by the quality estimation (QE) module, an alternative translation is generated using a GPT-like model.
However, this approach faces two primary challenges.
First, existing reference-free metrics struggle to align with human judgment,
resulting in inaccuracies being propagated <cite class="ltx_cite ltx_citemacro_citep">(Freitag et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib12" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib11" title="">2020</a>; Ma et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib24" title="">2019</a>; Rei et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib29" title="">2022a</a>)</cite>. Second, the integration of neural quality estimation modules and the sequential execution by LLMs leads to increased decoding time <cite class="ltx_cite ltx_citemacro_citep">(Tay et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib37" title="">2023</a>; Xu et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib40" title="">2021</a>)</cite>, which poses concerns for efficient translation in practical applications.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To address the above issues,
we propose an efficient implementation approach for system ensembles called <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">Co</span>operative <span class="ltx_text ltx_font_bold" id="S1.p5.1.2">Dec</span>oding (CoDec).
As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">1</span></a>(b),
the NMT system functions as the front-end module, generating an initial translation draft for a given input sentence.
Subsequently, the MT-oriented LLM serves as both an evaluator and a refiner, which firstly evaluates the draft from a language modeling perspective, and then the LLM refines the partial translation starting from a specific position where the token in the draft is not among the top-<math alttext="k" class="ltx_Math" display="inline" id="S1.p5.1.m1.1"><semantics id="S1.p5.1.m1.1a"><mi id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><ci id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S1.p5.1.m1.1d">italic_k</annotation></semantics></math> token candidates suggested by the LLM.
Since the evaluation process takes advantage of parallel computation and the front-end module can handle most situations effectively, CoDec is more efficient compared to using LLMs for complete decoding.
</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The contributions of this paper are three-fold:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We conduct in-depth analyses on the WMT22 test sets and a newly collected test set, WebCrawl, to identify the strengths and weaknesses of traditional NMT systems and MT-oriented LLMs, finding that MT-oriented LLMs can complement NMT systems.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We present CoDec, a novel hybrid framework that synergizes the strengths of NMT systems and MT-oriented LLMs.
By harnessing the complementary capabilities of MT-oriented LLMs, CoDec effectively overcomes the limitations of traditional NMT systems<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We release code and the translations of different systems at https://github.com/lemon0830/CoDec</span></span></span>.
</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We evaluate the performance of CoDec on various test sets.
Our CoDec, without the need for an additional quality estimation module, achieves competitive or even better performance than Hybrid Threshold.
Furthermore, CoDec offers a significant acceleration advantage, achieving an acceleration ratio of approximately 2x compared to directly using LLM for generation.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Large Language Models on Machine Translation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Research on Large Language Models (LLMs) for machine translation can be broadly divided into two categories: utilizing LLMs as an interface and optimizing them for specific translation tasks.
For the former, <cite class="ltx_cite ltx_citemacro_citet">Hendy et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib14" title="">2023</a>)</cite> evaluate ChatGPT, GPT3.5, and text-adavinci-002 in eighteen translation directions, while <cite class="ltx_cite ltx_citemacro_citet">Zhu et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib45" title="">2023</a>)</cite> assess XGLM, BLOOMZ, OPT, and ChatGPT across 202 directions and 102 languages.
Other researchers explore strategies for selecting translation exemplars <cite class="ltx_cite ltx_citemacro_citep">(Lin et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib22" title="">2022</a>; Agrawal et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib1" title="">2022</a>)</cite> and incorporating external knowledge <cite class="ltx_cite ltx_citemacro_citep">(Lu et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib23" title="">2023</a>)</cite> to enhance GPT translation.
Fine-tuning smaller models (e.g., 7B) specifically for translation tasks has attracted increasing attention <cite class="ltx_cite ltx_citemacro_citep">(Zeng et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib42" title="">2023</a>; Zhang et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib44" title="">2023</a>; Jiao et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib17" title="">2023</a>)</cite>.
Diverging from existing approaches, our research focuses on examining the capabilities and limitations of commercial NMT systems and MT-oriented LLMs and developing efficient hybrid frameworks that leverage their respective strengths.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.8">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.8.9.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S2.T1.8.9.1.1" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.9.1.1.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.8.9.1.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.9.1.2.1">COMET</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.8.9.1.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.9.1.3.1">COMETk.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.8.9.1.4" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.9.1.4.1">COMET</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T1.8.9.1.5" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.9.1.5.1">COMETk.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.8.9.1.6" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.9.1.6.1">COMET</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.8.9.1.7" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.9.1.7.1">COMETk.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.8.9.1.8" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.9.1.8.1">COMET</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.8.9.1.9" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.9.1.9.1">COMETk.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.4.4">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.4.4.5" style="padding-left:5.7pt;padding-right:5.7pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S2.T1.1.1.1" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_italic" id="S2.T1.1.1.1.1">DE<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.m1.1a"><mo id="S2.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S2.T1.1.1.1.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.m1.1d">‚áí</annotation></semantics></math>EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S2.T1.2.2.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_italic" id="S2.T1.2.2.2.1">EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S2.T1.2.2.2.1.m1.1"><semantics id="S2.T1.2.2.2.1.m1.1a"><mo id="S2.T1.2.2.2.1.m1.1.1" stretchy="false" xref="S2.T1.2.2.2.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.1.m1.1b"><ci id="S2.T1.2.2.2.1.m1.1.1.cmml" xref="S2.T1.2.2.2.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.2.2.2.1.m1.1d">‚áí</annotation></semantics></math>DE</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S2.T1.3.3.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_italic" id="S2.T1.3.3.3.1">ZH<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S2.T1.3.3.3.1.m1.1"><semantics id="S2.T1.3.3.3.1.m1.1a"><mo id="S2.T1.3.3.3.1.m1.1.1" stretchy="false" xref="S2.T1.3.3.3.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.1.m1.1b"><ci id="S2.T1.3.3.3.1.m1.1.1.cmml" xref="S2.T1.3.3.3.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.3.3.3.1.m1.1d">‚áí</annotation></semantics></math>EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S2.T1.4.4.4" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_italic" id="S2.T1.4.4.4.1">EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S2.T1.4.4.4.1.m1.1"><semantics id="S2.T1.4.4.4.1.m1.1a"><mo id="S2.T1.4.4.4.1.m1.1.1" stretchy="false" xref="S2.T1.4.4.4.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.4.1.m1.1b"><ci id="S2.T1.4.4.4.1.m1.1.1.cmml" xref="S2.T1.4.4.4.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.4.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.4.4.4.1.m1.1d">‚áí</annotation></semantics></math>ZH</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.8.10.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.8.10.1.1" style="padding-left:5.7pt;padding-right:5.7pt;">WMT-Best</th>
<td class="ltx_td ltx_align_center" id="S2.T1.8.10.1.2" style="padding-left:5.7pt;padding-right:5.7pt;">85.0</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.10.1.3" style="padding-left:5.7pt;padding-right:5.7pt;">81.4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.10.1.4" style="padding-left:5.7pt;padding-right:5.7pt;">87.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.8.10.1.5" style="padding-left:5.7pt;padding-right:5.7pt;">83.6</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.10.1.6" style="padding-left:5.7pt;padding-right:5.7pt;">81.0</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.10.1.7" style="padding-left:5.7pt;padding-right:5.7pt;">77.7</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.10.1.8" style="padding-left:5.7pt;padding-right:5.7pt;">86.7</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.10.1.9" style="padding-left:5.7pt;padding-right:5.7pt;">82.0</td>
</tr>
<tr class="ltx_tr" id="S2.T1.8.11.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.8.11.2.1" style="padding-left:5.7pt;padding-right:5.7pt;">
<span class="ltx_ERROR undefined" id="S2.T1.8.11.2.1.1">\hdashline</span>[0.5pt/0.5pt]
GoogleMT</th>
<td class="ltx_td ltx_align_center" id="S2.T1.8.11.2.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.11.2.2.1">85.8</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.11.2.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.11.2.3.1">81.8</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.11.2.4" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.11.2.4.1">88.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.8.11.2.5" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.11.2.5.1">84.1</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.11.2.6" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.11.2.6.1">82.7</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.11.2.7" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.11.2.7.1">79.3</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.11.2.8" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.11.2.8.1">88.2</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.11.2.9" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.11.2.9.1">82.7</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.8.12.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.8.12.3.1" style="padding-left:5.7pt;padding-right:5.7pt;">MicroMT</th>
<td class="ltx_td ltx_align_center" id="S2.T1.8.12.3.2" style="padding-left:5.7pt;padding-right:5.7pt;">85.1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.12.3.3" style="padding-left:5.7pt;padding-right:5.7pt;">81.4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.12.3.4" style="padding-left:5.7pt;padding-right:5.7pt;">87.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.8.12.3.5" style="padding-left:5.7pt;padding-right:5.7pt;">83.7</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.12.3.6" style="padding-left:5.7pt;padding-right:5.7pt;">80.3</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.12.3.7" style="padding-left:5.7pt;padding-right:5.7pt;">77.5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.12.3.8" style="padding-left:5.7pt;padding-right:5.7pt;">86.0</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.12.3.9" style="padding-left:5.7pt;padding-right:5.7pt;">81.3</td>
</tr>
<tr class="ltx_tr" id="S2.T1.8.13.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.8.13.4.1" style="padding-left:5.7pt;padding-right:5.7pt;">
<span class="ltx_ERROR undefined" id="S2.T1.8.13.4.1.1">\hdashline</span>[0.5pt/0.5pt]
BayLing-7B</th>
<td class="ltx_td ltx_align_center" id="S2.T1.8.13.4.2" style="padding-left:5.7pt;padding-right:5.7pt;">83.2</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.13.4.3" style="padding-left:5.7pt;padding-right:5.7pt;">80.1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.13.4.4" style="padding-left:5.7pt;padding-right:5.7pt;">82.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.8.13.4.5" style="padding-left:5.7pt;padding-right:5.7pt;">79.2</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.13.4.6" style="padding-left:5.7pt;padding-right:5.7pt;">77.5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.13.4.7" style="padding-left:5.7pt;padding-right:5.7pt;">75.1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.13.4.8" style="padding-left:5.7pt;padding-right:5.7pt;">84.4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.13.4.9" style="padding-left:5.7pt;padding-right:5.7pt;">79.6</td>
</tr>
<tr class="ltx_tr" id="S2.T1.8.14.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.8.14.5.1" style="padding-left:5.7pt;padding-right:5.7pt;">TIM-13B</th>
<td class="ltx_td ltx_align_center" id="S2.T1.8.14.5.2" style="padding-left:5.7pt;padding-right:5.7pt;">84.4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.14.5.3" style="padding-left:5.7pt;padding-right:5.7pt;">81.0</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.14.5.4" style="padding-left:5.7pt;padding-right:5.7pt;">86.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.8.14.5.5" style="padding-left:5.7pt;padding-right:5.7pt;">83.1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.14.5.6" style="padding-left:5.7pt;padding-right:5.7pt;">80.8</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.14.5.7" style="padding-left:5.7pt;padding-right:5.7pt;">77.8</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.14.5.8" style="padding-left:5.7pt;padding-right:5.7pt;">87.6</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.14.5.9" style="padding-left:5.7pt;padding-right:5.7pt;">82.3</td>
</tr>
<tr class="ltx_tr" id="S2.T1.8.8">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.8.8.5" style="padding-left:5.7pt;padding-right:5.7pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S2.T1.5.5.1" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_italic" id="S2.T1.5.5.1.1">RU<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S2.T1.5.5.1.1.m1.1"><semantics id="S2.T1.5.5.1.1.m1.1a"><mo id="S2.T1.5.5.1.1.m1.1.1" stretchy="false" xref="S2.T1.5.5.1.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.1.1.m1.1b"><ci id="S2.T1.5.5.1.1.m1.1.1.cmml" xref="S2.T1.5.5.1.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.5.5.1.1.m1.1d">‚áí</annotation></semantics></math>EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S2.T1.6.6.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_italic" id="S2.T1.6.6.2.1">EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S2.T1.6.6.2.1.m1.1"><semantics id="S2.T1.6.6.2.1.m1.1a"><mo id="S2.T1.6.6.2.1.m1.1.1" stretchy="false" xref="S2.T1.6.6.2.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S2.T1.6.6.2.1.m1.1b"><ci id="S2.T1.6.6.2.1.m1.1.1.cmml" xref="S2.T1.6.6.2.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.6.2.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.6.6.2.1.m1.1d">‚áí</annotation></semantics></math>RU</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S2.T1.7.7.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_italic" id="S2.T1.7.7.3.1">JA<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S2.T1.7.7.3.1.m1.1"><semantics id="S2.T1.7.7.3.1.m1.1a"><mo id="S2.T1.7.7.3.1.m1.1.1" stretchy="false" xref="S2.T1.7.7.3.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S2.T1.7.7.3.1.m1.1b"><ci id="S2.T1.7.7.3.1.m1.1.1.cmml" xref="S2.T1.7.7.3.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.7.3.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.7.7.3.1.m1.1d">‚áí</annotation></semantics></math>EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S2.T1.8.8.4" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_italic" id="S2.T1.8.8.4.1">EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S2.T1.8.8.4.1.m1.1"><semantics id="S2.T1.8.8.4.1.m1.1a"><mo id="S2.T1.8.8.4.1.m1.1.1" stretchy="false" xref="S2.T1.8.8.4.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S2.T1.8.8.4.1.m1.1b"><ci id="S2.T1.8.8.4.1.m1.1.1.cmml" xref="S2.T1.8.8.4.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.8.8.4.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.T1.8.8.4.1.m1.1d">‚áí</annotation></semantics></math>JA</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.8.15.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.8.15.6.1" style="padding-left:5.7pt;padding-right:5.7pt;">WMT-Best</th>
<td class="ltx_td ltx_align_center" id="S2.T1.8.15.6.2" style="padding-left:5.7pt;padding-right:5.7pt;">86.0</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.15.6.3" style="padding-left:5.7pt;padding-right:5.7pt;">81.7</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.15.6.4" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.15.6.4.1">89.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.8.15.6.5" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.15.6.5.1">84.4</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.15.6.6" style="padding-left:5.7pt;padding-right:5.7pt;">81.6</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.15.6.7" style="padding-left:5.7pt;padding-right:5.7pt;">80.3</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.15.6.8" style="padding-left:5.7pt;padding-right:5.7pt;">89.3</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.15.6.9" style="padding-left:5.7pt;padding-right:5.7pt;">85.8</td>
</tr>
<tr class="ltx_tr" id="S2.T1.8.16.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.8.16.7.1" style="padding-left:5.7pt;padding-right:5.7pt;">
<span class="ltx_ERROR undefined" id="S2.T1.8.16.7.1.1">\hdashline</span>[0.5pt/0.5pt]
GoogleMT</th>
<td class="ltx_td ltx_align_center" id="S2.T1.8.16.7.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.16.7.2.1">86.6</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.16.7.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.16.7.3.1">82.0</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.16.7.4" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.16.7.4.1">89.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.8.16.7.5" style="padding-left:5.7pt;padding-right:5.7pt;">84.2</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.16.7.6" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.16.7.6.1">84.0</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.16.7.7" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.16.7.7.1">81.7</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.16.7.8" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.16.7.8.1">90.2</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.16.7.9" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.8.16.7.9.1">86.5</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.8.17.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.8.17.8.1" style="padding-left:5.7pt;padding-right:5.7pt;">MicroMT</th>
<td class="ltx_td ltx_align_center" id="S2.T1.8.17.8.2" style="padding-left:5.7pt;padding-right:5.7pt;">85.5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.17.8.3" style="padding-left:5.7pt;padding-right:5.7pt;">81.1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.17.8.4" style="padding-left:5.7pt;padding-right:5.7pt;">88.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.8.17.8.5" style="padding-left:5.7pt;padding-right:5.7pt;">83.6</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.17.8.6" style="padding-left:5.7pt;padding-right:5.7pt;">81.5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.17.8.7" style="padding-left:5.7pt;padding-right:5.7pt;">80.1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.17.8.8" style="padding-left:5.7pt;padding-right:5.7pt;">88.0</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.17.8.9" style="padding-left:5.7pt;padding-right:5.7pt;">85.3</td>
</tr>
<tr class="ltx_tr" id="S2.T1.8.18.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.8.18.9.1" style="padding-left:5.7pt;padding-right:5.7pt;">
<span class="ltx_ERROR undefined" id="S2.T1.8.18.9.1.1">\hdashline</span>[0.5pt/0.5pt]
BayLing-7B</th>
<td class="ltx_td ltx_align_center" id="S2.T1.8.18.9.2" style="padding-left:5.7pt;padding-right:5.7pt;">82.5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.18.9.3" style="padding-left:5.7pt;padding-right:5.7pt;">79.3</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.18.9.4" style="padding-left:5.7pt;padding-right:5.7pt;">74.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.8.18.9.5" style="padding-left:5.7pt;padding-right:5.7pt;">70.6</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.18.9.6" style="padding-left:5.7pt;padding-right:5.7pt;">72.2</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.18.9.7" style="padding-left:5.7pt;padding-right:5.7pt;">72.5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.18.9.8" style="padding-left:5.7pt;padding-right:5.7pt;">71.2</td>
<td class="ltx_td ltx_align_center" id="S2.T1.8.18.9.9" style="padding-left:5.7pt;padding-right:5.7pt;">73.5</td>
</tr>
<tr class="ltx_tr" id="S2.T1.8.19.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S2.T1.8.19.10.1" style="padding-left:5.7pt;padding-right:5.7pt;">TIM-13B</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.8.19.10.2" style="padding-left:5.7pt;padding-right:5.7pt;">84.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.8.19.10.3" style="padding-left:5.7pt;padding-right:5.7pt;">80.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.8.19.10.4" style="padding-left:5.7pt;padding-right:5.7pt;">86.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S2.T1.8.19.10.5" style="padding-left:5.7pt;padding-right:5.7pt;">82.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.8.19.10.6" style="padding-left:5.7pt;padding-right:5.7pt;">80.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.8.19.10.7" style="padding-left:5.7pt;padding-right:5.7pt;">79.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.8.19.10.8" style="padding-left:5.7pt;padding-right:5.7pt;">87.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.8.19.10.9" style="padding-left:5.7pt;padding-right:5.7pt;">84.5</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>
<span class="ltx_text ltx_font_bold" id="S2.T1.10.1">Experimental results on the WMT22 test sets</span>.
MT-oriented LLMs have the potential to achieve comparable performance to
commercial NMT systems, eliminating the need for rule-based engineering techniques.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Accelerate Generation for Large Language Models</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Efforts to improve the inference efficiency of LLMs have been ongoing for several years <cite class="ltx_cite ltx_citemacro_citep">(Tay et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib37" title="">2023</a>; Xu et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib40" title="">2021</a>)</cite>, leveraging techniques such as knowledge distillation <cite class="ltx_cite ltx_citemacro_citep">(Hinton et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib16" title="">2015</a>; Jiao et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib18" title="">2020</a>; Wang et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib39" title="">2020</a>)</cite>, quantization <cite class="ltx_cite ltx_citemacro_citep">(Shen et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib34" title="">2020</a>; Sun et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib36" title="">2020</a>)</cite>, pruning<cite class="ltx_cite ltx_citemacro_citep">(Fan et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib9" title="">2020</a>)</cite>, and others <cite class="ltx_cite ltx_citemacro_citep">(Kim and Cho, <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib19" title="">2021</a>; Lei, <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib20" title="">2021</a>)</cite>.
The most related work is to leverage speculative execution <cite class="ltx_cite ltx_citemacro_citep">(Burton, <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib4" title="">1985</a>; Hennessy and Patterson, <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib15" title="">2012</a>)</cite> for the speedup of autoregressive models.
<cite class="ltx_cite ltx_citemacro_citet">Stern et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib35" title="">2018</a>)</cite> propose to decode several tokens in parallel to accelerate greedy decoding.
For LLMs, speculative decoding <cite class="ltx_cite ltx_citemacro_citep">(Chen et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib5" title="">2023a</a>; Leviathan et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib21" title="">2023</a>)</cite> uses an additional draft model and generates sequences with sampling.
<cite class="ltx_cite ltx_citemacro_citet">Yang et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib41" title="">2023</a>)</cite> copy some tokens from retrieved reference text to the decoder, which are validated with output probabilities.
<cite class="ltx_cite ltx_citemacro_citet">Santilli et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib32" title="">2023</a>)</cite> reframe MT‚Äôs standard greedy autoregressive decoding procedure with a parallel formulation.
We are pioneers in using speculative execution as a fusion approach for commercial NMT systems and MT-oriented LLMs, without requiring an auxiliary quality estimation module or modifications to the target LLMs‚Äô parameters.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Preliminary Experiments</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we conduct a series of analyses to quantitatively investigate
<span class="ltx_text ltx_font_italic" id="S3.p1.1.1">the characteristics of translations from different systems</span>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Setup</h3>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Commercial NMT Systems &amp; MT-oriented LLMs.</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">Our focus is the use of MT-oriented LLMs in industrial settings, and the chosen commercial NMT systems consist of Google Translate (<span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px1.p1.1.1">GoogleMT</span> for brevity)<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://translate.google.com/</span></span></span> and Microsoft Translate (<span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px1.p1.1.2">MicroMT</span> for brevity)<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://www.bing.com/translator</span></span></span> due to their strong performance and high reproducibility.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p2.7">Regarding MT-oriented LLMs, we utilize <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px1.p2.7.1">BayLing-7B</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhang et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib44" title="">2023</a>)</cite>.
We directly use the translations released on GitHub<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://github.com/ictnlp/BayLing/tree/main/exp/translation _benchmark/bayling-7b</span></span></span>.
Additionally, we develop an in-house MT-oriented LLM, trained on human-written validation data from previous WMT competitions<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://www.statmt.org/wmt22/translation-task.html</span></span></span>, such as the newstest2017-2021 of German<math alttext="\Leftrightarrow" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p2.1.m1.1"><semantics id="S3.SS1.SSS0.Px1.p2.1.m1.1a"><mo id="S3.SS1.SSS0.Px1.p2.1.m1.1.1" stretchy="false" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.cmml">‚áî</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.1.m1.1b"><ci id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1">‚áî</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.1.m1.1c">\Leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p2.1.m1.1d">‚áî</annotation></semantics></math>English, Chinese<math alttext="\Leftrightarrow" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p2.2.m2.1"><semantics id="S3.SS1.SSS0.Px1.p2.2.m2.1a"><mo id="S3.SS1.SSS0.Px1.p2.2.m2.1.1" stretchy="false" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.cmml">‚áî</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.2.m2.1b"><ci id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1">‚áî</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.2.m2.1c">\Leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p2.2.m2.1d">‚áî</annotation></semantics></math>English, Russian<math alttext="\Leftrightarrow" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p2.3.m3.1"><semantics id="S3.SS1.SSS0.Px1.p2.3.m3.1a"><mo id="S3.SS1.SSS0.Px1.p2.3.m3.1.1" stretchy="false" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.cmml">‚áî</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.3.m3.1b"><ci id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1">‚áî</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.3.m3.1c">\Leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p2.3.m3.1d">‚áî</annotation></semantics></math>English, and Jappanese<math alttext="\Leftrightarrow" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p2.4.m4.1"><semantics id="S3.SS1.SSS0.Px1.p2.4.m4.1a"><mo id="S3.SS1.SSS0.Px1.p2.4.m4.1.1" stretchy="false" xref="S3.SS1.SSS0.Px1.p2.4.m4.1.1.cmml">‚áî</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.4.m4.1b"><ci id="S3.SS1.SSS0.Px1.p2.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.1.1">‚áî</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.4.m4.1c">\Leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p2.4.m4.1d">‚áî</annotation></semantics></math>English.
In addition, we have incorporated high-quality bilingual sentence pairs in Chinese<math alttext="\Leftrightarrow" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p2.5.m5.1"><semantics id="S3.SS1.SSS0.Px1.p2.5.m5.1a"><mo id="S3.SS1.SSS0.Px1.p2.5.m5.1.1" stretchy="false" xref="S3.SS1.SSS0.Px1.p2.5.m5.1.1.cmml">‚áî</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.5.m5.1b"><ci id="S3.SS1.SSS0.Px1.p2.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.5.m5.1.1">‚áî</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.5.m5.1c">\Leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p2.5.m5.1d">‚áî</annotation></semantics></math>English, German<math alttext="\Leftrightarrow" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p2.6.m6.1"><semantics id="S3.SS1.SSS0.Px1.p2.6.m6.1a"><mo id="S3.SS1.SSS0.Px1.p2.6.m6.1.1" stretchy="false" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1.cmml">‚áî</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.6.m6.1b"><ci id="S3.SS1.SSS0.Px1.p2.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1">‚áî</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.6.m6.1c">\Leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p2.6.m6.1d">‚áî</annotation></semantics></math>English, and Russian<math alttext="\Leftrightarrow" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p2.7.m7.1"><semantics id="S3.SS1.SSS0.Px1.p2.7.m7.1a"><mo id="S3.SS1.SSS0.Px1.p2.7.m7.1.1" stretchy="false" xref="S3.SS1.SSS0.Px1.p2.7.m7.1.1.cmml">‚áî</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.7.m7.1b"><ci id="S3.SS1.SSS0.Px1.p2.7.m7.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.7.m7.1.1">‚áî</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.7.m7.1c">\Leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p2.7.m7.1d">‚áî</annotation></semantics></math>English, resulting in a total of two million sentences in our training data.
According to the data license of WMT22, the data released for the General MT task can be freely used for research purposes.
We fine-tune the tigerbot-13b-base<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://huggingface.co/TigerResearch/tigerbot-13b-base</span></span></span> with TIM <cite class="ltx_cite ltx_citemacro_citep">(Zeng et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib42" title="">2023</a>)</cite> as the final MT-oriented LLM<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>https://huggingface.co/Lemoooon/TIM_13B_forCoDec</span></span></span>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Automated MT Metrics.</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">We follow previous studies <cite class="ltx_cite ltx_citemacro_cite">Hendy et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib14" title="">2023</a>); Zhu et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib45" title="">2023</a>); Zeng et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib42" title="">2023</a>); Zhang et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib44" title="">2023</a>)</cite> to utilize COMET-22 (wmt22-COMET-da) <cite class="ltx_cite ltx_citemacro_citep">(Rei et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib31" title="">2021</a>)</cite>, and COMETkiwi (wmt22-COMET-kiwi-da) <cite class="ltx_cite ltx_citemacro_citep">(Rei et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib30" title="">2022b</a>)</cite> for reference-free quality estimation.
We also report ChrF <cite class="ltx_cite ltx_citemacro_citep">(Popovic, <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib27" title="">2015</a>)</cite> and SacreBLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib26" title="">2002</a>)</cite> in Table <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#A3.T7" title="Table 7 ‚Ä£ Appendix C WebCrawl test sets ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">7</span></a> in Appendix.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Analyses on WMT22 test sets</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To prevent data leakage <cite class="ltx_cite ltx_citemacro_citep">(Garcia et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib13" title="">2023</a>)</cite>, we analyze the WMT22 test sets.
Detailed statistics are reported in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#A1" title="Appendix A WMT22 test sets ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="218" id="S3.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
<span class="ltx_text ltx_font_bold" id="S3.F2.2.1">Off-target rates (%) of translations.</span> MT-oriented LLMs (i.e., BayLing and TIM) exhibit a higher prevalence of off-target translations than NMT systems.
</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="188" id="S3.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
<span class="ltx_text ltx_font_bold" id="S3.F3.2.1">Comparison of unaligned source words, unaligned target words, and the length of translations.</span>
MT-oriented LLMs consistently generate translations that are noticeably shorter in length and have a higher occurrence of unaligned source words across the test sets when compared to NMT models.

</figcaption>
</figure>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Main Results.</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">The experimental results are illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S2.T1" title="Table 1 ‚Ä£ 2.1 Large Language Models on Machine Translation ‚Ä£ 2 Related Work ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">1</span></a>.
We have made the following observations:
1) <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px1.p1.1.1">GoogleMT</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px1.p1.1.2">MicroMT</span> showcase excellent performance.
They consistently outperform the <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px1.p1.1.3">WMT winner</span> in most of the language pairs, highlighting the robust capabilities of these well-established translation engines.
2) Despite the existing performance gap,
MT-oriented LLMs still have untapped potential for further improvement.
Notably, <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px1.p1.1.4">TIM</span> outperforms <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px1.p1.1.5">BayLing</span> by a significant margin across all language pairs.
Moreover, <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px1.p1.1.6">TIM</span> exhibits slightly inferior performance compared to <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px1.p1.1.7">MicroMT</span> on most test sets.
This suggests that employing more effective fine-tuning methods with large amounts of high-quality parallel data can enhance
the translation capabilities of MT-oriented LLMs, making them close to commercial NMT systems.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Off-target Rates.</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1">Off-target indicates translations generated by machines involve segments of wrong languages or code-mixing, presenting a significant challenge in multilingual neural machine translation <cite class="ltx_cite ltx_citemacro_citep">(Chen et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib6" title="">2023b</a>; Zhang et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib43" title="">2020</a>)</cite>.
Here, we use langdetect<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>https://github.com/Mimino666/langdetect</span></span></span> to identify the language of each translation.
The off-target rate of a translation is the subtraction of the probability of the target language prediction from 1.
For a test set, we compute the average off-target rate across all the sentences.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p2.4">As depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.F2" title="Figure 2 ‚Ä£ 3.2 Analyses on WMT22 test sets ‚Ä£ 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>Due to limited space, we only present the results for English-to-Many translations here. The results for Many-to-English can be found in Figure <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#A3.F7" title="Figure 7 ‚Ä£ Appendix C WebCrawl test sets ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">7</span></a> in Appendix.</span></span></span>,
the MT-oriented LLMs tend to produce translations with higher off-target rates compared to NMT systems.
Specifically, <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px2.p2.4.1">BayLing</span> exhibits off-target rates of 21% and 53.83% for EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.1.m1.1"><semantics id="S3.SS2.SSS0.Px2.p2.1.m1.1a"><mo id="S3.SS2.SSS0.Px2.p2.1.m1.1.1" stretchy="false" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.1.m1.1b"><ci id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p2.1.m1.1d">‚áí</annotation></semantics></math>RU and EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.2.m2.1"><semantics id="S3.SS2.SSS0.Px2.p2.2.m2.1a"><mo id="S3.SS2.SSS0.Px2.p2.2.m2.1.1" stretchy="false" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.2.m2.1b"><ci id="S3.SS2.SSS0.Px2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.2.m2.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.2.m2.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p2.2.m2.1d">‚áí</annotation></semantics></math>JA translations, respectively, which falls outside the language scope covered by the training data.
This highlights a more pronounced off-target issue in LLMs, especially in zero-shot scenarios.
In contrast, <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px2.p2.4.2">TIM</span> achieves notably lower off-target rates in EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.3.m3.1"><semantics id="S3.SS2.SSS0.Px2.p2.3.m3.1a"><mo id="S3.SS2.SSS0.Px2.p2.3.m3.1.1" stretchy="false" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.3.m3.1b"><ci id="S3.SS2.SSS0.Px2.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.3.m3.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.3.m3.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p2.3.m3.1d">‚áí</annotation></semantics></math>RU and EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.4.m4.1"><semantics id="S3.SS2.SSS0.Px2.p2.4.m4.1a"><mo id="S3.SS2.SSS0.Px2.p2.4.m4.1.1" stretchy="false" xref="S3.SS2.SSS0.Px2.p2.4.m4.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.4.m4.1b"><ci id="S3.SS2.SSS0.Px2.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.4.m4.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.4.m4.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p2.4.m4.1d">‚áí</annotation></semantics></math>JA compared to <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px2.p2.4.3">BayLing</span>.
We speculate that this can be attributed to TIM‚Äôs incorporation of corresponding training data, which enhances its ability to handle language switching and produce more accurate translations.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="230" id="S3.F4.g1" src="x4.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
<span class="ltx_text ltx_font_bold" id="S3.F4.4.1">POS tags of unaligned source words.</span>
We show the top-6 POS tags of USW on the WMT22 EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.F4.2.m1.1"><semantics id="S3.F4.2.m1.1b"><mo id="S3.F4.2.m1.1.1" stretchy="false" xref="S3.F4.2.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S3.F4.2.m1.1c"><ci id="S3.F4.2.m1.1.1.cmml" xref="S3.F4.2.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.2.m1.1d">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.F4.2.m1.1e">‚áí</annotation></semantics></math>ZH test set, and the incremental USW of MT-oriented LLMs mainly lies in nouns (NN) and adjectives (JJ).

</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="231" id="S3.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
<span class="ltx_text ltx_font_bold" id="S3.F5.2.1">Examples of free translation generated by MT-oriented LLM.</span>
MT-oriented LLMs often produce shorter translations with significant paraphrasing, maintaining the original meaning while using different words and sentence structures.

</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Unaligned Source/Target Words.</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p1.1">To assess the literalness of the translation, we follow <cite class="ltx_cite ltx_citemacro_citet">Raunak et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib28" title="">2023</a>); Hendy et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib14" title="">2023</a>)</cite> to calculate the number of source and target words that do not align on a word-to-word basis.
More details can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#A2" title="Appendix B Unaligned Source/Target Words. ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">B</span></a>.
The left portion of Figure <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.F3" title="Figure 3 ‚Ä£ 3.2 Analyses on WMT22 test sets ‚Ä£ 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates that the MT-oriented LLMs incur a notably larger number of unaligned source words across the test sets than the NMT counterpart.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p2.1">We examine the top six part-of-speech (POS) tags by NLTK toolkit <cite class="ltx_cite ltx_citemacro_cite">Bird et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib2" title="">2009</a>)</cite> of the unaligned source words (in Figure <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.F4" title="Figure 4 ‚Ä£ Off-target Rates. ‚Ä£ 3.2 Analyses on WMT22 test sets ‚Ä£ 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">4</span></a>).
The difference mainly lies in nouns (NN) and adjectives (JJ),
indicating the possibility of increased paraphrasing or a higher degree of inadequacy, such as omitted or inserted content.
However, back to the middle part of Figure <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.F3" title="Figure 3 ‚Ä£ 3.2 Analyses on WMT22 test sets ‚Ä£ 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">3</span></a>, the number of unaligned target words of the MT-oriented LLMs does not significantly differ from those of NMT systems,
suggesting that the adequacy of translations produced by LLMs is comparable to NMT.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p3">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p3.1">Additionally, we calculate the average word count in the generated translations. As depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.F3" title="Figure 3 ‚Ä£ 3.2 Analyses on WMT22 test sets ‚Ä£ 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">3</span></a>, MT-oriented LLMs tend to produce shorter sentences, utilizing concise and precise language.
Humans often use concise language, especially in conversations, which is abundant in the training corpus of LLMs. This influence may result in LLMs generating shorter translations.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p4">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p4.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.F5" title="Figure 5 ‚Ä£ Off-target Rates. ‚Ä£ 3.2 Analyses on WMT22 test sets ‚Ä£ 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">5</span></a> presents several examples that highlight translation differences. For instance, the phrase ‚Äúfrustration shouting‚Äù should be translated as ‚ÄúÂ§ßÂñäÂ§ßÂè´ (scream in frustration)‚Äù. While <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px3.p4.1.1">MicroMT</span> aims for fidelity by using translation augmentation segments like ‚ÄúÊàëÈùûÂ∏∏Ê≤Æ‰∏ß (I feel extremely frustrated)‚Äù, <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px3.p4.1.2">TIM</span> demonstrates a better understanding of the entire sentence and provides more accurate translations.
However, in the third example, <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px3.p4.1.3">TIM</span> overlooks the inclusion of the expression ‚Äúthe receptive, super-alert brain of a tiny child‚Äù from the source text, resulting in a certain degree of translation oversight.
In summary, MT-oriented LLMs tend to generate shorter translations with substantial paraphrasing, where the original text is rephrased using different words and sentence structures while preserving the same meaning.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Analyses on Web Crawl test sets</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The WMT22 test set is meticulously screened and annotated, with source sentences free of errors and from common domains. While the sentences have strong syntactic structures and grammatical correctness, real-world translation scenarios may not always have these ideal conditions.
To reflect practical challenges, we collected a challenging test set from the open domain through web crawling.
Here, we focus on Chinese<math alttext="\Leftrightarrow" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mo id="S3.SS3.p1.1.m1.1.1" stretchy="false" xref="S3.SS3.p1.1.m1.1.1.cmml">‚áî</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">‚áî</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\Leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">‚áî</annotation></semantics></math>English directions.
To acquire the data, we follow the process outlined in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#A3" title="Appendix C WebCrawl test sets ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T2.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.3.4.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="S3.T2.3.4.1.1" style="padding-left:1.7pt;padding-right:1.7pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.3.4.1.1.1">System</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="S3.T2.3.4.1.2" style="padding-left:1.7pt;padding-right:1.7pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.3.4.1.2.1">Translation</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.5.2">
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" colspan="2" id="S3.T2.3.5.2.1" style="padding-left:1.7pt;padding-right:1.7pt;"><span class="ltx_text ltx_font_italic" id="S3.T2.3.5.2.1.1">Terminology/abbreviations</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.6.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S3.T2.3.6.3.1" style="padding-left:1.7pt;padding-right:1.7pt;">Source</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S3.T2.3.6.3.2" style="padding-left:1.7pt;padding-right:1.7pt;">ÂàöÂ•ΩÂáÜÂ§áÂéªÂé¶Èó®ÊóÖÊ∏∏ÔºåËøòËÉΩÈ°∫‰æøË∑ë‰∏™Âé¶È©¨,</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.7.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S3.T2.3.7.4.1" style="padding-left:1.7pt;padding-right:1.7pt;">GoogleMT</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S3.T2.3.7.4.2" style="padding-left:1.7pt;padding-right:1.7pt;">I was just about to go to Xiamen for a trip, and I could also run the Xiamen Horse Racing.</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.8.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S3.T2.3.8.5.1" style="padding-left:1.7pt;padding-right:1.7pt;">TIM</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S3.T2.3.8.5.2" style="padding-left:1.7pt;padding-right:1.7pt;">Just ready to go to Xiamen for a trip, and can also run the Xiamen Marathon,</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.9.6">
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" colspan="2" id="S3.T2.3.9.6.1" style="padding-left:1.7pt;padding-right:1.7pt;"><span class="ltx_text ltx_font_italic" id="S3.T2.3.9.6.1.1">Ill-informed text</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.10.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S3.T2.3.10.7.1" style="padding-left:1.7pt;padding-right:1.7pt;">Source</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S3.T2.3.10.7.2" style="padding-left:1.7pt;padding-right:1.7pt;">Use a no. 6 fi lbert to create the illusion of the rungs and ¬†¬†the back of the chair on the left.</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.11.8">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S3.T2.3.11.8.1" style="padding-left:1.7pt;padding-right:1.7pt;">GoogleMT</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S3.T2.3.11.8.2" style="padding-left:1.7pt;padding-right:1.7pt;">‰ΩøÁî®Âê¶„ÄÇ 6 Ê¶õÂ≠êÂàõÂª∫Â∑¶ËæπÁöÑÊ¢ØÁ∫ßÂíåÊ§ÖËÉåÁöÑÂπªËßâ„ÄÇ</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.12.9">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S3.T2.3.12.9.1" style="padding-left:1.7pt;padding-right:1.7pt;">TIM</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S3.T2.3.12.9.2" style="padding-left:1.7pt;padding-right:1.7pt;">Áî® 6 Âè∑ÁîªÁ¨îÂú®Â∑¶ËæπÁöÑÊ§ÖÂ≠ê‰∏äÂàõÈÄ†Ê¢ØÁ∫ßÂíåÊ§ÖËÉåÁöÑÈîôËßâ„ÄÇ</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.13.10">
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" colspan="2" id="S3.T2.3.13.10.1" style="padding-left:1.7pt;padding-right:1.7pt;"><span class="ltx_text ltx_font_italic" id="S3.T2.3.13.10.1.1">ComplexÔºåRepetition-containing</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S3.T2.1.1.2" style="padding-left:1.7pt;padding-right:1.7pt;">Source</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S3.T2.1.1.1" style="padding-left:1.7pt;padding-right:1.7pt;">ÊãØÊïëÂâßËçí<math alttext="ÔΩú" class="ltx_Math" display="inline" id="S3.T2.1.1.1.m1.1"><semantics id="S3.T2.1.1.1.m1.1a"><mi id="S3.T2.1.1.1.m1.1.1" mathvariant="normal" xref="S3.T2.1.1.1.m1.1.1.cmml">ÔΩú</mi><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.m1.1.1">ÔΩú</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.m1.1c">ÔΩú</annotation><annotation encoding="application/x-llamapun" id="S3.T2.1.1.1.m1.1d">ÔΩú</annotation></semantics></math>„ÄäÁà±‰πãÂÖ®ËöÄ„ÄãÂïäÂïäÂïäÂïäÂïäÂïäÂïä‰∏§‰∏™‰∫≤‰∫≤ÊÄ™ÔºÅÔºÅ</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S3.T2.2.2.2" style="padding-left:1.7pt;padding-right:1.7pt;">GoogleMT</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S3.T2.2.2.1" style="padding-left:1.7pt;padding-right:1.7pt;">Save the drama<math alttext="ÔΩú" class="ltx_Math" display="inline" id="S3.T2.2.2.1.m1.1"><semantics id="S3.T2.2.2.1.m1.1a"><mi id="S3.T2.2.2.1.m1.1.1" mathvariant="normal" xref="S3.T2.2.2.1.m1.1.1.cmml">ÔΩú</mi><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.1.m1.1b"><ci id="S3.T2.2.2.1.m1.1.1.cmml" xref="S3.T2.2.2.1.m1.1.1">ÔΩú</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.1.m1.1c">ÔΩú</annotation><annotation encoding="application/x-llamapun" id="S3.T2.2.2.1.m1.1d">ÔΩú</annotation></semantics></math>‚ÄúTotal Eclipse of Love‚Äù Ahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.14.11">
<td class="ltx_td ltx_nopad_l ltx_nopad_r" id="S3.T2.3.14.11.1" style="padding-left:1.7pt;padding-right:1.7pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S3.T2.3.14.11.2" style="padding-left:1.7pt;padding-right:1.7pt;">hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh being to To to to toto to Eclips e Love! !</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S3.T2.3.3.2" style="padding-left:1.7pt;padding-right:1.7pt;">TIM</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S3.T2.3.3.1" style="padding-left:1.7pt;padding-right:1.7pt;">Save the drama drought <math alttext="ÔΩú" class="ltx_Math" display="inline" id="S3.T2.3.3.1.m1.1"><semantics id="S3.T2.3.3.1.m1.1a"><mi id="S3.T2.3.3.1.m1.1.1" mathvariant="normal" xref="S3.T2.3.3.1.m1.1.1.cmml">ÔΩú</mi><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.1.m1.1b"><ci id="S3.T2.3.3.1.m1.1.1.cmml" xref="S3.T2.3.3.1.m1.1.1">ÔΩú</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.1.m1.1c">ÔΩú</annotation><annotation encoding="application/x-llamapun" id="S3.T2.3.3.1.m1.1d">ÔΩú</annotation></semantics></math> ‚ÄúTotal Eclipse of Love‚Äù ah ah ah ah ah two kissing monsters!!</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>
<span class="ltx_text ltx_font_bold" id="S3.T2.5.1">Case Study</span>. We present examples of several translation challenges that pose difficulties for NMT systems but are effectively mitigated by MT-oriented LLMs.
</figcaption>
</figure>
<figure class="ltx_table" id="S3.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T3.8">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.8.9.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T3.8.9.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.8.9.1.1.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.8.9.1.2"><span class="ltx_text ltx_font_bold" id="S3.T3.8.9.1.2.1">GoogleMT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.8.9.1.3"><span class="ltx_text ltx_font_bold" id="S3.T3.8.9.1.3.1">MicroMT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.8.9.1.4"><span class="ltx_text ltx_font_bold" id="S3.T3.8.9.1.4.1">TIM</span></th>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="4" id="S3.T3.1.1.1"><span class="ltx_text ltx_font_italic" id="S3.T3.1.1.1.1">ZH<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.T3.1.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.1.m1.1a"><mo id="S3.T3.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T3.1.1.1.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.m1.1b"><ci id="S3.T3.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.1.m1.1d">‚áí</annotation></semantics></math>EN</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.2.2.1">COMET<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T3.2.2.1.m1.1"><semantics id="S3.T3.2.2.1.m1.1a"><mo id="S3.T3.2.2.1.m1.1.1" stretchy="false" xref="S3.T3.2.2.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.1.m1.1b"><ci id="S3.T3.2.2.1.m1.1.1.cmml" xref="S3.T3.2.2.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.2.2.1.m1.1d">‚Üë</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.2">64.4</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.3">59.2</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.4"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.4.1">65.1</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.3.3.1">COMETk.<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T3.3.3.1.m1.1"><semantics id="S3.T3.3.3.1.m1.1a"><mo id="S3.T3.3.3.1.m1.1.1" stretchy="false" xref="S3.T3.3.3.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.1.m1.1b"><ci id="S3.T3.3.3.1.m1.1.1.cmml" xref="S3.T3.3.3.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.3.3.1.m1.1d">‚Üë</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.2">59.1</td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.3">57.4</td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.4"><span class="ltx_text ltx_font_bold" id="S3.T3.3.3.4.1">61.9</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.8.10.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.8.10.1.1">#Length</th>
<td class="ltx_td ltx_align_center" id="S3.T3.8.10.1.2">56.81</td>
<td class="ltx_td ltx_align_center" id="S3.T3.8.10.1.3">51.38</td>
<td class="ltx_td ltx_align_center" id="S3.T3.8.10.1.4">52.09</td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.4.4.1">Off-Target<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T3.4.4.1.m1.1"><semantics id="S3.T3.4.4.1.m1.1a"><mo id="S3.T3.4.4.1.m1.1.1" stretchy="false" xref="S3.T3.4.4.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S3.T3.4.4.1.m1.1b"><ci id="S3.T3.4.4.1.m1.1.1.cmml" xref="S3.T3.4.4.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.4.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.4.4.1.m1.1d">‚Üì</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.2">1.08%</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.3">1.04%</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.4">1.82%</td>
</tr>
<tr class="ltx_tr" id="S3.T3.8.11.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.8.11.2.1">USW</th>
<td class="ltx_td ltx_align_center" id="S3.T3.8.11.2.2">13.87%</td>
<td class="ltx_td ltx_align_center" id="S3.T3.8.11.2.3">13.70%</td>
<td class="ltx_td ltx_align_center" id="S3.T3.8.11.2.4">16.52%</td>
</tr>
<tr class="ltx_tr" id="S3.T3.8.12.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.8.12.3.1">UTW</th>
<td class="ltx_td ltx_align_center" id="S3.T3.8.12.3.2">31.29%</td>
<td class="ltx_td ltx_align_center" id="S3.T3.8.12.3.3">25.08%</td>
<td class="ltx_td ltx_align_center" id="S3.T3.8.12.3.4">27.91%</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="4" id="S3.T3.5.5.1"><span class="ltx_text ltx_font_italic" id="S3.T3.5.5.1.1">EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.T3.5.5.1.1.m1.1"><semantics id="S3.T3.5.5.1.1.m1.1a"><mo id="S3.T3.5.5.1.1.m1.1.1" stretchy="false" xref="S3.T3.5.5.1.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S3.T3.5.5.1.1.m1.1b"><ci id="S3.T3.5.5.1.1.m1.1.1.cmml" xref="S3.T3.5.5.1.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.5.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.5.5.1.1.m1.1d">‚áí</annotation></semantics></math>ZH</span></th>
</tr>
<tr class="ltx_tr" id="S3.T3.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.6.6.1">COMET<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T3.6.6.1.m1.1"><semantics id="S3.T3.6.6.1.m1.1a"><mo id="S3.T3.6.6.1.m1.1.1" stretchy="false" xref="S3.T3.6.6.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T3.6.6.1.m1.1b"><ci id="S3.T3.6.6.1.m1.1.1.cmml" xref="S3.T3.6.6.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.6.6.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.6.6.1.m1.1d">‚Üë</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.2">71.2</td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.3">68.9</td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.4"><span class="ltx_text ltx_font_bold" id="S3.T3.6.6.4.1">74.6</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.7.7.1">COMETk.<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T3.7.7.1.m1.1"><semantics id="S3.T3.7.7.1.m1.1a"><mo id="S3.T3.7.7.1.m1.1.1" stretchy="false" xref="S3.T3.7.7.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S3.T3.7.7.1.m1.1b"><ci id="S3.T3.7.7.1.m1.1.1.cmml" xref="S3.T3.7.7.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.7.7.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.7.7.1.m1.1d">‚Üë</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.2">60.5</td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.3">62.9</td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.4"><span class="ltx_text ltx_font_bold" id="S3.T3.7.7.4.1">64.9</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.8.13.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.8.13.4.1">#Length</th>
<td class="ltx_td ltx_align_center" id="S3.T3.8.13.4.2">48.51</td>
<td class="ltx_td ltx_align_center" id="S3.T3.8.13.4.3">47.99</td>
<td class="ltx_td ltx_align_center" id="S3.T3.8.13.4.4">46.57</td>
</tr>
<tr class="ltx_tr" id="S3.T3.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.8.8.1">Off-Target<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T3.8.8.1.m1.1"><semantics id="S3.T3.8.8.1.m1.1a"><mo id="S3.T3.8.8.1.m1.1.1" stretchy="false" xref="S3.T3.8.8.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S3.T3.8.8.1.m1.1b"><ci id="S3.T3.8.8.1.m1.1.1.cmml" xref="S3.T3.8.8.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.8.8.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.8.8.1.m1.1d">‚Üì</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center" id="S3.T3.8.8.2">15.55%</td>
<td class="ltx_td ltx_align_center" id="S3.T3.8.8.3">14.08%</td>
<td class="ltx_td ltx_align_center" id="S3.T3.8.8.4">22.41%</td>
</tr>
<tr class="ltx_tr" id="S3.T3.8.14.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.8.14.5.1">USW</th>
<td class="ltx_td ltx_align_center" id="S3.T3.8.14.5.2">21.76%</td>
<td class="ltx_td ltx_align_center" id="S3.T3.8.14.5.3">20.23%</td>
<td class="ltx_td ltx_align_center" id="S3.T3.8.14.5.4">25.70%</td>
</tr>
<tr class="ltx_tr" id="S3.T3.8.15.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T3.8.15.6.1">UTW</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.8.15.6.2">16.55%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.8.15.6.3">13.64%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.8.15.6.4">16.39%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>
<span class="ltx_text ltx_font_bold" id="S3.T3.10.1">Experimental results on WebCrawl test sets</span>.
LLMs hold promise as potential fallback systems when NMT systems fail to meet quality expectations.
</figcaption>
</figure>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Main Results.</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.2">Similarly, we compute various evaluation metrics for NMT systems and TIM on the WebCrawl test sets.
As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.T3" title="Table 3 ‚Ä£ 3.3 Analyses on Web Crawl test sets ‚Ä£ 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">3</span></a>,
it is noteworthy that <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS0.Px1.p1.2.1">TIM</span> demonstrates significant improvements in both ZH<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px1.p1.1.m1.1a"><mo id="S3.SS3.SSS0.Px1.p1.1.m1.1.1" stretchy="false" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.1.m1.1d">‚áí</annotation></semantics></math>EN and EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.2.m2.1"><semantics id="S3.SS3.SSS0.Px1.p1.2.m2.1a"><mo id="S3.SS3.SSS0.Px1.p1.2.m2.1.1" stretchy="false" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.2.m2.1b"><ci id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.2.m2.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.2.m2.1d">‚áí</annotation></semantics></math>ZH directions.
This surprising finding suggests that MT-oriented LLMs can serve as valuable fallback systems in cases where the quality of commercial NMT systems is unsatisfactory.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p2.1">To further support our hypothesis,
we calculate the COMETkiwi scores of the translations generated by <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS0.Px1.p2.1.1">GoogleMT</span> and <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS0.Px1.p2.1.2">TIM</span> against the source text, selecting a group of sentences where <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS0.Px1.p2.1.3">GoogleMT</span> has higher scores than <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS0.Px1.p2.1.4">TIM</span> by more than 3 points, and another group where <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS0.Px1.p2.1.5">TIM</span> has higher scores than <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS0.Px1.p2.1.6">GoogleMT</span>.
To mitigate the impact of sentence lengths, we retain only those sentences containing fewer than 60 tokens.
Next, we use gpt2-large<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>https://huggingface.co/gpt2-large</span></span></span> to calculate the perplexity for the two groups.
The perplexity for sentences in which <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS0.Px1.p2.1.7">GoogleMT</span> excels is 38.61, whereas for sentences in which <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS0.Px1.p2.1.8">TIM</span> performs better, it is 45.51.
The MT-oriented LLM showcases superior proficiency in handling complex source language sentences, as reflected by higher perplexity scores.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Case Study.</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.T2" title="Table 2 ‚Ä£ 3.3 Analyses on Web Crawl test sets ‚Ä£ 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">2</span></a>, we provide several examples that are hard for <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS0.Px2.p1.1.1">GoogleMT</span> to handle but are solved well by <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS0.Px2.p1.1.2">TIM</span>.
It shows that the NMT system struggles to understand the meaning of some professional terms and fails to produce suitable translations for ill-informed text.
In contrast, MT-oriented LLM demonstrates its superiority in handling such issues, which can be attributed to its enhanced ability to comprehend rare, specialized words, and informal texts.</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="482" id="S3.F6.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
<span class="ltx_text ltx_font_bold" id="S3.F6.6.1">Cooperative Decoding.</span>
The NMT model generates the initial translation (referred to as <span class="ltx_text ltx_font_italic" id="S3.F6.7.2">draft</span>), and the MT-oriented LLM assesses the quality of the <span class="ltx_text ltx_font_italic" id="S3.F6.8.3">draft</span> and takes over from the error position, performing verification and re-decoding steps (<span class="ltx_text ltx_font_italic" id="S3.F6.9.4">Verify</span> and <span class="ltx_text ltx_font_italic" id="S3.F6.10.5">Re-decoding</span>).
</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Discussion</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">The analysis in Section <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.SS2" title="3.2 Analyses on WMT22 test sets ‚Ä£ 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">3.2</span></a> demonstrates the accuracy of commercial NMT systems, likely due to their extensive training and cross-attention capability.
MT-oriented LLMs, known for their paraphrastic nature, can further enhance NMT‚Äôs ability to handle figurative text translations.
Moreover, MT-oriented LLMs excel on WebCrawl test sets (Section <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.SS3" title="3.3 Analyses on Web Crawl test sets ‚Ä£ 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">3.3</span></a>), particularly with specialized terminology and ill-formed sentences. This suggests that an effective hybrid framework combining NMT and LLMs can handle challenging input domains and figurative text.
Based on these insights, we propose to investigate an effective hybrid framework
to answer the second question: <span class="ltx_text ltx_font_italic" id="S3.SS4.p1.1.1">How can we effectively harness the capabilities of LLMs to enhance translation quality?</span></p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">Hybrid Threshold <cite class="ltx_cite ltx_citemacro_cite">Hendy et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib14" title="">2023</a>)</cite> employs the NMT model as the primary translation system, with a quality estimation module (e.g., COMETkiwi) to assess the translation.
If the quality falls below a certain threshold, the GPT-like model is used as an alternative translation engine.
However, it faces two main challenges: autoregressive decoding latency and reliable quality estimation.
The practical implementation of the hybrid approaches must ensure efficient decoding and high translation quality.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Cooperative Decoding</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We propose an innovative cooperative decoding approach.
This approach
leverages an NMT model as a pretranslation model and incorporates an MT-oriented LLM as a quality assessment module and fallback model if needed.
The overview of cooperative decoding is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S3.F6" title="Figure 6 ‚Ä£ Case Study. ‚Ä£ 3.3 Analyses on Web Crawl test sets ‚Ä£ 3 Preliminary Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">6</span></a> and we will give a detailed description of each step in the following.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.16">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T4.2.2.3" style="padding-left:4.3pt;padding-right:4.3pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="S4.T4.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1">ZH<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S4.T4.1.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.1.m1.1a"><mo id="S4.T4.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T4.1.1.1.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.1.m1.1d">‚áí</annotation></semantics></math>EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="S4.T4.2.2.2" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.2.2.2.1">EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S4.T4.2.2.2.1.m1.1"><semantics id="S4.T4.2.2.2.1.m1.1a"><mo id="S4.T4.2.2.2.1.m1.1.1" stretchy="false" xref="S4.T4.2.2.2.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.1.m1.1b"><ci id="S4.T4.2.2.2.1.m1.1.1.cmml" xref="S4.T4.2.2.2.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.1.m1.1d">‚áí</annotation></semantics></math>ZH</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.16.17.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.16.17.1.1" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.16.17.1.1.1">Method</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.16.17.1.2" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.16.17.1.2.1">COMET</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.16.17.1.3" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.16.17.1.3.1">COMETk.</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.16.17.1.4" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.16.17.1.4.1">Token/s</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.16.17.1.5" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.16.17.1.5.1">Speedup</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.16.17.1.6" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.16.17.1.6.1">Ratio</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.16.17.1.7" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.16.17.1.7.1">COMET</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.16.17.1.8" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.16.17.1.8.1">COMETk.</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.16.17.1.9" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.16.17.1.9.1">Token/s</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.16.17.1.10" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.16.17.1.10.1">Speedup</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.16.17.1.11" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.16.17.1.11.1">Ratio</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.16.18.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.16.18.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">GoogleMT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.16.18.2.2" style="padding-left:4.3pt;padding-right:4.3pt;">76.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.16.18.2.3" style="padding-left:4.3pt;padding-right:4.3pt;">72.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.16.18.2.4" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.16.18.2.5" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.16.18.2.6" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.16.18.2.7" style="padding-left:4.3pt;padding-right:4.3pt;">81.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.16.18.2.8" style="padding-left:4.3pt;padding-right:4.3pt;">74.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.16.18.2.9" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.16.18.2.10" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.16.18.2.11" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.4.4.3" style="padding-left:4.3pt;padding-right:4.3pt;">TIM</th>
<td class="ltx_td ltx_align_center" id="S4.T4.4.4.4" style="padding-left:4.3pt;padding-right:4.3pt;">75.6</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.4.5" style="padding-left:4.3pt;padding-right:4.3pt;">72.3</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.4.6" style="padding-left:4.3pt;padding-right:4.3pt;">21.8</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.1" style="padding-left:4.3pt;padding-right:4.3pt;">1.0<math alttext="\times" class="ltx_Math" display="inline" id="S4.T4.3.3.1.m1.1"><semantics id="S4.T4.3.3.1.m1.1a"><mo id="S4.T4.3.3.1.m1.1.1" xref="S4.T4.3.3.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.1.m1.1b"><times id="S4.T4.3.3.1.m1.1.1.cmml" xref="S4.T4.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.1.m1.1d">√ó</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.4.4.7" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.4.8" style="padding-left:4.3pt;padding-right:4.3pt;">83.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.4.9" style="padding-left:4.3pt;padding-right:4.3pt;">76.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.4.10" style="padding-left:4.3pt;padding-right:4.3pt;">20.7</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.4.2" style="padding-left:4.3pt;padding-right:4.3pt;">1.0<math alttext="\times" class="ltx_Math" display="inline" id="S4.T4.4.4.2.m1.1"><semantics id="S4.T4.4.4.2.m1.1a"><mo id="S4.T4.4.4.2.m1.1.1" xref="S4.T4.4.4.2.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.2.m1.1b"><times id="S4.T4.4.4.2.m1.1.1.cmml" xref="S4.T4.4.4.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T4.4.4.2.m1.1d">√ó</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.4.11" style="padding-left:4.3pt;padding-right:4.3pt;">-</td>
</tr>
<tr class="ltx_tr" id="S4.T4.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.6.6.3" style="padding-left:4.3pt;padding-right:4.3pt;">CoDec-4</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.6.6.4" style="padding-left:4.3pt;padding-right:4.3pt;">76.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.6.6.5" style="padding-left:4.3pt;padding-right:4.3pt;">73.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.6.6.6" style="padding-left:4.3pt;padding-right:4.3pt;">28.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.5.5.1" style="padding-left:4.3pt;padding-right:4.3pt;">1.3<math alttext="\times" class="ltx_Math" display="inline" id="S4.T4.5.5.1.m1.1"><semantics id="S4.T4.5.5.1.m1.1a"><mo id="S4.T4.5.5.1.m1.1.1" xref="S4.T4.5.5.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.1.m1.1b"><times id="S4.T4.5.5.1.m1.1.1.cmml" xref="S4.T4.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T4.5.5.1.m1.1d">√ó</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.6.6.7" style="padding-left:4.3pt;padding-right:4.3pt;">24.44</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.6.6.8" style="padding-left:4.3pt;padding-right:4.3pt;">83.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.6.6.9" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.6.9.1">76.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.6.6.10" style="padding-left:4.3pt;padding-right:4.3pt;">24.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.6.6.2" style="padding-left:4.3pt;padding-right:4.3pt;">1.2<math alttext="\times" class="ltx_Math" display="inline" id="S4.T4.6.6.2.m1.1"><semantics id="S4.T4.6.6.2.m1.1a"><mo id="S4.T4.6.6.2.m1.1.1" xref="S4.T4.6.6.2.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.2.m1.1b"><times id="S4.T4.6.6.2.m1.1.1.cmml" xref="S4.T4.6.6.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T4.6.6.2.m1.1d">√ó</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.6.6.11" style="padding-left:4.3pt;padding-right:4.3pt;">21.64</td>
</tr>
<tr class="ltx_tr" id="S4.T4.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.8.8.3" style="padding-left:4.3pt;padding-right:4.3pt;">CoDec-8</th>
<td class="ltx_td ltx_align_center" id="S4.T4.8.8.4" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.8.8.4.1">77.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.8.8.5" style="padding-left:4.3pt;padding-right:4.3pt;">73.2</td>
<td class="ltx_td ltx_align_center" id="S4.T4.8.8.6" style="padding-left:4.3pt;padding-right:4.3pt;">32.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.7.7.1" style="padding-left:4.3pt;padding-right:4.3pt;">1.5<math alttext="\times" class="ltx_Math" display="inline" id="S4.T4.7.7.1.m1.1"><semantics id="S4.T4.7.7.1.m1.1a"><mo id="S4.T4.7.7.1.m1.1.1" xref="S4.T4.7.7.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T4.7.7.1.m1.1b"><times id="S4.T4.7.7.1.m1.1.1.cmml" xref="S4.T4.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.7.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T4.7.7.1.m1.1d">√ó</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.8.8.7" style="padding-left:4.3pt;padding-right:4.3pt;">38.83</td>
<td class="ltx_td ltx_align_center" id="S4.T4.8.8.8" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.8.8.8.1">83.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.8.8.9" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.8.8.9.1">76.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.8.8.10" style="padding-left:4.3pt;padding-right:4.3pt;">25.8</td>
<td class="ltx_td ltx_align_center" id="S4.T4.8.8.2" style="padding-left:4.3pt;padding-right:4.3pt;">1.3<math alttext="\times" class="ltx_Math" display="inline" id="S4.T4.8.8.2.m1.1"><semantics id="S4.T4.8.8.2.m1.1a"><mo id="S4.T4.8.8.2.m1.1.1" xref="S4.T4.8.8.2.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T4.8.8.2.m1.1b"><times id="S4.T4.8.8.2.m1.1.1.cmml" xref="S4.T4.8.8.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.8.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T4.8.8.2.m1.1d">√ó</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.8.8.11" style="padding-left:4.3pt;padding-right:4.3pt;">32.69</td>
</tr>
<tr class="ltx_tr" id="S4.T4.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.10.10.3" style="padding-left:4.3pt;padding-right:4.3pt;">CoDec-16</th>
<td class="ltx_td ltx_align_center" id="S4.T4.10.10.4" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.10.10.4.1">77.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.10.5" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.10.10.5.1">73.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.10.6" style="padding-left:4.3pt;padding-right:4.3pt;">38.7</td>
<td class="ltx_td ltx_align_center" id="S4.T4.9.9.1" style="padding-left:4.3pt;padding-right:4.3pt;">1.8<math alttext="\times" class="ltx_Math" display="inline" id="S4.T4.9.9.1.m1.1"><semantics id="S4.T4.9.9.1.m1.1a"><mo id="S4.T4.9.9.1.m1.1.1" xref="S4.T4.9.9.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T4.9.9.1.m1.1b"><times id="S4.T4.9.9.1.m1.1.1.cmml" xref="S4.T4.9.9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.9.9.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T4.9.9.1.m1.1d">√ó</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.10.10.7" style="padding-left:4.3pt;padding-right:4.3pt;">55.11</td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.10.8" style="padding-left:4.3pt;padding-right:4.3pt;">83.1</td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.10.9" style="padding-left:4.3pt;padding-right:4.3pt;">76.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.10.10" style="padding-left:4.3pt;padding-right:4.3pt;">29.7</td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.10.2" style="padding-left:4.3pt;padding-right:4.3pt;">1.4<math alttext="\times" class="ltx_Math" display="inline" id="S4.T4.10.10.2.m1.1"><semantics id="S4.T4.10.10.2.m1.1a"><mo id="S4.T4.10.10.2.m1.1.1" xref="S4.T4.10.10.2.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T4.10.10.2.m1.1b"><times id="S4.T4.10.10.2.m1.1.1.cmml" xref="S4.T4.10.10.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.10.10.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T4.10.10.2.m1.1d">√ó</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.10.10.11" style="padding-left:4.3pt;padding-right:4.3pt;">46.06</td>
</tr>
<tr class="ltx_tr" id="S4.T4.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.12.12.3" style="padding-left:4.3pt;padding-right:4.3pt;">CoDec-32</th>
<td class="ltx_td ltx_align_center" id="S4.T4.12.12.4" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.12.12.4.1">77.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.12.12.5" style="padding-left:4.3pt;padding-right:4.3pt;">73.2</td>
<td class="ltx_td ltx_align_center" id="S4.T4.12.12.6" style="padding-left:4.3pt;padding-right:4.3pt;">47.9</td>
<td class="ltx_td ltx_align_center" id="S4.T4.11.11.1" style="padding-left:4.3pt;padding-right:4.3pt;">2.2<math alttext="\times" class="ltx_Math" display="inline" id="S4.T4.11.11.1.m1.1"><semantics id="S4.T4.11.11.1.m1.1a"><mo id="S4.T4.11.11.1.m1.1.1" xref="S4.T4.11.11.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T4.11.11.1.m1.1b"><times id="S4.T4.11.11.1.m1.1.1.cmml" xref="S4.T4.11.11.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.11.11.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T4.11.11.1.m1.1d">√ó</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.12.12.7" style="padding-left:4.3pt;padding-right:4.3pt;">67.36</td>
<td class="ltx_td ltx_align_center" id="S4.T4.12.12.8" style="padding-left:4.3pt;padding-right:4.3pt;">83.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.12.12.9" style="padding-left:4.3pt;padding-right:4.3pt;">75.8</td>
<td class="ltx_td ltx_align_center" id="S4.T4.12.12.10" style="padding-left:4.3pt;padding-right:4.3pt;">33.6</td>
<td class="ltx_td ltx_align_center" id="S4.T4.12.12.2" style="padding-left:4.3pt;padding-right:4.3pt;">1.6<math alttext="\times" class="ltx_Math" display="inline" id="S4.T4.12.12.2.m1.1"><semantics id="S4.T4.12.12.2.m1.1a"><mo id="S4.T4.12.12.2.m1.1.1" xref="S4.T4.12.12.2.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T4.12.12.2.m1.1b"><times id="S4.T4.12.12.2.m1.1.1.cmml" xref="S4.T4.12.12.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.12.12.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T4.12.12.2.m1.1d">√ó</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.12.12.11" style="padding-left:4.3pt;padding-right:4.3pt;">57.06</td>
</tr>
<tr class="ltx_tr" id="S4.T4.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.14.14.3" style="padding-left:4.3pt;padding-right:4.3pt;">CoDec-64</th>
<td class="ltx_td ltx_align_center" id="S4.T4.14.14.4" style="padding-left:4.3pt;padding-right:4.3pt;">77.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.14.14.5" style="padding-left:4.3pt;padding-right:4.3pt;">73.1</td>
<td class="ltx_td ltx_align_center" id="S4.T4.14.14.6" style="padding-left:4.3pt;padding-right:4.3pt;">57.7</td>
<td class="ltx_td ltx_align_center" id="S4.T4.13.13.1" style="padding-left:4.3pt;padding-right:4.3pt;">2.7<math alttext="\times" class="ltx_Math" display="inline" id="S4.T4.13.13.1.m1.1"><semantics id="S4.T4.13.13.1.m1.1a"><mo id="S4.T4.13.13.1.m1.1.1" xref="S4.T4.13.13.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T4.13.13.1.m1.1b"><times id="S4.T4.13.13.1.m1.1.1.cmml" xref="S4.T4.13.13.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.13.13.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T4.13.13.1.m1.1d">√ó</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.14.14.7" style="padding-left:4.3pt;padding-right:4.3pt;">76.23</td>
<td class="ltx_td ltx_align_center" id="S4.T4.14.14.8" style="padding-left:4.3pt;padding-right:4.3pt;">82.7</td>
<td class="ltx_td ltx_align_center" id="S4.T4.14.14.9" style="padding-left:4.3pt;padding-right:4.3pt;">75.6</td>
<td class="ltx_td ltx_align_center" id="S4.T4.14.14.10" style="padding-left:4.3pt;padding-right:4.3pt;">38.7</td>
<td class="ltx_td ltx_align_center" id="S4.T4.14.14.2" style="padding-left:4.3pt;padding-right:4.3pt;">1.9<math alttext="\times" class="ltx_Math" display="inline" id="S4.T4.14.14.2.m1.1"><semantics id="S4.T4.14.14.2.m1.1a"><mo id="S4.T4.14.14.2.m1.1.1" xref="S4.T4.14.14.2.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T4.14.14.2.m1.1b"><times id="S4.T4.14.14.2.m1.1.1.cmml" xref="S4.T4.14.14.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.14.14.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T4.14.14.2.m1.1d">√ó</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.14.14.11" style="padding-left:4.3pt;padding-right:4.3pt;">66.25</td>
</tr>
<tr class="ltx_tr" id="S4.T4.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T4.16.16.3" style="padding-left:4.3pt;padding-right:4.3pt;">CoDec-128</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.16.16.4" style="padding-left:4.3pt;padding-right:4.3pt;">77.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.16.16.5" style="padding-left:4.3pt;padding-right:4.3pt;">73.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.16.16.6" style="padding-left:4.3pt;padding-right:4.3pt;">73.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.15.15.1" style="padding-left:4.3pt;padding-right:4.3pt;">3.4<math alttext="\times" class="ltx_Math" display="inline" id="S4.T4.15.15.1.m1.1"><semantics id="S4.T4.15.15.1.m1.1a"><mo id="S4.T4.15.15.1.m1.1.1" xref="S4.T4.15.15.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T4.15.15.1.m1.1b"><times id="S4.T4.15.15.1.m1.1.1.cmml" xref="S4.T4.15.15.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.15.15.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T4.15.15.1.m1.1d">√ó</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.16.16.7" style="padding-left:4.3pt;padding-right:4.3pt;">84.29</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.16.16.8" style="padding-left:4.3pt;padding-right:4.3pt;">82.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.16.16.9" style="padding-left:4.3pt;padding-right:4.3pt;">75.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.16.16.10" style="padding-left:4.3pt;padding-right:4.3pt;">45.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.16.16.2" style="padding-left:4.3pt;padding-right:4.3pt;">2.2<math alttext="\times" class="ltx_Math" display="inline" id="S4.T4.16.16.2.m1.1"><semantics id="S4.T4.16.16.2.m1.1a"><mo id="S4.T4.16.16.2.m1.1.1" xref="S4.T4.16.16.2.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S4.T4.16.16.2.m1.1b"><times id="S4.T4.16.16.2.m1.1.1.cmml" xref="S4.T4.16.16.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.16.16.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T4.16.16.2.m1.1d">√ó</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.16.16.11" style="padding-left:4.3pt;padding-right:4.3pt;">74.35</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>
<span class="ltx_text ltx_font_bold" id="S4.T4.21.1">Effect of different values of <math alttext="k" class="ltx_Math" display="inline" id="S4.T4.21.1.m1.1"><semantics id="S4.T4.21.1.m1.1b"><mi id="S4.T4.21.1.m1.1.1" xref="S4.T4.21.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.T4.21.1.m1.1c"><ci id="S4.T4.21.1.m1.1.1.cmml" xref="S4.T4.21.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.21.1.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="S4.T4.21.1.m1.1e">italic_k</annotation></semantics></math> (Eq. <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S4.E1" title="In Step2: Verification. ‚Ä£ 4 Cooperative Decoding ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">1</span></a>) for CoDec</span>.
We present the results on ZH<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S4.T4.22.m1.1"><semantics id="S4.T4.22.m1.1b"><mo id="S4.T4.22.m1.1.1" stretchy="false" xref="S4.T4.22.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S4.T4.22.m1.1c"><ci id="S4.T4.22.m1.1.1.cmml" xref="S4.T4.22.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.22.m1.1d">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.22.m1.1e">‚áí</annotation></semantics></math>EN and EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S4.T4.23.m2.1"><semantics id="S4.T4.23.m2.1b"><mo id="S4.T4.23.m2.1.1" stretchy="false" xref="S4.T4.23.m2.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S4.T4.23.m2.1c"><ci id="S4.T4.23.m2.1.1.cmml" xref="S4.T4.23.m2.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.23.m2.1d">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.23.m2.1e">‚áí</annotation></semantics></math>ZH including COMET-22, COMETkiwi, decoding speed measured by tokens per second, decoding speedup, and the ratio of the number of tokens accepted at the verification stage to the total tokens of the draft. The choice of <math alttext="k" class="ltx_Math" display="inline" id="S4.T4.24.m3.1"><semantics id="S4.T4.24.m3.1b"><mi id="S4.T4.24.m3.1.1" xref="S4.T4.24.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.T4.24.m3.1c"><ci id="S4.T4.24.m3.1.1.cmml" xref="S4.T4.24.m3.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.24.m3.1d">k</annotation><annotation encoding="application/x-llamapun" id="S4.T4.24.m3.1e">italic_k</annotation></semantics></math> should be considered to strike a balance between performance and efficiency.
</figcaption>
</figure>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Step1: Draft Generation.</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.3">Given an input source sentence <math alttext="x" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S4.SS0.SSS0.Px1.p1.1.m1.1a"><mi id="S4.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.1.m1.1b"><ci id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1">ùë•</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px1.p1.1.m1.1d">italic_x</annotation></semantics></math>, the NMT system generates the translation <math alttext="o" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.2.m2.1"><semantics id="S4.SS0.SSS0.Px1.p1.2.m2.1a"><mi id="S4.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">o</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.2.m2.1b"><ci id="S4.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1">ùëú</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.2.m2.1c">o</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px1.p1.2.m2.1d">italic_o</annotation></semantics></math> using an autoregressive decoding strategy like beam search.
The difference is that the translation <math alttext="o" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px1.p1.3.m3.1"><semantics id="S4.SS0.SSS0.Px1.p1.3.m3.1a"><mi id="S4.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">o</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.3.m3.1b"><ci id="S4.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.3.m3.1.1">ùëú</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.3.m3.1c">o</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px1.p1.3.m3.1d">italic_o</annotation></semantics></math> is considered as a draft and requires further confirmation or modification before being used as the final output.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Step2: Verification.</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.8">We feed <math alttext="o" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S4.SS0.SSS0.Px2.p1.1.m1.1a"><mi id="S4.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">o</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.1.m1.1b"><ci id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1">ùëú</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.1.m1.1c">o</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.1.m1.1d">italic_o</annotation></semantics></math> into the MT-oriented LLM in a forward process, which fully utilizes parallel computing.
The procedure is the same as training LLMs, and we can obtain a probability distribution <math alttext="v_{t}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="S4.SS0.SSS0.Px2.p1.2.m2.1a"><msub id="S4.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.2" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml">v</mi><mi id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.3" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.2.m2.1b"><apply id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.2">ùë£</ci><ci id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.2.m2.1c">v_{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.2.m2.1d">italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> at each position, which is modeled as <math alttext="P(o_{t}|o_{&lt;t},x)" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.3.m3.2"><semantics id="S4.SS0.SSS0.Px2.p1.3.m3.2a"><mrow id="S4.SS0.SSS0.Px2.p1.3.m3.2.2" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.cmml"><mi id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.3" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.3.cmml">P</mi><mo id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.2" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.2.cmml">‚Å¢</mo><mrow id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.cmml"><mo id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.2" stretchy="false" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.cmml">(</mo><mrow id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.cmml"><msub id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.3" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.3.cmml"><mi id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.3.2" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.3.2.cmml">o</mi><mi id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.3.3" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.3.3.cmml">t</mi></msub><mo fence="false" id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.2" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.2.cmml">|</mo><mrow id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.2.cmml"><msub id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.cmml"><mi id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.2" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.2.cmml">o</mi><mrow id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.3" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.3.cmml"><mi id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.3.2" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.3.1" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.3.3" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.3.3.cmml">t</mi></mrow></msub><mo id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.2" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.2.cmml">,</mo><mi id="S4.SS0.SSS0.Px2.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1.cmml">x</mi></mrow></mrow><mo id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.3" stretchy="false" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.3.m3.2b"><apply id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2"><times id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.2.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.2"></times><ci id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.3.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.3">ùëÉ</ci><apply id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1"><csymbol cd="latexml" id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.2.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.2">conditional</csymbol><apply id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.3.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.3.1.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.3">subscript</csymbol><ci id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.3.2.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.3.2">ùëú</ci><ci id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.3.3.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.3.3">ùë°</ci></apply><list id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.2.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1"><apply id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.2.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.2">ùëú</ci><apply id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.3.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.3"><lt id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.3.1.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.3.2.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.3.3.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.2.2.1.1.1.1.1.1.3.3">ùë°</ci></apply></apply><ci id="S4.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1">ùë•</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.3.m3.2c">P(o_{t}|o_{&lt;t},x)</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.3.m3.2d">italic_P ( italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_o start_POSTSUBSCRIPT &lt; italic_t end_POSTSUBSCRIPT , italic_x )</annotation></semantics></math>.
The distribution can be regarded as the confidence of the LLM given the specific prefix of the draft <math alttext="o" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.4.m4.1"><semantics id="S4.SS0.SSS0.Px2.p1.4.m4.1a"><mi id="S4.SS0.SSS0.Px2.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.cmml">o</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.4.m4.1b"><ci id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1">ùëú</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.4.m4.1c">o</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.4.m4.1d">italic_o</annotation></semantics></math>, and we use it to verify the prediction of the NMT model.
One straightforward approach for verification is to check whether the token with the highest probability matches the prediction of NMT.
If <math alttext="o" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.5.m5.1"><semantics id="S4.SS0.SSS0.Px2.p1.5.m5.1a"><mi id="S4.SS0.SSS0.Px2.p1.5.m5.1.1" xref="S4.SS0.SSS0.Px2.p1.5.m5.1.1.cmml">o</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.5.m5.1b"><ci id="S4.SS0.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.5.m5.1.1">ùëú</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.5.m5.1c">o</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.5.m5.1d">italic_o</annotation></semantics></math> is fortunately exactly the same with <math alttext="\{{\rm argmax}(v_{1}),...,{\rm argmax}(v_{n})\}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.6.m6.3"><semantics id="S4.SS0.SSS0.Px2.p1.6.m6.3a"><mrow id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.3.cmml"><mo id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.3" stretchy="false" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.3.cmml">{</mo><mrow id="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1" xref="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.cmml"><mi id="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.3" xref="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.3.cmml">argmax</mi><mo id="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.2" xref="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.2.cmml">‚Å¢</mo><mrow id="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.1.1" xref="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.1.1.1.cmml"><mo id="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.1.1.2" stretchy="false" xref="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.1.1.1.cmml">(</mo><msub id="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.1.1.1" xref="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.1.1.1.cmml"><mi id="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.1.1.1.2" xref="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.1.1.1.2.cmml">v</mi><mn id="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.1.1.1.3" xref="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.1.1.3" stretchy="false" xref="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.4" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.3.cmml">,</mo><mi id="S4.SS0.SSS0.Px2.p1.6.m6.1.1" mathvariant="normal" xref="S4.SS0.SSS0.Px2.p1.6.m6.1.1.cmml">‚Ä¶</mi><mo id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.5" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.3.cmml">,</mo><mrow id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.cmml"><mi id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.3" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.3.cmml">argmax</mi><mo id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.2" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.2.cmml">‚Å¢</mo><mrow id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.1.1" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.1.1.1.cmml"><mo id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.1.1.2" stretchy="false" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.1.1.1.cmml">(</mo><msub id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.1.1.1" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.1.1.1.cmml"><mi id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.1.1.1.2" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.1.1.1.2.cmml">v</mi><mi id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.1.1.1.3" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.1.1.1.3.cmml">n</mi></msub><mo id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.1.1.3" stretchy="false" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.6" stretchy="false" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.6.m6.3b"><set id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.3.cmml" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2"><apply id="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1"><times id="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.2.cmml" xref="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.2"></times><ci id="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.3.cmml" xref="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.3">argmax</ci><apply id="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.1.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.1.1">subscript</csymbol><ci id="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.1.1.1.2.cmml" xref="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.1.1.1.2">ùë£</ci><cn id="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.1.1.1.3.cmml" type="integer" xref="S4.SS0.SSS0.Px2.p1.6.m6.2.2.1.1.1.1.1.3">1</cn></apply></apply><ci id="S4.SS0.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.6.m6.1.1">‚Ä¶</ci><apply id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.cmml" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2"><times id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.2.cmml" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.2"></times><ci id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.3.cmml" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.3">argmax</ci><apply id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.1.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.1.1">subscript</csymbol><ci id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.1.1.1.2.cmml" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.1.1.1.2">ùë£</ci><ci id="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.1.1.1.3.cmml" xref="S4.SS0.SSS0.Px2.p1.6.m6.3.3.2.2.1.1.1.3">ùëõ</ci></apply></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.6.m6.3c">\{{\rm argmax}(v_{1}),...,{\rm argmax}(v_{n})\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.6.m6.3d">{ roman_argmax ( italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , ‚Ä¶ , roman_argmax ( italic_v start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) }</annotation></semantics></math>, the inference will finish with <math alttext="o" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.7.m7.1"><semantics id="S4.SS0.SSS0.Px2.p1.7.m7.1a"><mi id="S4.SS0.SSS0.Px2.p1.7.m7.1.1" xref="S4.SS0.SSS0.Px2.p1.7.m7.1.1.cmml">o</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.7.m7.1b"><ci id="S4.SS0.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.7.m7.1.1">ùëú</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.7.m7.1c">o</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.7.m7.1d">italic_o</annotation></semantics></math> as the final translation.
However, high-quality generation does not follow a distribution of the highest probability of the next tokens, and the tokens in <math alttext="o" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.8.m8.1"><semantics id="S4.SS0.SSS0.Px2.p1.8.m8.1a"><mi id="S4.SS0.SSS0.Px2.p1.8.m8.1.1" xref="S4.SS0.SSS0.Px2.p1.8.m8.1.1.cmml">o</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.8.m8.1b"><ci id="S4.SS0.SSS0.Px2.p1.8.m8.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.8.m8.1.1">ùëú</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.8.m8.1c">o</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.8.m8.1d">italic_o</annotation></semantics></math> that can be regarded as accurate may appear outside of the top-1 selection, like in beam search.
To address this issue,
we relax the matching constraint using the top-<span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px2.p1.8.1">k</span> candidates of the LLM and define the verification criterion as</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="o_{t}\in{\rm top-}{k}(v_{t})." class="ltx_Math" display="block" id="S4.E1.m1.1"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><msub id="S4.E1.m1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.1.1.3.2" xref="S4.E1.m1.1.1.1.1.3.2.cmml">o</mi><mi id="S4.E1.m1.1.1.1.1.3.3" xref="S4.E1.m1.1.1.1.1.3.3.cmml">t</mi></msub><mo id="S4.E1.m1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.2.cmml">‚àà</mo><mrow id="S4.E1.m1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.3.cmml">top</mi><mo id="S4.E1.m1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.2.cmml">‚àí</mo><mrow id="S4.E1.m1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.3.cmml">k</mi><mo id="S4.E1.m1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.2.cmml">‚Å¢</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E1.m1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml">v</mi><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S4.E1.m1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S4.E1.m1.1.1.1.2" lspace="0em" xref="S4.E1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"><in id="S4.E1.m1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.2"></in><apply id="S4.E1.m1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.3.2">ùëú</ci><ci id="S4.E1.m1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.3.3">ùë°</ci></apply><apply id="S4.E1.m1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1"><minus id="S4.E1.m1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.2"></minus><ci id="S4.E1.m1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.3">top</ci><apply id="S4.E1.m1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1"><times id="S4.E1.m1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.2"></times><ci id="S4.E1.m1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.3">ùëò</ci><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.2">ùë£</ci><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.3">ùë°</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">o_{t}\in{\rm top-}{k}(v_{t}).</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.1d">italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚àà roman_top - italic_k ( italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Step3: Re-decoding.</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p1.2">The verification is performed from left to right, and we end the verification once there is a situation that does not meet the verification criteria, i.e., <math alttext="o_{t^{\prime}}\notin{\rm top-}{k}(v_{t^{\prime}})" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px3.p1.1.m1.1"><semantics id="S4.SS0.SSS0.Px3.p1.1.m1.1a"><mrow id="S4.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.cmml"><msub id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml"><mi id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.2" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.2.cmml">o</mi><msup id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.3" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.cmml"><mi id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.2" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.2.cmml">t</mi><mo id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.3" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.3.cmml">‚Ä≤</mo></msup></msub><mo id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.2" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml">‚àâ</mo><mrow id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml"><mi id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.3" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.3.cmml">top</mi><mo id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.2" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.2.cmml">‚àí</mo><mrow id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.cmml"><mi id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.3" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.3.cmml">k</mi><mo id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.2" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.2.cmml">‚Å¢</mo><mrow id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.cmml"><mo id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.2" stretchy="false" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.2" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.2.cmml">v</mi><msup id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.3" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.3.2" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.3.2.cmml">t</mi><mo id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.3.3" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.3.3.cmml">‚Ä≤</mo></msup></msub><mo id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.3" stretchy="false" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.1.m1.1b"><apply id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1"><notin id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.2"></notin><apply id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3">subscript</csymbol><ci id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.2.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.2">ùëú</ci><apply id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.3">superscript</csymbol><ci id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.2.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.2">ùë°</ci><ci id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.3.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.3">‚Ä≤</ci></apply></apply><apply id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1"><minus id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.2"></minus><ci id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.3">top</ci><apply id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1"><times id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.2"></times><ci id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.3">ùëò</ci><apply id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.2">ùë£</ci><apply id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.3.2">ùë°</ci><ci id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.1.1.1.1.3.3">‚Ä≤</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.1.m1.1c">o_{t^{\prime}}\notin{\rm top-}{k}(v_{t^{\prime}})</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px3.p1.1.m1.1d">italic_o start_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ‚àâ roman_top - italic_k ( italic_v start_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT )</annotation></semantics></math>.
Then, we feed the verified prefix <math alttext="o_{t^{\prime}-1}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px3.p1.2.m2.1"><semantics id="S4.SS0.SSS0.Px3.p1.2.m2.1a"><msub id="S4.SS0.SSS0.Px3.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.cmml"><mi id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.2" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml">o</mi><mrow id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml"><msup id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.2" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.2.cmml"><mi id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.2.2" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.2.2.cmml">t</mi><mo id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.2.3" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.2.3.cmml">‚Ä≤</mo></msup><mo id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.1" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.1.cmml">‚àí</mo><mn id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.3" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.2.m2.1b"><apply id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.2">ùëú</ci><apply id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3"><minus id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.1.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.1"></minus><apply id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.2.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.2"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.2.1.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.2">superscript</csymbol><ci id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.2.2.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.2.2">ùë°</ci><ci id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.2.3.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.2.3">‚Ä≤</ci></apply><cn id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.3.cmml" type="integer" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.2.m2.1c">o_{t^{\prime}-1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px3.p1.2.m2.1d">italic_o start_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT - 1 end_POSTSUBSCRIPT</annotation></semantics></math> into the MT-oriented LLM and use it to re-decode the subsequent sequence.
Compared to totally replacing NMT models with MT-oriented LLMs, our cooperative decoding can speed up the whole inference process due to the expensive cost of autoregressive decoding.
The speedup is more significant when the longer draft is accepted.
Moreover, the cooperative mechanism alleviates the issue of inaccuracy of LLMs by exploiting the output of NMT models.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Main Results</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We merge the WMT22 and WebCrawl test sets to simulate the distribution of translation requests in real-world scenarios.
For CoDec, we use GoogleMT as the NMT system, and TIM as the MT-oriented LLM.
In particular, we set the threshold as the 50th percentile of COMETkiwi scores of GoogleMT <cite class="ltx_cite ltx_citemacro_citep">(Hendy et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib14" title="">2023</a>)</cite>.
We use the MT-oriented LLM to generate the translation only when the COMETkiwi score of the NMT translation is under the threshold.
We use beam search with a beam size of 4 for TIM during inference.
The decoding and speed measurement processes are performed on a single A100 GPU.</p>
</div>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Effect of different values of <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.1.m1.1"><semantics id="S5.SS1.SSS0.Px1.1.m1.1b"><mi id="S5.SS1.SSS0.Px1.1.m1.1.1" xref="S5.SS1.SSS0.Px1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.1.m1.1c"><ci id="S5.SS1.SSS0.Px1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px1.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.1.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.1.m1.1e">italic_k</annotation></semantics></math>.</h4>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.2">Intuitively, as <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S5.SS1.SSS0.Px1.p1.1.m1.1a"><mi id="S5.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.1.m1.1b"><ci id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p1.1.m1.1d">italic_k</annotation></semantics></math> increases, cooperative decoding can accept a wider range of tokens in NMT translations during the verification stage.
As a result, less content needs to be re-decoded by LLMs, leading to a reduction in processing time.
Here, we examine the performance of CoDec under various values of <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.2.m2.1"><semantics id="S5.SS1.SSS0.Px1.p1.2.m2.1a"><mi id="S5.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.2.m2.1b"><ci id="S5.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.2.m2.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p1.2.m2.1d">italic_k</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p2.5">As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S4.T4" title="Table 4 ‚Ä£ 4 Cooperative Decoding ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">4</span></a>, with the increase of <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p2.1.m1.1"><semantics id="S5.SS1.SSS0.Px1.p2.1.m1.1a"><mi id="S5.SS1.SSS0.Px1.p2.1.m1.1.1" xref="S5.SS1.SSS0.Px1.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p2.1.m1.1b"><ci id="S5.SS1.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p2.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p2.1.m1.1d">italic_k</annotation></semantics></math>, the ratio of tokens accepted on average and the decoding speed increase consistently.
With a larger <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p2.2.m2.1"><semantics id="S5.SS1.SSS0.Px1.p2.2.m2.1a"><mi id="S5.SS1.SSS0.Px1.p2.2.m2.1.1" xref="S5.SS1.SSS0.Px1.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p2.2.m2.1b"><ci id="S5.SS1.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S5.SS1.SSS0.Px1.p2.2.m2.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p2.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p2.2.m2.1d">italic_k</annotation></semantics></math>, <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px1.p2.5.1">CoDeC-128</span> achieves a 3.4x and 2.2x speedup over <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px1.p2.5.2">TIM</span> in ZH<math alttext="\Leftrightarrow" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p2.3.m3.1"><semantics id="S5.SS1.SSS0.Px1.p2.3.m3.1a"><mo id="S5.SS1.SSS0.Px1.p2.3.m3.1.1" stretchy="false" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1.cmml">‚áî</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p2.3.m3.1b"><ci id="S5.SS1.SSS0.Px1.p2.3.m3.1.1.cmml" xref="S5.SS1.SSS0.Px1.p2.3.m3.1.1">‚áî</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p2.3.m3.1c">\Leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p2.3.m3.1d">‚áî</annotation></semantics></math>EN.
This signifies that CoDec effectively reduces decoding latency while maintaining translation quality.
Besides, our CoDec-(*) models exhibit superior performance compared to both <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px1.p2.5.3">GoogleMT</span> and <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px1.p2.5.4">TIM</span>.
This highlights the potential of cooperative decoding in improving translation accuracy and overall system performance.
Moreover, models with lower values of <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p2.4.m4.1"><semantics id="S5.SS1.SSS0.Px1.p2.4.m4.1a"><mi id="S5.SS1.SSS0.Px1.p2.4.m4.1.1" xref="S5.SS1.SSS0.Px1.p2.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p2.4.m4.1b"><ci id="S5.SS1.SSS0.Px1.p2.4.m4.1.1.cmml" xref="S5.SS1.SSS0.Px1.p2.4.m4.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p2.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p2.4.m4.1d">italic_k</annotation></semantics></math>, such as <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px1.p2.5.5">CoDec-8</span>, achieve better translation quality, suggesting that the choice of <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p2.5.m5.1"><semantics id="S5.SS1.SSS0.Px1.p2.5.m5.1a"><mi id="S5.SS1.SSS0.Px1.p2.5.m5.1.1" xref="S5.SS1.SSS0.Px1.p2.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p2.5.m5.1b"><ci id="S5.SS1.SSS0.Px1.p2.5.m5.1.1.cmml" xref="S5.SS1.SSS0.Px1.p2.5.m5.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p2.5.m5.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p2.5.m5.1d">italic_k</annotation></semantics></math> should be considered to strike a balance between performance and efficiency.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">CoDec vs. Hybrid Threshold.</h4>
<div class="ltx_para" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.1">In our comparison between CoDec and Hybrid Threshold,
we utilize different Quality Estimation (QE) methods, including
<span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px2.p1.1.1">HT(Random)</span>, where 50% of GoogleMT‚Äôs translations are randomly replaced with TIM‚Äôs translations,
<span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px2.p1.1.2">HT(BLEURT-12)</span>, which uses BLEURT-20-D12<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>https://huggingface.co/lucadiliello/BLEURT-20-D12</span></span></span> as the QE method;
<span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px2.p1.1.3">HT(BLEURT-20)</span>, which employs BLEURT-20<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>https://huggingface.co/lucadiliello/BLEURT-20</span></span></span> as the QE method;
and <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px2.p1.1.4">HT(COMETk.)</span>.
Additionally, CoDec is integrated into the Hybrid Threshold pipeline as a comparative system, referred to as <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px2.p1.1.5">HT(COMETk.) w/ CoDec</span>.
Furthermore, we follow <cite class="ltx_cite ltx_citemacro_citet">Hendy et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib14" title="">2023</a>)</cite> to use Hybrid Max-Routing to establish an upper bound by selecting the best translation from either system based on the COMETkiwi.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T5.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.2.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T5.2.2.3" rowspan="2" style="padding-left:1.4pt;padding-right:1.4pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.2.2.3.1">Model</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S5.T5.1.1.1" style="padding-left:1.4pt;padding-right:1.4pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1">ZH<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S5.T5.1.1.1.1.m1.1"><semantics id="S5.T5.1.1.1.1.m1.1a"><mo id="S5.T5.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T5.1.1.1.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.1.m1.1b"><ci id="S5.T5.1.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.1.1.1.1.m1.1d">‚áí</annotation></semantics></math>EN</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S5.T5.2.2.2" style="padding-left:1.4pt;padding-right:1.4pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.2.2.2.1">EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S5.T5.2.2.2.1.m1.1"><semantics id="S5.T5.2.2.2.1.m1.1a"><mo id="S5.T5.2.2.2.1.m1.1.1" stretchy="false" xref="S5.T5.2.2.2.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.2.1.m1.1b"><ci id="S5.T5.2.2.2.1.m1.1.1.cmml" xref="S5.T5.2.2.2.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.2.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.2.2.2.1.m1.1d">‚áí</annotation></semantics></math>ZH</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.3.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T5.2.3.1.1" style="padding-left:1.4pt;padding-right:1.4pt;">COMET/COMETk.</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T5.2.3.1.2" style="padding-left:1.4pt;padding-right:1.4pt;">COMET/COMETk.</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.4.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.2.4.2.1" style="padding-left:1.4pt;padding-right:1.4pt;">GoogleMT</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T5.2.4.2.2" style="padding-left:1.4pt;padding-right:1.4pt;">76.8/72.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T5.2.4.2.3" style="padding-left:1.4pt;padding-right:1.4pt;">81.9/74.5</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.5.3">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T5.2.5.3.1" style="padding-left:1.4pt;padding-right:1.4pt;">TIM</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T5.2.5.3.2" style="padding-left:1.4pt;padding-right:1.4pt;">75.6/72.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T5.2.5.3.3" style="padding-left:1.4pt;padding-right:1.4pt;">83.0/76.0</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.6.4">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.2.6.4.1" style="padding-left:1.4pt;padding-right:1.4pt;">HT(Random)</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T5.2.6.4.2" style="padding-left:1.4pt;padding-right:1.4pt;">76.2/72.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T5.2.6.4.3" style="padding-left:1.4pt;padding-right:1.4pt;">82.4/75.2</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.7.5">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T5.2.7.5.1" style="padding-left:1.4pt;padding-right:1.4pt;">HT(BLEURT-12)</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T5.2.7.5.2" style="padding-left:1.4pt;padding-right:1.4pt;">76.3/72.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T5.2.7.5.3" style="padding-left:1.4pt;padding-right:1.4pt;">82.6/75.1</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.8.6">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T5.2.8.6.1" style="padding-left:1.4pt;padding-right:1.4pt;">HT(BLEURT-20)</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T5.2.8.6.2" style="padding-left:1.4pt;padding-right:1.4pt;">76.3/72.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T5.2.8.6.3" style="padding-left:1.4pt;padding-right:1.4pt;">82.7/75.2</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.9.7">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T5.2.9.7.1" style="padding-left:1.4pt;padding-right:1.4pt;">HT(COMETk.)</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T5.2.9.7.2" style="padding-left:1.4pt;padding-right:1.4pt;">76.5/73.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T5.2.9.7.3" style="padding-left:1.4pt;padding-right:1.4pt;">83.3/<span class="ltx_text ltx_font_bold" id="S5.T5.2.9.7.3.1">76.2</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.10.8">
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T5.2.10.8.1" style="padding-left:1.4pt;padding-right:1.4pt;">‚ÄÜ¬†¬†¬†¬†w/ CoDec</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T5.2.10.8.2" style="padding-left:1.4pt;padding-right:1.4pt;">
<span class="ltx_text ltx_font_bold" id="S5.T5.2.10.8.2.1">77.1</span>/<span class="ltx_text ltx_font_bold" id="S5.T5.2.10.8.2.2">73.3</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T5.2.10.8.3" style="padding-left:1.4pt;padding-right:1.4pt;">
<span class="ltx_text ltx_font_bold" id="S5.T5.2.10.8.3.1">83.4</span>/<span class="ltx_text ltx_font_bold" id="S5.T5.2.10.8.3.2">76.2</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.11.9">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.2.11.9.1" style="padding-left:1.4pt;padding-right:1.4pt;">CoDec-8</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T5.2.11.9.2" style="padding-left:1.4pt;padding-right:1.4pt;">
<span class="ltx_text ltx_font_bold" id="S5.T5.2.11.9.2.1">77.1</span>/73.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T5.2.11.9.3" style="padding-left:1.4pt;padding-right:1.4pt;">
<span class="ltx_text ltx_font_bold" id="S5.T5.2.11.9.3.1">83.4</span>/76.1</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.12.10">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T5.2.12.10.1" style="padding-left:1.4pt;padding-right:1.4pt;">Max-Routing</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S5.T5.2.12.10.2" style="padding-left:1.4pt;padding-right:1.4pt;">77.4/74.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S5.T5.2.12.10.3" style="padding-left:1.4pt;padding-right:1.4pt;">84.0/76.5</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>
<span class="ltx_text ltx_font_bold" id="S5.T5.4.1">Comparison among CoDec-8 and Hybrid Threshold with different QE methods</span>.
Different QE methods in Hybrid Threshold (HT) show varying performances, whereas CoDec surpasses most HT models. Our CoDec achieves a better balance between efficiency and effectiveness.

</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.SSS0.Px2.p2">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p2.1">The performance comparison in Table <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S5.T5" title="Table 5 ‚Ä£ CoDec vs. Hybrid Threshold. ‚Ä£ 5.1 Main Results ‚Ä£ 5 Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">5</span></a> reveals a notable performance disparity between <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px2.p2.1.1">GoogleMT</span> and <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px2.p2.1.2">Max-Routing</span>.
This result supports our assertion that MT-oriented LLMs can play a crucial role as reliable fallback systems for NMT systems.
Moreover, the different QE modules employed in Hybrid Threshold yield varying performances, highlighting the dependence of Hybrid Threshold‚Äôs performance on the precision of the QE modules and the quality of LLM translations used as replacements.
In contrast, <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px2.p2.1.3">CoDec-8</span> surpasses most of the Hybrid Threshold models and achieves competitive results with <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px2.p2.1.4">HT(COMETk.) w/ CoDec</span>, suggesting that the QE modules may not be necessary.
The findings validate that our approach achieves a better balance between efficiency and effectiveness, resulting in enhanced translation quality without compromising system efficiency.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Human Evaluation</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">In addition, we carry out a human evaluation on the WebCrawl EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><mo id="S5.SS2.p1.1.m1.1.1" stretchy="false" xref="S5.SS2.p1.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><ci id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">‚áí</annotation></semantics></math>ZH dataset.
A total of 300 sentences are randomly selected from the test set, and two individuals are asked to evaluate the translations produced by GoogleMT, HT(COMETk.), and our CoDec-8.
We use the commonly used pairwise comparison method to count the number of better, similar, and worse translations from System 1 rather than System 2.
The result of CoDec vs. GoogleMT is 144:115:41, while the result of CoDec vs. HT(COMETk.) is 106:130:64.
It shows that our CoDec significantly outperforms the commercial NMT system and performs better than the Hybrid Threshold without an additional quality evaluation module.
</p>
</div>
<figure class="ltx_table" id="S5.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T6.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.2.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T6.2.2.3" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.2.2.3.1">Model</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T6.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.1.1">DE<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S5.T6.1.1.1.1.m1.1"><semantics id="S5.T6.1.1.1.1.m1.1a"><mo id="S5.T6.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T6.1.1.1.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S5.T6.1.1.1.1.m1.1b"><ci id="S5.T6.1.1.1.1.m1.1.1.cmml" xref="S5.T6.1.1.1.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.1.1.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T6.1.1.1.1.m1.1d">‚áí</annotation></semantics></math>EN</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T6.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.2.2.2.1">ZH<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S5.T6.2.2.2.1.m1.1"><semantics id="S5.T6.2.2.2.1.m1.1a"><mo id="S5.T6.2.2.2.1.m1.1.1" stretchy="false" xref="S5.T6.2.2.2.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="S5.T6.2.2.2.1.m1.1b"><ci id="S5.T6.2.2.2.1.m1.1.1.cmml" xref="S5.T6.2.2.2.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.2.2.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T6.2.2.2.1.m1.1d">‚áí</annotation></semantics></math>EN</span></th>
</tr>
<tr class="ltx_tr" id="S5.T6.3.4.1">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="S5.T6.3.4.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">COMET/ChrF</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="S5.T6.3.4.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">Suc.</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="S5.T6.3.4.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">COMET/ChrF</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="S5.T6.3.4.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">Suc.</th>
</tr>
<tr class="ltx_tr" id="S5.T6.3.5.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T6.3.5.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">Lingua Custodia</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.3.5.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">73.5/61.8</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.3.5.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">62.2</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.3.5.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">60.9/32.6</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.3.5.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">74.7</th>
</tr>
<tr class="ltx_tr" id="S5.T6.3.3">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S5.T6.3.3.1" style="padding-left:2.0pt;padding-right:2.0pt;"><math alttext="\text{UEDIN}_{\rm LLM}" class="ltx_Math" display="inline" id="S5.T6.3.3.1.m1.1"><semantics id="S5.T6.3.3.1.m1.1a"><msub id="S5.T6.3.3.1.m1.1.1" xref="S5.T6.3.3.1.m1.1.1.cmml"><mtext id="S5.T6.3.3.1.m1.1.1.2" xref="S5.T6.3.3.1.m1.1.1.2a.cmml">UEDIN</mtext><mi id="S5.T6.3.3.1.m1.1.1.3" xref="S5.T6.3.3.1.m1.1.1.3.cmml">LLM</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T6.3.3.1.m1.1b"><apply id="S5.T6.3.3.1.m1.1.1.cmml" xref="S5.T6.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T6.3.3.1.m1.1.1.1.cmml" xref="S5.T6.3.3.1.m1.1.1">subscript</csymbol><ci id="S5.T6.3.3.1.m1.1.1.2a.cmml" xref="S5.T6.3.3.1.m1.1.1.2"><mtext id="S5.T6.3.3.1.m1.1.1.2.cmml" xref="S5.T6.3.3.1.m1.1.1.2">UEDIN</mtext></ci><ci id="S5.T6.3.3.1.m1.1.1.3.cmml" xref="S5.T6.3.3.1.m1.1.1.3">LLM</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.3.3.1.m1.1c">\text{UEDIN}_{\rm LLM}</annotation><annotation encoding="application/x-llamapun" id="S5.T6.3.3.1.m1.1d">UEDIN start_POSTSUBSCRIPT roman_LLM end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="S5.T6.3.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">81.3/60.0</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="S5.T6.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">58.8</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="S5.T6.3.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_font_bold" id="S5.T6.3.3.4.1">75.7</span>/<span class="ltx_text ltx_font_bold" id="S5.T6.3.3.4.2">41.2</span>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column" id="S5.T6.3.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">75.3</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.3.6.1">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T6.3.6.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">GoogleMT</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T6.3.6.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">80.3/54.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T6.3.6.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">55.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T6.3.6.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">75.3/41.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T6.3.6.1.5" style="padding-left:2.0pt;padding-right:2.0pt;">67.1</td>
</tr>
<tr class="ltx_tr" id="S5.T6.3.7.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T6.3.7.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">TIM w/o term</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T6.3.7.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">79.6/54.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T6.3.7.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">54.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T6.3.7.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">73.8/38.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T6.3.7.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">58.6</td>
</tr>
<tr class="ltx_tr" id="S5.T6.3.8.3">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S5.T6.3.8.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">TIM w/ term</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T6.3.8.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text ltx_font_bold" id="S5.T6.3.8.3.2.1">82.3</span>/65.2</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T6.3.8.3.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.3.8.3.3.1">82.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T6.3.8.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">73.4/39.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T6.3.8.3.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.3.8.3.5.1">85.0</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.3.9.4">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T6.3.9.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">CoDec-8</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S5.T6.3.9.4.2" style="padding-left:2.0pt;padding-right:2.0pt;">80.7/56.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S5.T6.3.9.4.3" style="padding-left:2.0pt;padding-right:2.0pt;">59.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S5.T6.3.9.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">75.3/41.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S5.T6.3.9.4.5" style="padding-left:2.0pt;padding-right:2.0pt;">76.4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>
<span class="ltx_text ltx_font_bold" id="S5.T6.5.1">Performance on WMT23 terminology translation</span>.
‚ÄúSuc.‚Äù denotes Terminology Success Rate.
Our CoDec combines NMT‚Äôs superior translation quality with the constrained translation capabilities of MT-oriented LLMs.

</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Terminology Translation</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Unlike conventional NMT models, MT-oriented LLMs enable them to exploit instructions to handle various translation scenarios.
Here, we apply CoDec to assess the effectiveness of incorporating instructions in a dedicated terminology translation test set obtained from WMT23<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>https://wmt-terminology-task.github.io/</span></span></span>.
The result is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#S5.T6" title="Table 6 ‚Ä£ 5.2 Human Evaluation ‚Ä£ 5 Experiments ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">6</span></a>, evaluated by COMET, ChrF, and Terminology Success Rate.
The data statistics and details of baselines can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#A4" title="Appendix D Terminology Translation ‚Ä£ Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">The results indicate that the use of terminology information in instructions, as demonstrated by <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.2">TIM w/ term</span>, enables MT-oriented LLMs to achieve constrained machine translation, resulting in more accurate domain-specific terminology in the translated output.
When compared to <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.3">Lingua Custodia</span> and <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.1">UEDIN<sub class="ltx_sub" id="S5.SS3.p2.1.1.1"><span class="ltx_text ltx_font_upright" id="S5.SS3.p2.1.1.1.1">LLM</span></sub></span> <cite class="ltx_cite ltx_citemacro_cite">Semenov et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib33" title="">2023</a>)</cite>, <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.4">CoDec-8</span> combines the advantages of higher translation quality offered by NMT and the constrained translation capabilities of MT-oriented LLMs.
This combination leads to higher-quality translations while maintaining a higher Terminology Success Rate.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We explore the strengths of both NMT and LLM and propose CoDec that integrates the two to achieve superior performance compared to existing hybrid frameworks.
Notably, our CoDec offers reduced decoding latency compared to relying solely on LLMs for inference, and it does not require any modifications to the target LLMs.
We believe that exploring more effective utilization of LLMs while considering practicality in both industry and academia is a valuable direction.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitations</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This paper primarily concentrates on enhancing translation performance for medium and high-resource language pairs. Further investigation is required to analyze the translation characteristics of different systems in low-resource languages, which we defer to future research.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">Additionally, the draft translations were validated by directly utilizing the top-<math alttext="k" class="ltx_Math" display="inline" id="S7.p2.1.m1.1"><semantics id="S7.p2.1.m1.1a"><mi id="S7.p2.1.m1.1.1" xref="S7.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S7.p2.1.m1.1b"><ci id="S7.p2.1.m1.1.1.cmml" xref="S7.p2.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S7.p2.1.m1.1d">italic_k</annotation></semantics></math> candidates predicted by the target MT-oriented LLM. We acknowledge that the implementation of more meticulously designed token-level validation methods has the potential to further enhance CoDec, and we consider it as an avenue for future exploration.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et¬†al. (2022)</span>
<span class="ltx_bibblock">
Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2212.02437" title="">In-context examples selection for machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">CoRR</em>, abs/2212.02437.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bird et¬†al. (2009)</span>
<span class="ltx_bibblock">
Steven Bird, Ewan Klein, and Edward Loper. 2009.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://www.oreilly.de/catalog/9780596516499/index.html" title=""><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1.1">Natural Language Processing with Python</em></a>.

</span>
<span class="ltx_bibblock">O‚ÄôReilly.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et¬†al. (2020)</span>
<span class="ltx_bibblock">
Tom¬†B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel¬†M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burton (1985)</span>
<span class="ltx_bibblock">
F.¬†Warren Burton. 1985.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TC.1985.6312218" title="">Speculative computation, parallelism, and functional programming</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">IEEE Trans. Computers</em>, 34(12):1190‚Äì1193.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et¬†al. (2023a)</span>
<span class="ltx_bibblock">
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2302.01318" title="">Accelerating large language model decoding with speculative sampling</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">CoRR</em>, abs/2302.01318.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et¬†al. (2023b)</span>
<span class="ltx_bibblock">
Liang Chen, Shuming Ma, Dongdong Zhang, Furu Wei, and Baobao Chang. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-acl.608" title="">On the off-target problem of zero-shot multilingual neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 9542‚Äì9558, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dou and Neubig (2021)</span>
<span class="ltx_bibblock">
Zi-Yi Dou and Graham Neubig. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.eacl-main.181" title="">Word alignment by fine-tuning embeddings on parallel corpora</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</em>, pages 2112‚Äì2128, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et¬†al. (2021)</span>
<span class="ltx_bibblock">
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Michael Auli, and Armand Joulin. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://jmlr.org/papers/v22/20-1307.html" title="">Beyond english-centric multilingual machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">J. Mach. Learn. Res.</em>, 22:107:1‚Äì107:48.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et¬†al. (2020)</span>
<span class="ltx_bibblock">
Angela Fan, Edouard Grave, and Armand Joulin. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=SylO2yStDr" title="">Reducing transformer depth on demand with structured dropout</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag and Firat (2020)</span>
<span class="ltx_bibblock">
Markus Freitag and Orhan Firat. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.wmt-1.66" title="">Complete multilingual neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the Fifth Conference on Machine Translation</em>, pages 550‚Äì560, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag et¬†al. (2020)</span>
<span class="ltx_bibblock">
Markus Freitag, David Grangier, and Isaac Caswell. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.5" title="">BLEU might be guilty but references are not innocent</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 61‚Äì71, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag et¬†al. (2021)</span>
<span class="ltx_bibblock">
Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ond≈ôej Bojar. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.wmt-1.73" title="">Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the Sixth Conference on Machine Translation</em>, pages 733‚Äì774, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garcia et¬†al. (2023)</span>
<span class="ltx_bibblock">
Xavier Garcia, Yamini Bansal, Colin Cherry, George¬†F. Foster, Maxim Krikun, Fangxiaoyu Feng, Melvin Johnson, and Orhan Firat. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2302.01398" title="">The unreasonable effectiveness of few-shot learning for machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">CoRR</em>, abs/2302.01398.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendy et¬†al. (2023)</span>
<span class="ltx_bibblock">
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young¬†Jin Kim, Mohamed Afify, and Hany¬†Hassan Awadalla. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2302.09210" title="">How good are GPT models at machine translation? A comprehensive evaluation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">CoRR</em>, abs/2302.09210.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hennessy and Patterson (2012)</span>
<span class="ltx_bibblock">
John¬†L. Hennessy and David¬†A. Patterson. 2012.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Computer Architecture - A Quantitative Approach, 5th Edition</em>.

</span>
<span class="ltx_bibblock">Morgan Kaufmann.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton et¬†al. (2015)</span>
<span class="ltx_bibblock">
Geoffrey¬†E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1503.02531" title="">Distilling the knowledge in a neural network</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">CoRR</em>, abs/1503.02531.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiao et¬†al. (2023)</span>
<span class="ltx_bibblock">
Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang, Xing Wang, Shuming Shi, and Zhaopeng Tu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2304.02426" title="">Parrot: Translating during chat using large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">CoRR</em>, abs/2304.02426.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiao et¬†al. (2020)</span>
<span class="ltx_bibblock">
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.findings-emnlp.372" title="">TinyBERT: Distilling BERT for natural language understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Findings of the Association for Computational Linguistics: EMNLP 2020</em>, pages 4163‚Äì4174, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim and Cho (2021)</span>
<span class="ltx_bibblock">
Gyuwan Kim and Kyunghyun Cho. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.508" title="">Length-adaptive transformer: Train once with length drop, use anytime with search</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021</em>, pages 6501‚Äì6511. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei (2021)</span>
<span class="ltx_bibblock">
Tao Lei. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.emnlp-main.602" title="">When attention meets fast recurrence: Training language models with reduced compute</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 7633‚Äì7648, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leviathan et¬†al. (2023)</span>
<span class="ltx_bibblock">
Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v202/leviathan23a.html" title="">Fast inference from transformers via speculative decoding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA</em>, volume 202 of <em class="ltx_emph ltx_font_italic" id="bib.bib21.2.2">Proceedings of Machine Learning Research</em>, pages 19274‚Äì19286. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et¬†al. (2022)</span>
<span class="ltx_bibblock">
Xi¬†Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit¬†Singh Koura, Vishrav Chaudhary, Brian O‚ÄôHoro, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona¬†T. Diab, Veselin Stoyanov, and Xian Li. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.emnlp-main.616" title="">Few-shot learning with multilingual generative language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">EMNLP 2022</em>, pages 9019‚Äì9052.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et¬†al. (2023)</span>
<span class="ltx_bibblock">
Hongyuan Lu, Haoyang Huang, Dongdong Zhang, Haoran Yang, Wai Lam, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2305.06575" title="">Chain-of-dictionary prompting elicits translation in large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">CoRR</em>, abs/2305.06575.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et¬†al. (2019)</span>
<span class="ltx_bibblock">
Qingsong Ma, Johnny Wei, Ond≈ôej Bojar, and Yvette Graham. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W19-5302" title="">Results of the WMT19 metrics shared task: Segment-level and strong MT systems pose big challenges</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)</em>, pages 62‚Äì90, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2303.08774" title="">GPT-4 technical report</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">CoRR</em>, abs/2303.08774.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et¬†al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/1073083.1073135" title="">Bleu: a method for automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>, pages 311‚Äì318. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popovic (2015)</span>
<span class="ltx_bibblock">
Maja Popovic. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/w15-3049" title="">chrf: character n-gram f-score for automatic MT evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the Tenth Workshop on Statistical Machine Translation, WMT@EMNLP 2015, 17-18 September 2015, Lisbon, Portugal</em>, pages 392‚Äì395. The Association for Computer Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raunak et¬†al. (2023)</span>
<span class="ltx_bibblock">
Vikas Raunak, Arul Menezes, Matt Post, and Hany Hassan. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-short.90" title="">Do GPTs produce less literal translations?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, pages 1041‚Äì1050, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et¬†al. (2022a)</span>
<span class="ltx_bibblock">
Ricardo Rei, Jos√©¬†G. C.¬†de Souza, Duarte Alves, Chrysoula Zerva, Ana¬†C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and Andr√© F.¬†T. Martins. 2022a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.52" title="">COMET-22: Unbabel-IST 2022 submission for the metrics shared task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</em>, pages 578‚Äì585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et¬†al. (2022b)</span>
<span class="ltx_bibblock">
Ricardo Rei, Jos√© G.¬†C. de¬†Souza, Duarte¬†M. Alves, Chrysoula Zerva, Ana¬†C. Farinha, Taisiya Glushkova, Alon Lavie, Lu√≠sa Coheur, and Andr√© F.¬†T. Martins. 2022b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.52" title="">COMET-22: unbabel-ist 2022 submission for the metrics shared task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the Seventh Conference on Machine Translation, WMT 2022, Abu Dhabi, United Arab Emirates (Hybrid), December 7-8, 2022</em>, pages 578‚Äì585. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et¬†al. (2021)</span>
<span class="ltx_bibblock">
Ricardo Rei, Ana¬†C. Farinha, Chrysoula Zerva, Daan van Stigt, Craig Stewart, Pedro¬†G. Ramos, Taisiya Glushkova, Andr√© F.¬†T. Martins, and Alon Lavie. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.wmt-1.111" title="">Are references really needed? unbabel-ist 2021 submission for the metrics shared task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the Sixth Conference on Machine Translation, WMT@EMNLP 2021, Online Event, November 10-11, 2021</em>, pages 1030‚Äì1040. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Santilli et¬†al. (2023)</span>
<span class="ltx_bibblock">
Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodola. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.689" title="">Accelerating transformer inference for translation via parallel decoding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 12336‚Äì12355, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Semenov et¬†al. (2023)</span>
<span class="ltx_bibblock">
Kirill Semenov, Vil√©m Zouhar, Tom Kocmi, Dongdong Zhang, Wangchunshu Zhou, and Yuchen¬†Eleanor Jiang. 2023.

</span>
<span class="ltx_bibblock">Findings of the wmt 2023 shared task on machine translation with terminologies.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the Eight Conference on Machine Translation (WMT)</em>. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et¬†al. (2020)</span>
<span class="ltx_bibblock">
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael¬†W. Mahoney, and Kurt Keutzer. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1609/aaai.v34i05.6409" title="">Q-BERT: hessian based ultra low precision quantization of BERT</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020</em>, pages 8815‚Äì8821. AAAI Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stern et¬†al. (2018)</span>
<span class="ltx_bibblock">
Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2018/hash/c4127b9194fe8562c64dc0f5bf2c93bc-Abstract.html" title="">Blockwise parallel decoding for deep autoregressive models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr√©al, Canada</em>, pages 10107‚Äì10116.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et¬†al. (2020)</span>
<span class="ltx_bibblock">
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.195" title="">MobileBERT: a compact task-agnostic BERT for resource-limited devices</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 2158‚Äì2170, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et¬†al. (2023)</span>
<span class="ltx_bibblock">
Yi¬†Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3530811" title="">Efficient transformers: A survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">ACM Comput. Surv.</em>, 55(6):109:1‚Äì109:28.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et¬†al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan¬†N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://papers.nips.cc/paper/7181-attention-is-all-you-need" title="">Attention is all you need</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of NeurIPS 2017</em>, pages 5998‚Äì6008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et¬†al. (2020)</span>
<span class="ltx_bibblock">
Wenhui Wang, Furu Wei, Li¬†Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2020/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" title="">Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et¬†al. (2021)</span>
<span class="ltx_bibblock">
Jingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou, and Lei Li. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2111.05193" title="">A survey on green deep learning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">CoRR</em>, abs/2111.05193.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et¬†al. (2023)</span>
<span class="ltx_bibblock">
Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2304.04487" title="">Inference with reference: Lossless acceleration of large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">CoRR</em>, abs/2304.04487.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et¬†al. (2023)</span>
<span class="ltx_bibblock">
Jiali Zeng, Fandong Meng, Yongjing Yin, and Jie Zhou. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2307.04408" title="">TIM: teaching large language models to translate with comparison</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">CoRR</em>, abs/2307.04408.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et¬†al. (2020)</span>
<span class="ltx_bibblock">
Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.148" title="">Improving massively multilingual neural machine translation and zero-shot translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 1628‚Äì1639, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et¬†al. (2023)</span>
<span class="ltx_bibblock">
Shaolei Zhang, Qingkai Fang, Zhuocheng Zhang, Zhengrui Ma, Yan Zhou, Langlin Huang, Mengyu Bu, Shangtong Gui, Yunji Chen, Xilin Chen, and Yang Feng. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2306.10968" title="">Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">CoRR</em>, abs/2306.10968.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et¬†al. (2023)</span>
<span class="ltx_bibblock">
Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian Huang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2304.04675" title="">Multilingual machine translation with large language models: Empirical results and analysis</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">CoRR</em>, abs/2304.04675.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>WMT22 test sets</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.8">To prevent data leakage <cite class="ltx_cite ltx_citemacro_citep">(Garcia et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib13" title="">2023</a>)</cite>, we analyze the WMT22 test sets, consisting of recent content from diverse domains including news, social media, e-commerce, and conversation.
The test sets consist of the following number of samples for each language pair:
German-to-English (DE<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="A1.p1.1.m1.1"><semantics id="A1.p1.1.m1.1a"><mo id="A1.p1.1.m1.1.1" stretchy="false" xref="A1.p1.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><ci id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="A1.p1.1.m1.1d">‚áí</annotation></semantics></math>DE) - 1984 samples, English-to-German (EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="A1.p1.2.m2.1"><semantics id="A1.p1.2.m2.1a"><mo id="A1.p1.2.m2.1.1" stretchy="false" xref="A1.p1.2.m2.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="A1.p1.2.m2.1b"><ci id="A1.p1.2.m2.1.1.cmml" xref="A1.p1.2.m2.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.2.m2.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="A1.p1.2.m2.1d">‚áí</annotation></semantics></math>DE) - 2037 samples, Chinese-to-English (ZH<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="A1.p1.3.m3.1"><semantics id="A1.p1.3.m3.1a"><mo id="A1.p1.3.m3.1.1" stretchy="false" xref="A1.p1.3.m3.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="A1.p1.3.m3.1b"><ci id="A1.p1.3.m3.1.1.cmml" xref="A1.p1.3.m3.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.3.m3.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="A1.p1.3.m3.1d">‚áí</annotation></semantics></math>EN) - 1875 samples, English-to-Chinese (EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="A1.p1.4.m4.1"><semantics id="A1.p1.4.m4.1a"><mo id="A1.p1.4.m4.1.1" stretchy="false" xref="A1.p1.4.m4.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="A1.p1.4.m4.1b"><ci id="A1.p1.4.m4.1.1.cmml" xref="A1.p1.4.m4.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.4.m4.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="A1.p1.4.m4.1d">‚áí</annotation></semantics></math>ZH) - 2037 samples, Russian-to-English (RU<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="A1.p1.5.m5.1"><semantics id="A1.p1.5.m5.1a"><mo id="A1.p1.5.m5.1.1" stretchy="false" xref="A1.p1.5.m5.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="A1.p1.5.m5.1b"><ci id="A1.p1.5.m5.1.1.cmml" xref="A1.p1.5.m5.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.5.m5.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="A1.p1.5.m5.1d">‚áí</annotation></semantics></math>EN) - 2016 samples, English-to-Russian (EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="A1.p1.6.m6.1"><semantics id="A1.p1.6.m6.1a"><mo id="A1.p1.6.m6.1.1" stretchy="false" xref="A1.p1.6.m6.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="A1.p1.6.m6.1b"><ci id="A1.p1.6.m6.1.1.cmml" xref="A1.p1.6.m6.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.6.m6.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="A1.p1.6.m6.1d">‚áí</annotation></semantics></math>RU) - 2037 samples, Japanese-to-English (JA<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="A1.p1.7.m7.1"><semantics id="A1.p1.7.m7.1a"><mo id="A1.p1.7.m7.1.1" stretchy="false" xref="A1.p1.7.m7.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="A1.p1.7.m7.1b"><ci id="A1.p1.7.m7.1.1.cmml" xref="A1.p1.7.m7.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.7.m7.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="A1.p1.7.m7.1d">‚áí</annotation></semantics></math>EN) - 2008 samples, English-to-Japanese (EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="A1.p1.8.m8.1"><semantics id="A1.p1.8.m8.1a"><mo id="A1.p1.8.m8.1.1" stretchy="false" xref="A1.p1.8.m8.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="A1.p1.8.m8.1b"><ci id="A1.p1.8.m8.1.1.cmml" xref="A1.p1.8.m8.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.8.m8.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="A1.p1.8.m8.1d">‚áí</annotation></semantics></math>JA) - 2037 samples.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Unaligned Source/Target Words.</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">For English and German, we utilize the Moses tokenizer<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>https://github.com/moses-smt/mosesdecoder/tree/master/scripts/tokenizer</span></span></span>.
We use jieba<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span>https://github.com/fxsjy/jieba</span></span></span> and MeCab<span class="ltx_note ltx_role_footnote" id="footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span>https://github.com/SamuraiT/mecab-python3</span></span></span> for Chinese and Japanese, respectively.
We use <span class="ltx_text ltx_font_italic" id="A2.p1.1.1">awesome-align<span class="ltx_note ltx_role_footnote" id="footnote17"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright" id="footnote17.1.1.1">17</span></span><span class="ltx_text ltx_font_upright" id="footnote17.5">https://github.com/neulab/awesome-align</span></span></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Dou and Neubig, <a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib7" title="">2021</a>)</cite> to obtain the word alignments.
Unaligned source words (USW) indicate the number of words in the source text that have no corresponding translation in the target sentence.
Unaligned target words (UTW) assess the degree to which words are potentially added or inserted into the translation without any basis or support from the source sentence.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>WebCrawl test sets</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">To acquire the data, we follow the process outlined below:</p>
<ul class="ltx_itemize" id="A3.I1">
<li class="ltx_item" id="A3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A3.I1.i1.p1">
<p class="ltx_p" id="A3.I1.i1.p1.1">We extract snippets from web pages and use an in-house sentence segmentation tool to split them into individual sentences.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A3.I1.i2.p1">
<p class="ltx_p" id="A3.I1.i2.p1.1">We employ sensitive word filters, language identification tools, length ratio checks, and perplexity scores to filter out sentences of lower quality.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A3.I1.i3.p1">
<p class="ltx_p" id="A3.I1.i3.p1.1">We utilize Google Translator to obtain translations of the sentences, with a primary focus on the Chinese<math alttext="\Leftrightarrow" class="ltx_Math" display="inline" id="A3.I1.i3.p1.1.m1.1"><semantics id="A3.I1.i3.p1.1.m1.1a"><mo id="A3.I1.i3.p1.1.m1.1.1" stretchy="false" xref="A3.I1.i3.p1.1.m1.1.1.cmml">‚áî</mo><annotation-xml encoding="MathML-Content" id="A3.I1.i3.p1.1.m1.1b"><ci id="A3.I1.i3.p1.1.m1.1.1.cmml" xref="A3.I1.i3.p1.1.m1.1.1">‚áî</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.I1.i3.p1.1.m1.1c">\Leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="A3.I1.i3.p1.1.m1.1d">‚áî</annotation></semantics></math>English directions.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A3.I1.i4.p1">
<p class="ltx_p" id="A3.I1.i4.p1.1">We calculate COMETkiwi scores and retain sentences with scores below 65.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A3.p2">
<p class="ltx_p" id="A3.p2.1">In this way, we collected a total of 889 Chinese sentences and 1195 English sentences as our final test set, named <span class="ltx_text ltx_font_italic" id="A3.p2.1.1">WebCrawl test sets</span>.
We hire 2 annotators who have degrees in English Linguistics to annotate translations.
Before formal annotation, annotators were asked to annotate 100 samples randomly extracted from the dataset, and based on average annotation time we set a fair salary (i.e., 30 dollars per hour) for them.</p>
</div>
<figure class="ltx_table" id="A3.T7">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.T7.8">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T7.8.9.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A3.T7.8.9.1.1" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.9.1.1.1">System</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T7.8.9.1.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.9.1.2.1">ChrF</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T7.8.9.1.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.9.1.3.1">SacreBLEU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T7.8.9.1.4" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.9.1.4.1">ChrF</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A3.T7.8.9.1.5" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.9.1.5.1">SacreBLEU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T7.8.9.1.6" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.9.1.6.1">ChrF</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T7.8.9.1.7" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.9.1.7.1">SacreBLEU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T7.8.9.1.8" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.9.1.8.1">ChrF</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T7.8.9.1.9" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.9.1.9.1">SacreBLEU</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T7.4.4">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A3.T7.4.4.5" style="padding-left:5.7pt;padding-right:5.7pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="A3.T7.1.1.1" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_italic" id="A3.T7.1.1.1.1">DE<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="A3.T7.1.1.1.1.m1.1"><semantics id="A3.T7.1.1.1.1.m1.1a"><mo id="A3.T7.1.1.1.1.m1.1.1" stretchy="false" xref="A3.T7.1.1.1.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="A3.T7.1.1.1.1.m1.1b"><ci id="A3.T7.1.1.1.1.m1.1.1.cmml" xref="A3.T7.1.1.1.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T7.1.1.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T7.1.1.1.1.m1.1d">‚áí</annotation></semantics></math>EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="A3.T7.2.2.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_italic" id="A3.T7.2.2.2.1">EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="A3.T7.2.2.2.1.m1.1"><semantics id="A3.T7.2.2.2.1.m1.1a"><mo id="A3.T7.2.2.2.1.m1.1.1" stretchy="false" xref="A3.T7.2.2.2.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="A3.T7.2.2.2.1.m1.1b"><ci id="A3.T7.2.2.2.1.m1.1.1.cmml" xref="A3.T7.2.2.2.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T7.2.2.2.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T7.2.2.2.1.m1.1d">‚áí</annotation></semantics></math>DE</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="A3.T7.3.3.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_italic" id="A3.T7.3.3.3.1">ZH<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="A3.T7.3.3.3.1.m1.1"><semantics id="A3.T7.3.3.3.1.m1.1a"><mo id="A3.T7.3.3.3.1.m1.1.1" stretchy="false" xref="A3.T7.3.3.3.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="A3.T7.3.3.3.1.m1.1b"><ci id="A3.T7.3.3.3.1.m1.1.1.cmml" xref="A3.T7.3.3.3.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T7.3.3.3.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T7.3.3.3.1.m1.1d">‚áí</annotation></semantics></math>EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="A3.T7.4.4.4" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_italic" id="A3.T7.4.4.4.1">EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="A3.T7.4.4.4.1.m1.1"><semantics id="A3.T7.4.4.4.1.m1.1a"><mo id="A3.T7.4.4.4.1.m1.1.1" stretchy="false" xref="A3.T7.4.4.4.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="A3.T7.4.4.4.1.m1.1b"><ci id="A3.T7.4.4.4.1.m1.1.1.cmml" xref="A3.T7.4.4.4.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T7.4.4.4.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T7.4.4.4.1.m1.1d">‚áí</annotation></semantics></math>ZH</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.8.10.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A3.T7.8.10.1.1" style="padding-left:5.7pt;padding-right:5.7pt;">WMT-Best</th>
<td class="ltx_td ltx_align_center" id="A3.T7.8.10.1.2" style="padding-left:5.7pt;padding-right:5.7pt;">58.5</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.10.1.3" style="padding-left:5.7pt;padding-right:5.7pt;">33.4</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.10.1.4" style="padding-left:5.7pt;padding-right:5.7pt;">64.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T7.8.10.1.5" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.10.1.5.1">38.4</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.10.1.6" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.10.1.6.1">61.1</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.10.1.7" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.10.1.7.1">33.5</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.10.1.8" style="padding-left:5.7pt;padding-right:5.7pt;">41.1</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.10.1.9" style="padding-left:5.7pt;padding-right:5.7pt;">44.8</td>
</tr>
<tr class="ltx_tr" id="A3.T7.8.11.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A3.T7.8.11.2.1" style="padding-left:5.7pt;padding-right:5.7pt;">
<span class="ltx_ERROR undefined" id="A3.T7.8.11.2.1.1">\hdashline</span>[0.5pt/0.5pt]
GoogleMT</th>
<td class="ltx_td ltx_align_center" id="A3.T7.8.11.2.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.11.2.2.1">59.1</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.11.2.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.11.2.3.1">34.1</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.11.2.4" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.11.2.4.1">64.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T7.8.11.2.5" style="padding-left:5.7pt;padding-right:5.7pt;">37.5</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.11.2.6" style="padding-left:5.7pt;padding-right:5.7pt;">60.0</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.11.2.7" style="padding-left:5.7pt;padding-right:5.7pt;">29.4</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.11.2.8" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.11.2.8.1">45.8</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.11.2.9" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.11.2.9.1">50.5</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.8.12.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A3.T7.8.12.3.1" style="padding-left:5.7pt;padding-right:5.7pt;">MicroMT</th>
<td class="ltx_td ltx_align_center" id="A3.T7.8.12.3.2" style="padding-left:5.7pt;padding-right:5.7pt;">58.8</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.12.3.3" style="padding-left:5.7pt;padding-right:5.7pt;">33.9</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.12.3.4" style="padding-left:5.7pt;padding-right:5.7pt;">64.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T7.8.12.3.5" style="padding-left:5.7pt;padding-right:5.7pt;">37.5</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.12.3.6" style="padding-left:5.7pt;padding-right:5.7pt;">60.0</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.12.3.7" style="padding-left:5.7pt;padding-right:5.7pt;">29.4</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.12.3.8" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.12.3.8.1">45.8</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.12.3.9" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.12.3.9.1">50.5</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.8.13.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A3.T7.8.13.4.1" style="padding-left:5.7pt;padding-right:5.7pt;">
<span class="ltx_ERROR undefined" id="A3.T7.8.13.4.1.1">\hdashline</span>[0.5pt/0.5pt]
BayLing-7B</th>
<td class="ltx_td ltx_align_center" id="A3.T7.8.13.4.2" style="padding-left:5.7pt;padding-right:5.7pt;">53.6</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.13.4.3" style="padding-left:5.7pt;padding-right:5.7pt;">28.2</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.13.4.4" style="padding-left:5.7pt;padding-right:5.7pt;">53.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T7.8.13.4.5" style="padding-left:5.7pt;padding-right:5.7pt;">25.7</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.13.4.6" style="padding-left:5.7pt;padding-right:5.7pt;">49.9</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.13.4.7" style="padding-left:5.7pt;padding-right:5.7pt;">20.3</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.13.4.8" style="padding-left:5.7pt;padding-right:5.7pt;">34.5</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.13.4.9" style="padding-left:5.7pt;padding-right:5.7pt;">38.2</td>
</tr>
<tr class="ltx_tr" id="A3.T7.8.14.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A3.T7.8.14.5.1" style="padding-left:5.7pt;padding-right:5.7pt;">TIM-13B</th>
<td class="ltx_td ltx_align_center" id="A3.T7.8.14.5.2" style="padding-left:5.7pt;padding-right:5.7pt;">56.9</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.14.5.3" style="padding-left:5.7pt;padding-right:5.7pt;">31.7</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.14.5.4" style="padding-left:5.7pt;padding-right:5.7pt;">60.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T7.8.14.5.5" style="padding-left:5.7pt;padding-right:5.7pt;">33.2</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.14.5.6" style="padding-left:5.7pt;padding-right:5.7pt;">56.8</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.14.5.7" style="padding-left:5.7pt;padding-right:5.7pt;">26.9</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.14.5.8" style="padding-left:5.7pt;padding-right:5.7pt;">42.4</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.14.5.9" style="padding-left:5.7pt;padding-right:5.7pt;">46.9</td>
</tr>
<tr class="ltx_tr" id="A3.T7.8.8">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A3.T7.8.8.5" style="padding-left:5.7pt;padding-right:5.7pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="A3.T7.5.5.1" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_italic" id="A3.T7.5.5.1.1">RU<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="A3.T7.5.5.1.1.m1.1"><semantics id="A3.T7.5.5.1.1.m1.1a"><mo id="A3.T7.5.5.1.1.m1.1.1" stretchy="false" xref="A3.T7.5.5.1.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="A3.T7.5.5.1.1.m1.1b"><ci id="A3.T7.5.5.1.1.m1.1.1.cmml" xref="A3.T7.5.5.1.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T7.5.5.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T7.5.5.1.1.m1.1d">‚áí</annotation></semantics></math>EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="A3.T7.6.6.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_italic" id="A3.T7.6.6.2.1">EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="A3.T7.6.6.2.1.m1.1"><semantics id="A3.T7.6.6.2.1.m1.1a"><mo id="A3.T7.6.6.2.1.m1.1.1" stretchy="false" xref="A3.T7.6.6.2.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="A3.T7.6.6.2.1.m1.1b"><ci id="A3.T7.6.6.2.1.m1.1.1.cmml" xref="A3.T7.6.6.2.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T7.6.6.2.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T7.6.6.2.1.m1.1d">‚áí</annotation></semantics></math>RU</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="A3.T7.7.7.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_italic" id="A3.T7.7.7.3.1">JA<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="A3.T7.7.7.3.1.m1.1"><semantics id="A3.T7.7.7.3.1.m1.1a"><mo id="A3.T7.7.7.3.1.m1.1.1" stretchy="false" xref="A3.T7.7.7.3.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="A3.T7.7.7.3.1.m1.1b"><ci id="A3.T7.7.7.3.1.m1.1.1.cmml" xref="A3.T7.7.7.3.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T7.7.7.3.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T7.7.7.3.1.m1.1d">‚áí</annotation></semantics></math>EN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="A3.T7.8.8.4" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_italic" id="A3.T7.8.8.4.1">EN<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="A3.T7.8.8.4.1.m1.1"><semantics id="A3.T7.8.8.4.1.m1.1a"><mo id="A3.T7.8.8.4.1.m1.1.1" stretchy="false" xref="A3.T7.8.8.4.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="A3.T7.8.8.4.1.m1.1b"><ci id="A3.T7.8.8.4.1.m1.1.1.cmml" xref="A3.T7.8.8.4.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T7.8.8.4.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="A3.T7.8.8.4.1.m1.1d">‚áí</annotation></semantics></math>JA</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.8.15.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A3.T7.8.15.6.1" style="padding-left:5.7pt;padding-right:5.7pt;">WMT-Best</th>
<td class="ltx_td ltx_align_center" id="A3.T7.8.15.6.2" style="padding-left:5.7pt;padding-right:5.7pt;">68.9</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.15.6.3" style="padding-left:5.7pt;padding-right:5.7pt;">45.1</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.15.6.4" style="padding-left:5.7pt;padding-right:5.7pt;">58.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T7.8.15.6.5" style="padding-left:5.7pt;padding-right:5.7pt;">32.4</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.15.6.6" style="padding-left:5.7pt;padding-right:5.7pt;">49.8</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.15.6.7" style="padding-left:5.7pt;padding-right:5.7pt;">24.8</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.15.6.8" style="padding-left:5.7pt;padding-right:5.7pt;">36.8</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.15.6.9" style="padding-left:5.7pt;padding-right:5.7pt;">27.6</td>
</tr>
<tr class="ltx_tr" id="A3.T7.8.16.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A3.T7.8.16.7.1" style="padding-left:5.7pt;padding-right:5.7pt;">
<span class="ltx_ERROR undefined" id="A3.T7.8.16.7.1.1">\hdashline</span>[0.5pt/0.5pt]
GoogleMT</th>
<td class="ltx_td ltx_align_center" id="A3.T7.8.16.7.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.16.7.2.1">69.1</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.16.7.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.16.7.3.1">45.7</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.16.7.4" style="padding-left:5.7pt;padding-right:5.7pt;">59.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T7.8.16.7.5" style="padding-left:5.7pt;padding-right:5.7pt;">34.3</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.16.7.6" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.16.7.6.1">51.8</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.16.7.7" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.16.7.7.1">26.2</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.16.7.8" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.16.7.8.1">37.6</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.16.7.9" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.16.7.9.1">28.2</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.8.17.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A3.T7.8.17.8.1" style="padding-left:5.7pt;padding-right:5.7pt;">MicroMT</th>
<td class="ltx_td ltx_align_center" id="A3.T7.8.17.8.2" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.17.8.2.1">69.1</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.17.8.3" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.17.8.3.1">45.7</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.17.8.4" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.17.8.4.1">59.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T7.8.17.8.5" style="padding-left:5.7pt;padding-right:5.7pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.8.17.8.5.1">34.9</span></td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.17.8.6" style="padding-left:5.7pt;padding-right:5.7pt;">49.5</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.17.8.7" style="padding-left:5.7pt;padding-right:5.7pt;">24.6</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.17.8.8" style="padding-left:5.7pt;padding-right:5.7pt;">34.8</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.17.8.9" style="padding-left:5.7pt;padding-right:5.7pt;">25.1</td>
</tr>
<tr class="ltx_tr" id="A3.T7.8.18.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A3.T7.8.18.9.1" style="padding-left:5.7pt;padding-right:5.7pt;">
<span class="ltx_ERROR undefined" id="A3.T7.8.18.9.1.1">\hdashline</span>[0.5pt/0.5pt]
BayLing-7B</th>
<td class="ltx_td ltx_align_center" id="A3.T7.8.18.9.2" style="padding-left:5.7pt;padding-right:5.7pt;">60.4</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.18.9.3" style="padding-left:5.7pt;padding-right:5.7pt;">34.7</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.18.9.4" style="padding-left:5.7pt;padding-right:5.7pt;">35.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T7.8.18.9.5" style="padding-left:5.7pt;padding-right:5.7pt;">14.8</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.18.9.6" style="padding-left:5.7pt;padding-right:5.7pt;">34.7</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.18.9.7" style="padding-left:5.7pt;padding-right:5.7pt;">11.6</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.18.9.8" style="padding-left:5.7pt;padding-right:5.7pt;">9.6</td>
<td class="ltx_td ltx_align_center" id="A3.T7.8.18.9.9" style="padding-left:5.7pt;padding-right:5.7pt;">4.5</td>
</tr>
<tr class="ltx_tr" id="A3.T7.8.19.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A3.T7.8.19.10.1" style="padding-left:5.7pt;padding-right:5.7pt;">TIM-13B</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.8.19.10.2" style="padding-left:5.7pt;padding-right:5.7pt;">65.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.8.19.10.3" style="padding-left:5.7pt;padding-right:5.7pt;">40.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.8.19.10.4" style="padding-left:5.7pt;padding-right:5.7pt;">54.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A3.T7.8.19.10.5" style="padding-left:5.7pt;padding-right:5.7pt;">28.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.8.19.10.6" style="padding-left:5.7pt;padding-right:5.7pt;">46.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.8.19.10.7" style="padding-left:5.7pt;padding-right:5.7pt;">21.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.8.19.10.8" style="padding-left:5.7pt;padding-right:5.7pt;">29.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.8.19.10.9" style="padding-left:5.7pt;padding-right:5.7pt;">19.7</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>

Experimental results on the WMT22 test sets.
</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="216" id="A3.F7.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>
<span class="ltx_text ltx_font_bold" id="A3.F7.2.1">Off-target rates (%) of translations.</span> MT-oriented LLMs exhibit a higher prevalence of off-target translations than NMT systems.
</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Terminology Translation</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.5">Terminology translation is an extensively encountered application scenario, where the NMT (Neural Machine Translation) model is expected to precisely handle the provided domain-specific terminology.
In this experiment, we use the prompt ‚Äú<span class="ltx_text ltx_font_typewriter" id="A4.p1.5.1">{srcWord} means {tgtWord}. Translate the following sentences from {src} to {tgt}, and muse use the given word translations.{line}</span>‚Äù for inference of TIM.
The numbers of sentences on Zh<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="A4.p1.1.m1.1"><semantics id="A4.p1.1.m1.1a"><mo id="A4.p1.1.m1.1.1" stretchy="false" xref="A4.p1.1.m1.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="A4.p1.1.m1.1b"><ci id="A4.p1.1.m1.1.1.cmml" xref="A4.p1.1.m1.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="A4.p1.1.m1.1d">‚áí</annotation></semantics></math>En and De<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="A4.p1.2.m2.1"><semantics id="A4.p1.2.m2.1a"><mo id="A4.p1.2.m2.1.1" stretchy="false" xref="A4.p1.2.m2.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="A4.p1.2.m2.1b"><ci id="A4.p1.2.m2.1.1.cmml" xref="A4.p1.2.m2.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.2.m2.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="A4.p1.2.m2.1d">‚áí</annotation></semantics></math>En are 2640 and 2963, respectively.
The average numbers of terms per segment on Zh<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="A4.p1.3.m3.1"><semantics id="A4.p1.3.m3.1a"><mo id="A4.p1.3.m3.1.1" stretchy="false" xref="A4.p1.3.m3.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="A4.p1.3.m3.1b"><ci id="A4.p1.3.m3.1.1.cmml" xref="A4.p1.3.m3.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.3.m3.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="A4.p1.3.m3.1d">‚áí</annotation></semantics></math>En and De<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="A4.p1.4.m4.1"><semantics id="A4.p1.4.m4.1a"><mo id="A4.p1.4.m4.1.1" stretchy="false" xref="A4.p1.4.m4.1.1.cmml">‚áí</mo><annotation-xml encoding="MathML-Content" id="A4.p1.4.m4.1b"><ci id="A4.p1.4.m4.1.1.cmml" xref="A4.p1.4.m4.1.1">‚áí</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.4.m4.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="A4.p1.4.m4.1d">‚áí</annotation></semantics></math>En are 3.8 and 1.1, respectively.
We only highlight a few systems that achieved the best performance on specific metrics in the competition findings <cite class="ltx_cite ltx_citemacro_cite">Semenov et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2311.02851v2#bib.bib33" title="">2023</a>)</cite>.
Lingua Custodia, which utilizes a specialized Transformer architecture to ensure the inclusion of given terminology in the translation. Additionally, the <math alttext="\text{UEDIN}_{\rm LLM}" class="ltx_Math" display="inline" id="A4.p1.5.m5.1"><semantics id="A4.p1.5.m5.1a"><msub id="A4.p1.5.m5.1.1" xref="A4.p1.5.m5.1.1.cmml"><mtext id="A4.p1.5.m5.1.1.2" xref="A4.p1.5.m5.1.1.2a.cmml">UEDIN</mtext><mi id="A4.p1.5.m5.1.1.3" xref="A4.p1.5.m5.1.1.3.cmml">LLM</mi></msub><annotation-xml encoding="MathML-Content" id="A4.p1.5.m5.1b"><apply id="A4.p1.5.m5.1.1.cmml" xref="A4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="A4.p1.5.m5.1.1.1.cmml" xref="A4.p1.5.m5.1.1">subscript</csymbol><ci id="A4.p1.5.m5.1.1.2a.cmml" xref="A4.p1.5.m5.1.1.2"><mtext id="A4.p1.5.m5.1.1.2.cmml" xref="A4.p1.5.m5.1.1.2">UEDIN</mtext></ci><ci id="A4.p1.5.m5.1.1.3.cmml" xref="A4.p1.5.m5.1.1.3">LLM</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.5.m5.1c">\text{UEDIN}_{\rm LLM}</annotation><annotation encoding="application/x-llamapun" id="A4.p1.5.m5.1d">UEDIN start_POSTSUBSCRIPT roman_LLM end_POSTSUBSCRIPT</annotation></semantics></math> employs ChatGPT with prompts specifically designed for terminology translation.</p>
</div>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Different from traditional NMT with additional language models
</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">Traditional language models, such as causal language models are usually used as decoder initialization or reranking to improve fluency. We do not consider the prediction probabilities of LLMs during the decoding process of NMT.
Instead, we treat LLMs as independent translation systems and introduce speculative execution as a fusion approach for NMT systems and MT-oriented LLMs.
Notably, we do not introduce additional monolingual training data and our research does not encompass back-translation.
Building upon this insight, we propose a hybrid framework that combines the strengths of both NMT systems and MT-oriented LLMs and alleviates the limitations of individual systems.</p>
</div>
<figure class="ltx_table" id="A5.T8">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A5.T8.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T8.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A5.T8.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T8.1.1.1.1.1">System</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A5.T8.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A5.T8.1.1.1.2.1">Translation</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="A5.T8.1.2.2.1"><span class="ltx_text ltx_font_italic" id="A5.T8.1.2.2.1.1">Terminology/abbreviations</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.1.3.3">
<td class="ltx_td ltx_align_left" id="A5.T8.1.3.3.1">Source</td>
<td class="ltx_td ltx_align_left" id="A5.T8.1.3.3.2">Art. 18 GDPR: Right to restriction of data processing if the requirements Art. 18 para 1 lit. a to d are fulfilled.</td>
</tr>
<tr class="ltx_tr" id="A5.T8.1.4.4">
<td class="ltx_td ltx_align_left" id="A5.T8.1.4.4.1">GoogleMT</td>
<td class="ltx_td ltx_align_left" id="A5.T8.1.4.4.2">Ëâ∫ÊúØ„ÄÇ GDPR Á¨¨ 18 Êù°ÔºöÂ¶ÇÊûúÊª°Ë∂≥Á¨¨ 18 Êù°ÁöÑË¶ÅÊ±ÇÔºåÂàôÊúâÊùÉÈôêÂà∂Êï∞ÊçÆÂ§ÑÁêÜ„ÄÇ 18 ÊÆµ 1 Â≠ó„ÄÇ aÂà∞dÂùáÊª°Ë∂≥„ÄÇ</td>
</tr>
<tr class="ltx_tr" id="A5.T8.1.5.5">
<td class="ltx_td ltx_align_left" id="A5.T8.1.5.5.1">TIM</td>
<td class="ltx_td ltx_align_left" id="A5.T8.1.5.5.2">„ÄäÈÄöÁî®Êï∞ÊçÆ‰øùÊä§Êù°‰æã„ÄãÁ¨¨ 18 Êù°ÔºöÂ¶ÇÊûúÊª°Ë∂≥Á¨¨ 18 Êù°Á¨¨ 1 Ê¨æ a Ëá≥ d È°πÁöÑË¶ÅÊ±ÇÔºåÂàôÊúâÊùÉÈôêÂà∂Êï∞ÊçÆÂ§ÑÁêÜ„ÄÇ</td>
</tr>
<tr class="ltx_tr" id="A5.T8.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="A5.T8.1.6.6.1"><span class="ltx_text ltx_font_italic" id="A5.T8.1.6.6.1.1">Ill-informed text</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.1.7.7">
<td class="ltx_td ltx_align_left" id="A5.T8.1.7.7.1">Source</td>
<td class="ltx_td ltx_align_left" id="A5.T8.1.7.7.2">Êâπ„ÄäÈÅìË∑ØÊú∫Âä®ËΩ¶ËæÜÁîü‰∫ß‰ºÅ‰∏öÂèä‰∫ßÂìÅÂÖ¨Âëä„Äã‰∏≠ÔºåÊ±üÊ∑Æ</td>
</tr>
<tr class="ltx_tr" id="A5.T8.1.8.8">
<td class="ltx_td ltx_align_left" id="A5.T8.1.8.8.1">GoogleMT</td>
<td class="ltx_td ltx_align_left" id="A5.T8.1.8.8.2">In the batch of "Announcement of Road Motor Vehicle Manufacturers and Products", JAC</td>
</tr>
<tr class="ltx_tr" id="A5.T8.1.9.9">
<td class="ltx_td ltx_align_left" id="A5.T8.1.9.9.1">TIM</td>
<td class="ltx_td ltx_align_left" id="A5.T8.1.9.9.2">In the "Road Motor Vehicle Manufacturers and Products Announcement", Jianghuai</td>
</tr>
<tr class="ltx_tr" id="A5.T8.1.10.10">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="A5.T8.1.10.10.1"><span class="ltx_text ltx_font_italic" id="A5.T8.1.10.10.1.1">ComplexÔºåRepetition-containing</span></td>
</tr>
<tr class="ltx_tr" id="A5.T8.1.11.11">
<td class="ltx_td ltx_align_left" id="A5.T8.1.11.11.1">Source</td>
<td class="ltx_td ltx_align_left" id="A5.T8.1.11.11.2">let mut v = vec![10, 20, 30]; let handle = thread::spawn(||  v.push(10); );</td>
</tr>
<tr class="ltx_tr" id="A5.T8.1.12.12">
<td class="ltx_td ltx_align_left" id="A5.T8.1.12.12.1">GoogleMT</td>
<td class="ltx_td ltx_align_left" id="A5.T8.1.12.12.2">ËÆ© mut v = vec![10, 20, 30];ËÆ©Âè•ÊüÑ = thread::spawn(||  v.push(10); );</td>
</tr>
<tr class="ltx_tr" id="A5.T8.1.13.13">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A5.T8.1.13.13.1">TIM</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A5.T8.1.13.13.2">let mut v = vec! [10,20,30]; let handle = thread::spawn (||  v.push (10); );</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>
<span class="ltx_text ltx_font_bold" id="A5.T8.3.1">Case Study</span>. We present examples of several translation challenges that pose difficulties for NMT systems but are effectively mitigated by MT-oriented LLMs.
</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>About speedup</h2>
<div class="ltx_para" id="A6.p1">
<p class="ltx_p" id="A6.p1.1">The time consumption of the Hybrid Threshold is the sum of the inference time for both the NMT systems and the MT-oriented LLM, whereas the CoDec requires only the inference time of the NMT systems and a small amount of calculation of the LLM.
Considering the relatively negligible time consumption of Google Translate, we did not specifically factor in its inference time in our analysis, as it does not significantly impact the overall performance comparison.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon May 27 07:52:01 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
