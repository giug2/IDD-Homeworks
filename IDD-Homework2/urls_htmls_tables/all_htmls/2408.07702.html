<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models</title>
<!--Generated on Sun Aug 18 19:03:50 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.07702v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S1" title="In The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S2" title="In The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Preliminaries</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S2.SS1" title="In 2 Preliminaries ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Text-to-SQL Pipeline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S2.SS2" title="In 2 Preliminaries ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Schema Linking</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S3" title="In The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S3.SS1" title="In 3 Experimental Setup ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S3.SS2" title="In 3 Experimental Setup ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S3.SS3" title="In 3 Experimental Setup ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Methodology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S3.SS4" title="In 3 Experimental Setup ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S4" title="In The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S4.SS1" title="In 4 Results ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experiment 1: Impact of False Positives on Accuracy Given Perfect SLR</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S4.SS2" title="In 4 Results ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Experiment 2: Impact of False Positives on Accuracy Given Actual SLR</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S4.SS3" title="In 4 Results ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Experiment 3: Impact of Non-Filtering Stages and Techniques</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S4.SS4" title="In 4 Results ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Discussion and Proposed Approach</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S5" title="In The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\quotingsetup</span>
<p class="ltx_p" id="p1.2">leftmargin=1cm

















</p>
</div>
<h1 class="ltx_title ltx_title_document">The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Karime Maamari<sup class="ltx_sup" id="id7.7.id1"><span class="ltx_text ltx_font_italic" id="id7.7.id1.1">1</span></sup>  Fadhil Abubaker<sup class="ltx_sup" id="id8.8.id2"><span class="ltx_text ltx_font_italic" id="id8.8.id2.1">1</span></sup>  Daniel Jaroslawicz<sup class="ltx_sup" id="id9.9.id3"><span class="ltx_text ltx_font_italic" id="id9.9.id3.1">1</span></sup>  Amine Mhedhbi<sup class="ltx_sup" id="id10.10.id4"><span class="ltx_text ltx_font_italic" id="id10.10.id4.1">2</span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id11.11.id5">1</sup>Distyl AI  <sup class="ltx_sup" id="id12.12.id6">2</sup>Polytechnique Montreal
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id13.13.id7">{karime,fadhil,daniel}@distyl.ai
<br class="ltx_break"/>amine.mhedhbi@polymtl.ca</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id14.id1">Schema linking is a crucial step in Text-to-SQL pipelines.
Its goal is to retrieve the relevant tables and columns of a target database for a user’s query while disregarding irrelevant ones.
However, imperfect schema linking can often exclude required columns needed for accurate query generation.
In this work, we revisit schema linking when using the latest generation of large language models (LLMs).
We find empirically that newer models are adept at utilizing relevant schema elements during generation even in the presence of large numbers of irrelevant ones.
As such, our Text-to-SQL pipeline entirely forgoes schema linking in cases where the schema fits within the model’s context window in order to minimize issues due to filtering required schema elements.
Furthermore, instead of filtering contextual information, we highlight techniques such as augmentation, selection, and correction, and adopt them to improve the accuracy of our Text-to-SQL pipeline.
Our approach ranks first on the BIRD benchmark achieving an accuracy of 71.83%.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">We address the task of Text-to-SQL: generating a database-executable SQL query given a natural language inquiry <cite class="ltx_cite ltx_citemacro_citep">(Androutsopoulos et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib1" title="">1995</a>; Quamar et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib21" title="">2022</a>)</cite>.Text-to-SQL is crucial in democratizing data access as it allows querying databases using natural language The advent of large language models (LLMs) has significantly advanced Text-to-SQL by simplifying the translation of natural language into SQL.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">LLM-based Text-to-SQL approaches typically follow a multi-stage generation pipeline, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S2.F1" title="Figure 1 ‣ 2.2 Schema Linking ‣ 2 Preliminaries ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> <cite class="ltx_cite ltx_citemacro_citep">(Hong et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib9" title="">2024</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib12" title="">2024a</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib16" title="">2024</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib30" title="">2024</a>)</cite>.
The pipeline begins with a retrieval stage to collect contextual knowledge
such as the definition of terms and database schema elements.
This is followed by a generation stage, where an LLM produces a candidate SQL query.
Finally, the correction stage regenerates the SQL as needed based on encountered errors.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Selecting relevant elements of the database schema (tables and columns) – known as <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">schema linking</em> – provides the necessary context for the LLM to produce correct SQL in the downstream generation stage. Effective schema linking implies retrieving <em class="ltx_emph ltx_font_italic" id="S1.p3.1.2">all</em> the relevant database components associated with the natural language query. Missing even a single required column results in incorrect executable SQL.
Thus, it is vital to ensure that all essential columns are retrieved.
However, this does not mean that it should be overly inclusive.
Research has shown that false positives, <span class="ltx_text ltx_font_italic" id="S1.p3.1.3">i.e.</span>, the number of irrelevant columns passed to the LLM, can often degrade text-to-SQL accuracy.
For example, <cite class="ltx_cite ltx_citemacro_cite">Floratou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib4" title="">2024</a>)</cite> showed that even when the entire database schema fits into the context of an LLM, it is still advantageous to perform schema linking.
At the same time, attempts to prune irrelevant columns may also remove some required ones.
Thus, schema linking traditionally contains an inherent trade-off between minimizing false positives while also preserving relevant context.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">As LLM reasoning capabilities improve, we challenge the conventional wisdom that schema linking is necessary for accurate Text-to-SQL when the schema fits within the model’s context window.
We find empirically that as model reasoning improves, the benefits of reducing false positives diminishes, <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">i.e.</span>, newer models are more capable of sifting through the schema to identify relevant columns compared to older models <cite class="ltx_cite ltx_citemacro_citep">(Laban et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib10" title="">2024</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib14" title="">2024b</a>)</cite>.
This effect might be similar to how the latest LLMs when probed can reliably recall ‘a needle in multiple millions of
tokens of ”haystack“’ <cite class="ltx_cite ltx_citemacro_citep">(Gregory Kamradt,, <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib6" title="">2023</a>; Reid et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib22" title="">2024</a>)</cite>.
For these models, schema linking is unnecessary and can even be detrimental, as it may filter out essential columns.
Instead, we present alternatives to schema linking that improve accuracy without schema information loss.
Our approach based on these insights currently ranks first in accuracy at 71.83% on the BIRD benchmark <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib13" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Preliminaries</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">We first outline the key elements of the Text-to-SQL pipeline, with a focus on schema linking and its implications.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Text-to-SQL Pipeline</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">In current state-of-the-art approaches, Text-to-SQL uses multi-sage pipelines comprised of retrieval, generation, and correction stages.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">The retrieval stage gathers relevant contextual information, including schema elements, domain knowledge, and example queries <cite class="ltx_cite ltx_citemacro_citep">(Dong et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib3" title="">2023</a>; Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib5" title="">2023</a>; Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib11" title="">2024</a>; Pourreza &amp; Rafiei, <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib19" title="">2023</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib25" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">The generation stage often involves more than just producing a candidate SQL query associated given an input context.
Rather, approaches frequently augment the generation process through techniques like decomposed generation <cite class="ltx_cite ltx_citemacro_citep">(Maamari &amp; Mhedhbi, <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib18" title="">2024</a>; Pourreza &amp; Rafiei, <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib19" title="">2023</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib25" title="">2024</a>)</cite> and chain-of-thought prompting <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib29" title="">2023</a>)</cite>. In addition, most approaches employ methods like self-consistency and multi-choice selection to produce multiple results, selecting the best outcome <cite class="ltx_cite ltx_citemacro_citep">(Dong et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib3" title="">2023</a>; Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib5" title="">2023</a>; Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib11" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">The retrieval and generation stages often contain various techniques chained together. These techniques can be broadly categorized as filtering or augmenting, <span class="ltx_text ltx_font_italic" id="S2.SS1.p4.1.1">i.e.</span>, techniques can either strip away unnecessary contextual information or attempt to provide additional useful context.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">Finally, the correction stage will often employ some combination of execution-based feedback <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib27" title="">2018b</a>; Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib15" title="">2020</a>; He et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib8" title="">2019</a>; Lyu et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib17" title="">2020</a>)</cite> or model-based feedback <cite class="ltx_cite ltx_citemacro_citep">(Talaei et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib24" title="">2024</a>; Askari et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib2" title="">2024</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib26" title="">2018a</a>)</cite> to correct the generated query.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Schema Linking</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Within the retrieval stage, schema linking leverages sophisticated prompting techniques to produce variable-length representations, hierarchically retrieve components, and iteratively process the schema <cite class="ltx_cite ltx_citemacro_citep">(Talaei et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib24" title="">2024</a>; Dong et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib3" title="">2023</a>; Pourreza &amp; Rafiei, <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib19" title="">2023</a>; Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib11" title="">2024</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib25" title="">2024</a>; Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib5" title="">2023</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S2.F1">
<p class="ltx_p ltx_align_center ltx_align_center" id="S2.F1.1"><span class="ltx_text" id="S2.F1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="73" id="S2.F1.1.1.g1" src="x1.png" width="830"/></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.3.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.4.2" style="font-size:90%;">A typical Text-to-SQL pipeline comparised of retrieval, generation and correction stages.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">These techniques can vary in i) how they represent the schema and ii) how they perform linking on that representation. For instance, some may represent the schema in natural language, while others utilize code-like structures <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib5" title="">2023</a>)</cite>. The approach to linking can also differ across techniques, with some directly filtering the schema <cite class="ltx_cite ltx_citemacro_citep">(Dong et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib3" title="">2023</a>; Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib11" title="">2024</a>; Talaei et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib24" title="">2024</a>)</cite>, and others using intermediate representations to identify relevant tables and columns <cite class="ltx_cite ltx_citemacro_citep">(Qu et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib20" title="">2024</a>)</cite>. The choice of schema representation and linking strategy can have a significant influence on accuracy <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib5" title="">2023</a>)</cite>. This variability underscores the importance of selecting an appropriate method tailored to the specific requirements of the task, as the degree of filtering can directly impact the loss of schema information incurred during the schema linking process.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Most prior investigations of schema linking regardless of approach have arrived at the same conclusion – schema linking yields meaningful gains in accuracy <cite class="ltx_cite ltx_citemacro_citep">(Guo et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib7" title="">2019</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib12" title="">2024a</a>; Talaei et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib24" title="">2024</a>)</cite>. However, these explorations used LLMs that are more sensitive to the presence of irrelevant columns (false positives) as contextual information, where reducing the false positives yielded meaningful gains in performance <cite class="ltx_cite ltx_citemacro_citep">(Floratou et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib4" title="">2024</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setup</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our experimental analysis guides the design and implementation of our proposed Text-to-SQL pipeline (§<a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S4.SS4" title="4.4 Discussion and Proposed Approach ‣ 4 Results ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_tag">4.4</span></a>). Our experiments aim to answer empirically three research questions (RQs):</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><em class="ltx_emph ltx_font_italic" id="S3.I1.ix1.1.1.1">RQ1.</em></span>
<div class="ltx_para" id="S3.I1.ix1.p1">
<p class="ltx_p" id="S3.I1.ix1.p1.1">How does the inclusion of irrelevant schema elements impact SQL generation?</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.ix2" style="list-style-type:none;padding-top:2.0pt;">
<span class="ltx_tag ltx_tag_item"><em class="ltx_emph ltx_font_italic" id="S3.I1.ix2.1.1.1">RQ2.</em></span>
<div class="ltx_para" id="S3.I1.ix2.p1">
<p class="ltx_p" id="S3.I1.ix2.p1.1">How can the trade-off between precision and recall in schema linking techniques be characterized, and what is its downstream impact on SQL generation?</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.ix3" style="list-style-type:none;padding-top:2.0pt;">
<span class="ltx_tag ltx_tag_item"><em class="ltx_emph ltx_font_italic" id="S3.I1.ix3.1.1.1">RQ3.</em></span>
<div class="ltx_para" id="S3.I1.ix3.p1">
<p class="ltx_p" id="S3.I1.ix3.p1.1">How do other techniques and stages within Text-to-SQL pipelines, aside from schema linking, comparatively impact SQL generation?</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Next, we cover the details of our setup (datasets and models), and our methodology and metrics.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.3">We conducted our experiments using the BIRD dataset <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib13" title="">2023</a>)</cite>,
which is widely considered to be the most challenging Text-to-SQL benchmark. BIRD contains queries from 95 databases spanning a wide breadth of domains, such as education and hockey, and is designed to mimic the complexity of real-world databases.
This complexity arises from its <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.3.1">“dirty”</em> format, where data, queries, and external knowledge may contain flaws — queries can be incorrect, database columns might be improperly described, and databases can contain null values and unexpected encodings.
Our evaluation set consisted of <math alttext="10\%" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mn id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">10</mn><mo id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1">percent</csymbol><cn id="S3.SS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS1.p1.1.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">10 %</annotation></semantics></math> of the entries from each database in the dev set, as done in evaluations in prior work <cite class="ltx_cite ltx_citemacro_citep">(Talaei et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib24" title="">2024</a>)</cite>.
Our training set consisted of <math alttext="500" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><cn id="S3.SS1.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS1.p1.2.m2.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">500</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">500</annotation></semantics></math> of the <math alttext="9,428" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.2"><semantics id="S3.SS1.p1.3.m3.2a"><mrow id="S3.SS1.p1.3.m3.2.3.2" xref="S3.SS1.p1.3.m3.2.3.1.cmml"><mn id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">9</mn><mo id="S3.SS1.p1.3.m3.2.3.2.1" xref="S3.SS1.p1.3.m3.2.3.1.cmml">,</mo><mn id="S3.SS1.p1.3.m3.2.2" xref="S3.SS1.p1.3.m3.2.2.cmml">428</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.2b"><list id="S3.SS1.p1.3.m3.2.3.1.cmml" xref="S3.SS1.p1.3.m3.2.3.2"><cn id="S3.SS1.p1.3.m3.1.1.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1">9</cn><cn id="S3.SS1.p1.3.m3.2.2.cmml" type="integer" xref="S3.SS1.p1.3.m3.2.2">428</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.2c">9,428</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.2d">9 , 428</annotation></semantics></math> available samples in the BIRD training dataset.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Models</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We used the following language models with context windows sufficiently large to accommodate the entire schema for each query in the BIRD evaluation set:</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.SS2.p2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.SS2.p2.1.1.1">
<td class="ltx_td ltx_align_top" id="S3.SS2.p2.1.1.1.1"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.SS2.p2.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.SS2.p2.1.1.1.2.1">
<span class="ltx_p" id="S3.SS2.p2.1.1.1.2.1.1" style="width:143.1pt;"><span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.1.1.2.1.1.1">ft:GPT-4o (fine-tuned)</span></span>
<span class="ltx_p" id="S3.SS2.p2.1.1.1.2.1.2"><span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.1.1.2.1.2.1">GPT-4o</span></span>
<span class="ltx_p" id="S3.SS2.p2.1.1.1.2.1.3"><span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.1.1.2.1.3.1">GPT-4o-Mini</span></span>
<span class="ltx_p" id="S3.SS2.p2.1.1.1.2.1.4"><span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.1.1.2.1.4.1">GPT-4-Turbo</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.SS2.p2.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.SS2.p2.1.1.1.3.1">
<span class="ltx_p" id="S3.SS2.p2.1.1.1.3.1.1" style="width:121.4pt;"><span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.1.1.3.1.1.1">Llama 3.1-405b</span></span>
<span class="ltx_p" id="S3.SS2.p2.1.1.1.3.1.2"><span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.1.1.3.1.2.1">Llama 3.1-70b</span></span>
<span class="ltx_p" id="S3.SS2.p2.1.1.1.3.1.3"><span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.1.1.3.1.3.1">Llama 3.1-8b</span></span>
<span class="ltx_p" id="S3.SS2.p2.1.1.1.3.1.4"><span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.1.1.3.1.4.1">Deepseek Coder-V2</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.SS2.p2.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.SS2.p2.1.1.1.4.1">
<span class="ltx_p" id="S3.SS2.p2.1.1.1.4.1.1" style="width:134.4pt;"><span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.1.1.4.1.1.1">Claude 3.5 Sonnet</span></span>
<span class="ltx_p" id="S3.SS2.p2.1.1.1.4.1.2"><span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.1.1.4.1.2.1">Claude 3 Opus</span></span>
<span class="ltx_p" id="S3.SS2.p2.1.1.1.4.1.3"><span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.1.1.4.1.3.1">Mixtral-8x22B</span></span>
<span class="ltx_p" id="S3.SS2.p2.1.1.1.4.1.4"><span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.1.1.4.1.4.1">Gemini 1.5 Pro</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">We overview the fine-tuning approach of <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.1">GPT-4o</span> momentarily under methodology (§<a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S3.SS3" title="3.3 Methodology ‣ 3 Experimental Setup ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Methodology</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We design an empirical experiment for each research question:</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><em class="ltx_emph ltx_font_italic" id="S3.I2.ix1.1.1.1">Exp 1.</em></span>
<div class="ltx_para" id="S3.I2.ix1.p1">
<p class="ltx_p" id="S3.I2.ix1.p1.1">We use a simplified Text-to-SQL pipeline consisting of only schema linking within the retrieval stage followed by a single attempt at generation.
For each query, we provide all the required columns and vary the amount of irrelevant columns to look at the impact of irrelevant columns retrieved on the generation.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.ix2" style="list-style-type:none;padding-top:2.0pt;">
<span class="ltx_tag ltx_tag_item"><em class="ltx_emph ltx_font_italic" id="S3.I2.ix2.1.1.1">Exp 2.</em></span>
<div class="ltx_para" id="S3.I2.ix2.p1">
<p class="ltx_p" id="S3.I2.ix2.p1.1">Using the same simplified pipeline, we introduce different implementations of schema linking that vary in precision and recall and analyze their impact on generation.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.ix3" style="list-style-type:none;padding-top:2.0pt;">
<span class="ltx_tag ltx_tag_item"><em class="ltx_emph ltx_font_italic" id="S3.I2.ix3.1.1.1">Exp 3.</em></span>
<div class="ltx_para" id="S3.I2.ix3.p1">
<p class="ltx_p" id="S3.I2.ix3.p1.1">We introduce augmentation, selection, and correction techniques on top of the simplified pipeline without and with schema linking. We run an ablation study to understand the relative impact of each technique on end-to-end accuracy.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1"><em class="ltx_emph ltx_font_italic" id="S3.SS3.p2.1.1">Runs and input/output structure</em>. In all runs, the temperature was set to zero and structured output was used whenever possible.
Given that not all models are capable of reliable structured output generation,
the generated SQL query was fed through an identity call by <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p2.1.2">GPT-4o-Mini</span> in <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p2.1.3">JSON</span> mode to handle any potential issues with output formatting.
The relative position of any schema element in an input prompt follows the same ordering as that provided by the schema definition of the benchmark.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.2"><em class="ltx_emph ltx_font_italic" id="S3.SS3.p3.2.1">Fine-tuning <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.2.1.1">GPT-4o</span></em>. Fine-tuning is done iteratively.
At each iteration, we first fine-tune on a sample of <math alttext="N" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">italic_N</annotation></semantics></math> triples: natural language query, SQL query, and schema elements.
For each query, the schema includes all required columns and a random number of irrelevant columns picked uniformly at random.
We then evaluate on BIRD’s dev set.
For each failed query, we prompt the model to reason about the failure, aggregate the reasoning across queries, and use it to pick the sample for the next iteration.
Finally, based on the reasoning, we select a new sample of size <math alttext="N" class="ltx_Math" display="inline" id="S3.SS3.p3.2.m2.1"><semantics id="S3.SS3.p3.2.m2.1a"><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.2.m2.1d">italic_N</annotation></semantics></math>.
The iterations stop once a pre-determined accuracy score is reached.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1"><em class="ltx_emph ltx_font_italic" id="S3.SS3.p4.1.1">Generation prompts</em>. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S3.F2" title="Figure 2 ‣ 3.3 Methodology ‣ 3 Experimental Setup ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> shows the structure of the prompt used for SQL generation as well as an example schema, input query, and query hint.</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<p class="ltx_p ltx_align_center ltx_align_center" id="S3.F2.1"><span class="ltx_text" id="S3.F2.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="253" id="S3.F2.1.1.g1" src="x2.png" width="830"/></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.5.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S3.F2.6.2" style="font-size:90%;">(Red<span class="ltx_text ltx_font_medium" id="S3.F2.6.2.1">) Structure of SQL Generation prompt given input query, hint, and schema; 
<br class="ltx_break"/></span>(Blue<span class="ltx_text ltx_font_medium" id="S3.F2.6.2.2">) Examples of a schema, input query, and query hint which act as contextual inputs to a prompt.</span></span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Metrics</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">We rely on three different metrics in our experimental analysis:</p>
<ul class="ltx_itemize" id="S3.I3">
<li class="ltx_item" id="S3.I3.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">-</span>
<div class="ltx_para" id="S3.I3.ix1.p1">
<p class="ltx_p" id="S3.I3.ix1.p1.1"><em class="ltx_emph ltx_font_italic" id="S3.I3.ix1.p1.1.1">Execution Accuracy</em> (EX): The metric used by the BIRD benchmark to evaluate end-to-end Text-to-SQL pipelines.
It is the proportion of queries for which the output of the predicted SQL query is identical to that of the ground truth SQL query.
We report EX as a percentage over queries in the evaluation set.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.ix2" style="list-style-type:none;padding-top:2.0pt;">
<span class="ltx_tag ltx_tag_item">-</span>
<div class="ltx_para" id="S3.I3.ix2.p1">
<p class="ltx_p" id="S3.I3.ix2.p1.1"><em class="ltx_emph ltx_font_italic" id="S3.I3.ix2.p1.1.1">False Positive Rate</em> (FPR): For a given query, the proportion of irrelevant schema columns retrieved over the total number of retrieved columns.
We report its average over queries in the evaluation set.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.ix3" style="list-style-type:none;padding-top:2.0pt;">
<span class="ltx_tag ltx_tag_item">-</span>
<div class="ltx_para" id="S3.I3.ix3.p1">
<p class="ltx_p" id="S3.I3.ix3.p1.1"><em class="ltx_emph ltx_font_italic" id="S3.I3.ix3.p1.1.1">Schema Linking Recall</em> (SLR):
The proportion of queries for which all required columns are retrieved over the total number of queries.
We use SLR as the downstream generation requires all required columns to be retrieved to be correct.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">All queries in evaluation sets are across multiple databases.
Note that BIRD also has a second metric: valid efficiency score.
It assesses the efficiency of correctly predicted queries by comparing their execution speed to those of the corresponding ground truth queries.
In our experiments, we focus on EX, as our research questions are primarily concerned with SQL generation accuracy.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experiment 1: Impact of False Positives on Accuracy Given Perfect SLR</h3>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S4.F3.1" style="width:297.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="495" id="S4.F3.1.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.1.1.1.1" style="font-size:90%;">((a))</span> </span><span class="ltx_text" id="S4.F3.1.2.2" style="font-size:90%;">The idealized execution accuracy (IEX) as the false positive rate (FPR) varies from 99% to 0%.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S4.F3.2" style="width:118.8pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="1239" id="S4.F3.2.g1" src="x4.png" width="819"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.2.1.1.1" style="font-size:90%;">((b))</span> </span><span class="ltx_text" id="S4.F3.2.2.2" style="font-size:90%;">Capability and sensitivity relationship.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.5.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.6.2" style="font-size:90%;">Idealized execution accuracy (IEX), <span class="ltx_text ltx_font_italic" id="S4.F3.6.2.1">i.e.</span>, with perfect schema linking recall (SLR), as the false positive rate (FPR) varies for different LMs and the relationship between their capability (maximum IEX) and sensitivity to false positives.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In this first experiment, we assess the impact of retrieving irrelevant columns on SQL generation accuracy.
We create a scenario with perfect schema linking recall such that SQL generation issues are not due to missing required columns.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.4">To implement the experiment, we mock the
schema linker as follows.
We build an oracle that identifies all columns necessary for a given query using <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p2.4.1">SQLGLOT</span> (SQL Parser and Transpiler).
For an input query, the mocked schema linker uses the oracle to retrieve all required columns and injects a pre-defined rate of false positives (irrelevant columns retrieved / total columns retrieved).
To inject it, the mocked schema linker samples the irrelevant columns uniformly at random from the target database.
If there is not a sufficient number of columns in the target database,
<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.4.2">e.g.</span>, there are <math alttext="10" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><cn id="S4.SS1.p2.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p2.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">10</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">10</annotation></semantics></math> required columns and <math alttext="900" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.1"><semantics id="S4.SS1.p2.2.m2.1a"><mn id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">900</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><cn id="S4.SS1.p2.2.m2.1.1.cmml" type="integer" xref="S4.SS1.p2.2.m2.1.1">900</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">900</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.1d">900</annotation></semantics></math> irrelevant columns are needed to produce a <math alttext="99\%" class="ltx_Math" display="inline" id="S4.SS1.p2.3.m3.1"><semantics id="S4.SS1.p2.3.m3.1a"><mrow id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><mn id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml">99</mn><mo id="S4.SS1.p2.3.m3.1.1.1" xref="S4.SS1.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><csymbol cd="latexml" id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1">percent</csymbol><cn id="S4.SS1.p2.3.m3.1.1.2.cmml" type="integer" xref="S4.SS1.p2.3.m3.1.1.2">99</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">99\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.3.m3.1d">99 %</annotation></semantics></math> false positive rate, yet only <math alttext="50" class="ltx_Math" display="inline" id="S4.SS1.p2.4.m4.1"><semantics id="S4.SS1.p2.4.m4.1a"><mn id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><cn id="S4.SS1.p2.4.m4.1.1.cmml" type="integer" xref="S4.SS1.p2.4.m4.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">50</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.4.m4.1d">50</annotation></semantics></math> columns exist in the target database,
it supplements from other databases with columns not conflicting in name.
We use a simple pipeline: a retrieval stage containing only the mocked schema linker followed by a zero-shot generation stage.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.8">We run the pipeline using the <math alttext="12" class="ltx_Math" display="inline" id="S4.SS1.p3.1.m1.1"><semantics id="S4.SS1.p3.1.m1.1a"><mn id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><cn id="S4.SS1.p3.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p3.1.m1.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">12</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">12</annotation></semantics></math> selected models (§<a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S3.SS2" title="3.2 Models ‣ 3 Experimental Setup ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_tag">3.2</span></a>) for generation
while applying an equal false positive rate to each query in the evaluation set.
We run for <math alttext="10" class="ltx_Math" display="inline" id="S4.SS1.p3.2.m2.1"><semantics id="S4.SS1.p3.2.m2.1a"><mn id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><cn id="S4.SS1.p3.2.m2.1.1.cmml" type="integer" xref="S4.SS1.p3.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">10</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.2.m2.1d">10</annotation></semantics></math> different rates that are equally log-spaced from <math alttext="99\%" class="ltx_Math" display="inline" id="S4.SS1.p3.3.m3.1"><semantics id="S4.SS1.p3.3.m3.1a"><mrow id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml"><mn id="S4.SS1.p3.3.m3.1.1.2" xref="S4.SS1.p3.3.m3.1.1.2.cmml">99</mn><mo id="S4.SS1.p3.3.m3.1.1.1" xref="S4.SS1.p3.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><apply id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1"><csymbol cd="latexml" id="S4.SS1.p3.3.m3.1.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1.1">percent</csymbol><cn id="S4.SS1.p3.3.m3.1.1.2.cmml" type="integer" xref="S4.SS1.p3.3.m3.1.1.2">99</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">99\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.3.m3.1d">99 %</annotation></semantics></math> and <math alttext="0\%" class="ltx_Math" display="inline" id="S4.SS1.p3.4.m4.1"><semantics id="S4.SS1.p3.4.m4.1a"><mrow id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml"><mn id="S4.SS1.p3.4.m4.1.1.2" xref="S4.SS1.p3.4.m4.1.1.2.cmml">0</mn><mo id="S4.SS1.p3.4.m4.1.1.1" xref="S4.SS1.p3.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.1b"><apply id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1"><csymbol cd="latexml" id="S4.SS1.p3.4.m4.1.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1.1">percent</csymbol><cn id="S4.SS1.p3.4.m4.1.1.2.cmml" type="integer" xref="S4.SS1.p3.4.m4.1.1.2">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.1c">0\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.4.m4.1d">0 %</annotation></semantics></math>.
We refer to the execution accuracy under perfect schema linking recall as the idealized execution accuracy (IEX).
We track the change in IEX as the false positive rate changes.
The results in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S4.F3" title="Figure 3 ‣ 4.1 Experiment 1: Impact of False Positives on Accuracy Given Perfect SLR ‣ 4 Results ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>
show the broad trend that IEX improves as the rate of false positives decreases;
specifically, the stage of SQL generation improves as less irrelevant columns are included as contextual information.
At one extreme with a maximum rate of false positives (<math alttext="99\%" class="ltx_Math" display="inline" id="S4.SS1.p3.5.m5.1"><semantics id="S4.SS1.p3.5.m5.1a"><mrow id="S4.SS1.p3.5.m5.1.1" xref="S4.SS1.p3.5.m5.1.1.cmml"><mn id="S4.SS1.p3.5.m5.1.1.2" xref="S4.SS1.p3.5.m5.1.1.2.cmml">99</mn><mo id="S4.SS1.p3.5.m5.1.1.1" xref="S4.SS1.p3.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.5.m5.1b"><apply id="S4.SS1.p3.5.m5.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1"><csymbol cd="latexml" id="S4.SS1.p3.5.m5.1.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1.1">percent</csymbol><cn id="S4.SS1.p3.5.m5.1.1.2.cmml" type="integer" xref="S4.SS1.p3.5.m5.1.1.2">99</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.5.m5.1c">99\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.5.m5.1d">99 %</annotation></semantics></math>),
there is a <math alttext="\sim 28\%" class="ltx_Math" display="inline" id="S4.SS1.p3.6.m6.1"><semantics id="S4.SS1.p3.6.m6.1a"><mrow id="S4.SS1.p3.6.m6.1.1" xref="S4.SS1.p3.6.m6.1.1.cmml"><mi id="S4.SS1.p3.6.m6.1.1.2" xref="S4.SS1.p3.6.m6.1.1.2.cmml"></mi><mo id="S4.SS1.p3.6.m6.1.1.1" xref="S4.SS1.p3.6.m6.1.1.1.cmml">∼</mo><mrow id="S4.SS1.p3.6.m6.1.1.3" xref="S4.SS1.p3.6.m6.1.1.3.cmml"><mn id="S4.SS1.p3.6.m6.1.1.3.2" xref="S4.SS1.p3.6.m6.1.1.3.2.cmml">28</mn><mo id="S4.SS1.p3.6.m6.1.1.3.1" xref="S4.SS1.p3.6.m6.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.6.m6.1b"><apply id="S4.SS1.p3.6.m6.1.1.cmml" xref="S4.SS1.p3.6.m6.1.1"><csymbol cd="latexml" id="S4.SS1.p3.6.m6.1.1.1.cmml" xref="S4.SS1.p3.6.m6.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS1.p3.6.m6.1.1.2.cmml" xref="S4.SS1.p3.6.m6.1.1.2">absent</csymbol><apply id="S4.SS1.p3.6.m6.1.1.3.cmml" xref="S4.SS1.p3.6.m6.1.1.3"><csymbol cd="latexml" id="S4.SS1.p3.6.m6.1.1.3.1.cmml" xref="S4.SS1.p3.6.m6.1.1.3.1">percent</csymbol><cn id="S4.SS1.p3.6.m6.1.1.3.2.cmml" type="integer" xref="S4.SS1.p3.6.m6.1.1.3.2">28</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.6.m6.1c">\sim 28\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.6.m6.1d">∼ 28 %</annotation></semantics></math> IEX difference between the worst and best performing models.
In the other extreme with no false positives (<math alttext="0\%" class="ltx_Math" display="inline" id="S4.SS1.p3.7.m7.1"><semantics id="S4.SS1.p3.7.m7.1a"><mrow id="S4.SS1.p3.7.m7.1.1" xref="S4.SS1.p3.7.m7.1.1.cmml"><mn id="S4.SS1.p3.7.m7.1.1.2" xref="S4.SS1.p3.7.m7.1.1.2.cmml">0</mn><mo id="S4.SS1.p3.7.m7.1.1.1" xref="S4.SS1.p3.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.7.m7.1b"><apply id="S4.SS1.p3.7.m7.1.1.cmml" xref="S4.SS1.p3.7.m7.1.1"><csymbol cd="latexml" id="S4.SS1.p3.7.m7.1.1.1.cmml" xref="S4.SS1.p3.7.m7.1.1.1">percent</csymbol><cn id="S4.SS1.p3.7.m7.1.1.2.cmml" type="integer" xref="S4.SS1.p3.7.m7.1.1.2">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.7.m7.1c">0\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.7.m7.1d">0 %</annotation></semantics></math>), the IEX difference between the worst and best performing models gets reduced to <math alttext="\sim 14\%" class="ltx_Math" display="inline" id="S4.SS1.p3.8.m8.1"><semantics id="S4.SS1.p3.8.m8.1a"><mrow id="S4.SS1.p3.8.m8.1.1" xref="S4.SS1.p3.8.m8.1.1.cmml"><mi id="S4.SS1.p3.8.m8.1.1.2" xref="S4.SS1.p3.8.m8.1.1.2.cmml"></mi><mo id="S4.SS1.p3.8.m8.1.1.1" xref="S4.SS1.p3.8.m8.1.1.1.cmml">∼</mo><mrow id="S4.SS1.p3.8.m8.1.1.3" xref="S4.SS1.p3.8.m8.1.1.3.cmml"><mn id="S4.SS1.p3.8.m8.1.1.3.2" xref="S4.SS1.p3.8.m8.1.1.3.2.cmml">14</mn><mo id="S4.SS1.p3.8.m8.1.1.3.1" xref="S4.SS1.p3.8.m8.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.8.m8.1b"><apply id="S4.SS1.p3.8.m8.1.1.cmml" xref="S4.SS1.p3.8.m8.1.1"><csymbol cd="latexml" id="S4.SS1.p3.8.m8.1.1.1.cmml" xref="S4.SS1.p3.8.m8.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS1.p3.8.m8.1.1.2.cmml" xref="S4.SS1.p3.8.m8.1.1.2">absent</csymbol><apply id="S4.SS1.p3.8.m8.1.1.3.cmml" xref="S4.SS1.p3.8.m8.1.1.3"><csymbol cd="latexml" id="S4.SS1.p3.8.m8.1.1.3.1.cmml" xref="S4.SS1.p3.8.m8.1.1.3.1">percent</csymbol><cn id="S4.SS1.p3.8.m8.1.1.3.2.cmml" type="integer" xref="S4.SS1.p3.8.m8.1.1.3.2">14</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.8.m8.1c">\sim 14\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.8.m8.1d">∼ 14 %</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">We use a model’s maximum IEX in this experiment as a proxy for its SQL generation capability.
Models with higher generation capability, such as <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p4.1.1">Gemini 1.5 Pro</span>, demonstrate greater resilience to false positives compared to lower-performing ones like <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p4.1.2">Llama 3.1-8b</span>.
We characterize a model’s resilience to false positives by the relative change in its SQL generation capability as the false positive rate changes.
We define a metric for sensitivity to false positives as the proportion of change in IEX over the proportion of change in the false positive rate (FPR): <math alttext="d(IEX)/d(FRP)" class="ltx_Math" display="inline" id="S4.SS1.p4.1.m1.2"><semantics id="S4.SS1.p4.1.m1.2a"><mrow id="S4.SS1.p4.1.m1.2.2" xref="S4.SS1.p4.1.m1.2.2.cmml"><mrow id="S4.SS1.p4.1.m1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.cmml"><mrow id="S4.SS1.p4.1.m1.1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.1.cmml"><mi id="S4.SS1.p4.1.m1.1.1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.1.1.3.cmml">d</mi><mo id="S4.SS1.p4.1.m1.1.1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.SS1.p4.1.m1.1.1.1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.cmml"><mo id="S4.SS1.p4.1.m1.1.1.1.1.1.1.2" stretchy="false" xref="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.p4.1.m1.1.1.1.1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.2.cmml">I</mi><mo id="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.3.cmml">E</mi><mo id="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.1a" xref="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.4" xref="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.4.cmml">X</mi></mrow><mo id="S4.SS1.p4.1.m1.1.1.1.1.1.1.3" stretchy="false" xref="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS1.p4.1.m1.1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.1.2.cmml">/</mo><mi id="S4.SS1.p4.1.m1.1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.1.3.cmml">d</mi></mrow><mo id="S4.SS1.p4.1.m1.2.2.3" xref="S4.SS1.p4.1.m1.2.2.3.cmml">⁢</mo><mrow id="S4.SS1.p4.1.m1.2.2.2.1" xref="S4.SS1.p4.1.m1.2.2.2.1.1.cmml"><mo id="S4.SS1.p4.1.m1.2.2.2.1.2" stretchy="false" xref="S4.SS1.p4.1.m1.2.2.2.1.1.cmml">(</mo><mrow id="S4.SS1.p4.1.m1.2.2.2.1.1" xref="S4.SS1.p4.1.m1.2.2.2.1.1.cmml"><mi id="S4.SS1.p4.1.m1.2.2.2.1.1.2" xref="S4.SS1.p4.1.m1.2.2.2.1.1.2.cmml">F</mi><mo id="S4.SS1.p4.1.m1.2.2.2.1.1.1" xref="S4.SS1.p4.1.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="S4.SS1.p4.1.m1.2.2.2.1.1.3" xref="S4.SS1.p4.1.m1.2.2.2.1.1.3.cmml">R</mi><mo id="S4.SS1.p4.1.m1.2.2.2.1.1.1a" xref="S4.SS1.p4.1.m1.2.2.2.1.1.1.cmml">⁢</mo><mi id="S4.SS1.p4.1.m1.2.2.2.1.1.4" xref="S4.SS1.p4.1.m1.2.2.2.1.1.4.cmml">P</mi></mrow><mo id="S4.SS1.p4.1.m1.2.2.2.1.3" stretchy="false" xref="S4.SS1.p4.1.m1.2.2.2.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.2b"><apply id="S4.SS1.p4.1.m1.2.2.cmml" xref="S4.SS1.p4.1.m1.2.2"><times id="S4.SS1.p4.1.m1.2.2.3.cmml" xref="S4.SS1.p4.1.m1.2.2.3"></times><apply id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1"><divide id="S4.SS1.p4.1.m1.1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.1.2"></divide><apply id="S4.SS1.p4.1.m1.1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1"><times id="S4.SS1.p4.1.m1.1.1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1.2"></times><ci id="S4.SS1.p4.1.m1.1.1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1.3">𝑑</ci><apply id="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1.1.1"><times id="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.1"></times><ci id="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.2">𝐼</ci><ci id="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.3">𝐸</ci><ci id="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.4.cmml" xref="S4.SS1.p4.1.m1.1.1.1.1.1.1.1.4">𝑋</ci></apply></apply><ci id="S4.SS1.p4.1.m1.1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.1.3">𝑑</ci></apply><apply id="S4.SS1.p4.1.m1.2.2.2.1.1.cmml" xref="S4.SS1.p4.1.m1.2.2.2.1"><times id="S4.SS1.p4.1.m1.2.2.2.1.1.1.cmml" xref="S4.SS1.p4.1.m1.2.2.2.1.1.1"></times><ci id="S4.SS1.p4.1.m1.2.2.2.1.1.2.cmml" xref="S4.SS1.p4.1.m1.2.2.2.1.1.2">𝐹</ci><ci id="S4.SS1.p4.1.m1.2.2.2.1.1.3.cmml" xref="S4.SS1.p4.1.m1.2.2.2.1.1.3">𝑅</ci><ci id="S4.SS1.p4.1.m1.2.2.2.1.1.4.cmml" xref="S4.SS1.p4.1.m1.2.2.2.1.1.4">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.2c">d(IEX)/d(FRP)</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.1.m1.2d">italic_d ( italic_I italic_E italic_X ) / italic_d ( italic_F italic_R italic_P )</annotation></semantics></math>.
As such, sensitivity to false positives is the slope derived from the model’s (IEX, FPR) data points.
Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S4.F3" title="Figure 3 ‣ 4.1 Experiment 1: Impact of False Positives on Accuracy Given Perfect SLR ‣ 4 Results ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_tag">3</span></a> depicts a strong negative correlation between a model’s SQL generation capability (maximum IEX) and its sensitivity to false positives.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1"><em class="ltx_emph ltx_font_italic" id="S4.SS1.p5.1.1">Empirical Observation:</em> As the model’s SQL generation capability improves, its sensitivity to the presence of irrelevant columns as contextual information for generation decreases.
Perhaps surprisingly, both of these capabilities go hand-in-hand where models that are generally better at SQL generation are also more resilient to large amounts of irrelevant context.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experiment 2: Impact of False Positives on Accuracy Given Actual SLR</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In Experiment 1, all necessary columns for generation are provided, regardless of the false positive rate.
However, in practice, decreasing the amount of false positives requires pruning, which carries the risk of excluding required columns.
Here we assess the extent to which schema linking affects recall of required columns and the downstream impact of imperfect recall on generation.</p>
</div>
<figure class="ltx_figure" id="S4.F4">
<p class="ltx_p ltx_align_center ltx_align_center" id="S4.F4.1"><span class="ltx_text" id="S4.F4.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="231" id="S4.F4.1.1.g1" src="x5.png" width="830"/></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.5.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.6.2" style="font-size:90%;">Prompts used for schema linking.
<span class="ltx_text ltx_font_bold" id="S4.F4.6.2.1">(Left</span>) Single-Column Schema Linking (SCSL): identifying relevance of a particular column independent of the rest of the schema;
<span class="ltx_text ltx_font_bold" id="S4.F4.6.2.2">(Middle + Right)</span> Table-to-Column Schema Linking (TCSL): first identifying relevant tables then relevant columns.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">We explore four different schema linking approaches with a broad spectrum of ability to reduce the false positive rate:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">-</span>
<div class="ltx_para" id="S4.I1.ix1.p1">
<p class="ltx_p" id="S4.I1.ix1.p1.1"><em class="ltx_emph ltx_font_italic" id="S4.I1.ix1.p1.1.1">Single-Column Schema Linking</em> (SCSL): Model-determined column-wise relevance. The relevance of each column is assessed without context about other columns and tables.
The output is a boolean flag per column indicating the relevance of each column.
This is considered a more cautious approach less likely to filter out relevant columns.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.ix2" style="list-style-type:none;padding-top:2.0pt;">
<span class="ltx_tag ltx_tag_item">-</span>
<div class="ltx_para" id="S4.I1.ix2.p1">
<p class="ltx_p" id="S4.I1.ix2.p1.1"><em class="ltx_emph ltx_font_italic" id="S4.I1.ix2.p1.1.1">Hybrid SCSL</em> (HySCSL): SCSL with added keyword matching aiming for higher SLR.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.ix3" style="list-style-type:none;padding-top:2.0pt;">
<span class="ltx_tag ltx_tag_item">-</span>
<div class="ltx_para" id="S4.I1.ix3.p1">
<p class="ltx_p" id="S4.I1.ix3.p1.1"><em class="ltx_emph ltx_font_italic" id="S4.I1.ix3.p1.1.1">Table-then-Column Schema Linking</em> (TCSL): Model-determined table-then-column filtering approach, as proposed in <cite class="ltx_cite ltx_citemacro_cite">Talaei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib24" title="">2024</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Pourreza &amp; Rafiei (<a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib19" title="">2023</a>)</cite>.
The model first filters the schema to the relevant tables, then filters the columns within those tables.
The output is a set of relevant columns and tables.
This is a more aggressive filtering approach.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.ix4" style="list-style-type:none;padding-top:2.0pt;">
<span class="ltx_tag ltx_tag_item">-</span>
<div class="ltx_para" id="S4.I1.ix4.p1">
<p class="ltx_p" id="S4.I1.ix4.p1.1"><em class="ltx_emph ltx_font_italic" id="S4.I1.ix4.p1.1.1">Hybrid TCSL</em> (HyTCSL): TCSL with added keyword matching aiming for higher SLR.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">In our implementation, SCSL and HySCSL use <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p3.1.1">GPT-4o-Mini</span> and TCSL and HyTCSL use <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p3.1.2">GPT-4o</span>.
We use <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p3.1.3">GPT-4o-Mini</span> with SCSL and HySCSL as <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p3.1.4">GPT-4o</span> shows a negligible gain in our experiments but is much cheaper.
The prompts used in our implementation are shown in
Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S4.F4" title="Figure 4 ‣ 4.2 Experiment 2: Impact of False Positives on Accuracy Given Actual SLR ‣ 4 Results ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.2">Table <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S4.T1" title="Table 1 ‣ 4.2 Experiment 2: Impact of False Positives on Accuracy Given Actual SLR ‣ 4 Results ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> reports the mean (± stddev) of the false positive rate (FPR) and the schema linking recall (SLR) across our four schema linking techniques and without schema filtering at all (full schema) from <math alttext="12" class="ltx_Math" display="inline" id="S4.SS2.p4.1.m1.1"><semantics id="S4.SS2.p4.1.m1.1a"><mn id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><cn id="S4.SS2.p4.1.m1.1.1.cmml" type="integer" xref="S4.SS2.p4.1.m1.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">12</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.1.m1.1d">12</annotation></semantics></math> different runs.
Table <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S4.T1" title="Table 1 ‣ 4.2 Experiment 2: Impact of False Positives on Accuracy Given Actual SLR ‣ 4 Results ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> shows that these approaches are robust across runs and that as FPR decreases, <span class="ltx_text ltx_font_italic" id="S4.SS2.p4.2.1">i.e.</span>, more irrelevant columns are filtered, more required columns can be filtered and SLR decreases.
SLR as obtained from schema linking represents an upper bound on possible EX, where <math alttext="100\%-SLR" class="ltx_Math" display="inline" id="S4.SS2.p4.2.m2.1"><semantics id="S4.SS2.p4.2.m2.1a"><mrow id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml"><mrow id="S4.SS2.p4.2.m2.1.1.2" xref="S4.SS2.p4.2.m2.1.1.2.cmml"><mn id="S4.SS2.p4.2.m2.1.1.2.2" xref="S4.SS2.p4.2.m2.1.1.2.2.cmml">100</mn><mo id="S4.SS2.p4.2.m2.1.1.2.1" xref="S4.SS2.p4.2.m2.1.1.2.1.cmml">%</mo></mrow><mo id="S4.SS2.p4.2.m2.1.1.1" xref="S4.SS2.p4.2.m2.1.1.1.cmml">−</mo><mrow id="S4.SS2.p4.2.m2.1.1.3" xref="S4.SS2.p4.2.m2.1.1.3.cmml"><mi id="S4.SS2.p4.2.m2.1.1.3.2" xref="S4.SS2.p4.2.m2.1.1.3.2.cmml">S</mi><mo id="S4.SS2.p4.2.m2.1.1.3.1" xref="S4.SS2.p4.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.p4.2.m2.1.1.3.3" xref="S4.SS2.p4.2.m2.1.1.3.3.cmml">L</mi><mo id="S4.SS2.p4.2.m2.1.1.3.1a" xref="S4.SS2.p4.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS2.p4.2.m2.1.1.3.4" xref="S4.SS2.p4.2.m2.1.1.3.4.cmml">R</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><apply id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1"><minus id="S4.SS2.p4.2.m2.1.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1.1"></minus><apply id="S4.SS2.p4.2.m2.1.1.2.cmml" xref="S4.SS2.p4.2.m2.1.1.2"><csymbol cd="latexml" id="S4.SS2.p4.2.m2.1.1.2.1.cmml" xref="S4.SS2.p4.2.m2.1.1.2.1">percent</csymbol><cn id="S4.SS2.p4.2.m2.1.1.2.2.cmml" type="integer" xref="S4.SS2.p4.2.m2.1.1.2.2">100</cn></apply><apply id="S4.SS2.p4.2.m2.1.1.3.cmml" xref="S4.SS2.p4.2.m2.1.1.3"><times id="S4.SS2.p4.2.m2.1.1.3.1.cmml" xref="S4.SS2.p4.2.m2.1.1.3.1"></times><ci id="S4.SS2.p4.2.m2.1.1.3.2.cmml" xref="S4.SS2.p4.2.m2.1.1.3.2">𝑆</ci><ci id="S4.SS2.p4.2.m2.1.1.3.3.cmml" xref="S4.SS2.p4.2.m2.1.1.3.3">𝐿</ci><ci id="S4.SS2.p4.2.m2.1.1.3.4.cmml" xref="S4.SS2.p4.2.m2.1.1.3.4">𝑅</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">100\%-SLR</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.2.m2.1d">100 % - italic_S italic_L italic_R</annotation></semantics></math> represents the ratio of queries that will fail in the generation stage due to missing required columns.</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">We run the same simplified pipeline as our first experiment:
a retrieval stage containing one of the five schema linking approaches in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S4.T1" title="Table 1 ‣ 4.2 Experiment 2: Impact of False Positives on Accuracy Given Actual SLR ‣ 4 Results ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> followed by a zero-shot generation stage.
We do so across the <math alttext="12" class="ltx_Math" display="inline" id="S4.SS2.p5.1.m1.1"><semantics id="S4.SS2.p5.1.m1.1a"><mn id="S4.SS2.p5.1.m1.1.1" xref="S4.SS2.p5.1.m1.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.1.m1.1b"><cn id="S4.SS2.p5.1.m1.1.1.cmml" type="integer" xref="S4.SS2.p5.1.m1.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.1.m1.1c">12</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p5.1.m1.1d">12</annotation></semantics></math> selected models (§<a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S3.SS2" title="3.2 Models ‣ 3 Experimental Setup ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_tag">3.2</span></a>) and track the associated EX.
Fig. <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S4.F5" title="Figure 5 ‣ 4.2 Experiment 2: Impact of False Positives on Accuracy Given Actual SLR ‣ 4 Results ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_tag">5</span></a> shows five EX data points
given the FPR of the five different schema linking approaches.
We interpolate between two adjacent (FPR, EX) data points and show EX as a solid line.
We also add the idealized EX from Experiment 1 as a dashed line.
The difference between the two lines shows the EX difference due to changes in SLR.</p>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.1">We observe three different classes of models in our results: models where some schema linking method improves performance (<span class="ltx_text ltx_font_italic" id="S4.SS2.p6.1.1">e.g.</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p6.1.2">Llama 3.1-8b</span>), models where all schema linking methods degrade performance (<span class="ltx_text ltx_font_italic" id="S4.SS2.p6.1.3">e.g.</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p6.1.4">Gemini 1.5 Pro</span>), and models where schema linking has only negligible impact on performance (<span class="ltx_text ltx_font_italic" id="S4.SS2.p6.1.5">e.g.</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p6.1.6">GPT-4o-Mini</span>).
A model falls into one of these three buckets based on its SQL generation capability.
More capable models – e.g. <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p6.1.7">Gemini 1.5 Pro</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p6.1.8">ft:GPT-4o</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p6.1.9">Llama-3.1-405b</span> – end up with a reduction in EX. Less capable models – e.g. <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p6.1.10">Llama 3.1-8b</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p6.1.11">Mixtral-8x22b</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p6.1.12">Deepseek Coder-V2</span> – end up with a gain in EX.</p>
</div>
<div class="ltx_para" id="S4.SS2.p7">
<p class="ltx_p" id="S4.SS2.p7.1"><em class="ltx_emph ltx_font_italic" id="S4.SS2.p7.1.1">Empirical Observation:</em>
As the model’s SQL generation capability improves, the benefit of schema linking diminishes. In some cases, this can result in a net reduction in accuracy due to missing required columns for generation.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.6.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.6.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.6.1.1.1.1" style="background-color:#F1F1F7;">Approach</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.6.1.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.6.1.1.2.1" style="background-color:#F1F1F7;">FPR</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.6.1.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.6.1.1.3.1" style="background-color:#F1F1F7;">SLR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.6.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T1.6.2.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Without schema filtering (Full Schema)</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.6.2.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">94.62</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.6.2.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">100.00</td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.3.2" style="background-color:#F7F7F7;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.6.3.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S4.T1.6.3.2.1.1" style="background-color:#F7F7F7;">Hybrid Single-Column Schema Linking (HySCSL)</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.6.3.2.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S4.T1.6.3.2.2.1" style="background-color:#F7F7F7;">82.08 ± 0.44</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.3.2.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S4.T1.6.3.2.3.1" style="background-color:#F7F7F7;">90.36 ± 0.78</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.6.4.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Single-Column Schema Linking (SCSL)</th>
<td class="ltx_td ltx_align_center" id="S4.T1.6.4.3.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">67.23 ± 0.92</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.4.3.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">88.77 ± 0.75</td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.5.4" style="background-color:#F7F7F7;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.6.5.4.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S4.T1.6.5.4.1.1" style="background-color:#F7F7F7;">Hybrid Table-to-Column Schema Linking (HyTCSL)</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.6.5.4.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S4.T1.6.5.4.2.1" style="background-color:#F7F7F7;">19.85 ± 0.99</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.5.4.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S4.T1.6.5.4.3.1" style="background-color:#F7F7F7;">83.00 ± 0.92</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.6.6.5.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Table-to-Column Schema Linking (TCSL)</th>
<td class="ltx_td ltx_align_center" id="S4.T1.6.6.5.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">9.79 ± 0.73</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.6.5.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">77.44 ± 1.34</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.7.3.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.4.2" style="font-size:90%;">The mean (± stddev) false positive rate (FPR) and schema linking recall (SLR), across <math alttext="12" class="ltx_Math" display="inline" id="S4.T1.3.1.m1.1"><semantics id="S4.T1.3.1.m1.1b"><mn id="S4.T1.3.1.m1.1.1" xref="S4.T1.3.1.m1.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S4.T1.3.1.m1.1c"><cn id="S4.T1.3.1.m1.1.1.cmml" type="integer" xref="S4.T1.3.1.m1.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.1.m1.1d">12</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.1.m1.1e">12</annotation></semantics></math> runs, associated with five schema linking approaches.
The approaches are sorted in descending order of their ability to reduce false positives.
We report the mean values (± stddev) from <math alttext="12" class="ltx_Math" display="inline" id="S4.T1.4.2.m2.1"><semantics id="S4.T1.4.2.m2.1b"><mn id="S4.T1.4.2.m2.1.1" xref="S4.T1.4.2.m2.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S4.T1.4.2.m2.1c"><cn id="S4.T1.4.2.m2.1.1.cmml" type="integer" xref="S4.T1.4.2.m2.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.2.m2.1d">12</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.2.m2.1e">12</annotation></semantics></math> runs.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_align_center ltx_figure_panel ltx_align_center" id="S4.F5.1"><span class="ltx_text" id="S4.F5.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="409" id="S4.F5.1.1.g1" src="x6.png" width="831"/></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_align_center ltx_figure_panel ltx_align_center" id="S4.F5.2"><span class="ltx_text" id="S4.F5.2.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="156" id="S4.F5.2.1.g1" src="x7.png" width="710"/></span></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.4.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.5.2" style="font-size:90%;">The execution accuracy (EX) given different schema linking and idealized EX, assuming perfect Schema Linking Recall (SLR)
as the false positive rate (FPR) varies.
EX as a solid line and idealized EX as a dashed line. </span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Experiment 3: Impact of Non-Filtering Stages and Techniques</h3>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.17">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.17.18.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T2.17.18.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S4.T2.17.18.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.17.18.1.2.1" style="background-color:#F1F1F7;">Execution Accuracy (EX)</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.17.19.2" style="background-color:#F1F1F7;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S4.T2.17.19.2.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.17.19.2.1.1" style="background-color:#F1F1F7;">Method</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T2.17.19.2.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.17.19.2.2.1" style="background-color:#F1F1F7;">ft:GPT-4o</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T2.17.19.2.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.17.19.2.3.1" style="background-color:#F1F1F7;">Gemini 1.5 Pro</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T2.17.19.2.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.17.19.2.4.1" style="background-color:#F1F1F7;">Llama 3.1-405b</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.17.20.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T2.17.20.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">Full pipeline</th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.17.20.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">67.35</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.17.20.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">60.54</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.17.20.1.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">59.18</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2" style="background-color:#F7F7F7;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.2.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S4.T2.2.2.3.1" style="background-color:#F7F7F7;">w/o Augmentation</span></th>
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S4.T2.1.1.1.1" style="background-color:#F7F7F7;">64.63 (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.m1.1.1" mathbackground="#F7F7F7" stretchy="false" xref="S4.T2.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.m1.1d">↓</annotation></semantics></math> 2.72)</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.2.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S4.T2.2.2.4.1" style="background-color:#F7F7F7;">60.54</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.2.2.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S4.T2.2.2.2.1" style="background-color:#F7F7F7;">59.86 (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.2.2.2.1.m1.1"><semantics id="S4.T2.2.2.2.1.m1.1a"><mo id="S4.T2.2.2.2.1.m1.1.1" mathbackground="#F7F7F7" stretchy="false" xref="S4.T2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.1.m1.1b"><ci id="S4.T2.2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.1.m1.1d">↑</annotation></semantics></math> 0.68)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.5.5.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">w/o Selection</th>
<td class="ltx_td ltx_align_left" id="S4.T2.3.3.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">65.31 (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.3.3.1.m1.1"><semantics id="S4.T2.3.3.1.m1.1a"><mo id="S4.T2.3.3.1.m1.1.1" stretchy="false" xref="S4.T2.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.1.m1.1b"><ci id="S4.T2.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.1.m1.1d">↓</annotation></semantics></math> 2.04)</td>
<td class="ltx_td ltx_align_left" id="S4.T2.4.4.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">57.82 (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.4.4.2.m1.1"><semantics id="S4.T2.4.4.2.m1.1a"><mo id="S4.T2.4.4.2.m1.1.1" stretchy="false" xref="S4.T2.4.4.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.2.m1.1b"><ci id="S4.T2.4.4.2.m1.1.1.cmml" xref="S4.T2.4.4.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.2.m1.1d">↓</annotation></semantics></math> 2.72)</td>
<td class="ltx_td ltx_align_left" id="S4.T2.5.5.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">58.50 (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.5.5.3.m1.1"><semantics id="S4.T2.5.5.3.m1.1a"><mo id="S4.T2.5.5.3.m1.1.1" stretchy="false" xref="S4.T2.5.5.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.3.m1.1b"><ci id="S4.T2.5.5.3.m1.1.1.cmml" xref="S4.T2.5.5.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.3.m1.1d">↓</annotation></semantics></math> 0.68)</td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.8" style="background-color:#F7F7F7;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.8.8.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S4.T2.8.8.4.1" style="background-color:#F7F7F7;">w/o Correction</span></th>
<td class="ltx_td ltx_align_left" id="S4.T2.6.6.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S4.T2.6.6.1.1" style="background-color:#F7F7F7;">65.99 (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.6.6.1.1.m1.1"><semantics id="S4.T2.6.6.1.1.m1.1a"><mo id="S4.T2.6.6.1.1.m1.1.1" mathbackground="#F7F7F7" stretchy="false" xref="S4.T2.6.6.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.1.1.m1.1b"><ci id="S4.T2.6.6.1.1.m1.1.1.cmml" xref="S4.T2.6.6.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.6.1.1.m1.1d">↓</annotation></semantics></math> 1.36)</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.7.7.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S4.T2.7.7.2.1" style="background-color:#F7F7F7;">57.14 (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.7.7.2.1.m1.1"><semantics id="S4.T2.7.7.2.1.m1.1a"><mo id="S4.T2.7.7.2.1.m1.1.1" mathbackground="#F7F7F7" stretchy="false" xref="S4.T2.7.7.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.2.1.m1.1b"><ci id="S4.T2.7.7.2.1.m1.1.1.cmml" xref="S4.T2.7.7.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.7.2.1.m1.1d">↓</annotation></semantics></math> 3.40)</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.8.8.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S4.T2.8.8.3.1" style="background-color:#F7F7F7;">55.78 (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.8.8.3.1.m1.1"><semantics id="S4.T2.8.8.3.1.m1.1a"><mo id="S4.T2.8.8.3.1.m1.1.1" mathbackground="#F7F7F7" stretchy="false" xref="S4.T2.8.8.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.3.1.m1.1b"><ci id="S4.T2.8.8.3.1.m1.1.1.cmml" xref="S4.T2.8.8.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.8.3.1.m1.1d">↓</annotation></semantics></math> 3.40)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.11.11.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">w/ TCSL</th>
<td class="ltx_td ltx_align_left" id="S4.T2.9.9.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">62.58 (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.9.9.1.m1.1"><semantics id="S4.T2.9.9.1.m1.1a"><mo id="S4.T2.9.9.1.m1.1.1" stretchy="false" xref="S4.T2.9.9.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.1.m1.1b"><ci id="S4.T2.9.9.1.m1.1.1.cmml" xref="S4.T2.9.9.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.9.9.1.m1.1d">↓</annotation></semantics></math> 4.77)</td>
<td class="ltx_td ltx_align_left" id="S4.T2.10.10.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">55.78 (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.10.10.2.m1.1"><semantics id="S4.T2.10.10.2.m1.1a"><mo id="S4.T2.10.10.2.m1.1.1" stretchy="false" xref="S4.T2.10.10.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.2.m1.1b"><ci id="S4.T2.10.10.2.m1.1.1.cmml" xref="S4.T2.10.10.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.10.10.2.m1.1d">↓</annotation></semantics></math> 4.76)</td>
<td class="ltx_td ltx_align_left" id="S4.T2.11.11.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">56.46 (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.11.11.3.m1.1"><semantics id="S4.T2.11.11.3.m1.1a"><mo id="S4.T2.11.11.3.m1.1.1" stretchy="false" xref="S4.T2.11.11.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.11.11.3.m1.1b"><ci id="S4.T2.11.11.3.m1.1.1.cmml" xref="S4.T2.11.11.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.11.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.11.11.3.m1.1d">↓</annotation></semantics></math> 2.72)</td>
</tr>
<tr class="ltx_tr" id="S4.T2.14.14" style="background-color:#F7F7F7;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.14.14.4" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S4.T2.14.14.4.1" style="background-color:#F7F7F7;">w/ SCSL</span></th>
<td class="ltx_td ltx_align_left" id="S4.T2.12.12.1" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S4.T2.12.12.1.1" style="background-color:#F7F7F7;">55.78 (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.12.12.1.1.m1.1"><semantics id="S4.T2.12.12.1.1.m1.1a"><mo id="S4.T2.12.12.1.1.m1.1.1" mathbackground="#F7F7F7" stretchy="false" xref="S4.T2.12.12.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.12.12.1.1.m1.1b"><ci id="S4.T2.12.12.1.1.m1.1.1.cmml" xref="S4.T2.12.12.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.12.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.12.12.1.1.m1.1d">↓</annotation></semantics></math> 11.57)</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.13.13.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S4.T2.13.13.2.1" style="background-color:#F7F7F7;">55.10 (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.13.13.2.1.m1.1"><semantics id="S4.T2.13.13.2.1.m1.1a"><mo id="S4.T2.13.13.2.1.m1.1.1" mathbackground="#F7F7F7" stretchy="false" xref="S4.T2.13.13.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.13.13.2.1.m1.1b"><ci id="S4.T2.13.13.2.1.m1.1.1.cmml" xref="S4.T2.13.13.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.13.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.13.13.2.1.m1.1d">↓</annotation></semantics></math> 5.44)</span></td>
<td class="ltx_td ltx_align_left" id="S4.T2.14.14.3" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S4.T2.14.14.3.1" style="background-color:#F7F7F7;">54.42 (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.14.14.3.1.m1.1"><semantics id="S4.T2.14.14.3.1.m1.1a"><mo id="S4.T2.14.14.3.1.m1.1.1" mathbackground="#F7F7F7" stretchy="false" xref="S4.T2.14.14.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.14.14.3.1.m1.1b"><ci id="S4.T2.14.14.3.1.m1.1.1.cmml" xref="S4.T2.14.14.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.14.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.14.14.3.1.m1.1d">↓</annotation></semantics></math> 4.76)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.17.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.17.17.4" style="padding-top:1.5pt;padding-bottom:1.5pt;">Base model</th>
<td class="ltx_td ltx_align_left" id="S4.T2.15.15.1" style="padding-top:1.5pt;padding-bottom:1.5pt;">59.18 (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.15.15.1.m1.1"><semantics id="S4.T2.15.15.1.m1.1a"><mo id="S4.T2.15.15.1.m1.1.1" stretchy="false" xref="S4.T2.15.15.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.15.15.1.m1.1b"><ci id="S4.T2.15.15.1.m1.1.1.cmml" xref="S4.T2.15.15.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.15.15.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.15.15.1.m1.1d">↓</annotation></semantics></math> 8.17)</td>
<td class="ltx_td ltx_align_left" id="S4.T2.16.16.2" style="padding-top:1.5pt;padding-bottom:1.5pt;">57.82 (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.16.16.2.m1.1"><semantics id="S4.T2.16.16.2.m1.1a"><mo id="S4.T2.16.16.2.m1.1.1" stretchy="false" xref="S4.T2.16.16.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.16.16.2.m1.1b"><ci id="S4.T2.16.16.2.m1.1.1.cmml" xref="S4.T2.16.16.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.16.16.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.16.16.2.m1.1d">↓</annotation></semantics></math> 2.72)</td>
<td class="ltx_td ltx_align_left" id="S4.T2.17.17.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">53.74 (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.17.17.3.m1.1"><semantics id="S4.T2.17.17.3.m1.1a"><mo id="S4.T2.17.17.3.m1.1.1" stretchy="false" xref="S4.T2.17.17.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.17.17.3.m1.1b"><ci id="S4.T2.17.17.3.m1.1.1.cmml" xref="S4.T2.17.17.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.17.17.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.17.17.3.m1.1d">↓</annotation></semantics></math> 5.44)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.23.3.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.21.2" style="font-size:90%;">Ablation of different methods reporting the execution accuracy (EX) for fine-tuned GPT-4o, Gemini 1.5 Pro, and Llama 3.1-405b. The table compares the full pipeline to variations without augmentation, selection, and correction techniques; with Table-to-Column Schema Linking (TCSL) and Single-Column Schema Linking (SCSL); and as base model performance. Reductions (<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.20.1.m1.1"><semantics id="S4.T2.20.1.m1.1b"><mo id="S4.T2.20.1.m1.1.1" stretchy="false" xref="S4.T2.20.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.20.1.m1.1c"><ci id="S4.T2.20.1.m1.1.1.cmml" xref="S4.T2.20.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.20.1.m1.1d">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.20.1.m1.1e">↓</annotation></semantics></math>) or increases (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.21.2.m2.1"><semantics id="S4.T2.21.2.m2.1b"><mo id="S4.T2.21.2.m2.1.1" stretchy="false" xref="S4.T2.21.2.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.21.2.m2.1c"><ci id="S4.T2.21.2.m2.1.1.cmml" xref="S4.T2.21.2.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.21.2.m2.1d">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.21.2.m2.1e">↑</annotation></semantics></math>) in accuracy compared to the full pipeline are indicated.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Instead of filtering contextual information through schema linking, we focus on techniques that preserve information.
We assess the gains of using <em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.1.1">augmentation</em> and <em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.1.2">selection</em> techniques as well as adding a correction stage, which are detailed as follows:</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">-</span>
<div class="ltx_para" id="S4.I2.ix1.p1">
<p class="ltx_p" id="S4.I2.ix1.p1.1"><em class="ltx_emph ltx_font_italic" id="S4.I2.ix1.p1.1.1">Augmentation:</em> We add contextual information by:
(i) expanding column descriptions and add query hints and
(ii) adding structural expectations of the output, <span class="ltx_text ltx_font_italic" id="S4.I2.ix1.p1.1.2">e.g.</span>, expected orderings and aggregations, using CoT planning.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.ix2" style="list-style-type:none;padding-top:2.0pt;">
<span class="ltx_tag ltx_tag_item">-</span>
<div class="ltx_para" id="S4.I2.ix2.p1">
<p class="ltx_p" id="S4.I2.ix2.p1.1"><em class="ltx_emph ltx_font_italic" id="S4.I2.ix2.p1.1.1">Correction:</em>
After generating a candidate SQL query, we iteratively apply corrections through re-generation based on
database execution errors <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib27" title="">2018b</a>)</cite>,
revision through database administrator instructions <cite class="ltx_cite ltx_citemacro_citep">(Talaei et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib24" title="">2024</a>)</cite>, and
model-based feedback similar to Reflexion <cite class="ltx_cite ltx_citemacro_citep">(Shinn et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib23" title="">2023</a>)</cite>.
We use these corrections to generate instructions to also augment contextual information.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.ix3" style="list-style-type:none;padding-top:2.0pt;">
<span class="ltx_tag ltx_tag_item">-</span>
<div class="ltx_para" id="S4.I2.ix3.p1">
<p class="ltx_p" id="S4.I2.ix3.p1.1"><em class="ltx_emph ltx_font_italic" id="S4.I2.ix3.p1.1.1">Selection:</em> We use self-consistency <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#bib.bib28" title="">2023</a>)</cite> to generate multiple responses and select the <em class="ltx_emph ltx_font_italic" id="S4.I2.ix3.p1.1.2">most consistent</em> result.
We use selection across the whole pipeline for augmentation, SQL generation, and SQL correction.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">We implemented a full pipeline using zero-shot generation.
The pipeline contained augmentation, correction and selection as described above and no schema linking, <span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.1">i.e.</span>, always providing the full schema.
We ran an ablation to understand the relative impact of each technique or stage using the three top performing models from Experiment 1:
<span class="ltx_text ltx_font_typewriter" id="S4.SS3.p3.1.2">Llama 3.1-405b</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p3.1.3">Gemini 1.5 Pro</span>, and <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p3.1.4">ft:GPT-4o</span>.
Table <a class="ltx_ref" href="https://arxiv.org/html/2408.07702v2#S4.T2" title="Table 2 ‣ 4.3 Experiment 3: Impact of Non-Filtering Stages and Techniques ‣ 4 Results ‣ The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> shows the execution accuracy (EX) of the full pipeline and <math alttext="6" class="ltx_Math" display="inline" id="S4.SS3.p3.1.m1.1"><semantics id="S4.SS3.p3.1.m1.1a"><mn id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><cn id="S4.SS3.p3.1.m1.1.1.cmml" type="integer" xref="S4.SS3.p3.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">6</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.1.m1.1d">6</annotation></semantics></math> other variations:
without augmentation, correction, or selection,
with schema linking (TCSL and SCSL from Experiment 2),
and without any of these techniques <span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.5">i.e.</span>, providing the full schema alone to a base model.
We find that all techniques improve accuracy to varying degrees except schema linking with a noticeable difference when evaluating the full pipeline.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1"><em class="ltx_emph ltx_font_italic" id="S4.SS3.p4.1.1">Empirical Observation:</em> Each of augmentation, selection, and correction have a noticeable positive impact on generation accuracy.
Note that even though base models such as <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p4.1.2">Gemini 1.5 Pro</span> may show comparable performance to a fine-tuned <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p4.1.3">GPT-4o</span> in SQL generation,
they differ when evaluated within end-to-end Text-to-SQL pipelines.
For instance, augmentation leads to major benefits with <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p4.1.4">GPT-4o</span> when compared with <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p4.1.5">Gemini 1.5 Pro</span>. An interesting research direction is understanding whether the relative benefits from augmentation between models hold across other tasks as well.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Discussion and Proposed Approach</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Our empirical observations reveal several key insights that guide our proposed approach.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">First, as a model’s SQL generation capability improves, its ability to retrieve relevant schema elements from a full schema in its input context also improves.
As such, schema linking for state-of-the-art LLMs is less important if the schema fully fits within the context window.
However, it is still helpful for models with lower SQL generation accuracy as they struggle with false positives.
In our approach, <em class="ltx_emph ltx_font_italic" id="S4.SS4.p2.1.1">we maximize the use of the LLM context window to minimize filtering required columns</em>.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">Second, we find that combining augmentation, selection, and correction techniques heavily impacts the accuracy of a Text-to-SQL pipeline.
However, the impact is not the same when evaluated in an end-to-end fashion even when comparing models with similar generation capability.
In our approach, <em class="ltx_emph ltx_font_italic" id="S4.SS4.p3.1.1">we adopt augmentation, selection, and correction as detailed from the implementation in Experiment 3</em>.
We further use <span class="ltx_text ltx_font_typewriter" id="S4.SS4.p3.1.2">ft:GPT-4o</span> as the model of choice for generation since it provides the best end-to-end accuracy and makes the best use of augmentation.</p>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1">These design choices yield an approach that ranks first on execution accuracy and second on
valid efficiency score on the BIRD benchmark.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Is it the death of schema linking? For state-of-the-art models, if the schema fits within the context length – yes. However, for smaller or prior generation models, the accuracy gains from schema linking often justify the potential loss. Additionally, in real-world data-warehousing scenarios, the entire schema often exceeds the context window, requiring a multi-stage information retrieval pipeline. In such cases, we argue for maximally using the LLM context window in picking the top-K relevant columns to retain the necessary schema elements.
We conclude that while the need for schema linking is highly use-case dependent, its importance is diminishing as costs decrease, context windows widen, and generation capabilities improve.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We thank Jinyang Li, Yongbin Li and the rest of the BIRD team for evaluating our approach on the BIRD test set. We also thank John Allard for valuable insights regarding fine-tuning.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Androutsopoulos et al. (1995)</span>
<span class="ltx_bibblock">
I. Androutsopoulos, G. D. Ritchie, and P. Thanisch.

</span>
<span class="ltx_bibblock">Natural language interfaces to databases - an introduction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">CoRR</em>, abs/cmp-lg/9503016, 1995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Askari et al. (2024)</span>
<span class="ltx_bibblock">
Arian Askari, Christian Poelitz, and Xinye Tang.

</span>
<span class="ltx_bibblock">Magic: Generating self-correction guideline for in-context text-to-sql.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">CoRR</em>, abs/2406.12692, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. (2023)</span>
<span class="ltx_bibblock">
Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, lu Chen, Jinshu Lin, and Dongfang Lou.

</span>
<span class="ltx_bibblock">C3: Zero-shot text-to-sql with chatgpt.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">CoRR</em>, abs/2307.07306, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Floratou et al. (2024)</span>
<span class="ltx_bibblock">
Avrilia Floratou, Fotis Psallidas, Fuheng Zhao, Shaleen Deep, Gunther Hagleither, Wangda Tan, Joyce Cahoon, Rana Alotaibi, Jordan Henkel, Abhik Singla, Alex Van Grootel, Brandon Chow, Kai Deng, Katherine Lin, Marcos Campos, K. Venkatesh Emani, Vivek Pandit, Victor Shnayder, Wenjing Wang, and Carlo Curino.

</span>
<span class="ltx_bibblock">Nl2sql is a solved problem… not!

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Conference on Innovative Data Systems Research</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023)</span>
<span class="ltx_bibblock">
Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou.

</span>
<span class="ltx_bibblock">Text-to-sql empowered by large language models: A benchmark evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">CoRR</em>, abs/2308.15363, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gregory Kamradt,  (2023)</span>
<span class="ltx_bibblock">
Gregory Kamradt, 2023, 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack/blob/main/README.md" title="">https://github.com/gkamradt/LLMTest_NeedleInAHaystack/blob/main/README.md</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2019)</span>
<span class="ltx_bibblock">
Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-Guang Lou, Ting Liu, and Dongmei Zhang.

</span>
<span class="ltx_bibblock">Towards complex text-to-sql in cross-domain database with intermediate representation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">CoRR</em>, abs/1905.08205, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2019)</span>
<span class="ltx_bibblock">
Pengcheng He, Yi Mao, Kaushik Chakrabarti, and Weizhu Chen.

</span>
<span class="ltx_bibblock">X-sql: reinforce schema representation with context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">CoRR</em>, abs/1908.08113, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hong et al. (2024)</span>
<span class="ltx_bibblock">
Zijin Hong, Zheng Yuan, Qinggang Zhang, Hao Chen, Junnan Dong, Feiran Huang, and Xiao Huang.

</span>
<span class="ltx_bibblock">Next-generation database interfaces: A survey of llm-based text-to-sql.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">CoRR</em>, abs/2406.08426, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laban et al. (2024)</span>
<span class="ltx_bibblock">
Philippe Laban, Alexander R. Fabbri, Caiming Xiong, and Chien-Sheng Wu.

</span>
<span class="ltx_bibblock">Summary of a haystack: A challenge to long-context llms and rag systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">CoRR</em>, abs/2407.01370, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2024)</span>
<span class="ltx_bibblock">
Dongjun Lee, Choongwon Park, Jaehyuk Kim, and Heesoo Park.

</span>
<span class="ltx_bibblock">Mcs-sql: Leveraging multiple prompts and multiple-choice selection for text-to-sql generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">CoRR</em>, abs/2405.07467, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024a)</span>
<span class="ltx_bibblock">
Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li, and Nan Tang.

</span>
<span class="ltx_bibblock">The dawn of natural language to sql: Are we fully ready?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">CoRR</em>, abs/2406.01265, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin C. C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li.

</span>
<span class="ltx_bibblock">Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">CoRR</em>, abs/2305.03111, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024b)</span>
<span class="ltx_bibblock">
Mo Li, Songyang Zhang, Yunxin Liu, and Kai Chen.

</span>
<span class="ltx_bibblock">Needlebench: Can llms do retrieval and reasoning in 1 million context window?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">CoRR</em>, abs/2407.11963, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2020)</span>
<span class="ltx_bibblock">
Xi Victoria Lin, Richard Socher, and Caiming Xiong.

</span>
<span class="ltx_bibblock">Bridging textual and tabular data for cross-domain text-to-sql semantic parsing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">CoRR</em>, abs/2012.12627, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024)</span>
<span class="ltx_bibblock">
Xinyu Liu, Shuyu Shen, Boyan Li, Peixian Ma, Runzhi Jiang, Yuyu Luo, Yuxin Zhang, Ju Fan, Guoliang Li, and Nan Tang.

</span>
<span class="ltx_bibblock">A survey of nl2sql with large language models: Where are we, and where are we going?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">CoRR</em>, abs/2408.05109, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. (2020)</span>
<span class="ltx_bibblock">
Qin Lyu, Kaushik Chakrabarti, Shobhit Hathi, Souvik Kundu, Jianwen Zhang, and Zheng Chen.

</span>
<span class="ltx_bibblock">Hybrid ranking network for text-to-sql.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">CoRR</em>, abs/2008.04759, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maamari &amp; Mhedhbi (2024)</span>
<span class="ltx_bibblock">
Karime Maamari and Amine Mhedhbi.

</span>
<span class="ltx_bibblock">End-to-end text-to-sql generation within an analytics insight engine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">CoRR</em>, abs/2406.12104, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pourreza &amp; Rafiei (2023)</span>
<span class="ltx_bibblock">
Mohammadreza Pourreza and Davood Rafiei.

</span>
<span class="ltx_bibblock">Din-sql: Decomposed in-context learning of text-to-sql with self-correction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">CoRR</em>, abs/2304.11015, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu et al. (2024)</span>
<span class="ltx_bibblock">
Ge Qu, Jinyang Li, Bowen Li, Bowen Qin, Nan Huo, Chenhao Ma, and Reynold Cheng.

</span>
<span class="ltx_bibblock">Before generation, align it! a novel and effective strategy for mitigating hallucinations in text-to-sql generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">CoRR</em>, abs/2405.15307, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Quamar et al. (2022)</span>
<span class="ltx_bibblock">
Abdul Quamar, Vasilis Efthymiou, Chuan Lei, and Fatma Özcan.

</span>
<span class="ltx_bibblock">Natural language interfaces to data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Found. Trends Databases</em>, 11(4):319–414, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid et al. (2024)</span>
<span class="ltx_bibblock">
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, and et al.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">CoRR</em>, abs/2403.05530, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shinn et al. (2023)</span>
<span class="ltx_bibblock">
Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.

</span>
<span class="ltx_bibblock">Reflexion: Language agents with verbal reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">CoRR</em>, abs/2303.11366, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talaei et al. (2024)</span>
<span class="ltx_bibblock">
Shayan Talaei, Mohammadreza Pourreza, Yu-Chen Chang, Azalia Mirhoseini, and Amin Saberi.

</span>
<span class="ltx_bibblock">Chess: Contextual harnessing for efficient sql synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">CoRR</em>, abs/2405.16755, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024)</span>
<span class="ltx_bibblock">
Bing Wang, Changyu Ren, Jian Yang, Xinnian Liang, Jiaqi Bai, Linzheng Chai, Zhao Yan, Qian-Wen Zhang, Di Yin, Xing Sun, and Zhoujun Li.

</span>
<span class="ltx_bibblock">Mac-sql: A multi-agent collaborative framework for text-to-sql.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">CoRR</em>, abs/2312.11242, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2018a)</span>
<span class="ltx_bibblock">
Chenglong Wang, Kedar Tatwawadi, Marc Brockschmidt, Po-Sen Huang, Yi Mao, Oleksandr Polozov, and Rishabh Singh.

</span>
<span class="ltx_bibblock">Robust text-to-sql generation with execution-guided decoding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">CoRR</em>, abs/1807.03100, 2018a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2018b)</span>
<span class="ltx_bibblock">
Chenglong Wang, Kedar Tatwawadi, Marc Brockschmidt, Po-Sen Huang, Yi Mao, Oleksandr Polozov, and Rishabh Singh.

</span>
<span class="ltx_bibblock">Robust text-to-sql generation with execution-guided decoding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">CoRR</em>, abs/1807.03100, 2018b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.

</span>
<span class="ltx_bibblock">Self-consistency improves chain of thought reasoning in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">CoRR</em>, abs/2203.11171, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2023)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">CoRR</em>, abs/2201.11903, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Weixu Zhang, Yifei Wang, Yuanfeng Song, Victor Junqiu Wei, Yuxing Tian, Yiyan Qi, Jonathan H. Chan, Raymond Chi-Wing Wong, and Haiqin Yang.

</span>
<span class="ltx_bibblock">Natural language interfaces for tabular data querying and visualization: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">CoRR</em>, abs/2310.17894, 2024.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Aug 18 19:03:50 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
