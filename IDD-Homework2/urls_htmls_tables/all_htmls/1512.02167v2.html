<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1512.02167] Simple Baseline for Visual Question Answering</title><meta property="og:description" content="We describe a very simple bag-of-words baseline for visual question answering. This baseline concatenates the word features from the question and CNN features from the image to predict the answer. When evaluated on the…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Simple Baseline for Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Simple Baseline for Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1512.02167">

<!--Generated on Fri Mar  1 17:09:54 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Simple Baseline for Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bolei Zhou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Massachusetts Institute of Technology
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuandong Tian
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Facebook AI Research
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sainbayar Sukhbaatar
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Facebook AI Research
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Arthur Szlam
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Facebook AI Research
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rob Fergus
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Facebook AI Research
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">We describe a very simple bag-of-words baseline for visual question answering. This baseline concatenates the word features from the question and CNN features from the image to predict the answer. When evaluated on the challenging VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, it shows comparable performance to many recent approaches using recurrent neural networks. To explore the strength and weakness of the trained model, we also provide an interactive web demo<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="http://visualqa.csail.mit.edu" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://visualqa.csail.mit.edu</a></span></span></span>, and open-source code<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/metalbubble/VQAbaseline" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/metalbubble/VQAbaseline</a></span></span></span>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Combining Natural Language Processing with Computer Vision for high-level scene interpretation is a recent trend, e.g., image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. These works have
benefited from the rapid development of deep learning for visual recognition (object recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and scene recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>), and have been made possible by the emergence of large image datasets and text corpus (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>). Beyond image captioning, a natural next step is visual question answering (QA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Compared with the image captioning task, in which an algorithm is required to generate free-form text description for a given image, visual QA can involve a wider range of knowledge and reasoning skills. A captioning algorithm has the liberty to pick the easiest relevant descriptions of the image, whereas as responding to a question needs to find the correct answer for *that* question. Furthermore, the algorithms for visual QA are required to answer all kinds of questions people might ask about the image, some of which might be relevant to the image contents, such as “what books are under the television” and “what is the color of the boat”, while others might require knowledge or reasoning beyond the image content, such as “why is the baby crying?” and “which chair is the most expensive?”. Building robust algorithms for visual QA that perform at near human levels would be an important step towards solving AI.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">Recently, several papers have appeared on arXiv (after CVPR’16 submission deadline) proposing neural network architectures for visual question answering, such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Some of them are derived from the image captioning framework, in which the output of a recurrent neural network (e.g., LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>) applied to the question sentence is concatenated with visual features from VGG or other CNNs to feed a classifier to predict the answer. Other models integrate visual attention mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and visualize how the network learns to attend the local image regions relevant to the content of the question.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">Interestingly, we notice that in one of the earliest VQA papers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, the simple baseline Bag-of-words + image feature (referred to as BOWIMG baseline) outperforms the LSTM-based models on a synthesized visual QA dataset built up on top of the image captions of COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. For the recent much larger COCO VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, the BOWIMG baseline performs worse than the LSTM-based models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">In this work, we carefully implement the BOWIMG baseline model. We call it iBOWIMG to avoid confusion with the implementation in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. With proper setup and training, this simple baseline model shows comparable performance to many recent recurrent network-based approaches for visual QA. Further analysis shows that the baseline learns to correlate the informative words in the question sentence and visual concepts in the image with the answer. Furthermore, such correlations can be used to compute reasonable spatial attention map with the help of the CAM technique proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. The source code and the visual QA demo based on the trained model are publicly available. In the demo, iBOWIMG baseline gives answers to any question relevant to the given images. Playing with the visual QA models interactively could reveal the strengths and weakness of the trained model.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>iBOWIMG for Visual Question Answering</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">In most of the recent proposed models, visual QA is simplified to a classification task: the number of the different answers in the training set is the number of the final classes the models need to learn to predict. The general pipeline of those models is that the word feature extracted from the question sentence is concatenated with the visual feature extracted from the image, then they are fed into a softmax layer to predict the answer class. The visual feature is usually taken from the top of the VGG network or GoogLeNet, while the word features of the question sentence are usually the popular LSTM-based features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p">In our iBOWIMG model, we simply use naive bag-of-words as the text feature, and use the deep features from GoogLeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> as the visual features. Figure <a href="#S2.F1" title="Figure 1 ‣ 2 iBOWIMG for Visual Question Answering ‣ Simple Baseline for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the framework of the iBOWIMG model, which can be implemented in Torch with no more than 10 lines of code. The input question is first converted to a one-hot vector, which is transformed to word feature via a word embedding layer and then is concatenated with the image feature from CNN. The combined feature is sent to the softmax layer to predict the answer class, which essentially is a multi-class logistic regression model.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/1512.02167/assets/x1.png" id="S2.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="215" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Framework of the iBOWIMG. Features from the question sentence and image are concatenated then feed into softmax to predict the answer.</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">Here we train and evaluate the iBOWIMG model on the Full release of COCO VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, the largest VQA dataset so far. In the COCO VQA dataset, there are 3 questions annotated by Amazon Mechanical Turk (AMT) workers for each image in the COCO dataset. For each question, 10 answers are annotated by another batch of AMT workers. To pre-process the annotation for training, we perform majority voting on the 10 ground-truth answers to get the most certain answer for each question. Here the answer could be in single word or multiple words. Then we have the 3 question-answer pairs from each image for training. There are in total 248,349 pairs in train2014 and 121,512 pairs in val2014, for 123,287 images overall in the training set. Here train2014 and val2014 are the standard splits of the image set in the COCO dataset.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p">To generate the training set and validation set for our model, we first randomly split the images of COCO val2014 into 70% subset A and 30% subset B. To avoid potential overfitting, questions sharing the same image will be placed into the same split. The question-answer pairs from the images of COCO train2014 + val2014 subset A are combined and used for training, while the val2014 subset B is used as validation set for parameter tuning. After we find the best model parameters, we combine the whole train2014 and val2014 to train the final model. We submit the prediction result given by the final model on the testing set (COCO test2015) to the evaluation server, to get the final accuracy on the test-dev and test-standard set. For Open-Ended Question track, we take the top-1 predicted answer from the softmax output. For the Multiple-Choice Question track, we first get the softmax probability for each of the given choices then select the most confident one.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p">The code is implemented in Torch. The training takes about 10 hours on a single GPU NVIDIA Titan Black.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Benchmark Performance</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">According to the evaluation standard of the VQA dataset, the result of the any proposed VQA models should report accuracy on the test-standard set for fair comparison. We report our baseline on the test-dev set in Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Benchmark Performance ‣ 3 Experiments ‣ Simple Baseline for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and the test-standard set in Table <a href="#S3.T2" title="Table 2 ‣ 3.1 Benchmark Performance ‣ 3 Experiments ‣ Simple Baseline for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The test-dev set is used for debugging and validation experiments and allows for unlimited submission to the evaluation server, while test-standard is used for model comparison with limited submission times.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">Since this VQA dataset is rather new, the publicly available models evaluated on the dataset are all from non-peer reviewed arXiv papers. We include the performance of the models available at the time of writing (Dec.5, 2015) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Note that some models are evaluated on either test-dev or test-standard for either Open-Ended or Multiple-Choice track.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p">The full set of the VQA dataset was released on Oct.6 2015; previously the v0.1 version and v0.9 version had been released. We notice that some models are evaluated using non-standard setups, rendering performance comparisons difficult. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> (arXiv dated at Nov.17 2015) used v0.9 version of VQA with their own split of training and testing; <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> (arXiv dated at Nov.7 2015) used their own split of training and testing for the val2014; <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> (arXiv dated at Nov.18 2015) used v0.9 version of VQA dataset. So these are not included in the comparison.</p>
</div>
<figure id="S3.T1" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance comparison on test-dev.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">Open-Ended</th>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<td id="S3.T1.1.2.2.1" class="ltx_td ltx_border_t"></td>
<th id="S3.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T1.1.2.2.2.1" class="ltx_text ltx_font_bold">Overall</span></th>
<th id="S3.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">yes/no</th>
<th id="S3.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">number</th>
<th id="S3.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">others</th>
</tr>
<tr id="S3.T1.1.3.3" class="ltx_tr">
<td id="S3.T1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t">IMG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</td>
<td id="S3.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">28.13</td>
<td id="S3.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">64.01</td>
<td id="S3.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">00.42</td>
<td id="S3.T1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">03.77</td>
</tr>
<tr id="S3.T1.1.4.4" class="ltx_tr">
<td id="S3.T1.1.4.4.1" class="ltx_td ltx_align_center">BOW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</td>
<td id="S3.T1.1.4.4.2" class="ltx_td ltx_align_center">48.09</td>
<td id="S3.T1.1.4.4.3" class="ltx_td ltx_align_center">75.66</td>
<td id="S3.T1.1.4.4.4" class="ltx_td ltx_align_center">36.70</td>
<td id="S3.T1.1.4.4.5" class="ltx_td ltx_align_center">27.14</td>
</tr>
<tr id="S3.T1.1.5.5" class="ltx_tr">
<td id="S3.T1.1.5.5.1" class="ltx_td ltx_align_center">BOWIMG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</td>
<td id="S3.T1.1.5.5.2" class="ltx_td ltx_align_center">52.64</td>
<td id="S3.T1.1.5.5.3" class="ltx_td ltx_align_center">75.55</td>
<td id="S3.T1.1.5.5.4" class="ltx_td ltx_align_center">33.67</td>
<td id="S3.T1.1.5.5.5" class="ltx_td ltx_align_center">37.37</td>
</tr>
<tr id="S3.T1.1.6.6" class="ltx_tr">
<td id="S3.T1.1.6.6.1" class="ltx_td ltx_align_center">LSTMIMG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</td>
<td id="S3.T1.1.6.6.2" class="ltx_td ltx_align_center">53.74</td>
<td id="S3.T1.1.6.6.3" class="ltx_td ltx_align_center">78.94</td>
<td id="S3.T1.1.6.6.4" class="ltx_td ltx_align_center">35.24</td>
<td id="S3.T1.1.6.6.5" class="ltx_td ltx_align_center">36.42</td>
</tr>
<tr id="S3.T1.1.7.7" class="ltx_tr">
<td id="S3.T1.1.7.7.1" class="ltx_td ltx_align_center">CompMem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S3.T1.1.7.7.2" class="ltx_td ltx_align_center">52.62</td>
<td id="S3.T1.1.7.7.3" class="ltx_td ltx_align_center">78.33</td>
<td id="S3.T1.1.7.7.4" class="ltx_td ltx_align_center">35.93</td>
<td id="S3.T1.1.7.7.5" class="ltx_td ltx_align_center">34.46</td>
</tr>
<tr id="S3.T1.1.8.8" class="ltx_tr">
<td id="S3.T1.1.8.8.1" class="ltx_td ltx_align_center">NMN+LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
<td id="S3.T1.1.8.8.2" class="ltx_td ltx_align_center">54.80</td>
<td id="S3.T1.1.8.8.3" class="ltx_td ltx_align_center">77.70</td>
<td id="S3.T1.1.8.8.4" class="ltx_td ltx_align_center">37.20</td>
<td id="S3.T1.1.8.8.5" class="ltx_td ltx_align_center">39.30</td>
</tr>
<tr id="S3.T1.1.9.9" class="ltx_tr">
<td id="S3.T1.1.9.9.1" class="ltx_td ltx_align_center">WR Sel. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</td>
<td id="S3.T1.1.9.9.2" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.9.9.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.9.9.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.1.9.9.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T1.1.10.10" class="ltx_tr">
<td id="S3.T1.1.10.10.1" class="ltx_td ltx_align_center">ACK <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S3.T1.1.10.10.2" class="ltx_td ltx_align_center">55.72</td>
<td id="S3.T1.1.10.10.3" class="ltx_td ltx_align_center">79.23</td>
<td id="S3.T1.1.10.10.4" class="ltx_td ltx_align_center">36.13</td>
<td id="S3.T1.1.10.10.5" class="ltx_td ltx_align_center">40.08</td>
</tr>
<tr id="S3.T1.1.11.11" class="ltx_tr">
<td id="S3.T1.1.11.11.1" class="ltx_td ltx_align_center">DPPnet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S3.T1.1.11.11.2" class="ltx_td ltx_align_center"><span id="S3.T1.1.11.11.2.1" class="ltx_text ltx_font_bold">57.22</span></td>
<td id="S3.T1.1.11.11.3" class="ltx_td ltx_align_center">80.71</td>
<td id="S3.T1.1.11.11.4" class="ltx_td ltx_align_center">37.24</td>
<td id="S3.T1.1.11.11.5" class="ltx_td ltx_align_center">41.69</td>
</tr>
<tr id="S3.T1.1.12.12" class="ltx_tr">
<td id="S3.T1.1.12.12.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">iBOWIMG</td>
<td id="S3.T1.1.12.12.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">55.72</td>
<td id="S3.T1.1.12.12.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">76.55</td>
<td id="S3.T1.1.12.12.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">35.03</td>
<td id="S3.T1.1.12.12.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">42.62</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.T1.2" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.2.1.1" class="ltx_tr">
<th id="S3.T1.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">Multiple-Choice</th>
</tr>
<tr id="S3.T1.2.2.2" class="ltx_tr">
<th id="S3.T1.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T1.2.2.2.1.1" class="ltx_text ltx_font_bold">Overall</span></th>
<th id="S3.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">yes/no</th>
<th id="S3.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">number</th>
<th id="S3.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">others</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.2.3.1" class="ltx_tr">
<td id="S3.T1.2.3.1.1" class="ltx_td ltx_align_center ltx_border_t">30.53</td>
<td id="S3.T1.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">69.87</td>
<td id="S3.T1.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">00.45</td>
<td id="S3.T1.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">03.76</td>
</tr>
<tr id="S3.T1.2.4.2" class="ltx_tr">
<td id="S3.T1.2.4.2.1" class="ltx_td ltx_align_center">53.68</td>
<td id="S3.T1.2.4.2.2" class="ltx_td ltx_align_center">75.71</td>
<td id="S3.T1.2.4.2.3" class="ltx_td ltx_align_center">37.05</td>
<td id="S3.T1.2.4.2.4" class="ltx_td ltx_align_center">38.64</td>
</tr>
<tr id="S3.T1.2.5.3" class="ltx_tr">
<td id="S3.T1.2.5.3.1" class="ltx_td ltx_align_center">58.97</td>
<td id="S3.T1.2.5.3.2" class="ltx_td ltx_align_center">75.59</td>
<td id="S3.T1.2.5.3.3" class="ltx_td ltx_align_center">34.35</td>
<td id="S3.T1.2.5.3.4" class="ltx_td ltx_align_center">50.33</td>
</tr>
<tr id="S3.T1.2.6.4" class="ltx_tr">
<td id="S3.T1.2.6.4.1" class="ltx_td ltx_align_center">57.17</td>
<td id="S3.T1.2.6.4.2" class="ltx_td ltx_align_center">78.95</td>
<td id="S3.T1.2.6.4.3" class="ltx_td ltx_align_center">35.80</td>
<td id="S3.T1.2.6.4.4" class="ltx_td ltx_align_center">43.41</td>
</tr>
<tr id="S3.T1.2.7.5" class="ltx_tr">
<td id="S3.T1.2.7.5.1" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.2.7.5.2" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.2.7.5.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.2.7.5.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T1.2.8.6" class="ltx_tr">
<td id="S3.T1.2.8.6.1" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.2.8.6.2" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.2.8.6.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.2.8.6.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T1.2.9.7" class="ltx_tr">
<td id="S3.T1.2.9.7.1" class="ltx_td ltx_align_center">60.96</td>
<td id="S3.T1.2.9.7.2" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.2.9.7.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.2.9.7.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T1.2.10.8" class="ltx_tr">
<td id="S3.T1.2.10.8.1" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.2.10.8.2" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.2.10.8.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T1.2.10.8.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T1.2.11.9" class="ltx_tr">
<td id="S3.T1.2.11.9.1" class="ltx_td ltx_align_center"><span id="S3.T1.2.11.9.1.1" class="ltx_text ltx_font_bold">62.48</span></td>
<td id="S3.T1.2.11.9.2" class="ltx_td ltx_align_center">80.79</td>
<td id="S3.T1.2.11.9.3" class="ltx_td ltx_align_center">38.94</td>
<td id="S3.T1.2.11.9.4" class="ltx_td ltx_align_center">52.16</td>
</tr>
<tr id="S3.T1.2.12.10" class="ltx_tr">
<td id="S3.T1.2.12.10.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">61.68</td>
<td id="S3.T1.2.12.10.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">76.68</td>
<td id="S3.T1.2.12.10.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">37.05</td>
<td id="S3.T1.2.12.10.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">54.44</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
<figure id="S3.T2" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance comparison on test-standard.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">Open-Ended</th>
</tr>
<tr id="S3.T2.1.2.2" class="ltx_tr">
<td id="S3.T2.1.2.2.1" class="ltx_td ltx_border_t"></td>
<th id="S3.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.2.2.2.1" class="ltx_text ltx_font_bold">Overall</span></th>
<th id="S3.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">yes/no</th>
<th id="S3.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">number</th>
<th id="S3.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">others</th>
</tr>
<tr id="S3.T2.1.3.3" class="ltx_tr">
<td id="S3.T2.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t">LSTMIMG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</td>
<td id="S3.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">54.06</td>
<td id="S3.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S3.T2.1.4.4" class="ltx_tr">
<td id="S3.T2.1.4.4.1" class="ltx_td ltx_align_center">NMN+LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
<td id="S3.T2.1.4.4.2" class="ltx_td ltx_align_center">55.10</td>
<td id="S3.T2.1.4.4.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.4.4.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.4.4.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T2.1.5.5" class="ltx_tr">
<td id="S3.T2.1.5.5.1" class="ltx_td ltx_align_center">ACK <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S3.T2.1.5.5.2" class="ltx_td ltx_align_center">55.98</td>
<td id="S3.T2.1.5.5.3" class="ltx_td ltx_align_center">79.05</td>
<td id="S3.T2.1.5.5.4" class="ltx_td ltx_align_center">36.10</td>
<td id="S3.T2.1.5.5.5" class="ltx_td ltx_align_center">40.61</td>
</tr>
<tr id="S3.T2.1.6.6" class="ltx_tr">
<td id="S3.T2.1.6.6.1" class="ltx_td ltx_align_center">DPPnet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S3.T2.1.6.6.2" class="ltx_td ltx_align_center"><span id="S3.T2.1.6.6.2.1" class="ltx_text ltx_font_bold">57.36</span></td>
<td id="S3.T2.1.6.6.3" class="ltx_td ltx_align_center">80.28</td>
<td id="S3.T2.1.6.6.4" class="ltx_td ltx_align_center">36.92</td>
<td id="S3.T2.1.6.6.5" class="ltx_td ltx_align_center">42.24</td>
</tr>
<tr id="S3.T2.1.7.7" class="ltx_tr">
<td id="S3.T2.1.7.7.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">iBOWIMG</td>
<td id="S3.T2.1.7.7.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">55.89</td>
<td id="S3.T2.1.7.7.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">76.76</td>
<td id="S3.T2.1.7.7.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">34.98</td>
<td id="S3.T2.1.7.7.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">42.62</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.T2.2" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.2.1.1" class="ltx_tr">
<th id="S3.T2.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">Multiple-Choice</th>
</tr>
<tr id="S3.T2.2.2.2" class="ltx_tr">
<th id="S3.T2.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.2.2.2.1.1" class="ltx_text ltx_font_bold">Overall</span></th>
<th id="S3.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">yes/no</th>
<th id="S3.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">number</th>
<th id="S3.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">others</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.2.3.1" class="ltx_tr">
<td id="S3.T2.2.3.1.1" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S3.T2.2.4.2" class="ltx_tr">
<td id="S3.T2.2.4.2.1" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.2.4.2.2" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.2.4.2.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.2.4.2.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T2.2.5.3" class="ltx_tr">
<td id="S3.T2.2.5.3.1" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.2.5.3.2" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.2.5.3.3" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.2.5.3.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T2.2.6.4" class="ltx_tr">
<td id="S3.T2.2.6.4.1" class="ltx_td ltx_align_center"><span id="S3.T2.2.6.4.1.1" class="ltx_text ltx_font_bold">62.69</span></td>
<td id="S3.T2.2.6.4.2" class="ltx_td ltx_align_center">80.35</td>
<td id="S3.T2.2.6.4.3" class="ltx_td ltx_align_center">38.79</td>
<td id="S3.T2.2.6.4.4" class="ltx_td ltx_align_center">52.79</td>
</tr>
<tr id="S3.T2.2.7.5" class="ltx_tr">
<td id="S3.T2.2.7.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">61.97</td>
<td id="S3.T2.2.7.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">76.86</td>
<td id="S3.T2.2.7.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">37.30</td>
<td id="S3.T2.2.7.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">54.60</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p">Except for these IMG, BOW, BOWIMG baselines provided in the <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, all the compared methods use either deep or recursive neural networks. However, our iBOWIMG baseline shows comparable performances against these much more complex models, except for DPPnet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> that is about 1.5% better.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training Details</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">Learning rate and weight clip.</span> We find that setting up a different learning rate and weight clipping for the word embedding layer and softmax layer leads to better performance. The learning rate for the word embedding layer should be much higher than the learning rate of softmax layer to learn a good word embedding. From the performance of BOW in Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Benchmark Performance ‣ 3 Experiments ‣ Simple Baseline for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we can see that a good word model is crucial to the accuracy, as BOW model alone could achieve closely to 48%, even without looking at the image content.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Model parameters to tune.</span> Though our model could be considered as the simplest baseline so far for visual QA, there are several model parameters to tune: <span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_bold">1)</span> the number of epochs to train. <span id="S3.SS2.p2.1.3" class="ltx_text ltx_font_bold">2)</span> the learning rate and weight clip. <span id="S3.SS2.p2.1.4" class="ltx_text ltx_font_bold">3)</span> the threshold for removing less frequent question word and answer classes. We iterate to search the best value of each model parameter separately on the val2014 subset B. In our best model, there are 5,746 words in the dictionary of question sentence, 5,216 classes of answers. The specific model parameters can be found in the source code.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Understanding the Visual QA model</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">From the comparisons above, we can see that our baseline model performs as well as the recurrent neural network models on the VQA dataset. Furthermore, due to its simplicity, the behavior of the model could be easily interpreted, demonstrating what it learned for visual QA.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.12" class="ltx_p">Essentially, the BOWIMG baseline model learns to memorize the correlation between the answer class and the informative words in the question sentence along with the visual feature. We split the learned weights of softmax into two parts, one part for the word feature and the other part for the visual feature. Therefore,</p>
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\displaystyle r=\textbf{M}_{w}\textbf{x}_{w}+\textbf{M}_{v}\textbf{x}_{v}." display="inline"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">r</mi><mo id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mrow id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml"><msub id="S3.E1.m1.1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.1.1.1.1.3.2.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.2a.cmml">M</mtext><mi id="S3.E1.m1.1.1.1.1.3.2.2.3" xref="S3.E1.m1.1.1.1.1.3.2.2.3.cmml">w</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.3.2.1" xref="S3.E1.m1.1.1.1.1.3.2.1.cmml">​</mo><msub id="S3.E1.m1.1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.3.2.3.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.1.1.1.1.3.2.3.2" xref="S3.E1.m1.1.1.1.1.3.2.3.2a.cmml">x</mtext><mi id="S3.E1.m1.1.1.1.1.3.2.3.3" xref="S3.E1.m1.1.1.1.1.3.2.3.3.cmml">w</mi></msub></mrow><mo id="S3.E1.m1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml"><msub id="S3.E1.m1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.3.3.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.1.1.1.1.3.3.2.2" xref="S3.E1.m1.1.1.1.1.3.3.2.2a.cmml">M</mtext><mi id="S3.E1.m1.1.1.1.1.3.3.2.3" xref="S3.E1.m1.1.1.1.1.3.3.2.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.3.3.1.cmml">​</mo><msub id="S3.E1.m1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.cmml"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.1.1.1.1.3.3.3.2" xref="S3.E1.m1.1.1.1.1.3.3.3.2a.cmml">x</mtext><mi id="S3.E1.m1.1.1.1.1.3.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.3.cmml">v</mi></msub></mrow></mrow></mrow><mo lspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"></eq><ci id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2">𝑟</ci><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><plus id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1"></plus><apply id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2"><times id="S3.E1.m1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.1"></times><apply id="S3.E1.m1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.2.2a.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2.2"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.1.1.1.1.3.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2.2">M</mtext></ci><ci id="S3.E1.m1.1.1.1.1.3.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2.3">𝑤</ci></apply><apply id="S3.E1.m1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.3.2a.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.2"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.2">x</mtext></ci><ci id="S3.E1.m1.1.1.1.1.3.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.3">𝑤</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3"><times id="S3.E1.m1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.1"></times><apply id="S3.E1.m1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.3.2.2a.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.2"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.2">M</mtext></ci><ci id="S3.E1.m1.1.1.1.1.3.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.3">𝑣</ci></apply><apply id="S3.E1.m1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.3.3.2a.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.2"><mtext class="ltx_mathvariant_bold" id="S3.E1.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.2">x</mtext></ci><ci id="S3.E1.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.3">𝑣</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\displaystyle r=\textbf{M}_{w}\textbf{x}_{w}+\textbf{M}_{v}\textbf{x}_{v}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.11" class="ltx_p">Here the softmax matrix <span id="S3.SS3.p2.11.1" class="ltx_text ltx_markedasmath ltx_font_bold">M</span> is decomposed into the weights <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="\textbf{M}_{w}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><msub id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2a.cmml">M</mtext><mi id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2a.cmml" xref="S3.SS3.p2.2.m2.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">M</mtext></ci><ci id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\textbf{M}_{w}</annotation></semantics></math> for word feature <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="\textbf{x}_{w}" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><msub id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2a.cmml">x</mtext><mi id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p2.3.m3.1.1.2a.cmml" xref="S3.SS3.p2.3.m3.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2">x</mtext></ci><ci id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">\textbf{x}_{w}</annotation></semantics></math> and the weights <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="\textbf{M}_{v}" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><msub id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2a.cmml">M</mtext><mi id="S3.SS3.p2.4.m4.1.1.3" xref="S3.SS3.p2.4.m4.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p2.4.m4.1.1.2a.cmml" xref="S3.SS3.p2.4.m4.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2">M</mtext></ci><ci id="S3.SS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">\textbf{M}_{v}</annotation></semantics></math> for the visual feature <math id="S3.SS3.p2.5.m5.1" class="ltx_Math" alttext="\textbf{x}_{v}" display="inline"><semantics id="S3.SS3.p2.5.m5.1a"><msub id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.5.m5.1.1.2" xref="S3.SS3.p2.5.m5.1.1.2a.cmml">x</mtext><mi id="S3.SS3.p2.5.m5.1.1.3" xref="S3.SS3.p2.5.m5.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><apply id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.5.m5.1.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.p2.5.m5.1.1.2a.cmml" xref="S3.SS3.p2.5.m5.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.5.m5.1.1.2.cmml" xref="S3.SS3.p2.5.m5.1.1.2">x</mtext></ci><ci id="S3.SS3.p2.5.m5.1.1.3.cmml" xref="S3.SS3.p2.5.m5.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">\textbf{x}_{v}</annotation></semantics></math> whereas <math id="S3.SS3.p2.6.m6.2" class="ltx_Math" alttext="\textbf{M}=[\textbf{M}_{w},\textbf{M}_{v}]" display="inline"><semantics id="S3.SS3.p2.6.m6.2a"><mrow id="S3.SS3.p2.6.m6.2.2" xref="S3.SS3.p2.6.m6.2.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.6.m6.2.2.4" xref="S3.SS3.p2.6.m6.2.2.4a.cmml">M</mtext><mo id="S3.SS3.p2.6.m6.2.2.3" xref="S3.SS3.p2.6.m6.2.2.3.cmml">=</mo><mrow id="S3.SS3.p2.6.m6.2.2.2.2" xref="S3.SS3.p2.6.m6.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p2.6.m6.2.2.2.2.3" xref="S3.SS3.p2.6.m6.2.2.2.3.cmml">[</mo><msub id="S3.SS3.p2.6.m6.1.1.1.1.1" xref="S3.SS3.p2.6.m6.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.6.m6.1.1.1.1.1.2" xref="S3.SS3.p2.6.m6.1.1.1.1.1.2a.cmml">M</mtext><mi id="S3.SS3.p2.6.m6.1.1.1.1.1.3" xref="S3.SS3.p2.6.m6.1.1.1.1.1.3.cmml">w</mi></msub><mo id="S3.SS3.p2.6.m6.2.2.2.2.4" xref="S3.SS3.p2.6.m6.2.2.2.3.cmml">,</mo><msub id="S3.SS3.p2.6.m6.2.2.2.2.2" xref="S3.SS3.p2.6.m6.2.2.2.2.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.6.m6.2.2.2.2.2.2" xref="S3.SS3.p2.6.m6.2.2.2.2.2.2a.cmml">M</mtext><mi id="S3.SS3.p2.6.m6.2.2.2.2.2.3" xref="S3.SS3.p2.6.m6.2.2.2.2.2.3.cmml">v</mi></msub><mo stretchy="false" id="S3.SS3.p2.6.m6.2.2.2.2.5" xref="S3.SS3.p2.6.m6.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.2b"><apply id="S3.SS3.p2.6.m6.2.2.cmml" xref="S3.SS3.p2.6.m6.2.2"><eq id="S3.SS3.p2.6.m6.2.2.3.cmml" xref="S3.SS3.p2.6.m6.2.2.3"></eq><ci id="S3.SS3.p2.6.m6.2.2.4a.cmml" xref="S3.SS3.p2.6.m6.2.2.4"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.6.m6.2.2.4.cmml" xref="S3.SS3.p2.6.m6.2.2.4">M</mtext></ci><interval closure="closed" id="S3.SS3.p2.6.m6.2.2.2.3.cmml" xref="S3.SS3.p2.6.m6.2.2.2.2"><apply id="S3.SS3.p2.6.m6.1.1.1.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p2.6.m6.1.1.1.1.1.2a.cmml" xref="S3.SS3.p2.6.m6.1.1.1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.6.m6.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.6.m6.1.1.1.1.1.2">M</mtext></ci><ci id="S3.SS3.p2.6.m6.1.1.1.1.1.3.cmml" xref="S3.SS3.p2.6.m6.1.1.1.1.1.3">𝑤</ci></apply><apply id="S3.SS3.p2.6.m6.2.2.2.2.2.cmml" xref="S3.SS3.p2.6.m6.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.2.2.2.2.2.1.cmml" xref="S3.SS3.p2.6.m6.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p2.6.m6.2.2.2.2.2.2a.cmml" xref="S3.SS3.p2.6.m6.2.2.2.2.2.2"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.6.m6.2.2.2.2.2.2.cmml" xref="S3.SS3.p2.6.m6.2.2.2.2.2.2">M</mtext></ci><ci id="S3.SS3.p2.6.m6.2.2.2.2.2.3.cmml" xref="S3.SS3.p2.6.m6.2.2.2.2.2.3">𝑣</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.2c">\textbf{M}=[\textbf{M}_{w},\textbf{M}_{v}]</annotation></semantics></math>. <math id="S3.SS3.p2.7.m7.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS3.p2.7.m7.1a"><mi id="S3.SS3.p2.7.m7.1.1" xref="S3.SS3.p2.7.m7.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m7.1b"><ci id="S3.SS3.p2.7.m7.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m7.1c">r</annotation></semantics></math> is the response of the answer class before softmax normalization. Denote the response <math id="S3.SS3.p2.8.m8.1" class="ltx_Math" alttext="r_{w}=\textbf{M}_{w}\textbf{x}_{w}" display="inline"><semantics id="S3.SS3.p2.8.m8.1a"><mrow id="S3.SS3.p2.8.m8.1.1" xref="S3.SS3.p2.8.m8.1.1.cmml"><msub id="S3.SS3.p2.8.m8.1.1.2" xref="S3.SS3.p2.8.m8.1.1.2.cmml"><mi id="S3.SS3.p2.8.m8.1.1.2.2" xref="S3.SS3.p2.8.m8.1.1.2.2.cmml">r</mi><mi id="S3.SS3.p2.8.m8.1.1.2.3" xref="S3.SS3.p2.8.m8.1.1.2.3.cmml">w</mi></msub><mo id="S3.SS3.p2.8.m8.1.1.1" xref="S3.SS3.p2.8.m8.1.1.1.cmml">=</mo><mrow id="S3.SS3.p2.8.m8.1.1.3" xref="S3.SS3.p2.8.m8.1.1.3.cmml"><msub id="S3.SS3.p2.8.m8.1.1.3.2" xref="S3.SS3.p2.8.m8.1.1.3.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.8.m8.1.1.3.2.2" xref="S3.SS3.p2.8.m8.1.1.3.2.2a.cmml">M</mtext><mi id="S3.SS3.p2.8.m8.1.1.3.2.3" xref="S3.SS3.p2.8.m8.1.1.3.2.3.cmml">w</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p2.8.m8.1.1.3.1" xref="S3.SS3.p2.8.m8.1.1.3.1.cmml">​</mo><msub id="S3.SS3.p2.8.m8.1.1.3.3" xref="S3.SS3.p2.8.m8.1.1.3.3.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.8.m8.1.1.3.3.2" xref="S3.SS3.p2.8.m8.1.1.3.3.2a.cmml">x</mtext><mi id="S3.SS3.p2.8.m8.1.1.3.3.3" xref="S3.SS3.p2.8.m8.1.1.3.3.3.cmml">w</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.8.m8.1b"><apply id="S3.SS3.p2.8.m8.1.1.cmml" xref="S3.SS3.p2.8.m8.1.1"><eq id="S3.SS3.p2.8.m8.1.1.1.cmml" xref="S3.SS3.p2.8.m8.1.1.1"></eq><apply id="S3.SS3.p2.8.m8.1.1.2.cmml" xref="S3.SS3.p2.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.8.m8.1.1.2.1.cmml" xref="S3.SS3.p2.8.m8.1.1.2">subscript</csymbol><ci id="S3.SS3.p2.8.m8.1.1.2.2.cmml" xref="S3.SS3.p2.8.m8.1.1.2.2">𝑟</ci><ci id="S3.SS3.p2.8.m8.1.1.2.3.cmml" xref="S3.SS3.p2.8.m8.1.1.2.3">𝑤</ci></apply><apply id="S3.SS3.p2.8.m8.1.1.3.cmml" xref="S3.SS3.p2.8.m8.1.1.3"><times id="S3.SS3.p2.8.m8.1.1.3.1.cmml" xref="S3.SS3.p2.8.m8.1.1.3.1"></times><apply id="S3.SS3.p2.8.m8.1.1.3.2.cmml" xref="S3.SS3.p2.8.m8.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS3.p2.8.m8.1.1.3.2.1.cmml" xref="S3.SS3.p2.8.m8.1.1.3.2">subscript</csymbol><ci id="S3.SS3.p2.8.m8.1.1.3.2.2a.cmml" xref="S3.SS3.p2.8.m8.1.1.3.2.2"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.8.m8.1.1.3.2.2.cmml" xref="S3.SS3.p2.8.m8.1.1.3.2.2">M</mtext></ci><ci id="S3.SS3.p2.8.m8.1.1.3.2.3.cmml" xref="S3.SS3.p2.8.m8.1.1.3.2.3">𝑤</ci></apply><apply id="S3.SS3.p2.8.m8.1.1.3.3.cmml" xref="S3.SS3.p2.8.m8.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS3.p2.8.m8.1.1.3.3.1.cmml" xref="S3.SS3.p2.8.m8.1.1.3.3">subscript</csymbol><ci id="S3.SS3.p2.8.m8.1.1.3.3.2a.cmml" xref="S3.SS3.p2.8.m8.1.1.3.3.2"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.8.m8.1.1.3.3.2.cmml" xref="S3.SS3.p2.8.m8.1.1.3.3.2">x</mtext></ci><ci id="S3.SS3.p2.8.m8.1.1.3.3.3.cmml" xref="S3.SS3.p2.8.m8.1.1.3.3.3">𝑤</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.8.m8.1c">r_{w}=\textbf{M}_{w}\textbf{x}_{w}</annotation></semantics></math> as the contribution from question words and <math id="S3.SS3.p2.9.m9.1" class="ltx_Math" alttext="r_{v}=\textbf{M}_{v}\textbf{x}_{v}" display="inline"><semantics id="S3.SS3.p2.9.m9.1a"><mrow id="S3.SS3.p2.9.m9.1.1" xref="S3.SS3.p2.9.m9.1.1.cmml"><msub id="S3.SS3.p2.9.m9.1.1.2" xref="S3.SS3.p2.9.m9.1.1.2.cmml"><mi id="S3.SS3.p2.9.m9.1.1.2.2" xref="S3.SS3.p2.9.m9.1.1.2.2.cmml">r</mi><mi id="S3.SS3.p2.9.m9.1.1.2.3" xref="S3.SS3.p2.9.m9.1.1.2.3.cmml">v</mi></msub><mo id="S3.SS3.p2.9.m9.1.1.1" xref="S3.SS3.p2.9.m9.1.1.1.cmml">=</mo><mrow id="S3.SS3.p2.9.m9.1.1.3" xref="S3.SS3.p2.9.m9.1.1.3.cmml"><msub id="S3.SS3.p2.9.m9.1.1.3.2" xref="S3.SS3.p2.9.m9.1.1.3.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.9.m9.1.1.3.2.2" xref="S3.SS3.p2.9.m9.1.1.3.2.2a.cmml">M</mtext><mi id="S3.SS3.p2.9.m9.1.1.3.2.3" xref="S3.SS3.p2.9.m9.1.1.3.2.3.cmml">v</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p2.9.m9.1.1.3.1" xref="S3.SS3.p2.9.m9.1.1.3.1.cmml">​</mo><msub id="S3.SS3.p2.9.m9.1.1.3.3" xref="S3.SS3.p2.9.m9.1.1.3.3.cmml"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.9.m9.1.1.3.3.2" xref="S3.SS3.p2.9.m9.1.1.3.3.2a.cmml">x</mtext><mi id="S3.SS3.p2.9.m9.1.1.3.3.3" xref="S3.SS3.p2.9.m9.1.1.3.3.3.cmml">v</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.9.m9.1b"><apply id="S3.SS3.p2.9.m9.1.1.cmml" xref="S3.SS3.p2.9.m9.1.1"><eq id="S3.SS3.p2.9.m9.1.1.1.cmml" xref="S3.SS3.p2.9.m9.1.1.1"></eq><apply id="S3.SS3.p2.9.m9.1.1.2.cmml" xref="S3.SS3.p2.9.m9.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.9.m9.1.1.2.1.cmml" xref="S3.SS3.p2.9.m9.1.1.2">subscript</csymbol><ci id="S3.SS3.p2.9.m9.1.1.2.2.cmml" xref="S3.SS3.p2.9.m9.1.1.2.2">𝑟</ci><ci id="S3.SS3.p2.9.m9.1.1.2.3.cmml" xref="S3.SS3.p2.9.m9.1.1.2.3">𝑣</ci></apply><apply id="S3.SS3.p2.9.m9.1.1.3.cmml" xref="S3.SS3.p2.9.m9.1.1.3"><times id="S3.SS3.p2.9.m9.1.1.3.1.cmml" xref="S3.SS3.p2.9.m9.1.1.3.1"></times><apply id="S3.SS3.p2.9.m9.1.1.3.2.cmml" xref="S3.SS3.p2.9.m9.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS3.p2.9.m9.1.1.3.2.1.cmml" xref="S3.SS3.p2.9.m9.1.1.3.2">subscript</csymbol><ci id="S3.SS3.p2.9.m9.1.1.3.2.2a.cmml" xref="S3.SS3.p2.9.m9.1.1.3.2.2"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.9.m9.1.1.3.2.2.cmml" xref="S3.SS3.p2.9.m9.1.1.3.2.2">M</mtext></ci><ci id="S3.SS3.p2.9.m9.1.1.3.2.3.cmml" xref="S3.SS3.p2.9.m9.1.1.3.2.3">𝑣</ci></apply><apply id="S3.SS3.p2.9.m9.1.1.3.3.cmml" xref="S3.SS3.p2.9.m9.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS3.p2.9.m9.1.1.3.3.1.cmml" xref="S3.SS3.p2.9.m9.1.1.3.3">subscript</csymbol><ci id="S3.SS3.p2.9.m9.1.1.3.3.2a.cmml" xref="S3.SS3.p2.9.m9.1.1.3.3.2"><mtext class="ltx_mathvariant_bold" id="S3.SS3.p2.9.m9.1.1.3.3.2.cmml" xref="S3.SS3.p2.9.m9.1.1.3.3.2">x</mtext></ci><ci id="S3.SS3.p2.9.m9.1.1.3.3.3.cmml" xref="S3.SS3.p2.9.m9.1.1.3.3.3">𝑣</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.9.m9.1c">r_{v}=\textbf{M}_{v}\textbf{x}_{v}</annotation></semantics></math> as the contribution from the image contents. Thus for each predicted answer, we know exactly the proportions of contribution from word and image content respectively. We also could rank <math id="S3.SS3.p2.10.m10.1" class="ltx_Math" alttext="r_{w}" display="inline"><semantics id="S3.SS3.p2.10.m10.1a"><msub id="S3.SS3.p2.10.m10.1.1" xref="S3.SS3.p2.10.m10.1.1.cmml"><mi id="S3.SS3.p2.10.m10.1.1.2" xref="S3.SS3.p2.10.m10.1.1.2.cmml">r</mi><mi id="S3.SS3.p2.10.m10.1.1.3" xref="S3.SS3.p2.10.m10.1.1.3.cmml">w</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.10.m10.1b"><apply id="S3.SS3.p2.10.m10.1.1.cmml" xref="S3.SS3.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.10.m10.1.1.1.cmml" xref="S3.SS3.p2.10.m10.1.1">subscript</csymbol><ci id="S3.SS3.p2.10.m10.1.1.2.cmml" xref="S3.SS3.p2.10.m10.1.1.2">𝑟</ci><ci id="S3.SS3.p2.10.m10.1.1.3.cmml" xref="S3.SS3.p2.10.m10.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.10.m10.1c">r_{w}</annotation></semantics></math> and <math id="S3.SS3.p2.11.m11.1" class="ltx_Math" alttext="r_{v}" display="inline"><semantics id="S3.SS3.p2.11.m11.1a"><msub id="S3.SS3.p2.11.m11.1.1" xref="S3.SS3.p2.11.m11.1.1.cmml"><mi id="S3.SS3.p2.11.m11.1.1.2" xref="S3.SS3.p2.11.m11.1.1.2.cmml">r</mi><mi id="S3.SS3.p2.11.m11.1.1.3" xref="S3.SS3.p2.11.m11.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.11.m11.1b"><apply id="S3.SS3.p2.11.m11.1.1.cmml" xref="S3.SS3.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.11.m11.1.1.1.cmml" xref="S3.SS3.p2.11.m11.1.1">subscript</csymbol><ci id="S3.SS3.p2.11.m11.1.1.2.cmml" xref="S3.SS3.p2.11.m11.1.1.2">𝑟</ci><ci id="S3.SS3.p2.11.m11.1.1.3.cmml" xref="S3.SS3.p2.11.m11.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.11.m11.1c">r_{v}</annotation></semantics></math> to know what the predicted answer could be if the model only relies on one side of information.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/1512.02167/assets/x2.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="230" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples of visual question answering from the iBOWIMG baseline. For each image there are two questions and the top 3 predicted answers from the model. The prediction score of each answer is decomposed into the contributions of image and words respectively. The predicted answers which rely purely on question words or image are also shown.</figcaption>
</figure>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p">Figure <a href="#S3.F2" title="Figure 2 ‣ 3.3 Understanding the Visual QA model ‣ 3 Experiments ‣ Simple Baseline for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows some examples of the predictions, revealing that the question words usually have dominant influence on predicting the answer. For example, the correctly predicted answers for the two questions given for the first image ‘what is the color of sofa’ and ‘which brand is the laptop’ come mostly from the question words, without the need for image. This demonstrates the bias in the frequency of object and actions appearing in the images of COCO dataset. For the second image, we ask ‘what are they doing’: the words-only prediction gives ‘playing wii (10.62), eating (9.97), playing frisbee (9.24)’, while full prediction gives the correct prediction ‘playing baseball (10.67 = 2.01 [image] + 8.66 [word])’.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.1" class="ltx_p">To further understand the answers predicted by the model given the visual feature and question sentence, we first decompose the word contribution of the answer into single words of the question sentence, then we visualize the informative image regions relevant to the answer through the technique proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para ltx_noindent">
<p id="S3.SS3.p5.1" class="ltx_p">Since there are just two linear transformations (one is word embedding and the other is softmax matrix multiplication) from the one hot vector to the answer response, we could easily know the importance of each single word in the question to the predicted answer. In Figure <a href="#S3.F3" title="Figure 3 ‣ 3.3 Understanding the Visual QA model ‣ 3 Experiments ‣ Simple Baseline for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we plot the ranked word importance for each word in the question sentence. In the first image question word ‘doing’ is informative to the answer ‘texting’ while in the second image question word ‘eating’ is informative to the answer ‘hot dog’.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/1512.02167/assets/x3.png" id="S3.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="218" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The examples of the word importance of question sentences and the informative image regions relevant to the predicted answers.</figcaption>
</figure>
<div id="S3.SS3.p6" class="ltx_para ltx_noindent">
<p id="S3.SS3.p6.1" class="ltx_p">To highlight the informative image regions relevant to the predicted answer we apply a technique called Class Activation Mapping (CAM) proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. The CAM technique leverages the linear relation between the softmax prediction and the final convolutional feature map, which allows us to identify the most discriminative image regions relevant to the predicted result. In Figure <a href="#S3.F3" title="Figure 3 ‣ 3.3 Understanding the Visual QA model ‣ 3 Experiments ‣ Simple Baseline for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> we plot the heatmaps generated by the CAM associated with the predicted answer, which highlight the informative image regions such as the cellphone in the first image to the answer ‘texting’ and the hot dog in the first image to the answer ‘hot dog’. The example in lower part of Figure <a href="#S3.F3" title="Figure 3 ‣ 3.3 Understanding the Visual QA model ‣ 3 Experiments ‣ Simple Baseline for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the heatmaps generated by two different questions and answers. Visual features from CNN already have implicit attention and selectivity over the image region, thus the resulting class activation maps are similar to the maps generated by the attention mechanisms of the VQA models in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Interactive Visual QA Demo</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">Question answering is essentially an interactive activity, thus it would be good to make the trained models able to interact with people in real time. Aided by the simplicity of the baseline model, we built a web demo that people could type question about a given image and our AI system powered by iBOWIMG will reply the most possible answers. Here the deep feature of the images are extracted beforehand. Figure <a href="#S5.F4" title="Figure 4 ‣ 5 Concluding Remarks ‣ Simple Baseline for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows a snapshot of the demo. People could play with the demo to see the strength and weakness of VQA model.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Concluding Remarks</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">For visual question answering on COCO dataset, our implementation of a simple baseline achieves comparable performance to several recently proposed recurrent neural network-based approaches. To reach the correct prediction, the baseline captures the correlation between the informative words in the question and the answer, and that between image contents and the answer. How to move beyond this, from memorizing the correlations to actual reasoning and understanding of the question and image, is a goal for future research.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/1512.02167/assets/x4.png" id="S5.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="189" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Snapshot of the visual question answering demo. People could type questions into the demo and the demo will give answer predictions. Here we show the answer predictions for two questions.</figcaption>
</figure>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Andreas, M. Rohrbach, T. Darrell, and D. Klein.

</span>
<span class="ltx_bibblock">Deep compositional question answering with neural module networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.02799</span>, 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and
D. Parikh.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1505.00468</span>, 2015.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
K. Chen, J. Wang, L.-C. Chen, H. Gao, W. Xu, and R. Nevatia.

</span>
<span class="ltx_bibblock">Abc-cnn: An attention based convolutional neural network for visual
question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.05960</span>, 2015.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J. Devlin, S. Gupta, R. Girshick, M. Mitchell, and C. L. Zitnick.

</span>
<span class="ltx_bibblock">Exploring nearest neighbor approaches for image captioning.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1505.04467</span>, 2015.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu.

</span>
<span class="ltx_bibblock">Are you talking to a machine? dataset and methods for multilingual
image question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1505.05612</span>, 2015.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. Jiang, F. Wang, F. Porikli, and Y. Li.

</span>
<span class="ltx_bibblock">Compositional memory for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.05676</span>, 2015.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
R. Kiros, R. Salakhutdinov, and R. Zemel.

</span>
<span class="ltx_bibblock">Multimodal neural language models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proceedings of the 31st International Conference on Machine
Learning (ICML-14)</span>, pages 595–603, 2014.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. Krizhevsky, I. Sutskever, and G. E. Hinton.

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, pages
1097–1105, 2012.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Computer Vision–ECCV 2014</span>, pages 740–755. Springer, 2014.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J. Mao, W. Xu, Y. Yang, J. Wang, and A. Yuille.

</span>
<span class="ltx_bibblock">Deep captioning with multimodal recurrent neural networks (m-rnn).

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1412.6632</span>, 2014.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
H. Noh, P. H. Seo, and B. Han.

</span>
<span class="ltx_bibblock">Image question answering using convolutional neural network with
dynamic parameter prediction.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.05756</span>, 2015.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
M. Ren, R. Kiros, and R. Zemel.

</span>
<span class="ltx_bibblock">Exploring models and data for image question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">NIPS</span>, volume 1, page 3, 2015.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
K. J. Shih, S. Singh, and D. Hoiem.

</span>
<span class="ltx_bibblock">Where to look: Focus regions for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.07394</span>, 2015.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich.

</span>
<span class="ltx_bibblock">Going deeper with convolutions.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1409.4842</span>, 2014.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
O. Vinyals, A. Toshev, S. Bengio, and D. Erhan.

</span>
<span class="ltx_bibblock">Show and tell: A neural image caption generator.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1411.4555</span>, 2014.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Q. Wu, P. Wang, C. Shen, A. v. d. Hengel, and A. Dick.

</span>
<span class="ltx_bibblock">Ask me anything: Free-form visual question answering based on
knowledge from external sources.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.06973</span>, 2015.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
H. Xu and K. Saenko.

</span>
<span class="ltx_bibblock">Ask, attend and answer: Exploring question-guided spatial attention
for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.05234</span>, 2015.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Z. Yang, X. He, J. Gao, L. Deng, and A. Smola.

</span>
<span class="ltx_bibblock">Stacked attention networks for image question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.02274</span>, 2015.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba.

</span>
<span class="ltx_bibblock">Learning deep features for discriminative localization.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1512.04150</span>, 2015.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva.

</span>
<span class="ltx_bibblock">Learning deep features for scene recognition using places database.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, pages
487–495, 2014.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1512.02166" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1512.02167" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1512.02167">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1512.02167" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1512.02168" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 17:09:54 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
