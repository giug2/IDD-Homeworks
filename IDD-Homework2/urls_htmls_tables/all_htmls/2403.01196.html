<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021</title>
<!--Generated on Sat Mar  2 12:21:44 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.01196v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S1" title="1 Introduction ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S2" title="2 Background ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S2.SS1" title="2.1 Transformer ‣ 2 Background ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Transformer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S2.SS2" title="2.2 Domain adaptation ‣ 2 Background ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Domain adaptation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S3" title="3 Proposed Approach ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Proposed Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S3.SS1" title="3.1 Architecture Tuning ‣ 3 Proposed Approach ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Architecture Tuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S4" title="4 Empirical Evaluation ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Empirical Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S4.SS1" title="4.1 Experimental Setup ‣ 4 Empirical Evaluation ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S4.SS1.SSS1" title="4.1.1 Datasets ‣ 4.1 Experimental Setup ‣ 4 Empirical Evaluation ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S4.SS1.SSS2" title="4.1.2 Infrastructure ‣ 4.1 Experimental Setup ‣ 4 Empirical Evaluation ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Infrastructure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S4.SS1.SSS3" title="4.1.3 Metrics ‣ 4.1 Experimental Setup ‣ 4 Empirical Evaluation ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S4.SS2" title="4.2 Results ‣ 4 Empirical Evaluation ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S5" title="5 Discussion ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S6" title="6 Conclusion and Future Work ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion and Future Work</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewbox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: layout</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
</div><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2403.01196v1 [cs.CL] 02 Mar 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line"><span class="ltx_ERROR undefined" id="id1">\useunder</span>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1"><span class="ltx_text ltx_ulem_uline" id="p1.1.1"></span><span class="ltx_ERROR undefined" id="p1.1.2">\ul</span>
</p>
</div>
<h1 class="ltx_title ltx_font_bold ltx_title_document">Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="id1.1.id1">\name</span><span class="ltx_text ltx_font_bold" id="id2.2.id2">Séamus Lankford</span> <span class="ltx_ERROR undefined" id="id3.3.id3">\addr</span>seamus.lankford@adaptcentre.ie
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id4.4.id4">\addr</span>ADAPT Centre, School of Computing, Dublin City University, Dublin, Ireland.
<span class="ltx_ERROR undefined" id="id5.5.id5">\AND</span><span class="ltx_ERROR undefined" id="id6.6.id6">\name</span><span class="ltx_text ltx_font_bold" id="id7.7.id7">Haithem Afli</span> <span class="ltx_ERROR undefined" id="id8.8.id8">\addr</span>haithem.afli@adaptcentre.ie
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id9.9.id9">\addr</span>ADAPT Centre, Department of Computer Science, Munster Technological University, Ireland.
<span class="ltx_ERROR undefined" id="id10.10.id10">\AND</span><span class="ltx_ERROR undefined" id="id11.11.id11">\name</span><span class="ltx_text ltx_font_bold" id="id12.12.id12">Andy Way</span> <span class="ltx_ERROR undefined" id="id13.13.id13">\addr</span>andy.way@adaptcentre.ie
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id14.14.id14">\addr</span>ADAPT Centre, School of Computing, Dublin City University, Dublin, Ireland
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id15.id1">Translation models for the specific domain of translating Covid data from English to Irish were developed for the LoResMT 2021 shared task. Domain adaptation techniques, using a Covid-adapted generic 55k corpus from the Directorate General of Translation, were applied. Fine-tuning, mixed fine-tuning and combined dataset approaches were compared with models trained on an extended in-domain dataset. As part of this study, an English-Irish dataset of Covid related data, from the Health and Education domains, was developed. The highest-performing model used a Transformer architecture trained with an extended in-domain Covid dataset. In the context of this study, we have demonstrated that extending an 8k in-domain baseline dataset by just 5k lines improved the BLEU score by 27 points.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Neural Machine Translation (NMT) has routinely outperformed Statistical Machine Translation (SMT) when large parallel datasets are available <cite class="ltx_cite ltx_citemacro_citep">(Crego et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#bib.bib4" title="">2016</a>; Wu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#bib.bib15" title="">2016</a>)</cite>. Furthermore, Transformer based approaches have demonstrated impressive results in moderate low-resource scenarios <cite class="ltx_cite ltx_citemacro_citep">(Lankford et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#bib.bib7" title="">2021</a>)</cite>.
NMT involving Transformer model development will improve the performance in specific domains of low-resource languages  <cite class="ltx_cite ltx_citemacro_citep">(Araabi and Monz,, <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#bib.bib1" title="">2020</a>)</cite>. However, the benefits of NMT are less clear when using very low-resource Machine Translation (MT) on in-domain datasets of less than 10k lines.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The Irish language is a primary example of a low-resource language that will benefit from such research. This paper reports the results for the MT system developed for the English–Irish shared task at LoResMT 2021 <cite class="ltx_cite ltx_citemacro_citep">(Ojha et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#bib.bib8" title="">2021</a>)</cite>. Relevant work is presented in the background section followed by an overview of the proposed approach. The empirical findings are outlined in the results section. Finally, the key findings are presented and discussed.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Transformer</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">A novel architecture called Transformer was introduced in the paper ‘Attention Is All You Need’  <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#bib.bib14" title="">2017</a>)</cite>. Transformer is an architecture for transforming one sequence into another with the help of an Encoder and Decoder without relying on Recurrent Neural Networks.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Transformer models use attention to focus on previously generated tokens. This approach allows models to develop a long memory which is particularly useful in the domain of language translation.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Domain adaptation</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Domain adaptation is a proven approach in addressing the paucity of data in low-resource settings. Fine-tuning an out-of-domain model by further training with in-domain data is effective in improving the performance of translation models (Freitag and Al-Onaizan, 2016; Sennrich et al., 2016). With this approach an NMT model is initially trained using a large out-of-domain corpus. Once fully converged, the out-of-domain model is further trained by fine-tuning its parameters with a low resource in-domain corpus.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">A modification to this approach is known as mixed fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(Chu et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#bib.bib3" title="">2017</a>)</cite>. With this technique, an NMT model is trained on out-of-domain data until fully converged. This serves as a base model which is further trained using the combined in-domain and out-of-domain datasets.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Approach</h2>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="233" id="S3.F1.g1" src="extracted/5444441/is_feidir_linn_workshop.png" width="550"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Proposed Approach. Optimal hyperparameters are applied to Transformer models which are trained using one of several possible approaches. The training dataset composition is determined by the chosen approach. Models are subsequently evaluated using a suite of metrics.</figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Hyperparameter optimization of Recurrent Neural Network (RNN) models in low-resource settings has previously demonstrated considerable performance improvements <cite class="ltx_cite ltx_citemacro_citep">(Sennrich and Zhang,, <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#bib.bib11" title="">2019</a>)</cite>. The extent to which such optimization techniques may be applied to Transformer models in similar low-resource scenarios was evaluated in a previous study <cite class="ltx_cite ltx_citemacro_citep">(Lankford et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#bib.bib7" title="">2021</a>)</cite>. Evaluations included modifying the number of attention heads, the number of layers and experimenting with regularization techniques such as dropout and label smoothing. Most importantly, the choice of subword model type and the vocabulary size were evaluated.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">In order to test the effectiveness of our approach, models were trained using three English-Irish parallel datasets: a general corpus of 52k lines from the Directorate General for Translation (DGT) and two in-domain corpora of Covid data (8k and 5k lines). All experiments involved concatenating source and target corpora to create a shared vocabulary and a shared SentencePiece <cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson,, <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#bib.bib6" title="">2018</a>)</cite> subword model. The impact of using separate source and target subword models was not explored.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">The approach adopted is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S3.F1" title="Figure 1 ‣ 3 Proposed Approach ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_tag">1</span></a> and the datasets used in evaluating this approach are outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S3.T1" title="Table 1 ‣ 3 Proposed Approach ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_tag">1</span></a>. All models were developed using a Transformer architecture.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S3.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1">Approach</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1">Source</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S3.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.1">Lines</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.1.1">Covid baseline</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.1.2">Baseline</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.2.1.3">8k</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2">
<td class="ltx_td ltx_align_left" id="S3.T1.1.3.2.1">Covid extended</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.3.2.2">Baseline + Covid_DCU</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.3.2.3">13k</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.3">
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.3.1">Out-of-domain</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.3.2">DGT</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.3.3">52k</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.4">
<td class="ltx_td ltx_align_left" id="S3.T1.1.5.4.1">Fine-tuned</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.5.4.2">Baseline + Covid_DCU + DGT</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.5.4.3">65k</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.5">
<td class="ltx_td ltx_align_left" id="S3.T1.1.6.5.1">Mixed fine-tuned</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.6.5.2">Baseline + Covid_DCU + DGT</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.6.5.3">65k</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T1.1.7.6.1">Combined domains</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T1.1.7.6.2">Baseline + Covid_DCU + DGT</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S3.T1.1.7.6.3">65k</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Datasets used in proposed approach</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Architecture Tuning</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Long training times associated with NMT make it costly to tune systems using conventional Grid Search approaches. A previous study identified the hyperparameters required for optimal performance  <cite class="ltx_cite ltx_citemacro_citep">(Lankford et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#bib.bib7" title="">2021</a>)</cite>. Reducing the number of hidden layer neurons and increasing dropout led to significantly better performance. Furthermore, within the context of low-resource English to Irish translation, using a 16k BPE submodel resulted in the highest performing models. The Transformer hyperparameters, chosen in line with these findings, are outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S3.T2" title="Table 2 ‣ 3.1 Architecture Tuning ‣ 3 Proposed Approach ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table ltx_align_center" id="S3.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.1">Hyperparameter</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.2.1">Values</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.2.2.1">Learning rate</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.2.2.2">0.1, 0.01, 0.001, <span class="ltx_text ltx_font_bold" id="S3.T2.1.2.2.2.1">2</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.3.3.1">Batch size</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.3.3.2">1024, <span class="ltx_text ltx_font_bold" id="S3.T2.1.3.3.2.1">2048</span>, 4096, 8192</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.4.4.1">Attention heads</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.4.4.2">
<span class="ltx_text ltx_font_bold" id="S3.T2.1.4.4.2.1">2</span>, 4, <span class="ltx_text ltx_font_bold" id="S3.T2.1.4.4.2.2">8</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.5.5.1">Number of layers</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.5.5.2">5, <span class="ltx_text ltx_font_bold" id="S3.T2.1.5.5.2.1">6</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.6.6.1">Feed-forward dimension</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.6.6.2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.6.6.2.1">2048</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.7.7.1">Embedding dimension</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.7.7.2">128, <span class="ltx_text ltx_font_bold" id="S3.T2.1.7.7.2.1">256</span>, 512</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.8.8.1">Label smoothing</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.8.8.2">
<span class="ltx_text ltx_font_bold" id="S3.T2.1.8.8.2.1">0.1</span>, 0.3</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.9.9.1">Dropout</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.9.9.2">0.1, <span class="ltx_text ltx_font_bold" id="S3.T2.1.9.9.2.1">0.3</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.10.10.1">Attention dropout</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.10.10.2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.10.10.2.1">0.1</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S3.T2.1.11.11.1">Average Decay</th>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S3.T2.1.11.11.2">0, <span class="ltx_text ltx_font_bold" id="S3.T2.1.11.11.2.1">0.0001</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Hyperparameter optimization for Transformer models. Optimal parameters are highlighted in bold <cite class="ltx_cite ltx_citemacro_citep">(Lankford et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#bib.bib7" title="">2021</a>)</cite>.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Empirical Evaluation</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Datasets</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">The performance of the Transformer approach is evaluated on English to Irish parallel datasets in the Covid domain. Three datasets were used in the evaluation of our models. These consisted of a baseline Covid dataset (8k) provided by MT Summit 2021 <cite class="ltx_cite ltx_citemacro_citep">(Ojha et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#bib.bib8" title="">2021</a>)</cite>, an in-domain Covid dataset (5k) developed at DCU and a publicly available out-of-domain dataset (52k) provided by DGT <cite class="ltx_cite ltx_citemacro_citep">(Steinberger et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#bib.bib13" title="">2013</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Infrastructure</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">Models were developed using a lab of machines each of which has an AMD Ryzen 7 2700X processor, 16 GB memory, a 256 SSD and an NVIDIA GeForce GTX 1080 Ti. Rapid prototype development was enabled through a Google Colab Pro subscription using NVIDIA Tesla P100 PCIe 16 GB graphic cards and up to 27GB of memory when available <cite class="ltx_cite ltx_citemacro_citep">(Bisong,, <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#bib.bib2" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1">Our MT models were trained using the Pytorch implementation of OpenNMT 2.0, an open-source toolkit for NMT <cite class="ltx_cite ltx_citemacro_citep">(Klein et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#bib.bib5" title="">2017</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Metrics</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">Automated metrics were used to determine the translation quality. All models were trained and evaluated using the BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#bib.bib9" title="">2002</a>)</cite>, TER <cite class="ltx_cite ltx_citemacro_citep">(Snover et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#bib.bib12" title="">2006</a>)</cite> and ChrF <cite class="ltx_cite ltx_citemacro_citep">(Popović,, <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#bib.bib10" title="">2015</a>)</cite> evaluation metrics. Case-insensitive BLEU scores, at the corpus level, are reported. Model training was stopped once an early stopping criteria of no improvement in validation accuracy for 4 consecutive iterations was recorded.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Experimental results achieved using a Transformer architecture, with either 2 or 8 attention heads, are summarized in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S4.T3" title="Table 3 ‣ 4.2 Results ‣ 4 Empirical Evaluation ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_tag">3</span></a> and in Table <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S4.T4" title="Table 4 ‣ 4.2 Results ‣ 4 Empirical Evaluation ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_tag">4</span></a>. Clearly in the context of our low-resource experiments, it can be seen there is little performance difference using Transformer architectures with a differing number of attention heads. The largest difference occurs when using a fine-tuned approach (2.1 BLEU points). However the difference between a 2 head and an 8 head approach is less than 1 BLEU point for all other models. The highest performing approach uses the extended Covid dataset (13k) which is a combination of the MT summit Covid baseline and a custom DCU Covid dataset. This Transformer model, with 2 heads, performs well across all key translation metrics (BLEU: 36.0, TER: 0.63 and ChrF3: 0.32).</p>
</div>
<figure class="ltx_figure" id="S4.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2" id="S4.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="318" id="S4.F1.sf1.g1" src="extracted/5444441/lores_bleu.png" width="524"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>BLEU</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_flex_size_2" id="S4.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="320" id="S4.F1.sf2.g1" src="extracted/5444441/lores_ter.png" width="524"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>TER</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Translation performance of all approaches using Transformers with 2 heads</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T3.3.3.4"><span class="ltx_text ltx_font_bold" id="S4.T3.3.3.4.1">System</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T3.3.3.5">
<span class="ltx_text ltx_font_bold" id="S4.T3.3.3.5.1">Heads</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T3.3.3.6">
<span class="ltx_text ltx_font_bold" id="S4.T3.3.3.6.1">Lines</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T3.3.3.7">
<span class="ltx_text ltx_font_bold" id="S4.T3.3.3.7.1">Steps</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1">BLEU<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S4.T3.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">normal-↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T3.2.2.2">
<span class="ltx_text ltx_font_bold" id="S4.T3.2.2.2.1">TER<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.2.2.2.1.m1.1"><semantics id="S4.T3.2.2.2.1.m1.1a"><mo id="S4.T3.2.2.2.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S4.T3.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.m1.1b"><ci id="S4.T3.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1">normal-↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.1.m1.1d">↓</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T3.3.3.3">
<span class="ltx_text ltx_font_bold" id="S4.T3.3.3.3.1">ChrF3<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.3.3.3.1.m1.1"><semantics id="S4.T3.3.3.3.1.m1.1a"><mo id="S4.T3.3.3.3.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S4.T3.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.1.m1.1b"><ci id="S4.T3.3.3.3.1.m1.1.1.cmml" xref="S4.T3.3.3.3.1.m1.1.1">normal-↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.1.m1.1d">↑</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.3.4.1.1">Covid baseline</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T3.3.4.1.2">2</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.4.1.3">8k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.4.1.4">35k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.4.1.5">9.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.4.1.6">0.89</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.4.1.7">0.32</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.3.5.2.1">Covid extended</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T3.3.5.2.2">2</th>
<td class="ltx_td ltx_align_center" id="S4.T3.3.5.2.3">13k</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.5.2.4">35k</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.5.2.5">36.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.5.2.6">0.63</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.5.2.7">0.54</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.3.6.3.1">Out-of-domain</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T3.3.6.3.2">2</th>
<td class="ltx_td ltx_align_center" id="S4.T3.3.6.3.3">52k</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.6.3.4">200k</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.6.3.5">13.9</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.6.3.6">0.80</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.6.3.7">0.41</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.3.7.4.1">Fine-tuned</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T3.3.7.4.2">2</th>
<td class="ltx_td ltx_align_center" id="S4.T3.3.7.4.3">65k</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.7.4.4">35k</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.7.4.5">22.9</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.7.4.6">0.64</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.7.4.7">0.42</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.3.8.5.1">Mixed fine-tuned</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T3.3.8.5.2">2</th>
<td class="ltx_td ltx_align_center" id="S4.T3.3.8.5.3">65k</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.8.5.4">35k</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.8.5.5">18.2</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.8.5.6">0.71</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.8.5.7">0.42</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T3.3.9.6.1">Combined domains</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S4.T3.3.9.6.2">2</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.3.9.6.3">65k</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.3.9.6.4">35k</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.3.9.6.5">32.2</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.3.9.6.6">0.59</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.3.9.6.7">0.55</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of optimized Transformer performance with 2 attention heads</figcaption>
</figure>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T4.3.3.4"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.4.1">System</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T4.3.3.5">
<span class="ltx_text ltx_font_bold" id="S4.T4.3.3.5.1">Heads</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.3.3.6">
<span class="ltx_text ltx_font_bold" id="S4.T4.3.3.6.1">Lines</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.3.3.7">
<span class="ltx_text ltx_font_bold" id="S4.T4.3.3.7.1">Steps</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.1.1">
<span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1">BLEU<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.1.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.1.m1.1a"><mo id="S4.T4.1.1.1.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S4.T4.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">normal-↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.2.2.2">
<span class="ltx_text ltx_font_bold" id="S4.T4.2.2.2.1">TER<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.2.2.2.1.m1.1"><semantics id="S4.T4.2.2.2.1.m1.1a"><mo id="S4.T4.2.2.2.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S4.T4.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.1.m1.1b"><ci id="S4.T4.2.2.2.1.m1.1.1.cmml" xref="S4.T4.2.2.2.1.m1.1.1">normal-↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.1.m1.1d">↓</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.3.3.3">
<span class="ltx_text ltx_font_bold" id="S4.T4.3.3.3.1">ChrF3<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.3.3.3.1.m1.1"><semantics id="S4.T4.3.3.3.1.m1.1a"><mo id="S4.T4.3.3.3.1.m1.1.1" mathvariant="normal" stretchy="false" xref="S4.T4.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.1.m1.1b"><ci id="S4.T4.3.3.3.1.m1.1.1.cmml" xref="S4.T4.3.3.3.1.m1.1.1">normal-↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.3.1.m1.1d">↑</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.3.4.1.1">Covid baseline</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T4.3.4.1.2">8</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.4.1.3">8k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.4.1.4">35k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.4.1.5">9.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.4.1.6">0.91</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.4.1.7">0.33</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.3.5.2.1">Covid extended</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T4.3.5.2.2">8</th>
<td class="ltx_td ltx_align_center" id="S4.T4.3.5.2.3">13k</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.5.2.4">35k</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.5.2.5">35.7</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.5.2.6">0.61</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.5.2.7">0.55</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.3.6.3.1">Out-of-domain</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T4.3.6.3.2">8</th>
<td class="ltx_td ltx_align_center" id="S4.T4.3.6.3.3">52k</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.6.3.4">200k</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.6.3.5">13.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.6.3.6">0.80</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.6.3.7">0.40</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.3.7.4.1">Fine-tuned</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T4.3.7.4.2">8</th>
<td class="ltx_td ltx_align_center" id="S4.T4.3.7.4.3">65k</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.7.4.4">35k</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.7.4.5">25.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.7.4.6">0.63</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.7.4.7">0.43</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.3.8.5.1">Mixed fine-tuned</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T4.3.8.5.2">8</th>
<td class="ltx_td ltx_align_center" id="S4.T4.3.8.5.3">65k</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.8.5.4">35k</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.8.5.5">18.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.8.5.6">0.71</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.8.5.7">0.42</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T4.3.9.6.1">Combined domains</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S4.T4.3.9.6.2">8</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.3.9.6.3">65k</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.3.9.6.4">35k</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.3.9.6.5">32.8</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.3.9.6.6">0.59</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.3.9.6.7">0.57</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of optimized Transformer performance with 8 attention heads</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">The worst performing model uses the Covid baseline which is not surprising given that only 8k lines are available. The performance of the higher resourced models (out-of-domain, fine-tuned, mixed fine-tuned and combined domains) all lag that of the Covid extended model. In particular, the out-of-domain model, using the DGT dataset, performs very poorly with a BLEU score of just 13.9 on a Transformer model with 2 heads.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">The BLEU and TER scores for all approaches are illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S4.F1.sf1" title="1(a) ‣ Figure 2 ‣ 4.2 Results ‣ 4 Empirical Evaluation ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_tag">1(a)</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#S4.F1.sf2" title="1(b) ‣ Figure 2 ‣ 4.2 Results ‣ 4 Empirical Evaluation ‣ Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021"><span class="ltx_text ltx_ref_tag">1(b)</span></a>. As expected, there is a high level of inverse correlation between BLEU and TER. Well-performing models, with high BLEU scores, also required little post editing effort as indicated by their lower TER scores.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Standard Transformer parameters identified in a previous study were observed to perform well <cite class="ltx_cite ltx_citemacro_citep">(Lankford et al.,, <a class="ltx_ref" href="https://arxiv.org/html/2403.01196v1#bib.bib7" title="">2021</a>)</cite>. Reducing hidden neurons to 256 and increasing regularization dropout to 0.3 improved translation performance and these hyperparameters were chosen when building all Transformer models. Furthermore a batch size of 2048 and using 6 layers for the encoder / decoder were chosen throughout.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">The results demonstrate that translation performance for specific domains is driven by the amount of data which is available for that specific domain. It is noteworthy that an in-domain dataset of 13k lines (Covid extended), trained for just 35k steps outperformed by 22.1 BLEU points the corresponding out-of-domain 52k dataset (DGT) which was trained for 200k steps.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Future Work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In the official evaluation for LoResMT 2021, our English–Irish system was ranked first according to the BLEU, TER and ChrF scores. We demonstrate that a high performing in-domain translation model can be built with a dataset of 13k lines. Developing a small in-domain dataset, of just 5k lines, improved the BLEU score by 27 points when models were trained with the combined Covid baseline and custom Covid dataset.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Following on from our previous work, careful selection of Transformer hyperparameters, and using a 16k BPE SentencePiece submodel, enabled rapid development of high performing translation models in a low-resource setting.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Within the context of our research in low-resource English to Irish translation, we have shown that augmenting in-domain data, by a small amount, performed better than approaches which incorporate fine-tuning, mixed fine-tuning or the combination of domains.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">As part of our future work, we plan to develop English-Irish MT models trained on a dataset derived from the health domain. Domain adaptation, through fine-tuning such models with the Covid extended dataset may further improve Covid MT performance.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was supported by ADAPT, which is funded under the SFI Research Centres Programme (Grant 13/RC/2016) and is co-funded by the European Regional Development Fund. This research was also funded by the Munster Technological University.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Araabi and Monz,  (2020)</span>
<span class="ltx_bibblock">
Araabi, A. and Monz, C. (2020).

</span>
<span class="ltx_bibblock">Optimizing transformer for low-resource neural machine translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2011.02266</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisong,  (2019)</span>
<span class="ltx_bibblock">
Bisong, E. (2019).

</span>
<span class="ltx_bibblock">Google colaboratory.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Building Machine Learning and Deep Learning Models on Google Cloud Platform</span>, pages 59–64. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et al.,  (2017)</span>
<span class="ltx_bibblock">
Chu, C., Dabre, R., and Kurohashi, S. (2017).

</span>
<span class="ltx_bibblock">An empirical comparison of domain adaptation methods for neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</span>, pages 385–391.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Crego et al.,  (2016)</span>
<span class="ltx_bibblock">
Crego, J., Kim, J., Klein, G., Rebollo, A., Yang, K., Senellart, J., Akhanov, E., Brunelle, P., Coquard, A., Deng, Y., et al. (2016).

</span>
<span class="ltx_bibblock">Systran’s pure neural machine translation systems.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:1610.05540</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Klein et al.,  (2017)</span>
<span class="ltx_bibblock">
Klein, G., Kim, Y., Deng, Y., Senellart, J., and Rush, A. M. (2017).

</span>
<span class="ltx_bibblock">Opennmt: Open-source toolkit for neural machine translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:1701.02810</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson,  (2018)</span>
<span class="ltx_bibblock">
Kudo, T. and Richardson, J. (2018).

</span>
<span class="ltx_bibblock">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:1808.06226</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lankford et al.,  (2021)</span>
<span class="ltx_bibblock">
Lankford, S., Alfi, H., and Way, A. (2021).

</span>
<span class="ltx_bibblock">Transformers for low resource languages: Is feidir linn.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 18th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ojha et al.,  (2021)</span>
<span class="ltx_bibblock">
Ojha, A. K., Liu, C.-H., Kann, K., Ortega, J., Satam, S., and Fransen, T. (2021).

</span>
<span class="ltx_bibblock">Findings of the LoResMT 2021 Shared Task on COVID and Sign Language for Low-Resource Languages.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 4th Workshop on Technologies for MT of Low Resource Languages</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al.,  (2002)</span>
<span class="ltx_bibblock">
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002).

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</span>, pages 311–318.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popović,  (2015)</span>
<span class="ltx_bibblock">
Popović, M. (2015).

</span>
<span class="ltx_bibblock">chrf: character n-gram f-score for automatic mt evaluation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Proceedings of the Tenth Workshop on Statistical Machine Translation</span>, pages 392–395.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich and Zhang,  (2019)</span>
<span class="ltx_bibblock">
Sennrich, R. and Zhang, B. (2019).

</span>
<span class="ltx_bibblock">Revisiting low-resource neural machine translation: A case study.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:1905.11901</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snover et al.,  (2006)</span>
<span class="ltx_bibblock">
Snover, M., Dorr, B., Schwartz, R., Micciulla, L., and Makhoul, J. (2006).

</span>
<span class="ltx_bibblock">A study of translation edit rate with targeted human annotation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Proceedings of association for machine translation in the Americas</span>, volume 200. Citeseer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Steinberger et al.,  (2013)</span>
<span class="ltx_bibblock">
Steinberger, R., Eisele, A., Klocek, S., Pilos, S., and Schlüter, P. (2013).

</span>
<span class="ltx_bibblock">Dgt-tm: A freely available translation memory in 22 languages.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:1309.5226</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al.,  (2017)</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017).

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:1706.03762</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al.,  (2016)</span>
<span class="ltx_bibblock">
Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., et al. (2016).

</span>
<span class="ltx_bibblock">Google’s neural machine translation system: Bridging the gap between human and machine translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:1609.08144</span>.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Mar  2 12:21:44 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
