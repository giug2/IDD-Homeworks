<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2201.08992] Enhancing and Dissecting Crowd Counting By Synthetic Data</title><meta property="og:description" content="In this article, we propose a simulated crowd counting dataset CrowdX, which has a large scale, accurate labeling, parameterized realization, and high fidelity. The experimental results of using this dataset as data en…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Enhancing and Dissecting Crowd Counting By Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Enhancing and Dissecting Crowd Counting By Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2201.08992">

<!--Generated on Wed Mar  6 11:10:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Enhancing and Dissecting Crowd Counting By Synthetic Data</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">In this article, we propose a simulated crowd counting dataset CrowdX, which has a large scale, accurate labeling, parameterized realization, and high fidelity. The experimental results of using this dataset as data enhancement show that the performance of the proposed streamlined and efficient benchmark network ESA-Net can be improved by 8.4%. The other two classic heterogeneous architectures MCNN and CSRNet pre-trained on CrowdX also show significant performance improvements. Considering many influencing factors determine performance, such as background, camera angle, human density, and resolution. Although these factors are important, there is still a lack of research on how they affect crowd counting. Thanks to the CrowdX dataset with rich annotation information, we conduct a large number of data-driven comparative experiments to analyze these factors. Our research provides a reference for a deeper understanding of the crowd counting problem and puts forward some useful suggestions in the actual deployment of the algorithm.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">— </span></span>
Crowd Counting, Crowd Density Estimation, Synthetic Data, Domain Transfer</p>
</div>
<section id="S1" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The application of crowd counting is very wide, from the construction of smart cities to the statistics of passenger flow in front of shops, all of which need the help of crowd counting algorithms.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">A lot of work has been proposed to improve the performance of detection algorithms. These studies either focus on proposing more advanced network structures (for example multi-column network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, scale aggregation module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and scale adaptive module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>), or focus on designing more suitable loss functions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. These two focus points have greatly improved the performance of existing algorithms. In addition to the work on the algorithm side, on the dataset side, some open-source crowd counting datasets have also been proposed. The mainstream ones are UCFCC50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, ShanghaiTech A (ST_A) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, UCSD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, ShanghaiTech B (ST_B) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, and WorldExpo’10 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Among them, the number of people in UCFCC50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and ShanghaiTech A (ST_A) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is very crowded, and some even reach 3000 people in a single picture. These two datasets are collected from the Internet, and there are no unified scene and shooting characteristics. The other three datasets are captured from surveillance cameras with large differences in viewing angles, so the number of people is generally not crowded.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2201.08992/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="230" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text ltx_font_bold">Fig. 1</span>: </span>The left column shows a real image and a CrowdX image. The ground truth and the estimation results are shown in middle and right column.</figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2201.08992/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="139" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.2.1.1" class="ltx_text ltx_font_bold">Fig. 2</span>: </span>The proposed CrowdX dataset.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">These studies and datasets have improved the accuracy of crowd counting algorithms. However, these methods often have poor and unexplainable performance when they are migrated to the real environment. In summary, there are currently two issues that need to be resolved. 1) As shown in the table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Enhancing and Dissecting Crowd Counting By Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the existing real-world datasets are generally not large, and insufficient training data will cause data-driven methods to be unable to learn diverse, complex, and complete features. To solve this problem, a simple method is to annotate large amounts of data. However, manually annotating them is very laborious and expensive. 2) Scene background, camera angle of view, and crowd density have a great influence on crowd counting algorithms. There is currently a lack of research on how they affect performance. For the first question, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> proposed a simulation dataset GCC, which is constructed by game scenes and can be automatically labeled, but this method cannot control the key influence variables. For the second question, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> proposed a method for head size adaptation, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> proposed an architecture for head resolution adaptation, and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> proposes a training data selection strategy to improve algorithm performance. Although these methods have improved some performance, they only consider a single variable.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In our work, we refer to the application of simulated datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> in other fields, and construct a crowd simulation dataset that parameterizes the influencing factors to solve the first problem. To solve the second problem, we design a data-driven strategy to analyze the impact of key factors utilizing comparative experiments. Specifically, we propose a dataset generator based on the Unity3D<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, which can parametrically control the influencing factors and generate a very realistic simulation dataset. We call the generated dataset CrowdX, which contains 24,000 automatically annotated crowd images. Compared with existing real-world datasets, CrowdX is easy to collect and annotate. Compared with the existing simulation dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, CrowdX is more realistic and can control influencing factors.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In the experimental stage, to evaluate the effectiveness of using CrowdX for data enhancement, we first propose a streamlined and efficient benchmark network ESA-Net, then pre-train the network on CrowdX, and finally fine-tune the network with real-world datasets. The results show that the accuracy of the benchmark network on the ShanghaiTech B dataset has increased by 8.4% and the accuracy has reached the state-of-the-art. In addition, the accuracy of the other two methods MCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and CSRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> has also been significantly improved, which verifies the effectiveness of using CrowdX as a data augmentation method. To evaluate the impact of the key factors in the performance, we have done many comparative experiments and find some interesting conclusions. Based on these findings, we provide some suggestions for selecting training datasets and deploying surveillance cameras.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.10.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Statistics of some widely used crowd datasets. PC means partially parameterizd and FC means fully parameterizd.</figcaption>
<div id="S1.T1.8" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:345.6pt;height:137.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-30.5pt,12.1pt) scale(0.85,0.85) ;">
<table id="S1.T1.8.8" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.8.8.9.1" class="ltx_tr">
<th id="S1.T1.8.8.9.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></th>
<th id="S1.T1.8.8.9.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Number</th>
<th id="S1.T1.8.8.9.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Resolution</th>
<th id="S1.T1.8.8.9.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Average</th>
<th id="S1.T1.8.8.9.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Type</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.1.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">UCF_CC_50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</td>
<td id="S1.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">50</td>
<td id="S1.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">2101<math id="S1.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.1.1.1.1.m1.1a"><mo id="S1.T1.1.1.1.1.m1.1.1" xref="S1.T1.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.1.1.1.1.m1.1b"><times id="S1.T1.1.1.1.1.m1.1.1.cmml" xref="S1.T1.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.1.1.1.1.m1.1c">\times</annotation></semantics></math>2888</td>
<td id="S1.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">1279</td>
<td id="S1.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Real</td>
</tr>
<tr id="S1.T1.2.2.2" class="ltx_tr">
<td id="S1.T1.2.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">ST_A <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
<td id="S1.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">482</td>
<td id="S1.T1.2.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">589<math id="S1.T1.2.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.2.2.2.1.m1.1a"><mo id="S1.T1.2.2.2.1.m1.1.1" xref="S1.T1.2.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.2.2.2.1.m1.1b"><times id="S1.T1.2.2.2.1.m1.1.1.cmml" xref="S1.T1.2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.2.2.2.1.m1.1c">\times</annotation></semantics></math>868</td>
<td id="S1.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">501</td>
<td id="S1.T1.2.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Real</td>
</tr>
<tr id="S1.T1.3.3.3" class="ltx_tr">
<td id="S1.T1.3.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">ST_B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
<td id="S1.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">716</td>
<td id="S1.T1.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">768<math id="S1.T1.3.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.3.3.3.1.m1.1a"><mo id="S1.T1.3.3.3.1.m1.1.1" xref="S1.T1.3.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.3.3.3.1.m1.1b"><times id="S1.T1.3.3.3.1.m1.1.1.cmml" xref="S1.T1.3.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.3.3.3.1.m1.1c">\times</annotation></semantics></math>1024</td>
<td id="S1.T1.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">123</td>
<td id="S1.T1.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Real</td>
</tr>
<tr id="S1.T1.4.4.4" class="ltx_tr">
<td id="S1.T1.4.4.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">WorldExpo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S1.T1.4.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">3980</td>
<td id="S1.T1.4.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">576<math id="S1.T1.4.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.4.4.4.1.m1.1a"><mo id="S1.T1.4.4.4.1.m1.1.1" xref="S1.T1.4.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.4.4.4.1.m1.1b"><times id="S1.T1.4.4.4.1.m1.1.1.cmml" xref="S1.T1.4.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.4.4.4.1.m1.1c">\times</annotation></semantics></math>720</td>
<td id="S1.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">50</td>
<td id="S1.T1.4.4.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Real</td>
</tr>
<tr id="S1.T1.5.5.5" class="ltx_tr">
<td id="S1.T1.5.5.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">UCSD<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</td>
<td id="S1.T1.5.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">2000</td>
<td id="S1.T1.5.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">158<math id="S1.T1.5.5.5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.5.5.5.1.m1.1a"><mo id="S1.T1.5.5.5.1.m1.1.1" xref="S1.T1.5.5.5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.5.5.5.1.m1.1b"><times id="S1.T1.5.5.5.1.m1.1.1.cmml" xref="S1.T1.5.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.5.5.5.1.m1.1c">\times</annotation></semantics></math>238</td>
<td id="S1.T1.5.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">25</td>
<td id="S1.T1.5.5.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Real</td>
</tr>
<tr id="S1.T1.6.6.6" class="ltx_tr">
<td id="S1.T1.6.6.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">UCF_QNRF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</td>
<td id="S1.T1.6.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">1525</td>
<td id="S1.T1.6.6.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">2013<math id="S1.T1.6.6.6.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.6.6.6.1.m1.1a"><mo id="S1.T1.6.6.6.1.m1.1.1" xref="S1.T1.6.6.6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.6.6.6.1.m1.1b"><times id="S1.T1.6.6.6.1.m1.1.1.cmml" xref="S1.T1.6.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.6.6.6.1.m1.1c">\times</annotation></semantics></math>2902</td>
<td id="S1.T1.6.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">815</td>
<td id="S1.T1.6.6.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Real</td>
</tr>
<tr id="S1.T1.7.7.7" class="ltx_tr">
<td id="S1.T1.7.7.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">GCC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S1.T1.7.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">15212</td>
<td id="S1.T1.7.7.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">1080<math id="S1.T1.7.7.7.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.7.7.7.1.m1.1a"><mo id="S1.T1.7.7.7.1.m1.1.1" xref="S1.T1.7.7.7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.7.7.7.1.m1.1b"><times id="S1.T1.7.7.7.1.m1.1.1.cmml" xref="S1.T1.7.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.7.7.7.1.m1.1c">\times</annotation></semantics></math>1920</td>
<td id="S1.T1.7.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">501</td>
<td id="S1.T1.7.7.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Syn(PC)</td>
</tr>
<tr id="S1.T1.8.8.8" class="ltx_tr">
<td id="S1.T1.8.8.8.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">CrowdX</td>
<td id="S1.T1.8.8.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">24000</td>
<td id="S1.T1.8.8.8.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">768<math id="S1.T1.8.8.8.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.8.8.8.1.m1.1a"><mo id="S1.T1.8.8.8.1.m1.1.1" xref="S1.T1.8.8.8.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.8.8.8.1.m1.1b"><times id="S1.T1.8.8.8.1.m1.1.1.cmml" xref="S1.T1.8.8.8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.8.8.8.1.m1.1c">\times</annotation></semantics></math>1024</td>
<td id="S1.T1.8.8.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">500</td>
<td id="S1.T1.8.8.8.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Syn(FC)</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S1.F3" class="ltx_figure"><img src="/html/2201.08992/assets/x3.png" id="S1.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="70" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F3.2.1.1" class="ltx_text ltx_font_bold">Fig. 3</span>: </span>The proposed efficient self-attention network ESA-Net.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Proposed Synsetic Dataset: CrowdX</h2>

<section id="S2.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Assets and Tools Description</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Based on the work of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, we choose the PersonX dataset originally used for person re-identification to construct CrowdX. PersonX contains 1,266 3D pedestrian models with diversified heights, weights, skin colors, hairstyles, and clothing. In addition to static properties, the dynamic features are also editable (such as standing, walking, and running), which makes the generated simulation dataset highly realistic. In terms of the simulation engine, we chose to use Unity3D to build the dataset. With its excellent rendering capabilities, we can simulate scenes, pedestrians, cameras, lights, etc. Considering that the crowd counting algorithm tends to be used in urban scenes, we select three urban scenes from the Unity3D asset store, all of which contain urban components such as buildings, urban roads, and traffic lights. In addition to these virtual backgrounds, we also use five solid color backgrounds to build CrowdX. The generated crowd image is shown in <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Enhancing and Dissecting Crowd Counting By Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Proposed Generation Method</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">For the virtual background, we manually select some sampling points from the three simulated cities to place pedestrians. These sampling points include pedestrian streets, parks, motor vehicles, etc., with strong diversity and high fidelity. For the solid color background, we directly sample 5 colors as the background. For each scene, we set the camera pitch angle to 30°, 50°, 70°, and 90°to take photos. To be consistent with the actual application and simplify the process, we set the roll value of the camera’s Euler angle to 0 and fix the distance from the camera to the center of the scene. We sample a random number from 1 to 1000 and add the corresponding number of pedestrians to the scene with 100 as the step. The standing position is obtained by randomly sampling the visible area of the scene, and the standing direction is sampled according to a Gaussian distribution. The mean value of this Gaussian distribution is randomly generated, which can be considered as the main direction of the flow of people</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Properties of CrowdX</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">As shown in the figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Enhancing and Dissecting Crowd Counting By Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, CrowdX is composed of crowd images with a simulated city background and a solid color background, with a total of 24,000 automatically labeled simulated images. The scale of this dataset exceeds other existing datasets and provides richer annotation information. Pedestrian annotation parameters include each pedestrian’s id, 3D position, 2D camera plane position, height, and standing direction. The annotation parameters of the scene include the ID and type of the scene. The camera’s annotation parameters include position, rotation, and rendering resolution.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<section id="S3.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Efficient benchmark Method: ESA-Net</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The proposed ESA-Net architecture is shown in Fig <a href="#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ Enhancing and Dissecting Crowd Counting By Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, which consists of four parts, including 1) VGG16 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> based front-end for feature representation; 2) Two consecutive self-attention blocks for feature refining; 3) Two consecutive dilated convolutional layers used to expand the receptive field; 4) a <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mn id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">1</cn><cn type="integer" id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">1\times 1</annotation></semantics></math> convolutional layer at the end of the network, used to predict the density map. The network is streamlined and efficient enough, that is suitable to use in a production environment, compared with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. We use L2 loss to train the benchmark network, and use mean absolute error (MAE) and mean square error (MSE) to evaluate performance, which is defined as:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="MAE=\frac{1}{N}\sum_{i=1}^{N}{|C_{i}-C_{i}^{GT}|}\ \ \ MSE=\sqrt{\frac{1}{N}\sum_{i=1}^{N}{|C_{i}-C_{i}^{GT}|}^{2}}" display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3.2" xref="S3.E1.m1.3.3.3.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.3.2" xref="S3.E1.m1.2.2.1.1.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.3.1" xref="S3.E1.m1.2.2.1.1.3.1.cmml">​</mo><mi id="S3.E1.m1.2.2.1.1.3.3" xref="S3.E1.m1.2.2.1.1.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.3.1a" xref="S3.E1.m1.2.2.1.1.3.1.cmml">​</mo><mi id="S3.E1.m1.2.2.1.1.3.4" xref="S3.E1.m1.2.2.1.1.3.4.cmml">E</mi></mrow><mo id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml"><mfrac id="S3.E1.m1.2.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.3.cmml"><mn id="S3.E1.m1.2.2.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.1.3.2.cmml">1</mn><mi id="S3.E1.m1.2.2.1.1.1.3.3" xref="S3.E1.m1.2.2.1.1.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.2.2.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.cmml"><munderover id="S3.E1.m1.2.2.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E1.m1.2.2.1.1.1.1.2.2.2" xref="S3.E1.m1.2.2.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.2.2.3" xref="S3.E1.m1.2.2.1.1.1.1.2.2.3.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.2.2.3.2" xref="S3.E1.m1.2.2.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E1.m1.2.2.1.1.1.1.2.2.3.1" xref="S3.E1.m1.2.2.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E1.m1.2.2.1.1.1.1.2.2.3.3" xref="S3.E1.m1.2.2.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.2.2.1.1.1.1.2.3" xref="S3.E1.m1.2.2.1.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.2.1.cmml">|</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.2.cmml">C</mi><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.2.cmml">C</mi><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.3.cmml">i</mi><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.3.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.3.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.3.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.3.3.cmml">T</mi></mrow></msubsup></mrow><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow></mrow><mspace width="1.5em" id="S3.E1.m1.3.3.2.3" xref="S3.E1.m1.3.3.3a.cmml"></mspace><mrow id="S3.E1.m1.3.3.2.2" xref="S3.E1.m1.3.3.2.2.cmml"><mrow id="S3.E1.m1.3.3.2.2.2" xref="S3.E1.m1.3.3.2.2.2.cmml"><mi id="S3.E1.m1.3.3.2.2.2.2" xref="S3.E1.m1.3.3.2.2.2.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.2.2.2.1" xref="S3.E1.m1.3.3.2.2.2.1.cmml">​</mo><mi id="S3.E1.m1.3.3.2.2.2.3" xref="S3.E1.m1.3.3.2.2.2.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.2.2.2.1a" xref="S3.E1.m1.3.3.2.2.2.1.cmml">​</mo><mi id="S3.E1.m1.3.3.2.2.2.4" xref="S3.E1.m1.3.3.2.2.2.4.cmml">E</mi></mrow><mo id="S3.E1.m1.3.3.2.2.1" xref="S3.E1.m1.3.3.2.2.1.cmml">=</mo><msqrt id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mfrac id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml"><mn id="S3.E1.m1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.3.2.cmml">1</mn><mi id="S3.E1.m1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><munderover id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E1.m1.1.1.1.1.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E1.m1.1.1.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.2.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2.3.2" xref="S3.E1.m1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E1.m1.1.1.1.1.2.2.3.1" xref="S3.E1.m1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E1.m1.1.1.1.1.2.2.3.3" xref="S3.E1.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.2.3.cmml">N</mi></munderover><msup id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.1.cmml">|</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml">C</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.2.cmml">C</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.3.cmml">i</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml">T</mi></mrow></msubsup></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.2.1.cmml">|</mo></mrow><mn id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow></msqrt></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3a.cmml" xref="S3.E1.m1.3.3.2.3">formulae-sequence</csymbol><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1"><eq id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2"></eq><apply id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3"><times id="S3.E1.m1.2.2.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.3.1"></times><ci id="S3.E1.m1.2.2.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.3.2">𝑀</ci><ci id="S3.E1.m1.2.2.1.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.3.3">𝐴</ci><ci id="S3.E1.m1.2.2.1.1.3.4.cmml" xref="S3.E1.m1.2.2.1.1.3.4">𝐸</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"><times id="S3.E1.m1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.2"></times><apply id="S3.E1.m1.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3"><divide id="S3.E1.m1.2.2.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.3"></divide><cn type="integer" id="S3.E1.m1.2.2.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.3.2">1</cn><ci id="S3.E1.m1.2.2.1.1.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3.3">𝑁</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1"><apply id="S3.E1.m1.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2">superscript</csymbol><apply id="S3.E1.m1.2.2.1.1.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2">subscript</csymbol><sum id="S3.E1.m1.2.2.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2.2.2"></sum><apply id="S3.E1.m1.2.2.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2.2.3"><eq id="S3.E1.m1.2.2.1.1.1.1.2.2.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2.2.3.1"></eq><ci id="S3.E1.m1.2.2.1.1.1.1.2.2.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E1.m1.2.2.1.1.1.1.2.2.3.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.2.2.1.1.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2.3">𝑁</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1"><abs id="S3.E1.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2"></abs><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1"><minus id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.2">𝐶</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.2">𝐶</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.2.3">𝑖</ci></apply><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.3"><times id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.3.1"></times><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.3.2">𝐺</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.3.3">𝑇</ci></apply></apply></apply></apply></apply></apply></apply><apply id="S3.E1.m1.3.3.2.2.cmml" xref="S3.E1.m1.3.3.2.2"><eq id="S3.E1.m1.3.3.2.2.1.cmml" xref="S3.E1.m1.3.3.2.2.1"></eq><apply id="S3.E1.m1.3.3.2.2.2.cmml" xref="S3.E1.m1.3.3.2.2.2"><times id="S3.E1.m1.3.3.2.2.2.1.cmml" xref="S3.E1.m1.3.3.2.2.2.1"></times><ci id="S3.E1.m1.3.3.2.2.2.2.cmml" xref="S3.E1.m1.3.3.2.2.2.2">𝑀</ci><ci id="S3.E1.m1.3.3.2.2.2.3.cmml" xref="S3.E1.m1.3.3.2.2.2.3">𝑆</ci><ci id="S3.E1.m1.3.3.2.2.2.4.cmml" xref="S3.E1.m1.3.3.2.2.2.4">𝐸</ci></apply><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><root id="S3.E1.m1.1.1a.cmml" xref="S3.E1.m1.1.1"></root><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><times id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3"><divide id="S3.E1.m1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.3"></divide><cn type="integer" id="S3.E1.m1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.3.2">1</cn><ci id="S3.E1.m1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.3.3">𝑁</ci></apply><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2">subscript</csymbol><sum id="S3.E1.m1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2"></sum><apply id="S3.E1.m1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3"><eq id="S3.E1.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E1.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.2.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.3">𝑁</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><abs id="S3.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2"></abs><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.2">𝐶</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.2">𝐶</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.2.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.1"></times><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.2">𝐺</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.3.3">𝑇</ci></apply></apply></apply></apply><cn type="integer" id="S3.E1.m1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">MAE=\frac{1}{N}\sum_{i=1}^{N}{|C_{i}-C_{i}^{GT}|}\ \ \ MSE=\sqrt{\frac{1}{N}\sum_{i=1}^{N}{|C_{i}-C_{i}^{GT}|}^{2}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.5" class="ltx_p">where <math id="S3.SS1.p1.2.m1.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="S3.SS1.p1.2.m1.1a"><msub id="S3.SS1.p1.2.m1.1.1" xref="S3.SS1.p1.2.m1.1.1.cmml"><mi id="S3.SS1.p1.2.m1.1.1.2" xref="S3.SS1.p1.2.m1.1.1.2.cmml">C</mi><mi id="S3.SS1.p1.2.m1.1.1.3" xref="S3.SS1.p1.2.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m1.1b"><apply id="S3.SS1.p1.2.m1.1.1.cmml" xref="S3.SS1.p1.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m1.1.1.1.cmml" xref="S3.SS1.p1.2.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m1.1.1.2.cmml" xref="S3.SS1.p1.2.m1.1.1.2">𝐶</ci><ci id="S3.SS1.p1.2.m1.1.1.3.cmml" xref="S3.SS1.p1.2.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m1.1c">C_{i}</annotation></semantics></math> is the estiamted number of people, and <math id="S3.SS1.p1.3.m2.1" class="ltx_Math" alttext="C^{GT}_{i}" display="inline"><semantics id="S3.SS1.p1.3.m2.1a"><msubsup id="S3.SS1.p1.3.m2.1.1" xref="S3.SS1.p1.3.m2.1.1.cmml"><mi id="S3.SS1.p1.3.m2.1.1.2.2" xref="S3.SS1.p1.3.m2.1.1.2.2.cmml">C</mi><mi id="S3.SS1.p1.3.m2.1.1.3" xref="S3.SS1.p1.3.m2.1.1.3.cmml">i</mi><mrow id="S3.SS1.p1.3.m2.1.1.2.3" xref="S3.SS1.p1.3.m2.1.1.2.3.cmml"><mi id="S3.SS1.p1.3.m2.1.1.2.3.2" xref="S3.SS1.p1.3.m2.1.1.2.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.3.m2.1.1.2.3.1" xref="S3.SS1.p1.3.m2.1.1.2.3.1.cmml">​</mo><mi id="S3.SS1.p1.3.m2.1.1.2.3.3" xref="S3.SS1.p1.3.m2.1.1.2.3.3.cmml">T</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m2.1b"><apply id="S3.SS1.p1.3.m2.1.1.cmml" xref="S3.SS1.p1.3.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m2.1.1.1.cmml" xref="S3.SS1.p1.3.m2.1.1">subscript</csymbol><apply id="S3.SS1.p1.3.m2.1.1.2.cmml" xref="S3.SS1.p1.3.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m2.1.1.2.1.cmml" xref="S3.SS1.p1.3.m2.1.1">superscript</csymbol><ci id="S3.SS1.p1.3.m2.1.1.2.2.cmml" xref="S3.SS1.p1.3.m2.1.1.2.2">𝐶</ci><apply id="S3.SS1.p1.3.m2.1.1.2.3.cmml" xref="S3.SS1.p1.3.m2.1.1.2.3"><times id="S3.SS1.p1.3.m2.1.1.2.3.1.cmml" xref="S3.SS1.p1.3.m2.1.1.2.3.1"></times><ci id="S3.SS1.p1.3.m2.1.1.2.3.2.cmml" xref="S3.SS1.p1.3.m2.1.1.2.3.2">𝐺</ci><ci id="S3.SS1.p1.3.m2.1.1.2.3.3.cmml" xref="S3.SS1.p1.3.m2.1.1.2.3.3">𝑇</ci></apply></apply><ci id="S3.SS1.p1.3.m2.1.1.3.cmml" xref="S3.SS1.p1.3.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m2.1c">C^{GT}_{i}</annotation></semantics></math> is the groundtruth number. We pre-train the front-end on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, and then train the benchmark network on the crowd datasets. We use a stochastic gradient descent (SGD) optimizer with a batch size of 50, set Nesterov momentum to 0.9, and set the weight decay to <math id="S3.SS1.p1.4.m3.1" class="ltx_Math" alttext="5\times 10^{-4}" display="inline"><semantics id="S3.SS1.p1.4.m3.1a"><mrow id="S3.SS1.p1.4.m3.1.1" xref="S3.SS1.p1.4.m3.1.1.cmml"><mn id="S3.SS1.p1.4.m3.1.1.2" xref="S3.SS1.p1.4.m3.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.4.m3.1.1.1" xref="S3.SS1.p1.4.m3.1.1.1.cmml">×</mo><msup id="S3.SS1.p1.4.m3.1.1.3" xref="S3.SS1.p1.4.m3.1.1.3.cmml"><mn id="S3.SS1.p1.4.m3.1.1.3.2" xref="S3.SS1.p1.4.m3.1.1.3.2.cmml">10</mn><mrow id="S3.SS1.p1.4.m3.1.1.3.3" xref="S3.SS1.p1.4.m3.1.1.3.3.cmml"><mo id="S3.SS1.p1.4.m3.1.1.3.3a" xref="S3.SS1.p1.4.m3.1.1.3.3.cmml">−</mo><mn id="S3.SS1.p1.4.m3.1.1.3.3.2" xref="S3.SS1.p1.4.m3.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m3.1b"><apply id="S3.SS1.p1.4.m3.1.1.cmml" xref="S3.SS1.p1.4.m3.1.1"><times id="S3.SS1.p1.4.m3.1.1.1.cmml" xref="S3.SS1.p1.4.m3.1.1.1"></times><cn type="integer" id="S3.SS1.p1.4.m3.1.1.2.cmml" xref="S3.SS1.p1.4.m3.1.1.2">5</cn><apply id="S3.SS1.p1.4.m3.1.1.3.cmml" xref="S3.SS1.p1.4.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m3.1.1.3.1.cmml" xref="S3.SS1.p1.4.m3.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS1.p1.4.m3.1.1.3.2.cmml" xref="S3.SS1.p1.4.m3.1.1.3.2">10</cn><apply id="S3.SS1.p1.4.m3.1.1.3.3.cmml" xref="S3.SS1.p1.4.m3.1.1.3.3"><minus id="S3.SS1.p1.4.m3.1.1.3.3.1.cmml" xref="S3.SS1.p1.4.m3.1.1.3.3"></minus><cn type="integer" id="S3.SS1.p1.4.m3.1.1.3.3.2.cmml" xref="S3.SS1.p1.4.m3.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m3.1c">5\times 10^{-4}</annotation></semantics></math>. The learning rate is initially set to <math id="S3.SS1.p1.5.m4.1" class="ltx_Math" alttext="10^{-6}" display="inline"><semantics id="S3.SS1.p1.5.m4.1a"><msup id="S3.SS1.p1.5.m4.1.1" xref="S3.SS1.p1.5.m4.1.1.cmml"><mn id="S3.SS1.p1.5.m4.1.1.2" xref="S3.SS1.p1.5.m4.1.1.2.cmml">10</mn><mrow id="S3.SS1.p1.5.m4.1.1.3" xref="S3.SS1.p1.5.m4.1.1.3.cmml"><mo id="S3.SS1.p1.5.m4.1.1.3a" xref="S3.SS1.p1.5.m4.1.1.3.cmml">−</mo><mn id="S3.SS1.p1.5.m4.1.1.3.2" xref="S3.SS1.p1.5.m4.1.1.3.2.cmml">6</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m4.1b"><apply id="S3.SS1.p1.5.m4.1.1.cmml" xref="S3.SS1.p1.5.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m4.1.1.1.cmml" xref="S3.SS1.p1.5.m4.1.1">superscript</csymbol><cn type="integer" id="S3.SS1.p1.5.m4.1.1.2.cmml" xref="S3.SS1.p1.5.m4.1.1.2">10</cn><apply id="S3.SS1.p1.5.m4.1.1.3.cmml" xref="S3.SS1.p1.5.m4.1.1.3"><minus id="S3.SS1.p1.5.m4.1.1.3.1.cmml" xref="S3.SS1.p1.5.m4.1.1.3"></minus><cn type="integer" id="S3.SS1.p1.5.m4.1.1.3.2.cmml" xref="S3.SS1.p1.5.m4.1.1.3.2">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m4.1c">10^{-6}</annotation></semantics></math>, and then divided by 10 every 3 epochs.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.5.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>The effect of fine-tuning the models pre-trained on CrowdX on the real ShanghaiTech B dataset(MAE/MSE).</figcaption>
<div id="S3.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:257.2pt;height:64.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-14.3pt,3.6pt) scale(0.9,0.9) ;">
<table id="S3.T2.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.3.3.4.1" class="ltx_tr">
<th id="S3.T2.3.3.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Method</th>
<th id="S3.T2.3.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">None/ImageNet</th>
<th id="S3.T2.3.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">GCC</th>
<th id="S3.T2.3.3.4.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">CrowdX</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">MCNN</td>
<td id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">26.4/41.3</td>
<td id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">18.8/28.2</td>
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">16.9/24.4</span>(<math id="S3.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T2.1.1.1.1.m1.1a"><mo mathcolor="#FF0000" stretchy="false" id="S3.T2.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math><span id="S3.T2.1.1.1.1.2" class="ltx_text" style="color:#FF0000;">36.0%</span>)</td>
</tr>
<tr id="S3.T2.2.2.2" class="ltx_tr">
<td id="S3.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CSRNet</td>
<td id="S3.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.6/16.0</td>
<td id="S3.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.1/15.7</td>
<td id="S3.T2.2.2.2.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T2.2.2.2.1.1" class="ltx_text ltx_font_bold">9.7/14.6</span>(<math id="S3.T2.2.2.2.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T2.2.2.2.1.m1.1a"><mo mathcolor="#FF0000" stretchy="false" id="S3.T2.2.2.2.1.m1.1.1" xref="S3.T2.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.1.m1.1b"><ci id="S3.T2.2.2.2.1.m1.1.1.cmml" xref="S3.T2.2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.1.m1.1c">\downarrow</annotation></semantics></math><span id="S3.T2.2.2.2.1.2" class="ltx_text" style="color:#FF0000;">8.5%</span>)</td>
</tr>
<tr id="S3.T2.3.3.3" class="ltx_tr">
<td id="S3.T2.3.3.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">ESA-NET</td>
<td id="S3.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">8.3/12.9</td>
<td id="S3.T2.3.3.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">-</td>
<td id="S3.T2.3.3.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">
<span id="S3.T2.3.3.3.1.1" class="ltx_text ltx_font_bold">7.6/11.8</span>(<math id="S3.T2.3.3.3.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T2.3.3.3.1.m1.1a"><mo mathcolor="#FF0000" stretchy="false" id="S3.T2.3.3.3.1.m1.1.1" xref="S3.T2.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.3.1.m1.1b"><ci id="S3.T2.3.3.3.1.m1.1.1.cmml" xref="S3.T2.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.3.1.m1.1c">\downarrow</annotation></semantics></math><span id="S3.T2.3.3.3.1.2" class="ltx_text" style="color:#FF0000;">8.4%</span>)</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Performence on CrowdX</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We conduct two experiments to study the effectiveness of using CrowdX for data enhancement, including 1) training the benchmark model only on the ShanghaiTech B dataset, 2) pre-training on CrowdX, and then fine-tuning the model on the ShanghaiTech B dataset. The results show that the performance of the model pre-trained by CrowdX is improved from 8.3 to 7.6, and the accuracy is improved by about 8.4%. We also test it on the ShanghaiTech B dataset, and the absolute error of the first 100 test samples shows that in almost all samples, the pre-trained model is significantly better than the non-pre-trained model, which further proves the effectiveness of the proposed CrowdX dataset in data enhancement.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Furthermore, we conduct experiments on the other two latest models, MCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and CSRNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Table <a href="#S3.T2" title="Table 2 ‣ 3.1 Efficient benchmark Method: ESA-Net ‣ 3 Experiments ‣ Enhancing and Dissecting Crowd Counting By Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows that we achieve improvements on both architectures, with MCNN improved by 36% and CSRNet by 8.5%. Our method is better than GCC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, probably because the generation process of CrowdX has more comprehensive parameter control, and most of the background of CrowdX is an urban scene, which is consistent with the ShanghaiTech B.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.2.1.1" class="ltx_text ltx_font_bold">Table 3</span>: </span>The effect of the background scenes on experimental results. The value in row i and column j represents CMAE [dataset(i), dataset(j)], and the same below.</figcaption>
<div id="S3.T3.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:275.8pt;height:74.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-34.5pt,9.4pt) scale(0.8,0.8) ;">
<table id="S3.T3.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.3.1.1.1" class="ltx_tr">
<th id="S3.T3.3.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"></th>
<th id="S3.T3.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S3.T3.3.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T3.3.1.1.1.2.1.1" class="ltx_tr">
<td id="S3.T3.3.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Solid Color Scene</td>
</tr>
</table>
</th>
<th id="S3.T3.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S3.T3.3.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T3.3.1.1.1.3.1.1" class="ltx_tr">
<td id="S3.T3.3.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Synthetic Scene</td>
</tr>
</table>
</th>
<th id="S3.T3.3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<table id="S3.T3.3.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T3.3.1.1.1.4.1.1" class="ltx_tr">
<td id="S3.T3.3.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Real-world Scene</td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.3.1.2.1" class="ltx_tr">
<th id="S3.T3.3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">
<table id="S3.T3.3.1.2.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T3.3.1.2.1.1.1.1" class="ltx_tr">
<td id="S3.T3.3.1.2.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Solid Color</td>
</tr>
</table>
</th>
<td id="S3.T3.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.3.1.2.1.2.1" class="ltx_text ltx_font_bold">11.6(15.4)</span></td>
<td id="S3.T3.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">61.2(109.8)</td>
<td id="S3.T3.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">33.7(53.0)</td>
</tr>
<tr id="S3.T3.3.1.3.2" class="ltx_tr">
<th id="S3.T3.3.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">
<table id="S3.T3.3.1.3.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T3.3.1.3.2.1.1.1" class="ltx_tr">
<td id="S3.T3.3.1.3.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Synthetic</td>
</tr>
</table>
</th>
<td id="S3.T3.3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">33.3(41.8)</td>
<td id="S3.T3.3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.3.1.3.2.3.1" class="ltx_text ltx_font_bold">14.9(23.1)</span></td>
<td id="S3.T3.3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">37.8(68.6)</td>
</tr>
<tr id="S3.T3.3.1.4.3" class="ltx_tr">
<th id="S3.T3.3.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">
<table id="S3.T3.3.1.4.3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T3.3.1.4.3.1.1.1" class="ltx_tr">
<td id="S3.T3.3.1.4.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Real-world</td>
</tr>
</table>
</th>
<td id="S3.T3.3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">75.7(91.6)</td>
<td id="S3.T3.3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">158.0(200.6)</td>
<td id="S3.T3.3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S3.T3.3.1.4.3.4.1" class="ltx_text ltx_font_bold">15.9(23.1)</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T4.2.1.1" class="ltx_text ltx_font_bold">Table 4</span>: </span>The effect of the camera perspective on experimental results. CP(30) is a dataset whose data is collected by controlling the camera perspective angle to equal 30°. CP(30,50) is the union set of CP(30) and CP(50). </figcaption>
<div id="S3.T4.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:257.2pt;height:64.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-85.7pt,21.6pt) scale(0.6,0.6) ;">
<table id="S3.T4.3.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T4.3.1.1.1" class="ltx_tr">
<td id="S3.T4.3.1.1.1.1" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S3.T4.3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">CP(30)</td>
<td id="S3.T4.3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">CP(50)</td>
<td id="S3.T4.3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">CP(70)</td>
<td id="S3.T4.3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">CP(90)</td>
<td id="S3.T4.3.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">CP(30,50)</td>
<td id="S3.T4.3.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">ST_B</td>
</tr>
<tr id="S3.T4.3.1.2.2" class="ltx_tr">
<td id="S3.T4.3.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">CP(30)</td>
<td id="S3.T4.3.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">20.1(28.4)</td>
<td id="S3.T4.3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S3.T4.3.1.2.2.3.1" class="ltx_text ltx_font_bold">16.5(26.5)</span></td>
<td id="S3.T4.3.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">38.4(47.9)</td>
<td id="S3.T4.3.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">65.3(74.4)</td>
<td id="S3.T4.3.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">18.3(27.5)</td>
<td id="S3.T4.3.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">39.4(67.1)</td>
</tr>
<tr id="S3.T4.3.1.3.3" class="ltx_tr">
<td id="S3.T4.3.1.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">CP(50)</td>
<td id="S3.T4.3.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">41.7(60.7)</td>
<td id="S3.T4.3.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S3.T4.3.1.3.3.3.1" class="ltx_text ltx_font_bold">15.2(26.3)</span></td>
<td id="S3.T4.3.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">21.6(26.8)</td>
<td id="S3.T4.3.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">38.7(44.7)</td>
<td id="S3.T4.3.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">39.3(67.7)</td>
<td id="S3.T4.3.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">39.3(67.7)</td>
</tr>
<tr id="S3.T4.3.1.4.4" class="ltx_tr">
<td id="S3.T4.3.1.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">CP(70)</td>
<td id="S3.T4.3.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">134.5(167.3)</td>
<td id="S3.T4.3.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">52.1(69.0)</td>
<td id="S3.T4.3.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">13.3(22.3)</td>
<td id="S3.T4.3.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S3.T4.3.1.4.4.5.1" class="ltx_text ltx_font_bold">12.6(18.0)</span></td>
<td id="S3.T4.3.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">93.4(67.7)</td>
<td id="S3.T4.3.1.4.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">57.2(94.0)</td>
</tr>
<tr id="S3.T4.3.1.5.5" class="ltx_tr">
<td id="S3.T4.3.1.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">CP(90)</td>
<td id="S3.T4.3.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">179.5(270.5)</td>
<td id="S3.T4.3.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">98.2(124.0)</td>
<td id="S3.T4.3.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">21.7(36.0)</td>
<td id="S3.T4.3.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S3.T4.3.1.5.5.5.1" class="ltx_text ltx_font_bold">12.8(19.1)</span></td>
<td id="S3.T4.3.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">138.9(178.0)</td>
<td id="S3.T4.3.1.5.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">67.8(107.3)</td>
</tr>
<tr id="S3.T4.3.1.6.6" class="ltx_tr">
<td id="S3.T4.3.1.6.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">CP(30,50)</td>
<td id="S3.T4.3.1.6.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">17.6(23.9)</td>
<td id="S3.T4.3.1.6.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">12.3(22.4)</td>
<td id="S3.T4.3.1.6.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S3.T4.3.1.6.6.4.1" class="ltx_text ltx_font_bold">11.7(17.9)</span></td>
<td id="S3.T4.3.1.6.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">29.2(35.3)</td>
<td id="S3.T4.3.1.6.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">14.9(23.1)</td>
<td id="S3.T4.3.1.6.6.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">37.8(68.6)</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Analysis of Influential Factors</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.3" class="ltx_p">In this section, we firstly define an operator CMAE <math id="S3.SS3.p1.1.m1.2" class="ltx_Math" alttext="(\psi_{1},\psi_{2})" display="inline"><semantics id="S3.SS3.p1.1.m1.2a"><mrow id="S3.SS3.p1.1.m1.2.2.2" xref="S3.SS3.p1.1.m1.2.2.3.cmml"><mo stretchy="false" id="S3.SS3.p1.1.m1.2.2.2.3" xref="S3.SS3.p1.1.m1.2.2.3.cmml">(</mo><msub id="S3.SS3.p1.1.m1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.1.1.2.cmml">ψ</mi><mn id="S3.SS3.p1.1.m1.1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.p1.1.m1.2.2.2.4" xref="S3.SS3.p1.1.m1.2.2.3.cmml">,</mo><msub id="S3.SS3.p1.1.m1.2.2.2.2" xref="S3.SS3.p1.1.m1.2.2.2.2.cmml"><mi id="S3.SS3.p1.1.m1.2.2.2.2.2" xref="S3.SS3.p1.1.m1.2.2.2.2.2.cmml">ψ</mi><mn id="S3.SS3.p1.1.m1.2.2.2.2.3" xref="S3.SS3.p1.1.m1.2.2.2.2.3.cmml">2</mn></msub><mo stretchy="false" id="S3.SS3.p1.1.m1.2.2.2.5" xref="S3.SS3.p1.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.2b"><interval closure="open" id="S3.SS3.p1.1.m1.2.2.3.cmml" xref="S3.SS3.p1.1.m1.2.2.2"><apply id="S3.SS3.p1.1.m1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.2">𝜓</ci><cn type="integer" id="S3.SS3.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.3">1</cn></apply><apply id="S3.SS3.p1.1.m1.2.2.2.2.cmml" xref="S3.SS3.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.2.2.2.2.1.cmml" xref="S3.SS3.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p1.1.m1.2.2.2.2.2.cmml" xref="S3.SS3.p1.1.m1.2.2.2.2.2">𝜓</ci><cn type="integer" id="S3.SS3.p1.1.m1.2.2.2.2.3.cmml" xref="S3.SS3.p1.1.m1.2.2.2.2.3">2</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.2c">(\psi_{1},\psi_{2})</annotation></semantics></math> to represent the result obtained by training the benchmark on the training set of <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="\psi_{1}" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><msub id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">ψ</mi><mn id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">𝜓</ci><cn type="integer" id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\psi_{1}</annotation></semantics></math> and then evaluatingon the test set of <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="\psi_{2}" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><msub id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">ψ</mi><mn id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">𝜓</ci><cn type="integer" id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">\psi_{2}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">How does background affect performance?</span>
From the diagonal of Table <a href="#S3.T3" title="Table 3 ‣ 3.2 Performence on CrowdX ‣ 3 Experiments ‣ Enhancing and Dissecting Crowd Counting By Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we can know that the simpler the background of the dataset, the better the performance of the trained model. Besides, compared with the transfer from the simulated scene to the real scene, the benchmark model has better performance in the transfer from the solid color scene to the real scene, which indicates that the complex background hinders the feature transfer.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">How does camera perspective affect performance?</span>
Intuitively, the diagonal in Table <a href="#S3.T4" title="Table 4 ‣ 3.2 Performence on CrowdX ‣ 3 Experiments ‣ Enhancing and Dissecting Crowd Counting By Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> should all be the best performance, but many exceptions can be found. Firstly, we find that CMAE [CP (30), CP (30)] <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mo id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><gt id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">&gt;</annotation></semantics></math> CMAE [CP (30), CP (50)]. It can be inferred that the pitch value of the camera in CP (30) is too small, which increases the occlusion between people. The loss caused by occlusion is greater than the loss caused by the feature difference. In addition, we find that CMAE [CP(50), CP(ST B)] is smaller than CMAE [CP(70), CP(ST B)] and CMAE [CP(90), CP(ST B)]. We speculate that when the pitch value difference between the test set and the training set increases, the error will also increase. Furthermore, we find that the model trained on CP(30,50) is better than the model trained on a single CP(30) or CP(50). One possible reason is that although the camera pitch value is 30 degrees, there are still many people at a 50-degree angle.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.1" class="ltx_p"><span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_bold">How does person density affect performance?</span>
We combine the data of CP (30) and CP (50) to form a dataset named PN (all). Then, we divide PN (all) into three sub-sets PN (0-200), PN (200-400), and PN (400+). PN (200-400) means that the number of people is between 200 and 400. The experimental results are shown in Table <a href="#S3.T5" title="Table 5 ‣ 3.3 Analysis of Influential Factors ‣ 3 Experiments ‣ Enhancing and Dissecting Crowd Counting By Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We find that all models achieve the best performance on PN (0-200), which shows that the fewer people, the higher the accuracy. This is because the occlusion will be reduced when there are few people. In addition, we find that CMAE [PN(0-200), PN(400+)] is much larger than CMAE [PN(400+), PN(0-200)]. It seems that a model trained in a high-density scene is more versatile than a model trained in a low-density scene. We think this may be because the model trained on high-density datasets can learn occlusion features.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T5.2.1.1" class="ltx_text ltx_font_bold">Table 5</span>: </span>The effect of the number of people on experimental results. </figcaption>
<div id="S3.T5.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:267.4pt;height:93.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-33.4pt,11.7pt) scale(0.8,0.8) ;">
<table id="S3.T5.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T5.3.1.1.1" class="ltx_tr">
<th id="S3.T5.3.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"></th>
<th id="S3.T5.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S3.T5.3.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.3.1.1.1.2.1.1" class="ltx_tr">
<td id="S3.T5.3.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">PN(0-200)</td>
</tr>
</table>
</th>
<th id="S3.T5.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S3.T5.3.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.3.1.1.1.3.1.1" class="ltx_tr">
<td id="S3.T5.3.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">PN(200-400)</td>
</tr>
</table>
</th>
<th id="S3.T5.3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S3.T5.3.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.3.1.1.1.4.1.1" class="ltx_tr">
<td id="S3.T5.3.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">PN(400+)</td>
</tr>
</table>
</th>
<th id="S3.T5.3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<table id="S3.T5.3.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.3.1.1.1.5.1.1" class="ltx_tr">
<td id="S3.T5.3.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">PN(all)</td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T5.3.1.2.1" class="ltx_tr">
<th id="S3.T5.3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">
<table id="S3.T5.3.1.2.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.3.1.2.1.1.1.1" class="ltx_tr">
<td id="S3.T5.3.1.2.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">PN(0-200)</td>
</tr>
</table>
</th>
<td id="S3.T5.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.3.1.2.1.2.1" class="ltx_text ltx_font_bold">9.4(12.5)</span></td>
<td id="S3.T5.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.6(64.4)</td>
<td id="S3.T5.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">154.8(173.6)</td>
<td id="S3.T5.3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">62.8(97.7)</td>
</tr>
<tr id="S3.T5.3.1.3.2" class="ltx_tr">
<th id="S3.T5.3.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">
<table id="S3.T5.3.1.3.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.3.1.3.2.1.1.1" class="ltx_tr">
<td id="S3.T5.3.1.3.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">PN(200-400)</td>
</tr>
</table>
</th>
<td id="S3.T5.3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.3.1.3.2.2.1" class="ltx_text ltx_font_bold">13.4(17.7)</span></td>
<td id="S3.T5.3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17.6(22.1)</td>
<td id="S3.T5.3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">55.3(71.3)</td>
<td id="S3.T5.3.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t">26.6(41.8)</td>
</tr>
<tr id="S3.T5.3.1.4.3" class="ltx_tr">
<th id="S3.T5.3.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">
<table id="S3.T5.3.1.4.3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.3.1.4.3.1.1.1" class="ltx_tr">
<td id="S3.T5.3.1.4.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">PN(400+)</td>
</tr>
</table>
</th>
<td id="S3.T5.3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.3.1.4.3.2.1" class="ltx_text ltx_font_bold">27.8(31.2)</span></td>
<td id="S3.T5.3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">34.9(40.7)</td>
<td id="S3.T5.3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">33.2(44.9)</td>
<td id="S3.T5.3.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t">31.5(38.5)</td>
</tr>
<tr id="S3.T5.3.1.5.4" class="ltx_tr">
<th id="S3.T5.3.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">
<table id="S3.T5.3.1.5.4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.3.1.5.4.1.1.1" class="ltx_tr">
<td id="S3.T5.3.1.5.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">PN(all)</td>
</tr>
</table>
</th>
<td id="S3.T5.3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T5.3.1.5.4.2.1" class="ltx_text ltx_font_bold">8.8(12.0)</span></td>
<td id="S3.T5.3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">14.3(17.8)</td>
<td id="S3.T5.3.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">23.2(34.9)</td>
<td id="S3.T5.3.1.5.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">14.9(23.1)</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S3.SS3.p5" class="ltx_para ltx_noindent">
<p id="S3.SS3.p5.1" class="ltx_p"><span id="S3.SS3.p5.1.1" class="ltx_text ltx_font_bold">How dose image resolution affect counting performance?</span></p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.3" class="ltx_p">We use two image resolutions (<math id="S3.SS3.p6.1.m1.1" class="ltx_Math" alttext="1024\times 768" display="inline"><semantics id="S3.SS3.p6.1.m1.1a"><mrow id="S3.SS3.p6.1.m1.1.1" xref="S3.SS3.p6.1.m1.1.1.cmml"><mn id="S3.SS3.p6.1.m1.1.1.2" xref="S3.SS3.p6.1.m1.1.1.2.cmml">1024</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p6.1.m1.1.1.1" xref="S3.SS3.p6.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS3.p6.1.m1.1.1.3" xref="S3.SS3.p6.1.m1.1.1.3.cmml">768</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.1.m1.1b"><apply id="S3.SS3.p6.1.m1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1"><times id="S3.SS3.p6.1.m1.1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1.1"></times><cn type="integer" id="S3.SS3.p6.1.m1.1.1.2.cmml" xref="S3.SS3.p6.1.m1.1.1.2">1024</cn><cn type="integer" id="S3.SS3.p6.1.m1.1.1.3.cmml" xref="S3.SS3.p6.1.m1.1.1.3">768</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.1.m1.1c">1024\times 768</annotation></semantics></math>) and (<math id="S3.SS3.p6.2.m2.1" class="ltx_Math" alttext="512\times 384" display="inline"><semantics id="S3.SS3.p6.2.m2.1a"><mrow id="S3.SS3.p6.2.m2.1.1" xref="S3.SS3.p6.2.m2.1.1.cmml"><mn id="S3.SS3.p6.2.m2.1.1.2" xref="S3.SS3.p6.2.m2.1.1.2.cmml">512</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p6.2.m2.1.1.1" xref="S3.SS3.p6.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS3.p6.2.m2.1.1.3" xref="S3.SS3.p6.2.m2.1.1.3.cmml">384</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.2.m2.1b"><apply id="S3.SS3.p6.2.m2.1.1.cmml" xref="S3.SS3.p6.2.m2.1.1"><times id="S3.SS3.p6.2.m2.1.1.1.cmml" xref="S3.SS3.p6.2.m2.1.1.1"></times><cn type="integer" id="S3.SS3.p6.2.m2.1.1.2.cmml" xref="S3.SS3.p6.2.m2.1.1.2">512</cn><cn type="integer" id="S3.SS3.p6.2.m2.1.1.3.cmml" xref="S3.SS3.p6.2.m2.1.1.3">384</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.2.m2.1c">512\times 384</annotation></semantics></math>) to explore the effect of image resolution on the algorithm performance. To ignore the influence of factors such as background and perspective, we only use solid color background and camera perspective 30°in CrowdX. The results are shown in Table <a href="#S3.T6" title="Table 6 ‣ 3.3 Analysis of Influential Factors ‣ 3 Experiments ‣ Enhancing and Dissecting Crowd Counting By Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. It can be inferred from CMAE (Low resolution, Low resolution)<math id="S3.SS3.p6.3.m3.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS3.p6.3.m3.1a"><mo id="S3.SS3.p6.3.m3.1.1" xref="S3.SS3.p6.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.3.m3.1b"><lt id="S3.SS3.p6.3.m3.1.1.cmml" xref="S3.SS3.p6.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.3.m3.1c">&lt;</annotation></semantics></math>CMAE (High resolution, High resolution) that large resolution is not necessarily helpful for improving the performance. From CMAE (Low resolution, High resolution) is much larger than CMAE (High resolution, Low resolution), we can see that models trained on low-resolution datasets are more versatile than models trained on high-resolution datasets.</p>
</div>
<figure id="S3.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T6.2.1.1" class="ltx_text ltx_font_bold">Table 6</span>: </span>The effect of image resolution on experimental results. </figcaption>
<div id="S3.T6.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:220.9pt;height:58.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-22.6pt,6.0pt) scale(0.83,0.83) ;">
<table id="S3.T6.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T6.3.1.1.1" class="ltx_tr">
<th id="S3.T6.3.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"></th>
<th id="S3.T6.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S3.T6.3.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T6.3.1.1.1.2.1.1" class="ltx_tr">
<td id="S3.T6.3.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Low Resolution</td>
</tr>
</table>
</th>
<th id="S3.T6.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<table id="S3.T6.3.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T6.3.1.1.1.3.1.1" class="ltx_tr">
<td id="S3.T6.3.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">High Resolution</td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T6.3.1.2.1" class="ltx_tr">
<th id="S3.T6.3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">
<table id="S3.T6.3.1.2.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T6.3.1.2.1.1.1.1" class="ltx_tr">
<td id="S3.T6.3.1.2.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Low Resolution</td>
</tr>
</table>
</th>
<td id="S3.T6.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T6.3.1.2.1.2.1" class="ltx_text ltx_font_bold">15.1(20.7)</span></td>
<td id="S3.T6.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">18.3(25.5)</td>
</tr>
<tr id="S3.T6.3.1.3.2" class="ltx_tr">
<th id="S3.T6.3.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">
<table id="S3.T6.3.1.3.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T6.3.1.3.2.1.1.1" class="ltx_tr">
<td id="S3.T6.3.1.3.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">High Resolution</td>
</tr>
</table>
</th>
<td id="S3.T6.3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">32.7(43.2)</td>
<td id="S3.T6.3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S3.T6.3.1.3.2.3.1" class="ltx_text ltx_font_bold">19.8(27.1)</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S4" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this article, we propose a synthetic crowd dataset CrowdX to enhance the level of crowd counting algorithms. With this dataset, the main influencing factors are analyzed and application suggestions are given. The conclusions of these experiments can improve the industry’s understanding of the algorithm.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and Ma Yi,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">“Single-image crowd counting via multi-column convolutional neural
network,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision &amp; Pattern Recognition</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Lokesh Boominathan, Srinivas SS Kruthiventi, and R Venkatesh Babu,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">“Crowdnet: A deep convolutional network for dense crowd counting,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 24th ACM international conference on
Multimedia</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">. ACM, 2016, pp. 640–644.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Xinkun Cao, Zhipeng Wang, Yanyun Zhao, and Fei Su,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">“Scale aggregation network for accurate and efficient crowd
counting,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 734–750.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Lingke Zeng, Xiangmin Xu, Bolun Cai, Suo Qiu, and Tong Zhang,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">“Multi-scale convolutional neural networks for crowd counting,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE International Conference on Image Processing
(ICIP)</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2017, pp. 465–469.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Weizhe Liu, Krzysztof Lis, Mathieu Salzmann, and Pascal Fua,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">“Geometric and physical constraints for head plane crowd density
estimation in videos,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1803.08805</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Ze Wang, Zehao Xiao, Kai Xie, Qiang Qiu, Xiantong Zhen, and Xianbin Cao,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">“In defense of single-column networks for crowd counting,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1808.06133</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Siyu Huang, Xi Li, Zhi-Qi Cheng, Zhongfei Zhang, and Alexander Hauptmann,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">“Stacked pooling: Improving crowd counting by boosting scale
invariance,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1808.07456</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Zhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong Gong,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">“Bayesian loss for crowd count estimation with point supervision,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2019, pp. 6142–6151.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Haroon Idrees, Imran Saleemi, Cody Seibert, and Mubarak Shah,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">“Multi-source multi-scale counting in extremely dense crowd
images,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision &amp; Pattern Recognition</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
A. B. Chan, Z. S. Liang, and N. Vasconcelos,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">“Privacy preserving crowd monitoring: Counting people without people
models or tracking,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision &amp; Pattern Recognition</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">,
2008.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Cong Zhang, Kai Kang, Hongsheng Li, Xiaogang Wang, Rong Xie, and Xiaokang Yang,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">“Data-driven crowd understanding: A baseline for a large-scale crowd
dataset,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Multimedia</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, vol. 18, no. 6, pp.
1048–1061, 2016.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Qi Wang, Junyu Gao, Wei Lin, and Yuan Yuan,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">“Learning from synthetic data for crowd counting in the wild,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2019, pp. 8198–8207.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Youmei Zhang, Chunluan Zhou, Faliang Chang, and Alex C Kot,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">“Multi-resolution attention convolutional neural network for crowd
counting,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neurocomputing</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, vol. 329, pp. 144–152, 2019.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Deepak Babu Sam, Shiv Surya, and R Venkatesh Babu,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">“Switching convolutional neural network for crowd counting,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2017, pp. 4031–4039.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Swami Sankaranarayanan, Yogesh Balaji, Arpit Jain, Ser Nam Lim, and Rama
Chellappa,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">“Learning from synthetic data: Addressing domain shift for semantic
segmentation,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 3752–3761.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora Vig,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">“Virtual worlds as proxy for multi-object tracking analysis,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2016, pp. 4340–4349.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Xuan Li, Kunfeng Wang, Yonglin Tian, Lan Yan, Fang Deng, and Fei-Yue Wang,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">“The paralleleye dataset: A large collection of virtual images for
traffic vision research,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Intelligent Transportation Systems</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, vol.
20, no. 6, pp. 2072–2084, 2018.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Igor Barros Barbosa, Marco Cristani, Barbara Caputo, Aleksander Rognhaugen, and
Theoharis Theoharis,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">“Looking beyond appearances: Synthetic training data for deep cnns
in re-identification,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision and Image Understanding</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, vol. 167, pp. 50–62,
2018.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M
Lopez,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">“The synthia dataset: A large collection of synthetic images for
semantic segmentation of urban scenes,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2016, pp. 3234–3243.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
“Unity3d engine.,” </span><a target="_blank" href="https://unity3d.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://unity3d.com/</a><span id="bib.bib20.2.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Yuhong Li, Xiaofan Zhang, and Deming Chen,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">“Csrnet: Dilated convolutional neural networks for understanding the
highly congested scenes,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 1091–1100.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Haroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong Zhang, Somaya Al-Maadeed,
Nasir Rajpoot, and Mubarak Shah,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">“Composition loss for counting, density map estimation and
localization in dense crowds,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 532–546.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Xiaoxiao Sun and Liang Zheng,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">“Dissecting person re-identification from the viewpoint of
viewpoint,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2019, pp. 608–617.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Karen Simonyan and Andrew Zisserman,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">“Very deep convolutional networks for large-scale image
recognition,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Yi Hou, Chengyang Li, Fan Yang, Cong Ma, Liping Zhu, Yuan Li, Huizhu Jia, and
Xiaodong Xie,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">“Bba-net: A bi-branch attention network for crowd counting,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP 2020 - 2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 4072–4076.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">“Imagenet: A large-scale hierarchical image database,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2009 IEEE conference on computer vision and pattern
recognition</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">. Ieee, 2009, pp. 248–255.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2201.08991" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2201.08992" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2201.08992">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2201.08992" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2201.08993" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  6 11:10:34 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
