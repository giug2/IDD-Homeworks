<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.10861] Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions</title><meta property="og:description" content="Federated Learning (FL) is a distributed and privacy-preserving machine learning paradigm that coordinates multiple clients to train a model while keeping the raw data localized. However, this traditional FL poses some…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.10861">

<!--Generated on Fri Jul  5 19:24:53 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Federated distillation,  federated learning,  knowledge distillation,  privacy preservation,  non-IID,  communication efficiency,  personalization,  system heterogeneity">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Laiqiao Qin
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:isqlq@outlook.com">isqlq@outlook.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">City University of Macau</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_streetaddress">Avenida Padre Tomás Pereira Taipa</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_city">Macau 999078</span><span id="id4.4.id4" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tianqing Zhu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:Tianqing.Zhu@uts.edu.au">Tianqing.Zhu@uts.edu.au</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_affiliation_institution">University of Technology Sydney</span><span id="id6.2.id2" class="ltx_text ltx_affiliation_streetaddress">123 Broadway</span><span id="id7.3.id3" class="ltx_text ltx_affiliation_city">Sydney</span><span id="id8.4.id4" class="ltx_text ltx_affiliation_state">Ultimo NSW 2007</span><span id="id9.5.id5" class="ltx_text ltx_affiliation_country">Australia</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wanlei Zhou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id10.1.id1" class="ltx_text ltx_affiliation_institution">City University of Macau</span><span id="id11.2.id2" class="ltx_text ltx_affiliation_streetaddress">Avenida Padre Tomás Pereira Taipa</span><span id="id12.3.id3" class="ltx_text ltx_affiliation_city">Macau 999078</span><span id="id13.4.id4" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:wlzhou@cityu.edu.mo">wlzhou@cityu.edu.mo</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Philip S. Yu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id14.1.id1" class="ltx_text ltx_affiliation_institution">University of Illinois at Chicago</span><span id="id15.2.id2" class="ltx_text ltx_affiliation_streetaddress">1200 W Harrison St</span><span id="id16.3.id3" class="ltx_text ltx_affiliation_city">Chicago</span><span id="id17.4.id4" class="ltx_text ltx_affiliation_state">Illinois 60607</span><span id="id18.5.id5" class="ltx_text ltx_affiliation_country">United States</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:psyu@cs.uic.edu">psyu@cs.uic.edu</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id19.id1" class="ltx_p">Federated Learning (FL) is a distributed and privacy-preserving machine learning paradigm that coordinates multiple clients to train a model while keeping the raw data localized. However, this traditional FL poses some challenges, including privacy risks, data heterogeneity, communication bottlenecks, and system heterogeneity issues. To tackle these challenges, knowledge distillation (KD) has been widely applied in FL since 2020. KD is a validated and efficacious model compression and enhancement algorithm. The core concept of KD involves facilitating knowledge transfer between models by exchanging logits at intermediate or output layers. These properties make KD an excellent solution for the long-lasting challenges in FL.
Up to now, there have been few reviews that summarize and analyze the current trend and methods for how KD can be applied in FL efficiently.
This article aims to provide a comprehensive survey of KD-based FL, focusing on addressing the above challenges. First, we provide an overview of KD-based FL, including its motivation, basics, taxonomy, and a comparison with traditional FL and where KD should execute. We also analyze the critical factors in KD-based FL in the appendix, including teachers, knowledge, data, and methods. We discuss how KD can address the challenges in FL, including privacy protection, data heterogeneity, communication efficiency, and personalization. Finally, we discuss the challenges facing KD-based FL algorithms and future research directions. We hope this survey can provide insights and guidance for researchers and practitioners in the FL area.</p>
</div>
<div class="ltx_keywords">Federated distillation, federated learning, knowledge distillation, privacy preservation, non-IID, communication efficiency, personalization, system heterogeneity
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmcopyright</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_journal"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>JACM</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_journalvolume"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalvolume: </span>37</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_journalnumber"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalnumber: </span>4</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_article"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">article: </span>111</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_publicationmonth"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">publicationmonth: </span>8</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated Learning (FL) is a distributed collaborative machine learning paradigm to protect data privacy <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib102" title="" class="ltx_ref">2017a</a>)</cite>. FL enables participants to build a robust and generalized federated model collaboratively through periodic local updates and communication without sharing raw data. Traditional FL algorithms, such as FedAvg <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib103" title="" class="ltx_ref">2017b</a>)</cite>, involve multiple rounds of client communication for repeatedly averaging local model parameters under server coordination.
In each round, clients receive the global model from the server to initialize their local model, update their local model using private local data, and upload the updated local model to the server. The server aggregates the received local models to update the global model and then broadcasts the updated model parameters to the clients. The entire process is repeated until the global model converges.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, this traditional parameter-based paradigm of FL may have some long-lasting challenges. First, sharing model parameters (or gradients) may compromise data privacy <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib81" title="" class="ltx_ref">2020c</a>; Mothukuri et al<span class="ltx_text">.</span>, <a href="#bib.bib111" title="" class="ltx_ref">2021</a>)</cite>. For example, adversaries could use these to reconstruct the raw data <cite class="ltx_cite ltx_citemacro_citep">(Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite>. Second, clients’ data is mostly not independently and identically distributed (non-IID) <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib186" title="" class="ltx_ref">2018</a>)</cite>, causing local models to suffer from client drift <cite class="ltx_cite ltx_citemacro_citep">(Karimireddy et al<span class="ltx_text">.</span>, <a href="#bib.bib68" title="" class="ltx_ref">2019</a>)</cite> and deviate from the global optimization objective, resulting in slow convergence <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib148" title="" class="ltx_ref">2020</a>)</cite> and degraded model performance <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib186" title="" class="ltx_ref">2018</a>)</cite>. Third, the frequent uploading of local models can lead to a severe communication bottleneck <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib81" title="" class="ltx_ref">2020c</a>; Konečnỳ et al<span class="ltx_text">.</span>, <a href="#bib.bib71" title="" class="ltx_ref">2016</a>; Sattler et al<span class="ltx_text">.</span>, <a href="#bib.bib125" title="" class="ltx_ref">2019</a>)</cite>. Lastly, parameter-based algorithms lack support for heterogeneous model architectures <cite class="ltx_cite ltx_citemacro_citep">(Li and Wang, <a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite>, personalization for clients <cite class="ltx_cite ltx_citemacro_citep">(Tan et al<span class="ltx_text">.</span>, <a href="#bib.bib138" title="" class="ltx_ref">2022</a>)</cite>, and efficient handling of system heterogeneity <cite class="ltx_cite ltx_citemacro_citep">(Ozkara et al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2021</a>; Kim and Wu, <a href="#bib.bib70" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To tackle the above long-lasting challenges in the traditional parameter-based FL is not easy. There are four key reasons that hinder the development of the FL: 1) Since data cannot be shared directly between clients, the clients’ model updates lack global awareness <cite class="ltx_cite ltx_citemacro_citep">(Karimireddy et al<span class="ltx_text">.</span>, <a href="#bib.bib67" title="" class="ltx_ref">2020</a>; Varno et al<span class="ltx_text">.</span>, <a href="#bib.bib145" title="" class="ltx_ref">2022</a>)</cite>. Traditional FL hopes to solve this problem by aggregating the model parameters of each client. However, 2) model parameters are only the results of client training, not the statistical characteristics of the data. This will lead to deviations in the aggregation model in non-IID scenarios <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib78" title="" class="ltx_ref">2022</a>)</cite>. 3) The premise of the current mainstream machine learning algorithms is IID, which is often not true in FL <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2021a</a>)</cite>. 4) The information in the model parameters is too rich and redundant, far exceeding the minimum amount required for collaborative training, leading to privacy <cite class="ltx_cite ltx_citemacro_citep">(Mothukuri et al<span class="ltx_text">.</span>, <a href="#bib.bib111" title="" class="ltx_ref">2021</a>)</cite> and communication <cite class="ltx_cite ltx_citemacro_citep">(Hamer et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2020</a>)</cite> issues.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To fill the gaps, Knowledge distillation (KD) <cite class="ltx_cite ltx_citemacro_citep">(Hinton et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2015</a>)</cite> has been widely applied in FL <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib88" title="" class="ltx_ref">2020</a>)</cite> since 2020. KD is a highly effective model compression and enhancement technique involving transferring knowledge from a high-performance teacher to a student model <cite class="ltx_cite ltx_citemacro_citep">(Phuong and Lampert, <a href="#bib.bib119" title="" class="ltx_ref">2019</a>; Mirzadeh et al<span class="ltx_text">.</span>, <a href="#bib.bib108" title="" class="ltx_ref">2020</a>)</cite>. Figure <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> is the basic process of KD, which shows a teacher-student architecture. The teacher network outputs soft labels for the same training data, and the student network uses these soft labels as regularization constraints to learn from the teacher model <cite class="ltx_cite ltx_citemacro_citep">(Hinton et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2015</a>)</cite>. Instead of directly copying the parameters of the teacher model, the student model emulates the soft labels of the teacher model to achieve knowledge transfer. This distinctive approach to transferring knowledge in KD reduces communication costs <cite class="ltx_cite ltx_citemacro_citep">(Sattler et al<span class="ltx_text">.</span>, <a href="#bib.bib124" title="" class="ltx_ref">2021b</a>)</cite> and allows for different architectures between teacher and student models<cite class="ltx_cite ltx_citemacro_citep">(Ozkara et al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2021</a>)</cite>. By the teacher model’s soft labels, the student model is endowed with regularization constraints that facilitate superior generalization capabilities.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2406.10861/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="70" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>The typical training process of knowledge distillation</figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This typical teacher-student framework in KD is quite suitable for FL. KD-based FL algorithms only require clients to exchange the logits of their local models <cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite> without uploading model parameters, thus significantly reducing potential privacy risks and communication costs. Unlike parameter-based methods that exchange in the parameter space, KD-based methods exchange in the function space <cite class="ltx_cite ltx_citemacro_citep">(Taya et al<span class="ltx_text">.</span>, <a href="#bib.bib142" title="" class="ltx_ref">2022</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib83" title="" class="ltx_ref">2023</a>)</cite>. This allows for the preservation of personalized knowledge between different models to some extent in non-IID scenarios rather than being directly flattened or eliminated <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al<span class="ltx_text">.</span>, <a href="#bib.bib162" title="" class="ltx_ref">2021</a>)</cite>. Furthermore, KD is model-agnostic <cite class="ltx_cite ltx_citemacro_citep">(Hinton et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2015</a>)</cite>, and clients have the flexibility to create personalization-focused models with varying architectures<cite class="ltx_cite ltx_citemacro_citep">(Li and Wang, <a href="#bib.bib77" title="" class="ltx_ref">2019</a>; Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2023b</a>; Jin et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Many algorithmic solutions <cite class="ltx_cite ltx_citemacro_citep">(Bistritz et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2020</a>; He et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2020</a>; Horvath et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2021</a>; Ozkara et al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2021</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib182" title="" class="ltx_ref">2022b</a>; Itahara et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2021</a>)</cite> based on KD have been proposed to tackle the above challenges in FL. These solutions can be roughly divided into three categories: 1) feature-based federated distillation, 2) parameter-based federated distillation, and 3) data-based federated distillation. Feature-based federated distillation algorithms transmit only the model’s features between clients and the server, such as logits <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2018</a>; Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2022</a>)</cite>, attention <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2021</a>; Wen et al<span class="ltx_text">.</span>, <a href="#bib.bib156" title="" class="ltx_ref">2023</a>)</cite>, and intermediate features <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a href="#bib.bib131" title="" class="ltx_ref">2021</a>)</cite>. Since these features are much smaller and contain far less sensitive information than model parameters, they can effectively address the communication bottleneck <cite class="ltx_cite ltx_citemacro_citep">(Sattler et al<span class="ltx_text">.</span>, <a href="#bib.bib124" title="" class="ltx_ref">2021b</a>)</cite> and privacy risks <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2022</a>)</cite> in FL, as well as personalized requirements by using different model architectures <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib92" title="" class="ltx_ref">2022a</a>)</cite>. Parameter-based federated distillation algorithms share model parameters between clients and the server. This method is primarily used to address the issue of model performance degradation caused by directly aggregating model parameters in non-IID scenarios <cite class="ltx_cite ltx_citemacro_citep">(Tang et al<span class="ltx_text">.</span>, <a href="#bib.bib139" title="" class="ltx_ref">2023</a>; Nguyen et al<span class="ltx_text">.</span>, <a href="#bib.bib112" title="" class="ltx_ref">2023</a>; Peng et al<span class="ltx_text">.</span>, <a href="#bib.bib118" title="" class="ltx_ref">2023</a>)</cite>. Data-based federated distillation algorithms require clients to use dataset distillation <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib154" title="" class="ltx_ref">2018</a>)</cite> methods to compress their data into a small-scale dataset <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib188" title="" class="ltx_ref">2022</a>; Song et al<span class="ltx_text">.</span>, <a href="#bib.bib134" title="" class="ltx_ref">2023</a>)</cite>, which is not widely adopted in FL and this survey will not delve into this approach further. Table <a href="#S1.T1" title="Table 1 ‣ 1. Introduction ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes which challenges these three KD-based FL methods are used to solve in FL.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>KD-based FL methods are used to solve challenges in FL</figcaption>
<table id="S1.T1.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span></td>
<td id="S1.T1.1.1.2" class="ltx_td ltx_align_center">Privacy</td>
<td id="S1.T1.1.1.3" class="ltx_td ltx_align_center">Non-IID</td>
<td id="S1.T1.1.1.4" class="ltx_td ltx_align_center">Communication</td>
<td id="S1.T1.1.1.5" class="ltx_td ltx_align_center">Personalization</td>
</tr>
<tr id="S1.T1.1.2" class="ltx_tr">
<td id="S1.T1.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Feature-based <cite class="ltx_cite ltx_citemacro_citep">(Li and Wang, <a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S1.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_tt">✓</td>
<td id="S1.T1.1.2.3" class="ltx_td ltx_align_center ltx_border_tt">✓</td>
<td id="S1.T1.1.2.4" class="ltx_td ltx_align_center ltx_border_tt">✓</td>
<td id="S1.T1.1.2.5" class="ltx_td ltx_align_center ltx_border_tt">✓</td>
</tr>
<tr id="S1.T1.1.3" class="ltx_tr">
<td id="S1.T1.1.3.1" class="ltx_td ltx_align_center ltx_border_r">Parameter-based <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a href="#bib.bib130" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S1.T1.1.3.2" class="ltx_td"></td>
<td id="S1.T1.1.3.3" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.1.3.4" class="ltx_td"></td>
<td id="S1.T1.1.3.5" class="ltx_td"></td>
</tr>
<tr id="S1.T1.1.4" class="ltx_tr">
<td id="S1.T1.1.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b ltx_border_r">Data-based <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib189" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="S1.T1.1.4.2" class="ltx_td ltx_border_bb ltx_border_b"></td>
<td id="S1.T1.1.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">✓</td>
<td id="S1.T1.1.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_b">✓</td>
<td id="S1.T1.1.4.5" class="ltx_td ltx_border_bb ltx_border_b"></td>
</tr>
</table>
</figure>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Even though KD-based FL has shown great potential to resolve the long-lasting challenges in FL, the area remains in its early stages, and the associated research exhibits considerable breadth and diversity. The problems and corresponding solutions targeted by these KD-based algorithms are not identical, and the description of ”KD in FL” cannot fully summarize them. Different literature has diverse perspectives, explanations, and specific implementations. For instance, while <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2018</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Li and Wang, <a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite> both use the average value of clients’ logits as knowledge, <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2018</a>)</cite> uses local private data, whereas <cite class="ltx_cite ltx_citemacro_citep">(Li and Wang, <a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite> uses public data. Likewise, although <cite class="ltx_cite ltx_citemacro_citep">(Sattler et al<span class="ltx_text">.</span>, <a href="#bib.bib122" title="" class="ltx_ref">2021a</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a href="#bib.bib74" title="" class="ltx_ref">2022</a>)</cite> are both based on FedAvg, <cite class="ltx_cite ltx_citemacro_citep">(Sattler et al<span class="ltx_text">.</span>, <a href="#bib.bib122" title="" class="ltx_ref">2021a</a>)</cite> uses the local model as a teacher, whereas <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a href="#bib.bib74" title="" class="ltx_ref">2022</a>)</cite> uses the global model as a teacher. A comprehensive analysis of these methodologies can accelerate further application of this area.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">Our primary focus is on the inherent critical factors of KD-based FL algorithms and how KD addresses the long-lasting challenges of FL. The main contribution of this paper is to analyze the fundamental reasons why KD can resolve the long-lasting challenges on FL. We also analyzed some characteristics of using KD in FL and proposed a taxonomy. The details are summarized as follows:</p>
</div>
<div id="S1.p9" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We provide a systematic analysis of KD-based FL, summarizing the differences between KD-based FL and parameter-based FL algorithm paradigms and analyzing where KD should execute. By comparing the knowledge transfer methods used by different algorithms, we classify KD-based FL algorithms into three categories: feature-based federated distillation, parameter-based federated distillation, and data-based federated distillation.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We summarize the critical factors related to state-of-the-art KD-based FL algorithms, including the role of teachers, types of knowledge, dataset acquisition for distillation, and specific methods employed. Details can be found in the appendix. These factors must be considered by all KD-based FL algorithms. By analyzing these factors, we provide important references for future research in KD-based FL.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We provide a comprehensive introduction to the application of KD in solving various challenges in FL. These challenges include privacy preservation, mitigating slow convergence and model performance degradation in data heterogeneous scenarios, reducing communication bottlenecks, and addressing personalized model requirements and system heterogeneity issues resulting from variations in client network and hardware resources.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We discuss the challenges facing KD-based FL and future research directions.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Background </h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Federated Learning</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.13" class="ltx_p">FL enables multiple clients to collaboratively train a global model with the coordination of a central server.
Suppose there are <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">N</annotation></semantics></math> independent clients; the matrix <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><msub id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mi id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml">D</mi><mi id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2">𝐷</ci><ci id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">D_{i}</annotation></semantics></math> represents the local data of client <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">i</annotation></semantics></math> , where <math id="S2.SS1.p1.4.m4.2" class="ltx_Math" alttext="i\in[1,N]" display="inline"><semantics id="S2.SS1.p1.4.m4.2a"><mrow id="S2.SS1.p1.4.m4.2.3" xref="S2.SS1.p1.4.m4.2.3.cmml"><mi id="S2.SS1.p1.4.m4.2.3.2" xref="S2.SS1.p1.4.m4.2.3.2.cmml">i</mi><mo id="S2.SS1.p1.4.m4.2.3.1" xref="S2.SS1.p1.4.m4.2.3.1.cmml">∈</mo><mrow id="S2.SS1.p1.4.m4.2.3.3.2" xref="S2.SS1.p1.4.m4.2.3.3.1.cmml"><mo stretchy="false" id="S2.SS1.p1.4.m4.2.3.3.2.1" xref="S2.SS1.p1.4.m4.2.3.3.1.cmml">[</mo><mn id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">1</mn><mo id="S2.SS1.p1.4.m4.2.3.3.2.2" xref="S2.SS1.p1.4.m4.2.3.3.1.cmml">,</mo><mi id="S2.SS1.p1.4.m4.2.2" xref="S2.SS1.p1.4.m4.2.2.cmml">N</mi><mo stretchy="false" id="S2.SS1.p1.4.m4.2.3.3.2.3" xref="S2.SS1.p1.4.m4.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.2b"><apply id="S2.SS1.p1.4.m4.2.3.cmml" xref="S2.SS1.p1.4.m4.2.3"><in id="S2.SS1.p1.4.m4.2.3.1.cmml" xref="S2.SS1.p1.4.m4.2.3.1"></in><ci id="S2.SS1.p1.4.m4.2.3.2.cmml" xref="S2.SS1.p1.4.m4.2.3.2">𝑖</ci><interval closure="closed" id="S2.SS1.p1.4.m4.2.3.3.1.cmml" xref="S2.SS1.p1.4.m4.2.3.3.2"><cn type="integer" id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">1</cn><ci id="S2.SS1.p1.4.m4.2.2.cmml" xref="S2.SS1.p1.4.m4.2.2">𝑁</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.2c">i\in[1,N]</annotation></semantics></math>. Each client has a different local model <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="w_{i}" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><msub id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.p1.5.m5.1.1.2" xref="S2.SS1.p1.5.m5.1.1.2.cmml">w</mi><mi id="S2.SS1.p1.5.m5.1.1.3" xref="S2.SS1.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><apply id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.p1.5.m5.1.1.2">𝑤</ci><ci id="S2.SS1.p1.5.m5.1.1.3.cmml" xref="S2.SS1.p1.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">w_{i}</annotation></semantics></math> and objective function <math id="S2.SS1.p1.6.m6.1" class="ltx_Math" alttext="f_{i}(w)" display="inline"><semantics id="S2.SS1.p1.6.m6.1a"><mrow id="S2.SS1.p1.6.m6.1.2" xref="S2.SS1.p1.6.m6.1.2.cmml"><msub id="S2.SS1.p1.6.m6.1.2.2" xref="S2.SS1.p1.6.m6.1.2.2.cmml"><mi id="S2.SS1.p1.6.m6.1.2.2.2" xref="S2.SS1.p1.6.m6.1.2.2.2.cmml">f</mi><mi id="S2.SS1.p1.6.m6.1.2.2.3" xref="S2.SS1.p1.6.m6.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p1.6.m6.1.2.1" xref="S2.SS1.p1.6.m6.1.2.1.cmml">​</mo><mrow id="S2.SS1.p1.6.m6.1.2.3.2" xref="S2.SS1.p1.6.m6.1.2.cmml"><mo stretchy="false" id="S2.SS1.p1.6.m6.1.2.3.2.1" xref="S2.SS1.p1.6.m6.1.2.cmml">(</mo><mi id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml">w</mi><mo stretchy="false" id="S2.SS1.p1.6.m6.1.2.3.2.2" xref="S2.SS1.p1.6.m6.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><apply id="S2.SS1.p1.6.m6.1.2.cmml" xref="S2.SS1.p1.6.m6.1.2"><times id="S2.SS1.p1.6.m6.1.2.1.cmml" xref="S2.SS1.p1.6.m6.1.2.1"></times><apply id="S2.SS1.p1.6.m6.1.2.2.cmml" xref="S2.SS1.p1.6.m6.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.1.2.2.1.cmml" xref="S2.SS1.p1.6.m6.1.2.2">subscript</csymbol><ci id="S2.SS1.p1.6.m6.1.2.2.2.cmml" xref="S2.SS1.p1.6.m6.1.2.2.2">𝑓</ci><ci id="S2.SS1.p1.6.m6.1.2.2.3.cmml" xref="S2.SS1.p1.6.m6.1.2.2.3">𝑖</ci></apply><ci id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">f_{i}(w)</annotation></semantics></math>. Each client is given a different weight <math id="S2.SS1.p1.7.m7.1" class="ltx_Math" alttext="q_{i}" display="inline"><semantics id="S2.SS1.p1.7.m7.1a"><msub id="S2.SS1.p1.7.m7.1.1" xref="S2.SS1.p1.7.m7.1.1.cmml"><mi id="S2.SS1.p1.7.m7.1.1.2" xref="S2.SS1.p1.7.m7.1.1.2.cmml">q</mi><mi id="S2.SS1.p1.7.m7.1.1.3" xref="S2.SS1.p1.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><apply id="S2.SS1.p1.7.m7.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.7.m7.1.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1">subscript</csymbol><ci id="S2.SS1.p1.7.m7.1.1.2.cmml" xref="S2.SS1.p1.7.m7.1.1.2">𝑞</ci><ci id="S2.SS1.p1.7.m7.1.1.3.cmml" xref="S2.SS1.p1.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">q_{i}</annotation></semantics></math>. The goal of FL is to obtain a generalized global model <math id="S2.SS1.p1.8.m8.1" class="ltx_Math" alttext="w_{g}" display="inline"><semantics id="S2.SS1.p1.8.m8.1a"><msub id="S2.SS1.p1.8.m8.1.1" xref="S2.SS1.p1.8.m8.1.1.cmml"><mi id="S2.SS1.p1.8.m8.1.1.2" xref="S2.SS1.p1.8.m8.1.1.2.cmml">w</mi><mi id="S2.SS1.p1.8.m8.1.1.3" xref="S2.SS1.p1.8.m8.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m8.1b"><apply id="S2.SS1.p1.8.m8.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.8.m8.1.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S2.SS1.p1.8.m8.1.1.2.cmml" xref="S2.SS1.p1.8.m8.1.1.2">𝑤</ci><ci id="S2.SS1.p1.8.m8.1.1.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m8.1c">w_{g}</annotation></semantics></math> through multiple rounds of local training and global aggregation. <math id="S2.SS1.p1.9.m9.1" class="ltx_Math" alttext="w_{i}^{r}" display="inline"><semantics id="S2.SS1.p1.9.m9.1a"><msubsup id="S2.SS1.p1.9.m9.1.1" xref="S2.SS1.p1.9.m9.1.1.cmml"><mi id="S2.SS1.p1.9.m9.1.1.2.2" xref="S2.SS1.p1.9.m9.1.1.2.2.cmml">w</mi><mi id="S2.SS1.p1.9.m9.1.1.2.3" xref="S2.SS1.p1.9.m9.1.1.2.3.cmml">i</mi><mi id="S2.SS1.p1.9.m9.1.1.3" xref="S2.SS1.p1.9.m9.1.1.3.cmml">r</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.9.m9.1b"><apply id="S2.SS1.p1.9.m9.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.9.m9.1.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1">superscript</csymbol><apply id="S2.SS1.p1.9.m9.1.1.2.cmml" xref="S2.SS1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.9.m9.1.1.2.1.cmml" xref="S2.SS1.p1.9.m9.1.1">subscript</csymbol><ci id="S2.SS1.p1.9.m9.1.1.2.2.cmml" xref="S2.SS1.p1.9.m9.1.1.2.2">𝑤</ci><ci id="S2.SS1.p1.9.m9.1.1.2.3.cmml" xref="S2.SS1.p1.9.m9.1.1.2.3">𝑖</ci></apply><ci id="S2.SS1.p1.9.m9.1.1.3.cmml" xref="S2.SS1.p1.9.m9.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.9.m9.1c">w_{i}^{r}</annotation></semantics></math> represents the local model of client <math id="S2.SS1.p1.10.m10.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.p1.10.m10.1a"><mi id="S2.SS1.p1.10.m10.1.1" xref="S2.SS1.p1.10.m10.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.10.m10.1b"><ci id="S2.SS1.p1.10.m10.1.1.cmml" xref="S2.SS1.p1.10.m10.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.10.m10.1c">i</annotation></semantics></math> in the <math id="S2.SS1.p1.11.m11.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S2.SS1.p1.11.m11.1a"><mi id="S2.SS1.p1.11.m11.1.1" xref="S2.SS1.p1.11.m11.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.11.m11.1b"><ci id="S2.SS1.p1.11.m11.1.1.cmml" xref="S2.SS1.p1.11.m11.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.11.m11.1c">r</annotation></semantics></math>-th round of communication, and <math id="S2.SS1.p1.12.m12.1" class="ltx_Math" alttext="w_{g}^{r}" display="inline"><semantics id="S2.SS1.p1.12.m12.1a"><msubsup id="S2.SS1.p1.12.m12.1.1" xref="S2.SS1.p1.12.m12.1.1.cmml"><mi id="S2.SS1.p1.12.m12.1.1.2.2" xref="S2.SS1.p1.12.m12.1.1.2.2.cmml">w</mi><mi id="S2.SS1.p1.12.m12.1.1.2.3" xref="S2.SS1.p1.12.m12.1.1.2.3.cmml">g</mi><mi id="S2.SS1.p1.12.m12.1.1.3" xref="S2.SS1.p1.12.m12.1.1.3.cmml">r</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.12.m12.1b"><apply id="S2.SS1.p1.12.m12.1.1.cmml" xref="S2.SS1.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.12.m12.1.1.1.cmml" xref="S2.SS1.p1.12.m12.1.1">superscript</csymbol><apply id="S2.SS1.p1.12.m12.1.1.2.cmml" xref="S2.SS1.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.12.m12.1.1.2.1.cmml" xref="S2.SS1.p1.12.m12.1.1">subscript</csymbol><ci id="S2.SS1.p1.12.m12.1.1.2.2.cmml" xref="S2.SS1.p1.12.m12.1.1.2.2">𝑤</ci><ci id="S2.SS1.p1.12.m12.1.1.2.3.cmml" xref="S2.SS1.p1.12.m12.1.1.2.3">𝑔</ci></apply><ci id="S2.SS1.p1.12.m12.1.1.3.cmml" xref="S2.SS1.p1.12.m12.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.12.m12.1c">w_{g}^{r}</annotation></semantics></math> represents the global model in the <math id="S2.SS1.p1.13.m13.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S2.SS1.p1.13.m13.1a"><mi id="S2.SS1.p1.13.m13.1.1" xref="S2.SS1.p1.13.m13.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.13.m13.1b"><ci id="S2.SS1.p1.13.m13.1.1.cmml" xref="S2.SS1.p1.13.m13.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.13.m13.1c">r</annotation></semantics></math>-th round. The definition and training process of FL is as follows:</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.2" class="ltx_p"><span id="S2.SS1.p2.2.1" class="ltx_text ltx_font_bold">Definition</span> <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib173" title="" class="ltx_ref">2019</a>)</cite>. In FL settings, client <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mi id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">i</annotation></semantics></math> aims to achieve the learning objective by iteratively updating their local models through aggregating model results from multiple clients without exchanging or transmitting their local data <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><msub id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml"><mi id="S2.SS1.p2.2.m2.1.1.2" xref="S2.SS1.p2.2.m2.1.1.2.cmml">D</mi><mi id="S2.SS1.p2.2.m2.1.1.3" xref="S2.SS1.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.2.m2.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p2.2.m2.1.1.2.cmml" xref="S2.SS1.p2.2.m2.1.1.2">𝐷</ci><ci id="S2.SS1.p2.2.m2.1.1.3.cmml" xref="S2.SS1.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">D_{i}</annotation></semantics></math>, by sharing model parameters or outputs <cite class="ltx_cite ltx_citemacro_citep">(Kairouz et al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2021</a>)</cite>. This process often involves the assistance of a central server, but it is not necessary for certain scenarios or system architectures.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p"><span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_bold">Training process</span>.
As shown in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.1. Federated Learning ‣ 2. Background ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
the typical training process for FL is as follows:</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.3" class="ltx_p">Model distribution: In round <math id="S2.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S2.I1.i1.p1.1.m1.1a"><mi id="S2.I1.i1.p1.1.m1.1.1" xref="S2.I1.i1.p1.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.1.m1.1b"><ci id="S2.I1.i1.p1.1.m1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.1.m1.1c">r</annotation></semantics></math>, the central server distributes the current global model <math id="S2.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="w_{g}^{r}" display="inline"><semantics id="S2.I1.i1.p1.2.m2.1a"><msubsup id="S2.I1.i1.p1.2.m2.1.1" xref="S2.I1.i1.p1.2.m2.1.1.cmml"><mi id="S2.I1.i1.p1.2.m2.1.1.2.2" xref="S2.I1.i1.p1.2.m2.1.1.2.2.cmml">w</mi><mi id="S2.I1.i1.p1.2.m2.1.1.2.3" xref="S2.I1.i1.p1.2.m2.1.1.2.3.cmml">g</mi><mi id="S2.I1.i1.p1.2.m2.1.1.3" xref="S2.I1.i1.p1.2.m2.1.1.3.cmml">r</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.2.m2.1b"><apply id="S2.I1.i1.p1.2.m2.1.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.I1.i1.p1.2.m2.1.1.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1">superscript</csymbol><apply id="S2.I1.i1.p1.2.m2.1.1.2.cmml" xref="S2.I1.i1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.I1.i1.p1.2.m2.1.1.2.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1">subscript</csymbol><ci id="S2.I1.i1.p1.2.m2.1.1.2.2.cmml" xref="S2.I1.i1.p1.2.m2.1.1.2.2">𝑤</ci><ci id="S2.I1.i1.p1.2.m2.1.1.2.3.cmml" xref="S2.I1.i1.p1.2.m2.1.1.2.3">𝑔</ci></apply><ci id="S2.I1.i1.p1.2.m2.1.1.3.cmml" xref="S2.I1.i1.p1.2.m2.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.2.m2.1c">w_{g}^{r}</annotation></semantics></math> to each client. At the beginning, the global model <math id="S2.I1.i1.p1.3.m3.1" class="ltx_Math" alttext="w_{g}^{0}" display="inline"><semantics id="S2.I1.i1.p1.3.m3.1a"><msubsup id="S2.I1.i1.p1.3.m3.1.1" xref="S2.I1.i1.p1.3.m3.1.1.cmml"><mi id="S2.I1.i1.p1.3.m3.1.1.2.2" xref="S2.I1.i1.p1.3.m3.1.1.2.2.cmml">w</mi><mi id="S2.I1.i1.p1.3.m3.1.1.2.3" xref="S2.I1.i1.p1.3.m3.1.1.2.3.cmml">g</mi><mn id="S2.I1.i1.p1.3.m3.1.1.3" xref="S2.I1.i1.p1.3.m3.1.1.3.cmml">0</mn></msubsup><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.3.m3.1b"><apply id="S2.I1.i1.p1.3.m3.1.1.cmml" xref="S2.I1.i1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.I1.i1.p1.3.m3.1.1.1.cmml" xref="S2.I1.i1.p1.3.m3.1.1">superscript</csymbol><apply id="S2.I1.i1.p1.3.m3.1.1.2.cmml" xref="S2.I1.i1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.I1.i1.p1.3.m3.1.1.2.1.cmml" xref="S2.I1.i1.p1.3.m3.1.1">subscript</csymbol><ci id="S2.I1.i1.p1.3.m3.1.1.2.2.cmml" xref="S2.I1.i1.p1.3.m3.1.1.2.2">𝑤</ci><ci id="S2.I1.i1.p1.3.m3.1.1.2.3.cmml" xref="S2.I1.i1.p1.3.m3.1.1.2.3">𝑔</ci></apply><cn type="integer" id="S2.I1.i1.p1.3.m3.1.1.3.cmml" xref="S2.I1.i1.p1.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.3.m3.1c">w_{g}^{0}</annotation></semantics></math> can be randomly initialized.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.4" class="ltx_p">Local model update: The client <math id="S2.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.I1.i2.p1.1.m1.1a"><mi id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.1b"><ci id="S2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.1c">i</annotation></semantics></math> initializes the local model with the global model <math id="S2.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="w_{g}^{r}" display="inline"><semantics id="S2.I1.i2.p1.2.m2.1a"><msubsup id="S2.I1.i2.p1.2.m2.1.1" xref="S2.I1.i2.p1.2.m2.1.1.cmml"><mi id="S2.I1.i2.p1.2.m2.1.1.2.2" xref="S2.I1.i2.p1.2.m2.1.1.2.2.cmml">w</mi><mi id="S2.I1.i2.p1.2.m2.1.1.2.3" xref="S2.I1.i2.p1.2.m2.1.1.2.3.cmml">g</mi><mi id="S2.I1.i2.p1.2.m2.1.1.3" xref="S2.I1.i2.p1.2.m2.1.1.3.cmml">r</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.2.m2.1b"><apply id="S2.I1.i2.p1.2.m2.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.2.m2.1.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1">superscript</csymbol><apply id="S2.I1.i2.p1.2.m2.1.1.2.cmml" xref="S2.I1.i2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.2.m2.1.1.2.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.I1.i2.p1.2.m2.1.1.2.2.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2.2">𝑤</ci><ci id="S2.I1.i2.p1.2.m2.1.1.2.3.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2.3">𝑔</ci></apply><ci id="S2.I1.i2.p1.2.m2.1.1.3.cmml" xref="S2.I1.i2.p1.2.m2.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.2.m2.1c">w_{g}^{r}</annotation></semantics></math> and trains it with local data <math id="S2.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S2.I1.i2.p1.3.m3.1a"><msub id="S2.I1.i2.p1.3.m3.1.1" xref="S2.I1.i2.p1.3.m3.1.1.cmml"><mi id="S2.I1.i2.p1.3.m3.1.1.2" xref="S2.I1.i2.p1.3.m3.1.1.2.cmml">D</mi><mi id="S2.I1.i2.p1.3.m3.1.1.3" xref="S2.I1.i2.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.3.m3.1b"><apply id="S2.I1.i2.p1.3.m3.1.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.3.m3.1.1.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.I1.i2.p1.3.m3.1.1.2.cmml" xref="S2.I1.i2.p1.3.m3.1.1.2">𝐷</ci><ci id="S2.I1.i2.p1.3.m3.1.1.3.cmml" xref="S2.I1.i2.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.3.m3.1c">D_{i}</annotation></semantics></math> . When obtaining a new local model <math id="S2.I1.i2.p1.4.m4.1" class="ltx_Math" alttext="w_{i}^{r+1}" display="inline"><semantics id="S2.I1.i2.p1.4.m4.1a"><msubsup id="S2.I1.i2.p1.4.m4.1.1" xref="S2.I1.i2.p1.4.m4.1.1.cmml"><mi id="S2.I1.i2.p1.4.m4.1.1.2.2" xref="S2.I1.i2.p1.4.m4.1.1.2.2.cmml">w</mi><mi id="S2.I1.i2.p1.4.m4.1.1.2.3" xref="S2.I1.i2.p1.4.m4.1.1.2.3.cmml">i</mi><mrow id="S2.I1.i2.p1.4.m4.1.1.3" xref="S2.I1.i2.p1.4.m4.1.1.3.cmml"><mi id="S2.I1.i2.p1.4.m4.1.1.3.2" xref="S2.I1.i2.p1.4.m4.1.1.3.2.cmml">r</mi><mo id="S2.I1.i2.p1.4.m4.1.1.3.1" xref="S2.I1.i2.p1.4.m4.1.1.3.1.cmml">+</mo><mn id="S2.I1.i2.p1.4.m4.1.1.3.3" xref="S2.I1.i2.p1.4.m4.1.1.3.3.cmml">1</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.4.m4.1b"><apply id="S2.I1.i2.p1.4.m4.1.1.cmml" xref="S2.I1.i2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.4.m4.1.1.1.cmml" xref="S2.I1.i2.p1.4.m4.1.1">superscript</csymbol><apply id="S2.I1.i2.p1.4.m4.1.1.2.cmml" xref="S2.I1.i2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.4.m4.1.1.2.1.cmml" xref="S2.I1.i2.p1.4.m4.1.1">subscript</csymbol><ci id="S2.I1.i2.p1.4.m4.1.1.2.2.cmml" xref="S2.I1.i2.p1.4.m4.1.1.2.2">𝑤</ci><ci id="S2.I1.i2.p1.4.m4.1.1.2.3.cmml" xref="S2.I1.i2.p1.4.m4.1.1.2.3">𝑖</ci></apply><apply id="S2.I1.i2.p1.4.m4.1.1.3.cmml" xref="S2.I1.i2.p1.4.m4.1.1.3"><plus id="S2.I1.i2.p1.4.m4.1.1.3.1.cmml" xref="S2.I1.i2.p1.4.m4.1.1.3.1"></plus><ci id="S2.I1.i2.p1.4.m4.1.1.3.2.cmml" xref="S2.I1.i2.p1.4.m4.1.1.3.2">𝑟</ci><cn type="integer" id="S2.I1.i2.p1.4.m4.1.1.3.3.cmml" xref="S2.I1.i2.p1.4.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.4.m4.1c">w_{i}^{r+1}</annotation></semantics></math>, the client sends it to the central server.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.2" class="ltx_p">Global model update: The central server aggregates the local models <math id="S2.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="w_{i}^{r+1}" display="inline"><semantics id="S2.I1.i3.p1.1.m1.1a"><msubsup id="S2.I1.i3.p1.1.m1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.cmml"><mi id="S2.I1.i3.p1.1.m1.1.1.2.2" xref="S2.I1.i3.p1.1.m1.1.1.2.2.cmml">w</mi><mi id="S2.I1.i3.p1.1.m1.1.1.2.3" xref="S2.I1.i3.p1.1.m1.1.1.2.3.cmml">i</mi><mrow id="S2.I1.i3.p1.1.m1.1.1.3" xref="S2.I1.i3.p1.1.m1.1.1.3.cmml"><mi id="S2.I1.i3.p1.1.m1.1.1.3.2" xref="S2.I1.i3.p1.1.m1.1.1.3.2.cmml">r</mi><mo id="S2.I1.i3.p1.1.m1.1.1.3.1" xref="S2.I1.i3.p1.1.m1.1.1.3.1.cmml">+</mo><mn id="S2.I1.i3.p1.1.m1.1.1.3.3" xref="S2.I1.i3.p1.1.m1.1.1.3.3.cmml">1</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.1.m1.1b"><apply id="S2.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i3.p1.1.m1.1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1">superscript</csymbol><apply id="S2.I1.i3.p1.1.m1.1.1.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i3.p1.1.m1.1.1.2.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i3.p1.1.m1.1.1.2.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2.2">𝑤</ci><ci id="S2.I1.i3.p1.1.m1.1.1.2.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2.3">𝑖</ci></apply><apply id="S2.I1.i3.p1.1.m1.1.1.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3"><plus id="S2.I1.i3.p1.1.m1.1.1.3.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3.1"></plus><ci id="S2.I1.i3.p1.1.m1.1.1.3.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3.2">𝑟</ci><cn type="integer" id="S2.I1.i3.p1.1.m1.1.1.3.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.1.m1.1c">w_{i}^{r+1}</annotation></semantics></math> to obtain a new global model <math id="S2.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="w_{g}^{r+1}" display="inline"><semantics id="S2.I1.i3.p1.2.m2.1a"><msubsup id="S2.I1.i3.p1.2.m2.1.1" xref="S2.I1.i3.p1.2.m2.1.1.cmml"><mi id="S2.I1.i3.p1.2.m2.1.1.2.2" xref="S2.I1.i3.p1.2.m2.1.1.2.2.cmml">w</mi><mi id="S2.I1.i3.p1.2.m2.1.1.2.3" xref="S2.I1.i3.p1.2.m2.1.1.2.3.cmml">g</mi><mrow id="S2.I1.i3.p1.2.m2.1.1.3" xref="S2.I1.i3.p1.2.m2.1.1.3.cmml"><mi id="S2.I1.i3.p1.2.m2.1.1.3.2" xref="S2.I1.i3.p1.2.m2.1.1.3.2.cmml">r</mi><mo id="S2.I1.i3.p1.2.m2.1.1.3.1" xref="S2.I1.i3.p1.2.m2.1.1.3.1.cmml">+</mo><mn id="S2.I1.i3.p1.2.m2.1.1.3.3" xref="S2.I1.i3.p1.2.m2.1.1.3.3.cmml">1</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.2.m2.1b"><apply id="S2.I1.i3.p1.2.m2.1.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.I1.i3.p1.2.m2.1.1.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1">superscript</csymbol><apply id="S2.I1.i3.p1.2.m2.1.1.2.cmml" xref="S2.I1.i3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.I1.i3.p1.2.m2.1.1.2.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1">subscript</csymbol><ci id="S2.I1.i3.p1.2.m2.1.1.2.2.cmml" xref="S2.I1.i3.p1.2.m2.1.1.2.2">𝑤</ci><ci id="S2.I1.i3.p1.2.m2.1.1.2.3.cmml" xref="S2.I1.i3.p1.2.m2.1.1.2.3">𝑔</ci></apply><apply id="S2.I1.i3.p1.2.m2.1.1.3.cmml" xref="S2.I1.i3.p1.2.m2.1.1.3"><plus id="S2.I1.i3.p1.2.m2.1.1.3.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1.3.1"></plus><ci id="S2.I1.i3.p1.2.m2.1.1.3.2.cmml" xref="S2.I1.i3.p1.2.m2.1.1.3.2">𝑟</ci><cn type="integer" id="S2.I1.i3.p1.2.m2.1.1.3.3.cmml" xref="S2.I1.i3.p1.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.2.m2.1c">w_{g}^{r+1}</annotation></semantics></math>. Then, the process returns to step 1. The entire process is repeated until convergence.</p>
</div>
</li>
</ol>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2406.10861/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="129" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>The typical training process of federated learning consists of ① model distribution, ② local model update, and ③ global model update.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Knowledge Distillation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Knowledge distillation (KD) is an effective method for model compression and enhancement <cite class="ltx_cite ltx_citemacro_citep">(Wang and Yoon, <a href="#bib.bib151" title="" class="ltx_ref">2021</a>)</cite>. The main idea is to transfer knowledge from a high-performing teacher model to a student model, as shown in Figure <a href="#S2.F3" title="Figure 3 ‣ 2.2. Knowledge Distillation ‣ 2. Background ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. In KD, knowledge transfer is accomplished by having the student model mimic the soft labels of the teacher model instead of its parameters <cite class="ltx_cite ltx_citemacro_citep">(Phuong and Lampert, <a href="#bib.bib119" title="" class="ltx_ref">2019</a>; Gou et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2021</a>)</cite>. This method of knowledge transfer in KD reduces communication costs and permits the student model architecture to differ from that of the teacher model. The soft labels provided by the teacher model serve as regularization constraints for the student model <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a href="#bib.bib179" title="" class="ltx_ref">2020</a>)</cite>, which helps improve its generalization performance.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2406.10861/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="63" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>KD uses a teacher-student architecture. Teacher model transfers knowledge to the student model.</figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.20" class="ltx_p"><span id="S2.SS2.p2.20.1" class="ltx_text ltx_font_bold">Definition</span>. In the setting of KD, there are two models: the student model <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><mi id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><ci id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">S</annotation></semantics></math> and the teacher model <math id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><mi id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><ci id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">T</annotation></semantics></math>. <math id="S2.SS2.p2.3.m3.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.SS2.p2.3.m3.1a"><mi id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.1b"><ci id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.1c">S</annotation></semantics></math> is the model to be trained. By imitating the output of <math id="S2.SS2.p2.4.m4.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.SS2.p2.4.m4.1a"><mi id="S2.SS2.p2.4.m4.1.1" xref="S2.SS2.p2.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.4.m4.1b"><ci id="S2.SS2.p2.4.m4.1.1.cmml" xref="S2.SS2.p2.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.4.m4.1c">T</annotation></semantics></math> on the dataset <math id="S2.SS2.p2.5.m5.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S2.SS2.p2.5.m5.1a"><mi id="S2.SS2.p2.5.m5.1.1" xref="S2.SS2.p2.5.m5.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.5.m5.1b"><ci id="S2.SS2.p2.5.m5.1.1.cmml" xref="S2.SS2.p2.5.m5.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.5.m5.1c">D</annotation></semantics></math>, <math id="S2.SS2.p2.6.m6.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.SS2.p2.6.m6.1a"><mi id="S2.SS2.p2.6.m6.1.1" xref="S2.SS2.p2.6.m6.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.6.m6.1b"><ci id="S2.SS2.p2.6.m6.1.1.cmml" xref="S2.SS2.p2.6.m6.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.6.m6.1c">S</annotation></semantics></math> can improve its performance. Let <math id="S2.SS2.p2.7.m7.1" class="ltx_Math" alttext="S_{1}" display="inline"><semantics id="S2.SS2.p2.7.m7.1a"><msub id="S2.SS2.p2.7.m7.1.1" xref="S2.SS2.p2.7.m7.1.1.cmml"><mi id="S2.SS2.p2.7.m7.1.1.2" xref="S2.SS2.p2.7.m7.1.1.2.cmml">S</mi><mn id="S2.SS2.p2.7.m7.1.1.3" xref="S2.SS2.p2.7.m7.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.7.m7.1b"><apply id="S2.SS2.p2.7.m7.1.1.cmml" xref="S2.SS2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.7.m7.1.1.1.cmml" xref="S2.SS2.p2.7.m7.1.1">subscript</csymbol><ci id="S2.SS2.p2.7.m7.1.1.2.cmml" xref="S2.SS2.p2.7.m7.1.1.2">𝑆</ci><cn type="integer" id="S2.SS2.p2.7.m7.1.1.3.cmml" xref="S2.SS2.p2.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.7.m7.1c">S_{1}</annotation></semantics></math> be the model <math id="S2.SS2.p2.8.m8.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.SS2.p2.8.m8.1a"><mi id="S2.SS2.p2.8.m8.1.1" xref="S2.SS2.p2.8.m8.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.8.m8.1b"><ci id="S2.SS2.p2.8.m8.1.1.cmml" xref="S2.SS2.p2.8.m8.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.8.m8.1c">S</annotation></semantics></math> trained solely on <math id="S2.SS2.p2.9.m9.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S2.SS2.p2.9.m9.1a"><mi id="S2.SS2.p2.9.m9.1.1" xref="S2.SS2.p2.9.m9.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.9.m9.1b"><ci id="S2.SS2.p2.9.m9.1.1.cmml" xref="S2.SS2.p2.9.m9.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.9.m9.1c">D</annotation></semantics></math> without guidance from <math id="S2.SS2.p2.10.m10.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.SS2.p2.10.m10.1a"><mi id="S2.SS2.p2.10.m10.1.1" xref="S2.SS2.p2.10.m10.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.10.m10.1b"><ci id="S2.SS2.p2.10.m10.1.1.cmml" xref="S2.SS2.p2.10.m10.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.10.m10.1c">T</annotation></semantics></math>, and <math id="S2.SS2.p2.11.m11.1" class="ltx_Math" alttext="S_{2}" display="inline"><semantics id="S2.SS2.p2.11.m11.1a"><msub id="S2.SS2.p2.11.m11.1.1" xref="S2.SS2.p2.11.m11.1.1.cmml"><mi id="S2.SS2.p2.11.m11.1.1.2" xref="S2.SS2.p2.11.m11.1.1.2.cmml">S</mi><mn id="S2.SS2.p2.11.m11.1.1.3" xref="S2.SS2.p2.11.m11.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.11.m11.1b"><apply id="S2.SS2.p2.11.m11.1.1.cmml" xref="S2.SS2.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.11.m11.1.1.1.cmml" xref="S2.SS2.p2.11.m11.1.1">subscript</csymbol><ci id="S2.SS2.p2.11.m11.1.1.2.cmml" xref="S2.SS2.p2.11.m11.1.1.2">𝑆</ci><cn type="integer" id="S2.SS2.p2.11.m11.1.1.3.cmml" xref="S2.SS2.p2.11.m11.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.11.m11.1c">S_{2}</annotation></semantics></math> be the model <math id="S2.SS2.p2.12.m12.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.SS2.p2.12.m12.1a"><mi id="S2.SS2.p2.12.m12.1.1" xref="S2.SS2.p2.12.m12.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.12.m12.1b"><ci id="S2.SS2.p2.12.m12.1.1.cmml" xref="S2.SS2.p2.12.m12.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.12.m12.1c">S</annotation></semantics></math> trained on <math id="S2.SS2.p2.13.m13.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S2.SS2.p2.13.m13.1a"><mi id="S2.SS2.p2.13.m13.1.1" xref="S2.SS2.p2.13.m13.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.13.m13.1b"><ci id="S2.SS2.p2.13.m13.1.1.cmml" xref="S2.SS2.p2.13.m13.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.13.m13.1c">D</annotation></semantics></math> with guidance from <math id="S2.SS2.p2.14.m14.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.SS2.p2.14.m14.1a"><mi id="S2.SS2.p2.14.m14.1.1" xref="S2.SS2.p2.14.m14.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.14.m14.1b"><ci id="S2.SS2.p2.14.m14.1.1.cmml" xref="S2.SS2.p2.14.m14.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.14.m14.1c">T</annotation></semantics></math>. Let <math id="S2.SS2.p2.15.m15.1" class="ltx_Math" alttext="V_{t}" display="inline"><semantics id="S2.SS2.p2.15.m15.1a"><msub id="S2.SS2.p2.15.m15.1.1" xref="S2.SS2.p2.15.m15.1.1.cmml"><mi id="S2.SS2.p2.15.m15.1.1.2" xref="S2.SS2.p2.15.m15.1.1.2.cmml">V</mi><mi id="S2.SS2.p2.15.m15.1.1.3" xref="S2.SS2.p2.15.m15.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.15.m15.1b"><apply id="S2.SS2.p2.15.m15.1.1.cmml" xref="S2.SS2.p2.15.m15.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.15.m15.1.1.1.cmml" xref="S2.SS2.p2.15.m15.1.1">subscript</csymbol><ci id="S2.SS2.p2.15.m15.1.1.2.cmml" xref="S2.SS2.p2.15.m15.1.1.2">𝑉</ci><ci id="S2.SS2.p2.15.m15.1.1.3.cmml" xref="S2.SS2.p2.15.m15.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.15.m15.1c">V_{t}</annotation></semantics></math>, <math id="S2.SS2.p2.16.m16.1" class="ltx_Math" alttext="V_{s_{1}}" display="inline"><semantics id="S2.SS2.p2.16.m16.1a"><msub id="S2.SS2.p2.16.m16.1.1" xref="S2.SS2.p2.16.m16.1.1.cmml"><mi id="S2.SS2.p2.16.m16.1.1.2" xref="S2.SS2.p2.16.m16.1.1.2.cmml">V</mi><msub id="S2.SS2.p2.16.m16.1.1.3" xref="S2.SS2.p2.16.m16.1.1.3.cmml"><mi id="S2.SS2.p2.16.m16.1.1.3.2" xref="S2.SS2.p2.16.m16.1.1.3.2.cmml">s</mi><mn id="S2.SS2.p2.16.m16.1.1.3.3" xref="S2.SS2.p2.16.m16.1.1.3.3.cmml">1</mn></msub></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.16.m16.1b"><apply id="S2.SS2.p2.16.m16.1.1.cmml" xref="S2.SS2.p2.16.m16.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.16.m16.1.1.1.cmml" xref="S2.SS2.p2.16.m16.1.1">subscript</csymbol><ci id="S2.SS2.p2.16.m16.1.1.2.cmml" xref="S2.SS2.p2.16.m16.1.1.2">𝑉</ci><apply id="S2.SS2.p2.16.m16.1.1.3.cmml" xref="S2.SS2.p2.16.m16.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p2.16.m16.1.1.3.1.cmml" xref="S2.SS2.p2.16.m16.1.1.3">subscript</csymbol><ci id="S2.SS2.p2.16.m16.1.1.3.2.cmml" xref="S2.SS2.p2.16.m16.1.1.3.2">𝑠</ci><cn type="integer" id="S2.SS2.p2.16.m16.1.1.3.3.cmml" xref="S2.SS2.p2.16.m16.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.16.m16.1c">V_{s_{1}}</annotation></semantics></math>, and <math id="S2.SS2.p2.17.m17.1" class="ltx_Math" alttext="V_{s_{2}}" display="inline"><semantics id="S2.SS2.p2.17.m17.1a"><msub id="S2.SS2.p2.17.m17.1.1" xref="S2.SS2.p2.17.m17.1.1.cmml"><mi id="S2.SS2.p2.17.m17.1.1.2" xref="S2.SS2.p2.17.m17.1.1.2.cmml">V</mi><msub id="S2.SS2.p2.17.m17.1.1.3" xref="S2.SS2.p2.17.m17.1.1.3.cmml"><mi id="S2.SS2.p2.17.m17.1.1.3.2" xref="S2.SS2.p2.17.m17.1.1.3.2.cmml">s</mi><mn id="S2.SS2.p2.17.m17.1.1.3.3" xref="S2.SS2.p2.17.m17.1.1.3.3.cmml">2</mn></msub></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.17.m17.1b"><apply id="S2.SS2.p2.17.m17.1.1.cmml" xref="S2.SS2.p2.17.m17.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.17.m17.1.1.1.cmml" xref="S2.SS2.p2.17.m17.1.1">subscript</csymbol><ci id="S2.SS2.p2.17.m17.1.1.2.cmml" xref="S2.SS2.p2.17.m17.1.1.2">𝑉</ci><apply id="S2.SS2.p2.17.m17.1.1.3.cmml" xref="S2.SS2.p2.17.m17.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p2.17.m17.1.1.3.1.cmml" xref="S2.SS2.p2.17.m17.1.1.3">subscript</csymbol><ci id="S2.SS2.p2.17.m17.1.1.3.2.cmml" xref="S2.SS2.p2.17.m17.1.1.3.2">𝑠</ci><cn type="integer" id="S2.SS2.p2.17.m17.1.1.3.3.cmml" xref="S2.SS2.p2.17.m17.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.17.m17.1c">V_{s_{2}}</annotation></semantics></math> be the performance measures of <math id="S2.SS2.p2.18.m18.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.SS2.p2.18.m18.1a"><mi id="S2.SS2.p2.18.m18.1.1" xref="S2.SS2.p2.18.m18.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.18.m18.1b"><ci id="S2.SS2.p2.18.m18.1.1.cmml" xref="S2.SS2.p2.18.m18.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.18.m18.1c">T</annotation></semantics></math>, <math id="S2.SS2.p2.19.m19.1" class="ltx_Math" alttext="S_{1}" display="inline"><semantics id="S2.SS2.p2.19.m19.1a"><msub id="S2.SS2.p2.19.m19.1.1" xref="S2.SS2.p2.19.m19.1.1.cmml"><mi id="S2.SS2.p2.19.m19.1.1.2" xref="S2.SS2.p2.19.m19.1.1.2.cmml">S</mi><mn id="S2.SS2.p2.19.m19.1.1.3" xref="S2.SS2.p2.19.m19.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.19.m19.1b"><apply id="S2.SS2.p2.19.m19.1.1.cmml" xref="S2.SS2.p2.19.m19.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.19.m19.1.1.1.cmml" xref="S2.SS2.p2.19.m19.1.1">subscript</csymbol><ci id="S2.SS2.p2.19.m19.1.1.2.cmml" xref="S2.SS2.p2.19.m19.1.1.2">𝑆</ci><cn type="integer" id="S2.SS2.p2.19.m19.1.1.3.cmml" xref="S2.SS2.p2.19.m19.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.19.m19.1c">S_{1}</annotation></semantics></math>, and <math id="S2.SS2.p2.20.m20.1" class="ltx_Math" alttext="S_{2}" display="inline"><semantics id="S2.SS2.p2.20.m20.1a"><msub id="S2.SS2.p2.20.m20.1.1" xref="S2.SS2.p2.20.m20.1.1.cmml"><mi id="S2.SS2.p2.20.m20.1.1.2" xref="S2.SS2.p2.20.m20.1.1.2.cmml">S</mi><mn id="S2.SS2.p2.20.m20.1.1.3" xref="S2.SS2.p2.20.m20.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.20.m20.1b"><apply id="S2.SS2.p2.20.m20.1.1.cmml" xref="S2.SS2.p2.20.m20.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.20.m20.1.1.1.cmml" xref="S2.SS2.p2.20.m20.1.1">subscript</csymbol><ci id="S2.SS2.p2.20.m20.1.1.2.cmml" xref="S2.SS2.p2.20.m20.1.1.2">𝑆</ci><cn type="integer" id="S2.SS2.p2.20.m20.1.1.3.cmml" xref="S2.SS2.p2.20.m20.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.20.m20.1c">S_{2}</annotation></semantics></math>, respectively. Then,</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.1" class="ltx_Math" alttext="\left|V_{s_{2}}-V_{s_{1}}\right|\textless\delta,\left|V_{t}-V_{s_{2}}\right|\textless\eta," display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1.1"><mrow id="S2.E1.m1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.3.cmml"><mrow id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.2.cmml"><mo id="S2.E1.m1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.2.1.cmml">|</mo><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S2.E1.m1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">V</mi><msub id="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.3.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">s</mi><mn id="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.3.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.3.3.cmml">2</mn></msub></msub><mo id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S2.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">V</mi><msub id="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.3.2.cmml">s</mi><mn id="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">1</mn></msub></msub></mrow><mo id="S2.E1.m1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.2.1.cmml">|</mo></mrow><mo id="S2.E1.m1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml">&lt;</mo><mi id="S2.E1.m1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.3.cmml">δ</mi></mrow><mo id="S2.E1.m1.1.1.1.1.2.3" xref="S2.E1.m1.1.1.1.1.3a.cmml">,</mo><mrow id="S2.E1.m1.1.1.1.1.2.2" xref="S2.E1.m1.1.1.1.1.2.2.cmml"><mrow id="S2.E1.m1.1.1.1.1.2.2.1.1" xref="S2.E1.m1.1.1.1.1.2.2.1.2.cmml"><mo id="S2.E1.m1.1.1.1.1.2.2.1.1.2" xref="S2.E1.m1.1.1.1.1.2.2.1.2.1.cmml">|</mo><mrow id="S2.E1.m1.1.1.1.1.2.2.1.1.1" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.cmml"><msub id="S2.E1.m1.1.1.1.1.2.2.1.1.1.2" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.2.cmml"><mi id="S2.E1.m1.1.1.1.1.2.2.1.1.1.2.2" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.2.2.cmml">V</mi><mi id="S2.E1.m1.1.1.1.1.2.2.1.1.1.2.3" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.2.3.cmml">t</mi></msub><mo id="S2.E1.m1.1.1.1.1.2.2.1.1.1.1" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.1.cmml">−</mo><msub id="S2.E1.m1.1.1.1.1.2.2.1.1.1.3" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.cmml"><mi id="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.2.cmml">V</mi><msub id="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.3.cmml"><mi id="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.3.2" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.3.2.cmml">s</mi><mn id="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.3.3" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.3.3.cmml">2</mn></msub></msub></mrow><mo id="S2.E1.m1.1.1.1.1.2.2.1.1.3" xref="S2.E1.m1.1.1.1.1.2.2.1.2.1.cmml">|</mo></mrow><mo id="S2.E1.m1.1.1.1.1.2.2.2" xref="S2.E1.m1.1.1.1.1.2.2.2.cmml">&lt;</mo><mi id="S2.E1.m1.1.1.1.1.2.2.3" xref="S2.E1.m1.1.1.1.1.2.2.3.cmml">η</mi></mrow></mrow><mo id="S2.E1.m1.1.1.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.3a.cmml" xref="S2.E1.m1.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1"><lt id="S2.E1.m1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2"></lt><apply id="S2.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1"><abs id="S2.E1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2"></abs><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1"><minus id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.2">𝑉</ci><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.3.2">𝑠</ci><cn type="integer" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.3.3">2</cn></apply></apply><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.2">𝑉</ci><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.3.2">𝑠</ci><cn type="integer" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.3.3">1</cn></apply></apply></apply></apply><ci id="S2.E1.m1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3">𝛿</ci></apply><apply id="S2.E1.m1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2"><lt id="S2.E1.m1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2.2"></lt><apply id="S2.E1.m1.1.1.1.1.2.2.1.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1"><abs id="S2.E1.m1.1.1.1.1.2.2.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.2"></abs><apply id="S2.E1.m1.1.1.1.1.2.2.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1"><minus id="S2.E1.m1.1.1.1.1.2.2.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.1"></minus><apply id="S2.E1.m1.1.1.1.1.2.2.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.2.2.1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.2">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.2.2.1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.2.2">𝑉</ci><ci id="S2.E1.m1.1.1.1.1.2.2.1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.2.3">𝑡</ci></apply><apply id="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.2">𝑉</ci><apply id="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.3.1.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.3">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.3.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.3.2">𝑠</ci><cn type="integer" id="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.3.3.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.3.3">2</cn></apply></apply></apply></apply><ci id="S2.E1.m1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.1.1.1.1.2.2.3">𝜂</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\left|V_{s_{2}}-V_{s_{1}}\right|\textless\delta,\left|V_{t}-V_{s_{2}}\right|\textless\eta,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS2.p3.6" class="ltx_p">where <math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><mi id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><ci id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">\delta</annotation></semantics></math> and <math id="S2.SS2.p3.2.m2.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S2.SS2.p3.2.m2.1a"><mi id="S2.SS2.p3.2.m2.1.1" xref="S2.SS2.p3.2.m2.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.2.m2.1b"><ci id="S2.SS2.p3.2.m2.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.2.m2.1c">\eta</annotation></semantics></math> are non-negative real numbers that satisfy the above equation as small as possible. At this point, we say that <math id="S2.SS2.p3.3.m3.1" class="ltx_Math" alttext="V_{s_{2}}" display="inline"><semantics id="S2.SS2.p3.3.m3.1a"><msub id="S2.SS2.p3.3.m3.1.1" xref="S2.SS2.p3.3.m3.1.1.cmml"><mi id="S2.SS2.p3.3.m3.1.1.2" xref="S2.SS2.p3.3.m3.1.1.2.cmml">V</mi><msub id="S2.SS2.p3.3.m3.1.1.3" xref="S2.SS2.p3.3.m3.1.1.3.cmml"><mi id="S2.SS2.p3.3.m3.1.1.3.2" xref="S2.SS2.p3.3.m3.1.1.3.2.cmml">s</mi><mn id="S2.SS2.p3.3.m3.1.1.3.3" xref="S2.SS2.p3.3.m3.1.1.3.3.cmml">2</mn></msub></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.3.m3.1b"><apply id="S2.SS2.p3.3.m3.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p3.3.m3.1.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.p3.3.m3.1.1.2.cmml" xref="S2.SS2.p3.3.m3.1.1.2">𝑉</ci><apply id="S2.SS2.p3.3.m3.1.1.3.cmml" xref="S2.SS2.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p3.3.m3.1.1.3.1.cmml" xref="S2.SS2.p3.3.m3.1.1.3">subscript</csymbol><ci id="S2.SS2.p3.3.m3.1.1.3.2.cmml" xref="S2.SS2.p3.3.m3.1.1.3.2">𝑠</ci><cn type="integer" id="S2.SS2.p3.3.m3.1.1.3.3.cmml" xref="S2.SS2.p3.3.m3.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.3.m3.1c">V_{s_{2}}</annotation></semantics></math> has a performance gain of <math id="S2.SS2.p3.4.m4.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S2.SS2.p3.4.m4.1a"><mi id="S2.SS2.p3.4.m4.1.1" xref="S2.SS2.p3.4.m4.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.4.m4.1b"><ci id="S2.SS2.p3.4.m4.1.1.cmml" xref="S2.SS2.p3.4.m4.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.4.m4.1c">\delta</annotation></semantics></math>, which can be seen as the result of the student model learning from the teacher model. Typically, <math id="S2.SS2.p3.5.m5.1" class="ltx_Math" alttext="V_{t}\textgreater V_{s_{2}}" display="inline"><semantics id="S2.SS2.p3.5.m5.1a"><mrow id="S2.SS2.p3.5.m5.1.1" xref="S2.SS2.p3.5.m5.1.1.cmml"><msub id="S2.SS2.p3.5.m5.1.1.2" xref="S2.SS2.p3.5.m5.1.1.2.cmml"><mi id="S2.SS2.p3.5.m5.1.1.2.2" xref="S2.SS2.p3.5.m5.1.1.2.2.cmml">V</mi><mi id="S2.SS2.p3.5.m5.1.1.2.3" xref="S2.SS2.p3.5.m5.1.1.2.3.cmml">t</mi></msub><mo id="S2.SS2.p3.5.m5.1.1.1" xref="S2.SS2.p3.5.m5.1.1.1.cmml">&gt;</mo><msub id="S2.SS2.p3.5.m5.1.1.3" xref="S2.SS2.p3.5.m5.1.1.3.cmml"><mi id="S2.SS2.p3.5.m5.1.1.3.2" xref="S2.SS2.p3.5.m5.1.1.3.2.cmml">V</mi><msub id="S2.SS2.p3.5.m5.1.1.3.3" xref="S2.SS2.p3.5.m5.1.1.3.3.cmml"><mi id="S2.SS2.p3.5.m5.1.1.3.3.2" xref="S2.SS2.p3.5.m5.1.1.3.3.2.cmml">s</mi><mn id="S2.SS2.p3.5.m5.1.1.3.3.3" xref="S2.SS2.p3.5.m5.1.1.3.3.3.cmml">2</mn></msub></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.5.m5.1b"><apply id="S2.SS2.p3.5.m5.1.1.cmml" xref="S2.SS2.p3.5.m5.1.1"><gt id="S2.SS2.p3.5.m5.1.1.1.cmml" xref="S2.SS2.p3.5.m5.1.1.1"></gt><apply id="S2.SS2.p3.5.m5.1.1.2.cmml" xref="S2.SS2.p3.5.m5.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p3.5.m5.1.1.2.1.cmml" xref="S2.SS2.p3.5.m5.1.1.2">subscript</csymbol><ci id="S2.SS2.p3.5.m5.1.1.2.2.cmml" xref="S2.SS2.p3.5.m5.1.1.2.2">𝑉</ci><ci id="S2.SS2.p3.5.m5.1.1.2.3.cmml" xref="S2.SS2.p3.5.m5.1.1.2.3">𝑡</ci></apply><apply id="S2.SS2.p3.5.m5.1.1.3.cmml" xref="S2.SS2.p3.5.m5.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p3.5.m5.1.1.3.1.cmml" xref="S2.SS2.p3.5.m5.1.1.3">subscript</csymbol><ci id="S2.SS2.p3.5.m5.1.1.3.2.cmml" xref="S2.SS2.p3.5.m5.1.1.3.2">𝑉</ci><apply id="S2.SS2.p3.5.m5.1.1.3.3.cmml" xref="S2.SS2.p3.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS2.p3.5.m5.1.1.3.3.1.cmml" xref="S2.SS2.p3.5.m5.1.1.3.3">subscript</csymbol><ci id="S2.SS2.p3.5.m5.1.1.3.3.2.cmml" xref="S2.SS2.p3.5.m5.1.1.3.3.2">𝑠</ci><cn type="integer" id="S2.SS2.p3.5.m5.1.1.3.3.3.cmml" xref="S2.SS2.p3.5.m5.1.1.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.5.m5.1c">V_{t}\textgreater V_{s_{2}}</annotation></semantics></math> due to the capacity difference between the teacher and student model. However, when the complexity of the student model is equivalent to or even exceeds that of the teacher model, <math id="S2.SS2.p3.6.m6.1" class="ltx_Math" alttext="V_{s_{2}}\textgreater V_{t}" display="inline"><semantics id="S2.SS2.p3.6.m6.1a"><mrow id="S2.SS2.p3.6.m6.1.1" xref="S2.SS2.p3.6.m6.1.1.cmml"><msub id="S2.SS2.p3.6.m6.1.1.2" xref="S2.SS2.p3.6.m6.1.1.2.cmml"><mi id="S2.SS2.p3.6.m6.1.1.2.2" xref="S2.SS2.p3.6.m6.1.1.2.2.cmml">V</mi><msub id="S2.SS2.p3.6.m6.1.1.2.3" xref="S2.SS2.p3.6.m6.1.1.2.3.cmml"><mi id="S2.SS2.p3.6.m6.1.1.2.3.2" xref="S2.SS2.p3.6.m6.1.1.2.3.2.cmml">s</mi><mn id="S2.SS2.p3.6.m6.1.1.2.3.3" xref="S2.SS2.p3.6.m6.1.1.2.3.3.cmml">2</mn></msub></msub><mo id="S2.SS2.p3.6.m6.1.1.1" xref="S2.SS2.p3.6.m6.1.1.1.cmml">&gt;</mo><msub id="S2.SS2.p3.6.m6.1.1.3" xref="S2.SS2.p3.6.m6.1.1.3.cmml"><mi id="S2.SS2.p3.6.m6.1.1.3.2" xref="S2.SS2.p3.6.m6.1.1.3.2.cmml">V</mi><mi id="S2.SS2.p3.6.m6.1.1.3.3" xref="S2.SS2.p3.6.m6.1.1.3.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.6.m6.1b"><apply id="S2.SS2.p3.6.m6.1.1.cmml" xref="S2.SS2.p3.6.m6.1.1"><gt id="S2.SS2.p3.6.m6.1.1.1.cmml" xref="S2.SS2.p3.6.m6.1.1.1"></gt><apply id="S2.SS2.p3.6.m6.1.1.2.cmml" xref="S2.SS2.p3.6.m6.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p3.6.m6.1.1.2.1.cmml" xref="S2.SS2.p3.6.m6.1.1.2">subscript</csymbol><ci id="S2.SS2.p3.6.m6.1.1.2.2.cmml" xref="S2.SS2.p3.6.m6.1.1.2.2">𝑉</ci><apply id="S2.SS2.p3.6.m6.1.1.2.3.cmml" xref="S2.SS2.p3.6.m6.1.1.2.3"><csymbol cd="ambiguous" id="S2.SS2.p3.6.m6.1.1.2.3.1.cmml" xref="S2.SS2.p3.6.m6.1.1.2.3">subscript</csymbol><ci id="S2.SS2.p3.6.m6.1.1.2.3.2.cmml" xref="S2.SS2.p3.6.m6.1.1.2.3.2">𝑠</ci><cn type="integer" id="S2.SS2.p3.6.m6.1.1.2.3.3.cmml" xref="S2.SS2.p3.6.m6.1.1.2.3.3">2</cn></apply></apply><apply id="S2.SS2.p3.6.m6.1.1.3.cmml" xref="S2.SS2.p3.6.m6.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p3.6.m6.1.1.3.1.cmml" xref="S2.SS2.p3.6.m6.1.1.3">subscript</csymbol><ci id="S2.SS2.p3.6.m6.1.1.3.2.cmml" xref="S2.SS2.p3.6.m6.1.1.3.2">𝑉</ci><ci id="S2.SS2.p3.6.m6.1.1.3.3.cmml" xref="S2.SS2.p3.6.m6.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.6.m6.1c">V_{s_{2}}\textgreater V_{t}</annotation></semantics></math> may occur.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p"><span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_bold">Training Process</span>. Assuming there is a pre-trained teacher model T, as shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the typical training process of KD is as follows:</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<ol id="S2.I2" class="ltx_enumerate">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p">The teacher network outputs soft labels.</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p">The student network uses the soft labels from the teacher network as a regularization constraint for the loss function during training. Repeat the entire process until convergence.</p>
</div>
</li>
</ol>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.2" class="ltx_p">Usually use softmax to convert the logits, <math id="S2.SS2.p6.1.m1.1" class="ltx_Math" alttext="z_{i}" display="inline"><semantics id="S2.SS2.p6.1.m1.1a"><msub id="S2.SS2.p6.1.m1.1.1" xref="S2.SS2.p6.1.m1.1.1.cmml"><mi id="S2.SS2.p6.1.m1.1.1.2" xref="S2.SS2.p6.1.m1.1.1.2.cmml">z</mi><mi id="S2.SS2.p6.1.m1.1.1.3" xref="S2.SS2.p6.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.1.m1.1b"><apply id="S2.SS2.p6.1.m1.1.1.cmml" xref="S2.SS2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p6.1.m1.1.1.1.cmml" xref="S2.SS2.p6.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p6.1.m1.1.1.2.cmml" xref="S2.SS2.p6.1.m1.1.1.2">𝑧</ci><ci id="S2.SS2.p6.1.m1.1.1.3.cmml" xref="S2.SS2.p6.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.1.m1.1c">z_{i}</annotation></semantics></math>, into class probabilities, <math id="S2.SS2.p6.2.m2.1" class="ltx_Math" alttext="q_{i}" display="inline"><semantics id="S2.SS2.p6.2.m2.1a"><msub id="S2.SS2.p6.2.m2.1.1" xref="S2.SS2.p6.2.m2.1.1.cmml"><mi id="S2.SS2.p6.2.m2.1.1.2" xref="S2.SS2.p6.2.m2.1.1.2.cmml">q</mi><mi id="S2.SS2.p6.2.m2.1.1.3" xref="S2.SS2.p6.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.2.m2.1b"><apply id="S2.SS2.p6.2.m2.1.1.cmml" xref="S2.SS2.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p6.2.m2.1.1.1.cmml" xref="S2.SS2.p6.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p6.2.m2.1.1.2.cmml" xref="S2.SS2.p6.2.m2.1.1.2">𝑞</ci><ci id="S2.SS2.p6.2.m2.1.1.3.cmml" xref="S2.SS2.p6.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.2.m2.1c">q_{i}</annotation></semantics></math>, which are used as soft labels <cite class="ltx_cite ltx_citemacro_citep">(Hinton et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2015</a>)</cite>,</p>
</div>
<div id="S2.SS2.p7" class="ltx_para">
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.2" class="ltx_Math" alttext="q_{i}=\frac{exp(z_{i}/\tau)}{\sum_{j}{exp(z_{j}/\tau)}}" display="block"><semantics id="S2.E2.m1.2a"><mrow id="S2.E2.m1.2.3" xref="S2.E2.m1.2.3.cmml"><msub id="S2.E2.m1.2.3.2" xref="S2.E2.m1.2.3.2.cmml"><mi id="S2.E2.m1.2.3.2.2" xref="S2.E2.m1.2.3.2.2.cmml">q</mi><mi id="S2.E2.m1.2.3.2.3" xref="S2.E2.m1.2.3.2.3.cmml">i</mi></msub><mo id="S2.E2.m1.2.3.1" xref="S2.E2.m1.2.3.1.cmml">=</mo><mfrac id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml"><mrow id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.cmml"><mi id="S2.E2.m1.1.1.1.3" xref="S2.E2.m1.1.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.2" xref="S2.E2.m1.1.1.1.2.cmml">​</mo><mi id="S2.E2.m1.1.1.1.4" xref="S2.E2.m1.1.1.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.2a" xref="S2.E2.m1.1.1.1.2.cmml">​</mo><mi id="S2.E2.m1.1.1.1.5" xref="S2.E2.m1.1.1.1.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.2b" xref="S2.E2.m1.1.1.1.2.cmml">​</mo><mrow id="S2.E2.m1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.cmml"><msub id="S2.E2.m1.1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.2.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.2.2" xref="S2.E2.m1.1.1.1.1.1.1.2.2.cmml">z</mi><mi id="S2.E2.m1.1.1.1.1.1.1.2.3" xref="S2.E2.m1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S2.E2.m1.1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.1.cmml">/</mo><mi id="S2.E2.m1.1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.3.cmml">τ</mi></mrow><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mrow id="S2.E2.m1.2.2.2" xref="S2.E2.m1.2.2.2.cmml"><msub id="S2.E2.m1.2.2.2.2" xref="S2.E2.m1.2.2.2.2.cmml"><mo id="S2.E2.m1.2.2.2.2.2" xref="S2.E2.m1.2.2.2.2.2.cmml">∑</mo><mi id="S2.E2.m1.2.2.2.2.3" xref="S2.E2.m1.2.2.2.2.3.cmml">j</mi></msub><mrow id="S2.E2.m1.2.2.2.1" xref="S2.E2.m1.2.2.2.1.cmml"><mi id="S2.E2.m1.2.2.2.1.3" xref="S2.E2.m1.2.2.2.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.2.1.2" xref="S2.E2.m1.2.2.2.1.2.cmml">​</mo><mi id="S2.E2.m1.2.2.2.1.4" xref="S2.E2.m1.2.2.2.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.2.1.2a" xref="S2.E2.m1.2.2.2.1.2.cmml">​</mo><mi id="S2.E2.m1.2.2.2.1.5" xref="S2.E2.m1.2.2.2.1.5.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.2.1.2b" xref="S2.E2.m1.2.2.2.1.2.cmml">​</mo><mrow id="S2.E2.m1.2.2.2.1.1.1" xref="S2.E2.m1.2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.2.2.2.1.1.1.2" xref="S2.E2.m1.2.2.2.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.2.2.2.1.1.1.1" xref="S2.E2.m1.2.2.2.1.1.1.1.cmml"><msub id="S2.E2.m1.2.2.2.1.1.1.1.2" xref="S2.E2.m1.2.2.2.1.1.1.1.2.cmml"><mi id="S2.E2.m1.2.2.2.1.1.1.1.2.2" xref="S2.E2.m1.2.2.2.1.1.1.1.2.2.cmml">z</mi><mi id="S2.E2.m1.2.2.2.1.1.1.1.2.3" xref="S2.E2.m1.2.2.2.1.1.1.1.2.3.cmml">j</mi></msub><mo id="S2.E2.m1.2.2.2.1.1.1.1.1" xref="S2.E2.m1.2.2.2.1.1.1.1.1.cmml">/</mo><mi id="S2.E2.m1.2.2.2.1.1.1.1.3" xref="S2.E2.m1.2.2.2.1.1.1.1.3.cmml">τ</mi></mrow><mo stretchy="false" id="S2.E2.m1.2.2.2.1.1.1.3" xref="S2.E2.m1.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.2b"><apply id="S2.E2.m1.2.3.cmml" xref="S2.E2.m1.2.3"><eq id="S2.E2.m1.2.3.1.cmml" xref="S2.E2.m1.2.3.1"></eq><apply id="S2.E2.m1.2.3.2.cmml" xref="S2.E2.m1.2.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.2.3.2.1.cmml" xref="S2.E2.m1.2.3.2">subscript</csymbol><ci id="S2.E2.m1.2.3.2.2.cmml" xref="S2.E2.m1.2.3.2.2">𝑞</ci><ci id="S2.E2.m1.2.3.2.3.cmml" xref="S2.E2.m1.2.3.2.3">𝑖</ci></apply><apply id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2"><divide id="S2.E2.m1.2.2.3.cmml" xref="S2.E2.m1.2.2"></divide><apply id="S2.E2.m1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"><times id="S2.E2.m1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.2"></times><ci id="S2.E2.m1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.3">𝑒</ci><ci id="S2.E2.m1.1.1.1.4.cmml" xref="S2.E2.m1.1.1.1.4">𝑥</ci><ci id="S2.E2.m1.1.1.1.5.cmml" xref="S2.E2.m1.1.1.1.5">𝑝</ci><apply id="S2.E2.m1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1"><divide id="S2.E2.m1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1"></divide><apply id="S2.E2.m1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.2">𝑧</ci><ci id="S2.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S2.E2.m1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3">𝜏</ci></apply></apply><apply id="S2.E2.m1.2.2.2.cmml" xref="S2.E2.m1.2.2.2"><apply id="S2.E2.m1.2.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.2.2.1.cmml" xref="S2.E2.m1.2.2.2.2">subscript</csymbol><sum id="S2.E2.m1.2.2.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2.2"></sum><ci id="S2.E2.m1.2.2.2.2.3.cmml" xref="S2.E2.m1.2.2.2.2.3">𝑗</ci></apply><apply id="S2.E2.m1.2.2.2.1.cmml" xref="S2.E2.m1.2.2.2.1"><times id="S2.E2.m1.2.2.2.1.2.cmml" xref="S2.E2.m1.2.2.2.1.2"></times><ci id="S2.E2.m1.2.2.2.1.3.cmml" xref="S2.E2.m1.2.2.2.1.3">𝑒</ci><ci id="S2.E2.m1.2.2.2.1.4.cmml" xref="S2.E2.m1.2.2.2.1.4">𝑥</ci><ci id="S2.E2.m1.2.2.2.1.5.cmml" xref="S2.E2.m1.2.2.2.1.5">𝑝</ci><apply id="S2.E2.m1.2.2.2.1.1.1.1.cmml" xref="S2.E2.m1.2.2.2.1.1.1"><divide id="S2.E2.m1.2.2.2.1.1.1.1.1.cmml" xref="S2.E2.m1.2.2.2.1.1.1.1.1"></divide><apply id="S2.E2.m1.2.2.2.1.1.1.1.2.cmml" xref="S2.E2.m1.2.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.2.2.2.1.1.1.1.2.1.cmml" xref="S2.E2.m1.2.2.2.1.1.1.1.2">subscript</csymbol><ci id="S2.E2.m1.2.2.2.1.1.1.1.2.2.cmml" xref="S2.E2.m1.2.2.2.1.1.1.1.2.2">𝑧</ci><ci id="S2.E2.m1.2.2.2.1.1.1.1.2.3.cmml" xref="S2.E2.m1.2.2.2.1.1.1.1.2.3">𝑗</ci></apply><ci id="S2.E2.m1.2.2.2.1.1.1.1.3.cmml" xref="S2.E2.m1.2.2.2.1.1.1.1.3">𝜏</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.2c">q_{i}=\frac{exp(z_{i}/\tau)}{\sum_{j}{exp(z_{j}/\tau)}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS2.p7.3" class="ltx_p">where temperature <math id="S2.SS2.p7.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S2.SS2.p7.1.m1.1a"><mi id="S2.SS2.p7.1.m1.1.1" xref="S2.SS2.p7.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p7.1.m1.1b"><ci id="S2.SS2.p7.1.m1.1.1.cmml" xref="S2.SS2.p7.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p7.1.m1.1c">\tau</annotation></semantics></math> is a hyperparameter, and higher values of <math id="S2.SS2.p7.2.m2.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S2.SS2.p7.2.m2.1a"><mi id="S2.SS2.p7.2.m2.1.1" xref="S2.SS2.p7.2.m2.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p7.2.m2.1b"><ci id="S2.SS2.p7.2.m2.1.1.cmml" xref="S2.SS2.p7.2.m2.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p7.2.m2.1c">\tau</annotation></semantics></math> will produce smoother output probability distributions, which will make the model pay more attention to negative labels, and the student model can learn more knowledge from the teacher model <cite class="ltx_cite ltx_citemacro_citep">(Hinton et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2012</a>; Phuong and Lampert, <a href="#bib.bib119" title="" class="ltx_ref">2019</a>; Tang et al<span class="ltx_text">.</span>, <a href="#bib.bib140" title="" class="ltx_ref">2020</a>)</cite>. A special case is to directly match logits, which is equivalent to <math id="S2.SS2.p7.3.m3.1" class="ltx_Math" alttext="\tau\rightarrow+\infty" display="inline"><semantics id="S2.SS2.p7.3.m3.1a"><mrow id="S2.SS2.p7.3.m3.1.1" xref="S2.SS2.p7.3.m3.1.1.cmml"><mi id="S2.SS2.p7.3.m3.1.1.2" xref="S2.SS2.p7.3.m3.1.1.2.cmml">τ</mi><mo stretchy="false" id="S2.SS2.p7.3.m3.1.1.1" xref="S2.SS2.p7.3.m3.1.1.1.cmml">→</mo><mrow id="S2.SS2.p7.3.m3.1.1.3" xref="S2.SS2.p7.3.m3.1.1.3.cmml"><mo id="S2.SS2.p7.3.m3.1.1.3a" xref="S2.SS2.p7.3.m3.1.1.3.cmml">+</mo><mi mathvariant="normal" id="S2.SS2.p7.3.m3.1.1.3.2" xref="S2.SS2.p7.3.m3.1.1.3.2.cmml">∞</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p7.3.m3.1b"><apply id="S2.SS2.p7.3.m3.1.1.cmml" xref="S2.SS2.p7.3.m3.1.1"><ci id="S2.SS2.p7.3.m3.1.1.1.cmml" xref="S2.SS2.p7.3.m3.1.1.1">→</ci><ci id="S2.SS2.p7.3.m3.1.1.2.cmml" xref="S2.SS2.p7.3.m3.1.1.2">𝜏</ci><apply id="S2.SS2.p7.3.m3.1.1.3.cmml" xref="S2.SS2.p7.3.m3.1.1.3"><plus id="S2.SS2.p7.3.m3.1.1.3.1.cmml" xref="S2.SS2.p7.3.m3.1.1.3"></plus><infinity id="S2.SS2.p7.3.m3.1.1.3.2.cmml" xref="S2.SS2.p7.3.m3.1.1.3.2"></infinity></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p7.3.m3.1c">\tau\rightarrow+\infty</annotation></semantics></math>.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1. </span>Long-lasting Challenges in FL</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">Due to its unique setup, FL presents several distinct challenges compared to traditional centralized training.</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.1" class="ltx_p"><span id="S2.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Privacy Preservation</span>: The original intention of FL was to protect data privacy, but research has shown that sharing only client-side gradients can reveal private data <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib81" title="" class="ltx_ref">2020c</a>; Mothukuri et al<span class="ltx_text">.</span>, <a href="#bib.bib111" title="" class="ltx_ref">2021</a>)</cite>. In fact, model parameters such as weights or gradients can be viewed as a ”compression” of the dataset, which can be restored using certain techniques <cite class="ltx_cite ltx_citemacro_citep">(Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2020</a>; Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2021b</a>)</cite>. This creates a conflict where the sharing of intermediate results is necessary to train a global model, but at the same time, sharing these results also poses a risk to data privacy. Thus, there is a need to balance the compression of intermediate results and data privacy.</p>
</div>
<div id="S2.SS2.SSS1.p3" class="ltx_para">
<p id="S2.SS2.SSS1.p3.1" class="ltx_p"><span id="S2.SS2.SSS1.p3.1.1" class="ltx_text ltx_font_bold">Data Heterogeneity</span>: An essential assumption in machine learning is that the dataset is independent and identically distributed (IID). However, this assumption does not hold in FL. The datasets of various clients not only differ in size but are also often not identically distributed. Each client may have completely different model updates, causing the local objective to deviate entirely from the global objective, resulting in global knowledge forgetting and client drift <cite class="ltx_cite ltx_citemacro_citep">(Yashwanth et al<span class="ltx_text">.</span>, <a href="#bib.bib175" title="" class="ltx_ref">2023</a>)</cite>. This slows down model convergence and degrades model performance <cite class="ltx_cite ltx_citemacro_citep">(Karimireddy et al<span class="ltx_text">.</span>, <a href="#bib.bib68" title="" class="ltx_ref">2019</a>; Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib186" title="" class="ltx_ref">2018</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib148" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S2.SS2.SSS1.p4" class="ltx_para">
<p id="S2.SS2.SSS1.p4.1" class="ltx_p"><span id="S2.SS2.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Communication Bottleneck</span>: To achieve better model performance, participants in FL often use larger models with improved performance <cite class="ltx_cite ltx_citemacro_citep">(Allen-Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2019</a>; Zhou, <a href="#bib.bib191" title="" class="ltx_ref">2021</a>)</cite>. However, training a large model collaboratively can increase the communication cost between clients and the server when exchanging model parameters.
During FL training, there are often many rounds of local training and global aggregation, which require frequent uploading of local models and downloading of global models. In many scenarios, this creates a severe communication bottleneck (e.g., on mobile and edge devices), making FL impractical <cite class="ltx_cite ltx_citemacro_citep">(Konečnỳ et al<span class="ltx_text">.</span>, <a href="#bib.bib71" title="" class="ltx_ref">2016</a>)</cite>.</p>
</div>
<div id="S2.SS2.SSS1.p5" class="ltx_para">
<p id="S2.SS2.SSS1.p5.1" class="ltx_p"><span id="S2.SS2.SSS1.p5.1.1" class="ltx_text ltx_font_bold">Model Personalization</span>: For cross-device FL, clients typically use the same model. However, in many cross-silo scenarios, it is a reasonable and practical requirement for each client to design personalized models based on their dataset and specific application scenarios <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2021a</a>; Luo and Wu, <a href="#bib.bib94" title="" class="ltx_ref">2022</a>)</cite>. Currently, the mainstream methods in FL are based on parameter-sharing, which does not support heterogeneous models because the knowledge of different models is not compatible in form, and a personalized model cannot directly learn the knowledge of other models through parameters.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>KD-based FL</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">KD-based FL is an emerging paradigm that leverages KD to aggregate the features of client-side models and facilitate collaborative model training. The critical distinction between KD-based FL and parameter-based FL lies in their divergent definitions of knowledge. KD-based FL considers various aspects of the model’s features <cite class="ltx_cite ltx_citemacro_citep">(Gou et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2021</a>)</cite>, such as output layer features <cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>; Li and Wang, <a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite> and intermediate features <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2021</a>; Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib172" title="" class="ltx_ref">2022</a>)</cite>, as knowledge. In contrast, parameter-based FL considers the model’s parameters (or model gradients) as knowledge. The two methods exhibit distinct characteristics and face unique challenges within the same privacy protection framework in FL.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<ol id="S2.I3" class="ltx_enumerate">
<li id="S2.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S2.I3.i1.p1" class="ltx_para">
<p id="S2.I3.i1.p1.1" class="ltx_p">The models communicate through the transmission of dark knowledge <cite class="ltx_cite ltx_citemacro_citep">(Hinton et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2015</a>)</cite>, which is based on features instead of parameters.</p>
</div>
</li>
<li id="S2.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S2.I3.i2.p1" class="ltx_para">
<p id="S2.I3.i2.p1.1" class="ltx_p">KD-based FL needs a dataset for distillation, which can be client private data <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2018</a>)</cite>, publicly available data <cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>, or artificially generated synthetic data <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib182" title="" class="ltx_ref">2022b</a>)</cite>.</p>
</div>
</li>
<li id="S2.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S2.I3.i3.p1" class="ltx_para">
<p id="S2.I3.i3.p1.1" class="ltx_p">Typically, KD-based FL lacks a pre-trained teacher model <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib147" title="" class="ltx_ref">2022b</a>)</cite>, and the initial training performance of the teacher model is suboptimal. However, the teacher model gradually improves reliability and convergence as the training progresses.</p>
</div>
</li>
</ol>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2406.10861/assets/x4.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="110" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>The FL process consists of six steps: ① preprocessing of the global model by clients, ② local training by clients, ③ further processing of local models by clients, ④ preprocessing of local models by the server upon receiving them, ⑤ aggregation of local models by the server to obtain the global model, and ⑥ further processing of the global model by the server. KD can be employed in all six steps.</figcaption>
</figure>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">KD-based FL is a concept that lacks a precise definition. Some studies <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2018</a>; Sattler et al<span class="ltx_text">.</span>, <a href="#bib.bib123" title="" class="ltx_ref">2020</a>; Ahn et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2019</a>; Xing et al<span class="ltx_text">.</span>, <a href="#bib.bib164" title="" class="ltx_ref">2022</a>; Oh et al<span class="ltx_text">.</span>, <a href="#bib.bib114" title="" class="ltx_ref">2020</a>)</cite> refer to specific KD-based FL algorithms as federated distillation (FD), but the meaning of FD varies across papers. In this paper, KD-based FL refers to utilizing KD for addressing specific challenges in FL during local training or server aggregation stages. This paper does not differentiate between KD-based FL and FD concepts to avoid confusion. The section <a href="#S3.SS3.SSS3" title="3.3.3. Data-based FD (Based on dataset distillation) ‣ 3.3. KD-based FL: Taxonomy ‣ 3. Knowledge-distillation-based Federated Learning ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.3</span></a> will discuss a distinct type of FD that employs dataset distillation <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib154" title="" class="ltx_ref">2018</a>)</cite> instead of knowledge distillation. In this sense, the term FD better represents the discussed concept than KD-based FL.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">In the early stages of KD-based FL research, the primary focus was on minimizing communication costs <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2018</a>; Ahn et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2019</a>)</cite>, where clients would upload their local model outputs to the server for aggregation. Subsequently, KD evolved to tackle various challenges in FL and integrated with parameter-based FL. Figure <a href="#S2.F4" title="Figure 4 ‣ 2.3. KD-based FL ‣ 2. Background ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates the comprehensive training process of FL, where KD can be employed at all six steps.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Knowledge-distillation-based Federated Learning</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>KD-based FL: Motivation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In the context of FL, data cannot leave the local device, and collaborative model training is accomplished through periodic exchange of model information among clients <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib173" title="" class="ltx_ref">2019</a>)</cite>. Therefore, the model information exchanged in FL must satisfy specific requirements: 1) the model information must be sufficiently abstract to prevent exposure of raw data privacy, 2) the model information must be sufficiently representative to facilitate the collaborative training of a high-performance generalized model by clients, and 3) the model information must be compact enough to minimize communication costs. Traditional FL algorithms utilize model parameters (or parameter gradients) as the model information <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib102" title="" class="ltx_ref">2017a</a>, <a href="#bib.bib103" title="" class="ltx_ref">b</a>)</cite> to be exchanged among clients to achieve the learning objective of FL.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">However, related research <cite class="ltx_cite ltx_citemacro_citep">(Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2020</a>; Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib193" title="" class="ltx_ref">2019</a>)</cite> has shown that model parameters may not be an ideal model information in collaborative learning. Sharing parameters or gradients may lead to the potential disclosure of sensitive original data <cite class="ltx_cite ltx_citemacro_citep">(Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2020</a>; Song et al<span class="ltx_text">.</span>, <a href="#bib.bib133" title="" class="ltx_ref">2020</a>; Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2021b</a>; Lam et al<span class="ltx_text">.</span>, <a href="#bib.bib73" title="" class="ltx_ref">2021</a>)</cite>. Exposing the complete model parameters to potential attackers poses a significant security threat <cite class="ltx_cite ltx_citemacro_citep">(Yin et al<span class="ltx_text">.</span>, <a href="#bib.bib178" title="" class="ltx_ref">2021</a>)</cite>. Although some research <cite class="ltx_cite ltx_citemacro_citep">(Mothukuri et al<span class="ltx_text">.</span>, <a href="#bib.bib111" title="" class="ltx_ref">2021</a>; Yin et al<span class="ltx_text">.</span>, <a href="#bib.bib178" title="" class="ltx_ref">2021</a>)</cite> has been proposed to compensate for the shortcomings of sharing model parameters, inherent defects are challenging to overcome. The concern about privacy in parameter-sharing algorithms is because the information exchanged is too rich and redundant.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">In the training process of parameter-based FL, the primary challenge is the communication bottleneck <cite class="ltx_cite ltx_citemacro_citep">(Shahid et al<span class="ltx_text">.</span>, <a href="#bib.bib126" title="" class="ltx_ref">2021</a>; Rothchild et al<span class="ltx_text">.</span>, <a href="#bib.bib120" title="" class="ltx_ref">2020</a>; Hamer et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2020</a>)</cite>. In the context of FL, clients are distributed, and their communication can be unstable, slow, asymmetric, and expensive. Frequent transmission of complete model parameters results in unbearable communication overhead, thereby limiting the participation of potential clients, particularly in scenarios involving mobile or edge devices. Additionally, uploading model parameters slows down communication and prolongs the training process. Moreover, participants may seek to collaborate on training high-performance and large models, significantly augmenting the communication costs. Although some work <cite class="ltx_cite ltx_citemacro_citep">(Luping et al<span class="ltx_text">.</span>, <a href="#bib.bib96" title="" class="ltx_ref">2019</a>; Sattler et al<span class="ltx_text">.</span>, <a href="#bib.bib125" title="" class="ltx_ref">2019</a>; Konečnỳ et al<span class="ltx_text">.</span>, <a href="#bib.bib71" title="" class="ltx_ref">2016</a>)</cite> has been proposed to mitigate communication overhead, overall communication costs are related to model size.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Participants in FL exhibit diverse identities and goals, working collaboratively to achieve their respective objectives. However, parameter-based FL algorithms impose the constraint of utilizing a uniform model across all participants to attain their objectives, which is impractical in real-world scenarios <cite class="ltx_cite ltx_citemacro_citep">(Mansour et al<span class="ltx_text">.</span>, <a href="#bib.bib101" title="" class="ltx_ref">2020</a>; Kulkarni et al<span class="ltx_text">.</span>, <a href="#bib.bib72" title="" class="ltx_ref">2020</a>)</cite>. For instance, in the case of company participation, there may be a desire to acquire a high-performance and large model; however, such a model might not be suitable for certain participants aiming to deploy it on low-performance devices. Parameter-based algorithms fail to address the personalized modeling requirements of clients. Moreover, sharing model parameters fails to address the computational performance and network connectivity disparities arising from system heterogeneity <cite class="ltx_cite ltx_citemacro_citep">(Diao et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2020</a>; Luo et al<span class="ltx_text">.</span>, <a href="#bib.bib93" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">The most significant obstacle in FL is data heterogeneity <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib186" title="" class="ltx_ref">2018</a>; Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib192" title="" class="ltx_ref">2021b</a>)</cite>. The objective of participant collaboration is to obtain a more robust and generalizable model compared to the one solely trained on local data. The performance of the global model might be compromised by data heterogeneity, resulting in a decline in participants’ propensity to collaborate. The stochastic gradient descent method <cite class="ltx_cite ltx_citemacro_citep">(Amari, <a href="#bib.bib6" title="" class="ltx_ref">1993</a>)</cite> is widely employed in machine learning as the primary optimization technique, assuming that the training data follows an independent and identically distributed (IID) pattern. However, the data distribution of each client in FL is often non-IID<cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib192" title="" class="ltx_ref">2021b</a>)</cite>, resulting in model parameters that solely reflect the optimization outcomes of local private data and fail to capture the optimization results across all data. The optimization directions of different client model parameters exhibit significant variations, and a simple averaging approach fails to yield the globally optimal model. Furthermore, conventional parameter-based algorithms assign different weights to model updates of each client, prioritizing participants with larger datasets while neglecting updates from smaller datasets, leading to fairness concerns in FL <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2020</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2021b</a>)</cite>. Additionally, non-IID data distribution presents challenges for model convergence <cite class="ltx_cite ltx_citemacro_citep">(Wu and Wang, <a href="#bib.bib160" title="" class="ltx_ref">2022</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib148" title="" class="ltx_ref">2020</a>)</cite>, particularly in parameter-based algorithms where direct averaging of model parameters fails to capture the underlying knowledge derived from updates contributed by diverse clients.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">Therefore, FL urgently requires alternative methods for exchanging model information. KD-based FL adopts KD to exchange the logits of the local model to transmit model information between clients <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2018</a>; Chang et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>. This approach compresses the model information to its minimum, significantly reducing communication overhead and minimizing the sensitive information exposed. Using KD for model enhancement <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a href="#bib.bib179" title="" class="ltx_ref">2020</a>)</cite> enables KD-based FL to address non-IID challenges effectively. Moreover, even for traditional parameter-sharing algorithms, KD is a highly effective supplement that can contribute to client training <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2022b</a>)</cite> and server aggregation stages <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib182" title="" class="ltx_ref">2022b</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>How KD tackles the challenges of FL</h3>

<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>Properties of KD</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Due to the unique way of knowledge transfer, KD has some distinct properties that can help to tackle the long-lasting challenges of FL:</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Model agnosticism: KD is model-agnostic <cite class="ltx_cite ltx_citemacro_citep">(Tang et al<span class="ltx_text">.</span>, <a href="#bib.bib140" title="" class="ltx_ref">2020</a>)</cite>. The student model can be designed with a personalized architecture based on actual needs without being required to be the same as the teacher model <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2023b</a>)</cite>. This property makes personalization for clients in FL easy to achieve.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Model compression: KD was originally proposed to compress models for better production deployment <cite class="ltx_cite ltx_citemacro_citep">(Hinton et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2015</a>)</cite>. In the classic KD architecture, there will be a large model as the teacher network and a small model as the student network. The purpose of training is to obtain a student network as small as possible whose performance can meet actual needs.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Capacity gap: The teacher and student models may differ in capacity. The student model may be unable to learn more knowledge from a larger teacher model due to its own limitations <cite class="ltx_cite ltx_citemacro_citep">(Cho and Hariharan, <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>. Therefore, a larger teacher model may not always lead to better distillation results.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Model enhancement: The key to model compression is not just reducing model size but also ensuring that a smaller model can still have similar performance <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib85" title="" class="ltx_ref">2020b</a>; Cho and Hariharan, <a href="#bib.bib19" title="" class="ltx_ref">2019</a>; Phuong and Lampert, <a href="#bib.bib119" title="" class="ltx_ref">2019</a>)</cite>. Therefore, compressed models need to be enhanced to satisfy actual needs. KD can compress models by transferring dark knowledge while enhancing them.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(5)</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">Data dependence: KD is dependent on data <cite class="ltx_cite ltx_citemacro_citep">(Hinton et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2015</a>)</cite>. The teacher model conveys knowledge to the student model by predicting the training data. Therefore, the quality of the training data has a significant impact on the distillation effect. In addition, training data can be unlabeled <cite class="ltx_cite ltx_citemacro_citep">(Menghani and Ravi, <a href="#bib.bib105" title="" class="ltx_ref">2019</a>; Lee et al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
</li>
<li id="S3.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(6)</span> 
<div id="S3.I1.i6.p1" class="ltx_para">
<p id="S3.I1.i6.p1.1" class="ltx_p">Efficient communication: Since only logits need to be transmitted, KD has good communication characteristics in distributed learning scenarios <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2018</a>)</cite>. It enables collaboration between clients without transmitting complete model parameters.</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.1" class="ltx_p">Table <a href="#S3.T2" title="Table 2 ‣ 3.2.1. Properties of KD ‣ 3.2. How KD tackles the challenges of FL ‣ 3. Knowledge-distillation-based Federated Learning ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> puts the properties mentioned above of KD and the challenges in FL to see the relationship between them. These properties give KD an advantage in solving certain challenges in FL.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>The relationship between KD properties and FL challenges.</figcaption>
<div id="S3.T2.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:382.5pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-42.7pt,37.6pt) scale(0.835565017819368,0.835565017819368) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.1.1.1" class="ltx_p" style="width:69.4pt;"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span></span>
</span>
</td>
<td id="S3.T2.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.2.1.1" class="ltx_p" style="width:93.2pt;">Privacy
<br class="ltx_break">preservation</span>
</span>
</td>
<td id="S3.T2.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.3.1.1" class="ltx_p" style="width:93.2pt;">Non-IID</span>
</span>
</td>
<td id="S3.T2.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.4.1.1" class="ltx_p" style="width:93.2pt;">Communication
<br class="ltx_break">efficiency</span>
</span>
</td>
<td id="S3.T2.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.5.1.1" class="ltx_p" style="width:93.2pt;">Personalization</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.2" class="ltx_tr">
<td id="S3.T2.1.1.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.2.1.1.1" class="ltx_p" style="width:69.4pt;">Model
<br class="ltx_break">agnosticism <cite class="ltx_cite ltx_citemacro_citep">(Tang et al<span class="ltx_text">.</span>, <a href="#bib.bib140" title="" class="ltx_ref">2020</a>)</cite></span>
</span>
</td>
<td id="S3.T2.1.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.2.2.1.1" class="ltx_p" style="width:93.2pt;">The client’s model architecture is unknown to other clients, helping to protect privacy <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2022</a>)</cite>.</span>
</span>
</td>
<td id="S3.T2.1.1.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.2.3.1.1" class="ltx_p" style="width:93.2pt;">The client can choose an appropriate model architecture based on local data, which helps alleviate non-IID issues <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a href="#bib.bib130" title="" class="ltx_ref">2022</a>)</cite>.</span>
</span>
</td>
<td id="S3.T2.1.1.2.4" class="ltx_td ltx_align_center ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.1.1.2.4.1" class="ltx_text" style="position:relative; bottom:-20.0pt;">—</span></td>
<td id="S3.T2.1.1.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.2.5.1.1" class="ltx_p" style="width:93.2pt;">The model-agnostic nature of KD facilitates customer personalization <cite class="ltx_cite ltx_citemacro_citep">(Afonin and Karimireddy, <a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.3" class="ltx_tr">
<td id="S3.T2.1.1.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.3.1.1.1" class="ltx_p" style="width:69.4pt;">Model
<br class="ltx_break">compression <cite class="ltx_cite ltx_citemacro_citep">(Hinton et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2015</a>)</cite></span>
</span>
</td>
<td id="S3.T2.1.1.3.2" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.1.1.3.2.1" class="ltx_text" style="position:relative; bottom:-20.0pt;">—</span></td>
<td id="S3.T2.1.1.3.3" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.1.1.3.3.1" class="ltx_text" style="position:relative; bottom:-20.0pt;">—</span></td>
<td id="S3.T2.1.1.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.3.4.1.1" class="ltx_p" style="width:93.2pt;">Clients can use compressed small models as intermediary models for transmission <cite class="ltx_cite ltx_citemacro_citep">(Ozkara et al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2021</a>)</cite>.</span>
</span>
</td>
<td id="S3.T2.1.1.3.5" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.1.1.3.5.1" class="ltx_text" style="position:relative; bottom:-20.0pt;">—</span></td>
</tr>
<tr id="S3.T2.1.1.4" class="ltx_tr">
<td id="S3.T2.1.1.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.4.1.1.1" class="ltx_p" style="width:69.4pt;">Capacity
<br class="ltx_break">gap <cite class="ltx_cite ltx_citemacro_citep">(Cho and Hariharan, <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite></span>
</span>
</td>
<td id="S3.T2.1.1.4.2" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.1.1.4.2.1" class="ltx_text" style="position:relative; bottom:-30.0pt;">—</span></td>
<td id="S3.T2.1.1.4.3" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.1.1.4.3.1" class="ltx_text" style="position:relative; bottom:-30.0pt;">—</span></td>
<td id="S3.T2.1.1.4.4" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.1.1.4.4.1" class="ltx_text" style="position:relative; bottom:-30.0pt;">—</span></td>
<td id="S3.T2.1.1.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.4.5.1.1" class="ltx_p" style="width:93.2pt;">When implementing model personalization, the capacity gap needs to be considered to choose an appropriate model architecture.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1.5" class="ltx_tr">
<td id="S3.T2.1.1.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.5.1.1.1" class="ltx_p" style="width:69.4pt;">Model
<br class="ltx_break">enhancement <cite class="ltx_cite ltx_citemacro_citep">(Phuong and Lampert, <a href="#bib.bib119" title="" class="ltx_ref">2019</a>)</cite></span>
</span>
</td>
<td id="S3.T2.1.1.5.2" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.1.1.5.2.1" class="ltx_text" style="position:relative; bottom:-20.0pt;">—</span></td>
<td id="S3.T2.1.1.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.5.3.1.1" class="ltx_p" style="width:93.2pt;">The global model can learn personalized knowledge from the local model, which helps alleviate non-IID issues <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib182" title="" class="ltx_ref">2022b</a>)</cite>.</span>
</span>
</td>
<td id="S3.T2.1.1.5.4" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.1.1.5.4.1" class="ltx_text" style="position:relative; bottom:-20.0pt;">—</span></td>
<td id="S3.T2.1.1.5.5" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.1.1.5.5.1" class="ltx_text" style="position:relative; bottom:-20.0pt;">—</span></td>
</tr>
<tr id="S3.T2.1.1.6" class="ltx_tr">
<td id="S3.T2.1.1.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.6.1.1.1" class="ltx_p" style="width:69.4pt;">Data
<br class="ltx_break">dependence <cite class="ltx_cite ltx_citemacro_citep">(Hinton et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2015</a>)</cite></span>
</span>
</td>
<td id="S3.T2.1.1.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.6.2.1.1" class="ltx_p" style="width:93.2pt;">Knowledge transfer relies on data and may compromise privacy <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2022</a>)</cite>.</span>
</span>
</td>
<td id="S3.T2.1.1.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.6.3.1.1" class="ltx_p" style="width:93.2pt;">Depending on the correlation of the shared distillation dataset to the client dataset, the non-IID problem may be severe or mitigated <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib90" title="" class="ltx_ref">2022c</a>)</cite>.</span>
</span>
</td>
<td id="S3.T2.1.1.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.6.4.1.1" class="ltx_p" style="width:93.2pt;">There is a communication burden when the shared dataset is large <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib90" title="" class="ltx_ref">2022c</a>)</cite>.</span>
</span>
</td>
<td id="S3.T2.1.1.6.5" class="ltx_td ltx_align_center ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.1.1.6.5.1" class="ltx_text" style="position:relative; bottom:-40.0pt;">—</span></td>
</tr>
<tr id="S3.T2.1.1.7" class="ltx_tr">
<td id="S3.T2.1.1.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.7.1.1.1" class="ltx_p" style="width:69.4pt;">Efficient
<br class="ltx_break">communication <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2018</a>)</cite></span>
</span>
</td>
<td id="S3.T2.1.1.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.7.2.1.1" class="ltx_p" style="width:93.2pt;">Less information is transmitted, which helps protect data privacy <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2022</a>)</cite>.</span>
</span>
</td>
<td id="S3.T2.1.1.7.3" class="ltx_td ltx_align_center ltx_align_top ltx_border_bb ltx_border_b ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.1.1.7.3.1" class="ltx_text" style="position:relative; bottom:-20.0pt;">—</span></td>
<td id="S3.T2.1.1.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T2.1.1.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.7.4.1.1" class="ltx_p" style="width:93.2pt;">Only the logits of the model are transmitted to reduce communication costs <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2018</a>)</cite>.</span>
</span>
</td>
<td id="S3.T2.1.1.7.5" class="ltx_td ltx_align_center ltx_align_top ltx_border_bb ltx_border_b ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T2.1.1.7.5.1" class="ltx_text" style="position:relative; bottom:-20.0pt;">—</span></td>
</tr>
</table>
</span></div>
</figure>
<div id="S3.SS2.SSS1.p4" class="ltx_para">
<p id="S3.SS2.SSS1.p4.1" class="ltx_p">Specifically, the approach of KD in perceiving and employing knowledge demonstrates unique potential in addressing challenges within the domain of FL:</p>
</div>
<div id="S3.SS2.SSS1.p5" class="ltx_para">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">Privacy preservation: KD does not treat the model parameters as knowledge but the logits of the model as knowledge, effectively reducing privacy concerns associated with the potential leakage of model parameters <cite class="ltx_cite ltx_citemacro_citep">(Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2020</a>; Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2021b</a>)</cite>. This methodology aligns with the principle of data minimization<cite class="ltx_cite ltx_citemacro_citep">(Biega et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">Data heterogeneity: When dealing with non-IID data, KD distinguishes itself from direct parameter aggregation by utilizing implicit knowledge from diverse client models as mutual regularization constraints. Consequently, this mechanism prevents significant deviations among client models <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2022b</a>)</cite>. This regularization constraint is more flexible than parameter aggregation, permitting clients to balance localized and global knowledge <cite class="ltx_cite ltx_citemacro_citep">(Sattler et al<span class="ltx_text">.</span>, <a href="#bib.bib124" title="" class="ltx_ref">2021b</a>)</cite>.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p">Communication efficiency: KD maximizes efficiency in knowledge exchange by only sharing the model’s outputs among various clients. Consequently, knowledge transmission improved efficacy. Compared to sharing model parameters, distributing model logits significantly reduces communication costs <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2018</a>; Ahn et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
</li>
<li id="S3.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i4.p1" class="ltx_para">
<p id="S3.I2.i4.p1.1" class="ltx_p">Personalization: As previously mentioned, KD treats the logits of the model as knowledge, which is independent of the specific model architecture <cite class="ltx_cite ltx_citemacro_citep">(Tang et al<span class="ltx_text">.</span>, <a href="#bib.bib140" title="" class="ltx_ref">2020</a>)</cite>. This characteristic empowers individual clients to design personalized model architectures tailored to their distinct requirements <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2021a</a>; Ozkara et al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2021</a>; Yamasaki and Takase, <a href="#bib.bib170" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>KD-based FL: Taxonomy</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">According to the types of model information shared among clients, federated distillation (FD) can be classified into three categories: feature-based FD, parameter-based FD, and data-based FD. Figure <a href="#S3.F5" title="Figure 5 ‣ 3.3. KD-based FL: Taxonomy ‣ 3. Knowledge-distillation-based Federated Learning ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the differences between these three types of FD. Feature-based FD shares model features, parameter-based FD shares model parameters, and data-based FD shares local compressed data. Because the model information exchanged in these three FD methods is different, they exhibit different characteristics when dealing with different challenges in FL. Table <a href="#S3.T3" title="Table 3 ‣ 3.3. KD-based FL: Taxonomy ‣ 3. Knowledge-distillation-based Federated Learning ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> compares the three FDs in dealing with challenges in FL.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2406.10861/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="126" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Comparison of the three KD-based FL methods. Feature-based FD shares model features, parameter-based FD shares model parameters, and data-based FD shares local compressed dataset.</figcaption>
</figure>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>Comparison between the three KD-based FL algorithms (feature-based, parameter-based, and data-based) on dealing with challenges in FL</figcaption>
<div id="S3.T3.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:158.8pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.7pt,9.3pt) scale(0.894160109097544,0.894160109097544) ;">
<table id="S3.T3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T3.1.1.1" class="ltx_tr">
<td id="S3.T3.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T3.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.1.1.1.1" class="ltx_p" style="width:95.4pt;"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span></span>
</span>
</td>
<td id="S3.T3.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T3.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.1.2.1.1" class="ltx_p" style="width:112.7pt;">Feature-based <cite class="ltx_cite ltx_citemacro_citep">(Li and Wang, <a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite></span>
</span>
</td>
<td id="S3.T3.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T3.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.1.3.1.1" class="ltx_p" style="width:112.7pt;">Parameter-based <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a href="#bib.bib130" title="" class="ltx_ref">2022</a>)</cite></span>
</span>
</td>
<td id="S3.T3.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T3.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.1.4.1.1" class="ltx_p" style="width:112.7pt;">Data-based <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib189" title="" class="ltx_ref">2020</a>)</cite></span>
</span>
</td>
</tr>
<tr id="S3.T3.1.1.2" class="ltx_tr">
<td id="S3.T3.1.1.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T3.1.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.2.1.1.1" class="ltx_p" style="width:95.4pt;">Privacy</span>
</span>
</td>
<td id="S3.T3.1.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T3.1.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.2.2.1.1" class="ltx_p" style="width:112.7pt;">No need to share parameters, less privacy risk <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2022</a>)</cite></span>
</span>
</td>
<td id="S3.T3.1.1.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T3.1.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.2.3.1.1" class="ltx_p" style="width:112.7pt;">Need to share parameters, which poses privacy risks <cite class="ltx_cite ltx_citemacro_citep">(Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite></span>
</span>
</td>
<td id="S3.T3.1.1.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T3.1.1.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.2.4.1.1" class="ltx_p" style="width:112.7pt;">Need to upload local data (compressed), which lead to privacy concerns</span>
</span>
</td>
</tr>
<tr id="S3.T3.1.1.3" class="ltx_tr">
<td id="S3.T3.1.1.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T3.1.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.3.1.1.1" class="ltx_p" style="width:95.4pt;">Non-IID</span>
</span>
</td>
<td id="S3.T3.1.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T3.1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.3.2.1.1" class="ltx_p" style="width:112.7pt;">Clients with heterogeneous data can learn from each other, mitigating non-IID <cite class="ltx_cite ltx_citemacro_citep">(Wen et al<span class="ltx_text">.</span>, <a href="#bib.bib157" title="" class="ltx_ref">2022</a>)</cite></span>
</span>
</td>
<td id="S3.T3.1.1.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T3.1.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.3.3.1.1" class="ltx_p" style="width:112.7pt;">Clients with heterogeneous data can learn from each other, mitigating non-IID <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2022b</a>)</cite></span>
</span>
</td>
<td id="S3.T3.1.1.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T3.1.1.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.3.4.1.1" class="ltx_p" style="width:112.7pt;">Can effectively solve non-IID <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2023a</a>)</cite></span>
</span>
</td>
</tr>
<tr id="S3.T3.1.1.4" class="ltx_tr">
<td id="S3.T3.1.1.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T3.1.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.4.1.1.1" class="ltx_p" style="width:95.4pt;">Communication</span>
</span>
</td>
<td id="S3.T3.1.1.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T3.1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.4.2.1.1" class="ltx_p" style="width:112.7pt;">No need to share parameters, low communication cost <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2018</a>)</cite></span>
</span>
</td>
<td id="S3.T3.1.1.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T3.1.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.4.3.1.1" class="ltx_p" style="width:112.7pt;">Need to share parameters, high communication cost</span>
</span>
</td>
<td id="S3.T3.1.1.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T3.1.1.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.4.4.1.1" class="ltx_p" style="width:112.7pt;">Communication costs are lower <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a href="#bib.bib134" title="" class="ltx_ref">2023</a>)</cite></span>
</span>
</td>
</tr>
<tr id="S3.T3.1.1.5" class="ltx_tr">
<td id="S3.T3.1.1.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T3.1.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.5.1.1.1" class="ltx_p" style="width:95.4pt;">Personalization</span>
</span>
</td>
<td id="S3.T3.1.1.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T3.1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.5.2.1.1" class="ltx_p" style="width:112.7pt;">Support heterogeneous models <cite class="ltx_cite ltx_citemacro_citep">(Li and Wang, <a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite></span>
</span>
</td>
<td id="S3.T3.1.1.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T3.1.1.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.5.3.1.1" class="ltx_p" style="width:112.7pt;">Heterogeneous models are generally not supported</span>
</span>
</td>
<td id="S3.T3.1.1.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T3.1.1.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.5.4.1.1" class="ltx_p" style="width:112.7pt;">Heterogeneous models are not supported</span>
</span>
</td>
</tr>
</table>
</span></div>
</figure>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1. </span>Feature-based FD (No parameter sharing)</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p"><span id="S3.SS3.SSS1.p1.1.1" class="ltx_text ltx_font_bold">Definition</span>. In feature-based FD, clients share the model features rather than the model parameters, specifically by exchanging either the logits of the model output layer <cite class="ltx_cite ltx_citemacro_citep">(Li and Wang, <a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite> or intermediate representations <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite>. Feature-based FD is a classical algorithm in KD-based FL methods and has received significant attention in recent years due to its remarkable communication efficiency. Figure <a href="#S3.F6" title="Figure 6 ‣ 3.3.1. Feature-based FD (No parameter sharing) ‣ 3.3. KD-based FL: Taxonomy ‣ 3. Knowledge-distillation-based Federated Learning ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates the typical training process of feature-based FD:</p>
</div>
<div id="S3.SS3.SSS1.p2" class="ltx_para">
<ol id="S3.I3" class="ltx_enumerate">
<li id="S3.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S3.I3.i1.p1" class="ltx_para">
<p id="S3.I3.i1.p1.1" class="ltx_p">Local logits uploading: Clients train their local models using private data and subsequently submit the logits of these models on a public dataset to the central server.</p>
</div>
</li>
<li id="S3.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S3.I3.i2.p1" class="ltx_para">
<p id="S3.I3.i2.p1.1" class="ltx_p">Global logits aggregation: The central server aggregates the logits from all clients and sends the average global logits back to each client.</p>
</div>
</li>
<li id="S3.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S3.I3.i3.p1" class="ltx_para">
<p id="S3.I3.i3.p1.1" class="ltx_p">Local model updating: Each client computes the distillation loss by utilizing the global average logits and subsequently updates their local model. Repeat the above steps until the stop condition is satisfied.</p>
</div>
</li>
</ol>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2406.10861/assets/x6.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>The typical training process of feature-based FD: ① clients receive global average logits, ② clients use global average logits and public dataset to distill into the local models, ③ clients use the distilled models to continue training on the private dataset to obtain the updated models, ④ clients use the updated models to predict on the public dataset and obtain logits, ⑤ clients upload logits to the server, ⑥ the server aggregates the logits of all clients to obtain the global average logits.</figcaption>
</figure>
<div id="S3.SS3.SSS1.p3" class="ltx_para">
<p id="S3.SS3.SSS1.p3.1" class="ltx_p"><span id="S3.SS3.SSS1.p3.1.1" class="ltx_text ltx_font_bold">Advantages</span>: Feature-based FD leverages the dark knowledge of client models to enhance the efficiency of client communication. Given the universality of this dark knowledge across different model architectures <cite class="ltx_cite ltx_citemacro_citep">(Hinton et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2015</a>)</cite>, clients can easily design heterogeneous model architectures customized to their personalized needs and effectively address challenges arising from system heterogeneity <cite class="ltx_cite ltx_citemacro_citep">(Li and Wang, <a href="#bib.bib77" title="" class="ltx_ref">2019</a>; Fang and Ye, <a href="#bib.bib30" title="" class="ltx_ref">2022</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2021a</a>; He et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2020</a>)</cite>. The effective knowledge transfer method employed in KD enables clients with heterogeneous data distributions to learn from each other, offering a novel solution for mitigating non-IID problems <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a href="#bib.bib98" title="" class="ltx_ref">2021</a>, <a href="#bib.bib98" title="" class="ltx_ref">2021</a>; Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2021c</a>; Itahara et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2021</a>)</cite>. Additionally, the model’s output logits are small, and the communication overheads <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2018</a>; Ahn et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2019</a>; Bistritz et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite> and privacy risks <cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>; Kádár and Hadházi, <a href="#bib.bib63" title="" class="ltx_ref">2022</a>; Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2022</a>)</cite> are minimal compared with conventional FL algorithms.</p>
</div>
<div id="S3.SS3.SSS1.p4" class="ltx_para">
<p id="S3.SS3.SSS1.p4.1" class="ltx_p"><span id="S3.SS3.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Disadvantages</span>: KD relies on training data for knowledge transfer, and FL lacks access to clients’ private data; alternative approaches are often necessary to acquire data for distillation, such as leveraging public datasets <cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>; Li and Wang, <a href="#bib.bib77" title="" class="ltx_ref">2019</a>; Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2022</a>; Fang and Ye, <a href="#bib.bib30" title="" class="ltx_ref">2022</a>)</cite> or employing synthetic data generators <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2021a</a>; Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib195" title="" class="ltx_ref">2021a</a>)</cite>. However, the availability of public datasets may be limited in certain scenarios, and collaborative training of generators among clients can impose additional computational. Therefore, exploring simpler and more effective methods for obtaining datasets suitable for distillation is crucial.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2. </span>Parameter-based FD (Sharing Parameters)</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p"><span id="S3.SS3.SSS2.p1.1.1" class="ltx_text ltx_font_bold">Definition</span>: In parameter-based FD, clients are still required to share model parameters, but KD is utilized to address specific issues during the client training phase <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a href="#bib.bib130" title="" class="ltx_ref">2022</a>)</cite> or server aggregation phase <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib182" title="" class="ltx_ref">2022b</a>)</cite>. This approach extends the capabilities of parameter-based algorithms by enabling more efficient model fusion and updates through the integration of KD. Figure <a href="#S3.F7" title="Figure 7 ‣ 3.3.2. Parameter-based FD (Sharing Parameters) ‣ 3.3. KD-based FL: Taxonomy ‣ 3. Knowledge-distillation-based Federated Learning ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> illustrates a typical training process of parameter-based FD:</p>
</div>
<div id="S3.SS3.SSS2.p2" class="ltx_para">
<ol id="S3.I4" class="ltx_enumerate">
<li id="S3.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S3.I4.i1.p1" class="ltx_para">
<p id="S3.I4.i1.p1.1" class="ltx_p">Local common model upload: Clients update the local common models using local private models and private datasets and then upload the common model to the central server.</p>
</div>
</li>
<li id="S3.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S3.I4.i2.p1" class="ltx_para">
<p id="S3.I4.i2.p1.1" class="ltx_p">Global common model aggregation: The central server aggregates the common models of all clients to obtain a new global common model and then sends it to each client.</p>
</div>
</li>
<li id="S3.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S3.I4.i3.p1" class="ltx_para">
<p id="S3.I4.i3.p1.1" class="ltx_p">Local common model update: Each client uses the global common model to perform KD for the local private model and then uses the updated local private model to distill into the common model.</p>
</div>
</li>
</ol>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2406.10861/assets/x7.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>The typical training process of parameter-based FD: ① clients receive the global common model, ② clients use private datasets and the global common model to distill into local private models, ③ clients use private datasets and local private models, and distill into common models, ④ clients upload the updated common models to the server, ⑤ the server aggregates the common models of all clients to obtain a new global common model.</figcaption>
</figure>
<div id="S3.SS3.SSS2.p3" class="ltx_para">
<p id="S3.SS3.SSS2.p3.1" class="ltx_p"><span id="S3.SS3.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Advantages</span>: The parameter-based FD algorithm enhances traditional parameter-based FL algorithms. By introducing KD, the parameter-based FD approach can be seamlessly integrated with existing methodologies, demonstrating effectiveness across all stages of FL <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib88" title="" class="ltx_ref">2020</a>; Chen and Chao, <a href="#bib.bib13" title="" class="ltx_ref">2020</a>; Liang et al<span class="ltx_text">.</span>, <a href="#bib.bib86" title="" class="ltx_ref">2022</a>; Shang et al<span class="ltx_text">.</span>, <a href="#bib.bib128" title="" class="ltx_ref">2022</a>; He et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2022b</a>)</cite>. Instead of simply averaging them, the server can efficiently aggregate diverse local models through dark knowledge fusion. Meanwhile, clients can leverage KD to acquire global knowledge while retaining personalized knowledge.</p>
</div>
<div id="S3.SS3.SSS2.p4" class="ltx_para">
<p id="S3.SS3.SSS2.p4.1" class="ltx_p"><span id="S3.SS3.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Disadvantages</span>: Since the clients need to share model parameters, parameter-based FD has a large communication overhead. The process of distillation increases both the computational cost for clients and servers. For instance, achieving model heterogeneity in parameter-based FD settings requires transforming local personalized model architectures into a common architecture through KD <cite class="ltx_cite ltx_citemacro_citep">(Ni et al<span class="ltx_text">.</span>, <a href="#bib.bib113" title="" class="ltx_ref">2022</a>; Ozkara et al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2021</a>)</cite>. Furthermore, parameter-based FD also demands training data for distillation, similar to feature-based FD.</p>
</div>
</section>
<section id="S3.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3. </span>Data-based FD (Based on dataset distillation)</h4>

<div id="S3.SS3.SSS3.p1" class="ltx_para">
<p id="S3.SS3.SSS3.p1.1" class="ltx_p"><span id="S3.SS3.SSS3.p1.1.1" class="ltx_text ltx_font_bold">Definition</span>: In data-based FD, clients employ dataset distillation <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib154" title="" class="ltx_ref">2018</a>)</cite> to compress their local private data, resulting in a small-scale synthetic dataset. This dataset is then transmitted to the server for centralized training <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib189" title="" class="ltx_ref">2020</a>; Song et al<span class="ltx_text">.</span>, <a href="#bib.bib134" title="" class="ltx_ref">2023</a>)</cite>. Unlike traditional FL algorithms, data-based FD does not share model information but rather shares local data (compressed) from the client. To ensure data privacy protection, data-based FD processes the local data so that the original data cannot be recovered. Figure <a href="#S3.F8" title="Figure 8 ‣ 3.3.3. Data-based FD (Based on dataset distillation) ‣ 3.3. KD-based FL: Taxonomy ‣ 3. Knowledge-distillation-based Federated Learning ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> illustrates a typical training process of data-based FD:</p>
</div>
<div id="S3.SS3.SSS3.p2" class="ltx_para">
<ol id="S3.I5" class="ltx_enumerate">
<li id="S3.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S3.I5.i1.p1" class="ltx_para">
<p id="S3.I5.i1.p1.1" class="ltx_p">Local dataset distillation: Clients convert local datasets into small-scale compressed datasets through dataset distillation.</p>
</div>
</li>
<li id="S3.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S3.I5.i2.p1" class="ltx_para">
<p id="S3.I5.i2.p1.1" class="ltx_p">Local dataset upload: Clients upload the compressed datasets to the server.</p>
</div>
</li>
<li id="S3.I5.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S3.I5.i3.p1" class="ltx_para">
<p id="S3.I5.i3.p1.1" class="ltx_p">Server centralized training: The server performs centralized training using compressed datasets uploaded by all clients.</p>
</div>
</li>
</ol>
</div>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2406.10861/assets/x8.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="122" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>The typical training process of data-based FD: ① clients compress local datasets, ② clients upload compressed datasets, and ③ the server performs centralized training.</figcaption>
</figure>
<div id="S3.SS3.SSS3.p3" class="ltx_para">
<p id="S3.SS3.SSS3.p3.1" class="ltx_p"><span id="S3.SS3.SSS3.p3.1.1" class="ltx_text ltx_font_bold">Advantages</span>: Clients transmit their compressed data directly to the server, facilitating subsequent training on the server without necessitating further client-server communication. This approach ensures efficient communication while addressing non-IID issues by replacing distributed learning with centralized learning <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib188" title="" class="ltx_ref">2022</a>; Xiong et al<span class="ltx_text">.</span>, <a href="#bib.bib165" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S3.SS3.SSS3.p4" class="ltx_para">
<p id="S3.SS3.SSS3.p4.1" class="ltx_p"><span id="S3.SS3.SSS3.p4.1.1" class="ltx_text ltx_font_bold">Disadvantages</span>: The model’s performance may decline when trained on the small-scale dataset obtained after dataset distillation <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib154" title="" class="ltx_ref">2018</a>)</cite>, owing to the inherent limitations associated with dataset distillation techniques. Moreover, direct transmission of local data raises significant privacy concerns. Consequently, further investigation is warranted to develop more efficacious and privacy-preserving methodologies for dataset distillation.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>KD-based FL: Comparison with traditional FL</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">This section compares KD-based FL and traditional FL from multiple perspectives, as presented in Table <a href="#S3.T4" title="Table 4 ‣ 3.4. KD-based FL: Comparison with traditional FL ‣ 3. Knowledge-distillation-based Federated Learning ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">The traditional FL is characterized by its simplicity in design and applicable to a wide range of client data distributions (suitable for horizontal federated learning, vertical federated learning, and federated transfer learning <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib173" title="" class="ltx_ref">2019</a>)</cite>) and scenarios (suitable for cross-device, cross-silo <cite class="ltx_cite ltx_citemacro_citep">(Karimireddy et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2021</a>)</cite>) without requiring additional public datasets. Parameter-based algorithms encompass diverse models in the parameter space, including linear models, tree models, neural network models, and recommendation algorithms <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib180" title="" class="ltx_ref">2021</a>)</cite>. However, the communication overhead associated with parameter sharing is relatively high <cite class="ltx_cite ltx_citemacro_citep">(Luping et al<span class="ltx_text">.</span>, <a href="#bib.bib96" title="" class="ltx_ref">2019</a>)</cite> and typically scales with the model’s size. Since model parameters (weights) represent abstract dataset representations and gradients help identify landmarks for these representations, sharing parameters (or gradients) can lead to privacy leaks and security attacks <cite class="ltx_cite ltx_citemacro_citep">(Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2020</a>; Lam et al<span class="ltx_text">.</span>, <a href="#bib.bib73" title="" class="ltx_ref">2021</a>)</cite>. Furthermore, parameter sharing is limited to client models with homogeneous architectures.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">KD-based FL (specifically, feature-based FD) methods demonstrate a minimal communication overhead, which solely depends on the size of logits and remains unaffected by the model size. As there is no necessity to exchange or transmit model parameters (gradients), the associated privacy risks are notably mitigated. In KD, the student model does not need to be isomorphic to the teacher model, enabling each client’s model to possess unique architectures <cite class="ltx_cite ltx_citemacro_citep">(Ozkara et al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2021</a>)</cite>. However, several relevant algorithms <cite class="ltx_cite ltx_citemacro_citep">(Itahara et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2021</a>; Sun and Lyu, <a href="#bib.bib137" title="" class="ltx_ref">2021</a>; Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite> based on distillation require clients to download a public dataset, rendering them unsuitable for cross-device scenarios with limited storage capacity. Moreover, performing distillation operations requires additional computational efforts from clients and servers. Typically, distillation-based approaches are better suited for neural network models.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>Comparison of KD-based FL and traditional FL</figcaption>
<div id="S3.T4.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:190pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-19.7pt,8.6pt) scale(0.916847518218564,0.916847518218564) ;">
<table id="S3.T4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.1.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.1.1.1.1" class="ltx_p" style="width:86.7pt;"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span></span>
</span>
</td>
<td id="S3.T4.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.1.2.1.1" class="ltx_p" style="width:173.4pt;">KD-based FL 
<br class="ltx_break">(specifically, feature-based FD)</span>
</span>
</td>
<td id="S3.T4.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.1.3.1.1" class="ltx_p" style="width:173.4pt;">Traditional FL</span>
</span>
</td>
</tr>
<tr id="S3.T4.1.1.2" class="ltx_tr">
<td id="S3.T4.1.1.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.2.1.1.1" class="ltx_p" style="width:86.7pt;">Privacy</span>
</span>
</td>
<td id="S3.T4.1.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.2.2.1.1" class="ltx_p" style="width:173.4pt;">No need to share parameters, less privacy risk <cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite></span>
</span>
</td>
<td id="S3.T4.1.1.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.2.3.1.1" class="ltx_p" style="width:173.4pt;">Need to share parameters, there are privacy risks <cite class="ltx_cite ltx_citemacro_citep">(Yin et al<span class="ltx_text">.</span>, <a href="#bib.bib178" title="" class="ltx_ref">2021</a>)</cite></span>
</span>
</td>
</tr>
<tr id="S3.T4.1.1.3" class="ltx_tr">
<td id="S3.T4.1.1.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.3.1.1.1" class="ltx_p" style="width:86.7pt;">Non-IID</span>
</span>
</td>
<td id="S3.T4.1.1.3.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.3.2.1.1" class="ltx_p" style="width:173.4pt;">Clients with heterogeneous data can learn from each other, mitigating non-IID <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib195" title="" class="ltx_ref">2021a</a>)</cite></span>
</span>
</td>
<td id="S3.T4.1.1.3.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.3.3.1.1" class="ltx_p" style="width:173.4pt;">Direct aggregation causes serious non-IID problems <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib192" title="" class="ltx_ref">2021b</a>)</cite></span>
</span>
</td>
</tr>
<tr id="S3.T4.1.1.4" class="ltx_tr">
<td id="S3.T4.1.1.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.4.1.1.1" class="ltx_p" style="width:86.7pt;">Communication</span>
</span>
</td>
<td id="S3.T4.1.1.4.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.4.2.1.1" class="ltx_p" style="width:173.4pt;">No need to share parameters, low communication cost <cite class="ltx_cite ltx_citemacro_citep">(Ahn et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2019</a>)</cite></span>
</span>
</td>
<td id="S3.T4.1.1.4.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.4.3.1.1" class="ltx_p" style="width:173.4pt;">Need to share parameters, high communication cost <cite class="ltx_cite ltx_citemacro_citep">(Konečnỳ et al<span class="ltx_text">.</span>, <a href="#bib.bib71" title="" class="ltx_ref">2016</a>)</cite></span>
</span>
</td>
</tr>
<tr id="S3.T4.1.1.5" class="ltx_tr">
<td id="S3.T4.1.1.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.5.1.1.1" class="ltx_p" style="width:86.7pt;">Personalization</span>
</span>
</td>
<td id="S3.T4.1.1.5.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.5.2.1.1" class="ltx_p" style="width:173.4pt;">Support heterogeneous models <cite class="ltx_cite ltx_citemacro_citep">(Li and Wang, <a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite></span>
</span>
</td>
<td id="S3.T4.1.1.5.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.5.3.1.1" class="ltx_p" style="width:173.4pt;">Heterogeneous models are not supported</span>
</span>
</td>
</tr>
<tr id="S3.T4.1.1.6" class="ltx_tr">
<td id="S3.T4.1.1.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.6.1.1.1" class="ltx_p" style="width:86.7pt;">Public dataset</span>
</span>
</td>
<td id="S3.T4.1.1.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.6.2.1.1" class="ltx_p" style="width:173.4pt;">Usually requires public data sets <cite class="ltx_cite ltx_citemacro_citep">(Li and Wang, <a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite>, or synthetic data <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib195" title="" class="ltx_ref">2021a</a>)</cite></span>
</span>
</td>
<td id="S3.T4.1.1.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.6.3.1.1" class="ltx_p" style="width:173.4pt;">No public dataset required</span>
</span>
</td>
</tr>
<tr id="S3.T4.1.1.7" class="ltx_tr">
<td id="S3.T4.1.1.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.7.1.1.1" class="ltx_p" style="width:86.7pt;">Computing 
<br class="ltx_break">costs</span>
</span>
</td>
<td id="S3.T4.1.1.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.7.2.1.1" class="ltx_p" style="width:173.4pt;">The computational cost is higher due to the additional knowledge distillation process required</span>
</span>
</td>
<td id="S3.T4.1.1.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T4.1.1.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.7.3.1.1" class="ltx_p" style="width:173.4pt;">Just train the model itself, no additional distillation process required</span>
</span>
</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5. </span>KD-based FL: Where should KD execute</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">For an FD system, it may be necessary to consider whether KD should be executed on the client or the server. KD involves two processes: 1) the collection of training data and 2) knowledge transfer. The collection of training data requires a certain storage capacity from the client. KD requires enough training samples to achieve good distillation results <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib90" title="" class="ltx_ref">2022c</a>)</cite>, which is unsuitable for some client devices with limited storage capacity, such as downloading tens of thousands of images to a mobile phone with small storage space for distillation. The process of knowledge transfer may lead to increased computational overhead, which could pose a burden for devices with lower computing performance, particularly when extracting knowledge from a large model with tens of millions of parameters on low-performance IoT devices. Therefore, an FD algorithm that uses KD to solve the challenges of FL should also consider where KD is executed.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">In FL, KD can be executed on 1) clients or 2) the server. Table <a href="#S3.T5" title="Table 5 ‣ 3.5. KD-based FL: Where should KD execute ‣ 3. Knowledge-distillation-based Federated Learning ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> compares the advantages and disadvantages of executing KD on the client or server. The execution of KD on clients is generally more suitable for cross-silo <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2021a</a>)</cite> scenarios due to the higher computational power of participants’ computing devices. However, in the context of cross-device <cite class="ltx_cite ltx_citemacro_citep">(ur Rehman et al<span class="ltx_text">.</span>, <a href="#bib.bib144" title="" class="ltx_ref">2021</a>)</cite> scenarios, distillation training that needs a substantial amount of additional data and is time-consuming poses challenges due to the limited resources of client devices. When considering KD execution on a server, it is imperative to assess the availability of powerful servers. In traditional FL, the server only performs simple aggregation operations without engaging in resource-intensive tasks. However, if KD is conducted on the server, complexities arise, particularly when mutual distillation among diverse client models needs to be executed <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib172" title="" class="ltx_ref">2022</a>)</cite>. This presents a significant challenge for server performance.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5. </span>Advantages and Disadvantages of executing KD on Client or Server</figcaption>
<div id="S3.T5.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:188.1pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-39.3pt,17.0pt) scale(0.846438646830971,0.846438646830971) ;">
<table id="S3.T5.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.1.1.1" class="ltx_tr">
<td id="S3.T5.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1pt;padding-bottom:1pt;" rowspan="2">
<span id="S3.T5.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.1.1.1.1" class="ltx_p" style="width:69.4pt;"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span></span>
<span id="S3.T5.1.1.1.1.1.2" class="ltx_p"></span>
</span>
</td>
<td id="S3.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;" colspan="2">KD on client</td>
<td id="S3.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;" colspan="2">KD on the server</td>
</tr>
<tr id="S3.T5.1.1.2" class="ltx_tr">
<td id="S3.T5.1.1.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T5.1.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.2.1.1.1" class="ltx_p" style="width:93.2pt;">Advantages</span>
</span>
</td>
<td id="S3.T5.1.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T5.1.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.2.2.1.1" class="ltx_p" style="width:93.2pt;">Disadvantages</span>
</span>
</td>
<td id="S3.T5.1.1.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T5.1.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.2.3.1.1" class="ltx_p" style="width:93.2pt;">Advantages</span>
</span>
</td>
<td id="S3.T5.1.1.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T5.1.1.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.2.4.1.1" class="ltx_p" style="width:93.2pt;">Disadvantages</span>
</span>
</td>
</tr>
<tr id="S3.T5.1.1.3" class="ltx_tr">
<td id="S3.T5.1.1.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T5.1.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.3.1.1.1" class="ltx_p" style="width:69.4pt;">Storage</span>
</span>
</td>
<td id="S3.T5.1.1.3.2" class="ltx_td ltx_align_center ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T5.1.1.3.2.1" class="ltx_text" style="position:relative; bottom:-10.0pt;">—</span></td>
<td id="S3.T5.1.1.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T5.1.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.3.3.1.1" class="ltx_p" style="width:93.2pt;">A burden for clients with limited storage capacity</span>
</span>
</td>
<td id="S3.T5.1.1.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T5.1.1.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.3.4.1.1" class="ltx_p" style="width:93.2pt;">Usually the server does not need to consider storage capacity issues</span>
</span>
</td>
<td id="S3.T5.1.1.3.5" class="ltx_td ltx_align_center ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T5.1.1.3.5.1" class="ltx_text" style="position:relative; bottom:-10.0pt;">—</span></td>
</tr>
<tr id="S3.T5.1.1.4" class="ltx_tr">
<td id="S3.T5.1.1.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T5.1.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.4.1.1.1" class="ltx_p" style="width:69.4pt;">Computing
<br class="ltx_break">performance</span>
</span>
</td>
<td id="S3.T5.1.1.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T5.1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.4.2.1.1" class="ltx_p" style="width:93.2pt;">Executing KD on the client can reduce the pressure on server computation <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2022b</a>)</cite></span>
</span>
</td>
<td id="S3.T5.1.1.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T5.1.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.4.3.1.1" class="ltx_p" style="width:93.2pt;">High computing performance requirements for clients</span>
</span>
</td>
<td id="S3.T5.1.1.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T5.1.1.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.4.4.1.1" class="ltx_p" style="width:93.2pt;">Executing KD on the server can reduce the pressure on client computation</span>
</span>
</td>
<td id="S3.T5.1.1.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T5.1.1.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.4.5.1.1" class="ltx_p" style="width:93.2pt;">Computation bottlenecks occur when performing mutual distillation on large numbers of client nodes <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib172" title="" class="ltx_ref">2022</a>)</cite></span>
</span>
</td>
</tr>
<tr id="S3.T5.1.1.5" class="ltx_tr">
<td id="S3.T5.1.1.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T5.1.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.5.1.1.1" class="ltx_p" style="width:69.4pt;">Communication
<br class="ltx_break">cost</span>
</span>
</td>
<td id="S3.T5.1.1.5.2" class="ltx_td ltx_align_center ltx_align_top ltx_border_bb ltx_border_b ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T5.1.1.5.2.1" class="ltx_text" style="position:relative; bottom:-20.0pt;">—</span></td>
<td id="S3.T5.1.1.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T5.1.1.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.5.3.1.1" class="ltx_p" style="width:93.2pt;">When logits on a public dataset need to be transmitted to the server, the communication cost is substantial <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib90" title="" class="ltx_ref">2022c</a>)</cite></span>
</span>
</td>
<td id="S3.T5.1.1.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S3.T5.1.1.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.5.4.1.1" class="ltx_p" style="width:93.2pt;">No need for the client to transmit logits, and the communication cost is small</span>
</span>
</td>
<td id="S3.T5.1.1.5.5" class="ltx_td ltx_align_center ltx_align_top ltx_border_bb ltx_border_b ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;"><span id="S3.T5.1.1.5.5.1" class="ltx_text" style="position:relative; bottom:-20.0pt;">—</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>How KD-based FL tackles the long-lasting challenges</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we will first analyze the relationship between various challenges in FL, including privacy protection, non-IID, communication efficiency, and personalization. In fact, these challenges are not completely independent but interdependent and related. Figure <a href="#S4.F9" title="Figure 9 ‣ 4.1. KD-based FL for privacy protection ‣ 4. How KD-based FL tackles the long-lasting challenges ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows their relationships. Among them, communication efficiency and privacy protection are positively correlated, and the less information and rounds of communication, the smaller the risk of privacy leakage <cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib79" title="" class="ltx_ref">2020a</a>; Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite>. There is also a relationship between communication efficiency and non-IID. On the one hand, the non-IID characteristics of client data distribution will slow down the convergence of the model <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib192" title="" class="ltx_ref">2021b</a>, <a href="#bib.bib195" title="" class="ltx_ref">a</a>; Mills et al<span class="ltx_text">.</span>, <a href="#bib.bib107" title="" class="ltx_ref">2022</a>)</cite>, resulting in more communication rounds and lower communication efficiency. On the other hand, increasing the number of communication rounds can potentially alleviate the non-IID problem. Model personalization allows different clients to choose personalized model architectures based on their actual needs and local hardware resources, network connections, and other conditions, which can improve communication efficiency and alleviate the negative impact of non-IID <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2021a</a>; Jing et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2023</a>; Wu et al<span class="ltx_text">.</span>, <a href="#bib.bib161" title="" class="ltx_ref">2021</a>)</cite>. In addition, model personalization makes the local data structure unknown to other clients, increasing the difficulty of implementing reconstruction attacks. Usually, while solving one problem, other problems are also mitigated. For example, mitigating the non-IID problem will speed up model convergence, reduce communication times, and thereby reduce the risk of privacy leakage.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>KD-based FL for privacy protection</h3>

<figure id="S4.F9" class="ltx_figure"><img src="/html/2406.10861/assets/x9.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="91" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>The relationships between different challenges in FL. Increased communication helps alleviate non-IID but may lead to privacy concerns. Using too many privacy-preserving technologies can lead to a communication burden. Mitigating non-IID helps reduce communication. Personalization helps alleviate communication bottlenecks, privacy concerns, and non-IID.</figcaption>
</figure>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>The overview of privacy protection in KD-based FL</h4>

<section id="S4.SS1.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Why Privacy Challenges Exist</h5>

<div id="S4.SS1.SSS1.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.Px1.p1.1" class="ltx_p">While conventional FL algorithms seem to guarantee security, recent studies <cite class="ltx_cite ltx_citemacro_citep">(Geiping et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2020</a>; Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib193" title="" class="ltx_ref">2019</a>; Orekondy et al<span class="ltx_text">.</span>, <a href="#bib.bib115" title="" class="ltx_ref">2018</a>; Xu et al<span class="ltx_text">.</span>, <a href="#bib.bib168" title="" class="ltx_ref">2020</a>)</cite> have revealed that client privacy data can be obtained simply by sharing model parameters or gradients from the client. In fact, sensitive information from the client’s training data may be contained in the local weights, aggregated weights, and final model <cite class="ltx_cite ltx_citemacro_citep">(Yin et al<span class="ltx_text">.</span>, <a href="#bib.bib178" title="" class="ltx_ref">2021</a>; Xu et al<span class="ltx_text">.</span>, <a href="#bib.bib168" title="" class="ltx_ref">2020</a>)</cite>. Despite the fact that the raw data is not directly shared, the use of homogeneous models by clients allows the data structure to be known to all participants, making it easy for adversaries to exploit model parameters and gradients to gain additional sensitive information. This can result in severe privacy risks. For instance, in literature <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib185" title="" class="ltx_ref">2020</a>)</cite>, a method was proposed to extract accurate data from gradients, which is applicable to any differentiable model trained using cross-entropy loss on one-hot labels. In literature <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a href="#bib.bib168" title="" class="ltx_ref">2020</a>)</cite>, malicious participants can adjust the training data to cause weight fluctuations in the aggregated model, and privacy information can be obtained by analyzing the trends in weight fluctuations.</p>
</div>
</section>
<section id="S4.SS1.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Conventional Methods for Privacy Challenges</h5>

<div id="S4.SS1.SSS1.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS1.Px2.p1.1" class="ltx_p">Currently, there have been numerous studies <cite class="ltx_cite ltx_citemacro_citep">(Mothukuri et al<span class="ltx_text">.</span>, <a href="#bib.bib111" title="" class="ltx_ref">2021</a>)</cite> on privacy leakage in FL. Differential privacy <cite class="ltx_cite ltx_citemacro_citep">(Dwork, <a href="#bib.bib26" title="" class="ltx_ref">2006</a>)</cite> was used in <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib82" title="" class="ltx_ref">2019</a>)</cite> to protect client data, but this resulted in reduced model performance. <cite class="ltx_cite ltx_citemacro_citep">(Truex et al<span class="ltx_text">.</span>, <a href="#bib.bib143" title="" class="ltx_ref">2019</a>)</cite> combined differential privacy with secure multiparty computation <cite class="ltx_cite ltx_citemacro_citep">(Du and Atallah, <a href="#bib.bib24" title="" class="ltx_ref">2001</a>)</cite>, enabling the federated system to mitigate the impact of noise as the number of participants increased while maintaining the expected level of trust without compromising privacy. However, this increased computation overhead. <cite class="ltx_cite ltx_citemacro_citep">(Yin et al<span class="ltx_text">.</span>, <a href="#bib.bib178" title="" class="ltx_ref">2021</a>)</cite> investigated common privacy-preserving mechanisms in FL, including encryption-based privacy-preserving FL <cite class="ltx_cite ltx_citemacro_citep">(Aono et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2017</a>; Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2020</a>)</cite>, perturbation-based privacy-preserving FL <cite class="ltx_cite ltx_citemacro_citep">(Hao et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2019</a>; Hu et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2020</a>)</cite>, anonymization-based privacy-preserving FL <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a href="#bib.bib163" title="" class="ltx_ref">2019</a>; Choudhury et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite>, and hybrid privacy-preserving FL <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a href="#bib.bib167" title="" class="ltx_ref">2019a</a>; Truex et al<span class="ltx_text">.</span>, <a href="#bib.bib143" title="" class="ltx_ref">2019</a>)</cite>. These methods are all based on parameter sharing and do not fundamentally address privacy concerns caused by the rich and redundant information carried by model parameters.</p>
</div>
</section>
<section id="S4.SS1.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Why KD Can Address Privacy Challenge</h5>

<div id="S4.SS1.SSS1.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS1.Px3.p1.1" class="ltx_p">In contrast to the rich and redundant information represented by model parameters, KD represents simplicity and efficiency. KD uses a method of sharing the logits of the model to transfer knowledge among clients. Unlike parameter sharing-based methods, in KD, different models provide regularization constraints to other models rather than the models themselves. This regularization constraint is implemented on the same dataset. Typically, in feature-based FD, clients exchange logits generated on the same dataset, greatly reducing communication <cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>; Li and Wang, <a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite>. These logits act as regularizations for the local models, guiding the optimization direction of their training processes without the need to share more details of the local models, thereby protecting the privacy of local data. In <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite>, the predictions and attentions of client models on a public dataset are uploaded to the server to train the global model, reducing communication overhead and protecting privacy. Additionally, for parameter-based FD, although clients still need to share model parameters, privacy protection can still be achieved. For example, <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a href="#bib.bib159" title="" class="ltx_ref">2022</a>)</cite> uses a common model for communication between local personalized models and the global model and transfers knowledge through KD between the local models and the common model, which enhances local data privacy protection to some extent.</p>
</div>
</section>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>KD-based FL methods for Privacy Challenge</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">Currently, some KD-based FL methods have been proposed to enhance data privacy.</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2018</a>)</cite>, each client periodically uploads the average logits of local categories (labels) on a private dataset, and the server aggregates the global average logits of each category. Then, each client utilizes the global average logits as the distillation regularization term for local loss. Furthermore, additional local training in <cite class="ltx_cite ltx_citemacro_citep">(Ahn et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2019</a>)</cite> is added based on this approach <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2018</a>)</cite>. By uploading only the average logits of local labels, this method greatly protects the privacy of client data as it does not require uploading model parameters. However, it restricts knowledge sharing among clients, which makes it challenging to ensure the effectiveness of the global model, particularly in non-IID scenarios <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S4.SS1.SSS2.p3" class="ltx_para">
<p id="S4.SS1.SSS2.p3.1" class="ltx_p">Some research works <cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>; Li and Wang, <a href="#bib.bib77" title="" class="ltx_ref">2019</a>; Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2021c</a>; Kádár and Hadházi, <a href="#bib.bib63" title="" class="ltx_ref">2022</a>)</cite> utilize public datasets to achieve more diverse knowledge transfer. Specifically, <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2021c</a>)</cite> requires each client to upload the logits of its local model on the public dataset and the cross-entropy on its private dataset. After receiving them, the server directly sends them back to each client. Then, each client uses the sum of the mean squared error (MSE) between the logits of other clients and their own logits as the distillation loss to update their local model (where the logits of each client are weighted based on their cross-entropy and similarity to the current client) and then fine-tunes on their local private data. In <cite class="ltx_cite ltx_citemacro_citep">(Kádár and Hadházi, <a href="#bib.bib63" title="" class="ltx_ref">2022</a>)</cite>, each client uploads the logits of their local model on the public dataset to the server, and the server calculates the correlation matrix among clients using these logits. Then, the server uses this matrix to calculate a regularized representation (logits) for each client, which is used for distillation among all clients.</p>
</div>
<div id="S4.SS1.SSS2.p4" class="ltx_para">
<p id="S4.SS1.SSS2.p4.1" class="ltx_p">Privacy leaks are usually caused by frequent communication between clients and servers, so many works <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2022</a>; Eren et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2022</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib181" title="" class="ltx_ref">2022a</a>; Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib189" title="" class="ltx_ref">2020</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib79" title="" class="ltx_ref">2020a</a>; Guha et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2019</a>; Kang et al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">2023</a>)</cite> use the one-shot idea to greatly reduce the number of communication rounds. For example, <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib79" title="" class="ltx_ref">2020a</a>)</cite> divides local data into many parts, each of which is further divided into multiple subsets, and trains a teacher model on each subset. These teacher models vote for the labels of unlabeled public data, obtaining the predicted labels for each public sample. A student model is then trained on the public data with predicted labels. Each client uploads its trained student models for each part to the server, which then uses these student models to predict the labels of each public sample (voting system). Finally, the model is trained on the public data with predicted labels. Although these methods still require uploading student model parameters, they improve communication efficiency and reduce the risk of privacy leakage since only one communication is needed.</p>
</div>
<div id="S4.SS1.SSS2.p5" class="ltx_para">
<p id="S4.SS1.SSS2.p5.1" class="ltx_p">In addition, commonly employed privacy protection techniques are utilized in the present study. Specifically, an unlabeled public dataset is used for distillation in <cite class="ltx_cite ltx_citemacro_citep">(Sattler et al<span class="ltx_text">.</span>, <a href="#bib.bib122" title="" class="ltx_ref">2021a</a>)</cite>. During the preparation phase, each client trains a scorer to assign scores to the distillation samples. The training of the scorer incorporates differential privacy <cite class="ltx_cite ltx_citemacro_citep">(Dwork, <a href="#bib.bib27" title="" class="ltx_ref">2008</a>)</cite> to protect client privacy. To enhance privacy protection, differential privacy is applied throughout the entire process in <cite class="ltx_cite ltx_citemacro_citep">(Hoech et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2022</a>)</cite>, which supplements the approach used in <cite class="ltx_cite ltx_citemacro_citep">(Sattler et al<span class="ltx_text">.</span>, <a href="#bib.bib122" title="" class="ltx_ref">2021a</a>)</cite>. In <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2022</a>)</cite>, quantization and noise integration are implemented on the logits shared among clients to further privacy protection.</p>
</div>
<section id="S4.SS1.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Association of Privacy Challenges with Other Challenges</h5>

<div id="S4.SS1.SSS2.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS2.Px1.p1.1" class="ltx_p">Generally speaking, there is a strong correlation between communication efficiency and privacy protection since the greater the amount and frequency of communication exchanges, the higher the likelihood of privacy leakage <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2022</a>)</cite>. Therefore, from a communication perspective, privacy protection requires minimizing the amount of information exchanged and communication frequency as much as possible. One-shot distillation was proposed in <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib189" title="" class="ltx_ref">2020</a>; Li et al<span class="ltx_text">.</span>, <a href="#bib.bib79" title="" class="ltx_ref">2020a</a>; Guha et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2019</a>; Kang et al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">2023</a>)</cite>, which limits the number of communication exchanges between clients and servers to once or just a few times, significantly reducing communication costs and privacy risks. Moreover, as illustrated in Figure <a href="#S4.F9" title="Figure 9 ‣ 4.1. KD-based FL for privacy protection ‣ 4. How KD-based FL tackles the long-lasting challenges ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, mitigating non-IID can reduce communication frequency and protect data privacy. Thus, some non-IID mitigation work in FL <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2022</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib182" title="" class="ltx_ref">2022b</a>; He et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2022a</a>; Wen et al<span class="ltx_text">.</span>, <a href="#bib.bib157" title="" class="ltx_ref">2022</a>)</cite> has also provided some level of privacy protection. Personalization of client models hides the details of local models from other participants, thereby reducing the risk of privacy leakage.</p>
</div>
</section>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3. </span>Discussion of Privacy Protection in KD-based FL</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">However, privacy concerns exist in KD-based FL, and the primary concern comes from the data needed for distillation. Since KD depends on data, an extra dataset is typically required for knowledge transfer in FD. Many studies <cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>; Li and Wang, <a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite> use an additional public dataset and assume that it has the same distribution as the clients’ local data <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib82" title="" class="ltx_ref">2019</a>)</cite>. This assumption can pose a risk of privacy leakage and lead to potential security threats. Some works <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib195" title="" class="ltx_ref">2021a</a>; Wen et al<span class="ltx_text">.</span>, <a href="#bib.bib157" title="" class="ltx_ref">2022</a>)</cite> propose data-free distillation by collaboratively training a generator on the clients to generate synthetic data, eliminating the reliance on a public dataset. However, the process of collaboratively building the generator may still require parameter sharing, leading to privacy risks <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib193" title="" class="ltx_ref">2019</a>)</cite>. Moreover, the synthetic data generated by the generator may also leak clients’ privacy data. A potentially feasible method is to ensure that the training data and original privacy data have the same knowledge properties but different representations, such as dataset distillation <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib154" title="" class="ltx_ref">2018</a>; Xiong et al<span class="ltx_text">.</span>, <a href="#bib.bib165" title="" class="ltx_ref">2023</a>; Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib189" title="" class="ltx_ref">2020</a>; Song et al<span class="ltx_text">.</span>, <a href="#bib.bib134" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S4.SS1.SSS3.p2" class="ltx_para">
<p id="S4.SS1.SSS3.p2.1" class="ltx_p">Privacy-preserving techniques commonly used in FL, such as secure multi-party computation <cite class="ltx_cite ltx_citemacro_citep">(Goldreich, <a href="#bib.bib34" title="" class="ltx_ref">1998</a>)</cite>, homomorphic encryption <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a href="#bib.bib176" title="" class="ltx_ref">2014</a>)</cite>, and differential privacy <cite class="ltx_cite ltx_citemacro_citep">(Dwork, <a href="#bib.bib26" title="" class="ltx_ref">2006</a>)</cite>, can still be applied in federated distillation. For example, in <cite class="ltx_cite ltx_citemacro_citep">(Sattler et al<span class="ltx_text">.</span>, <a href="#bib.bib122" title="" class="ltx_ref">2021a</a>)</cite>, differential privacy is used in the local preparation stage to update the local scorers and protect clients’ privacy. Building on <cite class="ltx_cite ltx_citemacro_citep">(Sattler et al<span class="ltx_text">.</span>, <a href="#bib.bib122" title="" class="ltx_ref">2021a</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Hoech et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2022</a>)</cite> extends the use of differential privacy to the entire federated training process. These privacy-preserving techniques may increase communication overhead and have an impact on model performance. In practical use, finding a balance between privacy, communication, and performance is crucial.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>KD-based FL for non-IID</h3>

<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>The overview of non-IID in KD-based FL</h4>

<section id="S4.SS2.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Why Non-IID Challenges Exist</h5>

<div id="S4.SS2.SSS1.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.Px1.p1.1" class="ltx_p">In FL, the data among different clients is usually heterogeneous, which is non-IID <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib192" title="" class="ltx_ref">2021b</a>)</cite>. This makes the optimization algorithms used in traditional machine learning and distributed learning unsuitable for FL. Each client uses local data for model updating, and the update directions of each client can be quite different from each other. Direct aggregation of the updates may not represent the globally optimal optimization direction and may even cause the global model to fail to converge. In addition, the amount of data owned by each client may vary significantly, which can result in clients with only a small amount of data having little weight in the aggregation, making the global model unfair. The non-IID in FL can be divided into two dimensions <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib192" title="" class="ltx_ref">2021b</a>)</cite>: 1) spatial dimension and 2) temporal dimension. The spatial dimension non-IID is caused by different local data distributions among clients, while the temporal dimension non-IID is caused by different data distributions between new and old samples over time. The former can lead to client drift <cite class="ltx_cite ltx_citemacro_citep">(Mendieta et al<span class="ltx_text">.</span>, <a href="#bib.bib104" title="" class="ltx_ref">2022</a>)</cite>, while the latter can lead to catastrophic forgetting <cite class="ltx_cite ltx_citemacro_citep">(Shoham et al<span class="ltx_text">.</span>, <a href="#bib.bib132" title="" class="ltx_ref">2019</a>)</cite>. Although there are conceptual differences between the two, they are both essentially due to the deviation of the optimization direction caused by the difference in data distribution.</p>
</div>
</section>
<section id="S4.SS2.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Conventional Methods for Non-IID Challenges</h5>

<div id="S4.SS2.SSS1.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS1.Px2.p1.1" class="ltx_p">Currently, many studies <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib192" title="" class="ltx_ref">2021b</a>)</cite> have been conducted on the non-IID problem in FL. <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib186" title="" class="ltx_ref">2018</a>)</cite> indicates that the performance of the global model will significantly decrease in non-IID scenarios and proposes to use a small subset of data that is globally shared to improve the performance of the global model. <cite class="ltx_cite ltx_citemacro_citep">(Sattler et al<span class="ltx_text">.</span>, <a href="#bib.bib125" title="" class="ltx_ref">2019</a>)</cite> proposes a compression framework called ”Sparse Ternary Compression (STC)”, which extends the top-k gradient sparsity compression technique to achieve a higher downlink communication compression rate in non-IID scenarios. <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib148" title="" class="ltx_ref">2020</a>)</cite> offsets the model bias caused by non-IID by intelligently selecting appropriate clients to participate in each round of training. <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib192" title="" class="ltx_ref">2021b</a>)</cite> summarizes the main approaches to handling non-IID data, including data-based approaches <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib186" title="" class="ltx_ref">2018</a>; Duan et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite>, algorithm-based approaches <cite class="ltx_cite ltx_citemacro_citep">(Fallah et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2020</a>; Arivazhagan et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2019</a>)</cite>, and system-based approaches <cite class="ltx_cite ltx_citemacro_citep">(Ghosh et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2020</a>; Yin et al<span class="ltx_text">.</span>, <a href="#bib.bib177" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS2.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Why KD Can Address Non-IID Challenge</h5>

<div id="S4.SS2.SSS1.Px3.p1" class="ltx_para">
<p id="S4.SS2.SSS1.Px3.p1.1" class="ltx_p">Based on KD, FD alleviates non-IID problems from different perspectives through a special way of knowledge transmission. The main idea of FD for addressing non-IID problems is to guide models with different optimization directions to mutually influence each other within a certain range through dark knowledge, hoping that local models can learn personalized knowledge from other clients’ models and reduce the overfitting problem <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al<span class="ltx_text">.</span>, <a href="#bib.bib112" title="" class="ltx_ref">2023</a>)</cite>. Similar to how FedProx <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib81" title="" class="ltx_ref">2020c</a>)</cite> constrains the updates of local models during the local training stage, KD-based FL imposes a constraint on local model updates by aggregating the logits of diverse client models, thereby preventing excessive deviation of local models. In addition, KD can keep the local model stable <cite class="ltx_cite ltx_citemacro_citep">(Liu and Yang, <a href="#bib.bib89" title="" class="ltx_ref">2023</a>)</cite>. Compared with parameter-based FL that directly replaces local models with the global model, which leads to the forgetting of historical personalized knowledge <cite class="ltx_cite ltx_citemacro_citep">(Jin et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2022</a>)</cite>, KD guides local model training in a more gentle way. Moreover, KD is an ideal way to implement model personalization in FL, allowing clients to share knowledge while designing heterogeneous models.</p>
</div>
</section>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>KD-based FL methods for Non-IID Challenge</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">In FL, the most direct solution for client drift <cite class="ltx_cite ltx_citemacro_citep">(Karimireddy et al<span class="ltx_text">.</span>, <a href="#bib.bib67" title="" class="ltx_ref">2020</a>)</cite> is to learn from other models, including mutual learning <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib184" title="" class="ltx_ref">2018</a>)</cite> between local models or between local and global models. In <cite class="ltx_cite ltx_citemacro_citep">(Wu and Kwon, <a href="#bib.bib158" title="" class="ltx_ref">2023</a>)</cite>, Wasserstein distance <cite class="ltx_cite ltx_citemacro_citep">(Panaretos and Zemel, <a href="#bib.bib117" title="" class="ltx_ref">2019</a>)</cite> and regularization terms are introduced into the objective function of federated knowledge distillation to reduce the distribution difference between the global model and client models. <cite class="ltx_cite ltx_citemacro_citep">(Lee and Wu, <a href="#bib.bib76" title="" class="ltx_ref">2023</a>)</cite> proposes a new model aggregation architecture to aggregate models by evaluating the effectiveness of deep neural networks (DNN) and using KD and the uncertainty quantification method of DNN. In <cite class="ltx_cite ltx_citemacro_citep">(Shang et al<span class="ltx_text">.</span>, <a href="#bib.bib127" title="" class="ltx_ref">2023</a>)</cite>, knowledge from the global model is used to guide local training to alleviate the local deviation, and local models are used to fine-tune the global model to reduce the volatility of training. In order to deal with heterogeneous tag sets in a multi-domain environment, <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib152" title="" class="ltx_ref">2023c</a>)</cite> proposes a distillation method based on an instance weighting mechanism to facilitate cross-platform transfer of knowledge. In <cite class="ltx_cite ltx_citemacro_citep">(Xing et al<span class="ltx_text">.</span>, <a href="#bib.bib164" title="" class="ltx_ref">2022</a>)</cite>, a teacher network on the client side teaches a student network using local data, and the student network is sent to the server. The server pairs the student models according to the minimum mean square distance and sends the paired models to each other as their respective teacher networks. In <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2022b</a>)</cite>, an auxiliary dataset is used to evaluate the reliability of the global model on certain labels and dynamically adjust the weight of the distillation loss for each class during local training. Specifically, the server aggregates the client models to obtain the global model, and the server calculates the accuracy of the global model for each class using the auxiliary dataset and sends the accuracy to each client. The client optimizes the loss function (hard label cross-entropy + knowledge distillation loss) using the local dataset, where the weight of the KD loss is related to the accuracy of each class. In <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib172" title="" class="ltx_ref">2022</a>)</cite>, the clients send their local models to the server, and the server conducts mutual distillation <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib184" title="" class="ltx_ref">2018</a>)</cite> (including features and logits) among all the local models using a labeled public dataset. In <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a href="#bib.bib187" title="" class="ltx_ref">2023</a>)</cite>, clients use a self-distillation method to train local models. The server generates noise samples for each client and uses them to distill other client models.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">Catastrophic forgetting <cite class="ltx_cite ltx_citemacro_citep">(Shoham et al<span class="ltx_text">.</span>, <a href="#bib.bib132" title="" class="ltx_ref">2019</a>; Ma et al<span class="ltx_text">.</span>, <a href="#bib.bib100" title="" class="ltx_ref">2022</a>; Halbe et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2023</a>)</cite> in FL mainly manifests in two aspects: 1) inter-task forgetting and 2) intra-task forgetting. The solutions for both are similar, which is to learn from the old model through KD. In reference <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a href="#bib.bib99" title="" class="ltx_ref">[n. d.]</a>)</cite>, KD is performed on both the client and server sides. On the client side, an unlabeled proxy dataset is used to review the old model, and on the server side, an unlabeled proxy dataset is used to address the non-IID problem among clients. Specifically, a subset of clients is used to learn new tasks, while another subset of clients uses local unlabeled proxy datasets for KD to review old tasks. Client-side distillation is mainly used to address inter-task forgetting, while the server distributes the proxy dataset equally to all clients to obtain soft labels, and then the global model uses these soft labels for KD to address intra-task forgetting. The FL system may experience catastrophic forgetting due to the emergence of new class samples over time. <cite class="ltx_cite ltx_citemacro_citep">(Tang et al<span class="ltx_text">.</span>, <a href="#bib.bib139" title="" class="ltx_ref">2023</a>)</cite> proposes a method based on relational KD. The local model mines high-quality global knowledge from higher dimensions in the local training stage to better retain global knowledge and avoid forgetting. Reference <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2022</a>)</cite> proposes to learn a class-incremental model from a global and local perspective to mitigate catastrophic forgetting. To address local forgetting, the paper designs class-aware gradient compensation loss and class semantic relation distillation loss to balance the forgetting of old classes and extract consistent inter-class relations across tasks. To address global forgetting, the paper proposes a proxy server that assists in local relation extraction by selecting the best old global model.</p>
</div>
<div id="S4.SS2.SSS2.p3" class="ltx_para">
<p id="S4.SS2.SSS2.p3.1" class="ltx_p">The performance of the teacher model may be poor during the initial stage, which could potentially misguide the student model. Therefore, <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2022b</a>)</cite> uses an auxiliary dataset to evaluate the credibility of the global model. In reference <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2022a</a>)</cite>, this selective learning is further developed in two aspects: 1) the local sample level, i.e., the accuracy of the global model on a certain local sample, and 2) the class level, i.e., the accuracy of the global model on a certain class. Specifically, the server evaluates the global model using the labeled auxiliary dataset to obtain a credibility matrix and sends it to each client. The client uses its local dataset to distill its local model (with the global model as the teacher). During distillation, the local model selectively learns by using a vector obtained from the credibility matrix, which is related to the current sample and relevant classes. The MSE loss function is obtained using this vector matrix as the distillation loss term, and cross-entropy is used as the loss term for the true labels. The two terms are added together to obtain the loss function, and the local model is updated through backpropagation and uploaded to the server.</p>
</div>
<div id="S4.SS2.SSS2.p4" class="ltx_para">
<p id="S4.SS2.SSS2.p4.1" class="ltx_p">Non-IID data may cause performance differences in the global model across different clients, leading to fairness issues <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2021b</a>; Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib190" title="" class="ltx_ref">2021</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib155" title="" class="ltx_ref">2021</a>)</cite>. One approach to address this problem is to design personalized models for local data that participate in global model training through KD while maintaining performance on local data. In literature <cite class="ltx_cite ltx_citemacro_citep">(Ni et al<span class="ltx_text">.</span>, <a href="#bib.bib113" title="" class="ltx_ref">2022</a>)</cite>, the global common model is transformed into local personalized models through KD, which is then trained and transformed back to global common models. The personalized models can be uniquely designed based on the client’s data distribution and device performance, and the differences between different models can be compensated through deep communication between the models. In literature <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a href="#bib.bib69" title="" class="ltx_ref">2022</a>)</cite>, a novel hierarchical hybrid network is designed, which separates the local and global models into several blocks and connects them at different positions to form multiple paths, similar to a special type of intermediate layer distillation.</p>
</div>
<div id="S4.SS2.SSS2.p5" class="ltx_para">
<p id="S4.SS2.SSS2.p5.1" class="ltx_p">Existing FD methods generally do not take into account the diversity of all local models, which can lead to performance degradation of the aggregate model when some local models know little about the sample. In <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib149" title="" class="ltx_ref">2023a</a>)</cite>, the local data of each client is regarded as a specific domain, and a novel domain-knowledge-aware FD method is designed to identify the importance of local models to distillation samples and optimize soft predictions from different models. <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2023a</a>)</cite> uses entropy to determine the prediction confidence of each local model and select the most confident local model as a teacher to guide the learning of the global model.</p>
</div>
<section id="S4.SS2.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Association of Non-IID Challenges with Other Challenges</h5>

<div id="S4.SS2.SSS2.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS2.Px1.p1.1" class="ltx_p">As mentioned earlier, non-IID can slow down the convergence of the global model, resulting in more communication rounds between clients and the server. However, one feasible way to alleviate non-IID is to increase communication so that heterogeneous clients can have sufficient communication with each other. Of course, this communication is not just about increasing the number of communication rounds but may also involve mutual collaboration between models. In addition, the personalization of models allows clients to design different models based on their local data, which greatly alleviates non-IID problems.</p>
</div>
</section>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3. </span>Discussion of Non-IID in KD-based FL</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">Obviously, it is unrealistic to expect a teacher to teach well in an unfamiliar field. The training data used for KD should be relevant to the local data of each client to achieve better results. Many studies <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2022</a>; Mo et al<span class="ltx_text">.</span>, <a href="#bib.bib109" title="" class="ltx_ref">2022</a>; Kádár and Hadházi, <a href="#bib.bib63" title="" class="ltx_ref">2022</a>)</cite> assume the existence of a common dataset, which is not realistic in many practical scenarios. Even if a common dataset is available, domain shift <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a href="#bib.bib95" title="" class="ltx_ref">2019</a>; Sankaranarayanan et al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2018</a>)</cite> may still exist. Therefore, it is a worthwhile direction to study how to transfer knowledge from one domain to another for effective domain adaptation. Some progress has been made in this regard <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib152" title="" class="ltx_ref">2023c</a>, <a href="#bib.bib149" title="" class="ltx_ref">a</a>; Su et al<span class="ltx_text">.</span>, <a href="#bib.bib136" title="" class="ltx_ref">2022</a>)</cite>. In addition, the compatibility of knowledge between different personalized models is a problem that needs to be carefully considered. Many studies <cite class="ltx_cite ltx_citemacro_citep">(Mi et al<span class="ltx_text">.</span>, <a href="#bib.bib106" title="" class="ltx_ref">2021</a>; Ozkara et al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2021</a>; Bistritz et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2020</a>; Li and Wang, <a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite> use KD to achieve client model personalization based on the assumption that although the architectures of different models are different, the representation of knowledge is the same. This assumption seems reasonable in many FL algorithms, but it lacks more detailed theoretical analysis and proof.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>KD-based FL for communication efficiency</h3>

<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1. </span>The overview of communication efficiency in KD-based FL</h4>

<section id="S4.SS3.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Why Communication Efficiency Challenges Exist</h5>

<div id="S4.SS3.SSS1.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.Px1.p1.1" class="ltx_p">To facilitate collaboration among clients, FL requires periodic sharing of model parameters, which can lead to communication bottleneck problems. Communication efficiency is determined by two factors: the amount of information transmitted in each communication round and the number of communication rounds. Unfortunately, the performance of conventional parameter-based FL is inadequate in both aspects. Firstly, sharing complete model parameters (or gradients) in each communication round can result in a large amount of data transmission, which is positively correlated with the size of the model architecture. This can make FL infeasible when using large model architectures. Secondly, the non-IID nature of client data in FL can make it difficult for the global model to converge, which can result in more communication rounds. The decline in communication efficiency can directly lead to a significant prolong of the training time of the global model or even failure. In cross-device scenarios, the frequent joining and exiting of a large number of clients can further exacerbate the situation, leading to additional challenges for FL.</p>
</div>
</section>
<section id="S4.SS3.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Conventional Methods for Communication Efficiency Challenges</h5>

<div id="S4.SS3.SSS1.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS1.Px2.p1.1" class="ltx_p">Currently, numerous studies <cite class="ltx_cite ltx_citemacro_citep">(Shahid et al<span class="ltx_text">.</span>, <a href="#bib.bib126" title="" class="ltx_ref">2021</a>)</cite> have investigated communication efficiency issues in FL. Several methods have been proposed to reduce uplink communication costs, such as structured updates and sketched updates <cite class="ltx_cite ltx_citemacro_citep">(Konečnỳ et al<span class="ltx_text">.</span>, <a href="#bib.bib71" title="" class="ltx_ref">2016</a>)</cite>. Additionally, <cite class="ltx_cite ltx_citemacro_citep">(Luping et al<span class="ltx_text">.</span>, <a href="#bib.bib96" title="" class="ltx_ref">2019</a>)</cite> introduced an orthogonal approach to identify irrelevant client updates, which can help to reduce communication costs. In <cite class="ltx_cite ltx_citemacro_citep">(Sattler et al<span class="ltx_text">.</span>, <a href="#bib.bib125" title="" class="ltx_ref">2019</a>)</cite>, a sparse ternary compression framework was proposed to decrease the amount of data transferred in downlink communication. Another approach involves designing a probabilistic device selection scheme <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>, which facilitates the selection of high-quality clients in each communication round, as well as using quantization techniques to reduce the amount of communicated data. In <cite class="ltx_cite ltx_citemacro_citep">(Lim et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2020</a>)</cite>, common approaches for reducing communication costs were summarized. Although these methods have somewhat reduced the communication costs of downloading or uploading from clients, parameter-sharing-based algorithms generally result in communication costs that are positively correlated with the model size.</p>
</div>
</section>
<section id="S4.SS3.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Why KD Can Address Communication Efficiency Challenge</h5>

<div id="S4.SS3.SSS1.Px3.p1" class="ltx_para">
<p id="S4.SS3.SSS1.Px3.p1.1" class="ltx_p">Currently, traditional parameter-sharing FL algorithms require frequent exchange of model parameters, leading to communication bottlenecks. KD uses the model’s output instead of the model’s parameters as the medium for knowledge propagation, facilitating communication between clients. Compared to model parameters, the model’s output is much smaller, greatly reducing communication costs. Classic feature-based FD is mainly used to reduce the amount of data transmitted in a single communication round, not the frequency of communication rounds. Therefore, some one-shot methods <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">2023</a>; Eren et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2022</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib181" title="" class="ltx_ref">2022a</a>; Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2022</a>)</cite> have been proposed to reduce the frequency of communication rounds. Additionally, for parameter-based FD, although model parameters still need to be exchanged, communication costs can be greatly reduced because a smaller model can be obtained based on KD for communication between clients <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a href="#bib.bib174" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2. </span>KD-based FL methods for Communication Efficiency</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">FD was originally proposed to address the problem of the communication bottleneck <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2018</a>)</cite>. Compared with FL algorithms based on parameter sharing, FD greatly reduces communication costs <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib171" title="" class="ltx_ref">2023</a>)</cite>. As mentioned earlier, reducing communication costs involves two aspects: 1) reducing the amount of data transmitted in a single communication, and 2) reducing the rounds of communications. We will discuss these two aspects below.</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.p2.1" class="ltx_p">First, FD can greatly reduce the amount of data transmitted in a single communication. <cite class="ltx_cite ltx_citemacro_citep">(Tanghatari et al<span class="ltx_text">.</span>, <a href="#bib.bib141" title="" class="ltx_ref">2023</a>)</cite> uses KD to exchange the knowledge learned by the server and the edge device. In order to reduce the communication overhead between the server and the edge device, select the most valuable data to transfer. In <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2018</a>)</cite>, only the average logits of each client’s local classes are uploaded, which is negligible compared to the model parameters. Similarly, in <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2023c</a>)</cite>, clients compute the mean representations of the data classes in their local training dataset and the corresponding mean soft prediction, which is sent to the server for aggregation through differential privacy. However, these methods perform poorly in non-IID settings, such as scenarios where the same label has different features. Therefore, a more common method is to use a public dataset <cite class="ltx_cite ltx_citemacro_citep">(Li and Wang, <a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite> or synthetic data generated by a generator <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib195" title="" class="ltx_ref">2021a</a>)</cite>. In <cite class="ltx_cite ltx_citemacro_citep">(Sattler et al<span class="ltx_text">.</span>, <a href="#bib.bib124" title="" class="ltx_ref">2021b</a>)</cite>, clients use a public dataset for distillation to obtain a distilled model and then use it to train local private data to obtain a local model. Then, the local model predicts soft labels on the public dataset, and the soft labels are quantized, encoded, and sent to the server. The server aggregates the soft labels sent by all clients to obtain the global soft labels and then distills and predicts the global model to obtain soft labels, which are quantized, encoded, and sent to clients. However, regardless of whether the distillation dataset is a public dataset or synthetic data, each client needs to share the output of its local model on the distillation dataset. When dealing with a large distillation dataset, the communication bottlenecks may still be an issue. In <cite class="ltx_cite ltx_citemacro_citep">(Mo et al<span class="ltx_text">.</span>, <a href="#bib.bib109" title="" class="ltx_ref">2022</a>)</cite>, clients use a public dataset for the distillation of local models and then train on local data. Then, they predict soft labels on the public dataset using the local model, as well as compressing the local control variables, which are sent to the server. The server decompresses the compressed data, aggregates soft labels and control variables, and then distills and predicts on the public dataset. Finally, the new soft labels and control variables are compressed and sent to the clients.</p>
</div>
<div id="S4.SS3.SSS2.p3" class="ltx_para">
<p id="S4.SS3.SSS2.p3.1" class="ltx_p">In addition, some works focus on reducing the number of communications. In <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib181" title="" class="ltx_ref">2022a</a>)</cite>, clients upload their local models to the server, which trains a generator using the integrated models of all clients. The generator then generates synthetic data, and the server uses the average logits of each client’s model on the synthetic data (distillation loss) and the global model’s cross-entropy loss on the synthetic data to train the model.</p>
</div>
<div id="S4.SS3.SSS2.p4" class="ltx_para">
<p id="S4.SS3.SSS2.p4.1" class="ltx_p">In recent years, a federated learning method based on dataset distillation <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib154" title="" class="ltx_ref">2018</a>)</cite> has been proposed to address communication efficiency issues. Dataset distillation can compress large datasets into smaller ones while maintaining similar model performance. Inspired by dataset distillation, <cite class="ltx_cite ltx_citemacro_citep">(Xiong et al<span class="ltx_text">.</span>, <a href="#bib.bib165" title="" class="ltx_ref">2023</a>; Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib189" title="" class="ltx_ref">2020</a>)</cite> generate synthetic data for each class based on the parameters distributed by the server and send it to the server. The server uses the synthetic data to update the global model.</p>
</div>
<section id="S4.SS3.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Association of Communication Efficiency Challenges with Other Challenges</h5>

<div id="S4.SS3.SSS2.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS2.Px1.p1.1" class="ltx_p">FL involves collaboration among multiple clients, which typically relies on network communication and is inherently unstable and uncertain. In traditional centralized training and distributed training in data centers, communication has never been a problem. However, in the context of FL, communication itself can generate many issues that can directly or indirectly impact other aspects. In sections <a href="#S4.SS1" title="4.1. KD-based FL for privacy protection ‣ 4. How KD-based FL tackles the long-lasting challenges ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> and <a href="#S4.SS2" title="4.2. KD-based FL for non-IID ‣ 4. How KD-based FL tackles the long-lasting challenges ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>, we respectively discuss the correlation between communication efficiency and privacy protection and non-IID. This correlation can have a ripple effect on the design of FL algorithms. For example, non-IID leads to more communication, which in turn can lead to privacy risks. To address privacy concerns, common cryptographic techniques may be necessary to process communication, which can result in higher communication costs. Moreover, as shown in Figure <a href="#S4.F9" title="Figure 9 ‣ 4.1. KD-based FL for privacy protection ‣ 4. How KD-based FL tackles the long-lasting challenges ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, personalization seems to provide a new solution for heterogeneous client communication. It allows slower clients to choose smaller model architectures, which can improve communication speeds.</p>
</div>
</section>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3. </span>Discussion of Communication Efficiency in KD-based FL</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">FD only needs to share logits without sharing model parameters, which seems promising and can greatly reduce communication costs. However, KD relies on data. FD algorithms require clients to share logits for all samples in a shared dataset, and usually, the larger the dataset, the better the performance <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib90" title="" class="ltx_ref">2022c</a>)</cite>. This can lead to additional communication costs, although these costs are much smaller than directly sharing model parameters. In some scenarios where a common dataset is not available, clients need to collaboratively train a generator to produce synthetic data for distillation, which incurs more computation and communication costs.</p>
</div>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>KD-based FL for personalization</h3>

<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1. </span>The overview of personalization in KD-based FL</h4>

<section id="S4.SS4.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Why Personalization Challenges Exist</h5>

<div id="S4.SS4.SSS1.Px1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.Px1.p1.1" class="ltx_p">In a FL system, clients provide the data, so their own needs should be fully respected. However, in parameter-based FL, clients have difficulty expressing their individual needs because all participants are required to train the same model, even if this model may not be suitable for some participants. This can lead to many problems in practice. On the one hand, participants are likely to refuse to join the FL system because it cannot satisfy their personalized needs, resulting in a smaller number of participants in the FL system and less available training data, which affects the effectiveness of model training. On the other hand, the non-IID problem in FL means that homogeneous models may not have performance advantages for some participants, resulting in fairness issues. In addition, for some clients with lower computational performance, it is difficult to train large federated models, while for some devices with limited bandwidth, it is difficult to bear heavy communication costs. Therefore, personalization is essential for the practical implementation of federated systems. Personalization can be reflected in two aspects: 1) personalization of model architecture and 2) personalization of model size. Clients can design customized model architectures <cite class="ltx_cite ltx_citemacro_citep">(Gad and Fadlullah, <a href="#bib.bib31" title="" class="ltx_ref">2023</a>)</cite> based on the specific situations of their local data and the target application fields and can also design models of different sizes based on the computational performance and bandwidth resources of their devices.</p>
</div>
</section>
<section id="S4.SS4.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Conventional Methods for Personalization Challenges</h5>

<div id="S4.SS4.SSS1.Px2.p1" class="ltx_para">
<p id="S4.SS4.SSS1.Px2.p1.1" class="ltx_p">Currently, there are many studies <cite class="ltx_cite ltx_citemacro_citep">(Kulkarni et al<span class="ltx_text">.</span>, <a href="#bib.bib72" title="" class="ltx_ref">2020</a>)</cite> on personalization in FL. In <cite class="ltx_cite ltx_citemacro_citep">(Arivazhagan et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2019</a>)</cite>, a personalized layer method is proposed to alleviate the impact of non-IID. In <cite class="ltx_cite ltx_citemacro_citep">(Deng et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite>, an adaptive personalized FL is proposed, which combines global models and local models into a joint prediction model with an adaptive weight. In <cite class="ltx_cite ltx_citemacro_citep">(Mansour et al<span class="ltx_text">.</span>, <a href="#bib.bib101" title="" class="ltx_ref">2020</a>)</cite>, three methods for achieving personalization are proposed, including user clustering, data interpolation, and model interpolation. <cite class="ltx_cite ltx_citemacro_citep">(Kulkarni et al<span class="ltx_text">.</span>, <a href="#bib.bib72" title="" class="ltx_ref">2020</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Tan et al<span class="ltx_text">.</span>, <a href="#bib.bib138" title="" class="ltx_ref">2022</a>)</cite> summarize commonly used methods for model personalization.</p>
</div>
</section>
<section id="S4.SS4.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Why KD Can Address Personalization Challenge</h5>

<div id="S4.SS4.SSS1.Px3.p1" class="ltx_para">
<p id="S4.SS4.SSS1.Px3.p1.1" class="ltx_p">KD is model architecture agnostic, which means that the student model can use an architecture that is completely different from the teacher model. This property provides a natural advantage for achieving model personalization. In feature-based FD, clients can design personalized model architectures according to their own needs, as they only need to share logits that are independent of the model architecture. For parameter-based FD, KD can facilitate knowledge transfer between different model architectures. In this context, KD acts like a converter between models, transforming one into another. Therefore, local models can still be customized, and their knowledge can be transferred to the globally shared model architecture through KD for global aggregation.</p>
</div>
</section>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2. </span>KD-based FL methods for Personalization</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">Traditional FL algorithms require all clients to use homogeneous models, while FD allows clients to design heterogeneous models based on their actual needs to achieve personalization. The personalization of the model is mainly reflected in the size and architecture of the model. In conventional FL, clients may be passively or actively excluded due to the limitations of the global model’s scalability for local training and deployment or its inability to meet specific business requirements. Different FL settings focus on different personalization, with cross-device scenarios focusing on model size personalization and cross-silo scenarios focusing on model architecture personalization. FD uses the model agnosticism of KD to enable models of different sizes and architectures to exchange knowledge with each other.</p>
</div>
<div id="S4.SS4.SSS2.p2" class="ltx_para">
<p id="S4.SS4.SSS2.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citep">(Xu and Fan, <a href="#bib.bib169" title="" class="ltx_ref">2023</a>)</cite> propose a serverless framework that uses convolutional neural networks (CNN) to accumulate common knowledge and KD to transfer it. Missing common knowledge is cycled through each federation to provide a personalized model for each group. <cite class="ltx_cite ltx_citemacro_citep">(Su et al<span class="ltx_text">.</span>, <a href="#bib.bib135" title="" class="ltx_ref">2023</a>)</cite> introduces a dynamic selection algorithm that utilizes KD and weight correction to reduce the impact of model heterogeneity. <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib153" title="" class="ltx_ref">2023b</a>)</cite> proposes an effective federated graph learning method based on KD. Each client trains its own local model through KD. In <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib194" title="" class="ltx_ref">2022</a>)</cite>, a progressive self-distillation <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib183" title="" class="ltx_ref">2019</a>)</cite> method is proposed for system heterogeneity and unstable network connections. Simply put, this method divides a large model by column, and each client can download a certain proportion of model parameters based on local network conditions and then supplement the missing parts with local models. Then, the complete local model is updated through progressive self-distillation. Then, based on the network condition, a certain proportion of local model parameters are uploaded. The server aggregates all the incomplete models uploaded by all clients to obtain the global model and broadcasts it to each client. In <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite>, a decentralized FL system is designed using cyclic distillation to achieve knowledge accumulation. This method includes two stages: the public knowledge accumulation stage and the personalized stage. Both stages use a circular P2P architecture. In the public knowledge accumulation stage, clients decide whether to use distillation (with the previous client as the teacher) or directly use the previous client’s model as the initialization of their local model based on the comparison of the current local model’s accuracy and threshold. In the personalized stage, clients use the model and the comparison of the current local model’s accuracy and threshold sent by the previous client to determine the weight of the distillation loss term. In <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2022b</a>)</cite>, models of different sizes are assigned to clients with different computing capabilities, with larger models being assigned to stronger clients and smaller models being assigned to weaker clients. A momentum knowledge distillation method is also proposed to better transfer knowledge from the large models of stronger clients to the small models of weaker clients. In <cite class="ltx_cite ltx_citemacro_citep">(Ozkara et al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2021</a>; Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2023b</a>)</cite>, clients first use KD to learn from the global model locally to train a personalized model. Then, using KD, the personalized model is transformed into a globally common model, which is uploaded to the server for aggregation <cite class="ltx_cite ltx_citemacro_citep">(Ozkara et al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2021</a>)</cite> or to the other clients for distillation <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2023b</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib92" title="" class="ltx_ref">2022a</a>)</cite> proposes an adaptive quantization scheme for integrated distillation. This method divides clients into different clusters, and local models in the same cluster are isomorphic and have different quantization levels. Model aggregation includes single cluster model aggregation and cluster ensemble distillation loss aggregation.</p>
</div>
<section id="S4.SS4.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Association of Personalization Challenges with Other Challenges</h5>

<div id="S4.SS4.SSS2.Px1.p1" class="ltx_para">
<p id="S4.SS4.SSS2.Px1.p1.1" class="ltx_p">Personalization offers a new approach for solving communication bottlenecks, non-IID, and privacy leakage in FL, as introduced in sections <a href="#S4.SS1.SSS2.Px1" title="Association of Privacy Challenges with Other Challenges ‣ 4.1.2. KD-based FL methods for Privacy Challenge ‣ 4.1. KD-based FL for privacy protection ‣ 4. How KD-based FL tackles the long-lasting challenges ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1.2</span></a>, <a href="#S4.SS2.SSS2.Px1" title="Association of Non-IID Challenges with Other Challenges ‣ 4.2.2. KD-based FL methods for Non-IID Challenge ‣ 4.2. KD-based FL for non-IID ‣ 4. How KD-based FL tackles the long-lasting challenges ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>, and <a href="#S4.SS3.SSS2.Px1" title="Association of Communication Efficiency Challenges with Other Challenges ‣ 4.3.2. KD-based FL methods for Communication Efficiency ‣ 4.3. KD-based FL for communication efficiency ‣ 4. How KD-based FL tackles the long-lasting challenges ‣ Knowledge Distillation in Federated Learning: a Survey on Long Lasting Challenges and New Solutions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>, respectively. In fact, personalization in FL should not be regarded as a challenge but rather as an evolutionary form of FL, namely, an FL that respects differences. A concept related to personalization is system heterogeneity <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib81" title="" class="ltx_ref">2020c</a>)</cite>, which should be considered a challenge in FL. The system heterogeneity challenge refers to the difficulties in designing and implementing an FL system due to the heterogeneity of client devices in terms of computing power and bandwidth resources. Related concepts are system compatibility and scalability <cite class="ltx_cite ltx_citemacro_citep">(Hamood et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite>. In some work <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2023b</a>; Mohammed et al<span class="ltx_text">.</span>, <a href="#bib.bib110" title="" class="ltx_ref">2023</a>; Lyu et al<span class="ltx_text">.</span>, <a href="#bib.bib97" title="" class="ltx_ref">2023</a>)</cite>, KD has been used to solve system heterogeneous problems. For example, <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2023b</a>)</cite> proposes an FL optimization problem based on KD that considers dynamic local resources. This method is used to avoid occupying expensive network bandwidth or bringing a heavy burden on the network. However, the implementation of personalized FL has shifted the focus away from system heterogeneity. Personalization can not only address this problem but also reflect the personalized expectations of clients for the federated system at a higher level rather than just enabling low-performance devices to participate in federated training.</p>
</div>
</section>
</section>
<section id="S4.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3. </span>Discussion of Personalization in KD-based FL</h4>

<div id="S4.SS4.SSS3.p1" class="ltx_para">
<p id="S4.SS4.SSS3.p1.1" class="ltx_p">KD is model-agnostic, but certain drawbacks arise when employing it for personalized FL, which are inherent to the intrinsic characteristics of KD itself. Since KD is data-dependent, achieving personalization requires the sharing of a common dataset globally, which poses a challenge for storage-limited clients. For instance, it is impractical to download a public dataset containing tens of thousands of images into a user’s phone for distillation. Moreover, collecting a suitable public dataset is also a challenge, and in practical scenarios, such a dataset may not be available. Additionally, KD increases additional computational costs. Specifically, for parameter-based FD, a KD is required before uploading the local model after receiving the global model, significantly increasing the computational cost for clients and prolonging the training time of the global model.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Future direction</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Recently, KD-based FL has gained popularity as an FL paradigm, and numerous researchers have introduced KD into FL to tackle various challenges. However, KD-based FL faces several challenges. Here are some potential challenges and research directions:</p>
<ol id="S5.I1" class="ltx_enumerate">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">In FL, there is no pre-trained high-performance teacher model <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib147" title="" class="ltx_ref">2022b</a>; Shao et al<span class="ltx_text">.</span>, <a href="#bib.bib129" title="" class="ltx_ref">2023</a>)</cite>, and the performance of the teacher model stabilizes gradually during the federated training process. Therefore, the teacher model may mislead the student model. To address this issue, <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2022b</a>)</cite> proposes using an auxiliary dataset to evaluate the credibility of the teacher model on specific labels and dynamically changing the weight of the distillation loss on each category during local training. However, this approach requires a labeled public dataset, which limits its practical use. Thus, many studies <cite class="ltx_cite ltx_citemacro_citep">(Fang and Ye, <a href="#bib.bib30" title="" class="ltx_ref">2022</a>; Zhuang et al<span class="ltx_text">.</span>, <a href="#bib.bib196" title="" class="ltx_ref">2023</a>; Itahara et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2021</a>)</cite> use an unlabeled public dataset for distillation, which makes it difficult to evaluate the credibility of the teacher model. In fact, the non-iid characteristics in FL make this problem even more complex, as the teacher model may have better guidance for some clients but may seriously mislead other heterogeneous clients. Therefore, a potential research direction is to design a mechanism to identify teacher model misguidance in a timely manner and control it within a reasonable range.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">In a typical KD-based FL, each client uploads the logits of the local model output on the public dataset to the server for aggregation to obtain the globally averaged logits, guiding the subsequent training of the local model. This approach is based on the assumption that, although the data from different clients is heterogeneous, the knowledge distilled from this data is homogeneous. Although this assumption has achieved good experimental results in many studies <cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2019</a>; Li and Wang, <a href="#bib.bib77" title="" class="ltx_ref">2019</a>)</cite>, it lacks rigorous theoretical analysis to prove the compatibility of knowledge between different client models and determine the upper bound of this compatibility. Additionally, in non-iid scenarios, the difference between client models is significant, and directly aggregating client logits may lead to global knowledge deviation. Moreover, how to set appropriate hyperparameters in KD to improve learning effects is also an important research direction <cite class="ltx_cite ltx_citemacro_citep">(Alballa and Canini, <a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p">Many KD-based FL algorithms <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib181" title="" class="ltx_ref">2022a</a>, <a href="#bib.bib182" title="" class="ltx_ref">b</a>; Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib195" title="" class="ltx_ref">2021a</a>)</cite> have proposed data-free distillation to address the challenge of collecting public datasets. The main idea of these algorithms is to use a generator to generate synthetic data for distillation. However, this approach assumes that the data generated by the generator will not expose client privacy, which may not always be the case <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a href="#bib.bib166" title="" class="ltx_ref">2019b</a>)</cite>. Therefore, addressing the risk of client privacy brought by the generator is a potential research direction. In this regard, specific privacy protection methods and detailed theoretical analysis are needed to determine the extent to which generator-based methods leak privacy and how to balance them with model performance.</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p id="S5.I1.i4.p1.1" class="ltx_p">KD relies on the dataset, and generally, within a certain range, the more data, the better the distillation effect <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib90" title="" class="ltx_ref">2022c</a>)</cite>. However, adding more data will lead to larger communication costs. Some work <cite class="ltx_cite ltx_citemacro_citep">(Mo et al<span class="ltx_text">.</span>, <a href="#bib.bib109" title="" class="ltx_ref">2022</a>; Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2022</a>; Sattler et al<span class="ltx_text">.</span>, <a href="#bib.bib124" title="" class="ltx_ref">2021b</a>)</cite> have explored various methods to reduce communication costs, including quantization and coding. However, these methods still make communication costs proportional to the dataset size. When a large dataset is needed to achieve better distillation results, it still poses a challenge to communication bottlenecks. Therefore, finding a balance between the amount of the public dataset and transmission problems is a worthwhile research direction. Among them, dataset distillation is a potential solution that can compress a massive dataset into a small dataset with equivalent performance. This is very suitable for the FL scenario.</p>
</div>
</li>
<li id="S5.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(5)</span> 
<div id="S5.I1.i5.p1" class="ltx_para">
<p id="S5.I1.i5.p1.1" class="ltx_p">Currently, KD-based FL is mostly used in horizontal federated learning scenarios and is rarely used to solve vertical federated learning <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib173" title="" class="ltx_ref">2019</a>)</cite>. However, cooperation between enterprises with different feature data is a common requirement in the cross-silo scenario. If knowledge is transferred between clients with different features through exploration in the field of model enhancement using KD, it will solve some of the challenges in model training under vertical federated learning. In this regard, some studies <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib150" title="" class="ltx_ref">2022a</a>; Wan et al<span class="ltx_text">.</span>, <a href="#bib.bib146" title="" class="ltx_ref">2023</a>; Huang et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2023c</a>)</cite> have been conducted.</p>
</div>
</li>
<li id="S5.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(6)</span> 
<div id="S5.I1.i6.p1" class="ltx_para">
<p id="S5.I1.i6.p1.1" class="ltx_p">One challenge of using KD for personalized modeling in the FL scenario is that the incentive mechanism is ineffective. The traditional incentive mechanism for parameter-based FL is to calculate client contributions based on their updates. However, in KD-based FL, clients only need to upload logits on the public dataset, which makes it difficult to calculate the true contribution. The server only obtains another set of simplified numbers transformed from the public dataset. Additionally, heterogeneous data on the client side and the potential influence of the teacher model complicate contribution calculations.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">KD-based FL is an emerging FL paradigm that provides a series of new solutions to long-lasting challenges in FL by introducing KD. Given the lack of a comprehensive survey on the topic of KD-based FL in the current open literature, this paper provides a detailed survey of the application of KD in FL. First, we introduce the background of FL and KD in detail, especially how some properties in KD can be used to overcome some long-lasting challenges in FL. We then provide an overview of KD-based FL, discussing in detail the motivations, basics, taxonomy, comparison with traditional FL, and where should KD execute. We supplement several critical factors to consider in KD-based FL in the appendix, including the teachers, knowledge, data, and methods. These critical factors are crucial to designing a practical KD-based FL algorithm. We survey and summarize existing KD-based FL methods and divide KD-based FL into feature-based federated distillation, parameter-based federated distillation, and data-based federated distillation. Furthermore, we summarize how KD can be used to solve long-lasting challenges in FL, including privacy, non-IID, communication, and personalization. We also identify some open problems and future directions for further research in this emerging field.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Although current research on KD-based FL is still in its early stages, it is already generating significant interest. KD is expected to become an indispensable component in FL, bringing new possibilities to distributed learning. We hope that this survey paper can provide a useful reference for researchers and practitioners who are interested in KD-based FL and its applications.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Afonin and Karimireddy (2021)</span>
<span class="ltx_bibblock">
Andrei Afonin and Sai Praneeth Karimireddy. 2021.

</span>
<span class="ltx_bibblock">Towards model agnostic federated learning using knowledge distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.15210</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahn et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Jin-Hyun Ahn, Osvaldo Simeone, and Joonhyuk Kang. 2019.

</span>
<span class="ltx_bibblock">Wireless federated distillation for distributed edge learning with heterogeneous data. In <em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">2019 IEEE 30th Annual International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC)</em>. IEEE, 1–6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alballa and Canini (2023)</span>
<span class="ltx_bibblock">
Norah Alballa and Marco Canini. 2023.

</span>
<span class="ltx_bibblock">A First Look at the Impact of Distillation Hyper-Parameters in Federated Knowledge Distillation. In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 3rd Workshop on Machine Learning and Systems</em>. 123–130.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Allen-Zhu et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. 2019.

</span>
<span class="ltx_bibblock">A convergence theory for deep learning via over-parameterization. In <em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>. PMLR, 242–252.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amari (1993)</span>
<span class="ltx_bibblock">
Shun-ichi Amari. 1993.

</span>
<span class="ltx_bibblock">Backpropagation and stochastic gradient descent method.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em> 5, 4-5 (1993), 185–196.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aono et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Yoshinori Aono, Takuya Hayashi, Lihua Wang, Shiho Moriai, et al<span id="bib.bib7.3.1" class="ltx_text">.</span> 2017.

</span>
<span class="ltx_bibblock">Privacy-preserving deep learning via additively homomorphic encryption.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.4.1" class="ltx_emph ltx_font_italic">IEEE transactions on information forensics and security</em> 13, 5 (2017), 1333–1345.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arivazhagan et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. 2019.

</span>
<span class="ltx_bibblock">Federated learning with personalization layers.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.00818</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biega et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Asia J Biega, Peter Potash, Hal Daumé, Fernando Diaz, and Michèle Finck. 2020.

</span>
<span class="ltx_bibblock">Operationalizing the legal principle of data minimization for personalization. In <em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval</em>. 399–408.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bistritz et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Ilai Bistritz, Ariana Mann, and Nicholas Bambos. 2020.

</span>
<span class="ltx_bibblock">Distributed distillation for on-device learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 33 (2020), 22593–22604.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Hongyan Chang, Virat Shejwalkar, Reza Shokri, and Amir Houmansadr. 2019.

</span>
<span class="ltx_bibblock">Cronus: Robust and heterogeneous collaborative learning with black-box knowledge transfer.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.11279</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2023c)</span>
<span class="ltx_bibblock">
Huancheng Chen, Chaining Wang, and Haris Vikalo. 2023c.

</span>
<span class="ltx_bibblock">The Best of Both Worlds: Accurate Global and Personalized Models through Federated Learning with Data-Free Hyper-Knowledge Distillation. In <em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>. OpenReview.net.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://openreview.net/pdf?id=29V3AWjVAFi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/pdf?id=29V3AWjVAFi</a>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Chao (2020)</span>
<span class="ltx_bibblock">
Hong-You Chen and Wei-Lun Chao. 2020.

</span>
<span class="ltx_bibblock">Fedbe: Making bayesian model ensemble applicable to federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.01974</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Mingzhe Chen, Nir Shlezinger, H Vincent Poor, Yonina C Eldar, and Shuguang Cui. 2021.

</span>
<span class="ltx_bibblock">Communication-efficient federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">Proceedings of the National Academy of Sciences</em> 118, 17 (2021), e2024789118.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Yitao Chen, Dawei Chen, Haoxin Wang, Kyungtae Han, and Ming Zhao. 2023a.

</span>
<span class="ltx_bibblock">Confidence-Based Federated Distillation for Vision-Based Lane-Centering. In <em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2023 - Workshops, Rhodes Island, Greece, June 4-10, 2023</em>. IEEE, 1–5.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ICASSPW59220.2023.10193741" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICASSPW59220.2023.10193741</a>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Yiqiang Chen, Wang Lu, Xin Qin, Jindong Wang, and Xing Xie. 2022.

</span>
<span class="ltx_bibblock">Metafed: Federated learning among federations with cyclic knowledge distillation for personalized healthcare.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.08516</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yiqiang Chen, Xin Qin, Jindong Wang, Chaohui Yu, and Wen Gao. 2020.

</span>
<span class="ltx_bibblock">Fedhealth: A federated transfer learning framework for wearable healthcare.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">IEEE Intelligent Systems</em> 35, 4 (2020), 83–93.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Zheyi Chen, Pu Tian, Weixian Liao, Xuhui Chen, Guobin Xu, and Wei Yu. 2023b.

</span>
<span class="ltx_bibblock">Resource-Aware Knowledge Distillation for Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Emerging Topics in Computing</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho and Hariharan (2019)</span>
<span class="ltx_bibblock">
Jang Hyun Cho and Bharath Hariharan. 2019.

</span>
<span class="ltx_bibblock">On the efficacy of knowledge distillation. In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer vision</em>. 4794–4802.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choudhury et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Olivia Choudhury, Aris Gkoulalas-Divanis, Theodoros Salonidis, Issa Sylla, Yoonyoung Park, Grace Hsu, and Amar Das. 2020.

</span>
<span class="ltx_bibblock">A syntactic approach for privacy-preserving federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">ECAI 2020</em>. IOS Press, 1762–1769.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. 2020.

</span>
<span class="ltx_bibblock">Adaptive personalized federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.13461</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Diao et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Enmao Diao, Jie Ding, and Vahid Tarokh. 2020.

</span>
<span class="ltx_bibblock">Heterofl: Computation and communication efficient federated learning for heterogeneous clients.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.01264</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Jiahua Dong, Lixu Wang, Zhen Fang, Gan Sun, Shichao Xu, Xiao Wang, and Qi Zhu. 2022.

</span>
<span class="ltx_bibblock">Federated class-incremental learning. In <em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 10164–10173.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du and Atallah (2001)</span>
<span class="ltx_bibblock">
Wenliang Du and Mikhail J Atallah. 2001.

</span>
<span class="ltx_bibblock">Secure multi-party computation problems and their applications: a review and open problems. In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2001 workshop on New security paradigms</em>. 13–22.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duan et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Moming Duan, Duo Liu, Xianzhang Chen, Yujuan Tan, Jinting Ren, Lei Qiao, and Liang Liang. 2019.

</span>
<span class="ltx_bibblock">Astraea: Self-balancing federated learning for improving classification accuracy of mobile deep learning applications. In <em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">2019 IEEE 37th international conference on computer design (ICCD)</em>. IEEE, 246–254.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork (2006)</span>
<span class="ltx_bibblock">
Cynthia Dwork. 2006.

</span>
<span class="ltx_bibblock">Differential privacy. In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">International colloquium on automata, languages, and programming</em>. Springer, 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork (2008)</span>
<span class="ltx_bibblock">
Cynthia Dwork. 2008.

</span>
<span class="ltx_bibblock">Differential privacy: A survey of results. In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">International conference on theory and applications of models of computation</em>. Springer, 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eren et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Maksim E Eren, Luke E Richards, Manish Bhattarai, Roberto Yus, Charles Nicholas, and Boian S Alexandrov. 2022.

</span>
<span class="ltx_bibblock">Fedsplit: One-shot federated recommendation system based on non-negative joint matrix factorization and knowledge distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.02359</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fallah et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. 2020.

</span>
<span class="ltx_bibblock">Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 33 (2020), 3557–3568.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang and Ye (2022)</span>
<span class="ltx_bibblock">
Xiuwen Fang and Mang Ye. 2022.

</span>
<span class="ltx_bibblock">Robust federated learning with noisy and heterogeneous clients. In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 10072–10081.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gad and Fadlullah (2023)</span>
<span class="ltx_bibblock">
Gad Gad and Zubair Fadlullah. 2023.

</span>
<span class="ltx_bibblock">Federated learning via augmented knowledge distillation for heterogenous deep human activity recognition systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Sensors</em> 23, 1 (2023), 6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geiping et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller. 2020.

</span>
<span class="ltx_bibblock">Inverting gradients-how easy is it to break privacy in federated learning?

</span>
<span class="ltx_bibblock"><em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 33 (2020), 16937–16947.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghosh et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. 2020.

</span>
<span class="ltx_bibblock">An efficient framework for clustered federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 33 (2020), 19586–19597.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldreich (1998)</span>
<span class="ltx_bibblock">
Oded Goldreich. 1998.

</span>
<span class="ltx_bibblock">Secure multi-party computation.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Manuscript. Preliminary version</em> 78, 110 (1998).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Xuan Gong, Abhishek Sharma, Srikrishna Karanam, Ziyan Wu, Terrence Chen, David Doermann, and Arun Innanje. 2021.

</span>
<span class="ltx_bibblock">Ensemble attention distillation for privacy-preserving federated learning. In <em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 15076–15086.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Xuan Gong, Abhishek Sharma, Srikrishna Karanam, Ziyan Wu, Terrence Chen, David Doermann, and Arun Innanje. 2022.

</span>
<span class="ltx_bibblock">Preserving privacy in federated learning with ensemble cross-domain knowledge distillation. In <em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 36. 11891–11899.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gou et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. 2021.

</span>
<span class="ltx_bibblock">Knowledge distillation: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em> 129 (2021), 1789–1819.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guha et al<span id="bib.bib38.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Neel Guha, Ameet Talwalkar, and Virginia Smith. 2019.

</span>
<span class="ltx_bibblock">One-shot federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.11175</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Halbe et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Shaunak Halbe, James Seale Smith, Junjiao Tian, and Zsolt Kira. 2023.

</span>
<span class="ltx_bibblock">HePCo: Data-Free Heterogeneous Prompt Consolidation for Continual Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.09970</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hamer et al<span id="bib.bib40.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jenny Hamer, Mehryar Mohri, and Ananda Theertha Suresh. 2020.

</span>
<span class="ltx_bibblock">Fedboost: A communication-efficient algorithm for federated learning. In <em id="bib.bib40.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>. PMLR, 3973–3983.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hamood et al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Moqbel Hamood, Abdullatif Albaseer, Mohamed Abdallah, and Ala Al-Fuqaha. 2023.

</span>
<span class="ltx_bibblock">Clustered and Multi-Tasked Federated Distillation for Heterogeneous and Resource Constrained Industrial IoT Applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.3.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Magazine</em> 6, 2 (2023), 64–69.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao et al<span id="bib.bib42.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Meng Hao, Hongwei Li, Guowen Xu, Sen Liu, and Haomiao Yang. 2019.

</span>
<span class="ltx_bibblock">Towards efficient and privacy-preserving federated deep learning. In <em id="bib.bib42.3.1" class="ltx_emph ltx_font_italic">ICC 2019-2019 IEEE international conference on communications (ICC)</em>. IEEE, 1–6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Chaoyang He, Murali Annavaram, and Salman Avestimehr. 2020.

</span>
<span class="ltx_bibblock">Group knowledge transfer: Federated learning of large cnns at the edge.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 33 (2020), 14068–14080.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Yuting He, Yiqiang Chen, XiaoDong Yang, Hanchao Yu, Yi-Hua Huang, and Yang Gu. 2022a.

</span>
<span class="ltx_bibblock">Learning critically: Selective self-distillation in federated learning on non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Big Data</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Yuting He, Yiqiang Chen, Xiaodong Yang, Yingwei Zhang, and Bixiao Zeng. 2022b.

</span>
<span class="ltx_bibblock">Class-wise adaptive self distillation for heterogeneous federated learning. In <em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 36th AAAI Conference on Artificial Intelligence, Virtual</em>, Vol. 22.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton et al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. 2012.

</span>
<span class="ltx_bibblock">Neural networks for machine learning lecture 6a overview of mini-batch gradient descent.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.3.1" class="ltx_emph ltx_font_italic">Cited on</em> 14, 8 (2012), 2.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton et al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.

</span>
<span class="ltx_bibblock">Distilling the knowledge in a neural network.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1503.02531</em> (2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoech et al<span id="bib.bib48.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Haley Hoech, Roman Rischke, Karsten Müller, and Wojciech Samek. 2022.

</span>
<span class="ltx_bibblock">FedAUXfdp: Differentially Private One-Shot Federated Distillation. In <em id="bib.bib48.3.1" class="ltx_emph ltx_font_italic">International Workshop on Trustworthy Federated Learning</em>. Springer, 100–114.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Horvath et al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Samuel Horvath, Stefanos Laskaridis, Mario Almeida, Ilias Leontiadis, Stylianos Venieris, and Nicholas Lane. 2021.

</span>
<span class="ltx_bibblock">Fjord: Fair and accurate federated learning under heterogeneous targets with ordered dropout.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 34 (2021), 12876–12889.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Rui Hu, Yuanxiong Guo, Hongning Li, Qingqi Pei, and Yanmin Gong. 2020.

</span>
<span class="ltx_bibblock">Personalized federated learning with differential privacy.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.3.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em> 7, 10 (2020), 9530–9539.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> (2023c)</span>
<span class="ltx_bibblock">
Chung-ju Huang, Leye Wang, and Xiao Han. 2023c.

</span>
<span class="ltx_bibblock">Vertical Federated Knowledge Transfer via Representation Distillation for Healthcare Collaboration Networks. In <em id="bib.bib51.3.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM Web Conference 2023</em>. 4188–4199.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Chun-Yin Huang, Ruinan Jin, Can Zhao, Daguang Xu, and Xiaoxiao Li. 2023a.

</span>
<span class="ltx_bibblock">Federated Virtual Learning on Heterogeneous Data with Local-global Distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.02278</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span id="bib.bib53.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Wei Huang, Tianrui Li, Dexian Wang, Shengdong Du, and Junbo Zhang. 2020.

</span>
<span class="ltx_bibblock">Fairness and accuracy in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.10069</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Wenke Huang, Mang Ye, and Bo Du. 2022.

</span>
<span class="ltx_bibblock">Learn from others and be yourself in heterogeneous federated learning. In <em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 10143–10153.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Yutao Huang, Lingyang Chu, Zirui Zhou, Lanjun Wang, Jiangchuan Liu, Jian Pei, and Yong Zhang. 2021a.

</span>
<span class="ltx_bibblock">Personalized cross-silo federated learning on non-iid data. In <em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial intelligence</em>, Vol. 35. 7865–7873.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span id="bib.bib56.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Yangsibo Huang, Samyak Gupta, Zhao Song, Kai Li, and Sanjeev Arora. 2021b.

</span>
<span class="ltx_bibblock">Evaluating gradient inversion attacks and defenses in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 34 (2021), 7232–7241.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yue Huang, Lanju Kong, Qingzhong Li, and Baochen Zhang. 2023b.

</span>
<span class="ltx_bibblock">Decentralized Federated Learning Via Mutual Knowledge Distillation. In <em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">2023 IEEE International Conference on Multimedia and Expo (ICME)</em>. IEEE, 342–347.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span id="bib.bib58.2.2.1" class="ltx_text">.</span> (2021c)</span>
<span class="ltx_bibblock">
Ya-Lin Huang, Hao-Chun Yang, and Chi-Chun Lee. 2021c.

</span>
<span class="ltx_bibblock">Federated learning via conditional mutual learning for Alzheimer’s disease classification on T1w MRI. In <em id="bib.bib58.3.1" class="ltx_emph ltx_font_italic">2021 43rd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</em>. IEEE, 2427–2432.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Itahara et al<span id="bib.bib59.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Sohei Itahara, Takayuki Nishio, Yusuke Koda, Masahiro Morikura, and Koji Yamamoto. 2021.

</span>
<span class="ltx_bibblock">Distillation-based semi-supervised federated learning for communication-efficient collaborative training with non-iid private data.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Mobile Computing</em> 22, 1 (2021), 191–205.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jeong et al<span id="bib.bib60.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Eunjeong Jeong, Seungeun Oh, Hyesung Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim. 2018.

</span>
<span class="ltx_bibblock">Communication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.11479</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al<span id="bib.bib61.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Hai Jin, Dongshan Bai, Dezhong Yao, Yutong Dai, Lin Gu, Chen Yu, and Lichao Sun. 2022.

</span>
<span class="ltx_bibblock">Personalized edge intelligence via federated self-knowledge distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib61.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Parallel and Distributed Systems</em> 34, 2 (2022), 567–580.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jing et al<span id="bib.bib62.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Changxing Jing, Yan Huang, Yihong Zhuang, Liyan Sun, Zhenlong Xiao, Yue Huang, and Xinghao Ding. 2023.

</span>
<span class="ltx_bibblock">Exploring personalization via federated representation Learning on non-IID data.

</span>
<span class="ltx_bibblock"><em id="bib.bib62.3.1" class="ltx_emph ltx_font_italic">Neural Networks</em> 163 (2023), 354–366.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kádár and Hadházi (2022)</span>
<span class="ltx_bibblock">
Attila Kádár and Dániel Hadházi. 2022.

</span>
<span class="ltx_bibblock">FedLinked: A client-wise distilled representation based semi-supervised collaborative multitask learning scheme. In <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">2022 International Joint Conference on Neural Networks (IJCNN)</em>. IEEE, 1–8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz et al<span id="bib.bib64.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al<span id="bib.bib64.3.1" class="ltx_text">.</span> 2021.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.4.1" class="ltx_emph ltx_font_italic">Foundations and Trends® in Machine Learning</em> 14, 1–2 (2021), 1–210.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al<span id="bib.bib65.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Myeongkyun Kang, Philip Chikontwe, Soopil Kim, Kyong Hwan Jin, Ehsan Adeli, Kilian M Pohl, and Sang Hyun Park. 2023.

</span>
<span class="ltx_bibblock">One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation. In <em id="bib.bib65.3.1" class="ltx_emph ltx_font_italic">International Conference on Medical Image Computing and Computer-Assisted Intervention</em>. Springer, 521–531.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimireddy et al<span id="bib.bib66.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian U Stich, and Ananda Theertha Suresh. 2021.

</span>
<span class="ltx_bibblock">Breaking the centralized barrier for cross-device federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 34 (2021), 28663–28676.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimireddy et al<span id="bib.bib67.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. 2020.

</span>
<span class="ltx_bibblock">Scaffold: Stochastic controlled averaging for federated learning. In <em id="bib.bib67.3.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>. PMLR, 5132–5143.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimireddy et al<span id="bib.bib68.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich, and Ananda Theertha Suresh. 2019.

</span>
<span class="ltx_bibblock">SCAFFOLD: Stochastic Controlled Averaging for On-Device Federated Learning.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span id="bib.bib69.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Jinkyu Kim, Geeho Kim, and Bohyung Han. 2022.

</span>
<span class="ltx_bibblock">Multi-level branched regularization for federated learning. In <em id="bib.bib69.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>. PMLR, 11058–11073.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim and Wu (2021)</span>
<span class="ltx_bibblock">
Young Geun Kim and Carole-Jean Wu. 2021.

</span>
<span class="ltx_bibblock">Autofl: Enabling heterogeneity-aware energy efficient federated learning. In <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture</em>. 183–198.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečnỳ et al<span id="bib.bib71.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Jakub Konečnỳ, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha Suresh, and Dave Bacon. 2016.

</span>
<span class="ltx_bibblock">Federated learning: Strategies for improving communication efficiency.

</span>
<span class="ltx_bibblock"><em id="bib.bib71.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.05492</em> (2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kulkarni et al<span id="bib.bib72.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Viraj Kulkarni, Milind Kulkarni, and Aniruddha Pant. 2020.

</span>
<span class="ltx_bibblock">Survey of personalization techniques for federated learning. In <em id="bib.bib72.3.1" class="ltx_emph ltx_font_italic">2020 Fourth World Conference on Smart Trends in Systems, Security and Sustainability (WorldS4)</em>. IEEE, 794–797.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lam et al<span id="bib.bib73.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Maximilian Lam, Gu-Yeon Wei, David Brooks, Vijay Janapa Reddi, and Michael Mitzenmacher. 2021.

</span>
<span class="ltx_bibblock">Gradient disaggregation: Breaking privacy in federated learning by reconstructing the user participant matrix. In <em id="bib.bib73.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>. PMLR, 5959–5968.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span id="bib.bib74.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Gihun Lee, Minchan Jeong, Yongjin Shin, Sangmin Bae, and Se-Young Yun. 2022.

</span>
<span class="ltx_bibblock">Preservation of the global knowledge by not-true distillation in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib74.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 35 (2022), 38461–38474.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span id="bib.bib75.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Kibok Lee, Kimin Lee, Jinwoo Shin, and Honglak Lee. 2019.

</span>
<span class="ltx_bibblock">Overcoming catastrophic forgetting with unlabeled data in the wild. In <em id="bib.bib75.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 312–321.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee and Wu (2023)</span>
<span class="ltx_bibblock">
Shao-Ming Lee and Ja-Ling Wu. 2023.

</span>
<span class="ltx_bibblock">FedUA: An Uncertainty-Aware Distillation-Based Federated Learning Scheme for Image Classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Information</em> 14, 4 (2023), 234.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Wang (2019)</span>
<span class="ltx_bibblock">
Daliang Li and Junpu Wang. 2019.

</span>
<span class="ltx_bibblock">Fedmd: Heterogenous federated learning via model distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.03581</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib78.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. 2022.

</span>
<span class="ltx_bibblock">Federated learning on non-iid data silos: An experimental study. In <em id="bib.bib78.3.1" class="ltx_emph ltx_font_italic">2022 IEEE 38th International Conference on Data Engineering (ICDE)</em>. IEEE, 965–978.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib79.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Qinbin Li, Bingsheng He, and Dawn Song. 2020a.

</span>
<span class="ltx_bibblock">Practical one-shot federated learning for cross-silo setting.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.01017</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib80.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. 2021b.

</span>
<span class="ltx_bibblock">Ditto: Fair and robust federated learning through personalization. In <em id="bib.bib80.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>. PMLR, 6357–6368.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib81.2.2.1" class="ltx_text">.</span> (2020c)</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. 2020c.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib81.3.1" class="ltx_emph ltx_font_italic">IEEE signal processing magazine</em> 37, 3 (2020), 50–60.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib82.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Wenqi Li, Fausto Milletarì, Daguang Xu, Nicola Rieke, Jonny Hancox, Wentao Zhu, Maximilian Baust, Yan Cheng, Sébastien Ourselin, M Jorge Cardoso, et al<span id="bib.bib82.3.1" class="ltx_text">.</span> 2019.

</span>
<span class="ltx_bibblock">Privacy-preserving federated brain tumour segmentation. In <em id="bib.bib82.4.1" class="ltx_emph ltx_font_italic">Machine Learning in Medical Imaging: 10th International Workshop, MLMI 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings 10</em>. Springer, 133–141.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib83.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Xinjia Li, Boyu Chen, and Wenlian Lu. 2023.

</span>
<span class="ltx_bibblock">FedDKD: Federated learning with decentralized knowledge distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.3.1" class="ltx_emph ltx_font_italic">Applied Intelligence</em> (2023), 1–17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib84.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Xianxian Li, Yanxia Gong, Yuan Liang, and Li-e Wang. 2021a.

</span>
<span class="ltx_bibblock">Personalized federated learning with semisupervised distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib84.3.1" class="ltx_emph ltx_font_italic">Security and Communication Networks</em> 2021 (2021), 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib85.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Zheng Li, Ying Huang, Defang Chen, Tianren Luo, Ning Cai, and Zhigeng Pan. 2020b.

</span>
<span class="ltx_bibblock">Online knowledge distillation via multi-branch diversity enhancement. In <em id="bib.bib85.3.1" class="ltx_emph ltx_font_italic">Proceedings of the Asian Conference on Computer Vision</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al<span id="bib.bib86.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Xiaoxiao Liang, Yiqun Lin, Huazhu Fu, Lei Zhu, and Xiaomeng Li. 2022.

</span>
<span class="ltx_bibblock">Rscfed: Random sampling consensus federated semi-supervised learning. In <em id="bib.bib86.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 10154–10163.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lim et al<span id="bib.bib87.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Wei Yang Bryan Lim, Nguyen Cong Luong, Dinh Thai Hoang, Yutao Jiao, Ying-Chang Liang, Qiang Yang, Dusit Niyato, and Chunyan Miao. 2020.

</span>
<span class="ltx_bibblock">Federated learning in mobile edge networks: A comprehensive survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib87.3.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</em> 22, 3 (2020), 2031–2063.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span id="bib.bib88.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. 2020.

</span>
<span class="ltx_bibblock">Ensemble distillation for robust model fusion in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib88.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 33 (2020), 2351–2363.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Yang (2023)</span>
<span class="ltx_bibblock">
Fangbo Liu and Feng Yang. 2023.

</span>
<span class="ltx_bibblock">Medical Image Segmentation Based on Federated Distillation Optimization Learning on Non-IID Data. In <em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">Advanced Intelligent Computing Technology and Applications - 19th International Conference, ICIC 2023, Zhengzhou, China, August 10-13, 2023, Proceedings, Part III</em> <em id="bib.bib89.2.2" class="ltx_emph ltx_font_italic">(Lecture Notes in Computer Science, Vol. 14088)</em>, De-Shuang Huang, Prashan Premaratne, Baohua Jin, Boyang Qu, Kang-Hyun Jo, and Abir Hussain (Eds.). Springer, 347–358.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1007/978-981-99-4749-2_30" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/978-981-99-4749-2_30</a>

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib90.2.2.1" class="ltx_text">.</span> (2022c)</span>
<span class="ltx_bibblock">
Lumin Liu, Jun Zhang, SH Song, and Khaled B Letaief. 2022c.

</span>
<span class="ltx_bibblock">Communication-efficient federated distillation with active data sampling. In <em id="bib.bib90.3.1" class="ltx_emph ltx_font_italic">ICC 2022-IEEE International Conference on Communications</em>. IEEE, 201–206.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib91.2.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Ruixuan Liu, Fangzhao Wu, Chuhan Wu, Yanlin Wang, Lingjuan Lyu, Hong Chen, and Xing Xie. 2022b.

</span>
<span class="ltx_bibblock">No one left behind: Inclusive federated learning over heterogeneous devices. In <em id="bib.bib91.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>. 3398–3406.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib92.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Yi-Jing Liu, Gang Feng, Dusit Niyato, Shuang Qin, Jianhong Zhou, Xiaoqian Li, and Xinyi Xu. 2022a.

</span>
<span class="ltx_bibblock">Ensemble Distillation based Adaptive Quantization for Supporting Federated Learning in Wireless Networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib92.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Wireless Communications</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span id="bib.bib93.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Bing Luo, Wenli Xiao, Shiqiang Wang, Jianwei Huang, and Leandros Tassiulas. 2022.

</span>
<span class="ltx_bibblock">Tackling system and statistical heterogeneity for federated learning with adaptive client sampling. In <em id="bib.bib93.3.1" class="ltx_emph ltx_font_italic">IEEE INFOCOM 2022-IEEE conference on computer communications</em>. IEEE, 1739–1748.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo and Wu (2022)</span>
<span class="ltx_bibblock">
Jun Luo and Shandong Wu. 2022.

</span>
<span class="ltx_bibblock">Adapt to adaptation: Learning personalization for cross-silo federated learning. In <em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">IJCAI: proceedings of the conference</em>, Vol. 2022. NIH Public Access, 2166.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span id="bib.bib95.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Yawei Luo, Liang Zheng, Tao Guan, Junqing Yu, and Yi Yang. 2019.

</span>
<span class="ltx_bibblock">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation. In <em id="bib.bib95.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 2507–2516.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luping et al<span id="bib.bib96.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
WANG Luping, WANG Wei, and LI Bo. 2019.

</span>
<span class="ltx_bibblock">CMFL: Mitigating communication overhead for federated learning. In <em id="bib.bib96.3.1" class="ltx_emph ltx_font_italic">2019 IEEE 39th international conference on distributed computing systems (ICDCS)</em>. IEEE, 954–964.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al<span id="bib.bib97.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Feng Lyu, Cheng Tang, Yongheng Deng, Tong Liu, Yongmin Zhang, and Yaoxue Zhang. 2023.

</span>
<span class="ltx_bibblock">A Prototype-Based Knowledge Distillation Framework for Heterogeneous Federated Learning. In <em id="bib.bib97.3.1" class="ltx_emph ltx_font_italic">2023 IEEE 43rd International Conference on Distributed Computing Systems (ICDCS)</em>. IEEE, 1–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span id="bib.bib98.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jiaxin Ma, Ryo Yonetani, and Zahid Iqbal. 2021.

</span>
<span class="ltx_bibblock">Adaptive distillation for decentralized learning from heterogeneous clients. In <em id="bib.bib98.3.1" class="ltx_emph ltx_font_italic">2020 25th International Conference on Pattern Recognition (ICPR)</em>. IEEE, 7486–7492.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span id="bib.bib99.2.2.1" class="ltx_text">.</span> ([n. d.])</span>
<span class="ltx_bibblock">
Yuhang Ma, Zhongle Xie, Jue Wang, Ke Chen, and Lidan Shou. [n. d.].

</span>
<span class="ltx_bibblock">Continual Federated Learning Based on Knowledge Distillation.

</span>
<span class="ltx_bibblock">([n. d.]).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span id="bib.bib100.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Yuhang Ma, Zhongle Xie, Jue Wang, Ke Chen, and Lidan Shou. 2022.

</span>
<span class="ltx_bibblock">Continual federated learning based on knowledge distillation. In <em id="bib.bib100.3.1" class="ltx_emph ltx_font_italic">Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence</em>, Vol. 3.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mansour et al<span id="bib.bib101.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. 2020.

</span>
<span class="ltx_bibblock">Three approaches for personalization with applications to federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib101.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.10619</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al<span id="bib.bib102.2.2.1" class="ltx_text">.</span> (2017a)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. 2017a.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized data. In <em id="bib.bib102.3.1" class="ltx_emph ltx_font_italic">Artificial intelligence and statistics</em>. PMLR, 1273–1282.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al<span id="bib.bib103.2.2.1" class="ltx_text">.</span> (2017b)</span>
<span class="ltx_bibblock">
H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. 2017b.

</span>
<span class="ltx_bibblock">Learning differentially private recurrent language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib103.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1710.06963</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mendieta et al<span id="bib.bib104.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Matias Mendieta, Taojiannan Yang, Pu Wang, Minwoo Lee, Zhengming Ding, and Chen Chen. 2022.

</span>
<span class="ltx_bibblock">Local learning matters: Rethinking data heterogeneity in federated learning. In <em id="bib.bib104.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 8397–8406.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Menghani and Ravi (2019)</span>
<span class="ltx_bibblock">
Gaurav Menghani and Sujith Ravi. 2019.

</span>
<span class="ltx_bibblock">Learning from a teacher using unlabeled data.

</span>
<span class="ltx_bibblock"><em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.05275</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mi et al<span id="bib.bib106.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Yuxi Mi, Yutong Mu, Shuigeng Zhou, and Jihong Guan. 2021.

</span>
<span class="ltx_bibblock">Fedmdr: Federated model distillation with robust aggregation. In <em id="bib.bib106.3.1" class="ltx_emph ltx_font_italic">Asia-Pacific Web (APWeb) and Web-Age Information Management (WAIM) Joint International Conference on Web and Big Data</em>. Springer, 18–32.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mills et al<span id="bib.bib107.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Jed Mills, Jia Hu, and Geyong Min. 2022.

</span>
<span class="ltx_bibblock">Client-Side Optimization Strategies for Communication-Efficient Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib107.3.1" class="ltx_emph ltx_font_italic">IEEE Communications Magazine</em> 60, 7 (2022), 60–66.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mirzadeh et al<span id="bib.bib108.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. 2020.

</span>
<span class="ltx_bibblock">Improved knowledge distillation via teacher assistant. In <em id="bib.bib108.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial intelligence</em>, Vol. 34. 5191–5198.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mo et al<span id="bib.bib109.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Zijia Mo, Zhipeng Gao, Chen Zhao, and Yijing Lin. 2022.

</span>
<span class="ltx_bibblock">FedDQ: A communication-efficient federated learning approach for Internet of Vehicles.

</span>
<span class="ltx_bibblock"><em id="bib.bib109.3.1" class="ltx_emph ltx_font_italic">Journal of Systems Architecture</em> 131 (2022), 102690.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohammed et al<span id="bib.bib110.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Malik Naik Mohammed, Xinyue Zhang, Maria Valero, and Ying Xie. 2023.

</span>
<span class="ltx_bibblock">Poster: AsyncFedKD: Asynchronous Federated Learning with Knowledge Distillation. In <em id="bib.bib110.3.1" class="ltx_emph ltx_font_italic">2023 IEEE/ACM Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)</em>. IEEE, 207–208.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mothukuri et al<span id="bib.bib111.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Viraaji Mothukuri, Reza M Parizi, Seyedamin Pouriyeh, Yan Huang, Ali Dehghantanha, and Gautam Srivastava. 2021.

</span>
<span class="ltx_bibblock">A survey on security and privacy of federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib111.3.1" class="ltx_emph ltx_font_italic">Future Generation Computer Systems</em> 115 (2021), 619–640.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al<span id="bib.bib112.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Nang Hung Nguyen, Duc Long Nguyen, Trong Bang Nguyen, Thanh-Hung Nguyen, Huy Hieu Pham, Truong Thao Nguyen, and Phi Le Nguyen. 2023.

</span>
<span class="ltx_bibblock">Cadis: Handling cluster-skewed non-iid data in federated learning with clustered aggregation and knowledge distilled regularization. In <em id="bib.bib112.3.1" class="ltx_emph ltx_font_italic">2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid)</em>. IEEE, 249–261.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ni et al<span id="bib.bib113.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Xuanming Ni, Xinyuan Shen, and Huimin Zhao. 2022.

</span>
<span class="ltx_bibblock">Federated optimization via knowledge codistillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib113.3.1" class="ltx_emph ltx_font_italic">Expert Systems with Applications</em> 191 (2022), 116310.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oh et al<span id="bib.bib114.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Seungeun Oh, Jihong Park, Eunjeong Jeong, Hyesung Kim, Mehdi Bennis, and Seong-Lyun Kim. 2020.

</span>
<span class="ltx_bibblock">Mix2FLD: Downlink federated learning after uplink federated distillation with two-way mixup.

</span>
<span class="ltx_bibblock"><em id="bib.bib114.3.1" class="ltx_emph ltx_font_italic">IEEE Communications Letters</em> 24, 10 (2020), 2211–2215.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Orekondy et al<span id="bib.bib115.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Tribhuvanesh Orekondy, Seong Joon Oh, Yang Zhang, Bernt Schiele, and Mario Fritz. 2018.

</span>
<span class="ltx_bibblock">Gradient-leaks: Understanding and controlling deanonymization in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib115.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1805.05838</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ozkara et al<span id="bib.bib116.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Kaan Ozkara, Navjot Singh, Deepesh Data, and Suhas Diggavi. 2021.

</span>
<span class="ltx_bibblock">QuPeD: Quantized personalization via distillation with applications to federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib116.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 34 (2021), 3622–3634.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panaretos and Zemel (2019)</span>
<span class="ltx_bibblock">
Victor M Panaretos and Yoav Zemel. 2019.

</span>
<span class="ltx_bibblock">Statistical aspects of Wasserstein distances.

</span>
<span class="ltx_bibblock"><em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">Annual review of statistics and its application</em> 6 (2019), 405–431.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al<span id="bib.bib118.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Chao Peng, Yiming Guo, Yao Chen, Qilin Rui, Zhengfeng Yang, and Chenyang Xu. 2023.

</span>
<span class="ltx_bibblock">FedGM: Heterogeneous Federated Learning via Generative Learning and Mutual Distillation. In <em id="bib.bib118.3.1" class="ltx_emph ltx_font_italic">European Conference on Parallel Processing</em>. Springer, 339–351.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Phuong and Lampert (2019)</span>
<span class="ltx_bibblock">
Mary Phuong and Christoph Lampert. 2019.

</span>
<span class="ltx_bibblock">Towards understanding knowledge distillation. In <em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>. PMLR, 5142–5151.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rothchild et al<span id="bib.bib120.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Daniel Rothchild, Ashwinee Panda, Enayat Ullah, Nikita Ivkin, Ion Stoica, Vladimir Braverman, Joseph Gonzalez, and Raman Arora. 2020.

</span>
<span class="ltx_bibblock">Fetchsgd: Communication-efficient federated learning with sketching. In <em id="bib.bib120.3.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>. PMLR, 8253–8265.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sankaranarayanan et al<span id="bib.bib121.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Swami Sankaranarayanan, Yogesh Balaji, Arpit Jain, Ser Nam Lim, and Rama Chellappa. 2018.

</span>
<span class="ltx_bibblock">Learning from synthetic data: Addressing domain shift for semantic segmentation. In <em id="bib.bib121.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 3752–3761.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sattler et al<span id="bib.bib122.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Felix Sattler, Tim Korjakow, Roman Rischke, and Wojciech Samek. 2021a.

</span>
<span class="ltx_bibblock">Fedaux: Leveraging unlabeled auxiliary data in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib122.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sattler et al<span id="bib.bib123.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Felix Sattler, Arturo Marban, Roman Rischke, and Wojciech Samek. 2020.

</span>
<span class="ltx_bibblock">Communication-efficient federated distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib123.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.00632</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sattler et al<span id="bib.bib124.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Felix Sattler, Arturo Marban, Roman Rischke, and Wojciech Samek. 2021b.

</span>
<span class="ltx_bibblock">CFD: Communication-efficient federated distillation via soft-label quantization and delta coding.

</span>
<span class="ltx_bibblock"><em id="bib.bib124.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Network Science and Engineering</em> 9, 4 (2021), 2025–2038.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sattler et al<span id="bib.bib125.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Felix Sattler, Simon Wiedemann, Klaus-Robert Müller, and Wojciech Samek. 2019.

</span>
<span class="ltx_bibblock">Robust and communication-efficient federated learning from non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib125.3.1" class="ltx_emph ltx_font_italic">IEEE transactions on neural networks and learning systems</em> 31, 9 (2019), 3400–3413.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shahid et al<span id="bib.bib126.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Osama Shahid, Seyedamin Pouriyeh, Reza M Parizi, Quan Z Sheng, Gautam Srivastava, and Liang Zhao. 2021.

</span>
<span class="ltx_bibblock">Communication efficiency in federated learning: Achievements and challenges.

</span>
<span class="ltx_bibblock"><em id="bib.bib126.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.10996</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shang et al<span id="bib.bib127.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Ertong Shang, Hui Liu, Zhuo Yang, Junzhao Du, and Yiming Ge. 2023.

</span>
<span class="ltx_bibblock">FedBiKD: Federated Bidirectional Knowledge Distillation for Distracted Driving Detection.

</span>
<span class="ltx_bibblock"><em id="bib.bib127.3.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shang et al<span id="bib.bib128.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Xinyi Shang, Yang Lu, Yiu-ming Cheung, and Hanzi Wang. 2022.

</span>
<span class="ltx_bibblock">Fedic: Federated learning on non-iid and long-tailed data via calibrated distillation. In <em id="bib.bib128.3.1" class="ltx_emph ltx_font_italic">2022 IEEE International Conference on Multimedia and Expo (ICME)</em>. IEEE, 1–6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al<span id="bib.bib129.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Jiawei Shao, Fangzhao Wu, and Jun Zhang. 2023.

</span>
<span class="ltx_bibblock">Selective Knowledge Sharing for Privacy-Preserving Federated Distillation without A Good Teacher.

</span>
<span class="ltx_bibblock"><em id="bib.bib129.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.01731</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span id="bib.bib130.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Yiqing Shen, Yuyin Zhou, and Lequan Yu. 2022.

</span>
<span class="ltx_bibblock">Cd2-pfed: Cyclic distillation-guided channel decoupling for model personalization in federated learning. In <em id="bib.bib130.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 10041–10050.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al<span id="bib.bib131.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Haizhou Shi, Youcai Zhang, Zijin Shen, Siliang Tang, Yaqian Li, Yandong Guo, and Yueting Zhuang. 2021.

</span>
<span class="ltx_bibblock">Towards communication-efficient and privacy-preserving federated representation learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib131.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.14611</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shoham et al<span id="bib.bib132.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Neta Shoham, Tomer Avidor, Aviv Keren, Nadav Israel, Daniel Benditkis, Liron Mor-Yosef, and Itai Zeitak. 2019.

</span>
<span class="ltx_bibblock">Overcoming forgetting in federated learning on non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib132.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.07796</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al<span id="bib.bib133.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Mengkai Song, Zhibo Wang, Zhifei Zhang, Yang Song, Qian Wang, Ju Ren, and Hairong Qi. 2020.

</span>
<span class="ltx_bibblock">Analyzing user-level privacy attack against federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib133.3.1" class="ltx_emph ltx_font_italic">IEEE Journal on Selected Areas in Communications</em> 38, 10 (2020), 2430–2444.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al<span id="bib.bib134.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Rui Song, Dai Liu, Dave Zhenyu Chen, Andreas Festag, Carsten Trinitis, Martin Schulz, and Alois Knoll. 2023.

</span>
<span class="ltx_bibblock">Federated learning via decentralized dataset distillation in resource-constrained edge environments. In <em id="bib.bib134.3.1" class="ltx_emph ltx_font_italic">2023 International Joint Conference on Neural Networks (IJCNN)</em>. IEEE, 1–10.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al<span id="bib.bib135.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Caiyu Su, Jinri Wei, Yuan Lei, and Jiahui Li. 2023.

</span>
<span class="ltx_bibblock">A federated learning framework based on transfer learning and knowledge distillation for targeted advertising.

</span>
<span class="ltx_bibblock"><em id="bib.bib135.3.1" class="ltx_emph ltx_font_italic">PeerJ Computer Science</em> 9 (2023), e1496.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al<span id="bib.bib136.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Shangchao Su, Bin Li, and Xiangyang Xue. 2022.

</span>
<span class="ltx_bibblock">Domain Discrepancy Aware Distillation for Model Aggregation in Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib136.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.02190</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun and Lyu (2021)</span>
<span class="ltx_bibblock">
Lichao Sun and Lingjuan Lyu. 2021.

</span>
<span class="ltx_bibblock">Federated Model Distillation with Noise-Free Differential Privacy. In <em id="bib.bib137.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021</em>, Zhi-Hua Zhou (Ed.). ijcai.org, 1563–1570.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.24963/IJCAI.2021/216" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.24963/IJCAI.2021/216</a>

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al<span id="bib.bib138.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. 2022.

</span>
<span class="ltx_bibblock">Towards personalized federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib138.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al<span id="bib.bib139.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Jianwu Tang, Xuefeng Ding, Dasha Hu, Bing Guo, Yuncheng Shen, Pan Ma, and Yuming Jiang. 2023.

</span>
<span class="ltx_bibblock">FedRAD: Heterogeneous Federated Learning via Relational Adaptive Distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib139.3.1" class="ltx_emph ltx_font_italic">Sensors</em> 23, 14 (2023), 6518.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al<span id="bib.bib140.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jiaxi Tang, Rakesh Shivanna, Zhe Zhao, Dong Lin, Anima Singh, Ed H Chi, and Sagar Jain. 2020.

</span>
<span class="ltx_bibblock">Understanding and improving knowledge distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib140.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.03532</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tanghatari et al<span id="bib.bib141.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Ehsan Tanghatari, Mehdi Kamal, Ali Afzali-Kusha, and Massoud Pedram. 2023.

</span>
<span class="ltx_bibblock">Federated learning by employing knowledge distillation on edge devices with limited hardware resources.

</span>
<span class="ltx_bibblock"><em id="bib.bib141.3.1" class="ltx_emph ltx_font_italic">Neurocomputing</em> 531 (2023), 87–99.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taya et al<span id="bib.bib142.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Akihito Taya, Takayuki Nishio, Masahiro Morikura, and Koji Yamamoto. 2022.

</span>
<span class="ltx_bibblock">Decentralized and model-free federated learning: Consensus-based distillation in function space.

</span>
<span class="ltx_bibblock"><em id="bib.bib142.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Signal and Information Processing over Networks</em> 8 (2022), 799–814.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Truex et al<span id="bib.bib143.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Stacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, Rui Zhang, and Yi Zhou. 2019.

</span>
<span class="ltx_bibblock">A hybrid approach to privacy-preserving federated learning. In <em id="bib.bib143.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th ACM workshop on artificial intelligence and security</em>. 1–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ur Rehman et al<span id="bib.bib144.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Muhammad Habib ur Rehman, Ahmed Mukhtar Dirir, Khaled Salah, Ernesto Damiani, and Davor Svetinovic. 2021.

</span>
<span class="ltx_bibblock">TrustFed: A framework for fair and trustworthy cross-device federated learning in IIoT.

</span>
<span class="ltx_bibblock"><em id="bib.bib144.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Industrial Informatics</em> 17, 12 (2021), 8485–8494.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Varno et al<span id="bib.bib145.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Farshid Varno, Marzie Saghayi, Laya Rafiee Sevyeri, Sharut Gupta, Stan Matwin, and Mohammad Havaei. 2022.

</span>
<span class="ltx_bibblock">AdaBest: Minimizing Client Drift in Federated Learning via Adaptive Bias Estimation. In <em id="bib.bib145.3.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>. Springer, 710–726.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wan et al<span id="bib.bib146.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Sheng Wan, Dashan Gao, Hanlin Gu, and Daning Hu. 2023.

</span>
<span class="ltx_bibblock">FedPDD: A Privacy-preserving Double Distillation Framework for Cross-silo Federated Recommendation.

</span>
<span class="ltx_bibblock"><em id="bib.bib146.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.06272</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib147.2.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Dong Wang, Naifu Zhang, Meixia Tao, and Xu Chen. 2022b.

</span>
<span class="ltx_bibblock">Knowledge Selection and Local Updating Optimization for Federated Knowledge Distillation With Heterogeneous Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib147.3.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em> 17, 1 (2022), 82–97.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib148.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Hao Wang, Zakhary Kaplan, Di Niu, and Baochun Li. 2020.

</span>
<span class="ltx_bibblock">Optimizing federated learning on non-iid data with reinforcement learning. In <em id="bib.bib148.3.1" class="ltx_emph ltx_font_italic">IEEE INFOCOM 2020-IEEE Conference on Computer Communications</em>. IEEE, 1698–1707.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib149.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Haozhao Wang, Yichen Li, Wenchao Xu, Ruixuan Li, Yufeng Zhan, and Zhigang Zeng. 2023a.

</span>
<span class="ltx_bibblock">DaFKD: Domain-aware Federated Knowledge Distillation. In <em id="bib.bib149.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 20412–20421.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib150.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Leye Wang, Chongru Huang, and Xiao Han. 2022a.

</span>
<span class="ltx_bibblock">Vertical federated knowledge transfer via representation distillation. In <em id="bib.bib150.3.1" class="ltx_emph ltx_font_italic">FL-IJCAI workshop</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Yoon (2021)</span>
<span class="ltx_bibblock">
Lin Wang and Kuk-Jin Yoon. 2021.

</span>
<span class="ltx_bibblock">Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks.

</span>
<span class="ltx_bibblock"><em id="bib.bib151.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</em> 44, 6 (2021), 3048–3068.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib152.2.2.1" class="ltx_text">.</span> (2023c)</span>
<span class="ltx_bibblock">
Rui Wang, Tong Yu, Junda Wu, Handong Zhao, Sungchul Kim, Ruiyi Zhang, Subrata Mitra, and Ricardo Henao. 2023c.

</span>
<span class="ltx_bibblock">Federated Domain Adaptation for Named Entity Recognition via Distilling with Heterogeneous Tag Sets. In <em id="bib.bib152.3.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023</em>. 7449–7463.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib153.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Shiyu Wang, Jiahao Xie, Mingming Lu, and Neal N Xiong. 2023b.

</span>
<span class="ltx_bibblock">FedGraph-KD: An Effective Federated Graph Learning Scheme Based on Knowledge Distillation. In <em id="bib.bib153.3.1" class="ltx_emph ltx_font_italic">2023 IEEE 9th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing,(HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS)</em>. IEEE, 130–134.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib154.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. 2018.

</span>
<span class="ltx_bibblock">Dataset distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib154.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.10959</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib155.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Zheng Wang, Xiaoliang Fan, Jianzhong Qi, Chenglu Wen, Cheng Wang, and Rongshan Yu. 2021.

</span>
<span class="ltx_bibblock">Federated learning with fair averaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib155.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.14937</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen et al<span id="bib.bib156.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Hui Wen, Yue Wu, Jia Hu, Zi Wang, Hancong Duan, and Geyong Min. 2023.

</span>
<span class="ltx_bibblock">Communication-Efficient Federated Learning on Non-IID Data using Two-Step Knowledge Distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib156.3.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen et al<span id="bib.bib157.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Hui Wen, Yue Wu, Jingjing Li, and Hancong Duan. 2022.

</span>
<span class="ltx_bibblock">Communication-Efficient Federated Data Augmentation on Non-IID Data. In <em id="bib.bib157.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 3377–3386.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Kwon (2023)</span>
<span class="ltx_bibblock">
Aming Wu and Young-Woo Kwon. 2023.

</span>
<span class="ltx_bibblock">Enhancing Recommendation Capabilities Using Multi-Head Attention-based Federated Knowledge Distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib158.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span id="bib.bib159.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Chuhan Wu, Fangzhao Wu, Lingjuan Lyu, Yongfeng Huang, and Xing Xie. 2022.

</span>
<span class="ltx_bibblock">Communication-efficient federated learning via knowledge distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib159.3.1" class="ltx_emph ltx_font_italic">Nature communications</em> 13, 1 (2022), 1–8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Wang (2022)</span>
<span class="ltx_bibblock">
Hongda Wu and Ping Wang. 2022.

</span>
<span class="ltx_bibblock">Node selection toward faster convergence for federated learning on non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib160.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Network Science and Engineering</em> 9, 5 (2022), 3099–3111.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span id="bib.bib161.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Peng Wu, Tales Imbiriba, Junha Park, Sunwoo Kim, and Pau Closas. 2021.

</span>
<span class="ltx_bibblock">Personalized federated learning over non-iid data for indoor localization. In <em id="bib.bib161.3.1" class="ltx_emph ltx_font_italic">2021 IEEE 22nd International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)</em>. IEEE, 421–425.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al<span id="bib.bib162.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jianhang Xiao, Chunhui Du, Zijing Duan, and Wei Guo. 2021.

</span>
<span class="ltx_bibblock">A novel server-side aggregation strategy for federated learning in non-iid situations. In <em id="bib.bib162.3.1" class="ltx_emph ltx_font_italic">2021 20th international symposium on parallel and distributed computing (ISPDC)</em>. IEEE, 17–24.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span id="bib.bib163.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. 2019.

</span>
<span class="ltx_bibblock">Slsgd: Secure and efficient distributed on-device machine learning. In <em id="bib.bib163.3.1" class="ltx_emph ltx_font_italic">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em>. Springer, 213–228.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xing et al<span id="bib.bib164.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Huanlai Xing, Zhiwen Xiao, Rong Qu, Zonghai Zhu, and Bowen Zhao. 2022.

</span>
<span class="ltx_bibblock">An efficient federated distillation learning system for multitask time series classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib164.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Instrumentation and Measurement</em> 71 (2022), 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et al<span id="bib.bib165.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Yuanhao Xiong, Ruochen Wang, Minhao Cheng, Felix Yu, and Cho-Jui Hsieh. 2023.

</span>
<span class="ltx_bibblock">Feddm: Iterative distribution matching for communication-efficient federated learning. In <em id="bib.bib165.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 16323–16332.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span id="bib.bib166.2.2.1" class="ltx_text">.</span> (2019b)</span>
<span class="ltx_bibblock">
Chugui Xu, Ju Ren, Deyu Zhang, Yaoxue Zhang, Zhan Qin, and Kui Ren. 2019b.

</span>
<span class="ltx_bibblock">GANobfuscator: Mitigating information leakage under GAN via differential privacy.

</span>
<span class="ltx_bibblock"><em id="bib.bib166.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Forensics and Security</em> 14, 9 (2019), 2358–2371.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span id="bib.bib167.2.2.1" class="ltx_text">.</span> (2019a)</span>
<span class="ltx_bibblock">
Guowen Xu, Hongwei Li, Sen Liu, Kan Yang, and Xiaodong Lin. 2019a.

</span>
<span class="ltx_bibblock">Verifynet: Secure and verifiable federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib167.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Forensics and Security</em> 15 (2019), 911–926.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span id="bib.bib168.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Xiaoyun Xu, Jingzheng Wu, Mutian Yang, Tianyue Luo, Xu Duan, Weiheng Li, Yanjun Wu, and Bin Wu. 2020.

</span>
<span class="ltx_bibblock">Information leakage by model weights on federated learning. In <em id="bib.bib168.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Workshop on Privacy-Preserving Machine Learning in Practice</em>. 31–36.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu and Fan (2023)</span>
<span class="ltx_bibblock">
Yikai Xu and Hongbo Fan. 2023.

</span>
<span class="ltx_bibblock">FedDK: Improving Cyclic Knowledge Distillation for Personalized Healthcare Federated Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib169.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yamasaki and Takase (2023)</span>
<span class="ltx_bibblock">
Yusuke Yamasaki and Hideki Takase. 2023.

</span>
<span class="ltx_bibblock">F2MKD: Fog-enabled Federated Learning with Mutual Knowledge Distillation. In <em id="bib.bib170.1.1" class="ltx_emph ltx_font_italic">2023 IEEE 20th Consumer Communications &amp; Networking Conference (CCNC)</em>. IEEE, 682–683.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib171.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Peng Yang, Mengjiao Yan, Yaping Cui, Peng He, Dapeng Wu, Ruyan Wang, and Luo Chen. 2023.

</span>
<span class="ltx_bibblock">Communication-Efficient Federated Double Distillation in IoV.

</span>
<span class="ltx_bibblock"><em id="bib.bib171.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Cognitive Communications and Networking</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib172.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Qiang Yang, Juan Chen, Xiang Yin, Jing Xie, and Quan Wen. 2022.

</span>
<span class="ltx_bibblock">FedMMD: Heterogenous Federated Learning based on Multi-teacher and Multi-feature Distillation. In <em id="bib.bib172.3.1" class="ltx_emph ltx_font_italic">2022 7th International Conference on Computer and Communication Systems (ICCCS)</em>. IEEE, 897–902.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib173.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib173.3.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and Technology (TIST)</em> 10, 2 (2019), 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al<span id="bib.bib174.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhijie Yao, Jingyu Wang, Xiaojun Jing, and Junsheng Mu. 2023.

</span>
<span class="ltx_bibblock">The Recognition of Remote Sensing Image based on Federated Knowledge Distillation. In <em id="bib.bib174.3.1" class="ltx_emph ltx_font_italic">2023 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)</em>. IEEE, 1–6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yashwanth et al<span id="bib.bib175.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
M. Yashwanth, Gaurav Kumar Nayak, Arya Singh, Yogesh Singh, and Anirban Chakraborty. 2023.

</span>
<span class="ltx_bibblock">Federated Learning on Heterogeneous Data via Adaptive Self-Distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib175.3.1" class="ltx_emph ltx_font_italic">CoRR</em> abs/2305.19600 (2023).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/ARXIV.2305.19600" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/ARXIV.2305.19600</a>
arXiv:2305.19600

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yi et al<span id="bib.bib176.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Xun Yi, Russell Paulet, Elisa Bertino, Xun Yi, Russell Paulet, and Elisa Bertino. 2014.

</span>
<span class="ltx_bibblock"><em id="bib.bib176.3.1" class="ltx_emph ltx_font_italic">Homomorphic encryption</em>.

</span>
<span class="ltx_bibblock">Springer.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al<span id="bib.bib177.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Feng Yin, Zhidi Lin, Qinglei Kong, Yue Xu, Deshi Li, Sergios Theodoridis, and Shuguang Robert Cui. 2020.

</span>
<span class="ltx_bibblock">FedLoc: Federated learning framework for data-driven cooperative localization and location data processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib177.3.1" class="ltx_emph ltx_font_italic">IEEE Open Journal of Signal Processing</em> 1 (2020), 187–215.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al<span id="bib.bib178.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Xuefei Yin, Yanming Zhu, and Jiankun Hu. 2021.

</span>
<span class="ltx_bibblock">A comprehensive survey of privacy-preserving federated learning: A taxonomy, review, and future directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib178.3.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys (CSUR)</em> 54, 6 (2021), 1–36.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al<span id="bib.bib179.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. 2020.

</span>
<span class="ltx_bibblock">Revisiting knowledge distillation via label smoothing regularization. In <em id="bib.bib179.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 3903–3911.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib180.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Chen Zhang, Yu Xie, Hang Bai, Bin Yu, Weihong Li, and Yuan Gao. 2021.

</span>
<span class="ltx_bibblock">A survey on federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib180.3.1" class="ltx_emph ltx_font_italic">Knowledge-Based Systems</em> 216 (2021), 106775.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib181.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Jie Zhang, Chen Chen, Bo Li, Lingjuan Lyu, Shuang Wu, Shouhong Ding, Chunhua Shen, and Chao Wu. 2022a.

</span>
<span class="ltx_bibblock">Dense: Data-free one-shot federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib181.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 35 (2022), 21414–21428.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib182.2.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Lin Zhang, Li Shen, Liang Ding, Dacheng Tao, and Ling-Yu Duan. 2022b.

</span>
<span class="ltx_bibblock">Fine-tuning global model via data-free knowledge distillation for non-iid federated learning. In <em id="bib.bib182.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 10174–10183.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib183.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. 2019.

</span>
<span class="ltx_bibblock">Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In <em id="bib.bib183.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer vision</em>. 3713–3722.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib184.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Ying Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. 2018.

</span>
<span class="ltx_bibblock">Deep mutual learning. In <em id="bib.bib184.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 4320–4328.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span id="bib.bib185.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. 2020.

</span>
<span class="ltx_bibblock">idlg: Improved deep leakage from gradients.

</span>
<span class="ltx_bibblock"><em id="bib.bib185.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.02610</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span id="bib.bib186.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. 2018.

</span>
<span class="ltx_bibblock">Federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib186.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.00582</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span id="bib.bib187.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiaolin Zheng, Senci Ying, Fei Zheng, Jianwei Yin, Longfei Zheng, Chaochao Chen, and Fengqin Dong. 2023.

</span>
<span class="ltx_bibblock">Federated Learning on Non-iid Data via Local and Global Distillation. In <em id="bib.bib187.3.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Web Services, ICWS 2023, Chicago, IL, USA, July 2-8, 2023</em>, Claudio A. Ardagna, Boualem Benatallah, Hongyi Bian, Carl K. Chang, Rong N. Chang, Jing Fan, Geoffrey C. Fox, Zhi Jin, Xuanzhe Liu, Heiko Ludwig, Michael Sheng, and Jian Yang (Eds.). IEEE, 647–657.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ICWS60048.2023.00083" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICWS60048.2023.00083</a>

</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span id="bib.bib188.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Yanlin Zhou, Xiyao Ma, Dapeng Wu, and Xiaolin Li. 2022.

</span>
<span class="ltx_bibblock">Communication-Efficient and Attack-Resistant Federated Edge Learning with Dataset Distillation.

</span>
<span class="ltx_bibblock"><em id="bib.bib188.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Cloud Computing</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span id="bib.bib189.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yanlin Zhou, George Pu, Xiyao Ma, Xiaolin Li, and Dapeng Wu. 2020.

</span>
<span class="ltx_bibblock">Distilled one-shot federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib189.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.07999</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span id="bib.bib190.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Zirui Zhou, Lingyang Chu, Changxin Liu, Lanjun Wang, Jian Pei, and Yong Zhang. 2021.

</span>
<span class="ltx_bibblock">Towards fair federated learning. In <em id="bib.bib190.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</em>. 4100–4101.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou (2021)</span>
<span class="ltx_bibblock">
Zhi-Hua Zhou. 2021.

</span>
<span class="ltx_bibblock">Why over-parameterization of deep neural networks does not overfit?

</span>
<span class="ltx_bibblock"><em id="bib.bib191.1.1" class="ltx_emph ltx_font_italic">Science China Information Sciences</em> 64 (2021), 1–3.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span id="bib.bib192.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Hangyu Zhu, Jinjin Xu, Shiqing Liu, and Yaochu Jin. 2021b.

</span>
<span class="ltx_bibblock">Federated learning on non-IID data: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib192.3.1" class="ltx_emph ltx_font_italic">Neurocomputing</em> 465 (2021), 371–390.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span id="bib.bib193.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Ligeng Zhu, Zhijian Liu, and Song Han. 2019.

</span>
<span class="ltx_bibblock">Deep leakage from gradients.

</span>
<span class="ltx_bibblock"><em id="bib.bib193.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 32 (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span id="bib.bib194.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhuangdi Zhu, Junyuan Hong, Steve Drew, and Jiayu Zhou. 2022.

</span>
<span class="ltx_bibblock">Resilient and communication efficient learning for heterogeneous federated systems. In <em id="bib.bib194.3.1" class="ltx_emph ltx_font_italic">Proceedings of Thirty-ninth International Conference on Machine Learning (ICML 2022)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span id="bib.bib195.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. 2021a.

</span>
<span class="ltx_bibblock">Data-free knowledge distillation for heterogeneous federated learning. In <em id="bib.bib195.3.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>. PMLR, 12878–12889.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuang et al<span id="bib.bib196.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Weiming Zhuang, Xin Gan, Yonggang Wen, and Shuai Zhang. 2023.

</span>
<span class="ltx_bibblock">Optimizing performance of federated person re-identification: Benchmarking and analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib196.3.1" class="ltx_emph ltx_font_italic">ACM Transactions on Multimedia Computing, Communications and Applications</em> 19, 1s (2023), 1–18.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.10860" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.10861" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.10861">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.10861" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.10862" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 19:24:53 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
