<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Evaluating authenticity and quality of image captions via sentiment and semantic analyses</title>
<!--Generated on Sat Sep 14 23:41:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.09560v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#S1" title="In Evaluating authenticity and quality of image captions via sentiment and semantic analyses"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#S2" title="In Evaluating authenticity and quality of image captions via sentiment and semantic analyses"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Methods</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#S3" title="In Evaluating authenticity and quality of image captions via sentiment and semantic analyses"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Results</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#S4" title="In Evaluating authenticity and quality of image captions via sentiment and semantic analyses"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
Evaluating authenticity and quality of image captions via
sentiment and semantic analyses 
<br class="ltx_break"/>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aleksei Krotov<sup class="ltx_sup" id="id7.7.id1">1</sup>, Alison Tebo<sup class="ltx_sup" id="id8.8.id2">2</sup>, Dylan K. Picart<sup class="ltx_sup" id="id9.9.id3">3</sup>, Aaron Dean Algave
</span><span class="ltx_author_notes">*This work was facilitated by Neuromatch Academy DeepLearning course. Authors thank Joseph Akinyemi and Erum Afzal for their mentorship during coursework and project work.<sup class="ltx_sup" id="id10.10.id1"><span class="ltx_text ltx_font_italic" id="id10.10.id1.1">1</span></sup>Department of Bioengineering, Northeastern University, Boston, MA, <sup class="ltx_sup" id="id11.11.id2"><span class="ltx_text ltx_font_italic" id="id11.11.id2.1">2</span></sup>HHMI - Janelia Research Campus, Ashburn, VA, <sup class="ltx_sup" id="id12.12.id3"><span class="ltx_text ltx_font_italic" id="id12.12.id3.1">3</span></sup>Innovation Fellowship for Data Science, The Knowledge House, The Bronx, NY,
<span class="ltx_text ltx_font_typewriter" id="id13.13.id4" style="font-size:90%;">bronze.eye@gmail.com, teboa@janelia.hhmi.org, dylankpicart@gmail.com, algaveaaron@gmail.com</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id14.id1">The growth of deep learning (DL) relies heavily on huge amounts of labelled data for tasks such as natural language processing and computer vision. Specifically, in image-to-text or image-to-image pipelines, opinion (sentiment) may be inadvertently learned by a model from human-generated image captions. Additionally, learning may be affected by the variety and diversity of the provided captions. While labelling large datasets has largely relied on crowd-sourcing or data-worker pools, evaluating the quality of such training data is crucial.</p>
<p class="ltx_p" id="id15.id2">This study proposes an evaluation method focused on sentiment and semantic richness. That method was applied to the COCO-MS dataset, comprising approximately 150K images with segmented objects and corresponding crowd-sourced captions. We employed pre-trained models (Twitter-RoBERTa-base and BERT-base) to extract sentiment scores and variability of semantic embeddings from captions. The relation of the sentiment score and semantic variability with object categories was examined using multiple linear regression. Results indicate that while most captions were neutral, about 6% of the captions exhibited strong sentiment influenced by specific object categories. Semantic variability of within-image captions remained low and uncorrelated with object categories. Model-generated captions showed less than 1.5% of strong sentiment which was not influenced by object categories and did not correlate with the sentiment of the respective human-generated captions. This research demonstrates an approach to assess the quality of crowd- or worker-sourced captions informed by image content.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Deep learning (DL) research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#bib.bib1" title="">1</a>]</cite> has used a growing amount of worker-pooled or crowd-sourced labeling effort. Authenticity and quality of the such data are essential for training and evaluating the rapidly growing models in computer vision, natural language processing and other areas<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#bib.bib3" title="">3</a>]</cite>. However, systematic quality assessment of such data has remained limited <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this study we proposed such evaluation of human-generated data labels, focusing on the positive-negative sentiment analysis and semantic variance across the raters. To demonstrate the proposed approach, we applied it to the COCO-MS dataset, using human-generated captions and segmented objects.
We next discuss the results which indicate a presence of strong sentiments in the captions, which were associated with the object categories, yet little semantic diversity across captions.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Methods</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">We used the COCO-MS dataset consisting of about 150K images where 4-6 captions were sourced from data worker pool, and a thorough object segmentation was performed by a similar effort <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#bib.bib6" title="">6</a>]</cite>.
Each caption was a single-sentence description of an image, and 80 object categories and 12 supercategories classified the segmented objects in the same image.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The captions were matched with the object categories via image ID. Sentiment analysis was performed on each caption using a pre-trained model Twitter-roBERTa-base. Sentiment score between -1 and 1 was computed as a difference between positive and negative confidence values <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#bib.bib7" title="">7</a>]</cite>. Strong sentiments were defined when the caption’s score was larger than 0.5 or less than -0.5. The sentiment score was then examined against the presence or absence of object categories in the images. To that end, a subset of the data with strong sentiments was used. Influence of one-hot-encoded category presence on sentiment score was assessed via multiple linear regression (MLR) with first-order terms.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.2">Semantic analysis was performed on a set of the BERT-embeddings of the captions of each image. Variability was defined as the standard deviation of the cosine-similarity metric across the embeddings of the same image. To approximate that value without explicitly defining the mean in the cosine space, pairwise cosine similarities <math alttext="S_{c}(a_{i},a_{j})" class="ltx_Math" display="inline" id="S2.p3.1.m1.2"><semantics id="S2.p3.1.m1.2a"><mrow id="S2.p3.1.m1.2.2" xref="S2.p3.1.m1.2.2.cmml"><msub id="S2.p3.1.m1.2.2.4" xref="S2.p3.1.m1.2.2.4.cmml"><mi id="S2.p3.1.m1.2.2.4.2" xref="S2.p3.1.m1.2.2.4.2.cmml">S</mi><mi id="S2.p3.1.m1.2.2.4.3" xref="S2.p3.1.m1.2.2.4.3.cmml">c</mi></msub><mo id="S2.p3.1.m1.2.2.3" xref="S2.p3.1.m1.2.2.3.cmml">⁢</mo><mrow id="S2.p3.1.m1.2.2.2.2" xref="S2.p3.1.m1.2.2.2.3.cmml"><mo id="S2.p3.1.m1.2.2.2.2.3" stretchy="false" xref="S2.p3.1.m1.2.2.2.3.cmml">(</mo><msub id="S2.p3.1.m1.1.1.1.1.1" xref="S2.p3.1.m1.1.1.1.1.1.cmml"><mi id="S2.p3.1.m1.1.1.1.1.1.2" xref="S2.p3.1.m1.1.1.1.1.1.2.cmml">a</mi><mi id="S2.p3.1.m1.1.1.1.1.1.3" xref="S2.p3.1.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.p3.1.m1.2.2.2.2.4" xref="S2.p3.1.m1.2.2.2.3.cmml">,</mo><msub id="S2.p3.1.m1.2.2.2.2.2" xref="S2.p3.1.m1.2.2.2.2.2.cmml"><mi id="S2.p3.1.m1.2.2.2.2.2.2" xref="S2.p3.1.m1.2.2.2.2.2.2.cmml">a</mi><mi id="S2.p3.1.m1.2.2.2.2.2.3" xref="S2.p3.1.m1.2.2.2.2.2.3.cmml">j</mi></msub><mo id="S2.p3.1.m1.2.2.2.2.5" stretchy="false" xref="S2.p3.1.m1.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.2b"><apply id="S2.p3.1.m1.2.2.cmml" xref="S2.p3.1.m1.2.2"><times id="S2.p3.1.m1.2.2.3.cmml" xref="S2.p3.1.m1.2.2.3"></times><apply id="S2.p3.1.m1.2.2.4.cmml" xref="S2.p3.1.m1.2.2.4"><csymbol cd="ambiguous" id="S2.p3.1.m1.2.2.4.1.cmml" xref="S2.p3.1.m1.2.2.4">subscript</csymbol><ci id="S2.p3.1.m1.2.2.4.2.cmml" xref="S2.p3.1.m1.2.2.4.2">𝑆</ci><ci id="S2.p3.1.m1.2.2.4.3.cmml" xref="S2.p3.1.m1.2.2.4.3">𝑐</ci></apply><interval closure="open" id="S2.p3.1.m1.2.2.2.3.cmml" xref="S2.p3.1.m1.2.2.2.2"><apply id="S2.p3.1.m1.1.1.1.1.1.cmml" xref="S2.p3.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p3.1.m1.1.1.1.1.1.1.cmml" xref="S2.p3.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S2.p3.1.m1.1.1.1.1.1.2.cmml" xref="S2.p3.1.m1.1.1.1.1.1.2">𝑎</ci><ci id="S2.p3.1.m1.1.1.1.1.1.3.cmml" xref="S2.p3.1.m1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S2.p3.1.m1.2.2.2.2.2.cmml" xref="S2.p3.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.p3.1.m1.2.2.2.2.2.1.cmml" xref="S2.p3.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S2.p3.1.m1.2.2.2.2.2.2.cmml" xref="S2.p3.1.m1.2.2.2.2.2.2">𝑎</ci><ci id="S2.p3.1.m1.2.2.2.2.2.3.cmml" xref="S2.p3.1.m1.2.2.2.2.2.3">𝑗</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.2c">S_{c}(a_{i},a_{j})</annotation><annotation encoding="application/x-llamapun" id="S2.p3.1.m1.2d">italic_S start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )</annotation></semantics></math> for i-th and j-th captions were used in the following fashion: <math alttext="s=\sqrt{\frac{1}{N(N-1)}\sum_{i=1}^{N}\sum_{j=1}^{N-1}S_{c}(a_{i},a_{j})^{2}}" class="ltx_Math" display="inline" id="S2.p3.2.m2.3"><semantics id="S2.p3.2.m2.3a"><mrow id="S2.p3.2.m2.3.4" xref="S2.p3.2.m2.3.4.cmml"><mi id="S2.p3.2.m2.3.4.2" xref="S2.p3.2.m2.3.4.2.cmml">s</mi><mo id="S2.p3.2.m2.3.4.1" xref="S2.p3.2.m2.3.4.1.cmml">=</mo><msqrt id="S2.p3.2.m2.3.3" xref="S2.p3.2.m2.3.3.cmml"><mrow id="S2.p3.2.m2.3.3.3" xref="S2.p3.2.m2.3.3.3.cmml"><mfrac id="S2.p3.2.m2.1.1.1.1" xref="S2.p3.2.m2.1.1.1.1.cmml"><mn id="S2.p3.2.m2.1.1.1.1.3" xref="S2.p3.2.m2.1.1.1.1.3.cmml">1</mn><mrow id="S2.p3.2.m2.1.1.1.1.1" xref="S2.p3.2.m2.1.1.1.1.1.cmml"><mi id="S2.p3.2.m2.1.1.1.1.1.3" xref="S2.p3.2.m2.1.1.1.1.1.3.cmml">N</mi><mo id="S2.p3.2.m2.1.1.1.1.1.2" xref="S2.p3.2.m2.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.p3.2.m2.1.1.1.1.1.1.1" xref="S2.p3.2.m2.1.1.1.1.1.1.1.1.cmml"><mo id="S2.p3.2.m2.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.p3.2.m2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.p3.2.m2.1.1.1.1.1.1.1.1" xref="S2.p3.2.m2.1.1.1.1.1.1.1.1.cmml"><mi id="S2.p3.2.m2.1.1.1.1.1.1.1.1.2" xref="S2.p3.2.m2.1.1.1.1.1.1.1.1.2.cmml">N</mi><mo id="S2.p3.2.m2.1.1.1.1.1.1.1.1.1" xref="S2.p3.2.m2.1.1.1.1.1.1.1.1.1.cmml">−</mo><mn id="S2.p3.2.m2.1.1.1.1.1.1.1.1.3" xref="S2.p3.2.m2.1.1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S2.p3.2.m2.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.p3.2.m2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mfrac><mo id="S2.p3.2.m2.3.3.3.4" xref="S2.p3.2.m2.3.3.3.4.cmml">⁢</mo><mrow id="S2.p3.2.m2.3.3.3.3" xref="S2.p3.2.m2.3.3.3.3.cmml"><msubsup id="S2.p3.2.m2.3.3.3.3.3" xref="S2.p3.2.m2.3.3.3.3.3.cmml"><mo id="S2.p3.2.m2.3.3.3.3.3.2.2" rspace="0em" xref="S2.p3.2.m2.3.3.3.3.3.2.2.cmml">∑</mo><mrow id="S2.p3.2.m2.3.3.3.3.3.2.3" xref="S2.p3.2.m2.3.3.3.3.3.2.3.cmml"><mi id="S2.p3.2.m2.3.3.3.3.3.2.3.2" xref="S2.p3.2.m2.3.3.3.3.3.2.3.2.cmml">i</mi><mo id="S2.p3.2.m2.3.3.3.3.3.2.3.1" xref="S2.p3.2.m2.3.3.3.3.3.2.3.1.cmml">=</mo><mn id="S2.p3.2.m2.3.3.3.3.3.2.3.3" xref="S2.p3.2.m2.3.3.3.3.3.2.3.3.cmml">1</mn></mrow><mi id="S2.p3.2.m2.3.3.3.3.3.3" xref="S2.p3.2.m2.3.3.3.3.3.3.cmml">N</mi></msubsup><mrow id="S2.p3.2.m2.3.3.3.3.2" xref="S2.p3.2.m2.3.3.3.3.2.cmml"><msubsup id="S2.p3.2.m2.3.3.3.3.2.3" xref="S2.p3.2.m2.3.3.3.3.2.3.cmml"><mo id="S2.p3.2.m2.3.3.3.3.2.3.2.2" xref="S2.p3.2.m2.3.3.3.3.2.3.2.2.cmml">∑</mo><mrow id="S2.p3.2.m2.3.3.3.3.2.3.2.3" xref="S2.p3.2.m2.3.3.3.3.2.3.2.3.cmml"><mi id="S2.p3.2.m2.3.3.3.3.2.3.2.3.2" xref="S2.p3.2.m2.3.3.3.3.2.3.2.3.2.cmml">j</mi><mo id="S2.p3.2.m2.3.3.3.3.2.3.2.3.1" xref="S2.p3.2.m2.3.3.3.3.2.3.2.3.1.cmml">=</mo><mn id="S2.p3.2.m2.3.3.3.3.2.3.2.3.3" xref="S2.p3.2.m2.3.3.3.3.2.3.2.3.3.cmml">1</mn></mrow><mrow id="S2.p3.2.m2.3.3.3.3.2.3.3" xref="S2.p3.2.m2.3.3.3.3.2.3.3.cmml"><mi id="S2.p3.2.m2.3.3.3.3.2.3.3.2" xref="S2.p3.2.m2.3.3.3.3.2.3.3.2.cmml">N</mi><mo id="S2.p3.2.m2.3.3.3.3.2.3.3.1" xref="S2.p3.2.m2.3.3.3.3.2.3.3.1.cmml">−</mo><mn id="S2.p3.2.m2.3.3.3.3.2.3.3.3" xref="S2.p3.2.m2.3.3.3.3.2.3.3.3.cmml">1</mn></mrow></msubsup><mrow id="S2.p3.2.m2.3.3.3.3.2.2" xref="S2.p3.2.m2.3.3.3.3.2.2.cmml"><msub id="S2.p3.2.m2.3.3.3.3.2.2.4" xref="S2.p3.2.m2.3.3.3.3.2.2.4.cmml"><mi id="S2.p3.2.m2.3.3.3.3.2.2.4.2" xref="S2.p3.2.m2.3.3.3.3.2.2.4.2.cmml">S</mi><mi id="S2.p3.2.m2.3.3.3.3.2.2.4.3" xref="S2.p3.2.m2.3.3.3.3.2.2.4.3.cmml">c</mi></msub><mo id="S2.p3.2.m2.3.3.3.3.2.2.3" xref="S2.p3.2.m2.3.3.3.3.2.2.3.cmml">⁢</mo><msup id="S2.p3.2.m2.3.3.3.3.2.2.2" xref="S2.p3.2.m2.3.3.3.3.2.2.2.cmml"><mrow id="S2.p3.2.m2.3.3.3.3.2.2.2.2.2" xref="S2.p3.2.m2.3.3.3.3.2.2.2.2.3.cmml"><mo id="S2.p3.2.m2.3.3.3.3.2.2.2.2.2.3" stretchy="false" xref="S2.p3.2.m2.3.3.3.3.2.2.2.2.3.cmml">(</mo><msub id="S2.p3.2.m2.2.2.2.2.1.1.1.1.1.1" xref="S2.p3.2.m2.2.2.2.2.1.1.1.1.1.1.cmml"><mi id="S2.p3.2.m2.2.2.2.2.1.1.1.1.1.1.2" xref="S2.p3.2.m2.2.2.2.2.1.1.1.1.1.1.2.cmml">a</mi><mi id="S2.p3.2.m2.2.2.2.2.1.1.1.1.1.1.3" xref="S2.p3.2.m2.2.2.2.2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.p3.2.m2.3.3.3.3.2.2.2.2.2.4" xref="S2.p3.2.m2.3.3.3.3.2.2.2.2.3.cmml">,</mo><msub id="S2.p3.2.m2.3.3.3.3.2.2.2.2.2.2" xref="S2.p3.2.m2.3.3.3.3.2.2.2.2.2.2.cmml"><mi id="S2.p3.2.m2.3.3.3.3.2.2.2.2.2.2.2" xref="S2.p3.2.m2.3.3.3.3.2.2.2.2.2.2.2.cmml">a</mi><mi id="S2.p3.2.m2.3.3.3.3.2.2.2.2.2.2.3" xref="S2.p3.2.m2.3.3.3.3.2.2.2.2.2.2.3.cmml">j</mi></msub><mo id="S2.p3.2.m2.3.3.3.3.2.2.2.2.2.5" stretchy="false" xref="S2.p3.2.m2.3.3.3.3.2.2.2.2.3.cmml">)</mo></mrow><mn id="S2.p3.2.m2.3.3.3.3.2.2.2.4" xref="S2.p3.2.m2.3.3.3.3.2.2.2.4.cmml">2</mn></msup></mrow></mrow></mrow></mrow></msqrt></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.3b"><apply id="S2.p3.2.m2.3.4.cmml" xref="S2.p3.2.m2.3.4"><eq id="S2.p3.2.m2.3.4.1.cmml" xref="S2.p3.2.m2.3.4.1"></eq><ci id="S2.p3.2.m2.3.4.2.cmml" xref="S2.p3.2.m2.3.4.2">𝑠</ci><apply id="S2.p3.2.m2.3.3.cmml" xref="S2.p3.2.m2.3.3"><root id="S2.p3.2.m2.3.3a.cmml" xref="S2.p3.2.m2.3.3"></root><apply id="S2.p3.2.m2.3.3.3.cmml" xref="S2.p3.2.m2.3.3.3"><times id="S2.p3.2.m2.3.3.3.4.cmml" xref="S2.p3.2.m2.3.3.3.4"></times><apply id="S2.p3.2.m2.1.1.1.1.cmml" xref="S2.p3.2.m2.1.1.1.1"><divide id="S2.p3.2.m2.1.1.1.1.2.cmml" xref="S2.p3.2.m2.1.1.1.1"></divide><cn id="S2.p3.2.m2.1.1.1.1.3.cmml" type="integer" xref="S2.p3.2.m2.1.1.1.1.3">1</cn><apply id="S2.p3.2.m2.1.1.1.1.1.cmml" xref="S2.p3.2.m2.1.1.1.1.1"><times id="S2.p3.2.m2.1.1.1.1.1.2.cmml" xref="S2.p3.2.m2.1.1.1.1.1.2"></times><ci id="S2.p3.2.m2.1.1.1.1.1.3.cmml" xref="S2.p3.2.m2.1.1.1.1.1.3">𝑁</ci><apply id="S2.p3.2.m2.1.1.1.1.1.1.1.1.cmml" xref="S2.p3.2.m2.1.1.1.1.1.1.1"><minus id="S2.p3.2.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.p3.2.m2.1.1.1.1.1.1.1.1.1"></minus><ci id="S2.p3.2.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S2.p3.2.m2.1.1.1.1.1.1.1.1.2">𝑁</ci><cn id="S2.p3.2.m2.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S2.p3.2.m2.1.1.1.1.1.1.1.1.3">1</cn></apply></apply></apply><apply id="S2.p3.2.m2.3.3.3.3.cmml" xref="S2.p3.2.m2.3.3.3.3"><apply id="S2.p3.2.m2.3.3.3.3.3.cmml" xref="S2.p3.2.m2.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.p3.2.m2.3.3.3.3.3.1.cmml" xref="S2.p3.2.m2.3.3.3.3.3">superscript</csymbol><apply id="S2.p3.2.m2.3.3.3.3.3.2.cmml" xref="S2.p3.2.m2.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.p3.2.m2.3.3.3.3.3.2.1.cmml" xref="S2.p3.2.m2.3.3.3.3.3">subscript</csymbol><sum id="S2.p3.2.m2.3.3.3.3.3.2.2.cmml" xref="S2.p3.2.m2.3.3.3.3.3.2.2"></sum><apply id="S2.p3.2.m2.3.3.3.3.3.2.3.cmml" xref="S2.p3.2.m2.3.3.3.3.3.2.3"><eq id="S2.p3.2.m2.3.3.3.3.3.2.3.1.cmml" xref="S2.p3.2.m2.3.3.3.3.3.2.3.1"></eq><ci id="S2.p3.2.m2.3.3.3.3.3.2.3.2.cmml" xref="S2.p3.2.m2.3.3.3.3.3.2.3.2">𝑖</ci><cn id="S2.p3.2.m2.3.3.3.3.3.2.3.3.cmml" type="integer" xref="S2.p3.2.m2.3.3.3.3.3.2.3.3">1</cn></apply></apply><ci id="S2.p3.2.m2.3.3.3.3.3.3.cmml" xref="S2.p3.2.m2.3.3.3.3.3.3">𝑁</ci></apply><apply id="S2.p3.2.m2.3.3.3.3.2.cmml" xref="S2.p3.2.m2.3.3.3.3.2"><apply id="S2.p3.2.m2.3.3.3.3.2.3.cmml" xref="S2.p3.2.m2.3.3.3.3.2.3"><csymbol cd="ambiguous" id="S2.p3.2.m2.3.3.3.3.2.3.1.cmml" xref="S2.p3.2.m2.3.3.3.3.2.3">superscript</csymbol><apply id="S2.p3.2.m2.3.3.3.3.2.3.2.cmml" xref="S2.p3.2.m2.3.3.3.3.2.3"><csymbol cd="ambiguous" id="S2.p3.2.m2.3.3.3.3.2.3.2.1.cmml" xref="S2.p3.2.m2.3.3.3.3.2.3">subscript</csymbol><sum id="S2.p3.2.m2.3.3.3.3.2.3.2.2.cmml" xref="S2.p3.2.m2.3.3.3.3.2.3.2.2"></sum><apply id="S2.p3.2.m2.3.3.3.3.2.3.2.3.cmml" xref="S2.p3.2.m2.3.3.3.3.2.3.2.3"><eq id="S2.p3.2.m2.3.3.3.3.2.3.2.3.1.cmml" xref="S2.p3.2.m2.3.3.3.3.2.3.2.3.1"></eq><ci id="S2.p3.2.m2.3.3.3.3.2.3.2.3.2.cmml" xref="S2.p3.2.m2.3.3.3.3.2.3.2.3.2">𝑗</ci><cn id="S2.p3.2.m2.3.3.3.3.2.3.2.3.3.cmml" type="integer" xref="S2.p3.2.m2.3.3.3.3.2.3.2.3.3">1</cn></apply></apply><apply id="S2.p3.2.m2.3.3.3.3.2.3.3.cmml" xref="S2.p3.2.m2.3.3.3.3.2.3.3"><minus id="S2.p3.2.m2.3.3.3.3.2.3.3.1.cmml" xref="S2.p3.2.m2.3.3.3.3.2.3.3.1"></minus><ci id="S2.p3.2.m2.3.3.3.3.2.3.3.2.cmml" xref="S2.p3.2.m2.3.3.3.3.2.3.3.2">𝑁</ci><cn id="S2.p3.2.m2.3.3.3.3.2.3.3.3.cmml" type="integer" xref="S2.p3.2.m2.3.3.3.3.2.3.3.3">1</cn></apply></apply><apply id="S2.p3.2.m2.3.3.3.3.2.2.cmml" xref="S2.p3.2.m2.3.3.3.3.2.2"><times id="S2.p3.2.m2.3.3.3.3.2.2.3.cmml" xref="S2.p3.2.m2.3.3.3.3.2.2.3"></times><apply id="S2.p3.2.m2.3.3.3.3.2.2.4.cmml" xref="S2.p3.2.m2.3.3.3.3.2.2.4"><csymbol cd="ambiguous" id="S2.p3.2.m2.3.3.3.3.2.2.4.1.cmml" xref="S2.p3.2.m2.3.3.3.3.2.2.4">subscript</csymbol><ci id="S2.p3.2.m2.3.3.3.3.2.2.4.2.cmml" xref="S2.p3.2.m2.3.3.3.3.2.2.4.2">𝑆</ci><ci id="S2.p3.2.m2.3.3.3.3.2.2.4.3.cmml" xref="S2.p3.2.m2.3.3.3.3.2.2.4.3">𝑐</ci></apply><apply id="S2.p3.2.m2.3.3.3.3.2.2.2.cmml" xref="S2.p3.2.m2.3.3.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.p3.2.m2.3.3.3.3.2.2.2.3.cmml" xref="S2.p3.2.m2.3.3.3.3.2.2.2">superscript</csymbol><interval closure="open" id="S2.p3.2.m2.3.3.3.3.2.2.2.2.3.cmml" xref="S2.p3.2.m2.3.3.3.3.2.2.2.2.2"><apply id="S2.p3.2.m2.2.2.2.2.1.1.1.1.1.1.cmml" xref="S2.p3.2.m2.2.2.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p3.2.m2.2.2.2.2.1.1.1.1.1.1.1.cmml" xref="S2.p3.2.m2.2.2.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S2.p3.2.m2.2.2.2.2.1.1.1.1.1.1.2.cmml" xref="S2.p3.2.m2.2.2.2.2.1.1.1.1.1.1.2">𝑎</ci><ci id="S2.p3.2.m2.2.2.2.2.1.1.1.1.1.1.3.cmml" xref="S2.p3.2.m2.2.2.2.2.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S2.p3.2.m2.3.3.3.3.2.2.2.2.2.2.cmml" xref="S2.p3.2.m2.3.3.3.3.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.p3.2.m2.3.3.3.3.2.2.2.2.2.2.1.cmml" xref="S2.p3.2.m2.3.3.3.3.2.2.2.2.2.2">subscript</csymbol><ci id="S2.p3.2.m2.3.3.3.3.2.2.2.2.2.2.2.cmml" xref="S2.p3.2.m2.3.3.3.3.2.2.2.2.2.2.2">𝑎</ci><ci id="S2.p3.2.m2.3.3.3.3.2.2.2.2.2.2.3.cmml" xref="S2.p3.2.m2.3.3.3.3.2.2.2.2.2.2.3">𝑗</ci></apply></interval><cn id="S2.p3.2.m2.3.3.3.3.2.2.2.4.cmml" type="integer" xref="S2.p3.2.m2.3.3.3.3.2.2.2.4">2</cn></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.3c">s=\sqrt{\frac{1}{N(N-1)}\sum_{i=1}^{N}\sum_{j=1}^{N-1}S_{c}(a_{i},a_{j})^{2}}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.2.m2.3d">italic_s = square-root start_ARG divide start_ARG 1 end_ARG start_ARG italic_N ( italic_N - 1 ) end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N - 1 end_POSTSUPERSCRIPT italic_S start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG</annotation></semantics></math></p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">To examine a possible transfer of image-specific sentiment from the dataset to the model, the sentiment score was additionally compared to that of the model-generated captions for the same images. For producing captions, we used the Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (BLIP) model for image captioning(<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#bib.bib8" title="">8</a>]</cite>), which was trained on the COCO dataset. The train image dataset was used to generate captions with a minimum length of 8 words.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Results</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Most of the captions were found to carry a neutral sentiment (<a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#S3.F1" title="Figure 1 ‣ III Results ‣ Evaluating authenticity and quality of image captions via sentiment and semantic analyses"><span class="ltx_text ltx_ref_tag">1</span></a>A), while about 6% (32K) of the captions had either strong negative or strong positive sentiments, that correspond to 25K unique images. About 3.5K images had two strong captions, 800 had three strong captions, and 400 had three to five strong captions. The score mean and standard deviation per image did not appear to cluster in a two-dimensional space with respect to a category or a supercategory.
Unlike the long-tailed distribution of caption sentiments, the semantic variability within-image showed normal distribution with mean close to 0.6 (<a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#S3.F1" title="Figure 1 ‣ III Results ‣ Evaluating authenticity and quality of image captions via sentiment and semantic analyses"><span class="ltx_text ltx_ref_tag">1</span></a>B).</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="476" id="S3.F1.g1" src="extracted/5836679/Figures/Figure1.2.Sentiment_and_Semantic_histograms.png" width="277"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A: Distribution of Sentiment scores of human-generated captions of the COCO dataset. B: Distribution of Semantic variability of human-generated captions of the COCO dataset.</figcaption>
</figure>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">In the subset of the captions with strong sentiment (<a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#S3.F2" title="Figure 2 ‣ III Results ‣ Evaluating authenticity and quality of image captions via sentiment and semantic analyses"><span class="ltx_text ltx_ref_tag">2</span></a>A), a linear model fit by MLR explained 24% of the data variance and indicated that the presence or absence of most of the categories significantly affected the sentiment score. The results are summarized in <a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#S3.F2" title="Figure 2 ‣ III Results ‣ Evaluating authenticity and quality of image captions via sentiment and semantic analyses"><span class="ltx_text ltx_ref_tag">2</span></a>B which shows the coefficient, or slope, of each one-hot-encoded category with respect to the sentiment score, and asterisks denote the coefficients which were significantly different from zero with <span class="ltx_text ltx_font_italic" id="S3.p2.1.1">p</span> &lt; 0.01.
Semantic variability and sentiment score showed no correlation, with Pearson’s <span class="ltx_text ltx_font_italic" id="S3.p2.1.2">r</span> = 0.01.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Model-generated captions showed less-skewed distribution (<a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#S3.F3" title="Figure 3 ‣ III Results ‣ Evaluating authenticity and quality of image captions via sentiment and semantic analyses"><span class="ltx_text ltx_ref_tag">3</span></a>A), than their human-generated counterparts. Only about 1.5% of them (1800) indicated strong captions (<a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#S3.F3" title="Figure 3 ‣ III Results ‣ Evaluating authenticity and quality of image captions via sentiment and semantic analyses"><span class="ltx_text ltx_ref_tag">3</span></a>B). The MLR fit to the subset of strong sentiments explained less than 13% of their variance (<a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#S3.F3" title="Figure 3 ‣ III Results ‣ Evaluating authenticity and quality of image captions via sentiment and semantic analyses"><span class="ltx_text ltx_ref_tag">3</span></a>C). Coefficient values corresponding to the contribution of each category’s presence or absence resulted in a vastly different list from that in the human data, and only four of 80 were significantly different from zero.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="624" id="S3.F2.g1" src="extracted/5836679/Figures/Figure3.Sentiment_LM.png" width="277"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A: Distribution of Sentiment scores in the subset of strong-sentiment captions. B: Coefficients (slopes) of multiple linear regression of one-hot-encoded category vs. sentiment score. Asterisks mark coefficients significantly different from zero at p &lt; 0.01</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="758" id="S3.F3.g1" src="extracted/5836679/Figures/Figure5.AI-gen_Sentiment_LMs.png" width="256"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Distribution of Sentiment scores in model-generated captions, one per image.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Discussion</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Nearly 10 years after the publication of the dataset MS-COCO, automatic caption generation shows high quality with respect to various aspects being investigated and available caption styles <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#bib.bib9" title="">9</a>]</cite>. However, existing approaches remain dependent on human-generated reference captions. This work investigated two aspects of such reference captions, sentiment and semantic richness, and their relation with objects in the image.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Describing an image may be, in general, arbitrarily related to expressed sentiment. For objective captioning, however, weaker sentiment allows for conveying more factual information with less distraction. The workers providing captions for COCO dataset received instructions which promoted "describing important parts of the scene" but did not explicitly restrict or encourage sentiment expression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#bib.bib10" title="">10</a>]</cite>.
The caption relevance was then evaluated against some reference material via several semantic metrics (BLEU, Meteor, ROUGE and CIDEr) and via Precision at Human Recall, but was not seemingly inspected for expression of strong sentiment. Our study appears to be the first one addressing sentiment from the human-generating perspective, rather than evaluating model-generated instances.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">The distribution of sentiment peaking at zero reflects an objective generation of captions. It shows however two slight asymmetries: a slight positive skew around center, and a high-positive tail. While examining the relationship between all the captions and object categories resulted in less than 10% of variability explained, the object categories contributed to the captions with stronger sentiments, with a magnitude larger than 0.5.
Research suggested that the sentiment of a whole image may be predicted by the sentiment of one of more salient objects in the image, such as faces, human-made objects, close-up views, or indoor scenes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#bib.bib12" title="">12</a>]</cite>. Our results suggest that human faces did not affect caption sentiments much (correlation with "person" is relatively low), but that other, non-human factors did.
The training-data sentiment distribution asymmetry and its relation with objects may not necessarily transfer to synthetic captions. The approach employed in this work resulted in a tighter and more symmetric distribution of caption sentiments and a lack of relation to object categories. It remains, however, an aspect for potential scrutiny in new generative language models. One way of addressing that issue may be focusing on strongest-sentiment object categories and collecting additional data with the same categories but opposite sentiments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#bib.bib13" title="">13</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">Our approach of using cosine similarity of sentence BERT-embeddings is an example of a rapid off-the-shelf metric to quantify semantic difference and scale it to semantic variability. Semantic variability of captions showed a symmetric and narrow distribution, mostly contained between 0.5 and 0.65, and a lack of relation with object categories. Compared to standard deviation, used as variability, cosine similarity between embeddings of a pair of captions is slightly larger on average. With that and with non-negative pairwise distances, the observed distribution of standard deviation reflects high similarity and low variability across captions of each image. This may arise from the use of common words between workers and the employment of similar or parallel sentence structures. While this indicates a good agreement between the workers, it also suggests low semantic diversity of the captions therefore, limiting the variation of the models trained on those data.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">Research quantifying semantic diversity of human-sourced descriptions of situations suggests a relation with salient objects in a scene, which was not the case for our results <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09560v1#bib.bib15" title="">15</a>]</cite>. One likely explanation is that a lower semantic diversity and lack of its object-dependence are specific to the population of workers who generated the data and the nature of their task environment and instructions.
Taken together, the results suggest a cautious approach to crowd-sourcing caption-like data to ensure its objectiveness and diversity.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Y. Hirota, Y. Nakashima, and N. Garcia, “Quantifying Societal Bias Amplification in Image Captioning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">IEEE/CVF Computer Vision and Pattern Recognition(CVRP)</em>, 2022, pp. 13 450–13 459.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
C. Chai, J. Wang, Y. Luo, Z. Niu, and G. Li, “Data management for machine learning: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">IEEE Transactions on Knowledge and Data Engineering</em>, vol. 35, no. 5, pp. 4646–4667, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
I. F. Ilyas and T. Rekatsinas, “Machine learning and data cleaning: Which serves the other?” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">J. Data and Information Quality</em>, vol. 14, no. 3, jul 2022. [Online]. Available: https://doi.org/10.1145/3506712

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
C. Jörgensen, “Attributes of images in describing tasks,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Information Processing and Management</em>, vol. 34, no. 2-3, pp. 161–174, mar 1998. [Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/S0306457397000770

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
T. Ghandi, H. Pourreza, and H. Mahyar, “Deep Learning Approaches on Image Captioning: A Review,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">ACM Computing Surveys</em>, vol. 56, no. 3, pp. 1–39, mar 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B. Girshick, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick, “Microsoft COCO: Common Objects in Context,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">CoRR</em>, vol. abs/1405.0312, 2014. [Online]. Available: http://arxiv.org/abs/1405.0312

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
D. Loureiro, F. Barbieri, L. Neves, L. E. Anke, and J. Camacho-Collados, “Timelms: Diachronic language models from twitter,” <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">CoRR</em>, vol. abs/2202.03829, 2022. [Online]. Available: https://arxiv.org/abs/2202.03829

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J. Li, D. Li, C. Xiong, and S. Hoi, “Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation,” 2022. [Online]. Available: https://arxiv.org/abs/2201.12086

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
F. Klein, S. Mahajan, and S. Roth, “Diverse image captioning with grounded style,” 2022. [Online]. Available: https://arxiv.org/abs/2205.01813

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dollár, and C. L. Zitnick, “Microsoft COCO Captions: Data Collection and Evaluation Server,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">CoRR</em>, vol. abs/1504.00325, 2015. [Online]. Available: http://arxiv.org/abs/1504.00325

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
H. Zheng, T. Chen, Q. You, and J. Luo, “When saliency meets sentiment: Understanding how image content invokes emotion and sentiment,” in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">2017 IEEE International Conference on Image Processing (ICIP)</em>.   IEEE, sep 2017, pp. 630–634.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
M. Sun, J. Yang, K. Wang, and H. Shen, “Discovering affective regions in deep convolutional neural networks for visual sentiment prediction,” in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">2016 IEEE International Conference on Multimedia and Expo (ICME)</em>.   IEEE, jul 2016, pp. 1–6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Y. Mohammed, F. F. Khan, K. Haydarov, and M. Elhoseiny, “It is Okay to Not Be Okay: Overcoming Emotional Bias in Affective Image Captioning by Contrastive Data Collection,” in <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</em>, 2022, pp. 21 263–21 272.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
E. Takmaz, S. Pezzelle, and R. Fernández, “Describing images <span class="ltx_text ltx_markedasmath ltx_font_italic" id="bib.bib14.2.1">Fast and Slow</span>: Quantifying and predicting the variation in human signals during visuo-linguistic processes,” in <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.2">Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, Y. Graham and M. Purver, Eds.   St. Julian’s, Malta: Association for Computational Linguistics, Mar. 2024, pp. 2072–2087. [Online]. Available: https://aclanthology.org/2024.eacl-long.126

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
S. Parrigon, S. E. Woo, L. Tay, and T. Wang, “CAPTION-ing the situation: A lexically-derived taxonomy of psychological situation characteristics.” <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Journal of Personality and Social Psychology</em>, vol. 112, no. 4, pp. 642–681, apr 2017. [Online]. Available: https://doi.apa.org/doi/10.1037/pspp0000111

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Sep 14 23:41:00 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
