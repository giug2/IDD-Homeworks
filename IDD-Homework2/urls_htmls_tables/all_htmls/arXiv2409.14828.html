<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Two Deep Learning Solutions for Automatic Blurring of Faces in Videos</title>
<!--Generated on Mon Sep 23 08:55:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.14828v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S1" title="In Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S2" title="In Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Face Blurring using YOLO</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S2.SS1" title="In 2 Face Blurring using YOLO ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S2.SS2" title="In 2 Face Blurring using YOLO ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Dataset and Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S2.SS3" title="In 2 Face Blurring using YOLO ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Inference methodology</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S3" title="In Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Face Blurring using DeOldify</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S3.SS1" title="In 3 Face Blurring using DeOldify ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S3.SS2" title="In 3 Face Blurring using DeOldify ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Dataset for training</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S3.SS2.SSS1" title="In 3.2 Dataset for training ‣ 3 Face Blurring using DeOldify ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Datasets overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S3.SS2.SSS2" title="In 3.2 Dataset for training ‣ 3 Face Blurring using DeOldify ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Construction methodology</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S3.SS3" title="In 3 Face Blurring using DeOldify ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Training</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S3.SS3.SSS1" title="In 3.3 Training ‣ 3 Face Blurring using DeOldify ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Loss function</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S3.SS3.SSS2" title="In 3.3 Training ‣ 3 Face Blurring using DeOldify ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Training Procedure</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S3.SS4" title="In 3 Face Blurring using DeOldify ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Inference methodology</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4" title="In Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.SS1" title="In 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Evaluation Methodology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.SS2" title="In 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Experiments using DeOldify Unet</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.SS2.SSS1" title="In 4.2 Experiments using DeOldify Unet ‣ 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Influence of the downsampling dimension for inference</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.SS2.SSS2" title="In 4.2 Experiments using DeOldify Unet ‣ 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Influence of the loss function</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.SS2.SSS3" title="In 4.2 Experiments using DeOldify Unet ‣ 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Influence of the self-attention layer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.SS2.SSS4" title="In 4.2 Experiments using DeOldify Unet ‣ 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.4 </span>Experiments using YOLOV5Face</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.SS2.SSS5" title="In 4.2 Experiments using DeOldify Unet ‣ 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.5 </span>Computation time</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S5" title="In Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\Addlcwords</span>
<p class="ltx_p" id="p1.2">a an on and for the of in to with from which as his their at or those by its et ses des de du la en les aux le dans zu l0


</p>
</div>
<h1 class="ltx_title ltx_title_document">Two Deep Learning Solutions for Automatic Blurring of Faces in Videos</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Roman Plaud
<br class="ltx_break"/><span class="ltx_text" id="id1.1.id1" style="font-size:90%;">École des Ponts, ParisTech, France (<span class="ltx_text ltx_font_typewriter" id="id1.1.id1.1">plaud.roman@gmail.com</span>)</span>
<br class="ltx_break"/>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jose-Luis Lisani
<br class="ltx_break"/><span class="ltx_text" id="id2.1.id1" style="font-size:90%;">Universitat de les Illes Balears, Spain (<span class="ltx_text ltx_font_typewriter" id="id2.1.id1.1">joseluis.lisani@uib.es</span>)
<br class="ltx_break"/></span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">The widespread use of cameras in everyday life situations generates a vast amount of data that may contain
sensitive information about the people and vehicles moving in front of them (location, license plates, physical
characteristics, etc). In particular, people’s faces are recorded by surveillance cameras in public spaces.
In order to ensure the privacy of individuals, face blurring techniques can be applied to the collected videos.
In this paper we present two deep-learning based options to tackle the problem.
First, a direct approach, consisting of a classical object detector (based on the YOLO architecture)
trained to detect faces, which are subsequently blurred. Second, an indirect approach, in which a Unet-like segmentation network is trained to output a version of the input image in which all the faces have been blurred.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The emergence of laws that enforce the privacy of individuals
appearing in images and videos taken at public spaces
has led to the development of methods to anonymize visual data.
In this paper we address the problem of face anonymization, in particular
we seek to investigate methods to automatically blur people’s faces.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">We focus on the use of deep-learning techniques, and we investigate how
two popular architectures can help to tackle the problem.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">First we take a direct approach: detect the faces and then blur them in a post-processing step.
This can be achieved using an object detector and we choose in our tests the
well-known YOLO architecture, since it is fast and accurate and a version specifically
trained to detect faces is publicly available<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/deepcam-cn/yolov5-face" title="">https://github.com/deepcam-cn/yolov5-face</a></span></span></span>.
Section <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S2" title="2 Face Blurring using YOLO ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">2</span></a> gives details of the architecture, the training set and the post-processing
of the detections to obtain the final results.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We also investigate the use of another approach in which faces are not directly detected, but they
are directly blurred using a segmentation network. A recently proposed Unet-based network called
DeOldify<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/jantic/DeOldify" title="">https://github.com/jantic/DeOldify</a></span></span></span> has shown outstanding results for the colorization of gray images. An open source implementation of the method, together with a rigorous analysis of its steps, was
presented in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib9" title="">9</a>]</cite>. Even if the network is not designed to detect the objects or textures present
in an image, it has the ability to ascertain which is the appropriate color for each pixel of the images.
We wanted to check if this architecture, when trained with pairs of original and processed images (in which the faces had been blurred), was able to directly produce the blurred results, without the need of a detector
network. Details about DeOldify and its training are provided in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S3" title="3 Face Blurring using DeOldify ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Experiments comparing the results obtained with both approaches are displayed in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4" title="4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">4</span></a>
and some conclusions are exposed in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S5" title="5 Conclusion ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Face Blurring using YOLO</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we explore the world of face detection and the utilization of YOLOv5Face <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib5" title="">5</a>]</cite>, a powerful face detection model, to accomplish this task. Our objective is to detect faces within images and subsequently apply a blur to these detected faces. To do so we rely on <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib5" title="">5</a>]</cite>. We will explain the architecture of the YOLOv5Face model as well as its and training methodology. Then we will focus on the inference methodology employed to detect and blur faces effectively. 
<br class="ltx_break"/>YOLOv5Face is a model that draws its inspiration from the popular architecture known as YOLO (You Only Look Once). YOLO, developed by Joseph Redmon et al. in 2016 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib6" title="">6</a>]</cite>, revolutionized object detection algorithms with its real-time performance. YOLO introduced the concept of a single-stage object detector, which significantly reduced the computational complexity compared to traditional two-stage models like Faster R-CNN. YOLOv5Face builds upon this foundation, utilizing YOLO’s efficient architecture while specializing in facial detection tasks. By leveraging YOLO’s speed and accuracy, YOLOv5Face offers a highly effective and practical solution for detecting faces in various applications.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Architecture</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">YOLOv5Face is a convolutional neural network (CNN) which takes as input an image and returns coordinates of faces in that image. It returns also pixels values of 5 landmarks of detected faces, which we do not use in this study. These face coordinates are encoded as boxes, namely it returns pixel coordinates of the top left and bottom right points of the boxes. We show in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S2.F1" title="Figure 1 ‣ 2.1 Architecture ‣ 2 Face Blurring using YOLO ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">1</span></a> some results of YOLOv5Face.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="142" id="S2.F1.g1" src="extracted/5872765/figures/yolo.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">YOLOv5Face</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">As it is extensively detailed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib5" title="">5</a>]</cite>, we do not delve into details regarding the YOLOv5Face architecture. In a nutshell, it consists of the backbone, neck, and head whose architecture rely on YOLOv5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib11" title="">11</a>]</cite> as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S2.F2" title="Figure 2 ‣ 2.1 Architecture ‣ 2 Face Blurring using YOLO ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">2</span></a>. There exist different sizes of architecture from Nano to X-Large whose number of parameters range from 1.73M to 141.1M. To speed up the experiments, our implementation is based on the Nano model which is the tiniest one and has 1.73M parameters.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="312" id="S2.F2.g1" src="extracted/5872765/figures/yolov5.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">YOLOv5 architecture</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Dataset and Training</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">All models were trained on the WIDER dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib10" title="">10</a>]</cite>. Regarding the loss function, as for many YOLO models, it is a combination of losses: objectness score, class probability score, and bounding box regression score. In addition to these losses, a landmark regression loss was added to perform the 5 landmark regression. A SGD optimizer was used and the initial learning rate was set to 1e-2 (and a final learning rate of 1e-5). The weight decay was set to 5e-3. A
momentum of 0.8 was used in the first three warming-up epochs.
After that, the momentum was changed to 0.937. The training
run 250 epochs with a batch size of 64.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Inference methodology</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The YOLOv5Face is only a face detector so that we need postprocessing steps to blur faces. It is important to remark that, when the method is applied to a video, we operate at a frame level, without taking into account any temporal information. Let’s consider an image, we detail below how inference is performed and sum it up in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S2.F3" title="Figure 3 ‣ 2.3 Inference methodology ‣ 2 Face Blurring using YOLO ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="88" id="S2.F3.g1" src="extracted/5872765/figures/yolo_.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S2.F3.3.2" style="font-size:90%;">Yolo inference methodology</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS3.p2">
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">Face detection</span>. As explained, YOLOv5Face detects faces and returns a list of boxes coordinates of faces (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S2.F1" title="Figure 1 ‣ 2.1 Architecture ‣ 2 Face Blurring using YOLO ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">1</span></a>). The input image can be resized to speed up detection.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Transformation of boxes into ellipses</span>. To have a better rendering, we transform the detected boxes into ellipses. To do so, we consider the ellipses of angle <math alttext="0" class="ltx_Math" display="inline" id="S2.I1.i2.p1.1.m1.1"><semantics id="S2.I1.i2.p1.1.m1.1a"><mn id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.1b"><cn id="S2.I1.i2.p1.1.m1.1.1.cmml" type="integer" xref="S2.I1.i2.p1.1.m1.1.1">0</cn></annotation-xml></semantics></math> and whose major and minor radius correspond to half the width and height of the box. We generate a mask where white pixels correspond to the areas in the image in which the network has detected a face.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.1">Face blurring</span>. Our face blurring technique aims at being independent of the size of the faces in the images. In fact, we do not want to apply Gaussian blur with the same standard deviation to two faces in two different images. In fact, applying a Gaussian blur with the same standard deviation would result in a blurring which would be dependent on the size of the faces. We then choose, at a frame level, the standard deviation of the blurring kernel as a function of the minimum dimension of the face detected in that frame. We then blur the whole frame with the selected standard deviation. Then we replace all the pixels of the previously created mask by its blurred version.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS3.p3.1.1">Remark</span>. We could apply a Gaussian blur whose standard deviation is a function of each detected face dimension. But in the case of overlapping faces, the rendering is not satisfactory, creating edges that are undesirable.</p>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1">Some results are displayed in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4" title="4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Face Blurring using DeOldify</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">To perform the face blurring task, we heavily rely on the work in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib9" title="">9</a>]</cite>. We detail in this section our motivations.
First, we did not want to train a face detector, as there are already a great amount of them trained on a greater amount of data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib5" title="">5</a>]</cite>. With a face detector we would have a second step that would consist in blurring all the faces detected, as explained in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S2" title="2 Face Blurring using YOLO ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">2</span></a>. What we wanted is to directly operate on the image and automatically perform blurring, getting rid of the detection step. 
<br class="ltx_break"/>To do so, the Unet-like architecture described in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib9" title="">9</a>]</cite> seem well fitted. Notably, this model has demonstrated success in colorizing images, particularly skin tones and faces. This indicates the model’s capability to perform two tasks: skin/face identification and its colorization. Consequently, we were motivated to select the same architecture for our approach.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Architecture</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We keep the architecture used in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib9" title="">9</a>]</cite>, that is a Unet architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib7" title="">7</a>]</cite>, composed of an encoder whose weights are initialized with ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib2" title="">2</a>]</cite> checkpoint trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib8" title="">8</a>]</cite>. These encoder weights are <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">freezed</span> during training. The decoder is a standard Unet decoder, except from the fact that a self-attention layer is added as displayed in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S3.F4" title="Figure 4 ‣ 3.1 Architecture ‣ 3 Face Blurring using DeOldify ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">4</span></a>. We also conducted experiments getting rid of the self-attention layer (see Section <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4" title="4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="178" id="S3.F4.g1" src="extracted/5872765/figures/unet.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">DeOldify architecture (from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib9" title="">9</a>]</cite>). In red: pretrained ResNet, in blue: convolutional blocks, in green: upsample layers, in
orange: self-attention layer, and in pink: sigmoid layer. The black lines stand for the skip connections</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Dataset for training</h3>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Datasets overview</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.3">In this section, we detail how we perform the dataset construction. We use two different datasets:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Face Detection Data Set and Benchmark (FDDB) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib3" title="">3</a>]</cite></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">WIDER FACE: A Face Detection Benchmark (WIDER) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib10" title="">10</a>]</cite>.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">Both datasets are face detection datasets, which means that each image comes with annotations that provide information about the localization of every human face in the image.
<br class="ltx_break"/>The FDDB dataset contains annotations for 5171 faces in a set of 2845 images. For each image the face annotations are encrypted in an ellipse fashion <math alttext="\left(r_{a},r_{b},\theta,x,y\right)" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.1.m1.5"><semantics id="S3.SS2.SSS1.p1.1.m1.5a"><mrow id="S3.SS2.SSS1.p1.1.m1.5.5.2" xref="S3.SS2.SSS1.p1.1.m1.5.5.3.cmml"><mo id="S3.SS2.SSS1.p1.1.m1.5.5.2.3" xref="S3.SS2.SSS1.p1.1.m1.5.5.3.cmml">(</mo><msub id="S3.SS2.SSS1.p1.1.m1.4.4.1.1" xref="S3.SS2.SSS1.p1.1.m1.4.4.1.1.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.4.4.1.1.2" xref="S3.SS2.SSS1.p1.1.m1.4.4.1.1.2.cmml">r</mi><mi id="S3.SS2.SSS1.p1.1.m1.4.4.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.4.4.1.1.3.cmml">a</mi></msub><mo id="S3.SS2.SSS1.p1.1.m1.5.5.2.4" xref="S3.SS2.SSS1.p1.1.m1.5.5.3.cmml">,</mo><msub id="S3.SS2.SSS1.p1.1.m1.5.5.2.2" xref="S3.SS2.SSS1.p1.1.m1.5.5.2.2.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.5.5.2.2.2" xref="S3.SS2.SSS1.p1.1.m1.5.5.2.2.2.cmml">r</mi><mi id="S3.SS2.SSS1.p1.1.m1.5.5.2.2.3" xref="S3.SS2.SSS1.p1.1.m1.5.5.2.2.3.cmml">b</mi></msub><mo id="S3.SS2.SSS1.p1.1.m1.5.5.2.5" xref="S3.SS2.SSS1.p1.1.m1.5.5.3.cmml">,</mo><mi id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml">θ</mi><mo id="S3.SS2.SSS1.p1.1.m1.5.5.2.6" xref="S3.SS2.SSS1.p1.1.m1.5.5.3.cmml">,</mo><mi id="S3.SS2.SSS1.p1.1.m1.2.2" xref="S3.SS2.SSS1.p1.1.m1.2.2.cmml">x</mi><mo id="S3.SS2.SSS1.p1.1.m1.5.5.2.7" xref="S3.SS2.SSS1.p1.1.m1.5.5.3.cmml">,</mo><mi id="S3.SS2.SSS1.p1.1.m1.3.3" xref="S3.SS2.SSS1.p1.1.m1.3.3.cmml">y</mi><mo id="S3.SS2.SSS1.p1.1.m1.5.5.2.8" xref="S3.SS2.SSS1.p1.1.m1.5.5.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.5b"><vector id="S3.SS2.SSS1.p1.1.m1.5.5.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.5.5.2"><apply id="S3.SS2.SSS1.p1.1.m1.4.4.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.4.4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.4.4.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.4.4.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.4.4.1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.4.4.1.1.2">𝑟</ci><ci id="S3.SS2.SSS1.p1.1.m1.4.4.1.1.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.4.4.1.1.3">𝑎</ci></apply><apply id="S3.SS2.SSS1.p1.1.m1.5.5.2.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.5.5.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.5.5.2.2.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.5.5.2.2">subscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.5.5.2.2.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.5.5.2.2.2">𝑟</ci><ci id="S3.SS2.SSS1.p1.1.m1.5.5.2.2.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.5.5.2.2.3">𝑏</ci></apply><ci id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">𝜃</ci><ci id="S3.SS2.SSS1.p1.1.m1.2.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.2.2">𝑥</ci><ci id="S3.SS2.SSS1.p1.1.m1.3.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.3.3">𝑦</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.5c">\left(r_{a},r_{b},\theta,x,y\right)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.1.m1.5d">( italic_r start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT , italic_θ , italic_x , italic_y )</annotation></semantics></math> where:</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><math alttext="\left(r_{a},r_{b}\right)" class="ltx_Math" display="inline" id="S3.I2.i1.p1.1.m1.2"><semantics id="S3.I2.i1.p1.1.m1.2a"><mrow id="S3.I2.i1.p1.1.m1.2.2.2" xref="S3.I2.i1.p1.1.m1.2.2.3.cmml"><mo id="S3.I2.i1.p1.1.m1.2.2.2.3" xref="S3.I2.i1.p1.1.m1.2.2.3.cmml">(</mo><msub id="S3.I2.i1.p1.1.m1.1.1.1.1" xref="S3.I2.i1.p1.1.m1.1.1.1.1.cmml"><mi id="S3.I2.i1.p1.1.m1.1.1.1.1.2" xref="S3.I2.i1.p1.1.m1.1.1.1.1.2.cmml">r</mi><mi id="S3.I2.i1.p1.1.m1.1.1.1.1.3" xref="S3.I2.i1.p1.1.m1.1.1.1.1.3.cmml">a</mi></msub><mo id="S3.I2.i1.p1.1.m1.2.2.2.4" xref="S3.I2.i1.p1.1.m1.2.2.3.cmml">,</mo><msub id="S3.I2.i1.p1.1.m1.2.2.2.2" xref="S3.I2.i1.p1.1.m1.2.2.2.2.cmml"><mi id="S3.I2.i1.p1.1.m1.2.2.2.2.2" xref="S3.I2.i1.p1.1.m1.2.2.2.2.2.cmml">r</mi><mi id="S3.I2.i1.p1.1.m1.2.2.2.2.3" xref="S3.I2.i1.p1.1.m1.2.2.2.2.3.cmml">b</mi></msub><mo id="S3.I2.i1.p1.1.m1.2.2.2.5" xref="S3.I2.i1.p1.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.1.m1.2b"><interval closure="open" id="S3.I2.i1.p1.1.m1.2.2.3.cmml" xref="S3.I2.i1.p1.1.m1.2.2.2"><apply id="S3.I2.i1.p1.1.m1.1.1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.I2.i1.p1.1.m1.1.1.1.1.1.cmml" xref="S3.I2.i1.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.I2.i1.p1.1.m1.1.1.1.1.2.cmml" xref="S3.I2.i1.p1.1.m1.1.1.1.1.2">𝑟</ci><ci id="S3.I2.i1.p1.1.m1.1.1.1.1.3.cmml" xref="S3.I2.i1.p1.1.m1.1.1.1.1.3">𝑎</ci></apply><apply id="S3.I2.i1.p1.1.m1.2.2.2.2.cmml" xref="S3.I2.i1.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.I2.i1.p1.1.m1.2.2.2.2.1.cmml" xref="S3.I2.i1.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.I2.i1.p1.1.m1.2.2.2.2.2.cmml" xref="S3.I2.i1.p1.1.m1.2.2.2.2.2">𝑟</ci><ci id="S3.I2.i1.p1.1.m1.2.2.2.2.3.cmml" xref="S3.I2.i1.p1.1.m1.2.2.2.2.3">𝑏</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.1.m1.2c">\left(r_{a},r_{b}\right)</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.1.m1.2d">( italic_r start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT )</annotation></semantics></math> are the major and minor axis radius of the ellipse (in pixels)</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><math alttext="\theta" class="ltx_Math" display="inline" id="S3.I2.i2.p1.1.m1.1"><semantics id="S3.I2.i2.p1.1.m1.1a"><mi id="S3.I2.i2.p1.1.m1.1.1" xref="S3.I2.i2.p1.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.1.m1.1b"><ci id="S3.I2.i2.p1.1.m1.1.1.cmml" xref="S3.I2.i2.p1.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.1.m1.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.1.m1.1d">italic_θ</annotation></semantics></math> is the orientation angle of the ellipse</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1"><math alttext="\left(x,y\right)" class="ltx_Math" display="inline" id="S3.I2.i3.p1.1.m1.2"><semantics id="S3.I2.i3.p1.1.m1.2a"><mrow id="S3.I2.i3.p1.1.m1.2.3.2" xref="S3.I2.i3.p1.1.m1.2.3.1.cmml"><mo id="S3.I2.i3.p1.1.m1.2.3.2.1" xref="S3.I2.i3.p1.1.m1.2.3.1.cmml">(</mo><mi id="S3.I2.i3.p1.1.m1.1.1" xref="S3.I2.i3.p1.1.m1.1.1.cmml">x</mi><mo id="S3.I2.i3.p1.1.m1.2.3.2.2" xref="S3.I2.i3.p1.1.m1.2.3.1.cmml">,</mo><mi id="S3.I2.i3.p1.1.m1.2.2" xref="S3.I2.i3.p1.1.m1.2.2.cmml">y</mi><mo id="S3.I2.i3.p1.1.m1.2.3.2.3" xref="S3.I2.i3.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i3.p1.1.m1.2b"><interval closure="open" id="S3.I2.i3.p1.1.m1.2.3.1.cmml" xref="S3.I2.i3.p1.1.m1.2.3.2"><ci id="S3.I2.i3.p1.1.m1.1.1.cmml" xref="S3.I2.i3.p1.1.m1.1.1">𝑥</ci><ci id="S3.I2.i3.p1.1.m1.2.2.cmml" xref="S3.I2.i3.p1.1.m1.2.2">𝑦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i3.p1.1.m1.2c">\left(x,y\right)</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i3.p1.1.m1.2d">( italic_x , italic_y )</annotation></semantics></math> are the pixel coordinates of the center of the ellipse.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.SS2.SSS1.p1.2">The WIDER dataset contains 32,203 images and label 393,703 faces. For each image the face annotations are encrypted in as rectangle fashion <math alttext="\left(x,y,w,h\right)" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.2.m1.4"><semantics id="S3.SS2.SSS1.p1.2.m1.4a"><mrow id="S3.SS2.SSS1.p1.2.m1.4.5.2" xref="S3.SS2.SSS1.p1.2.m1.4.5.1.cmml"><mo id="S3.SS2.SSS1.p1.2.m1.4.5.2.1" xref="S3.SS2.SSS1.p1.2.m1.4.5.1.cmml">(</mo><mi id="S3.SS2.SSS1.p1.2.m1.1.1" xref="S3.SS2.SSS1.p1.2.m1.1.1.cmml">x</mi><mo id="S3.SS2.SSS1.p1.2.m1.4.5.2.2" xref="S3.SS2.SSS1.p1.2.m1.4.5.1.cmml">,</mo><mi id="S3.SS2.SSS1.p1.2.m1.2.2" xref="S3.SS2.SSS1.p1.2.m1.2.2.cmml">y</mi><mo id="S3.SS2.SSS1.p1.2.m1.4.5.2.3" xref="S3.SS2.SSS1.p1.2.m1.4.5.1.cmml">,</mo><mi id="S3.SS2.SSS1.p1.2.m1.3.3" xref="S3.SS2.SSS1.p1.2.m1.3.3.cmml">w</mi><mo id="S3.SS2.SSS1.p1.2.m1.4.5.2.4" xref="S3.SS2.SSS1.p1.2.m1.4.5.1.cmml">,</mo><mi id="S3.SS2.SSS1.p1.2.m1.4.4" xref="S3.SS2.SSS1.p1.2.m1.4.4.cmml">h</mi><mo id="S3.SS2.SSS1.p1.2.m1.4.5.2.5" xref="S3.SS2.SSS1.p1.2.m1.4.5.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m1.4b"><vector id="S3.SS2.SSS1.p1.2.m1.4.5.1.cmml" xref="S3.SS2.SSS1.p1.2.m1.4.5.2"><ci id="S3.SS2.SSS1.p1.2.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m1.1.1">𝑥</ci><ci id="S3.SS2.SSS1.p1.2.m1.2.2.cmml" xref="S3.SS2.SSS1.p1.2.m1.2.2">𝑦</ci><ci id="S3.SS2.SSS1.p1.2.m1.3.3.cmml" xref="S3.SS2.SSS1.p1.2.m1.3.3">𝑤</ci><ci id="S3.SS2.SSS1.p1.2.m1.4.4.cmml" xref="S3.SS2.SSS1.p1.2.m1.4.4">ℎ</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m1.4c">\left(x,y,w,h\right)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.2.m1.4d">( italic_x , italic_y , italic_w , italic_h )</annotation></semantics></math> where:</p>
<ul class="ltx_itemize" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i1.p1">
<p class="ltx_p" id="S3.I3.i1.p1.1"><math alttext="\left(x,y\right)" class="ltx_Math" display="inline" id="S3.I3.i1.p1.1.m1.2"><semantics id="S3.I3.i1.p1.1.m1.2a"><mrow id="S3.I3.i1.p1.1.m1.2.3.2" xref="S3.I3.i1.p1.1.m1.2.3.1.cmml"><mo id="S3.I3.i1.p1.1.m1.2.3.2.1" xref="S3.I3.i1.p1.1.m1.2.3.1.cmml">(</mo><mi id="S3.I3.i1.p1.1.m1.1.1" xref="S3.I3.i1.p1.1.m1.1.1.cmml">x</mi><mo id="S3.I3.i1.p1.1.m1.2.3.2.2" xref="S3.I3.i1.p1.1.m1.2.3.1.cmml">,</mo><mi id="S3.I3.i1.p1.1.m1.2.2" xref="S3.I3.i1.p1.1.m1.2.2.cmml">y</mi><mo id="S3.I3.i1.p1.1.m1.2.3.2.3" xref="S3.I3.i1.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I3.i1.p1.1.m1.2b"><interval closure="open" id="S3.I3.i1.p1.1.m1.2.3.1.cmml" xref="S3.I3.i1.p1.1.m1.2.3.2"><ci id="S3.I3.i1.p1.1.m1.1.1.cmml" xref="S3.I3.i1.p1.1.m1.1.1">𝑥</ci><ci id="S3.I3.i1.p1.1.m1.2.2.cmml" xref="S3.I3.i1.p1.1.m1.2.2">𝑦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.I3.i1.p1.1.m1.2c">\left(x,y\right)</annotation><annotation encoding="application/x-llamapun" id="S3.I3.i1.p1.1.m1.2d">( italic_x , italic_y )</annotation></semantics></math> are the pixel coordinates of the top left corner.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i2.p1">
<p class="ltx_p" id="S3.I3.i2.p1.1"><math alttext="\left(w,h\right)" class="ltx_Math" display="inline" id="S3.I3.i2.p1.1.m1.2"><semantics id="S3.I3.i2.p1.1.m1.2a"><mrow id="S3.I3.i2.p1.1.m1.2.3.2" xref="S3.I3.i2.p1.1.m1.2.3.1.cmml"><mo id="S3.I3.i2.p1.1.m1.2.3.2.1" xref="S3.I3.i2.p1.1.m1.2.3.1.cmml">(</mo><mi id="S3.I3.i2.p1.1.m1.1.1" xref="S3.I3.i2.p1.1.m1.1.1.cmml">w</mi><mo id="S3.I3.i2.p1.1.m1.2.3.2.2" xref="S3.I3.i2.p1.1.m1.2.3.1.cmml">,</mo><mi id="S3.I3.i2.p1.1.m1.2.2" xref="S3.I3.i2.p1.1.m1.2.2.cmml">h</mi><mo id="S3.I3.i2.p1.1.m1.2.3.2.3" xref="S3.I3.i2.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I3.i2.p1.1.m1.2b"><interval closure="open" id="S3.I3.i2.p1.1.m1.2.3.1.cmml" xref="S3.I3.i2.p1.1.m1.2.3.2"><ci id="S3.I3.i2.p1.1.m1.1.1.cmml" xref="S3.I3.i2.p1.1.m1.1.1">𝑤</ci><ci id="S3.I3.i2.p1.1.m1.2.2.cmml" xref="S3.I3.i2.p1.1.m1.2.2">ℎ</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.I3.i2.p1.1.m1.2c">\left(w,h\right)</annotation><annotation encoding="application/x-llamapun" id="S3.I3.i2.p1.1.m1.2d">( italic_w , italic_h )</annotation></semantics></math> are the width and the height of the rectangle (in pixels)</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.SS2.SSS1.p1.4">One observation that immediately stands out is that the FDDB face annotations appear to be more precise, as they are encrypted as ellipses. However, it is worth noting that the WIDER dataset is significantly larger in size.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Construction methodology</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">As previously mentioned, the input object for this task should be the raw image, while the desired output is the same image with blurred faces. To achieve this, we must combine the image with its corresponding annotations in order to generate the blurred version. In order to achieve this we proceed exactly as in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S2.SS3" title="2.3 Inference methodology ‣ 2 Face Blurring using YOLO ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">2.3</span></a>.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">For the FDDB dataset annotations come in an ellipse fashion so that we perform directly <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p2.1.1">step 3</span> (Face Blurring). For the WIDER dataset annotations come as boxes, so that we perform <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p2.1.2">step 2</span> (Transformation of boxes into ellipses) and <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p2.1.3">step 3</span> (Face Blurring).
<br class="ltx_break"/>It is worth noting that there is no clear ground truth in our problem as several blurs can perform as efficiently the task we want to implement. We display in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S3.F5" title="Figure 5 ‣ 3.2.2 Construction methodology ‣ 3.2 Dataset for training ‣ 3 Face Blurring using DeOldify ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">5</span></a> an example of both inputs and targets for both datasets.</p>
</div>
<figure class="ltx_figure" id="S3.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="352" id="S3.F5.1.g1" src="extracted/5872765/figures/image_1848.jpg" width="479"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="352" id="S3.F5.2.g1" src="extracted/5872765/figures/image_1848_blurred.jpg" width="479"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="323" id="S3.F5.3.g1" src="extracted/5872765/figures/image_6768.jpg" width="479"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="323" id="S3.F5.4.g1" src="extracted/5872765/figures/image_6768_blurred.jpg" width="479"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.6.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.7.2" style="font-size:90%;">Correspondence inputs-targets for an image of FDDB dataset (up) and WIDER dataset (down).</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.1">We then build a train/validation split consisting of 15148 training images pairs and 3803 validation images pairs. As the WIDER dataset came with a train/validation split we keep that split and create one only for the FBBD dataset.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Training</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Training is performed in a supervised fashion with the couple of input-targets described in the previous section. In this section we detail the choice of loss function and of training procedure.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Loss function</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">In the original paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib9" title="">9</a>]</cite>, training was performed using a feature loss inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib4" title="">4</a>]</cite>. It corresponds to computing a weighted average L1 loss of intermediate outputs of a VGG network. As we consider that the VGG features extracted from a face and its blurred counterpart are intuitively similar, we did not think relevant to use this loss. Instead we focus on more classical losses: L1 loss and Mean Square Error (MSE) for which we recall the definition below.</p>
<ul class="ltx_itemize" id="S3.I4">
<li class="ltx_item" id="S3.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I4.i1.p1">
<p class="ltx_p" id="S3.I4.i1.p1.1">L1 loss : <math alttext="l_{1}\left(\textbf{u},\textbf{v}\right)=\frac{1}{\left|\textbf{u}\right|}%
\underset{\textbf{i}\in\textbf{u}}{\sum}\left|\textbf{u}\left(\textbf{i}\right%
)-\textbf{v}\left(\textbf{i}\right)\right|" class="ltx_Math" display="inline" id="S3.I4.i1.p1.1.m1.6"><semantics id="S3.I4.i1.p1.1.m1.6a"><mrow id="S3.I4.i1.p1.1.m1.6.6" xref="S3.I4.i1.p1.1.m1.6.6.cmml"><mrow id="S3.I4.i1.p1.1.m1.6.6.3" xref="S3.I4.i1.p1.1.m1.6.6.3.cmml"><msub id="S3.I4.i1.p1.1.m1.6.6.3.2" xref="S3.I4.i1.p1.1.m1.6.6.3.2.cmml"><mi id="S3.I4.i1.p1.1.m1.6.6.3.2.2" xref="S3.I4.i1.p1.1.m1.6.6.3.2.2.cmml">l</mi><mn id="S3.I4.i1.p1.1.m1.6.6.3.2.3" xref="S3.I4.i1.p1.1.m1.6.6.3.2.3.cmml">1</mn></msub><mo id="S3.I4.i1.p1.1.m1.6.6.3.1" xref="S3.I4.i1.p1.1.m1.6.6.3.1.cmml">⁢</mo><mrow id="S3.I4.i1.p1.1.m1.6.6.3.3.2" xref="S3.I4.i1.p1.1.m1.6.6.3.3.1.cmml"><mo id="S3.I4.i1.p1.1.m1.6.6.3.3.2.1" xref="S3.I4.i1.p1.1.m1.6.6.3.3.1.cmml">(</mo><mtext class="ltx_mathvariant_bold" id="S3.I4.i1.p1.1.m1.2.2" xref="S3.I4.i1.p1.1.m1.2.2a.cmml">u</mtext><mo id="S3.I4.i1.p1.1.m1.6.6.3.3.2.2" xref="S3.I4.i1.p1.1.m1.6.6.3.3.1.cmml">,</mo><mtext class="ltx_mathvariant_bold" id="S3.I4.i1.p1.1.m1.3.3" xref="S3.I4.i1.p1.1.m1.3.3a.cmml">v</mtext><mo id="S3.I4.i1.p1.1.m1.6.6.3.3.2.3" xref="S3.I4.i1.p1.1.m1.6.6.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.I4.i1.p1.1.m1.6.6.2" xref="S3.I4.i1.p1.1.m1.6.6.2.cmml">=</mo><mrow id="S3.I4.i1.p1.1.m1.6.6.1" xref="S3.I4.i1.p1.1.m1.6.6.1.cmml"><mfrac id="S3.I4.i1.p1.1.m1.1.1" xref="S3.I4.i1.p1.1.m1.1.1.cmml"><mn id="S3.I4.i1.p1.1.m1.1.1.3" xref="S3.I4.i1.p1.1.m1.1.1.3.cmml">1</mn><mrow id="S3.I4.i1.p1.1.m1.1.1.1.3" xref="S3.I4.i1.p1.1.m1.1.1.1.2.cmml"><mo id="S3.I4.i1.p1.1.m1.1.1.1.3.1" xref="S3.I4.i1.p1.1.m1.1.1.1.2.1.cmml">|</mo><mtext class="ltx_mathvariant_bold" id="S3.I4.i1.p1.1.m1.1.1.1.1" xref="S3.I4.i1.p1.1.m1.1.1.1.1a.cmml">u</mtext><mo id="S3.I4.i1.p1.1.m1.1.1.1.3.2" xref="S3.I4.i1.p1.1.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S3.I4.i1.p1.1.m1.6.6.1.2" xref="S3.I4.i1.p1.1.m1.6.6.1.2.cmml">⁢</mo><munder accentunder="true" id="S3.I4.i1.p1.1.m1.6.6.1.3" xref="S3.I4.i1.p1.1.m1.6.6.1.3.cmml"><mo id="S3.I4.i1.p1.1.m1.6.6.1.3.2" xref="S3.I4.i1.p1.1.m1.6.6.1.3.2.cmml">∑</mo><mrow id="S3.I4.i1.p1.1.m1.6.6.1.3.1" xref="S3.I4.i1.p1.1.m1.6.6.1.3.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.I4.i1.p1.1.m1.6.6.1.3.1.2" xref="S3.I4.i1.p1.1.m1.6.6.1.3.1.2a.cmml">i</mtext><mo id="S3.I4.i1.p1.1.m1.6.6.1.3.1.1" xref="S3.I4.i1.p1.1.m1.6.6.1.3.1.1.cmml">∈</mo><mtext class="ltx_mathvariant_bold" id="S3.I4.i1.p1.1.m1.6.6.1.3.1.3" xref="S3.I4.i1.p1.1.m1.6.6.1.3.1.3a.cmml">u</mtext></mrow></munder><mo id="S3.I4.i1.p1.1.m1.6.6.1.2a" lspace="0em" xref="S3.I4.i1.p1.1.m1.6.6.1.2.cmml">⁢</mo><mrow id="S3.I4.i1.p1.1.m1.6.6.1.1.1" xref="S3.I4.i1.p1.1.m1.6.6.1.1.2.cmml"><mo id="S3.I4.i1.p1.1.m1.6.6.1.1.1.2" xref="S3.I4.i1.p1.1.m1.6.6.1.1.2.1.cmml">|</mo><mrow id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.cmml"><mrow id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.2" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.2.2" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.2.2a.cmml">u</mtext><mo id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.2.1" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.2.1.cmml">⁢</mo><mrow id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.2.3.2" xref="S3.I4.i1.p1.1.m1.4.4a.cmml"><mo id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.2.3.2.1" xref="S3.I4.i1.p1.1.m1.4.4a.cmml">(</mo><mtext class="ltx_mathvariant_bold" id="S3.I4.i1.p1.1.m1.4.4" xref="S3.I4.i1.p1.1.m1.4.4.cmml">i</mtext><mo id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.2.3.2.2" xref="S3.I4.i1.p1.1.m1.4.4a.cmml">)</mo></mrow></mrow><mo id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.1" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.1.cmml">−</mo><mrow id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.3" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.3.2" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.3.2a.cmml">v</mtext><mo id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.3.1" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.3.1.cmml">⁢</mo><mrow id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.3.3.2" xref="S3.I4.i1.p1.1.m1.5.5a.cmml"><mo id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.3.3.2.1" xref="S3.I4.i1.p1.1.m1.5.5a.cmml">(</mo><mtext class="ltx_mathvariant_bold" id="S3.I4.i1.p1.1.m1.5.5" xref="S3.I4.i1.p1.1.m1.5.5.cmml">i</mtext><mo id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.3.3.2.2" xref="S3.I4.i1.p1.1.m1.5.5a.cmml">)</mo></mrow></mrow></mrow><mo id="S3.I4.i1.p1.1.m1.6.6.1.1.1.3" xref="S3.I4.i1.p1.1.m1.6.6.1.1.2.1.cmml">|</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I4.i1.p1.1.m1.6b"><apply id="S3.I4.i1.p1.1.m1.6.6.cmml" xref="S3.I4.i1.p1.1.m1.6.6"><eq id="S3.I4.i1.p1.1.m1.6.6.2.cmml" xref="S3.I4.i1.p1.1.m1.6.6.2"></eq><apply id="S3.I4.i1.p1.1.m1.6.6.3.cmml" xref="S3.I4.i1.p1.1.m1.6.6.3"><times id="S3.I4.i1.p1.1.m1.6.6.3.1.cmml" xref="S3.I4.i1.p1.1.m1.6.6.3.1"></times><apply id="S3.I4.i1.p1.1.m1.6.6.3.2.cmml" xref="S3.I4.i1.p1.1.m1.6.6.3.2"><csymbol cd="ambiguous" id="S3.I4.i1.p1.1.m1.6.6.3.2.1.cmml" xref="S3.I4.i1.p1.1.m1.6.6.3.2">subscript</csymbol><ci id="S3.I4.i1.p1.1.m1.6.6.3.2.2.cmml" xref="S3.I4.i1.p1.1.m1.6.6.3.2.2">𝑙</ci><cn id="S3.I4.i1.p1.1.m1.6.6.3.2.3.cmml" type="integer" xref="S3.I4.i1.p1.1.m1.6.6.3.2.3">1</cn></apply><interval closure="open" id="S3.I4.i1.p1.1.m1.6.6.3.3.1.cmml" xref="S3.I4.i1.p1.1.m1.6.6.3.3.2"><ci id="S3.I4.i1.p1.1.m1.2.2a.cmml" xref="S3.I4.i1.p1.1.m1.2.2"><mtext class="ltx_mathvariant_bold" id="S3.I4.i1.p1.1.m1.2.2.cmml" xref="S3.I4.i1.p1.1.m1.2.2">u</mtext></ci><ci id="S3.I4.i1.p1.1.m1.3.3a.cmml" xref="S3.I4.i1.p1.1.m1.3.3"><mtext class="ltx_mathvariant_bold" id="S3.I4.i1.p1.1.m1.3.3.cmml" xref="S3.I4.i1.p1.1.m1.3.3">v</mtext></ci></interval></apply><apply id="S3.I4.i1.p1.1.m1.6.6.1.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1"><times id="S3.I4.i1.p1.1.m1.6.6.1.2.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.2"></times><apply id="S3.I4.i1.p1.1.m1.1.1.cmml" xref="S3.I4.i1.p1.1.m1.1.1"><divide id="S3.I4.i1.p1.1.m1.1.1.2.cmml" xref="S3.I4.i1.p1.1.m1.1.1"></divide><cn id="S3.I4.i1.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.I4.i1.p1.1.m1.1.1.3">1</cn><apply id="S3.I4.i1.p1.1.m1.1.1.1.2.cmml" xref="S3.I4.i1.p1.1.m1.1.1.1.3"><abs id="S3.I4.i1.p1.1.m1.1.1.1.2.1.cmml" xref="S3.I4.i1.p1.1.m1.1.1.1.3.1"></abs><ci id="S3.I4.i1.p1.1.m1.1.1.1.1a.cmml" xref="S3.I4.i1.p1.1.m1.1.1.1.1"><mtext class="ltx_mathvariant_bold" id="S3.I4.i1.p1.1.m1.1.1.1.1.cmml" mathsize="70%" xref="S3.I4.i1.p1.1.m1.1.1.1.1">u</mtext></ci></apply></apply><apply id="S3.I4.i1.p1.1.m1.6.6.1.3.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.3"><apply id="S3.I4.i1.p1.1.m1.6.6.1.3.1.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.3.1"><in id="S3.I4.i1.p1.1.m1.6.6.1.3.1.1.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.3.1.1"></in><ci id="S3.I4.i1.p1.1.m1.6.6.1.3.1.2a.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.3.1.2"><mtext class="ltx_mathvariant_bold" id="S3.I4.i1.p1.1.m1.6.6.1.3.1.2.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.3.1.2">i</mtext></ci><ci id="S3.I4.i1.p1.1.m1.6.6.1.3.1.3a.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.3.1.3"><mtext class="ltx_mathvariant_bold" id="S3.I4.i1.p1.1.m1.6.6.1.3.1.3.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.3.1.3">u</mtext></ci></apply><sum id="S3.I4.i1.p1.1.m1.6.6.1.3.2.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.3.2"></sum></apply><apply id="S3.I4.i1.p1.1.m1.6.6.1.1.2.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1"><abs id="S3.I4.i1.p1.1.m1.6.6.1.1.2.1.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.2"></abs><apply id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.1"><minus id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.1.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.1"></minus><apply id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.2.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.2"><times id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.2.1.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.2.1"></times><ci id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.2.2a.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.2.2.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.2.2">u</mtext></ci><ci id="S3.I4.i1.p1.1.m1.4.4a.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.2.3.2"><mtext class="ltx_mathvariant_bold" id="S3.I4.i1.p1.1.m1.4.4.cmml" xref="S3.I4.i1.p1.1.m1.4.4">i</mtext></ci></apply><apply id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.3.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.3"><times id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.3.1.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.3.1"></times><ci id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.3.2a.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.3.2.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.3.2">v</mtext></ci><ci id="S3.I4.i1.p1.1.m1.5.5a.cmml" xref="S3.I4.i1.p1.1.m1.6.6.1.1.1.1.3.3.2"><mtext class="ltx_mathvariant_bold" id="S3.I4.i1.p1.1.m1.5.5.cmml" xref="S3.I4.i1.p1.1.m1.5.5">i</mtext></ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I4.i1.p1.1.m1.6c">l_{1}\left(\textbf{u},\textbf{v}\right)=\frac{1}{\left|\textbf{u}\right|}%
\underset{\textbf{i}\in\textbf{u}}{\sum}\left|\textbf{u}\left(\textbf{i}\right%
)-\textbf{v}\left(\textbf{i}\right)\right|</annotation><annotation encoding="application/x-llamapun" id="S3.I4.i1.p1.1.m1.6d">italic_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( u , v ) = divide start_ARG 1 end_ARG start_ARG | u | end_ARG start_UNDERACCENT i ∈ u end_UNDERACCENT start_ARG ∑ end_ARG | u ( i ) - v ( i ) |</annotation></semantics></math></p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I4.i2.p1">
<p class="ltx_p" id="S3.I4.i2.p1.1">MSE : <math alttext="MSE\left(\textbf{u},\textbf{v}\right)=\frac{1}{\left|\textbf{u}\right|}%
\underset{\textbf{i}\in\textbf{u}}{\sum}\left(\textbf{u}\left(\textbf{i}\right%
)-\textbf{v}\left(\textbf{i}\right)\right)^{2}" class="ltx_Math" display="inline" id="S3.I4.i2.p1.1.m1.6"><semantics id="S3.I4.i2.p1.1.m1.6a"><mrow id="S3.I4.i2.p1.1.m1.6.6" xref="S3.I4.i2.p1.1.m1.6.6.cmml"><mrow id="S3.I4.i2.p1.1.m1.6.6.3" xref="S3.I4.i2.p1.1.m1.6.6.3.cmml"><mi id="S3.I4.i2.p1.1.m1.6.6.3.2" xref="S3.I4.i2.p1.1.m1.6.6.3.2.cmml">M</mi><mo id="S3.I4.i2.p1.1.m1.6.6.3.1" xref="S3.I4.i2.p1.1.m1.6.6.3.1.cmml">⁢</mo><mi id="S3.I4.i2.p1.1.m1.6.6.3.3" xref="S3.I4.i2.p1.1.m1.6.6.3.3.cmml">S</mi><mo id="S3.I4.i2.p1.1.m1.6.6.3.1a" xref="S3.I4.i2.p1.1.m1.6.6.3.1.cmml">⁢</mo><mi id="S3.I4.i2.p1.1.m1.6.6.3.4" xref="S3.I4.i2.p1.1.m1.6.6.3.4.cmml">E</mi><mo id="S3.I4.i2.p1.1.m1.6.6.3.1b" xref="S3.I4.i2.p1.1.m1.6.6.3.1.cmml">⁢</mo><mrow id="S3.I4.i2.p1.1.m1.6.6.3.5.2" xref="S3.I4.i2.p1.1.m1.6.6.3.5.1.cmml"><mo id="S3.I4.i2.p1.1.m1.6.6.3.5.2.1" xref="S3.I4.i2.p1.1.m1.6.6.3.5.1.cmml">(</mo><mtext class="ltx_mathvariant_bold" id="S3.I4.i2.p1.1.m1.2.2" xref="S3.I4.i2.p1.1.m1.2.2a.cmml">u</mtext><mo id="S3.I4.i2.p1.1.m1.6.6.3.5.2.2" xref="S3.I4.i2.p1.1.m1.6.6.3.5.1.cmml">,</mo><mtext class="ltx_mathvariant_bold" id="S3.I4.i2.p1.1.m1.3.3" xref="S3.I4.i2.p1.1.m1.3.3a.cmml">v</mtext><mo id="S3.I4.i2.p1.1.m1.6.6.3.5.2.3" xref="S3.I4.i2.p1.1.m1.6.6.3.5.1.cmml">)</mo></mrow></mrow><mo id="S3.I4.i2.p1.1.m1.6.6.2" xref="S3.I4.i2.p1.1.m1.6.6.2.cmml">=</mo><mrow id="S3.I4.i2.p1.1.m1.6.6.1" xref="S3.I4.i2.p1.1.m1.6.6.1.cmml"><mfrac id="S3.I4.i2.p1.1.m1.1.1" xref="S3.I4.i2.p1.1.m1.1.1.cmml"><mn id="S3.I4.i2.p1.1.m1.1.1.3" xref="S3.I4.i2.p1.1.m1.1.1.3.cmml">1</mn><mrow id="S3.I4.i2.p1.1.m1.1.1.1.3" xref="S3.I4.i2.p1.1.m1.1.1.1.2.cmml"><mo id="S3.I4.i2.p1.1.m1.1.1.1.3.1" xref="S3.I4.i2.p1.1.m1.1.1.1.2.1.cmml">|</mo><mtext class="ltx_mathvariant_bold" id="S3.I4.i2.p1.1.m1.1.1.1.1" xref="S3.I4.i2.p1.1.m1.1.1.1.1a.cmml">u</mtext><mo id="S3.I4.i2.p1.1.m1.1.1.1.3.2" xref="S3.I4.i2.p1.1.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S3.I4.i2.p1.1.m1.6.6.1.2" xref="S3.I4.i2.p1.1.m1.6.6.1.2.cmml">⁢</mo><munder accentunder="true" id="S3.I4.i2.p1.1.m1.6.6.1.3" xref="S3.I4.i2.p1.1.m1.6.6.1.3.cmml"><mo id="S3.I4.i2.p1.1.m1.6.6.1.3.2" xref="S3.I4.i2.p1.1.m1.6.6.1.3.2.cmml">∑</mo><mrow id="S3.I4.i2.p1.1.m1.6.6.1.3.1" xref="S3.I4.i2.p1.1.m1.6.6.1.3.1.cmml"><mtext class="ltx_mathvariant_bold" id="S3.I4.i2.p1.1.m1.6.6.1.3.1.2" xref="S3.I4.i2.p1.1.m1.6.6.1.3.1.2a.cmml">i</mtext><mo id="S3.I4.i2.p1.1.m1.6.6.1.3.1.1" xref="S3.I4.i2.p1.1.m1.6.6.1.3.1.1.cmml">∈</mo><mtext class="ltx_mathvariant_bold" id="S3.I4.i2.p1.1.m1.6.6.1.3.1.3" xref="S3.I4.i2.p1.1.m1.6.6.1.3.1.3a.cmml">u</mtext></mrow></munder><mo id="S3.I4.i2.p1.1.m1.6.6.1.2a" lspace="0em" xref="S3.I4.i2.p1.1.m1.6.6.1.2.cmml">⁢</mo><msup id="S3.I4.i2.p1.1.m1.6.6.1.1" xref="S3.I4.i2.p1.1.m1.6.6.1.1.cmml"><mrow id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.cmml"><mo id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.2" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.cmml">(</mo><mrow id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.cmml"><mrow id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.2" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.2.cmml"><mtext class="ltx_mathvariant_bold" id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.2.2" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.2.2a.cmml">u</mtext><mo id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.2.1" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.2.1.cmml">⁢</mo><mrow id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.2.3.2" xref="S3.I4.i2.p1.1.m1.4.4a.cmml"><mo id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.2.3.2.1" xref="S3.I4.i2.p1.1.m1.4.4a.cmml">(</mo><mtext class="ltx_mathvariant_bold" id="S3.I4.i2.p1.1.m1.4.4" xref="S3.I4.i2.p1.1.m1.4.4.cmml">i</mtext><mo id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.2.3.2.2" xref="S3.I4.i2.p1.1.m1.4.4a.cmml">)</mo></mrow></mrow><mo id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.1" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.1.cmml">−</mo><mrow id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.3" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.3.cmml"><mtext class="ltx_mathvariant_bold" id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.3.2" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.3.2a.cmml">v</mtext><mo id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.3.1" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.3.1.cmml">⁢</mo><mrow id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.3.3.2" xref="S3.I4.i2.p1.1.m1.5.5a.cmml"><mo id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.3.3.2.1" xref="S3.I4.i2.p1.1.m1.5.5a.cmml">(</mo><mtext class="ltx_mathvariant_bold" id="S3.I4.i2.p1.1.m1.5.5" xref="S3.I4.i2.p1.1.m1.5.5.cmml">i</mtext><mo id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.3.3.2.2" xref="S3.I4.i2.p1.1.m1.5.5a.cmml">)</mo></mrow></mrow></mrow><mo id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.3" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.cmml">)</mo></mrow><mn id="S3.I4.i2.p1.1.m1.6.6.1.1.3" xref="S3.I4.i2.p1.1.m1.6.6.1.1.3.cmml">2</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I4.i2.p1.1.m1.6b"><apply id="S3.I4.i2.p1.1.m1.6.6.cmml" xref="S3.I4.i2.p1.1.m1.6.6"><eq id="S3.I4.i2.p1.1.m1.6.6.2.cmml" xref="S3.I4.i2.p1.1.m1.6.6.2"></eq><apply id="S3.I4.i2.p1.1.m1.6.6.3.cmml" xref="S3.I4.i2.p1.1.m1.6.6.3"><times id="S3.I4.i2.p1.1.m1.6.6.3.1.cmml" xref="S3.I4.i2.p1.1.m1.6.6.3.1"></times><ci id="S3.I4.i2.p1.1.m1.6.6.3.2.cmml" xref="S3.I4.i2.p1.1.m1.6.6.3.2">𝑀</ci><ci id="S3.I4.i2.p1.1.m1.6.6.3.3.cmml" xref="S3.I4.i2.p1.1.m1.6.6.3.3">𝑆</ci><ci id="S3.I4.i2.p1.1.m1.6.6.3.4.cmml" xref="S3.I4.i2.p1.1.m1.6.6.3.4">𝐸</ci><interval closure="open" id="S3.I4.i2.p1.1.m1.6.6.3.5.1.cmml" xref="S3.I4.i2.p1.1.m1.6.6.3.5.2"><ci id="S3.I4.i2.p1.1.m1.2.2a.cmml" xref="S3.I4.i2.p1.1.m1.2.2"><mtext class="ltx_mathvariant_bold" id="S3.I4.i2.p1.1.m1.2.2.cmml" xref="S3.I4.i2.p1.1.m1.2.2">u</mtext></ci><ci id="S3.I4.i2.p1.1.m1.3.3a.cmml" xref="S3.I4.i2.p1.1.m1.3.3"><mtext class="ltx_mathvariant_bold" id="S3.I4.i2.p1.1.m1.3.3.cmml" xref="S3.I4.i2.p1.1.m1.3.3">v</mtext></ci></interval></apply><apply id="S3.I4.i2.p1.1.m1.6.6.1.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1"><times id="S3.I4.i2.p1.1.m1.6.6.1.2.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.2"></times><apply id="S3.I4.i2.p1.1.m1.1.1.cmml" xref="S3.I4.i2.p1.1.m1.1.1"><divide id="S3.I4.i2.p1.1.m1.1.1.2.cmml" xref="S3.I4.i2.p1.1.m1.1.1"></divide><cn id="S3.I4.i2.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.I4.i2.p1.1.m1.1.1.3">1</cn><apply id="S3.I4.i2.p1.1.m1.1.1.1.2.cmml" xref="S3.I4.i2.p1.1.m1.1.1.1.3"><abs id="S3.I4.i2.p1.1.m1.1.1.1.2.1.cmml" xref="S3.I4.i2.p1.1.m1.1.1.1.3.1"></abs><ci id="S3.I4.i2.p1.1.m1.1.1.1.1a.cmml" xref="S3.I4.i2.p1.1.m1.1.1.1.1"><mtext class="ltx_mathvariant_bold" id="S3.I4.i2.p1.1.m1.1.1.1.1.cmml" mathsize="70%" xref="S3.I4.i2.p1.1.m1.1.1.1.1">u</mtext></ci></apply></apply><apply id="S3.I4.i2.p1.1.m1.6.6.1.3.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.3"><apply id="S3.I4.i2.p1.1.m1.6.6.1.3.1.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.3.1"><in id="S3.I4.i2.p1.1.m1.6.6.1.3.1.1.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.3.1.1"></in><ci id="S3.I4.i2.p1.1.m1.6.6.1.3.1.2a.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.3.1.2"><mtext class="ltx_mathvariant_bold" id="S3.I4.i2.p1.1.m1.6.6.1.3.1.2.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.3.1.2">i</mtext></ci><ci id="S3.I4.i2.p1.1.m1.6.6.1.3.1.3a.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.3.1.3"><mtext class="ltx_mathvariant_bold" id="S3.I4.i2.p1.1.m1.6.6.1.3.1.3.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.3.1.3">u</mtext></ci></apply><sum id="S3.I4.i2.p1.1.m1.6.6.1.3.2.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.3.2"></sum></apply><apply id="S3.I4.i2.p1.1.m1.6.6.1.1.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.1"><csymbol cd="ambiguous" id="S3.I4.i2.p1.1.m1.6.6.1.1.2.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.1">superscript</csymbol><apply id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1"><minus id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.1.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.1"></minus><apply id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.2.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.2"><times id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.2.1.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.2.1"></times><ci id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.2.2a.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.2.2.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.2.2">u</mtext></ci><ci id="S3.I4.i2.p1.1.m1.4.4a.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.2.3.2"><mtext class="ltx_mathvariant_bold" id="S3.I4.i2.p1.1.m1.4.4.cmml" xref="S3.I4.i2.p1.1.m1.4.4">i</mtext></ci></apply><apply id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.3.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.3"><times id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.3.1.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.3.1"></times><ci id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.3.2a.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.3.2"><mtext class="ltx_mathvariant_bold" id="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.3.2.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.3.2">v</mtext></ci><ci id="S3.I4.i2.p1.1.m1.5.5a.cmml" xref="S3.I4.i2.p1.1.m1.6.6.1.1.1.1.1.3.3.2"><mtext class="ltx_mathvariant_bold" id="S3.I4.i2.p1.1.m1.5.5.cmml" xref="S3.I4.i2.p1.1.m1.5.5">i</mtext></ci></apply></apply><cn id="S3.I4.i2.p1.1.m1.6.6.1.1.3.cmml" type="integer" xref="S3.I4.i2.p1.1.m1.6.6.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I4.i2.p1.1.m1.6c">MSE\left(\textbf{u},\textbf{v}\right)=\frac{1}{\left|\textbf{u}\right|}%
\underset{\textbf{i}\in\textbf{u}}{\sum}\left(\textbf{u}\left(\textbf{i}\right%
)-\textbf{v}\left(\textbf{i}\right)\right)^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.I4.i2.p1.1.m1.6d">italic_M italic_S italic_E ( u , v ) = divide start_ARG 1 end_ARG start_ARG | u | end_ARG start_UNDERACCENT i ∈ u end_UNDERACCENT start_ARG ∑ end_ARG ( u ( i ) - v ( i ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.5">Where <math alttext="u" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.1.m1.1"><semantics id="S3.SS3.SSS1.p2.1.m1.1a"><mi id="S3.SS3.SSS1.p2.1.m1.1.1" xref="S3.SS3.SSS1.p2.1.m1.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.1.m1.1b"><ci id="S3.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1">𝑢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.1.m1.1c">u</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.1.m1.1d">italic_u</annotation></semantics></math> and <math alttext="v" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.2.m2.1"><semantics id="S3.SS3.SSS1.p2.2.m2.1a"><mi id="S3.SS3.SSS1.p2.2.m2.1.1" xref="S3.SS3.SSS1.p2.2.m2.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.2.m2.1b"><ci id="S3.SS3.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.2.m2.1c">v</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.2.m2.1d">italic_v</annotation></semantics></math> are, respectively, the output of the network and the ground truth. <math alttext="i" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.3.m3.1"><semantics id="S3.SS3.SSS1.p2.3.m3.1a"><mi id="S3.SS3.SSS1.p2.3.m3.1.1" xref="S3.SS3.SSS1.p2.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.3.m3.1b"><ci id="S3.SS3.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.3.m3.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.3.m3.1d">italic_i</annotation></semantics></math> represent a pixel and <math alttext="\left|\textbf{u}\right|" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.4.m4.1"><semantics id="S3.SS3.SSS1.p2.4.m4.1a"><mrow id="S3.SS3.SSS1.p2.4.m4.1.2.2" xref="S3.SS3.SSS1.p2.4.m4.1.2.1.cmml"><mo id="S3.SS3.SSS1.p2.4.m4.1.2.2.1" xref="S3.SS3.SSS1.p2.4.m4.1.2.1.1.cmml">|</mo><mtext class="ltx_mathvariant_bold" id="S3.SS3.SSS1.p2.4.m4.1.1" xref="S3.SS3.SSS1.p2.4.m4.1.1a.cmml">u</mtext><mo id="S3.SS3.SSS1.p2.4.m4.1.2.2.2" xref="S3.SS3.SSS1.p2.4.m4.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.4.m4.1b"><apply id="S3.SS3.SSS1.p2.4.m4.1.2.1.cmml" xref="S3.SS3.SSS1.p2.4.m4.1.2.2"><abs id="S3.SS3.SSS1.p2.4.m4.1.2.1.1.cmml" xref="S3.SS3.SSS1.p2.4.m4.1.2.2.1"></abs><ci id="S3.SS3.SSS1.p2.4.m4.1.1a.cmml" xref="S3.SS3.SSS1.p2.4.m4.1.1"><mtext class="ltx_mathvariant_bold" id="S3.SS3.SSS1.p2.4.m4.1.1.cmml" xref="S3.SS3.SSS1.p2.4.m4.1.1">u</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.4.m4.1c">\left|\textbf{u}\right|</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.4.m4.1d">| u |</annotation></semantics></math> the number of pixels of <math alttext="u" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p2.5.m5.1"><semantics id="S3.SS3.SSS1.p2.5.m5.1a"><mi id="S3.SS3.SSS1.p2.5.m5.1.1" xref="S3.SS3.SSS1.p2.5.m5.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.5.m5.1b"><ci id="S3.SS3.SSS1.p2.5.m5.1.1.cmml" xref="S3.SS3.SSS1.p2.5.m5.1.1">𝑢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.5.m5.1c">u</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p2.5.m5.1d">italic_u</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Training Procedure</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">To train the network, we follow the procedure in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib9" title="">9</a>]</cite> which proposes a progressive training that begins by learning on smaller image sizes and to progressively increase the size of images until we reach computation limitations. At each step,</p>
<ul class="ltx_itemize" id="S3.I5">
<li class="ltx_item" id="S3.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I5.i1.p1">
<p class="ltx_p" id="S3.I5.i1.p1.1">The model is initialized with the previous step final model (randomly if Step 1).</p>
</div>
</li>
<li class="ltx_item" id="S3.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I5.i2.p1">
<p class="ltx_p" id="S3.I5.i2.p1.1">The batch size is selected to match computation resources.</p>
</div>
</li>
<li class="ltx_item" id="S3.I5.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I5.i3.p1">
<p class="ltx_p" id="S3.I5.i3.p1.1">The learning rate is selected with regards to the batch size.</p>
</div>
</li>
<li class="ltx_item" id="S3.I5.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I5.i4.p1">
<p class="ltx_p" id="S3.I5.i4.p1.1">The model is trained for 20 epochs on the constructed dataset.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.SS3.SSS2.p1.2">We sum up in the Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S3.T1" title="Table 1 ‣ 3.3.2 Training Procedure ‣ 3.3 Training ‣ 3 Face Blurring using DeOldify ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">1</span></a> the training hyperparameters of each step of training.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.9" style="width:433.6pt;height:124.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(91.4pt,-26.3pt) scale(1.72928978905718,1.72928978905718) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.9.9">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.9.9.10.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T1.9.9.10.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S3.T1.9.9.10.1.2">Image size</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S3.T1.9.9.10.1.3">Batch Size</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.9.9.10.1.4">learning rate init.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.3.3.3.4">Step 1</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.1.1"><math alttext="64\times 64" class="ltx_Math" display="inline" id="S3.T1.1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.1.m1.1a"><mrow id="S3.T1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.cmml"><mn id="S3.T1.1.1.1.1.m1.1.1.2" xref="S3.T1.1.1.1.1.m1.1.1.2.cmml">64</mn><mo id="S3.T1.1.1.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T1.1.1.1.1.m1.1.1.1.cmml">×</mo><mn id="S3.T1.1.1.1.1.m1.1.1.3" xref="S3.T1.1.1.1.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><apply id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1"><times id="S3.T1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1.1"></times><cn id="S3.T1.1.1.1.1.m1.1.1.2.cmml" type="integer" xref="S3.T1.1.1.1.1.m1.1.1.2">64</cn><cn id="S3.T1.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S3.T1.1.1.1.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">64\times 64</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.1.m1.1d">64 × 64</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.2.2.2"><math alttext="80" class="ltx_Math" display="inline" id="S3.T1.2.2.2.2.m1.1"><semantics id="S3.T1.2.2.2.2.m1.1a"><mn id="S3.T1.2.2.2.2.m1.1.1" xref="S3.T1.2.2.2.2.m1.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.m1.1b"><cn id="S3.T1.2.2.2.2.m1.1.1.cmml" type="integer" xref="S3.T1.2.2.2.2.m1.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.m1.1c">80</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.2.m1.1d">80</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3"><math alttext="10^{-3}" class="ltx_Math" display="inline" id="S3.T1.3.3.3.3.m1.1"><semantics id="S3.T1.3.3.3.3.m1.1a"><msup id="S3.T1.3.3.3.3.m1.1.1" xref="S3.T1.3.3.3.3.m1.1.1.cmml"><mn id="S3.T1.3.3.3.3.m1.1.1.2" xref="S3.T1.3.3.3.3.m1.1.1.2.cmml">10</mn><mrow id="S3.T1.3.3.3.3.m1.1.1.3" xref="S3.T1.3.3.3.3.m1.1.1.3.cmml"><mo id="S3.T1.3.3.3.3.m1.1.1.3a" xref="S3.T1.3.3.3.3.m1.1.1.3.cmml">−</mo><mn id="S3.T1.3.3.3.3.m1.1.1.3.2" xref="S3.T1.3.3.3.3.m1.1.1.3.2.cmml">3</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.m1.1b"><apply id="S3.T1.3.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.3.3.3.3.m1.1.1.1.cmml" xref="S3.T1.3.3.3.3.m1.1.1">superscript</csymbol><cn id="S3.T1.3.3.3.3.m1.1.1.2.cmml" type="integer" xref="S3.T1.3.3.3.3.m1.1.1.2">10</cn><apply id="S3.T1.3.3.3.3.m1.1.1.3.cmml" xref="S3.T1.3.3.3.3.m1.1.1.3"><minus id="S3.T1.3.3.3.3.m1.1.1.3.1.cmml" xref="S3.T1.3.3.3.3.m1.1.1.3"></minus><cn id="S3.T1.3.3.3.3.m1.1.1.3.2.cmml" type="integer" xref="S3.T1.3.3.3.3.m1.1.1.3.2">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.m1.1c">10^{-3}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.3.3.m1.1d">10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.6.6.6.4">Step 2</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.4.4.4.1"><math alttext="128\times 128" class="ltx_Math" display="inline" id="S3.T1.4.4.4.1.m1.1"><semantics id="S3.T1.4.4.4.1.m1.1a"><mrow id="S3.T1.4.4.4.1.m1.1.1" xref="S3.T1.4.4.4.1.m1.1.1.cmml"><mn id="S3.T1.4.4.4.1.m1.1.1.2" xref="S3.T1.4.4.4.1.m1.1.1.2.cmml">128</mn><mo id="S3.T1.4.4.4.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T1.4.4.4.1.m1.1.1.1.cmml">×</mo><mn id="S3.T1.4.4.4.1.m1.1.1.3" xref="S3.T1.4.4.4.1.m1.1.1.3.cmml">128</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.1.m1.1b"><apply id="S3.T1.4.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.4.1.m1.1.1"><times id="S3.T1.4.4.4.1.m1.1.1.1.cmml" xref="S3.T1.4.4.4.1.m1.1.1.1"></times><cn id="S3.T1.4.4.4.1.m1.1.1.2.cmml" type="integer" xref="S3.T1.4.4.4.1.m1.1.1.2">128</cn><cn id="S3.T1.4.4.4.1.m1.1.1.3.cmml" type="integer" xref="S3.T1.4.4.4.1.m1.1.1.3">128</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.1.m1.1c">128\times 128</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.4.1.m1.1d">128 × 128</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.5.5.5.2"><math alttext="20" class="ltx_Math" display="inline" id="S3.T1.5.5.5.2.m1.1"><semantics id="S3.T1.5.5.5.2.m1.1a"><mn id="S3.T1.5.5.5.2.m1.1.1" xref="S3.T1.5.5.5.2.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.2.m1.1b"><cn id="S3.T1.5.5.5.2.m1.1.1.cmml" type="integer" xref="S3.T1.5.5.5.2.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.2.m1.1c">20</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.5.2.m1.1d">20</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.3"><math alttext="10^{-4}" class="ltx_Math" display="inline" id="S3.T1.6.6.6.3.m1.1"><semantics id="S3.T1.6.6.6.3.m1.1a"><msup id="S3.T1.6.6.6.3.m1.1.1" xref="S3.T1.6.6.6.3.m1.1.1.cmml"><mn id="S3.T1.6.6.6.3.m1.1.1.2" xref="S3.T1.6.6.6.3.m1.1.1.2.cmml">10</mn><mrow id="S3.T1.6.6.6.3.m1.1.1.3" xref="S3.T1.6.6.6.3.m1.1.1.3.cmml"><mo id="S3.T1.6.6.6.3.m1.1.1.3a" xref="S3.T1.6.6.6.3.m1.1.1.3.cmml">−</mo><mn id="S3.T1.6.6.6.3.m1.1.1.3.2" xref="S3.T1.6.6.6.3.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.3.m1.1b"><apply id="S3.T1.6.6.6.3.m1.1.1.cmml" xref="S3.T1.6.6.6.3.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.6.6.6.3.m1.1.1.1.cmml" xref="S3.T1.6.6.6.3.m1.1.1">superscript</csymbol><cn id="S3.T1.6.6.6.3.m1.1.1.2.cmml" type="integer" xref="S3.T1.6.6.6.3.m1.1.1.2">10</cn><apply id="S3.T1.6.6.6.3.m1.1.1.3.cmml" xref="S3.T1.6.6.6.3.m1.1.1.3"><minus id="S3.T1.6.6.6.3.m1.1.1.3.1.cmml" xref="S3.T1.6.6.6.3.m1.1.1.3"></minus><cn id="S3.T1.6.6.6.3.m1.1.1.3.2.cmml" type="integer" xref="S3.T1.6.6.6.3.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.3.m1.1c">10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.6.6.3.m1.1d">10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S3.T1.9.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.9.9.9.4">Step 3</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.7.7.7.1"><math alttext="192\times 192" class="ltx_Math" display="inline" id="S3.T1.7.7.7.1.m1.1"><semantics id="S3.T1.7.7.7.1.m1.1a"><mrow id="S3.T1.7.7.7.1.m1.1.1" xref="S3.T1.7.7.7.1.m1.1.1.cmml"><mn id="S3.T1.7.7.7.1.m1.1.1.2" xref="S3.T1.7.7.7.1.m1.1.1.2.cmml">192</mn><mo id="S3.T1.7.7.7.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T1.7.7.7.1.m1.1.1.1.cmml">×</mo><mn id="S3.T1.7.7.7.1.m1.1.1.3" xref="S3.T1.7.7.7.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.7.1.m1.1b"><apply id="S3.T1.7.7.7.1.m1.1.1.cmml" xref="S3.T1.7.7.7.1.m1.1.1"><times id="S3.T1.7.7.7.1.m1.1.1.1.cmml" xref="S3.T1.7.7.7.1.m1.1.1.1"></times><cn id="S3.T1.7.7.7.1.m1.1.1.2.cmml" type="integer" xref="S3.T1.7.7.7.1.m1.1.1.2">192</cn><cn id="S3.T1.7.7.7.1.m1.1.1.3.cmml" type="integer" xref="S3.T1.7.7.7.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.7.1.m1.1c">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S3.T1.7.7.7.1.m1.1d">192 × 192</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.8.8.8.2"><math alttext="8" class="ltx_Math" display="inline" id="S3.T1.8.8.8.2.m1.1"><semantics id="S3.T1.8.8.8.2.m1.1a"><mn id="S3.T1.8.8.8.2.m1.1.1" xref="S3.T1.8.8.8.2.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.8.2.m1.1b"><cn id="S3.T1.8.8.8.2.m1.1.1.cmml" type="integer" xref="S3.T1.8.8.8.2.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.8.2.m1.1c">8</annotation><annotation encoding="application/x-llamapun" id="S3.T1.8.8.8.2.m1.1d">8</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.9.9.9.3"><math alttext="5\times 10^{-5}" class="ltx_Math" display="inline" id="S3.T1.9.9.9.3.m1.1"><semantics id="S3.T1.9.9.9.3.m1.1a"><mrow id="S3.T1.9.9.9.3.m1.1.1" xref="S3.T1.9.9.9.3.m1.1.1.cmml"><mn id="S3.T1.9.9.9.3.m1.1.1.2" xref="S3.T1.9.9.9.3.m1.1.1.2.cmml">5</mn><mo id="S3.T1.9.9.9.3.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.T1.9.9.9.3.m1.1.1.1.cmml">×</mo><msup id="S3.T1.9.9.9.3.m1.1.1.3" xref="S3.T1.9.9.9.3.m1.1.1.3.cmml"><mn id="S3.T1.9.9.9.3.m1.1.1.3.2" xref="S3.T1.9.9.9.3.m1.1.1.3.2.cmml">10</mn><mrow id="S3.T1.9.9.9.3.m1.1.1.3.3" xref="S3.T1.9.9.9.3.m1.1.1.3.3.cmml"><mo id="S3.T1.9.9.9.3.m1.1.1.3.3a" xref="S3.T1.9.9.9.3.m1.1.1.3.3.cmml">−</mo><mn id="S3.T1.9.9.9.3.m1.1.1.3.3.2" xref="S3.T1.9.9.9.3.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.9.3.m1.1b"><apply id="S3.T1.9.9.9.3.m1.1.1.cmml" xref="S3.T1.9.9.9.3.m1.1.1"><times id="S3.T1.9.9.9.3.m1.1.1.1.cmml" xref="S3.T1.9.9.9.3.m1.1.1.1"></times><cn id="S3.T1.9.9.9.3.m1.1.1.2.cmml" type="integer" xref="S3.T1.9.9.9.3.m1.1.1.2">5</cn><apply id="S3.T1.9.9.9.3.m1.1.1.3.cmml" xref="S3.T1.9.9.9.3.m1.1.1.3"><csymbol cd="ambiguous" id="S3.T1.9.9.9.3.m1.1.1.3.1.cmml" xref="S3.T1.9.9.9.3.m1.1.1.3">superscript</csymbol><cn id="S3.T1.9.9.9.3.m1.1.1.3.2.cmml" type="integer" xref="S3.T1.9.9.9.3.m1.1.1.3.2">10</cn><apply id="S3.T1.9.9.9.3.m1.1.1.3.3.cmml" xref="S3.T1.9.9.9.3.m1.1.1.3.3"><minus id="S3.T1.9.9.9.3.m1.1.1.3.3.1.cmml" xref="S3.T1.9.9.9.3.m1.1.1.3.3"></minus><cn id="S3.T1.9.9.9.3.m1.1.1.3.3.2.cmml" type="integer" xref="S3.T1.9.9.9.3.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.9.3.m1.1c">5\times 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.9.9.9.3.m1.1d">5 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.11.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.12.2" style="font-size:90%;">Training details.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.1">Training is performed using an AdamW optimizer coupled with a exponential decay learning rate scheduler of multiplicative factor of <math alttext="0.8" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p2.1.m1.1"><semantics id="S3.SS3.SSS2.p2.1.m1.1a"><mn id="S3.SS3.SSS2.p2.1.m1.1.1" xref="S3.SS3.SSS2.p2.1.m1.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p2.1.m1.1b"><cn id="S3.SS3.SSS2.p2.1.m1.1.1.cmml" type="float" xref="S3.SS3.SSS2.p2.1.m1.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p2.1.m1.1c">0.8</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p2.1.m1.1d">0.8</annotation></semantics></math>. We use a GPU NVIDIA GeForce RTX 2080 Ti of 11GB.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Inference methodology</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Upon completing the training process, our objective is to develop a methodology that performs face blurring in an input image of arbitrary dimensions. As in the case of YOLO, when we want to blur a video we operate at a frame level, getting rid of any temporal information. 
<br class="ltx_break"/>A naive strategy would be to directly give the raw image as input to the network but the higher the image resolution the higher the computation time will be. If we do not control the input image resolution we can not control the computation time. Then, we develop an inference methodology which we detail below and which is summed up in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S3.F6" title="Figure 6 ‣ 3.4 Inference methodology ‣ 3 Face Blurring using DeOldify ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="521" id="S3.F6.g1" src="extracted/5872765/figures/schema_explicatif.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S3.F6.3.2" style="font-size:90%;">Inference methodology.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS4.p2">
<ul class="ltx_itemize" id="S3.I6">
<li class="ltx_item" id="S3.I6.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I6.i1.p1">
<p class="ltx_p" id="S3.I6.i1.p1.3"><span class="ltx_text ltx_font_bold" id="S3.I6.i1.p1.3.1">Downsampling</span>. The input image is downsampled to a given size. In practice we use either <math alttext="192\times 192" class="ltx_Math" display="inline" id="S3.I6.i1.p1.1.m1.1"><semantics id="S3.I6.i1.p1.1.m1.1a"><mrow id="S3.I6.i1.p1.1.m1.1.1" xref="S3.I6.i1.p1.1.m1.1.1.cmml"><mn id="S3.I6.i1.p1.1.m1.1.1.2" xref="S3.I6.i1.p1.1.m1.1.1.2.cmml">192</mn><mo id="S3.I6.i1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.I6.i1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.I6.i1.p1.1.m1.1.1.3" xref="S3.I6.i1.p1.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I6.i1.p1.1.m1.1b"><apply id="S3.I6.i1.p1.1.m1.1.1.cmml" xref="S3.I6.i1.p1.1.m1.1.1"><times id="S3.I6.i1.p1.1.m1.1.1.1.cmml" xref="S3.I6.i1.p1.1.m1.1.1.1"></times><cn id="S3.I6.i1.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.I6.i1.p1.1.m1.1.1.2">192</cn><cn id="S3.I6.i1.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.I6.i1.p1.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I6.i1.p1.1.m1.1c">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S3.I6.i1.p1.1.m1.1d">192 × 192</annotation></semantics></math>, <math alttext="256\times 256" class="ltx_Math" display="inline" id="S3.I6.i1.p1.2.m2.1"><semantics id="S3.I6.i1.p1.2.m2.1a"><mrow id="S3.I6.i1.p1.2.m2.1.1" xref="S3.I6.i1.p1.2.m2.1.1.cmml"><mn id="S3.I6.i1.p1.2.m2.1.1.2" xref="S3.I6.i1.p1.2.m2.1.1.2.cmml">256</mn><mo id="S3.I6.i1.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.I6.i1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.I6.i1.p1.2.m2.1.1.3" xref="S3.I6.i1.p1.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I6.i1.p1.2.m2.1b"><apply id="S3.I6.i1.p1.2.m2.1.1.cmml" xref="S3.I6.i1.p1.2.m2.1.1"><times id="S3.I6.i1.p1.2.m2.1.1.1.cmml" xref="S3.I6.i1.p1.2.m2.1.1.1"></times><cn id="S3.I6.i1.p1.2.m2.1.1.2.cmml" type="integer" xref="S3.I6.i1.p1.2.m2.1.1.2">256</cn><cn id="S3.I6.i1.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.I6.i1.p1.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I6.i1.p1.2.m2.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S3.I6.i1.p1.2.m2.1d">256 × 256</annotation></semantics></math> or <math alttext="512\times 512" class="ltx_Math" display="inline" id="S3.I6.i1.p1.3.m3.1"><semantics id="S3.I6.i1.p1.3.m3.1a"><mrow id="S3.I6.i1.p1.3.m3.1.1" xref="S3.I6.i1.p1.3.m3.1.1.cmml"><mn id="S3.I6.i1.p1.3.m3.1.1.2" xref="S3.I6.i1.p1.3.m3.1.1.2.cmml">512</mn><mo id="S3.I6.i1.p1.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.I6.i1.p1.3.m3.1.1.1.cmml">×</mo><mn id="S3.I6.i1.p1.3.m3.1.1.3" xref="S3.I6.i1.p1.3.m3.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I6.i1.p1.3.m3.1b"><apply id="S3.I6.i1.p1.3.m3.1.1.cmml" xref="S3.I6.i1.p1.3.m3.1.1"><times id="S3.I6.i1.p1.3.m3.1.1.1.cmml" xref="S3.I6.i1.p1.3.m3.1.1.1"></times><cn id="S3.I6.i1.p1.3.m3.1.1.2.cmml" type="integer" xref="S3.I6.i1.p1.3.m3.1.1.2">512</cn><cn id="S3.I6.i1.p1.3.m3.1.1.3.cmml" type="integer" xref="S3.I6.i1.p1.3.m3.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I6.i1.p1.3.m3.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S3.I6.i1.p1.3.m3.1d">512 × 512</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I6.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I6.i2.p1">
<p class="ltx_p" id="S3.I6.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I6.i2.p1.1.1">Model forward</span>. The resized image is passed through the network.</p>
</div>
</li>
<li class="ltx_item" id="S3.I6.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I6.i3.p1">
<p class="ltx_p" id="S3.I6.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I6.i3.p1.1.1">Upsampling</span>. The blurred image is resized to its original input size.</p>
</div>
</li>
<li class="ltx_item" id="S3.I6.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I6.i4.p1">
<p class="ltx_p" id="S3.I6.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I6.i4.p1.1.1">L1 norm threshold</span>. We compute the difference between the value at each pixel in the downsampled input image and the downsampled blurred image. By thresholding these differences a binary mask containing the face is obtained.</p>
</div>
</li>
<li class="ltx_item" id="S3.I6.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I6.i5.p1">
<p class="ltx_p" id="S3.I6.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I6.i5.p1.1.1">Mask Upsampling</span>. The mask is resized to obtain a new mask with the dimensions of the original input.</p>
</div>
</li>
<li class="ltx_item" id="S3.I6.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I6.i6.p1">
<p class="ltx_p" id="S3.I6.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I6.i6.p1.1.1">Blurred faces extraction</span>. We combine the resized mask and the resized upsampled output to extract only faces from the output.</p>
</div>
</li>
<li class="ltx_item" id="S3.I6.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I6.i7.p1">
<p class="ltx_p" id="S3.I6.i7.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I6.i7.p1.1.1">Face replacement in the full resolution input</span>. We combine the full resolution input with the blurred faces extracted at the previous step to construct the full resolution image with blurred faces.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">What is left to determine is the choice of hyperparameters in that methodology, namely the dimension on downsizing, as well as the level of thresholding for mask extraction.</p>
<ul class="ltx_itemize" id="S3.I7">
<li class="ltx_item" id="S3.I7.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I7.i1.p1">
<p class="ltx_p" id="S3.I7.i1.p1.3"><span class="ltx_text ltx_font_bold" id="S3.I7.i1.p1.3.1">Choice of thresholding</span>. This choice is totally empirical. After a careful study, we observe that a wide range of thresholds give almost the same results. In fact, following the code implementation in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib9" title="">9</a>]</cite>, the difference between the values of the input and the output of the network range from -3 to 3 so that if the model blurs a face the difference in the pixel is significantly different from <math alttext="0" class="ltx_Math" display="inline" id="S3.I7.i1.p1.1.m1.1"><semantics id="S3.I7.i1.p1.1.m1.1a"><mn id="S3.I7.i1.p1.1.m1.1.1" xref="S3.I7.i1.p1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.I7.i1.p1.1.m1.1b"><cn id="S3.I7.i1.p1.1.m1.1.1.cmml" type="integer" xref="S3.I7.i1.p1.1.m1.1.1">0</cn></annotation-xml></semantics></math> (on average a pixel blurred has a difference of <math alttext="0.4" class="ltx_Math" display="inline" id="S3.I7.i1.p1.2.m2.1"><semantics id="S3.I7.i1.p1.2.m2.1a"><mn id="S3.I7.i1.p1.2.m2.1.1" xref="S3.I7.i1.p1.2.m2.1.1.cmml">0.4</mn><annotation-xml encoding="MathML-Content" id="S3.I7.i1.p1.2.m2.1b"><cn id="S3.I7.i1.p1.2.m2.1.1.cmml" type="float" xref="S3.I7.i1.p1.2.m2.1.1">0.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.I7.i1.p1.2.m2.1c">0.4</annotation><annotation encoding="application/x-llamapun" id="S3.I7.i1.p1.2.m2.1d">0.4</annotation></semantics></math> with respect to the original value). We then set the L1 threshold at <math alttext="0.1" class="ltx_Math" display="inline" id="S3.I7.i1.p1.3.m3.1"><semantics id="S3.I7.i1.p1.3.m3.1a"><mn id="S3.I7.i1.p1.3.m3.1.1" xref="S3.I7.i1.p1.3.m3.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S3.I7.i1.p1.3.m3.1b"><cn id="S3.I7.i1.p1.3.m3.1.1.cmml" type="float" xref="S3.I7.i1.p1.3.m3.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.I7.i1.p1.3.m3.1c">0.1</annotation><annotation encoding="application/x-llamapun" id="S3.I7.i1.p1.3.m3.1d">0.1</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I7.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I7.i2.p1">
<p class="ltx_p" id="S3.I7.i2.p1.2"><span class="ltx_text ltx_font_bold" id="S3.I7.i2.p1.2.1">Downsampling size</span>. Regarding the size of downsampling, this hyperparameter is crucial. In fact, during training, the bigger the size of the face in the image, the bigger is the difference between input and target and the easier it is for the network to learn to detect and blur the face. Therefore, bigger faces tend to be much more easily detected and blurred. At inference time, what we observe is that if we downsample at the size of the last step of the network training (<math alttext="192\times 192" class="ltx_Math" display="inline" id="S3.I7.i2.p1.1.m1.1"><semantics id="S3.I7.i2.p1.1.m1.1a"><mrow id="S3.I7.i2.p1.1.m1.1.1" xref="S3.I7.i2.p1.1.m1.1.1.cmml"><mn id="S3.I7.i2.p1.1.m1.1.1.2" xref="S3.I7.i2.p1.1.m1.1.1.2.cmml">192</mn><mo id="S3.I7.i2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.I7.i2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.I7.i2.p1.1.m1.1.1.3" xref="S3.I7.i2.p1.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I7.i2.p1.1.m1.1b"><apply id="S3.I7.i2.p1.1.m1.1.1.cmml" xref="S3.I7.i2.p1.1.m1.1.1"><times id="S3.I7.i2.p1.1.m1.1.1.1.cmml" xref="S3.I7.i2.p1.1.m1.1.1.1"></times><cn id="S3.I7.i2.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.I7.i2.p1.1.m1.1.1.2">192</cn><cn id="S3.I7.i2.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.I7.i2.p1.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I7.i2.p1.1.m1.1c">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S3.I7.i2.p1.1.m1.1d">192 × 192</annotation></semantics></math>) it fails to detect small faces that do not cover much pixels (see Section <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4" title="4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">4</span></a> for examples of the phenomenon). For this reason, it is interesting to set a higher downsampling size (<math alttext="512\times 512" class="ltx_Math" display="inline" id="S3.I7.i2.p1.2.m2.1"><semantics id="S3.I7.i2.p1.2.m2.1a"><mrow id="S3.I7.i2.p1.2.m2.1.1" xref="S3.I7.i2.p1.2.m2.1.1.cmml"><mn id="S3.I7.i2.p1.2.m2.1.1.2" xref="S3.I7.i2.p1.2.m2.1.1.2.cmml">512</mn><mo id="S3.I7.i2.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.I7.i2.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.I7.i2.p1.2.m2.1.1.3" xref="S3.I7.i2.p1.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I7.i2.p1.2.m2.1b"><apply id="S3.I7.i2.p1.2.m2.1.1.cmml" xref="S3.I7.i2.p1.2.m2.1.1"><times id="S3.I7.i2.p1.2.m2.1.1.1.cmml" xref="S3.I7.i2.p1.2.m2.1.1.1"></times><cn id="S3.I7.i2.p1.2.m2.1.1.2.cmml" type="integer" xref="S3.I7.i2.p1.2.m2.1.1.2">512</cn><cn id="S3.I7.i2.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.I7.i2.p1.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I7.i2.p1.2.m2.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S3.I7.i2.p1.2.m2.1d">512 × 512</annotation></semantics></math> for example), so that small faces cover more pixels and consequently are more easily detected by the network. Of course, the counterpart is that inference time in then higher. This tradeoff will be discussed in Section  <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4" title="4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation Methodology</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In the face blurring context, evaluation can not be done completely quantitatively. In fact, for a given image, there is a large amount of different blurs that can complete the task. Still, we can compare methods using two criteria:</p>
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Visual evaluation and face counting</span>. We evaluate visually if face blurring is correctly done through a straight-forward criteria. Correctly blurred means we cannot recognize the original person in the output image. For each test set image, we count all correctly blurred faces.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Computation time</span>. This is a quantitative metric, which allows us to compare the inference time computation between models.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">To perform evaluation we construct a test set of 6 images which features several specific particularities, namely:</p>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1">Variations in face distance to the camera.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1">Variations in orientation of faces (faces not facing the camera).</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1">Variations in number of faces.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i4.p1">
<p class="ltx_p" id="S4.I2.i4.p1.1">Variations in the face itself (partially masked, wearing glasses or hats).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">In this section we will test the influence on the results of several hyperparameters of the Unet network. We will extensively study the downsampling dimension for inference, the choice of the loss and the relevance of the self-attention layer. We will also conduct some experiments using the YOLOv5Face face detector.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experiments using DeOldify Unet</h3>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Influence of the downsampling dimension for inference</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">In this section, we study the influence of the downsampling dimension using the Unet method. We recall that the downsampling dimension corresponds to the size to which the image is resized before passing it through the face blurring model. The last stage of training of the model uses input images of size <math alttext="192\times 192" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.1.m1.1"><semantics id="S4.SS2.SSS1.p1.1.m1.1a"><mrow id="S4.SS2.SSS1.p1.1.m1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.cmml"><mn id="S4.SS2.SSS1.p1.1.m1.1.1.2" xref="S4.SS2.SSS1.p1.1.m1.1.1.2.cmml">192</mn><mo id="S4.SS2.SSS1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.SSS1.p1.1.m1.1.1.3" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.1.m1.1b"><apply id="S4.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1"><times id="S4.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.1"></times><cn id="S4.SS2.SSS1.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.SS2.SSS1.p1.1.m1.1.1.2">192</cn><cn id="S4.SS2.SSS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS2.SSS1.p1.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.1.m1.1c">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p1.1.m1.1d">192 × 192</annotation></semantics></math>, we then could think the network is more fitted to be inferred at this size. But practically and as we said in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S3.SS4" title="3.4 Inference methodology ‣ 3 Face Blurring using DeOldify ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">3.4</span></a>, bigger faces tend to be more easily detected and blurred, as they cover more pixels during training, which facilitates detection. That observation motivates the study of the influence of the downsampling dimension. 
<br class="ltx_break"/>In this section, all results correspond to models that were trained with the L1 loss.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.3"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p2.3.1">Visual evaluation
<br class="ltx_break"/></span>We show in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.F7" title="Figure 7 ‣ 4.2.1 Influence of the downsampling dimension for inference ‣ 4.2 Experiments using DeOldify Unet ‣ 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">7</span></a> results obtained for three downsampling dimensions from left to right : <math alttext="192\times 192" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p2.1.m1.1"><semantics id="S4.SS2.SSS1.p2.1.m1.1a"><mrow id="S4.SS2.SSS1.p2.1.m1.1.1" xref="S4.SS2.SSS1.p2.1.m1.1.1.cmml"><mn id="S4.SS2.SSS1.p2.1.m1.1.1.2" xref="S4.SS2.SSS1.p2.1.m1.1.1.2.cmml">192</mn><mo id="S4.SS2.SSS1.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS1.p2.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.SSS1.p2.1.m1.1.1.3" xref="S4.SS2.SSS1.p2.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.1.m1.1b"><apply id="S4.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1"><times id="S4.SS2.SSS1.p2.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.p2.1.m1.1.1.1"></times><cn id="S4.SS2.SSS1.p2.1.m1.1.1.2.cmml" type="integer" xref="S4.SS2.SSS1.p2.1.m1.1.1.2">192</cn><cn id="S4.SS2.SSS1.p2.1.m1.1.1.3.cmml" type="integer" xref="S4.SS2.SSS1.p2.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.1.m1.1c">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p2.1.m1.1d">192 × 192</annotation></semantics></math>, <math alttext="256\times 256" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p2.2.m2.1"><semantics id="S4.SS2.SSS1.p2.2.m2.1a"><mrow id="S4.SS2.SSS1.p2.2.m2.1.1" xref="S4.SS2.SSS1.p2.2.m2.1.1.cmml"><mn id="S4.SS2.SSS1.p2.2.m2.1.1.2" xref="S4.SS2.SSS1.p2.2.m2.1.1.2.cmml">256</mn><mo id="S4.SS2.SSS1.p2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS1.p2.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS2.SSS1.p2.2.m2.1.1.3" xref="S4.SS2.SSS1.p2.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.2.m2.1b"><apply id="S4.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p2.2.m2.1.1"><times id="S4.SS2.SSS1.p2.2.m2.1.1.1.cmml" xref="S4.SS2.SSS1.p2.2.m2.1.1.1"></times><cn id="S4.SS2.SSS1.p2.2.m2.1.1.2.cmml" type="integer" xref="S4.SS2.SSS1.p2.2.m2.1.1.2">256</cn><cn id="S4.SS2.SSS1.p2.2.m2.1.1.3.cmml" type="integer" xref="S4.SS2.SSS1.p2.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.2.m2.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p2.2.m2.1d">256 × 256</annotation></semantics></math> and <math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p2.3.m3.1"><semantics id="S4.SS2.SSS1.p2.3.m3.1a"><mrow id="S4.SS2.SSS1.p2.3.m3.1.1" xref="S4.SS2.SSS1.p2.3.m3.1.1.cmml"><mn id="S4.SS2.SSS1.p2.3.m3.1.1.2" xref="S4.SS2.SSS1.p2.3.m3.1.1.2.cmml">512</mn><mo id="S4.SS2.SSS1.p2.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS1.p2.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS2.SSS1.p2.3.m3.1.1.3" xref="S4.SS2.SSS1.p2.3.m3.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p2.3.m3.1b"><apply id="S4.SS2.SSS1.p2.3.m3.1.1.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.1"><times id="S4.SS2.SSS1.p2.3.m3.1.1.1.cmml" xref="S4.SS2.SSS1.p2.3.m3.1.1.1"></times><cn id="S4.SS2.SSS1.p2.3.m3.1.1.2.cmml" type="integer" xref="S4.SS2.SSS1.p2.3.m3.1.1.2">512</cn><cn id="S4.SS2.SSS1.p2.3.m3.1.1.3.cmml" type="integer" xref="S4.SS2.SSS1.p2.3.m3.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p2.3.m3.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p2.3.m3.1d">512 × 512</annotation></semantics></math></p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1">We observe several phenomena.</p>
<ul class="ltx_itemize" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.3">When a face is big, meaning it covers a great percentage of the whole image (as in the two first images of Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.F7" title="Figure 7 ‣ 4.2.1 Influence of the downsampling dimension for inference ‣ 4.2 Experiments using DeOldify Unet ‣ 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">7</span></a>), small downsampling dimensions seem more fitted. In fact, we see in the first image, that for dimensions <math alttext="192\times 192" class="ltx_Math" display="inline" id="S4.I3.i1.p1.1.m1.1"><semantics id="S4.I3.i1.p1.1.m1.1a"><mrow id="S4.I3.i1.p1.1.m1.1.1" xref="S4.I3.i1.p1.1.m1.1.1.cmml"><mn id="S4.I3.i1.p1.1.m1.1.1.2" xref="S4.I3.i1.p1.1.m1.1.1.2.cmml">192</mn><mo id="S4.I3.i1.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.I3.i1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.I3.i1.p1.1.m1.1.1.3" xref="S4.I3.i1.p1.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I3.i1.p1.1.m1.1b"><apply id="S4.I3.i1.p1.1.m1.1.1.cmml" xref="S4.I3.i1.p1.1.m1.1.1"><times id="S4.I3.i1.p1.1.m1.1.1.1.cmml" xref="S4.I3.i1.p1.1.m1.1.1.1"></times><cn id="S4.I3.i1.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.I3.i1.p1.1.m1.1.1.2">192</cn><cn id="S4.I3.i1.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.I3.i1.p1.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i1.p1.1.m1.1c">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i1.p1.1.m1.1d">192 × 192</annotation></semantics></math> and <math alttext="256\times 256" class="ltx_Math" display="inline" id="S4.I3.i1.p1.2.m2.1"><semantics id="S4.I3.i1.p1.2.m2.1a"><mrow id="S4.I3.i1.p1.2.m2.1.1" xref="S4.I3.i1.p1.2.m2.1.1.cmml"><mn id="S4.I3.i1.p1.2.m2.1.1.2" xref="S4.I3.i1.p1.2.m2.1.1.2.cmml">256</mn><mo id="S4.I3.i1.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.I3.i1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S4.I3.i1.p1.2.m2.1.1.3" xref="S4.I3.i1.p1.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I3.i1.p1.2.m2.1b"><apply id="S4.I3.i1.p1.2.m2.1.1.cmml" xref="S4.I3.i1.p1.2.m2.1.1"><times id="S4.I3.i1.p1.2.m2.1.1.1.cmml" xref="S4.I3.i1.p1.2.m2.1.1.1"></times><cn id="S4.I3.i1.p1.2.m2.1.1.2.cmml" type="integer" xref="S4.I3.i1.p1.2.m2.1.1.2">256</cn><cn id="S4.I3.i1.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.I3.i1.p1.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i1.p1.2.m2.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i1.p1.2.m2.1d">256 × 256</annotation></semantics></math> faces are completely blurred whereas we can distinguish some face features for dimension <math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.I3.i1.p1.3.m3.1"><semantics id="S4.I3.i1.p1.3.m3.1a"><mrow id="S4.I3.i1.p1.3.m3.1.1" xref="S4.I3.i1.p1.3.m3.1.1.cmml"><mn id="S4.I3.i1.p1.3.m3.1.1.2" xref="S4.I3.i1.p1.3.m3.1.1.2.cmml">512</mn><mo id="S4.I3.i1.p1.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.I3.i1.p1.3.m3.1.1.1.cmml">×</mo><mn id="S4.I3.i1.p1.3.m3.1.1.3" xref="S4.I3.i1.p1.3.m3.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I3.i1.p1.3.m3.1b"><apply id="S4.I3.i1.p1.3.m3.1.1.cmml" xref="S4.I3.i1.p1.3.m3.1.1"><times id="S4.I3.i1.p1.3.m3.1.1.1.cmml" xref="S4.I3.i1.p1.3.m3.1.1.1"></times><cn id="S4.I3.i1.p1.3.m3.1.1.2.cmml" type="integer" xref="S4.I3.i1.p1.3.m3.1.1.2">512</cn><cn id="S4.I3.i1.p1.3.m3.1.1.3.cmml" type="integer" xref="S4.I3.i1.p1.3.m3.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i1.p1.3.m3.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i1.p1.3.m3.1d">512 × 512</annotation></semantics></math> (although, the face still remains unrecognizable). Then, on such images, small downsampling dimension seem better fitted.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.5">When faces are smaller, the option that uses dimensions <math alttext="192\times 192" class="ltx_Math" display="inline" id="S4.I3.i2.p1.1.m1.1"><semantics id="S4.I3.i2.p1.1.m1.1a"><mrow id="S4.I3.i2.p1.1.m1.1.1" xref="S4.I3.i2.p1.1.m1.1.1.cmml"><mn id="S4.I3.i2.p1.1.m1.1.1.2" xref="S4.I3.i2.p1.1.m1.1.1.2.cmml">192</mn><mo id="S4.I3.i2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.I3.i2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.I3.i2.p1.1.m1.1.1.3" xref="S4.I3.i2.p1.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I3.i2.p1.1.m1.1b"><apply id="S4.I3.i2.p1.1.m1.1.1.cmml" xref="S4.I3.i2.p1.1.m1.1.1"><times id="S4.I3.i2.p1.1.m1.1.1.1.cmml" xref="S4.I3.i2.p1.1.m1.1.1.1"></times><cn id="S4.I3.i2.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.I3.i2.p1.1.m1.1.1.2">192</cn><cn id="S4.I3.i2.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.I3.i2.p1.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i2.p1.1.m1.1c">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i2.p1.1.m1.1d">192 × 192</annotation></semantics></math> starts to struggle a lot to blur faces. First, it does not succeed in blurring the face of a person with glasses (third image) and also misses a lot of faces in the two last images. On the other hand, the option with <math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.I3.i2.p1.2.m2.1"><semantics id="S4.I3.i2.p1.2.m2.1a"><mrow id="S4.I3.i2.p1.2.m2.1.1" xref="S4.I3.i2.p1.2.m2.1.1.cmml"><mn id="S4.I3.i2.p1.2.m2.1.1.2" xref="S4.I3.i2.p1.2.m2.1.1.2.cmml">512</mn><mo id="S4.I3.i2.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.I3.i2.p1.2.m2.1.1.1.cmml">×</mo><mn id="S4.I3.i2.p1.2.m2.1.1.3" xref="S4.I3.i2.p1.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I3.i2.p1.2.m2.1b"><apply id="S4.I3.i2.p1.2.m2.1.1.cmml" xref="S4.I3.i2.p1.2.m2.1.1"><times id="S4.I3.i2.p1.2.m2.1.1.1.cmml" xref="S4.I3.i2.p1.2.m2.1.1.1"></times><cn id="S4.I3.i2.p1.2.m2.1.1.2.cmml" type="integer" xref="S4.I3.i2.p1.2.m2.1.1.2">512</cn><cn id="S4.I3.i2.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.I3.i2.p1.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i2.p1.2.m2.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i2.p1.2.m2.1d">512 × 512</annotation></semantics></math> dimensions almost never misses a face, and completely blurs it. The last image is blatant, the <math alttext="192\times 192" class="ltx_Math" display="inline" id="S4.I3.i2.p1.3.m3.1"><semantics id="S4.I3.i2.p1.3.m3.1a"><mrow id="S4.I3.i2.p1.3.m3.1.1" xref="S4.I3.i2.p1.3.m3.1.1.cmml"><mn id="S4.I3.i2.p1.3.m3.1.1.2" xref="S4.I3.i2.p1.3.m3.1.1.2.cmml">192</mn><mo id="S4.I3.i2.p1.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.I3.i2.p1.3.m3.1.1.1.cmml">×</mo><mn id="S4.I3.i2.p1.3.m3.1.1.3" xref="S4.I3.i2.p1.3.m3.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I3.i2.p1.3.m3.1b"><apply id="S4.I3.i2.p1.3.m3.1.1.cmml" xref="S4.I3.i2.p1.3.m3.1.1"><times id="S4.I3.i2.p1.3.m3.1.1.1.cmml" xref="S4.I3.i2.p1.3.m3.1.1.1"></times><cn id="S4.I3.i2.p1.3.m3.1.1.2.cmml" type="integer" xref="S4.I3.i2.p1.3.m3.1.1.2">192</cn><cn id="S4.I3.i2.p1.3.m3.1.1.3.cmml" type="integer" xref="S4.I3.i2.p1.3.m3.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i2.p1.3.m3.1c">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i2.p1.3.m3.1d">192 × 192</annotation></semantics></math> option misses more than half of the faces, <math alttext="256\times 256" class="ltx_Math" display="inline" id="S4.I3.i2.p1.4.m4.1"><semantics id="S4.I3.i2.p1.4.m4.1a"><mrow id="S4.I3.i2.p1.4.m4.1.1" xref="S4.I3.i2.p1.4.m4.1.1.cmml"><mn id="S4.I3.i2.p1.4.m4.1.1.2" xref="S4.I3.i2.p1.4.m4.1.1.2.cmml">256</mn><mo id="S4.I3.i2.p1.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.I3.i2.p1.4.m4.1.1.1.cmml">×</mo><mn id="S4.I3.i2.p1.4.m4.1.1.3" xref="S4.I3.i2.p1.4.m4.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I3.i2.p1.4.m4.1b"><apply id="S4.I3.i2.p1.4.m4.1.1.cmml" xref="S4.I3.i2.p1.4.m4.1.1"><times id="S4.I3.i2.p1.4.m4.1.1.1.cmml" xref="S4.I3.i2.p1.4.m4.1.1.1"></times><cn id="S4.I3.i2.p1.4.m4.1.1.2.cmml" type="integer" xref="S4.I3.i2.p1.4.m4.1.1.2">256</cn><cn id="S4.I3.i2.p1.4.m4.1.1.3.cmml" type="integer" xref="S4.I3.i2.p1.4.m4.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i2.p1.4.m4.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i2.p1.4.m4.1d">256 × 256</annotation></semantics></math> misses a quarter of them, whereas <math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.I3.i2.p1.5.m5.1"><semantics id="S4.I3.i2.p1.5.m5.1a"><mrow id="S4.I3.i2.p1.5.m5.1.1" xref="S4.I3.i2.p1.5.m5.1.1.cmml"><mn id="S4.I3.i2.p1.5.m5.1.1.2" xref="S4.I3.i2.p1.5.m5.1.1.2.cmml">512</mn><mo id="S4.I3.i2.p1.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.I3.i2.p1.5.m5.1.1.1.cmml">×</mo><mn id="S4.I3.i2.p1.5.m5.1.1.3" xref="S4.I3.i2.p1.5.m5.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I3.i2.p1.5.m5.1b"><apply id="S4.I3.i2.p1.5.m5.1.1.cmml" xref="S4.I3.i2.p1.5.m5.1.1"><times id="S4.I3.i2.p1.5.m5.1.1.1.cmml" xref="S4.I3.i2.p1.5.m5.1.1.1"></times><cn id="S4.I3.i2.p1.5.m5.1.1.2.cmml" type="integer" xref="S4.I3.i2.p1.5.m5.1.1.2">512</cn><cn id="S4.I3.i2.p1.5.m5.1.1.3.cmml" type="integer" xref="S4.I3.i2.p1.5.m5.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.i2.p1.5.m5.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.I3.i2.p1.5.m5.1d">512 × 512</annotation></semantics></math> only misses the three smallest faces.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S4.SS2.SSS1.p3.2">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.T2" title="Table 2 ‣ 4.2.1 Influence of the downsampling dimension for inference ‣ 4.2 Experiments using DeOldify Unet ‣ 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">2</span></a> displays the count of correctly blurred faces in each of the images used in the tests.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.9" style="width:433.6pt;height:161.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(23.8pt,-8.9pt) scale(1.12328602623237,1.12328602623237) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.9.9">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.9.9.10.1">
<td class="ltx_td ltx_border_r" id="S4.T2.9.9.10.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.9.9.10.1.2" rowspan="2"><span class="ltx_text" id="S4.T2.9.9.10.1.2.1">Number of faces</span></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" colspan="3" id="S4.T2.9.9.10.1.3">Number of faces blurred</td>
<td class="ltx_td ltx_align_center" id="S4.T2.9.9.10.1.4" rowspan="2"><span class="ltx_text" id="S4.T2.9.9.10.1.4.1">Best visual result</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3.3">
<td class="ltx_td ltx_border_r" id="S4.T2.3.3.3.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.1.1"><math alttext="192\times 192" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.m1.1a"><mrow id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml"><mn id="S4.T2.1.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.1.m1.1.1.2.cmml">192</mn><mo id="S4.T2.1.1.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T2.1.1.1.1.m1.1.1.1.cmml">×</mo><mn id="S4.T2.1.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1"><times id="S4.T2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1.1"></times><cn id="S4.T2.1.1.1.1.m1.1.1.2.cmml" type="integer" xref="S4.T2.1.1.1.1.m1.1.1.2">192</cn><cn id="S4.T2.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.1.1.1.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.m1.1d">192 × 192</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.2.2.2.2"><math alttext="256\times 256" class="ltx_Math" display="inline" id="S4.T2.2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.2.m1.1a"><mrow id="S4.T2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.cmml"><mn id="S4.T2.2.2.2.2.m1.1.1.2" xref="S4.T2.2.2.2.2.m1.1.1.2.cmml">256</mn><mo id="S4.T2.2.2.2.2.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T2.2.2.2.2.m1.1.1.1.cmml">×</mo><mn id="S4.T2.2.2.2.2.m1.1.1.3" xref="S4.T2.2.2.2.2.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><apply id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1"><times id="S4.T2.2.2.2.2.m1.1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1.1"></times><cn id="S4.T2.2.2.2.2.m1.1.1.2.cmml" type="integer" xref="S4.T2.2.2.2.2.m1.1.1.2">256</cn><cn id="S4.T2.2.2.2.2.m1.1.1.3.cmml" type="integer" xref="S4.T2.2.2.2.2.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.2.m1.1d">256 × 256</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.3.3.3.3"><math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.T2.3.3.3.3.m1.1"><semantics id="S4.T2.3.3.3.3.m1.1a"><mrow id="S4.T2.3.3.3.3.m1.1.1" xref="S4.T2.3.3.3.3.m1.1.1.cmml"><mn id="S4.T2.3.3.3.3.m1.1.1.2" xref="S4.T2.3.3.3.3.m1.1.1.2.cmml">512</mn><mo id="S4.T2.3.3.3.3.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T2.3.3.3.3.m1.1.1.1.cmml">×</mo><mn id="S4.T2.3.3.3.3.m1.1.1.3" xref="S4.T2.3.3.3.3.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.m1.1b"><apply id="S4.T2.3.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1"><times id="S4.T2.3.3.3.3.m1.1.1.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1.1"></times><cn id="S4.T2.3.3.3.3.m1.1.1.2.cmml" type="integer" xref="S4.T2.3.3.3.3.m1.1.1.2">512</cn><cn id="S4.T2.3.3.3.3.m1.1.1.3.cmml" type="integer" xref="S4.T2.3.3.3.3.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.3.m1.1d">512 × 512</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.4.4.4.2">image 1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.4.4.4.3">1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.4.4.4.4"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.4.4.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.4.4.4.5"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.4.5.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.4.4.4.6"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.4.6.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.4.4.4.1"><math alttext="192\times 192" class="ltx_Math" display="inline" id="S4.T2.4.4.4.1.m1.1"><semantics id="S4.T2.4.4.4.1.m1.1a"><mrow id="S4.T2.4.4.4.1.m1.1.1" xref="S4.T2.4.4.4.1.m1.1.1.cmml"><mn id="S4.T2.4.4.4.1.m1.1.1.2" xref="S4.T2.4.4.4.1.m1.1.1.2.cmml">192</mn><mo id="S4.T2.4.4.4.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T2.4.4.4.1.m1.1.1.1.cmml">×</mo><mn id="S4.T2.4.4.4.1.m1.1.1.3" xref="S4.T2.4.4.4.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.1.m1.1b"><apply id="S4.T2.4.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.4.1.m1.1.1"><times id="S4.T2.4.4.4.1.m1.1.1.1.cmml" xref="S4.T2.4.4.4.1.m1.1.1.1"></times><cn id="S4.T2.4.4.4.1.m1.1.1.2.cmml" type="integer" xref="S4.T2.4.4.4.1.m1.1.1.2">192</cn><cn id="S4.T2.4.4.4.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.4.4.4.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.1.m1.1c">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.4.1.m1.1d">192 × 192</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.5.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.5.5.5.2">image 2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.5.5.5.3">1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.5.5.5.4"><span class="ltx_text ltx_font_bold" id="S4.T2.5.5.5.4.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.5.5.5.5"><span class="ltx_text ltx_font_bold" id="S4.T2.5.5.5.5.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.5.5.5.6"><span class="ltx_text ltx_font_bold" id="S4.T2.5.5.5.6.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.5.5.1"><math alttext="192\times 192" class="ltx_Math" display="inline" id="S4.T2.5.5.5.1.m1.1"><semantics id="S4.T2.5.5.5.1.m1.1a"><mrow id="S4.T2.5.5.5.1.m1.1.1" xref="S4.T2.5.5.5.1.m1.1.1.cmml"><mn id="S4.T2.5.5.5.1.m1.1.1.2" xref="S4.T2.5.5.5.1.m1.1.1.2.cmml">192</mn><mo id="S4.T2.5.5.5.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T2.5.5.5.1.m1.1.1.1.cmml">×</mo><mn id="S4.T2.5.5.5.1.m1.1.1.3" xref="S4.T2.5.5.5.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.1.m1.1b"><apply id="S4.T2.5.5.5.1.m1.1.1.cmml" xref="S4.T2.5.5.5.1.m1.1.1"><times id="S4.T2.5.5.5.1.m1.1.1.1.cmml" xref="S4.T2.5.5.5.1.m1.1.1.1"></times><cn id="S4.T2.5.5.5.1.m1.1.1.2.cmml" type="integer" xref="S4.T2.5.5.5.1.m1.1.1.2">192</cn><cn id="S4.T2.5.5.5.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.5.5.5.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.1.m1.1c">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.5.1.m1.1d">192 × 192</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.6.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.6.6.6.2">image 3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.6.6.6.3">5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.6.6.6.4">4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.6.6.6.5"><span class="ltx_text ltx_font_bold" id="S4.T2.6.6.6.5.1">5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.6.6.6.6"><span class="ltx_text ltx_font_bold" id="S4.T2.6.6.6.6.1">5</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.6.6.6.1"><math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.T2.6.6.6.1.m1.1"><semantics id="S4.T2.6.6.6.1.m1.1a"><mrow id="S4.T2.6.6.6.1.m1.1.1" xref="S4.T2.6.6.6.1.m1.1.1.cmml"><mn id="S4.T2.6.6.6.1.m1.1.1.2" xref="S4.T2.6.6.6.1.m1.1.1.2.cmml">512</mn><mo id="S4.T2.6.6.6.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T2.6.6.6.1.m1.1.1.1.cmml">×</mo><mn id="S4.T2.6.6.6.1.m1.1.1.3" xref="S4.T2.6.6.6.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.1.m1.1b"><apply id="S4.T2.6.6.6.1.m1.1.1.cmml" xref="S4.T2.6.6.6.1.m1.1.1"><times id="S4.T2.6.6.6.1.m1.1.1.1.cmml" xref="S4.T2.6.6.6.1.m1.1.1.1"></times><cn id="S4.T2.6.6.6.1.m1.1.1.2.cmml" type="integer" xref="S4.T2.6.6.6.1.m1.1.1.2">512</cn><cn id="S4.T2.6.6.6.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.6.6.6.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.1.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.6.6.1.m1.1d">512 × 512</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T2.7.7.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.7.7.7.2">image 4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.7.7.7.3">6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.7.7.7.4"><span class="ltx_text ltx_font_bold" id="S4.T2.7.7.7.4.1">6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.7.7.7.5"><span class="ltx_text ltx_font_bold" id="S4.T2.7.7.7.5.1">6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.7.7.7.6"><span class="ltx_text ltx_font_bold" id="S4.T2.7.7.7.6.1">6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.7.7.7.1"><math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.T2.7.7.7.1.m1.1"><semantics id="S4.T2.7.7.7.1.m1.1a"><mrow id="S4.T2.7.7.7.1.m1.1.1" xref="S4.T2.7.7.7.1.m1.1.1.cmml"><mn id="S4.T2.7.7.7.1.m1.1.1.2" xref="S4.T2.7.7.7.1.m1.1.1.2.cmml">512</mn><mo id="S4.T2.7.7.7.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T2.7.7.7.1.m1.1.1.1.cmml">×</mo><mn id="S4.T2.7.7.7.1.m1.1.1.3" xref="S4.T2.7.7.7.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.7.1.m1.1b"><apply id="S4.T2.7.7.7.1.m1.1.1.cmml" xref="S4.T2.7.7.7.1.m1.1.1"><times id="S4.T2.7.7.7.1.m1.1.1.1.cmml" xref="S4.T2.7.7.7.1.m1.1.1.1"></times><cn id="S4.T2.7.7.7.1.m1.1.1.2.cmml" type="integer" xref="S4.T2.7.7.7.1.m1.1.1.2">512</cn><cn id="S4.T2.7.7.7.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.7.7.7.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.7.1.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.7.7.1.m1.1d">512 × 512</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T2.8.8.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.8.8.8.2">image 5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.8.8.8.3">18</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.8.8.8.4">13</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.8.8.8.5"><span class="ltx_text ltx_font_bold" id="S4.T2.8.8.8.5.1">18</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.8.8.8.6"><span class="ltx_text ltx_font_bold" id="S4.T2.8.8.8.6.1">18</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.8.8.8.1"><math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.T2.8.8.8.1.m1.1"><semantics id="S4.T2.8.8.8.1.m1.1a"><mrow id="S4.T2.8.8.8.1.m1.1.1" xref="S4.T2.8.8.8.1.m1.1.1.cmml"><mn id="S4.T2.8.8.8.1.m1.1.1.2" xref="S4.T2.8.8.8.1.m1.1.1.2.cmml">512</mn><mo id="S4.T2.8.8.8.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T2.8.8.8.1.m1.1.1.1.cmml">×</mo><mn id="S4.T2.8.8.8.1.m1.1.1.3" xref="S4.T2.8.8.8.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.8.1.m1.1b"><apply id="S4.T2.8.8.8.1.m1.1.1.cmml" xref="S4.T2.8.8.8.1.m1.1.1"><times id="S4.T2.8.8.8.1.m1.1.1.1.cmml" xref="S4.T2.8.8.8.1.m1.1.1.1"></times><cn id="S4.T2.8.8.8.1.m1.1.1.2.cmml" type="integer" xref="S4.T2.8.8.8.1.m1.1.1.2">512</cn><cn id="S4.T2.8.8.8.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.8.8.8.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.8.1.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.8.8.1.m1.1d">512 × 512</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.9.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.9.9.2">image 6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.9.9.3">51</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.9.9.4">20</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.9.9.5">38</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.9.9.6"><span class="ltx_text ltx_font_bold" id="S4.T2.9.9.9.6.1">48</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.9.9.1"><math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.T2.9.9.9.1.m1.1"><semantics id="S4.T2.9.9.9.1.m1.1a"><mrow id="S4.T2.9.9.9.1.m1.1.1" xref="S4.T2.9.9.9.1.m1.1.1.cmml"><mn id="S4.T2.9.9.9.1.m1.1.1.2" xref="S4.T2.9.9.9.1.m1.1.1.2.cmml">512</mn><mo id="S4.T2.9.9.9.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T2.9.9.9.1.m1.1.1.1.cmml">×</mo><mn id="S4.T2.9.9.9.1.m1.1.1.3" xref="S4.T2.9.9.9.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.9.1.m1.1b"><apply id="S4.T2.9.9.9.1.m1.1.1.cmml" xref="S4.T2.9.9.9.1.m1.1.1"><times id="S4.T2.9.9.9.1.m1.1.1.1.cmml" xref="S4.T2.9.9.9.1.m1.1.1.1"></times><cn id="S4.T2.9.9.9.1.m1.1.1.2.cmml" type="integer" xref="S4.T2.9.9.9.1.m1.1.1.2">512</cn><cn id="S4.T2.9.9.9.1.m1.1.1.3.cmml" type="integer" xref="S4.T2.9.9.9.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.9.1.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.T2.9.9.9.1.m1.1d">512 × 512</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.11.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.12.2" style="font-size:90%;">Face counting results (the best result for each image is in bold).</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS1.p4">
<p class="ltx_p" id="S4.SS2.SSS1.p4.1">We see that, depending on the input image, the optimal downsampling size can be different. In the code and online demo associated with this publication (see below), one can vary the input image size.
<br class="ltx_break"/>Still, overall the <math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p4.1.m1.1"><semantics id="S4.SS2.SSS1.p4.1.m1.1a"><mrow id="S4.SS2.SSS1.p4.1.m1.1.1" xref="S4.SS2.SSS1.p4.1.m1.1.1.cmml"><mn id="S4.SS2.SSS1.p4.1.m1.1.1.2" xref="S4.SS2.SSS1.p4.1.m1.1.1.2.cmml">512</mn><mo id="S4.SS2.SSS1.p4.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS1.p4.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.SSS1.p4.1.m1.1.1.3" xref="S4.SS2.SSS1.p4.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p4.1.m1.1b"><apply id="S4.SS2.SSS1.p4.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p4.1.m1.1.1"><times id="S4.SS2.SSS1.p4.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.p4.1.m1.1.1.1"></times><cn id="S4.SS2.SSS1.p4.1.m1.1.1.2.cmml" type="integer" xref="S4.SS2.SSS1.p4.1.m1.1.1.2">512</cn><cn id="S4.SS2.SSS1.p4.1.m1.1.1.3.cmml" type="integer" xref="S4.SS2.SSS1.p4.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p4.1.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p4.1.m1.1d">512 × 512</annotation></semantics></math> downsampling size seem better in most cases, surpassing all other dimensions when it comes to blur small faces and still performs face blurring of big faces, even if it is visually a little worse than with smaller dimensions. However, a higher downsampling size comes also with a higher computation time. That is what we are going to briefly discuss in the next section.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p5">
<p class="ltx_p" id="S4.SS2.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p5.1.1">Computation Time</span>.
<br class="ltx_break"/>We perform inference using a GPU (NVIDIA GeForce RTX 2080 Ti with 11GB VRAM) in all our tests.
We conduct three experiments:</p>
<ul class="ltx_itemize" id="S4.I4">
<li class="ltx_item" id="S4.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I4.i1.p1">
<p class="ltx_p" id="S4.I4.i1.p1.1">One experiment assuming that input images have already been downsampled to the desired input size of the network (either 192 x 192, 256 x 256 or 512 x 512). In this case the downsampling, binary mask extraction and upsampling steps described in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S3.F6" title="Figure 6 ‣ 3.4 Inference methodology ‣ 3 Face Blurring using DeOldify ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">6</span></a> do not need to be performed.</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I4.i2.p1">
<p class="ltx_p" id="S4.I4.i2.p1.1">One experiment assuming that input images are of size <math alttext="1024\times 1024" class="ltx_Math" display="inline" id="S4.I4.i2.p1.1.m1.1"><semantics id="S4.I4.i2.p1.1.m1.1a"><mrow id="S4.I4.i2.p1.1.m1.1.1" xref="S4.I4.i2.p1.1.m1.1.1.cmml"><mn id="S4.I4.i2.p1.1.m1.1.1.2" xref="S4.I4.i2.p1.1.m1.1.1.2.cmml">1024</mn><mo id="S4.I4.i2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.I4.i2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.I4.i2.p1.1.m1.1.1.3" xref="S4.I4.i2.p1.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I4.i2.p1.1.m1.1b"><apply id="S4.I4.i2.p1.1.m1.1.1.cmml" xref="S4.I4.i2.p1.1.m1.1.1"><times id="S4.I4.i2.p1.1.m1.1.1.1.cmml" xref="S4.I4.i2.p1.1.m1.1.1.1"></times><cn id="S4.I4.i2.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.I4.i2.p1.1.m1.1.1.2">1024</cn><cn id="S4.I4.i2.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.I4.i2.p1.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I4.i2.p1.1.m1.1c">1024\times 1024</annotation><annotation encoding="application/x-llamapun" id="S4.I4.i2.p1.1.m1.1d">1024 × 1024</annotation></semantics></math> (we follow the algorithm described in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S3.F6" title="Figure 6 ‣ 3.4 Inference methodology ‣ 3 Face Blurring using DeOldify ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">6</span></a>)</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I4.i3.p1">
<p class="ltx_p" id="S4.I4.i3.p1.1">One experiment assuming that input images are of size <math alttext="2048\times 2048" class="ltx_Math" display="inline" id="S4.I4.i3.p1.1.m1.1"><semantics id="S4.I4.i3.p1.1.m1.1a"><mrow id="S4.I4.i3.p1.1.m1.1.1" xref="S4.I4.i3.p1.1.m1.1.1.cmml"><mn id="S4.I4.i3.p1.1.m1.1.1.2" xref="S4.I4.i3.p1.1.m1.1.1.2.cmml">2048</mn><mo id="S4.I4.i3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.I4.i3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.I4.i3.p1.1.m1.1.1.3" xref="S4.I4.i3.p1.1.m1.1.1.3.cmml">2048</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I4.i3.p1.1.m1.1b"><apply id="S4.I4.i3.p1.1.m1.1.1.cmml" xref="S4.I4.i3.p1.1.m1.1.1"><times id="S4.I4.i3.p1.1.m1.1.1.1.cmml" xref="S4.I4.i3.p1.1.m1.1.1.1"></times><cn id="S4.I4.i3.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.I4.i3.p1.1.m1.1.1.2">2048</cn><cn id="S4.I4.i3.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.I4.i3.p1.1.m1.1.1.3">2048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I4.i3.p1.1.m1.1c">2048\times 2048</annotation><annotation encoding="application/x-llamapun" id="S4.I4.i3.p1.1.m1.1d">2048 × 2048</annotation></semantics></math> (we follow the algorithm described in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S3.F6" title="Figure 6 ‣ 3.4 Inference methodology ‣ 3 Face Blurring using DeOldify ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S4.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="89" id="S4.F7.g1" src="extracted/5872765/figures/test_set/original_images/image_0.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="89" id="S4.F7.g2" src="extracted/5872765/figures/test_set/l1_192/image_0.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="89" id="S4.F7.g3" src="extracted/5872765/figures/test_set/l1_256/image_0.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="89" id="S4.F7.g4" src="extracted/5872765/figures/test_set/l1_512/image_0.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="145" id="S4.F7.g5" src="extracted/5872765/figures/test_set/original_images/image_25.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="145" id="S4.F7.g6" src="extracted/5872765/figures/test_set/l1_192/image_25.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="145" id="S4.F7.g7" src="extracted/5872765/figures/test_set/l1_256/image_25.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="145" id="S4.F7.g8" src="extracted/5872765/figures/test_set/l1_512/image_25.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="81" id="S4.F7.g9" src="extracted/5872765/figures/test_set/original_images/image_1.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="81" id="S4.F7.g10" src="extracted/5872765/figures/test_set/l1_192/image_1.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="81" id="S4.F7.g11" src="extracted/5872765/figures/test_set/l1_256/image_1.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="81" id="S4.F7.g12" src="extracted/5872765/figures/test_set/l1_512/image_1.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="108" id="S4.F7.g13" src="extracted/5872765/figures/test_set/original_images/image_16.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="108" id="S4.F7.g14" src="extracted/5872765/figures/test_set/l1_192/image_16.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="108" id="S4.F7.g15" src="extracted/5872765/figures/test_set/l1_256/image_16.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="108" id="S4.F7.g16" src="extracted/5872765/figures/test_set/l1_512/image_16.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F7.g17" src="extracted/5872765/figures/test_set/original_images/image_57.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F7.g18" src="extracted/5872765/figures/test_set/l1_192/image_57.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F7.g19" src="extracted/5872765/figures/test_set/l1_256/image_57.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F7.g20" src="extracted/5872765/figures/test_set/l1_512/image_57.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F7.g21" src="extracted/5872765/figures/test_set/original_images/image_24.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F7.g22" src="extracted/5872765/figures/test_set/l1_192/image_24.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F7.g23" src="extracted/5872765/figures/test_set/l1_256/image_24.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F7.g24" src="extracted/5872765/figures/test_set/l1_512/image_24.jpg" width="144"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.8.4.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S4.F7.6.3" style="font-size:90%;">Test set results. Original image (left), result for <math alttext="192\times 192" class="ltx_Math" display="inline" id="S4.F7.4.1.m1.1"><semantics id="S4.F7.4.1.m1.1b"><mrow id="S4.F7.4.1.m1.1.1" xref="S4.F7.4.1.m1.1.1.cmml"><mn id="S4.F7.4.1.m1.1.1.2" xref="S4.F7.4.1.m1.1.1.2.cmml">192</mn><mo id="S4.F7.4.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.F7.4.1.m1.1.1.1.cmml">×</mo><mn id="S4.F7.4.1.m1.1.1.3" xref="S4.F7.4.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F7.4.1.m1.1c"><apply id="S4.F7.4.1.m1.1.1.cmml" xref="S4.F7.4.1.m1.1.1"><times id="S4.F7.4.1.m1.1.1.1.cmml" xref="S4.F7.4.1.m1.1.1.1"></times><cn id="S4.F7.4.1.m1.1.1.2.cmml" type="integer" xref="S4.F7.4.1.m1.1.1.2">192</cn><cn id="S4.F7.4.1.m1.1.1.3.cmml" type="integer" xref="S4.F7.4.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.4.1.m1.1d">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.F7.4.1.m1.1e">192 × 192</annotation></semantics></math> inference dimension (center left), result for <math alttext="256\times 256" class="ltx_Math" display="inline" id="S4.F7.5.2.m2.1"><semantics id="S4.F7.5.2.m2.1b"><mrow id="S4.F7.5.2.m2.1.1" xref="S4.F7.5.2.m2.1.1.cmml"><mn id="S4.F7.5.2.m2.1.1.2" xref="S4.F7.5.2.m2.1.1.2.cmml">256</mn><mo id="S4.F7.5.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.F7.5.2.m2.1.1.1.cmml">×</mo><mn id="S4.F7.5.2.m2.1.1.3" xref="S4.F7.5.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F7.5.2.m2.1c"><apply id="S4.F7.5.2.m2.1.1.cmml" xref="S4.F7.5.2.m2.1.1"><times id="S4.F7.5.2.m2.1.1.1.cmml" xref="S4.F7.5.2.m2.1.1.1"></times><cn id="S4.F7.5.2.m2.1.1.2.cmml" type="integer" xref="S4.F7.5.2.m2.1.1.2">256</cn><cn id="S4.F7.5.2.m2.1.1.3.cmml" type="integer" xref="S4.F7.5.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.5.2.m2.1d">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S4.F7.5.2.m2.1e">256 × 256</annotation></semantics></math> inference dimension (center right), result for <math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.F7.6.3.m3.1"><semantics id="S4.F7.6.3.m3.1b"><mrow id="S4.F7.6.3.m3.1.1" xref="S4.F7.6.3.m3.1.1.cmml"><mn id="S4.F7.6.3.m3.1.1.2" xref="S4.F7.6.3.m3.1.1.2.cmml">512</mn><mo id="S4.F7.6.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.F7.6.3.m3.1.1.1.cmml">×</mo><mn id="S4.F7.6.3.m3.1.1.3" xref="S4.F7.6.3.m3.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F7.6.3.m3.1c"><apply id="S4.F7.6.3.m3.1.1.cmml" xref="S4.F7.6.3.m3.1.1"><times id="S4.F7.6.3.m3.1.1.1.cmml" xref="S4.F7.6.3.m3.1.1.1"></times><cn id="S4.F7.6.3.m3.1.1.2.cmml" type="integer" xref="S4.F7.6.3.m3.1.1.2">512</cn><cn id="S4.F7.6.3.m3.1.1.3.cmml" type="integer" xref="S4.F7.6.3.m3.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F7.6.3.m3.1d">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.F7.6.3.m3.1e">512 × 512</annotation></semantics></math> inference dimension (right)</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS1.p6">
<p class="ltx_p" id="S4.SS2.SSS1.p6.1">We calculate the computation times for <math alttext="100" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p6.1.m1.1"><semantics id="S4.SS2.SSS1.p6.1.m1.1a"><mn id="S4.SS2.SSS1.p6.1.m1.1.1" xref="S4.SS2.SSS1.p6.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p6.1.m1.1b"><cn id="S4.SS2.SSS1.p6.1.m1.1.1.cmml" type="integer" xref="S4.SS2.SSS1.p6.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p6.1.m1.1c">100</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p6.1.m1.1d">100</annotation></semantics></math> images and average them for each experiment. Results are displayed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.T3" title="Table 3 ‣ 4.2.1 Influence of the downsampling dimension for inference ‣ 4.2 Experiments using DeOldify Unet ‣ 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">3</span></a> and expressed in frames per second.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.14" style="width:433.6pt;height:107.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(71.1pt,-17.6pt) scale(1.48811369388616,1.48811369388616) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.14.14">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.3.3.3">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r" id="S4.T3.3.3.3.4"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T3.1.1.1.1"><math alttext="192\times 192" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.m1.1a"><mrow id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml"><mn id="S4.T3.1.1.1.1.m1.1.1.2" xref="S4.T3.1.1.1.1.m1.1.1.2.cmml">192</mn><mo id="S4.T3.1.1.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T3.1.1.1.1.m1.1.1.1.cmml">×</mo><mn id="S4.T3.1.1.1.1.m1.1.1.3" xref="S4.T3.1.1.1.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1"><times id="S4.T3.1.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1.1"></times><cn id="S4.T3.1.1.1.1.m1.1.1.2.cmml" type="integer" xref="S4.T3.1.1.1.1.m1.1.1.2">192</cn><cn id="S4.T3.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S4.T3.1.1.1.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.m1.1d">192 × 192</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T3.2.2.2.2"><math alttext="256\times 256" class="ltx_Math" display="inline" id="S4.T3.2.2.2.2.m1.1"><semantics id="S4.T3.2.2.2.2.m1.1a"><mrow id="S4.T3.2.2.2.2.m1.1.1" xref="S4.T3.2.2.2.2.m1.1.1.cmml"><mn id="S4.T3.2.2.2.2.m1.1.1.2" xref="S4.T3.2.2.2.2.m1.1.1.2.cmml">256</mn><mo id="S4.T3.2.2.2.2.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T3.2.2.2.2.m1.1.1.1.cmml">×</mo><mn id="S4.T3.2.2.2.2.m1.1.1.3" xref="S4.T3.2.2.2.2.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.2.m1.1b"><apply id="S4.T3.2.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.2.m1.1.1"><times id="S4.T3.2.2.2.2.m1.1.1.1.cmml" xref="S4.T3.2.2.2.2.m1.1.1.1"></times><cn id="S4.T3.2.2.2.2.m1.1.1.2.cmml" type="integer" xref="S4.T3.2.2.2.2.m1.1.1.2">256</cn><cn id="S4.T3.2.2.2.2.m1.1.1.3.cmml" type="integer" xref="S4.T3.2.2.2.2.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.2.m1.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.2.m1.1d">256 × 256</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.3.3.3.3"><math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.T3.3.3.3.3.m1.1"><semantics id="S4.T3.3.3.3.3.m1.1a"><mrow id="S4.T3.3.3.3.3.m1.1.1" xref="S4.T3.3.3.3.3.m1.1.1.cmml"><mn id="S4.T3.3.3.3.3.m1.1.1.2" xref="S4.T3.3.3.3.3.m1.1.1.2.cmml">512</mn><mo id="S4.T3.3.3.3.3.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T3.3.3.3.3.m1.1.1.1.cmml">×</mo><mn id="S4.T3.3.3.3.3.m1.1.1.3" xref="S4.T3.3.3.3.3.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.3.m1.1b"><apply id="S4.T3.3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.3.m1.1.1"><times id="S4.T3.3.3.3.3.m1.1.1.1.cmml" xref="S4.T3.3.3.3.3.m1.1.1.1"></times><cn id="S4.T3.3.3.3.3.m1.1.1.2.cmml" type="integer" xref="S4.T3.3.3.3.3.m1.1.1.2">512</cn><cn id="S4.T3.3.3.3.3.m1.1.1.3.cmml" type="integer" xref="S4.T3.3.3.3.3.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.3.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.3.m1.1d">512 × 512</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.6.6.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.6.6.6.4">No resizing needed</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.4.4.4.1"><math alttext="28.4" class="ltx_Math" display="inline" id="S4.T3.4.4.4.1.m1.1"><semantics id="S4.T3.4.4.4.1.m1.1a"><mn id="S4.T3.4.4.4.1.m1.1.1" xref="S4.T3.4.4.4.1.m1.1.1.cmml">28.4</mn><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.1.m1.1b"><cn id="S4.T3.4.4.4.1.m1.1.1.cmml" type="float" xref="S4.T3.4.4.4.1.m1.1.1">28.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.1.m1.1c">28.4</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.4.1.m1.1d">28.4</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.5.5.5.2"><math alttext="22.0" class="ltx_Math" display="inline" id="S4.T3.5.5.5.2.m1.1"><semantics id="S4.T3.5.5.5.2.m1.1a"><mn id="S4.T3.5.5.5.2.m1.1.1" xref="S4.T3.5.5.5.2.m1.1.1.cmml">22.0</mn><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.2.m1.1b"><cn id="S4.T3.5.5.5.2.m1.1.1.cmml" type="float" xref="S4.T3.5.5.5.2.m1.1.1">22.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.2.m1.1c">22.0</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.5.5.2.m1.1d">22.0</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.6.6.6.3"><math alttext="9.6" class="ltx_Math" display="inline" id="S4.T3.6.6.6.3.m1.1"><semantics id="S4.T3.6.6.6.3.m1.1a"><mn id="S4.T3.6.6.6.3.m1.1.1" xref="S4.T3.6.6.6.3.m1.1.1.cmml">9.6</mn><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.3.m1.1b"><cn id="S4.T3.6.6.6.3.m1.1.1.cmml" type="float" xref="S4.T3.6.6.6.3.m1.1.1">9.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.3.m1.1c">9.6</annotation><annotation encoding="application/x-llamapun" id="S4.T3.6.6.6.3.m1.1d">9.6</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T3.10.10.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.7.7.1">Input size of <math alttext="1024\times 1024" class="ltx_Math" display="inline" id="S4.T3.7.7.7.1.m1.1"><semantics id="S4.T3.7.7.7.1.m1.1a"><mrow id="S4.T3.7.7.7.1.m1.1.1" xref="S4.T3.7.7.7.1.m1.1.1.cmml"><mn id="S4.T3.7.7.7.1.m1.1.1.2" xref="S4.T3.7.7.7.1.m1.1.1.2.cmml">1024</mn><mo id="S4.T3.7.7.7.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T3.7.7.7.1.m1.1.1.1.cmml">×</mo><mn id="S4.T3.7.7.7.1.m1.1.1.3" xref="S4.T3.7.7.7.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.7.7.7.1.m1.1b"><apply id="S4.T3.7.7.7.1.m1.1.1.cmml" xref="S4.T3.7.7.7.1.m1.1.1"><times id="S4.T3.7.7.7.1.m1.1.1.1.cmml" xref="S4.T3.7.7.7.1.m1.1.1.1"></times><cn id="S4.T3.7.7.7.1.m1.1.1.2.cmml" type="integer" xref="S4.T3.7.7.7.1.m1.1.1.2">1024</cn><cn id="S4.T3.7.7.7.1.m1.1.1.3.cmml" type="integer" xref="S4.T3.7.7.7.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.7.7.1.m1.1c">1024\times 1024</annotation><annotation encoding="application/x-llamapun" id="S4.T3.7.7.7.1.m1.1d">1024 × 1024</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.8.8.8.2"><math alttext="9.2" class="ltx_Math" display="inline" id="S4.T3.8.8.8.2.m1.1"><semantics id="S4.T3.8.8.8.2.m1.1a"><mn id="S4.T3.8.8.8.2.m1.1.1" xref="S4.T3.8.8.8.2.m1.1.1.cmml">9.2</mn><annotation-xml encoding="MathML-Content" id="S4.T3.8.8.8.2.m1.1b"><cn id="S4.T3.8.8.8.2.m1.1.1.cmml" type="float" xref="S4.T3.8.8.8.2.m1.1.1">9.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.8.8.2.m1.1c">9.2</annotation><annotation encoding="application/x-llamapun" id="S4.T3.8.8.8.2.m1.1d">9.2</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.9.9.9.3"><math alttext="8.3" class="ltx_Math" display="inline" id="S4.T3.9.9.9.3.m1.1"><semantics id="S4.T3.9.9.9.3.m1.1a"><mn id="S4.T3.9.9.9.3.m1.1.1" xref="S4.T3.9.9.9.3.m1.1.1.cmml">8.3</mn><annotation-xml encoding="MathML-Content" id="S4.T3.9.9.9.3.m1.1b"><cn id="S4.T3.9.9.9.3.m1.1.1.cmml" type="float" xref="S4.T3.9.9.9.3.m1.1.1">8.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.9.9.3.m1.1c">8.3</annotation><annotation encoding="application/x-llamapun" id="S4.T3.9.9.9.3.m1.1d">8.3</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.10.10.10.4"><math alttext="5.4" class="ltx_Math" display="inline" id="S4.T3.10.10.10.4.m1.1"><semantics id="S4.T3.10.10.10.4.m1.1a"><mn id="S4.T3.10.10.10.4.m1.1.1" xref="S4.T3.10.10.10.4.m1.1.1.cmml">5.4</mn><annotation-xml encoding="MathML-Content" id="S4.T3.10.10.10.4.m1.1b"><cn id="S4.T3.10.10.10.4.m1.1.1.cmml" type="float" xref="S4.T3.10.10.10.4.m1.1.1">5.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.10.10.10.4.m1.1c">5.4</annotation><annotation encoding="application/x-llamapun" id="S4.T3.10.10.10.4.m1.1d">5.4</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T3.14.14.14">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.11.11.11.1">Input size of <math alttext="2048\times 2048" class="ltx_Math" display="inline" id="S4.T3.11.11.11.1.m1.1"><semantics id="S4.T3.11.11.11.1.m1.1a"><mrow id="S4.T3.11.11.11.1.m1.1.1" xref="S4.T3.11.11.11.1.m1.1.1.cmml"><mn id="S4.T3.11.11.11.1.m1.1.1.2" xref="S4.T3.11.11.11.1.m1.1.1.2.cmml">2048</mn><mo id="S4.T3.11.11.11.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T3.11.11.11.1.m1.1.1.1.cmml">×</mo><mn id="S4.T3.11.11.11.1.m1.1.1.3" xref="S4.T3.11.11.11.1.m1.1.1.3.cmml">2048</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.11.11.11.1.m1.1b"><apply id="S4.T3.11.11.11.1.m1.1.1.cmml" xref="S4.T3.11.11.11.1.m1.1.1"><times id="S4.T3.11.11.11.1.m1.1.1.1.cmml" xref="S4.T3.11.11.11.1.m1.1.1.1"></times><cn id="S4.T3.11.11.11.1.m1.1.1.2.cmml" type="integer" xref="S4.T3.11.11.11.1.m1.1.1.2">2048</cn><cn id="S4.T3.11.11.11.1.m1.1.1.3.cmml" type="integer" xref="S4.T3.11.11.11.1.m1.1.1.3">2048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.11.11.11.1.m1.1c">2048\times 2048</annotation><annotation encoding="application/x-llamapun" id="S4.T3.11.11.11.1.m1.1d">2048 × 2048</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.12.12.12.2"><math alttext="3.0" class="ltx_Math" display="inline" id="S4.T3.12.12.12.2.m1.1"><semantics id="S4.T3.12.12.12.2.m1.1a"><mn id="S4.T3.12.12.12.2.m1.1.1" xref="S4.T3.12.12.12.2.m1.1.1.cmml">3.0</mn><annotation-xml encoding="MathML-Content" id="S4.T3.12.12.12.2.m1.1b"><cn id="S4.T3.12.12.12.2.m1.1.1.cmml" type="float" xref="S4.T3.12.12.12.2.m1.1.1">3.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.12.12.12.2.m1.1c">3.0</annotation><annotation encoding="application/x-llamapun" id="S4.T3.12.12.12.2.m1.1d">3.0</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.13.13.13.3"><math alttext="2.8" class="ltx_Math" display="inline" id="S4.T3.13.13.13.3.m1.1"><semantics id="S4.T3.13.13.13.3.m1.1a"><mn id="S4.T3.13.13.13.3.m1.1.1" xref="S4.T3.13.13.13.3.m1.1.1.cmml">2.8</mn><annotation-xml encoding="MathML-Content" id="S4.T3.13.13.13.3.m1.1b"><cn id="S4.T3.13.13.13.3.m1.1.1.cmml" type="float" xref="S4.T3.13.13.13.3.m1.1.1">2.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.13.13.13.3.m1.1c">2.8</annotation><annotation encoding="application/x-llamapun" id="S4.T3.13.13.13.3.m1.1d">2.8</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.14.14.14.4"><math alttext="2.4" class="ltx_Math" display="inline" id="S4.T3.14.14.14.4.m1.1"><semantics id="S4.T3.14.14.14.4.m1.1a"><mn id="S4.T3.14.14.14.4.m1.1.1" xref="S4.T3.14.14.14.4.m1.1.1.cmml">2.4</mn><annotation-xml encoding="MathML-Content" id="S4.T3.14.14.14.4.m1.1b"><cn id="S4.T3.14.14.14.4.m1.1.1.cmml" type="float" xref="S4.T3.14.14.14.4.m1.1.1">2.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.14.14.14.4.m1.1c">2.4</annotation><annotation encoding="application/x-llamapun" id="S4.T3.14.14.14.4.m1.1d">2.4</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.16.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S4.T3.17.2" style="font-size:90%;">Number of frames per second processed by the algorithm in function of the downsampling size.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS1.p7">
<p class="ltx_p" id="S4.SS2.SSS1.p7.1">We observe that all the resizing steps (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S3.F6" title="Figure 6 ‣ 3.4 Inference methodology ‣ 3 Face Blurring using DeOldify ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">6</span></a>) greatly deteriorate the algorithm performance. To avoid this phenomenon, we could take advantage of multiprocessing using several cores. We did not implement this idea, but as images are processed independently, we think it could greatly improve performances. We also observed that the bigger the input image is, the more similar are all computation times.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Influence of the loss function</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.2">As in the previous section, we use a visual evaluation of the results to assess the influence of the loss function. We compare two models, one which was trained using <math alttext="MSE" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.1.m1.1"><semantics id="S4.SS2.SSS2.p1.1.m1.1a"><mrow id="S4.SS2.SSS2.p1.1.m1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS2.p1.1.m1.1.1.2" xref="S4.SS2.SSS2.p1.1.m1.1.1.2.cmml">M</mi><mo id="S4.SS2.SSS2.p1.1.m1.1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS2.p1.1.m1.1.1.3" xref="S4.SS2.SSS2.p1.1.m1.1.1.3.cmml">S</mi><mo id="S4.SS2.SSS2.p1.1.m1.1.1.1a" xref="S4.SS2.SSS2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS2.p1.1.m1.1.1.4" xref="S4.SS2.SSS2.p1.1.m1.1.1.4.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.1.m1.1b"><apply id="S4.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1"><times id="S4.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.1"></times><ci id="S4.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.2">𝑀</ci><ci id="S4.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.3">𝑆</ci><ci id="S4.SS2.SSS2.p1.1.m1.1.1.4.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.4">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.1.m1.1c">MSE</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.1.m1.1d">italic_M italic_S italic_E</annotation></semantics></math> loss and another using L1 loss. For both models, we use <math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p1.2.m2.1"><semantics id="S4.SS2.SSS2.p1.2.m2.1a"><mrow id="S4.SS2.SSS2.p1.2.m2.1.1" xref="S4.SS2.SSS2.p1.2.m2.1.1.cmml"><mn id="S4.SS2.SSS2.p1.2.m2.1.1.2" xref="S4.SS2.SSS2.p1.2.m2.1.1.2.cmml">512</mn><mo id="S4.SS2.SSS2.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS2.p1.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS2.SSS2.p1.2.m2.1.1.3" xref="S4.SS2.SSS2.p1.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.2.m2.1b"><apply id="S4.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1"><times id="S4.SS2.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1.1"></times><cn id="S4.SS2.SSS2.p1.2.m2.1.1.2.cmml" type="integer" xref="S4.SS2.SSS2.p1.2.m2.1.1.2">512</cn><cn id="S4.SS2.SSS2.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.SS2.SSS2.p1.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.2.m2.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p1.2.m2.1d">512 × 512</annotation></semantics></math> as inference downsampling size.</p>
</div>
<figure class="ltx_figure" id="S4.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="89" id="S4.F8.g1" src="extracted/5872765/figures/test_set/original_images/image_0.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="89" id="S4.F8.g2" src="extracted/5872765/figures/test_set/mse_512/image_0.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="89" id="S4.F8.g3" src="extracted/5872765/figures/test_set/l1_512/image_0.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="145" id="S4.F8.g4" src="extracted/5872765/figures/test_set/original_images/image_25.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="145" id="S4.F8.g5" src="extracted/5872765/figures/test_set/mse_512/image_25.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="145" id="S4.F8.g6" src="extracted/5872765/figures/test_set/l1_512/image_25.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="81" id="S4.F8.g7" src="extracted/5872765/figures/test_set/original_images/image_1.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="81" id="S4.F8.g8" src="extracted/5872765/figures/test_set/mse_512/image_1.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="81" id="S4.F8.g9" src="extracted/5872765/figures/test_set/l1_512/image_1.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="108" id="S4.F8.g10" src="extracted/5872765/figures/test_set/original_images/image_16.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="108" id="S4.F8.g11" src="extracted/5872765/figures/test_set/mse_512/image_16.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="108" id="S4.F8.g12" src="extracted/5872765/figures/test_set/l1_512/image_16.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F8.g13" src="extracted/5872765/figures/test_set/original_images/image_57.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F8.g14" src="extracted/5872765/figures/test_set/mse_512/image_57.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F8.g15" src="extracted/5872765/figures/test_set/l1_512/image_57.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F8.g16" src="extracted/5872765/figures/test_set/original_images/image_24.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F8.g17" src="extracted/5872765/figures/test_set/mse_512/image_24.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F8.g18" src="extracted/5872765/figures/test_set/l1_512/image_24.jpg" width="144"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.6.3.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S4.F8.4.2" style="font-size:90%;">Test set results. Original image (left), result for <math alttext="MSE" class="ltx_Math" display="inline" id="S4.F8.3.1.m1.1"><semantics id="S4.F8.3.1.m1.1b"><mrow id="S4.F8.3.1.m1.1.1" xref="S4.F8.3.1.m1.1.1.cmml"><mi id="S4.F8.3.1.m1.1.1.2" xref="S4.F8.3.1.m1.1.1.2.cmml">M</mi><mo id="S4.F8.3.1.m1.1.1.1" xref="S4.F8.3.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.F8.3.1.m1.1.1.3" xref="S4.F8.3.1.m1.1.1.3.cmml">S</mi><mo id="S4.F8.3.1.m1.1.1.1b" xref="S4.F8.3.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.F8.3.1.m1.1.1.4" xref="S4.F8.3.1.m1.1.1.4.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F8.3.1.m1.1c"><apply id="S4.F8.3.1.m1.1.1.cmml" xref="S4.F8.3.1.m1.1.1"><times id="S4.F8.3.1.m1.1.1.1.cmml" xref="S4.F8.3.1.m1.1.1.1"></times><ci id="S4.F8.3.1.m1.1.1.2.cmml" xref="S4.F8.3.1.m1.1.1.2">𝑀</ci><ci id="S4.F8.3.1.m1.1.1.3.cmml" xref="S4.F8.3.1.m1.1.1.3">𝑆</ci><ci id="S4.F8.3.1.m1.1.1.4.cmml" xref="S4.F8.3.1.m1.1.1.4">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F8.3.1.m1.1d">MSE</annotation><annotation encoding="application/x-llamapun" id="S4.F8.3.1.m1.1e">italic_M italic_S italic_E</annotation></semantics></math> loss (center), result for L1 loss (right). Downsampling size <math alttext="=~{}512\times 512" class="ltx_Math" display="inline" id="S4.F8.4.2.m2.1"><semantics id="S4.F8.4.2.m2.1b"><mrow id="S4.F8.4.2.m2.1.1" xref="S4.F8.4.2.m2.1.1.cmml"><mi id="S4.F8.4.2.m2.1.1.2" xref="S4.F8.4.2.m2.1.1.2.cmml"></mi><mo id="S4.F8.4.2.m2.1.1.1" rspace="0.578em" xref="S4.F8.4.2.m2.1.1.1.cmml">=</mo><mrow id="S4.F8.4.2.m2.1.1.3" xref="S4.F8.4.2.m2.1.1.3.cmml"><mn id="S4.F8.4.2.m2.1.1.3.2" xref="S4.F8.4.2.m2.1.1.3.2.cmml">512</mn><mo id="S4.F8.4.2.m2.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S4.F8.4.2.m2.1.1.3.1.cmml">×</mo><mn id="S4.F8.4.2.m2.1.1.3.3" xref="S4.F8.4.2.m2.1.1.3.3.cmml">512</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.F8.4.2.m2.1c"><apply id="S4.F8.4.2.m2.1.1.cmml" xref="S4.F8.4.2.m2.1.1"><eq id="S4.F8.4.2.m2.1.1.1.cmml" xref="S4.F8.4.2.m2.1.1.1"></eq><csymbol cd="latexml" id="S4.F8.4.2.m2.1.1.2.cmml" xref="S4.F8.4.2.m2.1.1.2">absent</csymbol><apply id="S4.F8.4.2.m2.1.1.3.cmml" xref="S4.F8.4.2.m2.1.1.3"><times id="S4.F8.4.2.m2.1.1.3.1.cmml" xref="S4.F8.4.2.m2.1.1.3.1"></times><cn id="S4.F8.4.2.m2.1.1.3.2.cmml" type="integer" xref="S4.F8.4.2.m2.1.1.3.2">512</cn><cn id="S4.F8.4.2.m2.1.1.3.3.cmml" type="integer" xref="S4.F8.4.2.m2.1.1.3.3">512</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F8.4.2.m2.1d">=~{}512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.F8.4.2.m2.1e">= 512 × 512</annotation></semantics></math></span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.7">What stands out in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.F8" title="Figure 8 ‣ 4.2.2 Influence of the loss function ‣ 4.2 Experiments using DeOldify Unet ‣ 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">8</span></a> is that results are very similar. At first sight, we struggle to distinguish any difference. But in fact, there are some. First if we look closer a at “big faces” images (the first two ones), we can see less face features in these images: face blurring is better performed with <math alttext="MSE" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p2.1.m1.1"><semantics id="S4.SS2.SSS2.p2.1.m1.1a"><mrow id="S4.SS2.SSS2.p2.1.m1.1.1" xref="S4.SS2.SSS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.SSS2.p2.1.m1.1.1.2" xref="S4.SS2.SSS2.p2.1.m1.1.1.2.cmml">M</mi><mo id="S4.SS2.SSS2.p2.1.m1.1.1.1" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS2.p2.1.m1.1.1.3" xref="S4.SS2.SSS2.p2.1.m1.1.1.3.cmml">S</mi><mo id="S4.SS2.SSS2.p2.1.m1.1.1.1a" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS2.p2.1.m1.1.1.4" xref="S4.SS2.SSS2.p2.1.m1.1.1.4.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.1.m1.1b"><apply id="S4.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1"><times id="S4.SS2.SSS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1.1"></times><ci id="S4.SS2.SSS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1.2">𝑀</ci><ci id="S4.SS2.SSS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1.3">𝑆</ci><ci id="S4.SS2.SSS2.p2.1.m1.1.1.4.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1.4">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.1.m1.1c">MSE</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p2.1.m1.1d">italic_M italic_S italic_E</annotation></semantics></math>. Then, for images 3, 4 and 5, results are almost the same. Finally, we see in the last image, that the <math alttext="MSE" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p2.2.m2.1"><semantics id="S4.SS2.SSS2.p2.2.m2.1a"><mrow id="S4.SS2.SSS2.p2.2.m2.1.1" xref="S4.SS2.SSS2.p2.2.m2.1.1.cmml"><mi id="S4.SS2.SSS2.p2.2.m2.1.1.2" xref="S4.SS2.SSS2.p2.2.m2.1.1.2.cmml">M</mi><mo id="S4.SS2.SSS2.p2.2.m2.1.1.1" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS2.p2.2.m2.1.1.3" xref="S4.SS2.SSS2.p2.2.m2.1.1.3.cmml">S</mi><mo id="S4.SS2.SSS2.p2.2.m2.1.1.1a" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS2.p2.2.m2.1.1.4" xref="S4.SS2.SSS2.p2.2.m2.1.1.4.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.2.m2.1b"><apply id="S4.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1"><times id="S4.SS2.SSS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1.1"></times><ci id="S4.SS2.SSS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1.2">𝑀</ci><ci id="S4.SS2.SSS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1.3">𝑆</ci><ci id="S4.SS2.SSS2.p2.2.m2.1.1.4.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1.4">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.2.m2.1c">MSE</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p2.2.m2.1d">italic_M italic_S italic_E</annotation></semantics></math> trained network succeeds in blurring the tiniest faces that the L1 trained did not blur. That is a already a strong improvement : <math alttext="MSE" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p2.3.m3.1"><semantics id="S4.SS2.SSS2.p2.3.m3.1a"><mrow id="S4.SS2.SSS2.p2.3.m3.1.1" xref="S4.SS2.SSS2.p2.3.m3.1.1.cmml"><mi id="S4.SS2.SSS2.p2.3.m3.1.1.2" xref="S4.SS2.SSS2.p2.3.m3.1.1.2.cmml">M</mi><mo id="S4.SS2.SSS2.p2.3.m3.1.1.1" xref="S4.SS2.SSS2.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS2.p2.3.m3.1.1.3" xref="S4.SS2.SSS2.p2.3.m3.1.1.3.cmml">S</mi><mo id="S4.SS2.SSS2.p2.3.m3.1.1.1a" xref="S4.SS2.SSS2.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS2.p2.3.m3.1.1.4" xref="S4.SS2.SSS2.p2.3.m3.1.1.4.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.3.m3.1b"><apply id="S4.SS2.SSS2.p2.3.m3.1.1.cmml" xref="S4.SS2.SSS2.p2.3.m3.1.1"><times id="S4.SS2.SSS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.SSS2.p2.3.m3.1.1.1"></times><ci id="S4.SS2.SSS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.SSS2.p2.3.m3.1.1.2">𝑀</ci><ci id="S4.SS2.SSS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.SSS2.p2.3.m3.1.1.3">𝑆</ci><ci id="S4.SS2.SSS2.p2.3.m3.1.1.4.cmml" xref="S4.SS2.SSS2.p2.3.m3.1.1.4">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.3.m3.1c">MSE</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p2.3.m3.1d">italic_M italic_S italic_E</annotation></semantics></math> tends to provide better results for small face than L1.
<br class="ltx_break"/>To be able to assert and verify this previous statement, we investigate the performance when considering an even smaller downsampling size. In that framework, faces appear to cover less pixels, turning them into smaller faces for the network. We consider a downsampling size of <math alttext="192\times 192" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p2.4.m4.1"><semantics id="S4.SS2.SSS2.p2.4.m4.1a"><mrow id="S4.SS2.SSS2.p2.4.m4.1.1" xref="S4.SS2.SSS2.p2.4.m4.1.1.cmml"><mn id="S4.SS2.SSS2.p2.4.m4.1.1.2" xref="S4.SS2.SSS2.p2.4.m4.1.1.2.cmml">192</mn><mo id="S4.SS2.SSS2.p2.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS2.p2.4.m4.1.1.1.cmml">×</mo><mn id="S4.SS2.SSS2.p2.4.m4.1.1.3" xref="S4.SS2.SSS2.p2.4.m4.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.4.m4.1b"><apply id="S4.SS2.SSS2.p2.4.m4.1.1.cmml" xref="S4.SS2.SSS2.p2.4.m4.1.1"><times id="S4.SS2.SSS2.p2.4.m4.1.1.1.cmml" xref="S4.SS2.SSS2.p2.4.m4.1.1.1"></times><cn id="S4.SS2.SSS2.p2.4.m4.1.1.2.cmml" type="integer" xref="S4.SS2.SSS2.p2.4.m4.1.1.2">192</cn><cn id="S4.SS2.SSS2.p2.4.m4.1.1.3.cmml" type="integer" xref="S4.SS2.SSS2.p2.4.m4.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.4.m4.1c">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p2.4.m4.1d">192 × 192</annotation></semantics></math> and display the results for both losses in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.F9" title="Figure 9 ‣ 4.2.2 Influence of the loss function ‣ 4.2 Experiments using DeOldify Unet ‣ 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">9</span></a>. We observe, as expected, that the <math alttext="MSE" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p2.5.m5.1"><semantics id="S4.SS2.SSS2.p2.5.m5.1a"><mrow id="S4.SS2.SSS2.p2.5.m5.1.1" xref="S4.SS2.SSS2.p2.5.m5.1.1.cmml"><mi id="S4.SS2.SSS2.p2.5.m5.1.1.2" xref="S4.SS2.SSS2.p2.5.m5.1.1.2.cmml">M</mi><mo id="S4.SS2.SSS2.p2.5.m5.1.1.1" xref="S4.SS2.SSS2.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS2.p2.5.m5.1.1.3" xref="S4.SS2.SSS2.p2.5.m5.1.1.3.cmml">S</mi><mo id="S4.SS2.SSS2.p2.5.m5.1.1.1a" xref="S4.SS2.SSS2.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS2.p2.5.m5.1.1.4" xref="S4.SS2.SSS2.p2.5.m5.1.1.4.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.5.m5.1b"><apply id="S4.SS2.SSS2.p2.5.m5.1.1.cmml" xref="S4.SS2.SSS2.p2.5.m5.1.1"><times id="S4.SS2.SSS2.p2.5.m5.1.1.1.cmml" xref="S4.SS2.SSS2.p2.5.m5.1.1.1"></times><ci id="S4.SS2.SSS2.p2.5.m5.1.1.2.cmml" xref="S4.SS2.SSS2.p2.5.m5.1.1.2">𝑀</ci><ci id="S4.SS2.SSS2.p2.5.m5.1.1.3.cmml" xref="S4.SS2.SSS2.p2.5.m5.1.1.3">𝑆</ci><ci id="S4.SS2.SSS2.p2.5.m5.1.1.4.cmml" xref="S4.SS2.SSS2.p2.5.m5.1.1.4">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.5.m5.1c">MSE</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p2.5.m5.1d">italic_M italic_S italic_E</annotation></semantics></math> trained network detects and blurs the great majority of faces in all the images while the L1 trained network misses some of them in images 3, 5 and 6. The most impressive results are for image 5 in which the <math alttext="MSE" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p2.6.m6.1"><semantics id="S4.SS2.SSS2.p2.6.m6.1a"><mrow id="S4.SS2.SSS2.p2.6.m6.1.1" xref="S4.SS2.SSS2.p2.6.m6.1.1.cmml"><mi id="S4.SS2.SSS2.p2.6.m6.1.1.2" xref="S4.SS2.SSS2.p2.6.m6.1.1.2.cmml">M</mi><mo id="S4.SS2.SSS2.p2.6.m6.1.1.1" xref="S4.SS2.SSS2.p2.6.m6.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS2.p2.6.m6.1.1.3" xref="S4.SS2.SSS2.p2.6.m6.1.1.3.cmml">S</mi><mo id="S4.SS2.SSS2.p2.6.m6.1.1.1a" xref="S4.SS2.SSS2.p2.6.m6.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS2.p2.6.m6.1.1.4" xref="S4.SS2.SSS2.p2.6.m6.1.1.4.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.6.m6.1b"><apply id="S4.SS2.SSS2.p2.6.m6.1.1.cmml" xref="S4.SS2.SSS2.p2.6.m6.1.1"><times id="S4.SS2.SSS2.p2.6.m6.1.1.1.cmml" xref="S4.SS2.SSS2.p2.6.m6.1.1.1"></times><ci id="S4.SS2.SSS2.p2.6.m6.1.1.2.cmml" xref="S4.SS2.SSS2.p2.6.m6.1.1.2">𝑀</ci><ci id="S4.SS2.SSS2.p2.6.m6.1.1.3.cmml" xref="S4.SS2.SSS2.p2.6.m6.1.1.3">𝑆</ci><ci id="S4.SS2.SSS2.p2.6.m6.1.1.4.cmml" xref="S4.SS2.SSS2.p2.6.m6.1.1.4">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.6.m6.1c">MSE</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p2.6.m6.1d">italic_M italic_S italic_E</annotation></semantics></math> trained network detects and blurs 5 more faces than the L1 trained network. Same for image 6 in which it detects and blurs 25 more faces. This shows that <math alttext="MSE" class="ltx_Math" display="inline" id="S4.SS2.SSS2.p2.7.m7.1"><semantics id="S4.SS2.SSS2.p2.7.m7.1a"><mrow id="S4.SS2.SSS2.p2.7.m7.1.1" xref="S4.SS2.SSS2.p2.7.m7.1.1.cmml"><mi id="S4.SS2.SSS2.p2.7.m7.1.1.2" xref="S4.SS2.SSS2.p2.7.m7.1.1.2.cmml">M</mi><mo id="S4.SS2.SSS2.p2.7.m7.1.1.1" xref="S4.SS2.SSS2.p2.7.m7.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS2.p2.7.m7.1.1.3" xref="S4.SS2.SSS2.p2.7.m7.1.1.3.cmml">S</mi><mo id="S4.SS2.SSS2.p2.7.m7.1.1.1a" xref="S4.SS2.SSS2.p2.7.m7.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS2.p2.7.m7.1.1.4" xref="S4.SS2.SSS2.p2.7.m7.1.1.4.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.7.m7.1b"><apply id="S4.SS2.SSS2.p2.7.m7.1.1.cmml" xref="S4.SS2.SSS2.p2.7.m7.1.1"><times id="S4.SS2.SSS2.p2.7.m7.1.1.1.cmml" xref="S4.SS2.SSS2.p2.7.m7.1.1.1"></times><ci id="S4.SS2.SSS2.p2.7.m7.1.1.2.cmml" xref="S4.SS2.SSS2.p2.7.m7.1.1.2">𝑀</ci><ci id="S4.SS2.SSS2.p2.7.m7.1.1.3.cmml" xref="S4.SS2.SSS2.p2.7.m7.1.1.3">𝑆</ci><ci id="S4.SS2.SSS2.p2.7.m7.1.1.4.cmml" xref="S4.SS2.SSS2.p2.7.m7.1.1.4">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.7.m7.1c">MSE</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p2.7.m7.1d">italic_M italic_S italic_E</annotation></semantics></math> is more sensitive to small faces while performing as well as L1 loss on bigger faces. We sum up in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.T4" title="Table 4 ‣ 4.2.2 Influence of the loss function ‣ 4.2 Experiments using DeOldify Unet ‣ 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">4</span></a> the blurred face counting.</p>
</div>
<figure class="ltx_figure" id="S4.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="89" id="S4.F9.g1" src="extracted/5872765/figures/test_set/original_images/image_0.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="89" id="S4.F9.g2" src="extracted/5872765/figures/test_set/mse_192/image_0.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="89" id="S4.F9.g3" src="extracted/5872765/figures/test_set/l1_192/image_0.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="145" id="S4.F9.g4" src="extracted/5872765/figures/test_set/original_images/image_25.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="145" id="S4.F9.g5" src="extracted/5872765/figures/test_set/mse_192/image_25.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="145" id="S4.F9.g6" src="extracted/5872765/figures/test_set/l1_192/image_25.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="81" id="S4.F9.g7" src="extracted/5872765/figures/test_set/original_images/image_1.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="81" id="S4.F9.g8" src="extracted/5872765/figures/test_set/mse_192/image_1.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="81" id="S4.F9.g9" src="extracted/5872765/figures/test_set/l1_192/image_1.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="108" id="S4.F9.g10" src="extracted/5872765/figures/test_set/original_images/image_16.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="108" id="S4.F9.g11" src="extracted/5872765/figures/test_set/mse_192/image_16.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="108" id="S4.F9.g12" src="extracted/5872765/figures/test_set/l1_192/image_16.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F9.g13" src="extracted/5872765/figures/test_set/original_images/image_57.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F9.g14" src="extracted/5872765/figures/test_set/mse_192/image_57.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F9.g15" src="extracted/5872765/figures/test_set/l1_192/image_57.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F9.g16" src="extracted/5872765/figures/test_set/original_images/image_24.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F9.g17" src="extracted/5872765/figures/test_set/mse_192/image_24.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F9.g18" src="extracted/5872765/figures/test_set/l1_192/image_24.jpg" width="144"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9.6.3.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="S4.F9.4.2" style="font-size:90%;">Test set results. Original image (left), result for <math alttext="MSE" class="ltx_Math" display="inline" id="S4.F9.3.1.m1.1"><semantics id="S4.F9.3.1.m1.1b"><mrow id="S4.F9.3.1.m1.1.1" xref="S4.F9.3.1.m1.1.1.cmml"><mi id="S4.F9.3.1.m1.1.1.2" xref="S4.F9.3.1.m1.1.1.2.cmml">M</mi><mo id="S4.F9.3.1.m1.1.1.1" xref="S4.F9.3.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.F9.3.1.m1.1.1.3" xref="S4.F9.3.1.m1.1.1.3.cmml">S</mi><mo id="S4.F9.3.1.m1.1.1.1b" xref="S4.F9.3.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.F9.3.1.m1.1.1.4" xref="S4.F9.3.1.m1.1.1.4.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F9.3.1.m1.1c"><apply id="S4.F9.3.1.m1.1.1.cmml" xref="S4.F9.3.1.m1.1.1"><times id="S4.F9.3.1.m1.1.1.1.cmml" xref="S4.F9.3.1.m1.1.1.1"></times><ci id="S4.F9.3.1.m1.1.1.2.cmml" xref="S4.F9.3.1.m1.1.1.2">𝑀</ci><ci id="S4.F9.3.1.m1.1.1.3.cmml" xref="S4.F9.3.1.m1.1.1.3">𝑆</ci><ci id="S4.F9.3.1.m1.1.1.4.cmml" xref="S4.F9.3.1.m1.1.1.4">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F9.3.1.m1.1d">MSE</annotation><annotation encoding="application/x-llamapun" id="S4.F9.3.1.m1.1e">italic_M italic_S italic_E</annotation></semantics></math> loss (center), result for L1 loss (right). Downsampling size <math alttext="=~{}192\times 192" class="ltx_Math" display="inline" id="S4.F9.4.2.m2.1"><semantics id="S4.F9.4.2.m2.1b"><mrow id="S4.F9.4.2.m2.1.1" xref="S4.F9.4.2.m2.1.1.cmml"><mi id="S4.F9.4.2.m2.1.1.2" xref="S4.F9.4.2.m2.1.1.2.cmml"></mi><mo id="S4.F9.4.2.m2.1.1.1" rspace="0.578em" xref="S4.F9.4.2.m2.1.1.1.cmml">=</mo><mrow id="S4.F9.4.2.m2.1.1.3" xref="S4.F9.4.2.m2.1.1.3.cmml"><mn id="S4.F9.4.2.m2.1.1.3.2" xref="S4.F9.4.2.m2.1.1.3.2.cmml">192</mn><mo id="S4.F9.4.2.m2.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S4.F9.4.2.m2.1.1.3.1.cmml">×</mo><mn id="S4.F9.4.2.m2.1.1.3.3" xref="S4.F9.4.2.m2.1.1.3.3.cmml">192</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.F9.4.2.m2.1c"><apply id="S4.F9.4.2.m2.1.1.cmml" xref="S4.F9.4.2.m2.1.1"><eq id="S4.F9.4.2.m2.1.1.1.cmml" xref="S4.F9.4.2.m2.1.1.1"></eq><csymbol cd="latexml" id="S4.F9.4.2.m2.1.1.2.cmml" xref="S4.F9.4.2.m2.1.1.2">absent</csymbol><apply id="S4.F9.4.2.m2.1.1.3.cmml" xref="S4.F9.4.2.m2.1.1.3"><times id="S4.F9.4.2.m2.1.1.3.1.cmml" xref="S4.F9.4.2.m2.1.1.3.1"></times><cn id="S4.F9.4.2.m2.1.1.3.2.cmml" type="integer" xref="S4.F9.4.2.m2.1.1.3.2">192</cn><cn id="S4.F9.4.2.m2.1.1.3.3.cmml" type="integer" xref="S4.F9.4.2.m2.1.1.3.3">192</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F9.4.2.m2.1d">=~{}192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.F9.4.2.m2.1e">= 192 × 192</annotation></semantics></math></span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.5" style="width:433.6pt;height:199.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(40.6pt,-18.7pt) scale(1.23061176271694,1.23061176271694) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.5.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.5.5.6.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.T4.5.5.6.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.5.5.6.1.2" rowspan="3"><span class="ltx_text" id="S4.T4.5.5.6.1.2.1">Number of faces</span></th>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" colspan="4" id="S4.T4.5.5.6.1.3">Number of faces blurred</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.1.1.2"></th>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" colspan="2" id="S4.T4.1.1.1.1"><math alttext="MSE" class="ltx_Math" display="inline" id="S4.T4.1.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.1.m1.1a"><mrow id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml"><mi id="S4.T4.1.1.1.1.m1.1.1.2" xref="S4.T4.1.1.1.1.m1.1.1.2.cmml">M</mi><mo id="S4.T4.1.1.1.1.m1.1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.T4.1.1.1.1.m1.1.1.3" xref="S4.T4.1.1.1.1.m1.1.1.3.cmml">S</mi><mo id="S4.T4.1.1.1.1.m1.1.1.1a" xref="S4.T4.1.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.T4.1.1.1.1.m1.1.1.4" xref="S4.T4.1.1.1.1.m1.1.1.4.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><apply id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1"><times id="S4.T4.1.1.1.1.m1.1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1.1"></times><ci id="S4.T4.1.1.1.1.m1.1.1.2.cmml" xref="S4.T4.1.1.1.1.m1.1.1.2">𝑀</ci><ci id="S4.T4.1.1.1.1.m1.1.1.3.cmml" xref="S4.T4.1.1.1.1.m1.1.1.3">𝑆</ci><ci id="S4.T4.1.1.1.1.m1.1.1.4.cmml" xref="S4.T4.1.1.1.1.m1.1.1.4">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">MSE</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.1.m1.1d">italic_M italic_S italic_E</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" colspan="2" id="S4.T4.1.1.1.3">L1</td>
</tr>
<tr class="ltx_tr" id="S4.T4.5.5.5">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.T4.5.5.5.5"></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.2.2.2.1"><math alttext="192\times 192" class="ltx_Math" display="inline" id="S4.T4.2.2.2.1.m1.1"><semantics id="S4.T4.2.2.2.1.m1.1a"><mrow id="S4.T4.2.2.2.1.m1.1.1" xref="S4.T4.2.2.2.1.m1.1.1.cmml"><mn id="S4.T4.2.2.2.1.m1.1.1.2" xref="S4.T4.2.2.2.1.m1.1.1.2.cmml">192</mn><mo id="S4.T4.2.2.2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T4.2.2.2.1.m1.1.1.1.cmml">×</mo><mn id="S4.T4.2.2.2.1.m1.1.1.3" xref="S4.T4.2.2.2.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.1.m1.1b"><apply id="S4.T4.2.2.2.1.m1.1.1.cmml" xref="S4.T4.2.2.2.1.m1.1.1"><times id="S4.T4.2.2.2.1.m1.1.1.1.cmml" xref="S4.T4.2.2.2.1.m1.1.1.1"></times><cn id="S4.T4.2.2.2.1.m1.1.1.2.cmml" type="integer" xref="S4.T4.2.2.2.1.m1.1.1.2">192</cn><cn id="S4.T4.2.2.2.1.m1.1.1.3.cmml" type="integer" xref="S4.T4.2.2.2.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.1.m1.1c">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.1.m1.1d">192 × 192</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.3.3.3.2"><math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.T4.3.3.3.2.m1.1"><semantics id="S4.T4.3.3.3.2.m1.1a"><mrow id="S4.T4.3.3.3.2.m1.1.1" xref="S4.T4.3.3.3.2.m1.1.1.cmml"><mn id="S4.T4.3.3.3.2.m1.1.1.2" xref="S4.T4.3.3.3.2.m1.1.1.2.cmml">512</mn><mo id="S4.T4.3.3.3.2.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T4.3.3.3.2.m1.1.1.1.cmml">×</mo><mn id="S4.T4.3.3.3.2.m1.1.1.3" xref="S4.T4.3.3.3.2.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.2.m1.1b"><apply id="S4.T4.3.3.3.2.m1.1.1.cmml" xref="S4.T4.3.3.3.2.m1.1.1"><times id="S4.T4.3.3.3.2.m1.1.1.1.cmml" xref="S4.T4.3.3.3.2.m1.1.1.1"></times><cn id="S4.T4.3.3.3.2.m1.1.1.2.cmml" type="integer" xref="S4.T4.3.3.3.2.m1.1.1.2">512</cn><cn id="S4.T4.3.3.3.2.m1.1.1.3.cmml" type="integer" xref="S4.T4.3.3.3.2.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.2.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.3.2.m1.1d">512 × 512</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.4.4.4.3"><math alttext="192\times 192" class="ltx_Math" display="inline" id="S4.T4.4.4.4.3.m1.1"><semantics id="S4.T4.4.4.4.3.m1.1a"><mrow id="S4.T4.4.4.4.3.m1.1.1" xref="S4.T4.4.4.4.3.m1.1.1.cmml"><mn id="S4.T4.4.4.4.3.m1.1.1.2" xref="S4.T4.4.4.4.3.m1.1.1.2.cmml">192</mn><mo id="S4.T4.4.4.4.3.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T4.4.4.4.3.m1.1.1.1.cmml">×</mo><mn id="S4.T4.4.4.4.3.m1.1.1.3" xref="S4.T4.4.4.4.3.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.3.m1.1b"><apply id="S4.T4.4.4.4.3.m1.1.1.cmml" xref="S4.T4.4.4.4.3.m1.1.1"><times id="S4.T4.4.4.4.3.m1.1.1.1.cmml" xref="S4.T4.4.4.4.3.m1.1.1.1"></times><cn id="S4.T4.4.4.4.3.m1.1.1.2.cmml" type="integer" xref="S4.T4.4.4.4.3.m1.1.1.2">192</cn><cn id="S4.T4.4.4.4.3.m1.1.1.3.cmml" type="integer" xref="S4.T4.4.4.4.3.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.3.m1.1c">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.T4.4.4.4.3.m1.1d">192 × 192</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.5.5.5.4"><math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.T4.5.5.5.4.m1.1"><semantics id="S4.T4.5.5.5.4.m1.1a"><mrow id="S4.T4.5.5.5.4.m1.1.1" xref="S4.T4.5.5.5.4.m1.1.1.cmml"><mn id="S4.T4.5.5.5.4.m1.1.1.2" xref="S4.T4.5.5.5.4.m1.1.1.2.cmml">512</mn><mo id="S4.T4.5.5.5.4.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T4.5.5.5.4.m1.1.1.1.cmml">×</mo><mn id="S4.T4.5.5.5.4.m1.1.1.3" xref="S4.T4.5.5.5.4.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.4.m1.1b"><apply id="S4.T4.5.5.5.4.m1.1.1.cmml" xref="S4.T4.5.5.5.4.m1.1.1"><times id="S4.T4.5.5.5.4.m1.1.1.1.cmml" xref="S4.T4.5.5.5.4.m1.1.1.1"></times><cn id="S4.T4.5.5.5.4.m1.1.1.2.cmml" type="integer" xref="S4.T4.5.5.5.4.m1.1.1.2">512</cn><cn id="S4.T4.5.5.5.4.m1.1.1.3.cmml" type="integer" xref="S4.T4.5.5.5.4.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.4.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.T4.5.5.5.4.m1.1d">512 × 512</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T4.5.5.7.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T4.5.5.7.2.1">image 1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T4.5.5.7.2.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.5.5.7.2.3"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.7.2.3.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.5.5.7.2.4"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.7.2.4.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.5.5.7.2.5"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.7.2.5.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.5.5.7.2.6"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.7.2.6.1">1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.5.5.8.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.5.5.8.3.1">image 2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.5.5.8.3.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.5.5.8.3.3"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.8.3.3.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.5.5.8.3.4"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.8.3.4.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.5.5.8.3.5"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.8.3.5.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.5.5.8.3.6"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.8.3.6.1">1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.5.5.9.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T4.5.5.9.4.1">image 3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T4.5.5.9.4.2">5</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.5.5.9.4.3"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.9.4.3.1">5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.5.5.9.4.4"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.9.4.4.1">5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.5.5.9.4.5">4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.5.5.9.4.6"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.9.4.6.1">5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.5.5.10.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.5.5.10.5.1">image 4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.5.5.10.5.2">6</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.5.5.10.5.3"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.10.5.3.1">6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.5.5.10.5.4"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.10.5.4.1">6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.5.5.10.5.5"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.10.5.5.1">6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.5.5.10.5.6"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.10.5.6.1">6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.5.5.11.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T4.5.5.11.6.1">image 5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T4.5.5.11.6.2">18</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.5.5.11.6.3"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.11.6.3.1">18</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.5.5.11.6.4"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.11.6.4.1">18</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.5.5.11.6.5">13</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.5.5.11.6.6"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.11.6.6.1">18</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.5.5.12.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.5.5.12.7.1">image 6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.5.5.12.7.2">51</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.5.5.12.7.3">45</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.5.5.12.7.4"><span class="ltx_text ltx_font_bold" id="S4.T4.5.5.12.7.4.1">50</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.5.5.12.7.5">20</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.5.5.12.7.6">48</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.7.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S4.T4.8.2" style="font-size:90%;">Face counting results (the best results for each image is in bold)</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.1">We do not compare the computation times in this section as there are exactly the same as in the previous section. In fact, models have the exact same number of parameters and the pre and postprocessing steps are identical.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Influence of the self-attention layer</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">In this section, we study the importance of the self-attention layer and if it is relevant in our case.
In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib9" title="">9</a>]</cite>, the introduction of this layer is motivated by the long-time studied question of non-locality in images. Paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib1" title="">1</a>]</cite> was one of the first articles to propose a non-local algorithm in the context of denoising. This seminal work proved that to denoise a given patch of an image, all other patches of the images, provided that they are similar to the original patch, contain information relevant to denoise the patch. The self-attention mechanism is a similar process, thus more general. In the context of colorizing, this layer appears to be relevant in order to combine information, and in particular to propagate colorization to patches that are similar.
<br class="ltx_break"/>In our context, we hypothesize that this could permit to combine information from faces all over the image and improve the face blurring performance. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.2"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p2.2.1">Visual evaluation
<br class="ltx_break"/></span>In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.F10" title="Figure 10 ‣ 4.2.3 Influence of the self-attention layer ‣ 4.2 Experiments using DeOldify Unet ‣ 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">10</span></a>, we compare the results of the <math alttext="MSE" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.1.m1.1"><semantics id="S4.SS2.SSS3.p2.1.m1.1a"><mrow id="S4.SS2.SSS3.p2.1.m1.1.1" xref="S4.SS2.SSS3.p2.1.m1.1.1.cmml"><mi id="S4.SS2.SSS3.p2.1.m1.1.1.2" xref="S4.SS2.SSS3.p2.1.m1.1.1.2.cmml">M</mi><mo id="S4.SS2.SSS3.p2.1.m1.1.1.1" xref="S4.SS2.SSS3.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS3.p2.1.m1.1.1.3" xref="S4.SS2.SSS3.p2.1.m1.1.1.3.cmml">S</mi><mo id="S4.SS2.SSS3.p2.1.m1.1.1.1a" xref="S4.SS2.SSS3.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS2.SSS3.p2.1.m1.1.1.4" xref="S4.SS2.SSS3.p2.1.m1.1.1.4.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.1.m1.1b"><apply id="S4.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1"><times id="S4.SS2.SSS3.p2.1.m1.1.1.1.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1.1"></times><ci id="S4.SS2.SSS3.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1.2">𝑀</ci><ci id="S4.SS2.SSS3.p2.1.m1.1.1.3.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1.3">𝑆</ci><ci id="S4.SS2.SSS3.p2.1.m1.1.1.4.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1.4">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.1.m1.1c">MSE</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.1.m1.1d">italic_M italic_S italic_E</annotation></semantics></math> trained network, with a downsampling size of <math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p2.2.m2.1"><semantics id="S4.SS2.SSS3.p2.2.m2.1a"><mrow id="S4.SS2.SSS3.p2.2.m2.1.1" xref="S4.SS2.SSS3.p2.2.m2.1.1.cmml"><mn id="S4.SS2.SSS3.p2.2.m2.1.1.2" xref="S4.SS2.SSS3.p2.2.m2.1.1.2.cmml">512</mn><mo id="S4.SS2.SSS3.p2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS3.p2.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS2.SSS3.p2.2.m2.1.1.3" xref="S4.SS2.SSS3.p2.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.2.m2.1b"><apply id="S4.SS2.SSS3.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS3.p2.2.m2.1.1"><times id="S4.SS2.SSS3.p2.2.m2.1.1.1.cmml" xref="S4.SS2.SSS3.p2.2.m2.1.1.1"></times><cn id="S4.SS2.SSS3.p2.2.m2.1.1.2.cmml" type="integer" xref="S4.SS2.SSS3.p2.2.m2.1.1.2">512</cn><cn id="S4.SS2.SSS3.p2.2.m2.1.1.3.cmml" type="integer" xref="S4.SS2.SSS3.p2.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.2.m2.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p2.2.m2.1d">512 × 512</annotation></semantics></math> with and without the self-attention layer.</p>
</div>
<figure class="ltx_figure" id="S4.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="89" id="S4.F10.g1" src="extracted/5872765/figures/test_set/original_images/image_0.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="89" id="S4.F10.g2" src="extracted/5872765/figures/test_set/mse_512_wsa/image_0.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="89" id="S4.F10.g3" src="extracted/5872765/figures/test_set/mse_512/image_0.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="145" id="S4.F10.g4" src="extracted/5872765/figures/test_set/original_images/image_25.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="145" id="S4.F10.g5" src="extracted/5872765/figures/test_set/mse_512_wsa/image_25.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="145" id="S4.F10.g6" src="extracted/5872765/figures/test_set/mse_512/image_25.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="81" id="S4.F10.g7" src="extracted/5872765/figures/test_set/original_images/image_1.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="81" id="S4.F10.g8" src="extracted/5872765/figures/test_set/mse_512_wsa/image_1.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="81" id="S4.F10.g9" src="extracted/5872765/figures/test_set/mse_512/image_1.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="108" id="S4.F10.g10" src="extracted/5872765/figures/test_set/original_images/image_16.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="108" id="S4.F10.g11" src="extracted/5872765/figures/test_set/mse_512_wsa/image_16.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="108" id="S4.F10.g12" src="extracted/5872765/figures/test_set/mse_512/image_16.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F10.g13" src="extracted/5872765/figures/test_set/original_images/image_57.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F10.g14" src="extracted/5872765/figures/test_set/mse_512_wsa/image_57.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F10.g15" src="extracted/5872765/figures/test_set/mse_512/image_57.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F10.g16" src="extracted/5872765/figures/test_set/original_images/image_24.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F10.g17" src="extracted/5872765/figures/test_set/mse_512_wsa/image_24.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F10.g18" src="extracted/5872765/figures/test_set/mse_512/image_24.jpg" width="144"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F10.6.3.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" id="S4.F10.4.2" style="font-size:90%;">Test set results. Original image (left), result without self-attention layer (center), with self-attention layer (right). Model trained with <math alttext="MSE" class="ltx_Math" display="inline" id="S4.F10.3.1.m1.1"><semantics id="S4.F10.3.1.m1.1b"><mrow id="S4.F10.3.1.m1.1.1" xref="S4.F10.3.1.m1.1.1.cmml"><mi id="S4.F10.3.1.m1.1.1.2" xref="S4.F10.3.1.m1.1.1.2.cmml">M</mi><mo id="S4.F10.3.1.m1.1.1.1" xref="S4.F10.3.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.F10.3.1.m1.1.1.3" xref="S4.F10.3.1.m1.1.1.3.cmml">S</mi><mo id="S4.F10.3.1.m1.1.1.1b" xref="S4.F10.3.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.F10.3.1.m1.1.1.4" xref="S4.F10.3.1.m1.1.1.4.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F10.3.1.m1.1c"><apply id="S4.F10.3.1.m1.1.1.cmml" xref="S4.F10.3.1.m1.1.1"><times id="S4.F10.3.1.m1.1.1.1.cmml" xref="S4.F10.3.1.m1.1.1.1"></times><ci id="S4.F10.3.1.m1.1.1.2.cmml" xref="S4.F10.3.1.m1.1.1.2">𝑀</ci><ci id="S4.F10.3.1.m1.1.1.3.cmml" xref="S4.F10.3.1.m1.1.1.3">𝑆</ci><ci id="S4.F10.3.1.m1.1.1.4.cmml" xref="S4.F10.3.1.m1.1.1.4">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F10.3.1.m1.1d">MSE</annotation><annotation encoding="application/x-llamapun" id="S4.F10.3.1.m1.1e">italic_M italic_S italic_E</annotation></semantics></math> loss. Downsampling size <math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.F10.4.2.m2.1"><semantics id="S4.F10.4.2.m2.1b"><mrow id="S4.F10.4.2.m2.1.1" xref="S4.F10.4.2.m2.1.1.cmml"><mn id="S4.F10.4.2.m2.1.1.2" xref="S4.F10.4.2.m2.1.1.2.cmml">512</mn><mo id="S4.F10.4.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.F10.4.2.m2.1.1.1.cmml">×</mo><mn id="S4.F10.4.2.m2.1.1.3" xref="S4.F10.4.2.m2.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F10.4.2.m2.1c"><apply id="S4.F10.4.2.m2.1.1.cmml" xref="S4.F10.4.2.m2.1.1"><times id="S4.F10.4.2.m2.1.1.1.cmml" xref="S4.F10.4.2.m2.1.1.1"></times><cn id="S4.F10.4.2.m2.1.1.2.cmml" type="integer" xref="S4.F10.4.2.m2.1.1.2">512</cn><cn id="S4.F10.4.2.m2.1.1.3.cmml" type="integer" xref="S4.F10.4.2.m2.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F10.4.2.m2.1d">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.F10.4.2.m2.1e">512 × 512</annotation></semantics></math>.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS3.p3">
<p class="ltx_p" id="S4.SS2.SSS3.p3.2">Visually, on Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.F10" title="Figure 10 ‣ 4.2.3 Influence of the self-attention layer ‣ 4.2 Experiments using DeOldify Unet ‣ 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">10</span></a> we struggle to find differences for a downsampling size of <math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p3.1.m1.1"><semantics id="S4.SS2.SSS3.p3.1.m1.1a"><mrow id="S4.SS2.SSS3.p3.1.m1.1.1" xref="S4.SS2.SSS3.p3.1.m1.1.1.cmml"><mn id="S4.SS2.SSS3.p3.1.m1.1.1.2" xref="S4.SS2.SSS3.p3.1.m1.1.1.2.cmml">512</mn><mo id="S4.SS2.SSS3.p3.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS3.p3.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.SSS3.p3.1.m1.1.1.3" xref="S4.SS2.SSS3.p3.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p3.1.m1.1b"><apply id="S4.SS2.SSS3.p3.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p3.1.m1.1.1"><times id="S4.SS2.SSS3.p3.1.m1.1.1.1.cmml" xref="S4.SS2.SSS3.p3.1.m1.1.1.1"></times><cn id="S4.SS2.SSS3.p3.1.m1.1.1.2.cmml" type="integer" xref="S4.SS2.SSS3.p3.1.m1.1.1.2">512</cn><cn id="S4.SS2.SSS3.p3.1.m1.1.1.3.cmml" type="integer" xref="S4.SS2.SSS3.p3.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p3.1.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p3.1.m1.1d">512 × 512</annotation></semantics></math>. We then display in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.F11" title="Figure 11 ‣ 4.2.3 Influence of the self-attention layer ‣ 4.2 Experiments using DeOldify Unet ‣ 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">11</span></a> the results on the test set for a downsampling size of <math alttext="192\times 192" class="ltx_Math" display="inline" id="S4.SS2.SSS3.p3.2.m2.1"><semantics id="S4.SS2.SSS3.p3.2.m2.1a"><mrow id="S4.SS2.SSS3.p3.2.m2.1.1" xref="S4.SS2.SSS3.p3.2.m2.1.1.cmml"><mn id="S4.SS2.SSS3.p3.2.m2.1.1.2" xref="S4.SS2.SSS3.p3.2.m2.1.1.2.cmml">192</mn><mo id="S4.SS2.SSS3.p3.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS3.p3.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS2.SSS3.p3.2.m2.1.1.3" xref="S4.SS2.SSS3.p3.2.m2.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p3.2.m2.1b"><apply id="S4.SS2.SSS3.p3.2.m2.1.1.cmml" xref="S4.SS2.SSS3.p3.2.m2.1.1"><times id="S4.SS2.SSS3.p3.2.m2.1.1.1.cmml" xref="S4.SS2.SSS3.p3.2.m2.1.1.1"></times><cn id="S4.SS2.SSS3.p3.2.m2.1.1.2.cmml" type="integer" xref="S4.SS2.SSS3.p3.2.m2.1.1.2">192</cn><cn id="S4.SS2.SSS3.p3.2.m2.1.1.3.cmml" type="integer" xref="S4.SS2.SSS3.p3.2.m2.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p3.2.m2.1c">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS3.p3.2.m2.1d">192 × 192</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="S4.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="89" id="S4.F11.g1" src="extracted/5872765/figures/test_set/original_images/image_0.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="89" id="S4.F11.g2" src="extracted/5872765/figures/test_set/mse_192_wsa/image_0.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="89" id="S4.F11.g3" src="extracted/5872765/figures/test_set/mse_192/image_0.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="145" id="S4.F11.g4" src="extracted/5872765/figures/test_set/original_images/image_25.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="145" id="S4.F11.g5" src="extracted/5872765/figures/test_set/mse_192_wsa/image_25.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="145" id="S4.F11.g6" src="extracted/5872765/figures/test_set/mse_192/image_25.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="81" id="S4.F11.g7" src="extracted/5872765/figures/test_set/original_images/image_1.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="81" id="S4.F11.g8" src="extracted/5872765/figures/test_set/mse_192_wsa/image_1.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="81" id="S4.F11.g9" src="extracted/5872765/figures/test_set/mse_192/image_1.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="108" id="S4.F11.g10" src="extracted/5872765/figures/test_set/original_images/image_16.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="108" id="S4.F11.g11" src="extracted/5872765/figures/test_set/mse_192_wsa/image_16.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="108" id="S4.F11.g12" src="extracted/5872765/figures/test_set/mse_192/image_16.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F11.g13" src="extracted/5872765/figures/test_set/original_images/image_57.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F11.g14" src="extracted/5872765/figures/test_set/mse_192_wsa/image_57.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F11.g15" src="extracted/5872765/figures/test_set/mse_192/image_57.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F11.g16" src="extracted/5872765/figures/test_set/original_images/image_24.jpg" width="144"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F11.g17" src="extracted/5872765/figures/test_set/mse_192_wsa/image_24.jpg" width="144"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="96" id="S4.F11.g18" src="extracted/5872765/figures/test_set/mse_192/image_24.jpg" width="144"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F11.6.3.1" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text" id="S4.F11.4.2" style="font-size:90%;">Test set results. Original image (left), result without self-attention layer (center), with self-attention layer (right). Model trained with <math alttext="MSE" class="ltx_Math" display="inline" id="S4.F11.3.1.m1.1"><semantics id="S4.F11.3.1.m1.1b"><mrow id="S4.F11.3.1.m1.1.1" xref="S4.F11.3.1.m1.1.1.cmml"><mi id="S4.F11.3.1.m1.1.1.2" xref="S4.F11.3.1.m1.1.1.2.cmml">M</mi><mo id="S4.F11.3.1.m1.1.1.1" xref="S4.F11.3.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.F11.3.1.m1.1.1.3" xref="S4.F11.3.1.m1.1.1.3.cmml">S</mi><mo id="S4.F11.3.1.m1.1.1.1b" xref="S4.F11.3.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.F11.3.1.m1.1.1.4" xref="S4.F11.3.1.m1.1.1.4.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F11.3.1.m1.1c"><apply id="S4.F11.3.1.m1.1.1.cmml" xref="S4.F11.3.1.m1.1.1"><times id="S4.F11.3.1.m1.1.1.1.cmml" xref="S4.F11.3.1.m1.1.1.1"></times><ci id="S4.F11.3.1.m1.1.1.2.cmml" xref="S4.F11.3.1.m1.1.1.2">𝑀</ci><ci id="S4.F11.3.1.m1.1.1.3.cmml" xref="S4.F11.3.1.m1.1.1.3">𝑆</ci><ci id="S4.F11.3.1.m1.1.1.4.cmml" xref="S4.F11.3.1.m1.1.1.4">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F11.3.1.m1.1d">MSE</annotation><annotation encoding="application/x-llamapun" id="S4.F11.3.1.m1.1e">italic_M italic_S italic_E</annotation></semantics></math> loss. Downsampling size <math alttext="192\times 192" class="ltx_Math" display="inline" id="S4.F11.4.2.m2.1"><semantics id="S4.F11.4.2.m2.1b"><mrow id="S4.F11.4.2.m2.1.1" xref="S4.F11.4.2.m2.1.1.cmml"><mn id="S4.F11.4.2.m2.1.1.2" xref="S4.F11.4.2.m2.1.1.2.cmml">192</mn><mo id="S4.F11.4.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.F11.4.2.m2.1.1.1.cmml">×</mo><mn id="S4.F11.4.2.m2.1.1.3" xref="S4.F11.4.2.m2.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F11.4.2.m2.1c"><apply id="S4.F11.4.2.m2.1.1.cmml" xref="S4.F11.4.2.m2.1.1"><times id="S4.F11.4.2.m2.1.1.1.cmml" xref="S4.F11.4.2.m2.1.1.1"></times><cn id="S4.F11.4.2.m2.1.1.2.cmml" type="integer" xref="S4.F11.4.2.m2.1.1.2">192</cn><cn id="S4.F11.4.2.m2.1.1.3.cmml" type="integer" xref="S4.F11.4.2.m2.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F11.4.2.m2.1d">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.F11.4.2.m2.1e">192 × 192</annotation></semantics></math>.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS3.p4">
<p class="ltx_p" id="S4.SS2.SSS3.p4.1">Once again, it is very hard to spot any visual difference between the two models on Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.F11" title="Figure 11 ‣ 4.2.3 Influence of the self-attention layer ‣ 4.2 Experiments using DeOldify Unet ‣ 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">11</span></a>. Our conclusion is that it is not relevant to add this self-attention layer to our model.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS3.p5">
<p class="ltx_p" id="S4.SS2.SSS3.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p5.1.1">Computation time
<br class="ltx_break"/></span>It in interesting to compare computation times with and without the self-attention layer. In fact the self-attention layer has a quadratic complexity with respect to the input dimension. To this extent, we expect that inference time with self-attention layer has a higher computation time. Here we conduct only one experiment assuming that input images are already of the right downsampling size. In fact, the only difference in computation time is when we pass through the model, pre and post processing steps are exactly the same.
<br class="ltx_break"/>We calculate computation times for 100 images and average them for each experiment. Results are displayed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.T5" title="Table 5 ‣ 4.2.3 Influence of the self-attention layer ‣ 4.2 Experiments using DeOldify Unet ‣ 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">5</span></a> and expressed in frames per second.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T5.9" style="width:433.6pt;height:84pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(77.4pt,-15.0pt) scale(1.55481147437881,1.55481147437881) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T5.9.9">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.3.3.3">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T5.3.3.3.4"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T5.1.1.1.1"><math alttext="192\times 192" class="ltx_Math" display="inline" id="S4.T5.1.1.1.1.m1.1"><semantics id="S4.T5.1.1.1.1.m1.1a"><mrow id="S4.T5.1.1.1.1.m1.1.1" xref="S4.T5.1.1.1.1.m1.1.1.cmml"><mn id="S4.T5.1.1.1.1.m1.1.1.2" xref="S4.T5.1.1.1.1.m1.1.1.2.cmml">192</mn><mo id="S4.T5.1.1.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T5.1.1.1.1.m1.1.1.1.cmml">×</mo><mn id="S4.T5.1.1.1.1.m1.1.1.3" xref="S4.T5.1.1.1.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.m1.1b"><apply id="S4.T5.1.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.1.m1.1.1"><times id="S4.T5.1.1.1.1.m1.1.1.1.cmml" xref="S4.T5.1.1.1.1.m1.1.1.1"></times><cn id="S4.T5.1.1.1.1.m1.1.1.2.cmml" type="integer" xref="S4.T5.1.1.1.1.m1.1.1.2">192</cn><cn id="S4.T5.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S4.T5.1.1.1.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.m1.1c">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.T5.1.1.1.1.m1.1d">192 × 192</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T5.2.2.2.2"><math alttext="256\times 256" class="ltx_Math" display="inline" id="S4.T5.2.2.2.2.m1.1"><semantics id="S4.T5.2.2.2.2.m1.1a"><mrow id="S4.T5.2.2.2.2.m1.1.1" xref="S4.T5.2.2.2.2.m1.1.1.cmml"><mn id="S4.T5.2.2.2.2.m1.1.1.2" xref="S4.T5.2.2.2.2.m1.1.1.2.cmml">256</mn><mo id="S4.T5.2.2.2.2.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T5.2.2.2.2.m1.1.1.1.cmml">×</mo><mn id="S4.T5.2.2.2.2.m1.1.1.3" xref="S4.T5.2.2.2.2.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.2.m1.1b"><apply id="S4.T5.2.2.2.2.m1.1.1.cmml" xref="S4.T5.2.2.2.2.m1.1.1"><times id="S4.T5.2.2.2.2.m1.1.1.1.cmml" xref="S4.T5.2.2.2.2.m1.1.1.1"></times><cn id="S4.T5.2.2.2.2.m1.1.1.2.cmml" type="integer" xref="S4.T5.2.2.2.2.m1.1.1.2">256</cn><cn id="S4.T5.2.2.2.2.m1.1.1.3.cmml" type="integer" xref="S4.T5.2.2.2.2.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.2.m1.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S4.T5.2.2.2.2.m1.1d">256 × 256</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T5.3.3.3.3"><math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.T5.3.3.3.3.m1.1"><semantics id="S4.T5.3.3.3.3.m1.1a"><mrow id="S4.T5.3.3.3.3.m1.1.1" xref="S4.T5.3.3.3.3.m1.1.1.cmml"><mn id="S4.T5.3.3.3.3.m1.1.1.2" xref="S4.T5.3.3.3.3.m1.1.1.2.cmml">512</mn><mo id="S4.T5.3.3.3.3.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T5.3.3.3.3.m1.1.1.1.cmml">×</mo><mn id="S4.T5.3.3.3.3.m1.1.1.3" xref="S4.T5.3.3.3.3.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.3.3.m1.1b"><apply id="S4.T5.3.3.3.3.m1.1.1.cmml" xref="S4.T5.3.3.3.3.m1.1.1"><times id="S4.T5.3.3.3.3.m1.1.1.1.cmml" xref="S4.T5.3.3.3.3.m1.1.1.1"></times><cn id="S4.T5.3.3.3.3.m1.1.1.2.cmml" type="integer" xref="S4.T5.3.3.3.3.m1.1.1.2">512</cn><cn id="S4.T5.3.3.3.3.m1.1.1.3.cmml" type="integer" xref="S4.T5.3.3.3.3.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.3.3.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.T5.3.3.3.3.m1.1d">512 × 512</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.6.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T5.6.6.6.4">With self-attention</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T5.4.4.4.1"><math alttext="28.4" class="ltx_Math" display="inline" id="S4.T5.4.4.4.1.m1.1"><semantics id="S4.T5.4.4.4.1.m1.1a"><mn id="S4.T5.4.4.4.1.m1.1.1" xref="S4.T5.4.4.4.1.m1.1.1.cmml">28.4</mn><annotation-xml encoding="MathML-Content" id="S4.T5.4.4.4.1.m1.1b"><cn id="S4.T5.4.4.4.1.m1.1.1.cmml" type="float" xref="S4.T5.4.4.4.1.m1.1.1">28.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.4.4.4.1.m1.1c">28.4</annotation><annotation encoding="application/x-llamapun" id="S4.T5.4.4.4.1.m1.1d">28.4</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T5.5.5.5.2"><math alttext="22.0" class="ltx_Math" display="inline" id="S4.T5.5.5.5.2.m1.1"><semantics id="S4.T5.5.5.5.2.m1.1a"><mn id="S4.T5.5.5.5.2.m1.1.1" xref="S4.T5.5.5.5.2.m1.1.1.cmml">22.0</mn><annotation-xml encoding="MathML-Content" id="S4.T5.5.5.5.2.m1.1b"><cn id="S4.T5.5.5.5.2.m1.1.1.cmml" type="float" xref="S4.T5.5.5.5.2.m1.1.1">22.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.5.5.5.2.m1.1c">22.0</annotation><annotation encoding="application/x-llamapun" id="S4.T5.5.5.5.2.m1.1d">22.0</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.6.6.6.3"><math alttext="9.6" class="ltx_Math" display="inline" id="S4.T5.6.6.6.3.m1.1"><semantics id="S4.T5.6.6.6.3.m1.1a"><mn id="S4.T5.6.6.6.3.m1.1.1" xref="S4.T5.6.6.6.3.m1.1.1.cmml">9.6</mn><annotation-xml encoding="MathML-Content" id="S4.T5.6.6.6.3.m1.1b"><cn id="S4.T5.6.6.6.3.m1.1.1.cmml" type="float" xref="S4.T5.6.6.6.3.m1.1.1">9.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.6.6.6.3.m1.1c">9.6</annotation><annotation encoding="application/x-llamapun" id="S4.T5.6.6.6.3.m1.1d">9.6</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T5.9.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T5.9.9.9.4">Without self-attention</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.7.7.7.1"><math alttext="28.8" class="ltx_Math" display="inline" id="S4.T5.7.7.7.1.m1.1"><semantics id="S4.T5.7.7.7.1.m1.1a"><mn id="S4.T5.7.7.7.1.m1.1.1" xref="S4.T5.7.7.7.1.m1.1.1.cmml">28.8</mn><annotation-xml encoding="MathML-Content" id="S4.T5.7.7.7.1.m1.1b"><cn id="S4.T5.7.7.7.1.m1.1.1.cmml" type="float" xref="S4.T5.7.7.7.1.m1.1.1">28.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.7.7.7.1.m1.1c">28.8</annotation><annotation encoding="application/x-llamapun" id="S4.T5.7.7.7.1.m1.1d">28.8</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.8.8.8.2"><math alttext="22.6" class="ltx_Math" display="inline" id="S4.T5.8.8.8.2.m1.1"><semantics id="S4.T5.8.8.8.2.m1.1a"><mn id="S4.T5.8.8.8.2.m1.1.1" xref="S4.T5.8.8.8.2.m1.1.1.cmml">22.6</mn><annotation-xml encoding="MathML-Content" id="S4.T5.8.8.8.2.m1.1b"><cn id="S4.T5.8.8.8.2.m1.1.1.cmml" type="float" xref="S4.T5.8.8.8.2.m1.1.1">22.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.8.8.8.2.m1.1c">22.6</annotation><annotation encoding="application/x-llamapun" id="S4.T5.8.8.8.2.m1.1d">22.6</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.9.9.9.3"><math alttext="9.8" class="ltx_Math" display="inline" id="S4.T5.9.9.9.3.m1.1"><semantics id="S4.T5.9.9.9.3.m1.1a"><mn id="S4.T5.9.9.9.3.m1.1.1" xref="S4.T5.9.9.9.3.m1.1.1.cmml">9.8</mn><annotation-xml encoding="MathML-Content" id="S4.T5.9.9.9.3.m1.1b"><cn id="S4.T5.9.9.9.3.m1.1.1.cmml" type="float" xref="S4.T5.9.9.9.3.m1.1.1">9.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.9.9.9.3.m1.1c">9.8</annotation><annotation encoding="application/x-llamapun" id="S4.T5.9.9.9.3.m1.1d">9.8</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T5.11.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S4.T5.12.2" style="font-size:90%;">Number of frames per second processed by the algorithm with and without self-attention.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS3.p6">
<p class="ltx_p" id="S4.SS2.SSS3.p6.1">We conclude that the differences in computation time are negligible considering that, in practice, we have pre and post processing steps that will make the difference even tinier.
In our study, we cannot conclude that adding a self-attention layer is relevant. First, visual differences in results are imperceptible and the difference in computation time using one method or the other is negligible.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4 </span>Experiments using YOLOV5Face</h4>
<div class="ltx_para" id="S4.SS2.SSS4.p1">
<p class="ltx_p" id="S4.SS2.SSS4.p1.1">In this section, we evaluate the YOLO-based face blur method described in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S2" title="2 Face Blurring using YOLO ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">2</span></a>. We recall that the YOLOV5Face methodology consists in detecting faces and then blur them (see Section <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S2.SS3" title="2.3 Inference methodology ‣ 2 Face Blurring using YOLO ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">2.3</span></a>).
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS4.p2">
<p class="ltx_p" id="S4.SS2.SSS4.p2.3"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS4.p2.3.1">Visual evaluation
<br class="ltx_break"/></span>The YOLO method is very straightforward. We display in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.F12" title="Figure 12 ‣ 4.2.4 Experiments using YOLOV5Face ‣ 4.2 Experiments using DeOldify Unet ‣ 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">12</span></a> visual results for several input resizing (<math alttext="192\times 192" class="ltx_Math" display="inline" id="S4.SS2.SSS4.p2.1.m1.1"><semantics id="S4.SS2.SSS4.p2.1.m1.1a"><mrow id="S4.SS2.SSS4.p2.1.m1.1.1" xref="S4.SS2.SSS4.p2.1.m1.1.1.cmml"><mn id="S4.SS2.SSS4.p2.1.m1.1.1.2" xref="S4.SS2.SSS4.p2.1.m1.1.1.2.cmml">192</mn><mo id="S4.SS2.SSS4.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS4.p2.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.SSS4.p2.1.m1.1.1.3" xref="S4.SS2.SSS4.p2.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p2.1.m1.1b"><apply id="S4.SS2.SSS4.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS4.p2.1.m1.1.1"><times id="S4.SS2.SSS4.p2.1.m1.1.1.1.cmml" xref="S4.SS2.SSS4.p2.1.m1.1.1.1"></times><cn id="S4.SS2.SSS4.p2.1.m1.1.1.2.cmml" type="integer" xref="S4.SS2.SSS4.p2.1.m1.1.1.2">192</cn><cn id="S4.SS2.SSS4.p2.1.m1.1.1.3.cmml" type="integer" xref="S4.SS2.SSS4.p2.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p2.1.m1.1c">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS4.p2.1.m1.1d">192 × 192</annotation></semantics></math>, <math alttext="256\times 256" class="ltx_Math" display="inline" id="S4.SS2.SSS4.p2.2.m2.1"><semantics id="S4.SS2.SSS4.p2.2.m2.1a"><mrow id="S4.SS2.SSS4.p2.2.m2.1.1" xref="S4.SS2.SSS4.p2.2.m2.1.1.cmml"><mn id="S4.SS2.SSS4.p2.2.m2.1.1.2" xref="S4.SS2.SSS4.p2.2.m2.1.1.2.cmml">256</mn><mo id="S4.SS2.SSS4.p2.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS4.p2.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS2.SSS4.p2.2.m2.1.1.3" xref="S4.SS2.SSS4.p2.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p2.2.m2.1b"><apply id="S4.SS2.SSS4.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS4.p2.2.m2.1.1"><times id="S4.SS2.SSS4.p2.2.m2.1.1.1.cmml" xref="S4.SS2.SSS4.p2.2.m2.1.1.1"></times><cn id="S4.SS2.SSS4.p2.2.m2.1.1.2.cmml" type="integer" xref="S4.SS2.SSS4.p2.2.m2.1.1.2">256</cn><cn id="S4.SS2.SSS4.p2.2.m2.1.1.3.cmml" type="integer" xref="S4.SS2.SSS4.p2.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p2.2.m2.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS4.p2.2.m2.1d">256 × 256</annotation></semantics></math>, <math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.SS2.SSS4.p2.3.m3.1"><semantics id="S4.SS2.SSS4.p2.3.m3.1a"><mrow id="S4.SS2.SSS4.p2.3.m3.1.1" xref="S4.SS2.SSS4.p2.3.m3.1.1.cmml"><mn id="S4.SS2.SSS4.p2.3.m3.1.1.2" xref="S4.SS2.SSS4.p2.3.m3.1.1.2.cmml">512</mn><mo id="S4.SS2.SSS4.p2.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.SSS4.p2.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS2.SSS4.p2.3.m3.1.1.3" xref="S4.SS2.SSS4.p2.3.m3.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p2.3.m3.1b"><apply id="S4.SS2.SSS4.p2.3.m3.1.1.cmml" xref="S4.SS2.SSS4.p2.3.m3.1.1"><times id="S4.SS2.SSS4.p2.3.m3.1.1.1.cmml" xref="S4.SS2.SSS4.p2.3.m3.1.1.1"></times><cn id="S4.SS2.SSS4.p2.3.m3.1.1.2.cmml" type="integer" xref="S4.SS2.SSS4.p2.3.m3.1.1.2">512</cn><cn id="S4.SS2.SSS4.p2.3.m3.1.1.3.cmml" type="integer" xref="S4.SS2.SSS4.p2.3.m3.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p2.3.m3.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS4.p2.3.m3.1d">512 × 512</annotation></semantics></math>) as well as for no resizing (keeping original input size).</p>
</div>
<figure class="ltx_figure" id="S4.F12">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.F12.30">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.F12.5.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.1.1.1" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="75" id="S4.F12.1.1.1.g1" src="extracted/5872765/figures/test_set/original_images/image_0.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.2.2.2" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="75" id="S4.F12.2.2.2.g1" src="extracted/5872765/figures/test_set/yolo_192/image_0.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.3.3.3" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="75" id="S4.F12.3.3.3.g1" src="extracted/5872765/figures/test_set/yolo_256/image_0.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.4.4.4" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="75" id="S4.F12.4.4.4.g1" src="extracted/5872765/figures/test_set/yolo_512/image_0.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.5.5.5" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="75" id="S4.F12.5.5.5.g1" src="extracted/5872765/figures/test_set/yolo/image_0.jpg" width="120"/></td>
</tr>
<tr class="ltx_tr" id="S4.F12.10.10">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.6.6.1" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="121" id="S4.F12.6.6.1.g1" src="extracted/5872765/figures/test_set/original_images/image_25.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.7.7.2" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="121" id="S4.F12.7.7.2.g1" src="extracted/5872765/figures/test_set/yolo_192/image_25.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.8.8.3" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="121" id="S4.F12.8.8.3.g1" src="extracted/5872765/figures/test_set/yolo_256/image_25.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.9.9.4" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="121" id="S4.F12.9.9.4.g1" src="extracted/5872765/figures/test_set/yolo_512/image_25.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.10.10.5" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="121" id="S4.F12.10.10.5.g1" src="extracted/5872765/figures/test_set/yolo/image_25.jpg" width="120"/></td>
</tr>
<tr class="ltx_tr" id="S4.F12.15.15">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.11.11.1" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="68" id="S4.F12.11.11.1.g1" src="extracted/5872765/figures/test_set/original_images/image_1.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.12.12.2" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="68" id="S4.F12.12.12.2.g1" src="extracted/5872765/figures/test_set/yolo_192/image_1.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.13.13.3" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="68" id="S4.F12.13.13.3.g1" src="extracted/5872765/figures/test_set/yolo_256/image_1.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.14.14.4" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="68" id="S4.F12.14.14.4.g1" src="extracted/5872765/figures/test_set/yolo_512/image_1.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.15.15.5" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="68" id="S4.F12.15.15.5.g1" src="extracted/5872765/figures/test_set/yolo/image_1.jpg" width="120"/></td>
</tr>
<tr class="ltx_tr" id="S4.F12.20.20">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.16.16.1" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="90" id="S4.F12.16.16.1.g1" src="extracted/5872765/figures/test_set/original_images/image_16.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.17.17.2" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="90" id="S4.F12.17.17.2.g1" src="extracted/5872765/figures/test_set/yolo_192/image_16.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.18.18.3" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="90" id="S4.F12.18.18.3.g1" src="extracted/5872765/figures/test_set/yolo_256/image_16.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.19.19.4" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="90" id="S4.F12.19.19.4.g1" src="extracted/5872765/figures/test_set/yolo_512/image_16.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.20.20.5" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="90" id="S4.F12.20.20.5.g1" src="extracted/5872765/figures/test_set/yolo/image_16.jpg" width="120"/></td>
</tr>
<tr class="ltx_tr" id="S4.F12.25.25">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.21.21.1" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="80" id="S4.F12.21.21.1.g1" src="extracted/5872765/figures/test_set/original_images/image_57.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.22.22.2" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="80" id="S4.F12.22.22.2.g1" src="extracted/5872765/figures/test_set/yolo_192/image_57.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.23.23.3" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="80" id="S4.F12.23.23.3.g1" src="extracted/5872765/figures/test_set/yolo_256/image_57.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.24.24.4" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="80" id="S4.F12.24.24.4.g1" src="extracted/5872765/figures/test_set/yolo_512/image_57.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.25.25.5" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="80" id="S4.F12.25.25.5.g1" src="extracted/5872765/figures/test_set/yolo/image_57.jpg" width="120"/></td>
</tr>
<tr class="ltx_tr" id="S4.F12.30.30">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.26.26.1" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="80" id="S4.F12.26.26.1.g1" src="extracted/5872765/figures/test_set/original_images/image_24.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.27.27.2" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="80" id="S4.F12.27.27.2.g1" src="extracted/5872765/figures/test_set/yolo_192/image_24.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.28.28.3" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="80" id="S4.F12.28.28.3.g1" src="extracted/5872765/figures/test_set/yolo_256/image_24.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.29.29.4" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="80" id="S4.F12.29.29.4.g1" src="extracted/5872765/figures/test_set/yolo_512/image_24.jpg" width="120"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.F12.30.30.5" style="padding-left:0.1pt;padding-right:0.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="80" id="S4.F12.30.30.5.g1" src="extracted/5872765/figures/test_set/yolo/image_24.jpg" width="120"/></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F12.38.4.1" style="font-size:90%;">Figure 12</span>: </span><span class="ltx_text" id="S4.F12.36.3" style="font-size:90%;">Test set results. Original image (left), result for <math alttext="192\times 192" class="ltx_Math" display="inline" id="S4.F12.34.1.m1.1"><semantics id="S4.F12.34.1.m1.1b"><mrow id="S4.F12.34.1.m1.1.1" xref="S4.F12.34.1.m1.1.1.cmml"><mn id="S4.F12.34.1.m1.1.1.2" xref="S4.F12.34.1.m1.1.1.2.cmml">192</mn><mo id="S4.F12.34.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.F12.34.1.m1.1.1.1.cmml">×</mo><mn id="S4.F12.34.1.m1.1.1.3" xref="S4.F12.34.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F12.34.1.m1.1c"><apply id="S4.F12.34.1.m1.1.1.cmml" xref="S4.F12.34.1.m1.1.1"><times id="S4.F12.34.1.m1.1.1.1.cmml" xref="S4.F12.34.1.m1.1.1.1"></times><cn id="S4.F12.34.1.m1.1.1.2.cmml" type="integer" xref="S4.F12.34.1.m1.1.1.2">192</cn><cn id="S4.F12.34.1.m1.1.1.3.cmml" type="integer" xref="S4.F12.34.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F12.34.1.m1.1d">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.F12.34.1.m1.1e">192 × 192</annotation></semantics></math> inference dimension (center left), result for <math alttext="256\times 256" class="ltx_Math" display="inline" id="S4.F12.35.2.m2.1"><semantics id="S4.F12.35.2.m2.1b"><mrow id="S4.F12.35.2.m2.1.1" xref="S4.F12.35.2.m2.1.1.cmml"><mn id="S4.F12.35.2.m2.1.1.2" xref="S4.F12.35.2.m2.1.1.2.cmml">256</mn><mo id="S4.F12.35.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.F12.35.2.m2.1.1.1.cmml">×</mo><mn id="S4.F12.35.2.m2.1.1.3" xref="S4.F12.35.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F12.35.2.m2.1c"><apply id="S4.F12.35.2.m2.1.1.cmml" xref="S4.F12.35.2.m2.1.1"><times id="S4.F12.35.2.m2.1.1.1.cmml" xref="S4.F12.35.2.m2.1.1.1"></times><cn id="S4.F12.35.2.m2.1.1.2.cmml" type="integer" xref="S4.F12.35.2.m2.1.1.2">256</cn><cn id="S4.F12.35.2.m2.1.1.3.cmml" type="integer" xref="S4.F12.35.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F12.35.2.m2.1d">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S4.F12.35.2.m2.1e">256 × 256</annotation></semantics></math> inference dimension (center), result for <math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.F12.36.3.m3.1"><semantics id="S4.F12.36.3.m3.1b"><mrow id="S4.F12.36.3.m3.1.1" xref="S4.F12.36.3.m3.1.1.cmml"><mn id="S4.F12.36.3.m3.1.1.2" xref="S4.F12.36.3.m3.1.1.2.cmml">512</mn><mo id="S4.F12.36.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.F12.36.3.m3.1.1.1.cmml">×</mo><mn id="S4.F12.36.3.m3.1.1.3" xref="S4.F12.36.3.m3.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F12.36.3.m3.1c"><apply id="S4.F12.36.3.m3.1.1.cmml" xref="S4.F12.36.3.m3.1.1"><times id="S4.F12.36.3.m3.1.1.1.cmml" xref="S4.F12.36.3.m3.1.1.1"></times><cn id="S4.F12.36.3.m3.1.1.2.cmml" type="integer" xref="S4.F12.36.3.m3.1.1.2">512</cn><cn id="S4.F12.36.3.m3.1.1.3.cmml" type="integer" xref="S4.F12.36.3.m3.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F12.36.3.m3.1d">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.F12.36.3.m3.1e">512 × 512</annotation></semantics></math> inference dimension (center right), result for original input size (right).</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS4.p3">
<p class="ltx_p" id="S4.SS2.SSS4.p3.1">We conclude that the YOLO-based method is very efficient, faces are very often detected, especially when they are “big”, meaning that the face covers a great number of pixels. However, and as the Unet before, for small downsampling sizes, YOLO struggles to detect small faces in images 5 and 6.
<br class="ltx_break"/>But, the higher we set the input dimension size, the higher number of faces YOLOv5Face detects. When we keep the original dimension, almost no faces are missed except one in the last image. We sum up the results in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.T6" title="Table 6 ‣ 4.2.4 Experiments using YOLOV5Face ‣ 4.2 Experiments using DeOldify Unet ‣ 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T6.3" style="width:433.6pt;height:159.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(21.1pt,-7.8pt) scale(1.10805684343164,1.10805684343164) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T6.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.3.3.4.1">
<td class="ltx_td ltx_border_r" id="S4.T6.3.3.4.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.3.3.4.1.2" rowspan="2"><span class="ltx_text" id="S4.T6.3.3.4.1.2.1">Number of faces</span></td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" colspan="4" id="S4.T6.3.3.4.1.3">Number of faces blurred</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.3.3">
<td class="ltx_td ltx_border_r" id="S4.T6.3.3.3.4"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.1.1.1.1"><math alttext="192\times 192" class="ltx_Math" display="inline" id="S4.T6.1.1.1.1.m1.1"><semantics id="S4.T6.1.1.1.1.m1.1a"><mrow id="S4.T6.1.1.1.1.m1.1.1" xref="S4.T6.1.1.1.1.m1.1.1.cmml"><mn id="S4.T6.1.1.1.1.m1.1.1.2" xref="S4.T6.1.1.1.1.m1.1.1.2.cmml">192</mn><mo id="S4.T6.1.1.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T6.1.1.1.1.m1.1.1.1.cmml">×</mo><mn id="S4.T6.1.1.1.1.m1.1.1.3" xref="S4.T6.1.1.1.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.1.m1.1b"><apply id="S4.T6.1.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.1.m1.1.1"><times id="S4.T6.1.1.1.1.m1.1.1.1.cmml" xref="S4.T6.1.1.1.1.m1.1.1.1"></times><cn id="S4.T6.1.1.1.1.m1.1.1.2.cmml" type="integer" xref="S4.T6.1.1.1.1.m1.1.1.2">192</cn><cn id="S4.T6.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S4.T6.1.1.1.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.1.m1.1c">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.T6.1.1.1.1.m1.1d">192 × 192</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.2.2.2.2"><math alttext="256\times 256" class="ltx_Math" display="inline" id="S4.T6.2.2.2.2.m1.1"><semantics id="S4.T6.2.2.2.2.m1.1a"><mrow id="S4.T6.2.2.2.2.m1.1.1" xref="S4.T6.2.2.2.2.m1.1.1.cmml"><mn id="S4.T6.2.2.2.2.m1.1.1.2" xref="S4.T6.2.2.2.2.m1.1.1.2.cmml">256</mn><mo id="S4.T6.2.2.2.2.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T6.2.2.2.2.m1.1.1.1.cmml">×</mo><mn id="S4.T6.2.2.2.2.m1.1.1.3" xref="S4.T6.2.2.2.2.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.2.2.m1.1b"><apply id="S4.T6.2.2.2.2.m1.1.1.cmml" xref="S4.T6.2.2.2.2.m1.1.1"><times id="S4.T6.2.2.2.2.m1.1.1.1.cmml" xref="S4.T6.2.2.2.2.m1.1.1.1"></times><cn id="S4.T6.2.2.2.2.m1.1.1.2.cmml" type="integer" xref="S4.T6.2.2.2.2.m1.1.1.2">256</cn><cn id="S4.T6.2.2.2.2.m1.1.1.3.cmml" type="integer" xref="S4.T6.2.2.2.2.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.2.2.m1.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S4.T6.2.2.2.2.m1.1d">256 × 256</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.3.3.3.3"><math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.T6.3.3.3.3.m1.1"><semantics id="S4.T6.3.3.3.3.m1.1a"><mrow id="S4.T6.3.3.3.3.m1.1.1" xref="S4.T6.3.3.3.3.m1.1.1.cmml"><mn id="S4.T6.3.3.3.3.m1.1.1.2" xref="S4.T6.3.3.3.3.m1.1.1.2.cmml">512</mn><mo id="S4.T6.3.3.3.3.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T6.3.3.3.3.m1.1.1.1.cmml">×</mo><mn id="S4.T6.3.3.3.3.m1.1.1.3" xref="S4.T6.3.3.3.3.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.3.3.3.3.m1.1b"><apply id="S4.T6.3.3.3.3.m1.1.1.cmml" xref="S4.T6.3.3.3.3.m1.1.1"><times id="S4.T6.3.3.3.3.m1.1.1.1.cmml" xref="S4.T6.3.3.3.3.m1.1.1.1"></times><cn id="S4.T6.3.3.3.3.m1.1.1.2.cmml" type="integer" xref="S4.T6.3.3.3.3.m1.1.1.2">512</cn><cn id="S4.T6.3.3.3.3.m1.1.1.3.cmml" type="integer" xref="S4.T6.3.3.3.3.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.3.3.3.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.T6.3.3.3.3.m1.1d">512 × 512</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T6.3.3.3.5">Original input size</td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.3.5.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.3.5.2.1">image 1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.3.5.2.2">1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.3.5.2.3"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.5.2.3.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.3.5.2.4"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.5.2.4.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.3.5.2.5"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.5.2.5.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.3.5.2.6"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.5.2.6.1">1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.3.6.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.3.6.3.1">image 2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.3.6.3.2">1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.3.6.3.3"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.6.3.3.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.3.6.3.4"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.6.3.4.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.3.6.3.5"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.6.3.5.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.3.6.3.6"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.6.3.6.1">1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.3.7.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.3.7.4.1">image 3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.3.7.4.2">5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.3.7.4.3"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.7.4.3.1">5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.3.7.4.4"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.7.4.4.1">5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.3.7.4.5"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.7.4.5.1">5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.3.7.4.6"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.7.4.6.1">5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.3.8.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.3.8.5.1">image 4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.3.8.5.2">6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.3.8.5.3"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.8.5.3.1">6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.3.8.5.4"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.8.5.4.1">6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.3.8.5.5"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.8.5.5.1">6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.3.8.5.6"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.8.5.6.1">6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.3.9.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.3.9.6.1">image 5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.3.9.6.2">18</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.3.9.6.3">16</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.3.9.6.4">16</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.3.9.6.5"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.9.6.5.1">18</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T6.3.3.9.6.6"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.9.6.6.1">18</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.3.3.10.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.3.10.7.1">image 6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.3.10.7.2">51</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.3.10.7.3">40</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.3.10.7.4">43</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.3.10.7.5">49</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T6.3.3.10.7.6"><span class="ltx_text ltx_font_bold" id="S4.T6.3.3.10.7.6.1">50</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T6.5.1.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="S4.T6.6.2" style="font-size:90%;">Face counting results (the best results for each image is in bold).</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.5 </span>Computation time</h4>
<div class="ltx_para" id="S4.SS2.SSS5.p1">
<p class="ltx_p" id="S4.SS2.SSS5.p1.1">We conduct three experiments :</p>
<ul class="ltx_itemize" id="S4.I5">
<li class="ltx_item" id="S4.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I5.i1.p1">
<p class="ltx_p" id="S4.I5.i1.p1.1">One experiment assuming that input images have already been downsampled to the desired input size of the network (either 192 x 192, 256 x 256 or 512 x 512). In this case the downsampling, binary mask extraction and upsampling steps described in Figure 6 do not need to be performed.</p>
</div>
</li>
<li class="ltx_item" id="S4.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I5.i2.p1">
<p class="ltx_p" id="S4.I5.i2.p1.1">One experiment assuming that input images are of size <math alttext="1024\times 1024" class="ltx_Math" display="inline" id="S4.I5.i2.p1.1.m1.1"><semantics id="S4.I5.i2.p1.1.m1.1a"><mrow id="S4.I5.i2.p1.1.m1.1.1" xref="S4.I5.i2.p1.1.m1.1.1.cmml"><mn id="S4.I5.i2.p1.1.m1.1.1.2" xref="S4.I5.i2.p1.1.m1.1.1.2.cmml">1024</mn><mo id="S4.I5.i2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.I5.i2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.I5.i2.p1.1.m1.1.1.3" xref="S4.I5.i2.p1.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I5.i2.p1.1.m1.1b"><apply id="S4.I5.i2.p1.1.m1.1.1.cmml" xref="S4.I5.i2.p1.1.m1.1.1"><times id="S4.I5.i2.p1.1.m1.1.1.1.cmml" xref="S4.I5.i2.p1.1.m1.1.1.1"></times><cn id="S4.I5.i2.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.I5.i2.p1.1.m1.1.1.2">1024</cn><cn id="S4.I5.i2.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.I5.i2.p1.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I5.i2.p1.1.m1.1c">1024\times 1024</annotation><annotation encoding="application/x-llamapun" id="S4.I5.i2.p1.1.m1.1d">1024 × 1024</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I5.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I5.i3.p1">
<p class="ltx_p" id="S4.I5.i3.p1.1">One experiment assuming that input images are of size <math alttext="2048\times 2048" class="ltx_Math" display="inline" id="S4.I5.i3.p1.1.m1.1"><semantics id="S4.I5.i3.p1.1.m1.1a"><mrow id="S4.I5.i3.p1.1.m1.1.1" xref="S4.I5.i3.p1.1.m1.1.1.cmml"><mn id="S4.I5.i3.p1.1.m1.1.1.2" xref="S4.I5.i3.p1.1.m1.1.1.2.cmml">2048</mn><mo id="S4.I5.i3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.I5.i3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.I5.i3.p1.1.m1.1.1.3" xref="S4.I5.i3.p1.1.m1.1.1.3.cmml">2048</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I5.i3.p1.1.m1.1b"><apply id="S4.I5.i3.p1.1.m1.1.1.cmml" xref="S4.I5.i3.p1.1.m1.1.1"><times id="S4.I5.i3.p1.1.m1.1.1.1.cmml" xref="S4.I5.i3.p1.1.m1.1.1.1"></times><cn id="S4.I5.i3.p1.1.m1.1.1.2.cmml" type="integer" xref="S4.I5.i3.p1.1.m1.1.1.2">2048</cn><cn id="S4.I5.i3.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.I5.i3.p1.1.m1.1.1.3">2048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I5.i3.p1.1.m1.1c">2048\times 2048</annotation><annotation encoding="application/x-llamapun" id="S4.I5.i3.p1.1.m1.1d">2048 × 2048</annotation></semantics></math>.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S4.SS2.SSS5.p1.2">.</p>
</div>
<figure class="ltx_table" id="S4.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T7.21" style="width:433.6pt;height:74.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(6.9pt,-1.2pt) scale(1.03283790329296,1.03283790329296) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T7.21.21">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T7.5.5.5">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r" id="S4.T7.5.5.5.6"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T7.1.1.1.1"><math alttext="192\times 192" class="ltx_Math" display="inline" id="S4.T7.1.1.1.1.m1.1"><semantics id="S4.T7.1.1.1.1.m1.1a"><mrow id="S4.T7.1.1.1.1.m1.1.1" xref="S4.T7.1.1.1.1.m1.1.1.cmml"><mn id="S4.T7.1.1.1.1.m1.1.1.2" xref="S4.T7.1.1.1.1.m1.1.1.2.cmml">192</mn><mo id="S4.T7.1.1.1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T7.1.1.1.1.m1.1.1.1.cmml">×</mo><mn id="S4.T7.1.1.1.1.m1.1.1.3" xref="S4.T7.1.1.1.1.m1.1.1.3.cmml">192</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.1.1.1.1.m1.1b"><apply id="S4.T7.1.1.1.1.m1.1.1.cmml" xref="S4.T7.1.1.1.1.m1.1.1"><times id="S4.T7.1.1.1.1.m1.1.1.1.cmml" xref="S4.T7.1.1.1.1.m1.1.1.1"></times><cn id="S4.T7.1.1.1.1.m1.1.1.2.cmml" type="integer" xref="S4.T7.1.1.1.1.m1.1.1.2">192</cn><cn id="S4.T7.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S4.T7.1.1.1.1.m1.1.1.3">192</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.1.1.1.1.m1.1c">192\times 192</annotation><annotation encoding="application/x-llamapun" id="S4.T7.1.1.1.1.m1.1d">192 × 192</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T7.2.2.2.2"><math alttext="256\times 256" class="ltx_Math" display="inline" id="S4.T7.2.2.2.2.m1.1"><semantics id="S4.T7.2.2.2.2.m1.1a"><mrow id="S4.T7.2.2.2.2.m1.1.1" xref="S4.T7.2.2.2.2.m1.1.1.cmml"><mn id="S4.T7.2.2.2.2.m1.1.1.2" xref="S4.T7.2.2.2.2.m1.1.1.2.cmml">256</mn><mo id="S4.T7.2.2.2.2.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T7.2.2.2.2.m1.1.1.1.cmml">×</mo><mn id="S4.T7.2.2.2.2.m1.1.1.3" xref="S4.T7.2.2.2.2.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.2.2.2.2.m1.1b"><apply id="S4.T7.2.2.2.2.m1.1.1.cmml" xref="S4.T7.2.2.2.2.m1.1.1"><times id="S4.T7.2.2.2.2.m1.1.1.1.cmml" xref="S4.T7.2.2.2.2.m1.1.1.1"></times><cn id="S4.T7.2.2.2.2.m1.1.1.2.cmml" type="integer" xref="S4.T7.2.2.2.2.m1.1.1.2">256</cn><cn id="S4.T7.2.2.2.2.m1.1.1.3.cmml" type="integer" xref="S4.T7.2.2.2.2.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.2.2.2.2.m1.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="S4.T7.2.2.2.2.m1.1d">256 × 256</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T7.3.3.3.3"><math alttext="512\times 512" class="ltx_Math" display="inline" id="S4.T7.3.3.3.3.m1.1"><semantics id="S4.T7.3.3.3.3.m1.1a"><mrow id="S4.T7.3.3.3.3.m1.1.1" xref="S4.T7.3.3.3.3.m1.1.1.cmml"><mn id="S4.T7.3.3.3.3.m1.1.1.2" xref="S4.T7.3.3.3.3.m1.1.1.2.cmml">512</mn><mo id="S4.T7.3.3.3.3.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T7.3.3.3.3.m1.1.1.1.cmml">×</mo><mn id="S4.T7.3.3.3.3.m1.1.1.3" xref="S4.T7.3.3.3.3.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.3.3.3.3.m1.1b"><apply id="S4.T7.3.3.3.3.m1.1.1.cmml" xref="S4.T7.3.3.3.3.m1.1.1"><times id="S4.T7.3.3.3.3.m1.1.1.1.cmml" xref="S4.T7.3.3.3.3.m1.1.1.1"></times><cn id="S4.T7.3.3.3.3.m1.1.1.2.cmml" type="integer" xref="S4.T7.3.3.3.3.m1.1.1.2">512</cn><cn id="S4.T7.3.3.3.3.m1.1.1.3.cmml" type="integer" xref="S4.T7.3.3.3.3.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.3.3.3.3.m1.1c">512\times 512</annotation><annotation encoding="application/x-llamapun" id="S4.T7.3.3.3.3.m1.1d">512 × 512</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T7.4.4.4.4"><math alttext="1024\times 1024" class="ltx_Math" display="inline" id="S4.T7.4.4.4.4.m1.1"><semantics id="S4.T7.4.4.4.4.m1.1a"><mrow id="S4.T7.4.4.4.4.m1.1.1" xref="S4.T7.4.4.4.4.m1.1.1.cmml"><mn id="S4.T7.4.4.4.4.m1.1.1.2" xref="S4.T7.4.4.4.4.m1.1.1.2.cmml">1024</mn><mo id="S4.T7.4.4.4.4.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T7.4.4.4.4.m1.1.1.1.cmml">×</mo><mn id="S4.T7.4.4.4.4.m1.1.1.3" xref="S4.T7.4.4.4.4.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.4.4.4.4.m1.1b"><apply id="S4.T7.4.4.4.4.m1.1.1.cmml" xref="S4.T7.4.4.4.4.m1.1.1"><times id="S4.T7.4.4.4.4.m1.1.1.1.cmml" xref="S4.T7.4.4.4.4.m1.1.1.1"></times><cn id="S4.T7.4.4.4.4.m1.1.1.2.cmml" type="integer" xref="S4.T7.4.4.4.4.m1.1.1.2">1024</cn><cn id="S4.T7.4.4.4.4.m1.1.1.3.cmml" type="integer" xref="S4.T7.4.4.4.4.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.4.4.4.4.m1.1c">1024\times 1024</annotation><annotation encoding="application/x-llamapun" id="S4.T7.4.4.4.4.m1.1d">1024 × 1024</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T7.5.5.5.5"><math alttext="2048\times 2048" class="ltx_Math" display="inline" id="S4.T7.5.5.5.5.m1.1"><semantics id="S4.T7.5.5.5.5.m1.1a"><mrow id="S4.T7.5.5.5.5.m1.1.1" xref="S4.T7.5.5.5.5.m1.1.1.cmml"><mn id="S4.T7.5.5.5.5.m1.1.1.2" xref="S4.T7.5.5.5.5.m1.1.1.2.cmml">2048</mn><mo id="S4.T7.5.5.5.5.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T7.5.5.5.5.m1.1.1.1.cmml">×</mo><mn id="S4.T7.5.5.5.5.m1.1.1.3" xref="S4.T7.5.5.5.5.m1.1.1.3.cmml">2048</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.5.5.5.5.m1.1b"><apply id="S4.T7.5.5.5.5.m1.1.1.cmml" xref="S4.T7.5.5.5.5.m1.1.1"><times id="S4.T7.5.5.5.5.m1.1.1.1.cmml" xref="S4.T7.5.5.5.5.m1.1.1.1"></times><cn id="S4.T7.5.5.5.5.m1.1.1.2.cmml" type="integer" xref="S4.T7.5.5.5.5.m1.1.1.2">2048</cn><cn id="S4.T7.5.5.5.5.m1.1.1.3.cmml" type="integer" xref="S4.T7.5.5.5.5.m1.1.1.3">2048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.5.5.5.5.m1.1c">2048\times 2048</annotation><annotation encoding="application/x-llamapun" id="S4.T7.5.5.5.5.m1.1d">2048 × 2048</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T7.10.10.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T7.10.10.10.6">No resizing needed</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T7.6.6.6.1"><math alttext="52.5" class="ltx_Math" display="inline" id="S4.T7.6.6.6.1.m1.1"><semantics id="S4.T7.6.6.6.1.m1.1a"><mn id="S4.T7.6.6.6.1.m1.1.1" xref="S4.T7.6.6.6.1.m1.1.1.cmml">52.5</mn><annotation-xml encoding="MathML-Content" id="S4.T7.6.6.6.1.m1.1b"><cn id="S4.T7.6.6.6.1.m1.1.1.cmml" type="float" xref="S4.T7.6.6.6.1.m1.1.1">52.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.6.6.6.1.m1.1c">52.5</annotation><annotation encoding="application/x-llamapun" id="S4.T7.6.6.6.1.m1.1d">52.5</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T7.7.7.7.2"><math alttext="48.9" class="ltx_Math" display="inline" id="S4.T7.7.7.7.2.m1.1"><semantics id="S4.T7.7.7.7.2.m1.1a"><mn id="S4.T7.7.7.7.2.m1.1.1" xref="S4.T7.7.7.7.2.m1.1.1.cmml">48.9</mn><annotation-xml encoding="MathML-Content" id="S4.T7.7.7.7.2.m1.1b"><cn id="S4.T7.7.7.7.2.m1.1.1.cmml" type="float" xref="S4.T7.7.7.7.2.m1.1.1">48.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.7.7.7.2.m1.1c">48.9</annotation><annotation encoding="application/x-llamapun" id="S4.T7.7.7.7.2.m1.1d">48.9</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T7.8.8.8.3"><math alttext="26.7" class="ltx_Math" display="inline" id="S4.T7.8.8.8.3.m1.1"><semantics id="S4.T7.8.8.8.3.m1.1a"><mn id="S4.T7.8.8.8.3.m1.1.1" xref="S4.T7.8.8.8.3.m1.1.1.cmml">26.7</mn><annotation-xml encoding="MathML-Content" id="S4.T7.8.8.8.3.m1.1b"><cn id="S4.T7.8.8.8.3.m1.1.1.cmml" type="float" xref="S4.T7.8.8.8.3.m1.1.1">26.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.8.8.8.3.m1.1c">26.7</annotation><annotation encoding="application/x-llamapun" id="S4.T7.8.8.8.3.m1.1d">26.7</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T7.9.9.9.4"><math alttext="5.7" class="ltx_Math" display="inline" id="S4.T7.9.9.9.4.m1.1"><semantics id="S4.T7.9.9.9.4.m1.1a"><mn id="S4.T7.9.9.9.4.m1.1.1" xref="S4.T7.9.9.9.4.m1.1.1.cmml">5.7</mn><annotation-xml encoding="MathML-Content" id="S4.T7.9.9.9.4.m1.1b"><cn id="S4.T7.9.9.9.4.m1.1.1.cmml" type="float" xref="S4.T7.9.9.9.4.m1.1.1">5.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.9.9.9.4.m1.1c">5.7</annotation><annotation encoding="application/x-llamapun" id="S4.T7.9.9.9.4.m1.1d">5.7</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T7.10.10.10.5"><math alttext="1.9" class="ltx_Math" display="inline" id="S4.T7.10.10.10.5.m1.1"><semantics id="S4.T7.10.10.10.5.m1.1a"><mn id="S4.T7.10.10.10.5.m1.1.1" xref="S4.T7.10.10.10.5.m1.1.1.cmml">1.9</mn><annotation-xml encoding="MathML-Content" id="S4.T7.10.10.10.5.m1.1b"><cn id="S4.T7.10.10.10.5.m1.1.1.cmml" type="float" xref="S4.T7.10.10.10.5.m1.1.1">1.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.10.10.10.5.m1.1c">1.9</annotation><annotation encoding="application/x-llamapun" id="S4.T7.10.10.10.5.m1.1d">1.9</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T7.15.15.15">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T7.11.11.11.1">Input size of <math alttext="1024\times 1024" class="ltx_Math" display="inline" id="S4.T7.11.11.11.1.m1.1"><semantics id="S4.T7.11.11.11.1.m1.1a"><mrow id="S4.T7.11.11.11.1.m1.1.1" xref="S4.T7.11.11.11.1.m1.1.1.cmml"><mn id="S4.T7.11.11.11.1.m1.1.1.2" xref="S4.T7.11.11.11.1.m1.1.1.2.cmml">1024</mn><mo id="S4.T7.11.11.11.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T7.11.11.11.1.m1.1.1.1.cmml">×</mo><mn id="S4.T7.11.11.11.1.m1.1.1.3" xref="S4.T7.11.11.11.1.m1.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.11.11.11.1.m1.1b"><apply id="S4.T7.11.11.11.1.m1.1.1.cmml" xref="S4.T7.11.11.11.1.m1.1.1"><times id="S4.T7.11.11.11.1.m1.1.1.1.cmml" xref="S4.T7.11.11.11.1.m1.1.1.1"></times><cn id="S4.T7.11.11.11.1.m1.1.1.2.cmml" type="integer" xref="S4.T7.11.11.11.1.m1.1.1.2">1024</cn><cn id="S4.T7.11.11.11.1.m1.1.1.3.cmml" type="integer" xref="S4.T7.11.11.11.1.m1.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.11.11.11.1.m1.1c">1024\times 1024</annotation><annotation encoding="application/x-llamapun" id="S4.T7.11.11.11.1.m1.1d">1024 × 1024</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T7.12.12.12.2"><math alttext="7.2" class="ltx_Math" display="inline" id="S4.T7.12.12.12.2.m1.1"><semantics id="S4.T7.12.12.12.2.m1.1a"><mn id="S4.T7.12.12.12.2.m1.1.1" xref="S4.T7.12.12.12.2.m1.1.1.cmml">7.2</mn><annotation-xml encoding="MathML-Content" id="S4.T7.12.12.12.2.m1.1b"><cn id="S4.T7.12.12.12.2.m1.1.1.cmml" type="float" xref="S4.T7.12.12.12.2.m1.1.1">7.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.12.12.12.2.m1.1c">7.2</annotation><annotation encoding="application/x-llamapun" id="S4.T7.12.12.12.2.m1.1d">7.2</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T7.13.13.13.3"><math alttext="6.9" class="ltx_Math" display="inline" id="S4.T7.13.13.13.3.m1.1"><semantics id="S4.T7.13.13.13.3.m1.1a"><mn id="S4.T7.13.13.13.3.m1.1.1" xref="S4.T7.13.13.13.3.m1.1.1.cmml">6.9</mn><annotation-xml encoding="MathML-Content" id="S4.T7.13.13.13.3.m1.1b"><cn id="S4.T7.13.13.13.3.m1.1.1.cmml" type="float" xref="S4.T7.13.13.13.3.m1.1.1">6.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.13.13.13.3.m1.1c">6.9</annotation><annotation encoding="application/x-llamapun" id="S4.T7.13.13.13.3.m1.1d">6.9</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T7.14.14.14.4"><math alttext="6.3" class="ltx_Math" display="inline" id="S4.T7.14.14.14.4.m1.1"><semantics id="S4.T7.14.14.14.4.m1.1a"><mn id="S4.T7.14.14.14.4.m1.1.1" xref="S4.T7.14.14.14.4.m1.1.1.cmml">6.3</mn><annotation-xml encoding="MathML-Content" id="S4.T7.14.14.14.4.m1.1b"><cn id="S4.T7.14.14.14.4.m1.1.1.cmml" type="float" xref="S4.T7.14.14.14.4.m1.1.1">6.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.14.14.14.4.m1.1c">6.3</annotation><annotation encoding="application/x-llamapun" id="S4.T7.14.14.14.4.m1.1d">6.3</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T7.15.15.15.5"><math alttext="5.7" class="ltx_Math" display="inline" id="S4.T7.15.15.15.5.m1.1"><semantics id="S4.T7.15.15.15.5.m1.1a"><mn id="S4.T7.15.15.15.5.m1.1.1" xref="S4.T7.15.15.15.5.m1.1.1.cmml">5.7</mn><annotation-xml encoding="MathML-Content" id="S4.T7.15.15.15.5.m1.1b"><cn id="S4.T7.15.15.15.5.m1.1.1.cmml" type="float" xref="S4.T7.15.15.15.5.m1.1.1">5.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.15.15.15.5.m1.1c">5.7</annotation><annotation encoding="application/x-llamapun" id="S4.T7.15.15.15.5.m1.1d">5.7</annotation></semantics></math></td>
<td class="ltx_td ltx_border_t" id="S4.T7.15.15.15.6"></td>
</tr>
<tr class="ltx_tr" id="S4.T7.21.21.21">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T7.16.16.16.1">Input size of <math alttext="2048\times 2048" class="ltx_Math" display="inline" id="S4.T7.16.16.16.1.m1.1"><semantics id="S4.T7.16.16.16.1.m1.1a"><mrow id="S4.T7.16.16.16.1.m1.1.1" xref="S4.T7.16.16.16.1.m1.1.1.cmml"><mn id="S4.T7.16.16.16.1.m1.1.1.2" xref="S4.T7.16.16.16.1.m1.1.1.2.cmml">2048</mn><mo id="S4.T7.16.16.16.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.T7.16.16.16.1.m1.1.1.1.cmml">×</mo><mn id="S4.T7.16.16.16.1.m1.1.1.3" xref="S4.T7.16.16.16.1.m1.1.1.3.cmml">2048</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T7.16.16.16.1.m1.1b"><apply id="S4.T7.16.16.16.1.m1.1.1.cmml" xref="S4.T7.16.16.16.1.m1.1.1"><times id="S4.T7.16.16.16.1.m1.1.1.1.cmml" xref="S4.T7.16.16.16.1.m1.1.1.1"></times><cn id="S4.T7.16.16.16.1.m1.1.1.2.cmml" type="integer" xref="S4.T7.16.16.16.1.m1.1.1.2">2048</cn><cn id="S4.T7.16.16.16.1.m1.1.1.3.cmml" type="integer" xref="S4.T7.16.16.16.1.m1.1.1.3">2048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.16.16.16.1.m1.1c">2048\times 2048</annotation><annotation encoding="application/x-llamapun" id="S4.T7.16.16.16.1.m1.1d">2048 × 2048</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T7.17.17.17.2"><math alttext="4.4" class="ltx_Math" display="inline" id="S4.T7.17.17.17.2.m1.1"><semantics id="S4.T7.17.17.17.2.m1.1a"><mn id="S4.T7.17.17.17.2.m1.1.1" xref="S4.T7.17.17.17.2.m1.1.1.cmml">4.4</mn><annotation-xml encoding="MathML-Content" id="S4.T7.17.17.17.2.m1.1b"><cn id="S4.T7.17.17.17.2.m1.1.1.cmml" type="float" xref="S4.T7.17.17.17.2.m1.1.1">4.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.17.17.17.2.m1.1c">4.4</annotation><annotation encoding="application/x-llamapun" id="S4.T7.17.17.17.2.m1.1d">4.4</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T7.18.18.18.3"><math alttext="4.2" class="ltx_Math" display="inline" id="S4.T7.18.18.18.3.m1.1"><semantics id="S4.T7.18.18.18.3.m1.1a"><mn id="S4.T7.18.18.18.3.m1.1.1" xref="S4.T7.18.18.18.3.m1.1.1.cmml">4.2</mn><annotation-xml encoding="MathML-Content" id="S4.T7.18.18.18.3.m1.1b"><cn id="S4.T7.18.18.18.3.m1.1.1.cmml" type="float" xref="S4.T7.18.18.18.3.m1.1.1">4.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.18.18.18.3.m1.1c">4.2</annotation><annotation encoding="application/x-llamapun" id="S4.T7.18.18.18.3.m1.1d">4.2</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T7.19.19.19.4"><math alttext="2.4" class="ltx_Math" display="inline" id="S4.T7.19.19.19.4.m1.1"><semantics id="S4.T7.19.19.19.4.m1.1a"><mn id="S4.T7.19.19.19.4.m1.1.1" xref="S4.T7.19.19.19.4.m1.1.1.cmml">2.4</mn><annotation-xml encoding="MathML-Content" id="S4.T7.19.19.19.4.m1.1b"><cn id="S4.T7.19.19.19.4.m1.1.1.cmml" type="float" xref="S4.T7.19.19.19.4.m1.1.1">2.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.19.19.19.4.m1.1c">2.4</annotation><annotation encoding="application/x-llamapun" id="S4.T7.19.19.19.4.m1.1d">2.4</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T7.20.20.20.5"><math alttext="2.0" class="ltx_Math" display="inline" id="S4.T7.20.20.20.5.m1.1"><semantics id="S4.T7.20.20.20.5.m1.1a"><mn id="S4.T7.20.20.20.5.m1.1.1" xref="S4.T7.20.20.20.5.m1.1.1.cmml">2.0</mn><annotation-xml encoding="MathML-Content" id="S4.T7.20.20.20.5.m1.1b"><cn id="S4.T7.20.20.20.5.m1.1.1.cmml" type="float" xref="S4.T7.20.20.20.5.m1.1.1">2.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.20.20.20.5.m1.1c">2.0</annotation><annotation encoding="application/x-llamapun" id="S4.T7.20.20.20.5.m1.1d">2.0</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.21.21.21.6"><math alttext="1.9" class="ltx_Math" display="inline" id="S4.T7.21.21.21.6.m1.1"><semantics id="S4.T7.21.21.21.6.m1.1a"><mn id="S4.T7.21.21.21.6.m1.1.1" xref="S4.T7.21.21.21.6.m1.1.1.cmml">1.9</mn><annotation-xml encoding="MathML-Content" id="S4.T7.21.21.21.6.m1.1b"><cn id="S4.T7.21.21.21.6.m1.1.1.cmml" type="float" xref="S4.T7.21.21.21.6.m1.1.1">1.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.21.21.21.6.m1.1c">1.9</annotation><annotation encoding="application/x-llamapun" id="S4.T7.21.21.21.6.m1.1d">1.9</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T7.23.1.1" style="font-size:90%;">Table 7</span>: </span><span class="ltx_text" id="S4.T7.24.2" style="font-size:90%;">Number of frames per second processed by the algorithm in function of the downsampling size.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS5.p2">
<p class="ltx_p" id="S4.SS2.SSS5.p2.1">What stands out in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#S4.T7" title="Table 7 ‣ 4.2.5 Computation time ‣ 4.2 Experiments using DeOldify Unet ‣ 4 Experiments ‣ Two Deep Learning Solutions for Automatic Blurring of Faces in Videos"><span class="ltx_text ltx_ref_tag">7</span></a> is that the number of frames processed by second by the YOLOv5Face methodology is always higher than with the Unet based method. In fact, we use here the Nano model, which has the smaller number of parameters which speeds up the inference. We observe, as for the Unet-based method, that all the resizing steps greatly deteriorate the algorithm performance.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we have investigated the automatic blurring of faces in images and videos. We have presented two methods, one one-step direct method based on the Unet architecture (DeOldify <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib9" title="">9</a>]</cite>), and another two-step method which relies on the well-known YOLO object detector <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">The YOLO-based method first detects faces which are then blurred using a Gaussian kernel. The Unet-based method directly outputs images in which faces are blurred. We have constructed a dataset of pairs of original and face-blurred images to train this network. The original images come from two popular face-detection datasets (FDDB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib3" title="">3</a>]</cite> and WIDER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib10" title="">10</a>]</cite>).</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">The experiments show that both methods are able to correctly blur faces in images and that they are robust to variations in size and pose.
In terms of computation time, the YOLO-based method is faster, since it benefits from all the optimizations introduced in the YOLO architecture
to increase its speed. However, the ability of the Unet-based network to blur the faces without detecting them explicitly is
an interesting property that is worth exploring and, in the future, we will investigate how to optimize its speed.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">On Line Demo and Code</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">An online demo is available for the interested readers that want to test the performance of both methods in their own videos: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ipolcore.ipol.im/demo/clientApp/demo.html?id=77777000406" title="">https://ipolcore.ipol.im/demo/clientApp/demo.html?id=77777000406</a>.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">Moreover, the code used to obtain the results displayed in the previous sections is available here:
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/RomanPlaud/script-face-blurring-ipol" title="">https://github.com/RomanPlaud/script-face-blurring-ipol</a>.
The original source codes are borrowed from <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.ipol.im/pub/art/2022/403/" title="">http://www.ipol.im/pub/art/2022/403/</a>,
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/jantic/DeOldify" title="">https://github.com/jantic/DeOldify</a>
and <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/elyha7/yoloface" title="">https://github.com/elyha7/yoloface</a>.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgment</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">For the second author the publication is part of the project PID2021-125711OB-I00, financed by MCIN/AEI/10.13039/501100011033/FEDER, EU.</p>
</div>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Image Credits</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1"><span class="ltx_text" id="Sx3.p1.1.1" style="font-size:90%;">All the original images displayed in the paper come from the FDDB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib3" title="">3</a>]</cite> and WIDER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14828v1#bib.bib10" title="">10</a>]</cite> datasets.
</span></p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib1.1.1">A. Buades, B. Coll, and J.-M. Morel</span>, <span class="ltx_ERROR undefined" id="bib.bib1.2.2">\titlecap</span><span class="ltx_text ltx_font_italic" id="bib.bib1.3.3">A non-local algorithm
for image denoising</span>, in <span class="ltx_ERROR undefined" id="bib.bib1.4.4">\titlecap</span>IEEE computer society conference on
computer vision and pattern recognition, vol. 2, IEEE, 2005, pp. 60–65,
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/CVPR.2005.38" title="">https://doi.org/10.1109/CVPR.2005.38</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib2.1.1">K. He, X. Zhang, S. Ren, and J. Sun</span>, <span class="ltx_ERROR undefined" id="bib.bib2.2.2">\titlecap</span><span class="ltx_text ltx_font_italic" id="bib.bib2.3.3">Deep residual
learning for image recognition</span>, in <span class="ltx_ERROR undefined" id="bib.bib2.4.4">\titlecap</span>IEEE conference on computer
vision and pattern recognition, 2016, pp. 770–778,
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.1512.03385" title="">https://doi.org/10.48550/arXiv.1512.03385</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib3.1.1">V. Jain and E. Learned-Miller</span>, <span class="ltx_ERROR undefined" id="bib.bib3.2.2">\titlecap</span><span class="ltx_text ltx_font_italic" id="bib.bib3.3.3">FDDB: A Benchmark for Face
Detection in Unconstrained Settings</span>, Tech. Report UM-CS-2010-009,
University of Massachusetts, Amherst, 2010.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://vis-www.cs.umass.edu/fddb/" title="">https://vis-www.cs.umass.edu/fddb/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib4.1.1">J. Johnson, A. Alahi, and L. Fei-Fei</span>, <span class="ltx_ERROR undefined" id="bib.bib4.2.2">\titlecap</span><span class="ltx_text ltx_font_italic" id="bib.bib4.3.3">Perceptual losses
for real-time style transfer and super-resolution</span>, in <span class="ltx_ERROR undefined" id="bib.bib4.4.4">\titlecap</span>European
Conference in Computer Vision, Springer, 2016, pp. 694–711,
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-3-319-46475-6_43" title="">https://doi.org/10.1007/978-3-319-46475-6_43</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib5.1.1">D. Qi, W. Tan, Q. Yao, and J. Liu</span>, <span class="ltx_ERROR undefined" id="bib.bib5.2.2">\titlecap</span><span class="ltx_text ltx_font_italic" id="bib.bib5.3.3">YOLO5Face: Why
reinventing a face detector</span>, in <span class="ltx_ERROR undefined" id="bib.bib5.4.4">\titlecap</span>European Conference on Computer
Vision, Springer, 2022, pp. 228–244,
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-3-031-25072-9_15" title="">https://doi.org/10.1007/978-3-031-25072-9_15</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib6.1.1">J. Redmon, S. Divvala, R. Girshick, and A. Farhadi</span>, <span class="ltx_ERROR undefined" id="bib.bib6.2.2">\titlecap</span><span class="ltx_text ltx_font_italic" id="bib.bib6.3.3">You
only look once: Unified, real-time object detection</span>, in <span class="ltx_ERROR undefined" id="bib.bib6.4.4">\titlecap</span>IEEE
conference on computer vision and pattern recognition, 2016, pp. 779–788,
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.1506.02640" title="">https://doi.org/10.48550/arXiv.1506.02640</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib7.1.1">O. Ronneberger, P. Fischer, and T. Brox</span>, <span class="ltx_ERROR undefined" id="bib.bib7.2.2">\titlecap</span><span class="ltx_text ltx_font_italic" id="bib.bib7.3.3">U-net:
Convolutional networks for biomedical image segmentation</span>, in
<span class="ltx_ERROR undefined" id="bib.bib7.4.4">\titlecap</span>International Conference on Medical image computing and
computer-assisted intervention, Springer, 2015, pp. 234–241,
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-3-319-24574-4_28" title="">https://doi.org/10.1007/978-3-319-24574-4_28</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib8.1.1">O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein, et al.</span>, <span class="ltx_ERROR undefined" id="bib.bib8.2.2">\titlecap</span><span class="ltx_text ltx_font_italic" id="bib.bib8.3.3">Imagenet large
scale visual recognition challenge</span>, <span class="ltx_ERROR undefined" id="bib.bib8.4.4">\titlecap</span>International journal of
computer vision, 115 (2015), pp. 211–252,
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s11263-015-0816-y" title="">https://doi.org/10.1007/s11263-015-0816-y</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib9.1.1">A. Salmona, L. Bouza, and J. Delon</span>, <span class="ltx_ERROR undefined" id="bib.bib9.2.2">\titlecap</span><span class="ltx_text ltx_font_italic" id="bib.bib9.3.3">DeOldify: A Review and
Implementation of an Automatic Colorization Method</span>, <span class="ltx_ERROR undefined" id="bib.bib9.4.4">\titlecap</span>Image
Processing On Line, 12 (2022), pp. 347–368,
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.5201/ipol.2022.403" title="">https://doi.org/10.5201/ipol.2022.403</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib10.1.1">S. Yang, P. Luo, C. C. Loy, and X. Tang</span>, <span class="ltx_ERROR undefined" id="bib.bib10.2.2">\titlecap</span><span class="ltx_text ltx_font_italic" id="bib.bib10.3.3">WIDER FACE: A
Face Detection Benchmark</span>, in <span class="ltx_ERROR undefined" id="bib.bib10.4.4">\titlecap</span>IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2016,
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.1511.06523" title="">https://doi.org/10.48550/arXiv.1511.06523</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
<span class="ltx_text ltx_font_smallcaps" id="bib.bib11.1.1">X. Zhu, S. Lyu, X. Wang, and Q. Zhao</span>, <span class="ltx_ERROR undefined" id="bib.bib11.2.2">\titlecap</span><span class="ltx_text ltx_font_italic" id="bib.bib11.3.3">TPH-YOLOv5: Improved
YOLOv5 based on transformer prediction head for object detection on
drone-captured scenarios</span>, in <span class="ltx_ERROR undefined" id="bib.bib11.4.4">\titlecap</span>IEEE/CVF international conference on
computer vision, 2021, pp. 2778–2788,
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICCVW54120.2021.00312" title="">https://doi.org/10.1109/ICCVW54120.2021.00312</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 23 08:55:03 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
