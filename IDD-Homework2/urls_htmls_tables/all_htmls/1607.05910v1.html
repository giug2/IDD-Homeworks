<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1607.05910] Visual Question Answering: A Survey of Methods and Datasets</title><meta property="og:description" content="Visual Question Answering (VQA) is a challenging task that has received increasing attention from both the computer vision and the natural language processing communities. Given an image and a question in natural langu…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Visual Question Answering: A Survey of Methods and Datasets">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Visual Question Answering: A Survey of Methods and Datasets">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1607.05910">

<!--Generated on Sun Mar  3 12:06:07 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_fleqn">
<h1 class="ltx_title ltx_title_document">Visual Question Answering: A Survey of Methods and Datasets</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qi Wu
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Damien Teney
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Peng Wang
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chunhua Shen
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anthony Dick
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anton van den Hengel
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">e-mail: firstname.lastname@adelaide.edu.au 
<br class="ltx_break">School of Computer Science, The University of
Adelaide, SA 5005, Australia
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Visual Question Answering (VQA) is a challenging task that has received increasing attention from both the computer vision and the natural language processing communities. Given an image and a question in natural language, it requires reasoning over visual elements of the image and general knowledge to infer the correct answer. In the first part of this survey, we examine the state of the art by comparing modern approaches to the problem. We classify methods by their mechanism to connect the visual and textual modalities. In particular, we examine the common approach of combining convolutional and recurrent neural networks to map images and questions to a common feature space. We also discuss memory-augmented and modular architectures that interface with structured knowledge bases. In the second part of this survey, we review the datasets available for training and evaluating VQA systems. The various datatsets contain questions at different levels of complexity, which require different capabilities and types of reasoning. We examine in depth the question/answer pairs from the Visual Genome project, and evaluate the relevance of the structured annotations of images with scene graphs for VQA. Finally, we discuss promising future directions for the field, in particular the connection to structured knowledge bases and the use of natural language processing models.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Visual Question Answering , Natural Language Processing , Knowledge Bases , Recurrent Neural Networks

</div>
<nav class="ltx_TOC ltx_list_toc ltx_toc_toc"><h6 class="ltx_title ltx_title_contents">Contents</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S1" title="In Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S2" title="In Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methods for VQA</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS1" title="In 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Joint embedding approaches</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS2" title="In 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Attention mechanisms</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#S2.SS3" title="In 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Compositional Models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S2.SS3.SSS1" title="In 2.3 Compositional Models ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.1 </span>Neural Module Networks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S2.SS3.SSS2" title="In 2.3 Compositional Models ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.2 </span>Dynamic Memory Networks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS4" title="In 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Models using external knowledge bases</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S3" title="In Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Datasets and evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS1" title="In 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Datasets of natural images</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS2" title="In 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Datasets of clipart images</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS3" title="In 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Knowledge base-enhanced datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS4" title="In 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Other datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S4" title="In Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Structured scene annotations for VQA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S5" title="In Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion and future directions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S6" title="In Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual question answering is a task that was proposed to connect computer vision and natural language processing (NLP), to stimulate research, and push the boundaries of both fields. On the one hand, computer vision studies methods for acquiring, processing, and understanding images. In short, its aim is to teach machines <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">how to see</span>. On the the other hand, NLP is the field concerned with enabling interactions between computers and humans in natural language, <em id="S1.p1.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p1.1.3" class="ltx_text"></span> teaching machines <span id="S1.p1.1.4" class="ltx_text ltx_font_italic">how to read</span>, among other tasks. Both computer vision and NLP belong to the domain of artificial intelligence and they share similar methods rooted in machine learning. However, they have historically developed separately. Both fields have seen significant advances towards their respective goals in the past few decades, and the combined explosive growth of visual and textual data is pushing towards a marriage of efforts from both fields. For example, research in image captioning, <em id="S1.p1.1.5" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p1.1.6" class="ltx_text"></span> automatic image description <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib93" title="" class="ltx_ref">93</a>, <a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> has produced powerful methods for jointly learning from image and text inputs to form higher-level representations. A successful approach is to combine convolutional neural networks (CNNs), trained on object recognition, with word embeddings, trained on large text corpora.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In the most common form of Visual Question Answering (VQA), the computer is presented with an image and a textual question about this image (see examples in Figures <a href="#S3.T3" title="Table 3 ‣ Visual Genome and Visual7W ‣ 3.1 Datasets of natural images ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>–<a href="#S3.T5" title="Table 5 ‣ Visual Genome and Visual7W ‣ 3.1 Datasets of natural images ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). It must then determine the correct answer, typically a few words or a short phrase. Variants include binary (yes/no) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite> and multiple-choice settings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite>, in which candidate answers are proposed. A closely related task is to “fill in the blank” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>, where an affirmation describing the image must be completed with one or several missing words. These affirmations essentially amount to questions phrased in declarative form. A major distinction between VQA and other tasks in computer vision is that the question to be answered is not determined until run time. In traditional problems such as segmentation or object detection, the single question to be answered by an algorithm is predetermined and only the input image changes. In VQA, in contrast, the form that the question will take is unknown, as is the set of operations required to answer it. In this sense, it more closely reflects the challenge of general image understanding. VQA is related to the task of textual question answering, in which the answer is to be found in a specific textual narrative (<em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p2.1.2" class="ltx_text"></span> reading comprehension) or in large knowledge bases (<em id="S1.p2.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p2.1.4" class="ltx_text"></span> information retrieval). Textual QA has been studied for a long time in the NLP community, and VQA is its extension to additional visual supporting information. The added challenge is significant, as images are much higher dimensional, and typically more noisy than pure text. Moreover, images lack the structure and grammatical rules of language, and there is no direct equivalent to the NLP tools such as syntactic parsers and regular expression matching. Finally, images capture more of the richness of the real world, whereas natural language already represents a higher level of abstraction. For example, compare the phrase ‘<span id="S1.p2.1.5" class="ltx_text ltx_font_typewriter">a red hat</span>’ with the multitude of its representations that one can picture, and in which many styles could not be described in a short sentence.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Visual question answering is a significantly more complex problem than image captioning, as it frequently requires information not present in the image. The type of this extra required information may range from common sense to encyclopedic knowledge about a specific element from the image. In this respect, VQA constitutes a truly AI-complete task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, as it requires multimodal knowledge beyond a single sub-domain. This comforts the increased interest in VQA, as it provides a proxy to evaluate our progress towards AI systems capable of advanced reasoning combined with deep language and image understanding. Note that image understanding could in principle be evaluated equally well through image captioning. Practically however, VQA has the advantage of an easier evaluation metric. Answers typically contain only a few words. The long ground truth image captions are more difficult to compare with predicted ones. Although advanced evaluation metrics have been studied, this is still an open research problem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">One of the first integrations of vision and language is the “SHRDLU” from system from 1972 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> which allowed users to use language to instruct a computer to move various objects around in a “blocks world”. More recent attempts at creating conversational robotic agents <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> are also grounded in the visual world. However, these works were often limited to specific domains and/or on restricted language forms. In comparison, VQA specifically addresses free-form open-ended questions. The increasing interest in VQA is driven by the existence of mature techniques in both computer vision and NLP and the availability of relevant large-scale datasets. Therefore, a large body of literature on VQA has appeared over the last few years. The aim of this survey is to give a comprehensive overview of the field, covering models, datasets, and to suggest promising future directions. To the best of our knowledge, this article is the first survey in the field of VQA.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In the first part of this survey (Section <a href="#S2" title="2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), we present a comprehensive review of VQA methods through four categories based on the nature of their main contribution. Incremental contributions means that most methods belong to multiple of these categories (see Table <a href="#S2.T1" title="Table 1 ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). First, the <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">joint embedding approaches</span> (Section <a href="#S2.SS1" title="2.1 Joint embedding approaches ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>) are motivated by the advances of deep neural networks in both computer vision and NLP. They use convolutional and recurrent neural networks (CNNs and RNNs) to learn embeddings of images and sentences in a common feature space. This allows one to subsequently feed them together to a classifier that predicts an answer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. Second, <span id="S1.p5.1.2" class="ltx_text ltx_font_italic">attention mechanisms</span> (Section <a href="#S2.SS2" title="2.2 Attention mechanisms ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>) improve on the above method by focusing on specific parts of the input (image and/or question). Attention in VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>, <a href="#bib.bib90" title="" class="ltx_ref">90</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite> was inspired by the success of similar techniques in the context of image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>. The main idea is to replace holistic (image-wide) features with spatial feature maps, and to allow interactions between the question and specific regions of these maps. Third, <span id="S1.p5.1.3" class="ltx_text ltx_font_italic">compositional models</span> (Section <a href="#S2.SS3" title="2.3 Compositional Models ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>) allow to tailor the performed computations to each problem instance. For example, Andreas <em id="S1.p5.1.4" class="ltx_emph ltx_font_italic">et al</em>.<span id="S1.p5.1.5" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> use a parser to decompose a given question, then build a neural network out of modules whose composition reflect the structure of the question. Fourth, <span id="S1.p5.1.6" class="ltx_text ltx_font_italic">knowledge base-enhanced approaches</span> (Section <a href="#S2.SS4" title="2.4 Models using external knowledge bases ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a>) address the use of external data by querying structured knowledge bases. This allows retrieving information that is not present in the common visual datasets such as ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> or COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, which are only labeled with classes, bounding boxes, and/or captions. Information available from knowledge bases ranges from common sense to encyclopedic level, and can be accessed with no need for being available at training time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In the second part of this survey (Section <a href="#S3" title="3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), we examine datasets available for training and evaluating VQA systems. These datasets vary widely along three dimensions: (i) their size, <em id="S1.p6.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S1.p6.1.2" class="ltx_text"></span> the number of images, questions, and different concepts represented. (ii) the amount of required reasoning, <em id="S1.p6.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p6.1.4" class="ltx_text"></span> whether the detection of a single object is sufficient or whether inference is required over multiple facts or concepts, and (iii) how much information beyond that present in the actual images is necessary, be it common sense or subject-specific information. Our review points out that existing datasets lean towards visual-level questions, and require little external knowledge, with few exceptions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>. These characteristics reflect the struggle with simple visual questions still faced by the current state of the art, but these characteristics must not be forgotten when VQA is presented as an AI-complete evaluation proxy. We conclude that more varied and sophisticated datasets will eventually be required.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Another significant contribution of this survey is an in-depth analysis of the question/answer pairs provided in the Visual Genome dataset (Section <a href="#S4" title="4 Structured scene annotations for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). They constitute the largest VQA dataset available at the time of this writing, and, importantly, it includes rich structured images annotations in the form of scene graphs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. We evaluate the relevance of these annotations for VQA, by comparing the occurrence of concepts involved in the provided questions, answers, and image annotations. We find out that only about 40% of the answers directly match elements in the scene graphs. We further show that this matching rate can be significantly increased by relating scene graphs to external knowledge bases. We conclude this paper in Section <a href="#S5" title="5 Discussion and future directions ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> by discussing the potential of better connection to such knowledge bases, together with better use of existing work from the field of NLP.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods for VQA</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">One of the first attempts at “open-world” visual question answering was proposed by Malinowski <em id="S2.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. They described a method combining semantic text parsing with image segmentation in a Bayesian formulation that samples from nearest neighbors in the training set. The method requires human-defined predicates, which are inevitably dataset-specific and difficult to scale. It is also very dependent on the accuracy of the image segmentation algorithm and of the estimated image depth information. Another early attempt at VQA by Tu <em id="S2.p1.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> was based on a joint parse graph from text and videos. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, Geman <em id="S2.p1.1.5" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.6" class="ltx_text"></span> proposed an automatic “query generator” that is trained on annotated images and then produces a sequence of binary questions from any given test image. A common characteristic of these early approaches is to restrict questions to predefined forms. The remainder of this article focuses on modern approaches aimed at answering free-form open-ended questions. We will present methods through four categories: joint embedding approaches, attention mechanisms, compositional models, and knowledge base-enhanced approaches. As summarized in Table <a href="#S2.T1" title="Table 1 ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, most methods combine multiple strategies and thus belong to several categories.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<div id="S2.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:273.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-239.1pt,151.0pt) scale(0.47553207096157,0.47553207096157) ;">
<table id="S2.T1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span></td>
<td id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Joint</td>
<td id="S2.T1.1.1.1.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Attention</td>
<td id="S2.T1.1.1.1.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Compositional</td>
<td id="S2.T1.1.1.1.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Knowledge</td>
<td id="S2.T1.1.1.1.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Answer</td>
<td id="S2.T1.1.1.1.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Image</td>
</tr>
<tr id="S2.T1.1.1.2" class="ltx_tr">
<td id="S2.T1.1.1.2.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Method</td>
<td id="S2.T1.1.1.2.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">embedding</td>
<td id="S2.T1.1.1.2.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">mechanism</td>
<td id="S2.T1.1.1.2.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">model</td>
<td id="S2.T1.1.1.2.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">base</td>
<td id="S2.T1.1.1.2.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">class. / gen.</td>
<td id="S2.T1.1.1.2.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">features</td>
</tr>
<tr id="S2.T1.1.1.3" class="ltx_tr">
<td id="S2.T1.1.1.3.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span>
Neural-Image-QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>
</td>
<td id="S2.T1.1.1.3.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.3.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.3.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.3.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.3.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">generation</td>
<td id="S2.T1.1.1.3.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">GoogLeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.4" class="ltx_tr">
<td id="S2.T1.1.1.4.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">VIS+LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>
</td>
<td id="S2.T1.1.1.4.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.4.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.4.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.4.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.4.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.4.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">VGG-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.5" class="ltx_tr">
<td id="S2.T1.1.1.5.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Multimodal QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</td>
<td id="S2.T1.1.1.5.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.5.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.5.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.5.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.5.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">generation</td>
<td id="S2.T1.1.1.5.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">GoogLeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.6" class="ltx_tr">
<td id="S2.T1.1.1.6.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">DPPnet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>
</td>
<td id="S2.T1.1.1.6.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.6.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.6.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.6.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.6.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.6.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">VGG-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.7" class="ltx_tr">
<td id="S2.T1.1.1.7.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">MCB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</td>
<td id="S2.T1.1.1.7.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.7.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.7.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.7.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.7.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.7.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.8" class="ltx_tr">
<td id="S2.T1.1.1.8.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">MCB-Att <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</td>
<td id="S2.T1.1.1.8.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.8.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.8.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.8.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.8.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.8.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.9" class="ltx_tr">
<td id="S2.T1.1.1.9.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">MRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S2.T1.1.1.9.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.9.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.9.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.9.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.9.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.9.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.10" class="ltx_tr">
<td id="S2.T1.1.1.10.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Multimodal-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S2.T1.1.1.10.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.10.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.10.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.10.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.10.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.10.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">VGG-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.11" class="ltx_tr">
<td id="S2.T1.1.1.11.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">iBOWING <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>
</td>
<td id="S2.T1.1.1.11.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.11.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.11.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.11.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.11.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.11.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">GoogLeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.12" class="ltx_tr">
<td id="S2.T1.1.1.12.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">VQA team <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</td>
<td id="S2.T1.1.1.12.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.12.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.12.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.12.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.12.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.12.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">VGG-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.13" class="ltx_tr">
<td id="S2.T1.1.1.13.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Bayesian <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S2.T1.1.1.13.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.13.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.13.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.13.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.13.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.13.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.14" class="ltx_tr">
<td id="S2.T1.1.1.14.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">DualNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>
</td>
<td id="S2.T1.1.1.14.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.14.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.14.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.14.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.14.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.14.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">VGG-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> &amp; ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.15" class="ltx_tr">
<td id="S2.T1.1.1.15.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">MLP-AQI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="S2.T1.1.1.15.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.15.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.15.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.15.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.15.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.15.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.16" class="ltx_tr">
<td id="S2.T1.1.1.16.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">LSTM-Att <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite>
</td>
<td id="S2.T1.1.1.16.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.16.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.16.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.16.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.16.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.16.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">VGG-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.17" class="ltx_tr">
<td id="S2.T1.1.1.17.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Com-Mem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</td>
<td id="S2.T1.1.1.17.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.17.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.17.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.17.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.17.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">generation</td>
<td id="S2.T1.1.1.17.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">VGG-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.18" class="ltx_tr">
<td id="S2.T1.1.1.18.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">QAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S2.T1.1.1.18.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.18.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.18.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.18.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.18.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.18.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">VGG-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.19" class="ltx_tr">
<td id="S2.T1.1.1.19.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">SAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>
</td>
<td id="S2.T1.1.1.19.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.19.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.19.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.19.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.19.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.19.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">GoogLeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.20" class="ltx_tr">
<td id="S2.T1.1.1.20.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">SMem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>
</td>
<td id="S2.T1.1.1.20.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.20.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.20.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.20.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.20.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.20.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">GoogLeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.21" class="ltx_tr">
<td id="S2.T1.1.1.21.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Region-Sel <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>
</td>
<td id="S2.T1.1.1.21.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.21.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.21.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.21.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.21.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.21.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">VGG-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.22" class="ltx_tr">
<td id="S2.T1.1.1.22.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">FDA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S2.T1.1.1.22.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.22.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.22.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.22.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.22.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.22.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.23" class="ltx_tr">
<td id="S2.T1.1.1.23.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">HieCoAtt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
<td id="S2.T1.1.1.23.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.23.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.23.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.23.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.23.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.23.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.24" class="ltx_tr">
<td id="S2.T1.1.1.24.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">NMN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</td>
<td id="S2.T1.1.1.24.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.24.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.24.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.24.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.24.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.24.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">VGG-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.25" class="ltx_tr">
<td id="S2.T1.1.1.25.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">DMN+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>
</td>
<td id="S2.T1.1.1.25.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.25.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.25.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.25.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.25.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.25.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">VGG-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.26" class="ltx_tr">
<td id="S2.T1.1.1.26.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Joint-Loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>
</td>
<td id="S2.T1.1.1.26.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.26.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.26.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.26.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.26.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">classification</td>
<td id="S2.T1.1.1.26.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.27" class="ltx_tr">
<td id="S2.T1.1.1.27.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Attributes-LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>
</td>
<td id="S2.T1.1.1.27.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.27.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.27.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.27.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.27.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">generation</td>
<td id="S2.T1.1.1.27.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">VGG-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.28" class="ltx_tr">
<td id="S2.T1.1.1.28.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">ACK <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>
</td>
<td id="S2.T1.1.1.28.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.28.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.28.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.28.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.28.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">generation</td>
<td id="S2.T1.1.1.28.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">VGG-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.29" class="ltx_tr">
<td id="S2.T1.1.1.29.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Ahab <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>
</td>
<td id="S2.T1.1.1.29.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.29.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.29.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.29.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.29.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">generation</td>
<td id="S2.T1.1.1.29.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">VGG-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.30" class="ltx_tr">
<td id="S2.T1.1.1.30.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Facts-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>
</td>
<td id="S2.T1.1.1.30.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.30.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.30.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.30.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.30.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">generation</td>
<td id="S2.T1.1.1.30.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">VGG-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.31" class="ltx_tr">
<td id="S2.T1.1.1.31.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Multimodal KB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite>
</td>
<td id="S2.T1.1.1.31.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.31.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.31.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.31.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">✓</td>
<td id="S2.T1.1.1.31.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">generation</td>
<td id="S2.T1.1.1.31.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">ZeilerNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite>
</td>
</tr>
<tr id="S2.T1.1.1.32" class="ltx_tr">
<td id="S2.T1.1.1.32.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span></td>
<td id="S2.T1.1.1.32.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.32.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.32.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.32.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.32.6" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S2.T1.1.1.32.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Overview of existing approaches to VQA, characterized by the use of a joint embedding of image and language features (Section <a href="#S2.SS1" title="2.1 Joint embedding approaches ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>), the use of an attention mechanism (Section <a href="#S2.SS2" title="2.2 Attention mechanisms ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>), an explicitly compositional neural network architecture (Section <a href="#S2.SS3" title="2.3 Compositional Models ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>), and the use of information from an external structured knowledge base (Section <a href="#S2.SS4" title="2.4 Models using external knowledge bases ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a>). We also note whether the output answer is obtained by classification over a predefined set of common words and short phrases, or generated, typically with a recurrent neural network. The last column indicates the type of convolutional network used to obtain image feature.</figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_font_italic ltx_title_subsection">2.1   Joint embedding approaches</h3>

<section id="S2.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Motivation</h5>

<div id="S2.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px1.p1.1" class="ltx_p">The concept of jointly embedding images and text was first explored for the task of image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib93" title="" class="ltx_ref">93</a>, <a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>. It was motivated by the success of deep learning methods in both computer vision and NLP, which allow one to learn representations in a common feature space. In comparison to the task of image captioning, this motive is further reinforced in VQA by the need to perform further reasoning over both modalities together. A representation in a common space allows learning interactions and performing inference over the question and the image contents. Practically, image representations are obtained with convolutional neural networks (CNNs) pre-trained on object recognition. Text representations are obtained with word embeddings pre-trained on large text corpora. Word embeddings practically map words to a space in which distances reflect semantic similarities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>. The embeddings of the individual words of a question are then typically fed to a recurrent neural network to capture syntactic patterns and handle variable-length sequences.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/1607.05910/assets/x1.png" id="S2.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="451" height="189" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S2.F1.8.1" class="ltx_text ltx_font_italic">(Top)</span> A common approach to VQA is to map both the input image and question to a common embedding space (Section <a href="#S2.SS1" title="2.1 Joint embedding approaches ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>). These features are produced by deep convolutional and recurrent neural networks. They are combined in an output stage, which can take the form of a classifier (<em id="S2.F1.9.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.F1.10.3" class="ltx_text"></span> a multilayer perceptron) to predict short answers from predefined set or a recurrent network (<em id="S2.F1.11.4" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.F1.12.5" class="ltx_text"></span> an LSTM) to produce variable-length phrases. <span id="S2.F1.13.6" class="ltx_text ltx_font_italic">(Bottom)</span> Attention mechanisms build up on this basic approach with a spatial selection of image features. Attention weights are derived from both the image and the question and allow the output stage to focus on relevant parts of the image.</figcaption>
</figure>
</section>
<section id="S2.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Methods</h5>

<div id="S2.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px2.p1.1" class="ltx_p">Malinowski <em id="S2.SS1.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS1.SSS0.Px2.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> propose an approach named “Neural-Image-QA” with a Recurrent Neural Network (RNN) implemented with Long Short-Term Memory cells (LSTMs) (Figure <a href="#S2.F1" title="Figure 1 ‣ Motivation ‣ 2.1 Joint embedding approaches ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). The motivation behind RNNs is to handle inputs (questions) and outputs (answers) of variable size. Image features are produced by a CNN pre-trained for object recognition. Question and image features are both fed together to a first “encoder” LSTM. It produces a feature vector of fixed-size that is then passed to a second “decoder” LSTM. The decoder produces variable-length answers, one word per recurrent iteration. At each iteration, the last predicted word is fed through the recurrent loop into the LSTM until a special <span id="S2.SS1.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_typewriter">&lt;END&gt;</span> symbol is predicted. Several variants of this approach were proposed. For example, the “VIS+LSTM” of Ren <em id="S2.SS1.SSS0.Px2.p1.1.4" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS1.SSS0.Px2.p1.1.5" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> directly feed the feature vector produced by the encoder LSTM into a classifier to produce single-word answers from a predefined vocabulary. In other words, they formulate the answering as a classification problem, whereas Malinowski <em id="S2.SS1.SSS0.Px2.p1.1.6" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS1.SSS0.Px2.p1.1.7" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> was treating it as a sequence generation procedure. Ren <em id="S2.SS1.SSS0.Px2.p1.1.8" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS1.SSS0.Px2.p1.1.9" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> propose other technical improvements with the “2-VIS+BLSTM” model. It uses two sources of image features as input, fed to the LSTM at the start and at the end of the question sentence. It also uses LSTMs that scan questions in both forward and backward directions. Those bidirectional LSTMs better capture relations between distant words in the question.</p>
</div>
<div id="S2.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS1.SSS0.Px2.p2.1" class="ltx_p">Gao <em id="S2.SS1.SSS0.Px2.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS1.SSS0.Px2.p2.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> propose a slightly different method named “Multimodal QA” (mQA). It employs LSTMs to encode the question and produce the answer, with two differences from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. First, whereas <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> used common shared weights between the encoder and decoder LSTMs, mQA learns distinct parameters and only shares the word embedding. This is motivated by potentially different properties (<em id="S2.SS1.SSS0.Px2.p2.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS1.SSS0.Px2.p2.1.4" class="ltx_text"></span> in terms of grammar) of questions and answers. Second, the CNN features used as image representations are not fed into the encoder prior to the question, but at every time step.</p>
</div>
<div id="S2.SS1.SSS0.Px2.p3" class="ltx_para">
<p id="S2.SS1.SSS0.Px2.p3.1" class="ltx_p">Noh <em id="S2.SS1.SSS0.Px2.p3.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS1.SSS0.Px2.p3.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> tackle VQA by learning a CNN with a dynamic parameter layer (DPPnet) of which the weights are determined adaptively based on the question. For the adaptive parameter prediction, they employ a separate parameter prediction network, which consists of gated recurrent units (GRUs, a variant of LSTMs) taking a question as input and producing candidate weights through a fully-connected layer at its output. This arrangement was shown to significantly improve answering accuracy compared to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>. One can note a similarity in spirit with the modular approaches of Section <a href="#S2.SS3" title="2.3 Compositional Models ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>, in the sense that the question is used to tailor the main computations to each particular instance.</p>
</div>
<div id="S2.SS1.SSS0.Px2.p4" class="ltx_para">
<p id="S2.SS1.SSS0.Px2.p4.1" class="ltx_p">Fukui <em id="S2.SS1.SSS0.Px2.p4.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS1.SSS0.Px2.p4.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> propose a pooling method to perform the joint embedding visual and text features. They perform their “Multimodal Compact Bilinear pooling” (MCB) by randomly projecting the image and text features to a higher-dimensional space and then convolve both vectors with multiplications in the Fourier space for efficiency. Kim <em id="S2.SS1.SSS0.Px2.p4.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS1.SSS0.Px2.p4.1.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> use a multimodal residual learning framework (MRN) to learn the joint representation of images and language. Saito <em id="S2.SS1.SSS0.Px2.p4.1.5" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS1.SSS0.Px2.p4.1.6" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> propose a “DualNet” which integrates two kinds of operations, namely element-wise summations and element-wise multiplications to embed their visual and textual features. Similarly as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>, they formulate the answering as a classification problem over a predefined set of possible answers. Kafle <em id="S2.SS1.SSS0.Px2.p4.1.7" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS1.SSS0.Px2.p4.1.8" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> integrate an explicit prediction of the type of expected answer from the question and formulate the answering in a Bayesian framework.</p>
</div>
<div id="S2.SS1.SSS0.Px2.p5" class="ltx_para">
<p id="S2.SS1.SSS0.Px2.p5.1" class="ltx_p">Some other works do not make use of RNNs to encode questions. Ma <em id="S2.SS1.SSS0.Px2.p5.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS1.SSS0.Px2.p5.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> use CNNs to process the questions. Features from the image CNN and the text CNN are embedded in a common space through additional layers (a “multimodal CNN”) forming an overall homogeneous convolutional architecture. Zhou <em id="S2.SS1.SSS0.Px2.p5.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS1.SSS0.Px2.p5.1.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> and Antol <em id="S2.SS1.SSS0.Px2.p5.1.5" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS1.SSS0.Px2.p5.1.6" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> both use a traditional bag-of-words representation of the questions.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Performance and limitations</h5>

<div id="S2.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS1.SSS0.Px3.p1.1" class="ltx_p">We summarize performances of all discussed methods and datasets in Tables <a href="#S3.T6" title="Table 6 ‣ Evaluation Measures ‣ 3.1 Datasets of natural images ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>–<a href="#S3.T9" title="Table 9 ‣ Results of existing methods ‣ 3.1 Datasets of natural images ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. The “Neural-Image-QA”, as one of the earliest introduced methods, is considered the <span id="S2.SS1.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">de facto</span> baseline result. The “2-VIS+BLSTM” improves slightly on the DAQUAR dataset, mostly thanks to the bidirectional LSTM used to encode the questions. The “mQA” model was unfortunately not tested on publicly available datasets and is therefore not comparable. The DPPnet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> showed significant benefit from tailoring computations adaptively to each question through the dynamic parameter layer. At the time of its publication, it outperformed other joint embeddings methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>. The more recent MCB pooling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and multimodal residual learning (MRN) bring further improvements and achieve the top performances at the time of this writing.</p>
</div>
<div id="S2.SS1.SSS0.Px3.p2" class="ltx_para">
<p id="S2.SS1.SSS0.Px3.p2.1" class="ltx_p">The joint embedding approaches are straightforward in their principle and constitute the base of most current approaches to VQA. The latest improvements, exemplified by MCB and MRN, still showed potential room for improvement on both the extraction of features and their projection to the embedding space.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_font_italic ltx_title_subsection">2.2   Attention mechanisms</h3>

<section id="S2.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Motivation</h5>

<div id="S2.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p1.1" class="ltx_p">A limitation of most models presented above is to use global (image-wide) features to represent the visual input. This may feed irrelevant or noisy information to the prediction stage. The aim of attention mechanisms is to address this issue by using local image features, and allowing the model to assign different importance to features from different regions. An early application of attention to visual tasks was proposed in the context of image captioning by Xu <em id="S2.SS2.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.SSS0.Px1.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>. The attentional component of their model identifies salient regions in an image, and further processing then focuses the caption generation on those regions. This concept translates readily to the task of VQA for focusing on image regions relevant to the question. In some respect, the attention process forces an explicit additional step in the reasoning process that identifies “where to look” before performing further computations.</p>
</div>
<div id="S2.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p2.1" class="ltx_p">Although attention models were inspired by computational models of human vision, the apparent resemblance with biological systems can be misleading. Attention in artificial neural networks likely helps by allowing additional non-linearities and/or types of interactions (<em id="S2.SS2.SSS0.Px1.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS2.SSS0.Px1.p2.1.2" class="ltx_text"></span> multiplicative interaction through attention weights), whereas attention in biological visual systems is more likely a consequence of limited resources such as resolution, field of view, and processing capacity.</p>
</div>
</section>
<section id="S2.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Methods</h5>

<div id="S2.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px2.p1.16" class="ltx_p">Zhu <em id="S2.SS2.SSS0.Px2.p1.16.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.SSS0.Px2.p1.16.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> described how to add spatial attention to the standard LSTM model. The computations performed by an attention-enhanced LSTM are described as follows:</p>
<table id="Sx1.EGx1" class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table">

<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E1.m1.1" class="ltx_Math" alttext="\displaystyle i_{t}" display="inline"><semantics id="S2.E1.m1.1a"><msub id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><mi id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml">i</mi><mi id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1">subscript</csymbol><ci id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2">𝑖</ci><ci id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\displaystyle i_{t}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math id="S2.E1.m2.1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics id="S2.E1.m2.1a"><mo id="S2.E1.m2.1.1" xref="S2.E1.m2.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S2.E1.m2.1b"><eq id="S2.E1.m2.1.1.cmml" xref="S2.E1.m2.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m2.1c">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E1.m3.1" class="ltx_Math" alttext="\displaystyle\sigma(W_{xi}x_{t}+W_{hi}h_{t-1}+W_{zi}z_{t}+b_{i})" display="inline"><semantics id="S2.E1.m3.1a"><mrow id="S2.E1.m3.1.1" xref="S2.E1.m3.1.1.cmml"><mi id="S2.E1.m3.1.1.3" xref="S2.E1.m3.1.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S2.E1.m3.1.1.2" xref="S2.E1.m3.1.1.2.cmml">​</mo><mrow id="S2.E1.m3.1.1.1.1" xref="S2.E1.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m3.1.1.1.1.2" xref="S2.E1.m3.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m3.1.1.1.1.1" xref="S2.E1.m3.1.1.1.1.1.cmml"><mrow id="S2.E1.m3.1.1.1.1.1.2" xref="S2.E1.m3.1.1.1.1.1.2.cmml"><msub id="S2.E1.m3.1.1.1.1.1.2.2" xref="S2.E1.m3.1.1.1.1.1.2.2.cmml"><mi id="S2.E1.m3.1.1.1.1.1.2.2.2" xref="S2.E1.m3.1.1.1.1.1.2.2.2.cmml">W</mi><mrow id="S2.E1.m3.1.1.1.1.1.2.2.3" xref="S2.E1.m3.1.1.1.1.1.2.2.3.cmml"><mi id="S2.E1.m3.1.1.1.1.1.2.2.3.2" xref="S2.E1.m3.1.1.1.1.1.2.2.3.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.E1.m3.1.1.1.1.1.2.2.3.1" xref="S2.E1.m3.1.1.1.1.1.2.2.3.1.cmml">​</mo><mi id="S2.E1.m3.1.1.1.1.1.2.2.3.3" xref="S2.E1.m3.1.1.1.1.1.2.2.3.3.cmml">i</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E1.m3.1.1.1.1.1.2.1" xref="S2.E1.m3.1.1.1.1.1.2.1.cmml">​</mo><msub id="S2.E1.m3.1.1.1.1.1.2.3" xref="S2.E1.m3.1.1.1.1.1.2.3.cmml"><mi id="S2.E1.m3.1.1.1.1.1.2.3.2" xref="S2.E1.m3.1.1.1.1.1.2.3.2.cmml">x</mi><mi id="S2.E1.m3.1.1.1.1.1.2.3.3" xref="S2.E1.m3.1.1.1.1.1.2.3.3.cmml">t</mi></msub></mrow><mo id="S2.E1.m3.1.1.1.1.1.1" xref="S2.E1.m3.1.1.1.1.1.1.cmml">+</mo><mrow id="S2.E1.m3.1.1.1.1.1.3" xref="S2.E1.m3.1.1.1.1.1.3.cmml"><msub id="S2.E1.m3.1.1.1.1.1.3.2" xref="S2.E1.m3.1.1.1.1.1.3.2.cmml"><mi id="S2.E1.m3.1.1.1.1.1.3.2.2" xref="S2.E1.m3.1.1.1.1.1.3.2.2.cmml">W</mi><mrow id="S2.E1.m3.1.1.1.1.1.3.2.3" xref="S2.E1.m3.1.1.1.1.1.3.2.3.cmml"><mi id="S2.E1.m3.1.1.1.1.1.3.2.3.2" xref="S2.E1.m3.1.1.1.1.1.3.2.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.E1.m3.1.1.1.1.1.3.2.3.1" xref="S2.E1.m3.1.1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S2.E1.m3.1.1.1.1.1.3.2.3.3" xref="S2.E1.m3.1.1.1.1.1.3.2.3.3.cmml">i</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E1.m3.1.1.1.1.1.3.1" xref="S2.E1.m3.1.1.1.1.1.3.1.cmml">​</mo><msub id="S2.E1.m3.1.1.1.1.1.3.3" xref="S2.E1.m3.1.1.1.1.1.3.3.cmml"><mi id="S2.E1.m3.1.1.1.1.1.3.3.2" xref="S2.E1.m3.1.1.1.1.1.3.3.2.cmml">h</mi><mrow id="S2.E1.m3.1.1.1.1.1.3.3.3" xref="S2.E1.m3.1.1.1.1.1.3.3.3.cmml"><mi id="S2.E1.m3.1.1.1.1.1.3.3.3.2" xref="S2.E1.m3.1.1.1.1.1.3.3.3.2.cmml">t</mi><mo id="S2.E1.m3.1.1.1.1.1.3.3.3.1" xref="S2.E1.m3.1.1.1.1.1.3.3.3.1.cmml">−</mo><mn id="S2.E1.m3.1.1.1.1.1.3.3.3.3" xref="S2.E1.m3.1.1.1.1.1.3.3.3.3.cmml">1</mn></mrow></msub></mrow><mo id="S2.E1.m3.1.1.1.1.1.1a" xref="S2.E1.m3.1.1.1.1.1.1.cmml">+</mo><mrow id="S2.E1.m3.1.1.1.1.1.4" xref="S2.E1.m3.1.1.1.1.1.4.cmml"><msub id="S2.E1.m3.1.1.1.1.1.4.2" xref="S2.E1.m3.1.1.1.1.1.4.2.cmml"><mi id="S2.E1.m3.1.1.1.1.1.4.2.2" xref="S2.E1.m3.1.1.1.1.1.4.2.2.cmml">W</mi><mrow id="S2.E1.m3.1.1.1.1.1.4.2.3" xref="S2.E1.m3.1.1.1.1.1.4.2.3.cmml"><mi id="S2.E1.m3.1.1.1.1.1.4.2.3.2" xref="S2.E1.m3.1.1.1.1.1.4.2.3.2.cmml">z</mi><mo lspace="0em" rspace="0em" id="S2.E1.m3.1.1.1.1.1.4.2.3.1" xref="S2.E1.m3.1.1.1.1.1.4.2.3.1.cmml">​</mo><mi id="S2.E1.m3.1.1.1.1.1.4.2.3.3" xref="S2.E1.m3.1.1.1.1.1.4.2.3.3.cmml">i</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E1.m3.1.1.1.1.1.4.1" xref="S2.E1.m3.1.1.1.1.1.4.1.cmml">​</mo><msub id="S2.E1.m3.1.1.1.1.1.4.3" xref="S2.E1.m3.1.1.1.1.1.4.3.cmml"><mi id="S2.E1.m3.1.1.1.1.1.4.3.2" xref="S2.E1.m3.1.1.1.1.1.4.3.2.cmml">z</mi><mi id="S2.E1.m3.1.1.1.1.1.4.3.3" xref="S2.E1.m3.1.1.1.1.1.4.3.3.cmml">t</mi></msub></mrow><mo id="S2.E1.m3.1.1.1.1.1.1b" xref="S2.E1.m3.1.1.1.1.1.1.cmml">+</mo><msub id="S2.E1.m3.1.1.1.1.1.5" xref="S2.E1.m3.1.1.1.1.1.5.cmml"><mi id="S2.E1.m3.1.1.1.1.1.5.2" xref="S2.E1.m3.1.1.1.1.1.5.2.cmml">b</mi><mi id="S2.E1.m3.1.1.1.1.1.5.3" xref="S2.E1.m3.1.1.1.1.1.5.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S2.E1.m3.1.1.1.1.3" xref="S2.E1.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m3.1b"><apply id="S2.E1.m3.1.1.cmml" xref="S2.E1.m3.1.1"><times id="S2.E1.m3.1.1.2.cmml" xref="S2.E1.m3.1.1.2"></times><ci id="S2.E1.m3.1.1.3.cmml" xref="S2.E1.m3.1.1.3">𝜎</ci><apply id="S2.E1.m3.1.1.1.1.1.cmml" xref="S2.E1.m3.1.1.1.1"><plus id="S2.E1.m3.1.1.1.1.1.1.cmml" xref="S2.E1.m3.1.1.1.1.1.1"></plus><apply id="S2.E1.m3.1.1.1.1.1.2.cmml" xref="S2.E1.m3.1.1.1.1.1.2"><times id="S2.E1.m3.1.1.1.1.1.2.1.cmml" xref="S2.E1.m3.1.1.1.1.1.2.1"></times><apply id="S2.E1.m3.1.1.1.1.1.2.2.cmml" xref="S2.E1.m3.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E1.m3.1.1.1.1.1.2.2.1.cmml" xref="S2.E1.m3.1.1.1.1.1.2.2">subscript</csymbol><ci id="S2.E1.m3.1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m3.1.1.1.1.1.2.2.2">𝑊</ci><apply id="S2.E1.m3.1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m3.1.1.1.1.1.2.2.3"><times id="S2.E1.m3.1.1.1.1.1.2.2.3.1.cmml" xref="S2.E1.m3.1.1.1.1.1.2.2.3.1"></times><ci id="S2.E1.m3.1.1.1.1.1.2.2.3.2.cmml" xref="S2.E1.m3.1.1.1.1.1.2.2.3.2">𝑥</ci><ci id="S2.E1.m3.1.1.1.1.1.2.2.3.3.cmml" xref="S2.E1.m3.1.1.1.1.1.2.2.3.3">𝑖</ci></apply></apply><apply id="S2.E1.m3.1.1.1.1.1.2.3.cmml" xref="S2.E1.m3.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E1.m3.1.1.1.1.1.2.3.1.cmml" xref="S2.E1.m3.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.E1.m3.1.1.1.1.1.2.3.2.cmml" xref="S2.E1.m3.1.1.1.1.1.2.3.2">𝑥</ci><ci id="S2.E1.m3.1.1.1.1.1.2.3.3.cmml" xref="S2.E1.m3.1.1.1.1.1.2.3.3">𝑡</ci></apply></apply><apply id="S2.E1.m3.1.1.1.1.1.3.cmml" xref="S2.E1.m3.1.1.1.1.1.3"><times id="S2.E1.m3.1.1.1.1.1.3.1.cmml" xref="S2.E1.m3.1.1.1.1.1.3.1"></times><apply id="S2.E1.m3.1.1.1.1.1.3.2.cmml" xref="S2.E1.m3.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m3.1.1.1.1.1.3.2.1.cmml" xref="S2.E1.m3.1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E1.m3.1.1.1.1.1.3.2.2.cmml" xref="S2.E1.m3.1.1.1.1.1.3.2.2">𝑊</ci><apply id="S2.E1.m3.1.1.1.1.1.3.2.3.cmml" xref="S2.E1.m3.1.1.1.1.1.3.2.3"><times id="S2.E1.m3.1.1.1.1.1.3.2.3.1.cmml" xref="S2.E1.m3.1.1.1.1.1.3.2.3.1"></times><ci id="S2.E1.m3.1.1.1.1.1.3.2.3.2.cmml" xref="S2.E1.m3.1.1.1.1.1.3.2.3.2">ℎ</ci><ci id="S2.E1.m3.1.1.1.1.1.3.2.3.3.cmml" xref="S2.E1.m3.1.1.1.1.1.3.2.3.3">𝑖</ci></apply></apply><apply id="S2.E1.m3.1.1.1.1.1.3.3.cmml" xref="S2.E1.m3.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E1.m3.1.1.1.1.1.3.3.1.cmml" xref="S2.E1.m3.1.1.1.1.1.3.3">subscript</csymbol><ci id="S2.E1.m3.1.1.1.1.1.3.3.2.cmml" xref="S2.E1.m3.1.1.1.1.1.3.3.2">ℎ</ci><apply id="S2.E1.m3.1.1.1.1.1.3.3.3.cmml" xref="S2.E1.m3.1.1.1.1.1.3.3.3"><minus id="S2.E1.m3.1.1.1.1.1.3.3.3.1.cmml" xref="S2.E1.m3.1.1.1.1.1.3.3.3.1"></minus><ci id="S2.E1.m3.1.1.1.1.1.3.3.3.2.cmml" xref="S2.E1.m3.1.1.1.1.1.3.3.3.2">𝑡</ci><cn type="integer" id="S2.E1.m3.1.1.1.1.1.3.3.3.3.cmml" xref="S2.E1.m3.1.1.1.1.1.3.3.3.3">1</cn></apply></apply></apply><apply id="S2.E1.m3.1.1.1.1.1.4.cmml" xref="S2.E1.m3.1.1.1.1.1.4"><times id="S2.E1.m3.1.1.1.1.1.4.1.cmml" xref="S2.E1.m3.1.1.1.1.1.4.1"></times><apply id="S2.E1.m3.1.1.1.1.1.4.2.cmml" xref="S2.E1.m3.1.1.1.1.1.4.2"><csymbol cd="ambiguous" id="S2.E1.m3.1.1.1.1.1.4.2.1.cmml" xref="S2.E1.m3.1.1.1.1.1.4.2">subscript</csymbol><ci id="S2.E1.m3.1.1.1.1.1.4.2.2.cmml" xref="S2.E1.m3.1.1.1.1.1.4.2.2">𝑊</ci><apply id="S2.E1.m3.1.1.1.1.1.4.2.3.cmml" xref="S2.E1.m3.1.1.1.1.1.4.2.3"><times id="S2.E1.m3.1.1.1.1.1.4.2.3.1.cmml" xref="S2.E1.m3.1.1.1.1.1.4.2.3.1"></times><ci id="S2.E1.m3.1.1.1.1.1.4.2.3.2.cmml" xref="S2.E1.m3.1.1.1.1.1.4.2.3.2">𝑧</ci><ci id="S2.E1.m3.1.1.1.1.1.4.2.3.3.cmml" xref="S2.E1.m3.1.1.1.1.1.4.2.3.3">𝑖</ci></apply></apply><apply id="S2.E1.m3.1.1.1.1.1.4.3.cmml" xref="S2.E1.m3.1.1.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E1.m3.1.1.1.1.1.4.3.1.cmml" xref="S2.E1.m3.1.1.1.1.1.4.3">subscript</csymbol><ci id="S2.E1.m3.1.1.1.1.1.4.3.2.cmml" xref="S2.E1.m3.1.1.1.1.1.4.3.2">𝑧</ci><ci id="S2.E1.m3.1.1.1.1.1.4.3.3.cmml" xref="S2.E1.m3.1.1.1.1.1.4.3.3">𝑡</ci></apply></apply><apply id="S2.E1.m3.1.1.1.1.1.5.cmml" xref="S2.E1.m3.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S2.E1.m3.1.1.1.1.1.5.1.cmml" xref="S2.E1.m3.1.1.1.1.1.5">subscript</csymbol><ci id="S2.E1.m3.1.1.1.1.1.5.2.cmml" xref="S2.E1.m3.1.1.1.1.1.5.2">𝑏</ci><ci id="S2.E1.m3.1.1.1.1.1.5.3.cmml" xref="S2.E1.m3.1.1.1.1.1.5.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m3.1c">\displaystyle\sigma(W_{xi}x_{t}+W_{hi}h_{t-1}+W_{zi}z_{t}+b_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E2.m1.1" class="ltx_Math" alttext="\displaystyle f_{t}" display="inline"><semantics id="S2.E2.m1.1a"><msub id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml"><mi id="S2.E2.m1.1.1.2" xref="S2.E2.m1.1.1.2.cmml">f</mi><mi id="S2.E2.m1.1.1.3" xref="S2.E2.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><apply id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.cmml" xref="S2.E2.m1.1.1">subscript</csymbol><ci id="S2.E2.m1.1.1.2.cmml" xref="S2.E2.m1.1.1.2">𝑓</ci><ci id="S2.E2.m1.1.1.3.cmml" xref="S2.E2.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">\displaystyle f_{t}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math id="S2.E2.m2.1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics id="S2.E2.m2.1a"><mo id="S2.E2.m2.1.1" xref="S2.E2.m2.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S2.E2.m2.1b"><eq id="S2.E2.m2.1.1.cmml" xref="S2.E2.m2.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m2.1c">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E2.m3.1" class="ltx_Math" alttext="\displaystyle\sigma(W_{xf}x_{t}+W_{hf}h_{t-1}+W_{zf}z_{t}+b_{f})" display="inline"><semantics id="S2.E2.m3.1a"><mrow id="S2.E2.m3.1.1" xref="S2.E2.m3.1.1.cmml"><mi id="S2.E2.m3.1.1.3" xref="S2.E2.m3.1.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S2.E2.m3.1.1.2" xref="S2.E2.m3.1.1.2.cmml">​</mo><mrow id="S2.E2.m3.1.1.1.1" xref="S2.E2.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m3.1.1.1.1.2" xref="S2.E2.m3.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m3.1.1.1.1.1" xref="S2.E2.m3.1.1.1.1.1.cmml"><mrow id="S2.E2.m3.1.1.1.1.1.2" xref="S2.E2.m3.1.1.1.1.1.2.cmml"><msub id="S2.E2.m3.1.1.1.1.1.2.2" xref="S2.E2.m3.1.1.1.1.1.2.2.cmml"><mi id="S2.E2.m3.1.1.1.1.1.2.2.2" xref="S2.E2.m3.1.1.1.1.1.2.2.2.cmml">W</mi><mrow id="S2.E2.m3.1.1.1.1.1.2.2.3" xref="S2.E2.m3.1.1.1.1.1.2.2.3.cmml"><mi id="S2.E2.m3.1.1.1.1.1.2.2.3.2" xref="S2.E2.m3.1.1.1.1.1.2.2.3.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.E2.m3.1.1.1.1.1.2.2.3.1" xref="S2.E2.m3.1.1.1.1.1.2.2.3.1.cmml">​</mo><mi id="S2.E2.m3.1.1.1.1.1.2.2.3.3" xref="S2.E2.m3.1.1.1.1.1.2.2.3.3.cmml">f</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E2.m3.1.1.1.1.1.2.1" xref="S2.E2.m3.1.1.1.1.1.2.1.cmml">​</mo><msub id="S2.E2.m3.1.1.1.1.1.2.3" xref="S2.E2.m3.1.1.1.1.1.2.3.cmml"><mi id="S2.E2.m3.1.1.1.1.1.2.3.2" xref="S2.E2.m3.1.1.1.1.1.2.3.2.cmml">x</mi><mi id="S2.E2.m3.1.1.1.1.1.2.3.3" xref="S2.E2.m3.1.1.1.1.1.2.3.3.cmml">t</mi></msub></mrow><mo id="S2.E2.m3.1.1.1.1.1.1" xref="S2.E2.m3.1.1.1.1.1.1.cmml">+</mo><mrow id="S2.E2.m3.1.1.1.1.1.3" xref="S2.E2.m3.1.1.1.1.1.3.cmml"><msub id="S2.E2.m3.1.1.1.1.1.3.2" xref="S2.E2.m3.1.1.1.1.1.3.2.cmml"><mi id="S2.E2.m3.1.1.1.1.1.3.2.2" xref="S2.E2.m3.1.1.1.1.1.3.2.2.cmml">W</mi><mrow id="S2.E2.m3.1.1.1.1.1.3.2.3" xref="S2.E2.m3.1.1.1.1.1.3.2.3.cmml"><mi id="S2.E2.m3.1.1.1.1.1.3.2.3.2" xref="S2.E2.m3.1.1.1.1.1.3.2.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.E2.m3.1.1.1.1.1.3.2.3.1" xref="S2.E2.m3.1.1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S2.E2.m3.1.1.1.1.1.3.2.3.3" xref="S2.E2.m3.1.1.1.1.1.3.2.3.3.cmml">f</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E2.m3.1.1.1.1.1.3.1" xref="S2.E2.m3.1.1.1.1.1.3.1.cmml">​</mo><msub id="S2.E2.m3.1.1.1.1.1.3.3" xref="S2.E2.m3.1.1.1.1.1.3.3.cmml"><mi id="S2.E2.m3.1.1.1.1.1.3.3.2" xref="S2.E2.m3.1.1.1.1.1.3.3.2.cmml">h</mi><mrow id="S2.E2.m3.1.1.1.1.1.3.3.3" xref="S2.E2.m3.1.1.1.1.1.3.3.3.cmml"><mi id="S2.E2.m3.1.1.1.1.1.3.3.3.2" xref="S2.E2.m3.1.1.1.1.1.3.3.3.2.cmml">t</mi><mo id="S2.E2.m3.1.1.1.1.1.3.3.3.1" xref="S2.E2.m3.1.1.1.1.1.3.3.3.1.cmml">−</mo><mn id="S2.E2.m3.1.1.1.1.1.3.3.3.3" xref="S2.E2.m3.1.1.1.1.1.3.3.3.3.cmml">1</mn></mrow></msub></mrow><mo id="S2.E2.m3.1.1.1.1.1.1a" xref="S2.E2.m3.1.1.1.1.1.1.cmml">+</mo><mrow id="S2.E2.m3.1.1.1.1.1.4" xref="S2.E2.m3.1.1.1.1.1.4.cmml"><msub id="S2.E2.m3.1.1.1.1.1.4.2" xref="S2.E2.m3.1.1.1.1.1.4.2.cmml"><mi id="S2.E2.m3.1.1.1.1.1.4.2.2" xref="S2.E2.m3.1.1.1.1.1.4.2.2.cmml">W</mi><mrow id="S2.E2.m3.1.1.1.1.1.4.2.3" xref="S2.E2.m3.1.1.1.1.1.4.2.3.cmml"><mi id="S2.E2.m3.1.1.1.1.1.4.2.3.2" xref="S2.E2.m3.1.1.1.1.1.4.2.3.2.cmml">z</mi><mo lspace="0em" rspace="0em" id="S2.E2.m3.1.1.1.1.1.4.2.3.1" xref="S2.E2.m3.1.1.1.1.1.4.2.3.1.cmml">​</mo><mi id="S2.E2.m3.1.1.1.1.1.4.2.3.3" xref="S2.E2.m3.1.1.1.1.1.4.2.3.3.cmml">f</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E2.m3.1.1.1.1.1.4.1" xref="S2.E2.m3.1.1.1.1.1.4.1.cmml">​</mo><msub id="S2.E2.m3.1.1.1.1.1.4.3" xref="S2.E2.m3.1.1.1.1.1.4.3.cmml"><mi id="S2.E2.m3.1.1.1.1.1.4.3.2" xref="S2.E2.m3.1.1.1.1.1.4.3.2.cmml">z</mi><mi id="S2.E2.m3.1.1.1.1.1.4.3.3" xref="S2.E2.m3.1.1.1.1.1.4.3.3.cmml">t</mi></msub></mrow><mo id="S2.E2.m3.1.1.1.1.1.1b" xref="S2.E2.m3.1.1.1.1.1.1.cmml">+</mo><msub id="S2.E2.m3.1.1.1.1.1.5" xref="S2.E2.m3.1.1.1.1.1.5.cmml"><mi id="S2.E2.m3.1.1.1.1.1.5.2" xref="S2.E2.m3.1.1.1.1.1.5.2.cmml">b</mi><mi id="S2.E2.m3.1.1.1.1.1.5.3" xref="S2.E2.m3.1.1.1.1.1.5.3.cmml">f</mi></msub></mrow><mo stretchy="false" id="S2.E2.m3.1.1.1.1.3" xref="S2.E2.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m3.1b"><apply id="S2.E2.m3.1.1.cmml" xref="S2.E2.m3.1.1"><times id="S2.E2.m3.1.1.2.cmml" xref="S2.E2.m3.1.1.2"></times><ci id="S2.E2.m3.1.1.3.cmml" xref="S2.E2.m3.1.1.3">𝜎</ci><apply id="S2.E2.m3.1.1.1.1.1.cmml" xref="S2.E2.m3.1.1.1.1"><plus id="S2.E2.m3.1.1.1.1.1.1.cmml" xref="S2.E2.m3.1.1.1.1.1.1"></plus><apply id="S2.E2.m3.1.1.1.1.1.2.cmml" xref="S2.E2.m3.1.1.1.1.1.2"><times id="S2.E2.m3.1.1.1.1.1.2.1.cmml" xref="S2.E2.m3.1.1.1.1.1.2.1"></times><apply id="S2.E2.m3.1.1.1.1.1.2.2.cmml" xref="S2.E2.m3.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E2.m3.1.1.1.1.1.2.2.1.cmml" xref="S2.E2.m3.1.1.1.1.1.2.2">subscript</csymbol><ci id="S2.E2.m3.1.1.1.1.1.2.2.2.cmml" xref="S2.E2.m3.1.1.1.1.1.2.2.2">𝑊</ci><apply id="S2.E2.m3.1.1.1.1.1.2.2.3.cmml" xref="S2.E2.m3.1.1.1.1.1.2.2.3"><times id="S2.E2.m3.1.1.1.1.1.2.2.3.1.cmml" xref="S2.E2.m3.1.1.1.1.1.2.2.3.1"></times><ci id="S2.E2.m3.1.1.1.1.1.2.2.3.2.cmml" xref="S2.E2.m3.1.1.1.1.1.2.2.3.2">𝑥</ci><ci id="S2.E2.m3.1.1.1.1.1.2.2.3.3.cmml" xref="S2.E2.m3.1.1.1.1.1.2.2.3.3">𝑓</ci></apply></apply><apply id="S2.E2.m3.1.1.1.1.1.2.3.cmml" xref="S2.E2.m3.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E2.m3.1.1.1.1.1.2.3.1.cmml" xref="S2.E2.m3.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.E2.m3.1.1.1.1.1.2.3.2.cmml" xref="S2.E2.m3.1.1.1.1.1.2.3.2">𝑥</ci><ci id="S2.E2.m3.1.1.1.1.1.2.3.3.cmml" xref="S2.E2.m3.1.1.1.1.1.2.3.3">𝑡</ci></apply></apply><apply id="S2.E2.m3.1.1.1.1.1.3.cmml" xref="S2.E2.m3.1.1.1.1.1.3"><times id="S2.E2.m3.1.1.1.1.1.3.1.cmml" xref="S2.E2.m3.1.1.1.1.1.3.1"></times><apply id="S2.E2.m3.1.1.1.1.1.3.2.cmml" xref="S2.E2.m3.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E2.m3.1.1.1.1.1.3.2.1.cmml" xref="S2.E2.m3.1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E2.m3.1.1.1.1.1.3.2.2.cmml" xref="S2.E2.m3.1.1.1.1.1.3.2.2">𝑊</ci><apply id="S2.E2.m3.1.1.1.1.1.3.2.3.cmml" xref="S2.E2.m3.1.1.1.1.1.3.2.3"><times id="S2.E2.m3.1.1.1.1.1.3.2.3.1.cmml" xref="S2.E2.m3.1.1.1.1.1.3.2.3.1"></times><ci id="S2.E2.m3.1.1.1.1.1.3.2.3.2.cmml" xref="S2.E2.m3.1.1.1.1.1.3.2.3.2">ℎ</ci><ci id="S2.E2.m3.1.1.1.1.1.3.2.3.3.cmml" xref="S2.E2.m3.1.1.1.1.1.3.2.3.3">𝑓</ci></apply></apply><apply id="S2.E2.m3.1.1.1.1.1.3.3.cmml" xref="S2.E2.m3.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E2.m3.1.1.1.1.1.3.3.1.cmml" xref="S2.E2.m3.1.1.1.1.1.3.3">subscript</csymbol><ci id="S2.E2.m3.1.1.1.1.1.3.3.2.cmml" xref="S2.E2.m3.1.1.1.1.1.3.3.2">ℎ</ci><apply id="S2.E2.m3.1.1.1.1.1.3.3.3.cmml" xref="S2.E2.m3.1.1.1.1.1.3.3.3"><minus id="S2.E2.m3.1.1.1.1.1.3.3.3.1.cmml" xref="S2.E2.m3.1.1.1.1.1.3.3.3.1"></minus><ci id="S2.E2.m3.1.1.1.1.1.3.3.3.2.cmml" xref="S2.E2.m3.1.1.1.1.1.3.3.3.2">𝑡</ci><cn type="integer" id="S2.E2.m3.1.1.1.1.1.3.3.3.3.cmml" xref="S2.E2.m3.1.1.1.1.1.3.3.3.3">1</cn></apply></apply></apply><apply id="S2.E2.m3.1.1.1.1.1.4.cmml" xref="S2.E2.m3.1.1.1.1.1.4"><times id="S2.E2.m3.1.1.1.1.1.4.1.cmml" xref="S2.E2.m3.1.1.1.1.1.4.1"></times><apply id="S2.E2.m3.1.1.1.1.1.4.2.cmml" xref="S2.E2.m3.1.1.1.1.1.4.2"><csymbol cd="ambiguous" id="S2.E2.m3.1.1.1.1.1.4.2.1.cmml" xref="S2.E2.m3.1.1.1.1.1.4.2">subscript</csymbol><ci id="S2.E2.m3.1.1.1.1.1.4.2.2.cmml" xref="S2.E2.m3.1.1.1.1.1.4.2.2">𝑊</ci><apply id="S2.E2.m3.1.1.1.1.1.4.2.3.cmml" xref="S2.E2.m3.1.1.1.1.1.4.2.3"><times id="S2.E2.m3.1.1.1.1.1.4.2.3.1.cmml" xref="S2.E2.m3.1.1.1.1.1.4.2.3.1"></times><ci id="S2.E2.m3.1.1.1.1.1.4.2.3.2.cmml" xref="S2.E2.m3.1.1.1.1.1.4.2.3.2">𝑧</ci><ci id="S2.E2.m3.1.1.1.1.1.4.2.3.3.cmml" xref="S2.E2.m3.1.1.1.1.1.4.2.3.3">𝑓</ci></apply></apply><apply id="S2.E2.m3.1.1.1.1.1.4.3.cmml" xref="S2.E2.m3.1.1.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E2.m3.1.1.1.1.1.4.3.1.cmml" xref="S2.E2.m3.1.1.1.1.1.4.3">subscript</csymbol><ci id="S2.E2.m3.1.1.1.1.1.4.3.2.cmml" xref="S2.E2.m3.1.1.1.1.1.4.3.2">𝑧</ci><ci id="S2.E2.m3.1.1.1.1.1.4.3.3.cmml" xref="S2.E2.m3.1.1.1.1.1.4.3.3">𝑡</ci></apply></apply><apply id="S2.E2.m3.1.1.1.1.1.5.cmml" xref="S2.E2.m3.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S2.E2.m3.1.1.1.1.1.5.1.cmml" xref="S2.E2.m3.1.1.1.1.1.5">subscript</csymbol><ci id="S2.E2.m3.1.1.1.1.1.5.2.cmml" xref="S2.E2.m3.1.1.1.1.1.5.2">𝑏</ci><ci id="S2.E2.m3.1.1.1.1.1.5.3.cmml" xref="S2.E2.m3.1.1.1.1.1.5.3">𝑓</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m3.1c">\displaystyle\sigma(W_{xf}x_{t}+W_{hf}h_{t-1}+W_{zf}z_{t}+b_{f})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
<tbody id="S2.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E3.m1.1" class="ltx_Math" alttext="\displaystyle o_{t}" display="inline"><semantics id="S2.E3.m1.1a"><msub id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml"><mi id="S2.E3.m1.1.1.2" xref="S2.E3.m1.1.1.2.cmml">o</mi><mi id="S2.E3.m1.1.1.3" xref="S2.E3.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.E3.m1.1b"><apply id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.cmml" xref="S2.E3.m1.1.1">subscript</csymbol><ci id="S2.E3.m1.1.1.2.cmml" xref="S2.E3.m1.1.1.2">𝑜</ci><ci id="S2.E3.m1.1.1.3.cmml" xref="S2.E3.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.1c">\displaystyle o_{t}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math id="S2.E3.m2.1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics id="S2.E3.m2.1a"><mo id="S2.E3.m2.1.1" xref="S2.E3.m2.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S2.E3.m2.1b"><eq id="S2.E3.m2.1.1.cmml" xref="S2.E3.m2.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m2.1c">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E3.m3.1" class="ltx_Math" alttext="\displaystyle\sigma(W_{xo}x_{t}+W_{ho}h_{t-1}+W_{zo}z_{t}+b_{o})" display="inline"><semantics id="S2.E3.m3.1a"><mrow id="S2.E3.m3.1.1" xref="S2.E3.m3.1.1.cmml"><mi id="S2.E3.m3.1.1.3" xref="S2.E3.m3.1.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S2.E3.m3.1.1.2" xref="S2.E3.m3.1.1.2.cmml">​</mo><mrow id="S2.E3.m3.1.1.1.1" xref="S2.E3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E3.m3.1.1.1.1.2" xref="S2.E3.m3.1.1.1.1.1.cmml">(</mo><mrow id="S2.E3.m3.1.1.1.1.1" xref="S2.E3.m3.1.1.1.1.1.cmml"><mrow id="S2.E3.m3.1.1.1.1.1.2" xref="S2.E3.m3.1.1.1.1.1.2.cmml"><msub id="S2.E3.m3.1.1.1.1.1.2.2" xref="S2.E3.m3.1.1.1.1.1.2.2.cmml"><mi id="S2.E3.m3.1.1.1.1.1.2.2.2" xref="S2.E3.m3.1.1.1.1.1.2.2.2.cmml">W</mi><mrow id="S2.E3.m3.1.1.1.1.1.2.2.3" xref="S2.E3.m3.1.1.1.1.1.2.2.3.cmml"><mi id="S2.E3.m3.1.1.1.1.1.2.2.3.2" xref="S2.E3.m3.1.1.1.1.1.2.2.3.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.E3.m3.1.1.1.1.1.2.2.3.1" xref="S2.E3.m3.1.1.1.1.1.2.2.3.1.cmml">​</mo><mi id="S2.E3.m3.1.1.1.1.1.2.2.3.3" xref="S2.E3.m3.1.1.1.1.1.2.2.3.3.cmml">o</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E3.m3.1.1.1.1.1.2.1" xref="S2.E3.m3.1.1.1.1.1.2.1.cmml">​</mo><msub id="S2.E3.m3.1.1.1.1.1.2.3" xref="S2.E3.m3.1.1.1.1.1.2.3.cmml"><mi id="S2.E3.m3.1.1.1.1.1.2.3.2" xref="S2.E3.m3.1.1.1.1.1.2.3.2.cmml">x</mi><mi id="S2.E3.m3.1.1.1.1.1.2.3.3" xref="S2.E3.m3.1.1.1.1.1.2.3.3.cmml">t</mi></msub></mrow><mo id="S2.E3.m3.1.1.1.1.1.1" xref="S2.E3.m3.1.1.1.1.1.1.cmml">+</mo><mrow id="S2.E3.m3.1.1.1.1.1.3" xref="S2.E3.m3.1.1.1.1.1.3.cmml"><msub id="S2.E3.m3.1.1.1.1.1.3.2" xref="S2.E3.m3.1.1.1.1.1.3.2.cmml"><mi id="S2.E3.m3.1.1.1.1.1.3.2.2" xref="S2.E3.m3.1.1.1.1.1.3.2.2.cmml">W</mi><mrow id="S2.E3.m3.1.1.1.1.1.3.2.3" xref="S2.E3.m3.1.1.1.1.1.3.2.3.cmml"><mi id="S2.E3.m3.1.1.1.1.1.3.2.3.2" xref="S2.E3.m3.1.1.1.1.1.3.2.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.E3.m3.1.1.1.1.1.3.2.3.1" xref="S2.E3.m3.1.1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S2.E3.m3.1.1.1.1.1.3.2.3.3" xref="S2.E3.m3.1.1.1.1.1.3.2.3.3.cmml">o</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E3.m3.1.1.1.1.1.3.1" xref="S2.E3.m3.1.1.1.1.1.3.1.cmml">​</mo><msub id="S2.E3.m3.1.1.1.1.1.3.3" xref="S2.E3.m3.1.1.1.1.1.3.3.cmml"><mi id="S2.E3.m3.1.1.1.1.1.3.3.2" xref="S2.E3.m3.1.1.1.1.1.3.3.2.cmml">h</mi><mrow id="S2.E3.m3.1.1.1.1.1.3.3.3" xref="S2.E3.m3.1.1.1.1.1.3.3.3.cmml"><mi id="S2.E3.m3.1.1.1.1.1.3.3.3.2" xref="S2.E3.m3.1.1.1.1.1.3.3.3.2.cmml">t</mi><mo id="S2.E3.m3.1.1.1.1.1.3.3.3.1" xref="S2.E3.m3.1.1.1.1.1.3.3.3.1.cmml">−</mo><mn id="S2.E3.m3.1.1.1.1.1.3.3.3.3" xref="S2.E3.m3.1.1.1.1.1.3.3.3.3.cmml">1</mn></mrow></msub></mrow><mo id="S2.E3.m3.1.1.1.1.1.1a" xref="S2.E3.m3.1.1.1.1.1.1.cmml">+</mo><mrow id="S2.E3.m3.1.1.1.1.1.4" xref="S2.E3.m3.1.1.1.1.1.4.cmml"><msub id="S2.E3.m3.1.1.1.1.1.4.2" xref="S2.E3.m3.1.1.1.1.1.4.2.cmml"><mi id="S2.E3.m3.1.1.1.1.1.4.2.2" xref="S2.E3.m3.1.1.1.1.1.4.2.2.cmml">W</mi><mrow id="S2.E3.m3.1.1.1.1.1.4.2.3" xref="S2.E3.m3.1.1.1.1.1.4.2.3.cmml"><mi id="S2.E3.m3.1.1.1.1.1.4.2.3.2" xref="S2.E3.m3.1.1.1.1.1.4.2.3.2.cmml">z</mi><mo lspace="0em" rspace="0em" id="S2.E3.m3.1.1.1.1.1.4.2.3.1" xref="S2.E3.m3.1.1.1.1.1.4.2.3.1.cmml">​</mo><mi id="S2.E3.m3.1.1.1.1.1.4.2.3.3" xref="S2.E3.m3.1.1.1.1.1.4.2.3.3.cmml">o</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E3.m3.1.1.1.1.1.4.1" xref="S2.E3.m3.1.1.1.1.1.4.1.cmml">​</mo><msub id="S2.E3.m3.1.1.1.1.1.4.3" xref="S2.E3.m3.1.1.1.1.1.4.3.cmml"><mi id="S2.E3.m3.1.1.1.1.1.4.3.2" xref="S2.E3.m3.1.1.1.1.1.4.3.2.cmml">z</mi><mi id="S2.E3.m3.1.1.1.1.1.4.3.3" xref="S2.E3.m3.1.1.1.1.1.4.3.3.cmml">t</mi></msub></mrow><mo id="S2.E3.m3.1.1.1.1.1.1b" xref="S2.E3.m3.1.1.1.1.1.1.cmml">+</mo><msub id="S2.E3.m3.1.1.1.1.1.5" xref="S2.E3.m3.1.1.1.1.1.5.cmml"><mi id="S2.E3.m3.1.1.1.1.1.5.2" xref="S2.E3.m3.1.1.1.1.1.5.2.cmml">b</mi><mi id="S2.E3.m3.1.1.1.1.1.5.3" xref="S2.E3.m3.1.1.1.1.1.5.3.cmml">o</mi></msub></mrow><mo stretchy="false" id="S2.E3.m3.1.1.1.1.3" xref="S2.E3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m3.1b"><apply id="S2.E3.m3.1.1.cmml" xref="S2.E3.m3.1.1"><times id="S2.E3.m3.1.1.2.cmml" xref="S2.E3.m3.1.1.2"></times><ci id="S2.E3.m3.1.1.3.cmml" xref="S2.E3.m3.1.1.3">𝜎</ci><apply id="S2.E3.m3.1.1.1.1.1.cmml" xref="S2.E3.m3.1.1.1.1"><plus id="S2.E3.m3.1.1.1.1.1.1.cmml" xref="S2.E3.m3.1.1.1.1.1.1"></plus><apply id="S2.E3.m3.1.1.1.1.1.2.cmml" xref="S2.E3.m3.1.1.1.1.1.2"><times id="S2.E3.m3.1.1.1.1.1.2.1.cmml" xref="S2.E3.m3.1.1.1.1.1.2.1"></times><apply id="S2.E3.m3.1.1.1.1.1.2.2.cmml" xref="S2.E3.m3.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E3.m3.1.1.1.1.1.2.2.1.cmml" xref="S2.E3.m3.1.1.1.1.1.2.2">subscript</csymbol><ci id="S2.E3.m3.1.1.1.1.1.2.2.2.cmml" xref="S2.E3.m3.1.1.1.1.1.2.2.2">𝑊</ci><apply id="S2.E3.m3.1.1.1.1.1.2.2.3.cmml" xref="S2.E3.m3.1.1.1.1.1.2.2.3"><times id="S2.E3.m3.1.1.1.1.1.2.2.3.1.cmml" xref="S2.E3.m3.1.1.1.1.1.2.2.3.1"></times><ci id="S2.E3.m3.1.1.1.1.1.2.2.3.2.cmml" xref="S2.E3.m3.1.1.1.1.1.2.2.3.2">𝑥</ci><ci id="S2.E3.m3.1.1.1.1.1.2.2.3.3.cmml" xref="S2.E3.m3.1.1.1.1.1.2.2.3.3">𝑜</ci></apply></apply><apply id="S2.E3.m3.1.1.1.1.1.2.3.cmml" xref="S2.E3.m3.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E3.m3.1.1.1.1.1.2.3.1.cmml" xref="S2.E3.m3.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.E3.m3.1.1.1.1.1.2.3.2.cmml" xref="S2.E3.m3.1.1.1.1.1.2.3.2">𝑥</ci><ci id="S2.E3.m3.1.1.1.1.1.2.3.3.cmml" xref="S2.E3.m3.1.1.1.1.1.2.3.3">𝑡</ci></apply></apply><apply id="S2.E3.m3.1.1.1.1.1.3.cmml" xref="S2.E3.m3.1.1.1.1.1.3"><times id="S2.E3.m3.1.1.1.1.1.3.1.cmml" xref="S2.E3.m3.1.1.1.1.1.3.1"></times><apply id="S2.E3.m3.1.1.1.1.1.3.2.cmml" xref="S2.E3.m3.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E3.m3.1.1.1.1.1.3.2.1.cmml" xref="S2.E3.m3.1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E3.m3.1.1.1.1.1.3.2.2.cmml" xref="S2.E3.m3.1.1.1.1.1.3.2.2">𝑊</ci><apply id="S2.E3.m3.1.1.1.1.1.3.2.3.cmml" xref="S2.E3.m3.1.1.1.1.1.3.2.3"><times id="S2.E3.m3.1.1.1.1.1.3.2.3.1.cmml" xref="S2.E3.m3.1.1.1.1.1.3.2.3.1"></times><ci id="S2.E3.m3.1.1.1.1.1.3.2.3.2.cmml" xref="S2.E3.m3.1.1.1.1.1.3.2.3.2">ℎ</ci><ci id="S2.E3.m3.1.1.1.1.1.3.2.3.3.cmml" xref="S2.E3.m3.1.1.1.1.1.3.2.3.3">𝑜</ci></apply></apply><apply id="S2.E3.m3.1.1.1.1.1.3.3.cmml" xref="S2.E3.m3.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E3.m3.1.1.1.1.1.3.3.1.cmml" xref="S2.E3.m3.1.1.1.1.1.3.3">subscript</csymbol><ci id="S2.E3.m3.1.1.1.1.1.3.3.2.cmml" xref="S2.E3.m3.1.1.1.1.1.3.3.2">ℎ</ci><apply id="S2.E3.m3.1.1.1.1.1.3.3.3.cmml" xref="S2.E3.m3.1.1.1.1.1.3.3.3"><minus id="S2.E3.m3.1.1.1.1.1.3.3.3.1.cmml" xref="S2.E3.m3.1.1.1.1.1.3.3.3.1"></minus><ci id="S2.E3.m3.1.1.1.1.1.3.3.3.2.cmml" xref="S2.E3.m3.1.1.1.1.1.3.3.3.2">𝑡</ci><cn type="integer" id="S2.E3.m3.1.1.1.1.1.3.3.3.3.cmml" xref="S2.E3.m3.1.1.1.1.1.3.3.3.3">1</cn></apply></apply></apply><apply id="S2.E3.m3.1.1.1.1.1.4.cmml" xref="S2.E3.m3.1.1.1.1.1.4"><times id="S2.E3.m3.1.1.1.1.1.4.1.cmml" xref="S2.E3.m3.1.1.1.1.1.4.1"></times><apply id="S2.E3.m3.1.1.1.1.1.4.2.cmml" xref="S2.E3.m3.1.1.1.1.1.4.2"><csymbol cd="ambiguous" id="S2.E3.m3.1.1.1.1.1.4.2.1.cmml" xref="S2.E3.m3.1.1.1.1.1.4.2">subscript</csymbol><ci id="S2.E3.m3.1.1.1.1.1.4.2.2.cmml" xref="S2.E3.m3.1.1.1.1.1.4.2.2">𝑊</ci><apply id="S2.E3.m3.1.1.1.1.1.4.2.3.cmml" xref="S2.E3.m3.1.1.1.1.1.4.2.3"><times id="S2.E3.m3.1.1.1.1.1.4.2.3.1.cmml" xref="S2.E3.m3.1.1.1.1.1.4.2.3.1"></times><ci id="S2.E3.m3.1.1.1.1.1.4.2.3.2.cmml" xref="S2.E3.m3.1.1.1.1.1.4.2.3.2">𝑧</ci><ci id="S2.E3.m3.1.1.1.1.1.4.2.3.3.cmml" xref="S2.E3.m3.1.1.1.1.1.4.2.3.3">𝑜</ci></apply></apply><apply id="S2.E3.m3.1.1.1.1.1.4.3.cmml" xref="S2.E3.m3.1.1.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E3.m3.1.1.1.1.1.4.3.1.cmml" xref="S2.E3.m3.1.1.1.1.1.4.3">subscript</csymbol><ci id="S2.E3.m3.1.1.1.1.1.4.3.2.cmml" xref="S2.E3.m3.1.1.1.1.1.4.3.2">𝑧</ci><ci id="S2.E3.m3.1.1.1.1.1.4.3.3.cmml" xref="S2.E3.m3.1.1.1.1.1.4.3.3">𝑡</ci></apply></apply><apply id="S2.E3.m3.1.1.1.1.1.5.cmml" xref="S2.E3.m3.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S2.E3.m3.1.1.1.1.1.5.1.cmml" xref="S2.E3.m3.1.1.1.1.1.5">subscript</csymbol><ci id="S2.E3.m3.1.1.1.1.1.5.2.cmml" xref="S2.E3.m3.1.1.1.1.1.5.2">𝑏</ci><ci id="S2.E3.m3.1.1.1.1.1.5.3.cmml" xref="S2.E3.m3.1.1.1.1.1.5.3">𝑜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m3.1c">\displaystyle\sigma(W_{xo}x_{t}+W_{ho}h_{t-1}+W_{zo}z_{t}+b_{o})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
<tbody id="S2.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E4.m1.1" class="ltx_Math" alttext="\displaystyle g_{t}" display="inline"><semantics id="S2.E4.m1.1a"><msub id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml"><mi id="S2.E4.m1.1.1.2" xref="S2.E4.m1.1.1.2.cmml">g</mi><mi id="S2.E4.m1.1.1.3" xref="S2.E4.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.E4.m1.1b"><apply id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.1.1.1.cmml" xref="S2.E4.m1.1.1">subscript</csymbol><ci id="S2.E4.m1.1.1.2.cmml" xref="S2.E4.m1.1.1.2">𝑔</ci><ci id="S2.E4.m1.1.1.3.cmml" xref="S2.E4.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.1c">\displaystyle g_{t}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math id="S2.E4.m2.1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics id="S2.E4.m2.1a"><mo id="S2.E4.m2.1.1" xref="S2.E4.m2.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S2.E4.m2.1b"><eq id="S2.E4.m2.1.1.cmml" xref="S2.E4.m2.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m2.1c">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E4.m3.2" class="ltx_Math" alttext="\displaystyle\tanh(W_{xg}x_{t}+W_{hg}h_{t-1}+W_{zg}z_{t}+b_{c})" display="inline"><semantics id="S2.E4.m3.2a"><mrow id="S2.E4.m3.2.2.1" xref="S2.E4.m3.2.2.2.cmml"><mi id="S2.E4.m3.1.1" xref="S2.E4.m3.1.1.cmml">tanh</mi><mo id="S2.E4.m3.2.2.1a" xref="S2.E4.m3.2.2.2.cmml">⁡</mo><mrow id="S2.E4.m3.2.2.1.1" xref="S2.E4.m3.2.2.2.cmml"><mo stretchy="false" id="S2.E4.m3.2.2.1.1.2" xref="S2.E4.m3.2.2.2.cmml">(</mo><mrow id="S2.E4.m3.2.2.1.1.1" xref="S2.E4.m3.2.2.1.1.1.cmml"><mrow id="S2.E4.m3.2.2.1.1.1.2" xref="S2.E4.m3.2.2.1.1.1.2.cmml"><msub id="S2.E4.m3.2.2.1.1.1.2.2" xref="S2.E4.m3.2.2.1.1.1.2.2.cmml"><mi id="S2.E4.m3.2.2.1.1.1.2.2.2" xref="S2.E4.m3.2.2.1.1.1.2.2.2.cmml">W</mi><mrow id="S2.E4.m3.2.2.1.1.1.2.2.3" xref="S2.E4.m3.2.2.1.1.1.2.2.3.cmml"><mi id="S2.E4.m3.2.2.1.1.1.2.2.3.2" xref="S2.E4.m3.2.2.1.1.1.2.2.3.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.E4.m3.2.2.1.1.1.2.2.3.1" xref="S2.E4.m3.2.2.1.1.1.2.2.3.1.cmml">​</mo><mi id="S2.E4.m3.2.2.1.1.1.2.2.3.3" xref="S2.E4.m3.2.2.1.1.1.2.2.3.3.cmml">g</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E4.m3.2.2.1.1.1.2.1" xref="S2.E4.m3.2.2.1.1.1.2.1.cmml">​</mo><msub id="S2.E4.m3.2.2.1.1.1.2.3" xref="S2.E4.m3.2.2.1.1.1.2.3.cmml"><mi id="S2.E4.m3.2.2.1.1.1.2.3.2" xref="S2.E4.m3.2.2.1.1.1.2.3.2.cmml">x</mi><mi id="S2.E4.m3.2.2.1.1.1.2.3.3" xref="S2.E4.m3.2.2.1.1.1.2.3.3.cmml">t</mi></msub></mrow><mo id="S2.E4.m3.2.2.1.1.1.1" xref="S2.E4.m3.2.2.1.1.1.1.cmml">+</mo><mrow id="S2.E4.m3.2.2.1.1.1.3" xref="S2.E4.m3.2.2.1.1.1.3.cmml"><msub id="S2.E4.m3.2.2.1.1.1.3.2" xref="S2.E4.m3.2.2.1.1.1.3.2.cmml"><mi id="S2.E4.m3.2.2.1.1.1.3.2.2" xref="S2.E4.m3.2.2.1.1.1.3.2.2.cmml">W</mi><mrow id="S2.E4.m3.2.2.1.1.1.3.2.3" xref="S2.E4.m3.2.2.1.1.1.3.2.3.cmml"><mi id="S2.E4.m3.2.2.1.1.1.3.2.3.2" xref="S2.E4.m3.2.2.1.1.1.3.2.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.E4.m3.2.2.1.1.1.3.2.3.1" xref="S2.E4.m3.2.2.1.1.1.3.2.3.1.cmml">​</mo><mi id="S2.E4.m3.2.2.1.1.1.3.2.3.3" xref="S2.E4.m3.2.2.1.1.1.3.2.3.3.cmml">g</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E4.m3.2.2.1.1.1.3.1" xref="S2.E4.m3.2.2.1.1.1.3.1.cmml">​</mo><msub id="S2.E4.m3.2.2.1.1.1.3.3" xref="S2.E4.m3.2.2.1.1.1.3.3.cmml"><mi id="S2.E4.m3.2.2.1.1.1.3.3.2" xref="S2.E4.m3.2.2.1.1.1.3.3.2.cmml">h</mi><mrow id="S2.E4.m3.2.2.1.1.1.3.3.3" xref="S2.E4.m3.2.2.1.1.1.3.3.3.cmml"><mi id="S2.E4.m3.2.2.1.1.1.3.3.3.2" xref="S2.E4.m3.2.2.1.1.1.3.3.3.2.cmml">t</mi><mo id="S2.E4.m3.2.2.1.1.1.3.3.3.1" xref="S2.E4.m3.2.2.1.1.1.3.3.3.1.cmml">−</mo><mn id="S2.E4.m3.2.2.1.1.1.3.3.3.3" xref="S2.E4.m3.2.2.1.1.1.3.3.3.3.cmml">1</mn></mrow></msub></mrow><mo id="S2.E4.m3.2.2.1.1.1.1a" xref="S2.E4.m3.2.2.1.1.1.1.cmml">+</mo><mrow id="S2.E4.m3.2.2.1.1.1.4" xref="S2.E4.m3.2.2.1.1.1.4.cmml"><msub id="S2.E4.m3.2.2.1.1.1.4.2" xref="S2.E4.m3.2.2.1.1.1.4.2.cmml"><mi id="S2.E4.m3.2.2.1.1.1.4.2.2" xref="S2.E4.m3.2.2.1.1.1.4.2.2.cmml">W</mi><mrow id="S2.E4.m3.2.2.1.1.1.4.2.3" xref="S2.E4.m3.2.2.1.1.1.4.2.3.cmml"><mi id="S2.E4.m3.2.2.1.1.1.4.2.3.2" xref="S2.E4.m3.2.2.1.1.1.4.2.3.2.cmml">z</mi><mo lspace="0em" rspace="0em" id="S2.E4.m3.2.2.1.1.1.4.2.3.1" xref="S2.E4.m3.2.2.1.1.1.4.2.3.1.cmml">​</mo><mi id="S2.E4.m3.2.2.1.1.1.4.2.3.3" xref="S2.E4.m3.2.2.1.1.1.4.2.3.3.cmml">g</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E4.m3.2.2.1.1.1.4.1" xref="S2.E4.m3.2.2.1.1.1.4.1.cmml">​</mo><msub id="S2.E4.m3.2.2.1.1.1.4.3" xref="S2.E4.m3.2.2.1.1.1.4.3.cmml"><mi id="S2.E4.m3.2.2.1.1.1.4.3.2" xref="S2.E4.m3.2.2.1.1.1.4.3.2.cmml">z</mi><mi id="S2.E4.m3.2.2.1.1.1.4.3.3" xref="S2.E4.m3.2.2.1.1.1.4.3.3.cmml">t</mi></msub></mrow><mo id="S2.E4.m3.2.2.1.1.1.1b" xref="S2.E4.m3.2.2.1.1.1.1.cmml">+</mo><msub id="S2.E4.m3.2.2.1.1.1.5" xref="S2.E4.m3.2.2.1.1.1.5.cmml"><mi id="S2.E4.m3.2.2.1.1.1.5.2" xref="S2.E4.m3.2.2.1.1.1.5.2.cmml">b</mi><mi id="S2.E4.m3.2.2.1.1.1.5.3" xref="S2.E4.m3.2.2.1.1.1.5.3.cmml">c</mi></msub></mrow><mo stretchy="false" id="S2.E4.m3.2.2.1.1.3" xref="S2.E4.m3.2.2.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m3.2b"><apply id="S2.E4.m3.2.2.2.cmml" xref="S2.E4.m3.2.2.1"><tanh id="S2.E4.m3.1.1.cmml" xref="S2.E4.m3.1.1"></tanh><apply id="S2.E4.m3.2.2.1.1.1.cmml" xref="S2.E4.m3.2.2.1.1.1"><plus id="S2.E4.m3.2.2.1.1.1.1.cmml" xref="S2.E4.m3.2.2.1.1.1.1"></plus><apply id="S2.E4.m3.2.2.1.1.1.2.cmml" xref="S2.E4.m3.2.2.1.1.1.2"><times id="S2.E4.m3.2.2.1.1.1.2.1.cmml" xref="S2.E4.m3.2.2.1.1.1.2.1"></times><apply id="S2.E4.m3.2.2.1.1.1.2.2.cmml" xref="S2.E4.m3.2.2.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E4.m3.2.2.1.1.1.2.2.1.cmml" xref="S2.E4.m3.2.2.1.1.1.2.2">subscript</csymbol><ci id="S2.E4.m3.2.2.1.1.1.2.2.2.cmml" xref="S2.E4.m3.2.2.1.1.1.2.2.2">𝑊</ci><apply id="S2.E4.m3.2.2.1.1.1.2.2.3.cmml" xref="S2.E4.m3.2.2.1.1.1.2.2.3"><times id="S2.E4.m3.2.2.1.1.1.2.2.3.1.cmml" xref="S2.E4.m3.2.2.1.1.1.2.2.3.1"></times><ci id="S2.E4.m3.2.2.1.1.1.2.2.3.2.cmml" xref="S2.E4.m3.2.2.1.1.1.2.2.3.2">𝑥</ci><ci id="S2.E4.m3.2.2.1.1.1.2.2.3.3.cmml" xref="S2.E4.m3.2.2.1.1.1.2.2.3.3">𝑔</ci></apply></apply><apply id="S2.E4.m3.2.2.1.1.1.2.3.cmml" xref="S2.E4.m3.2.2.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E4.m3.2.2.1.1.1.2.3.1.cmml" xref="S2.E4.m3.2.2.1.1.1.2.3">subscript</csymbol><ci id="S2.E4.m3.2.2.1.1.1.2.3.2.cmml" xref="S2.E4.m3.2.2.1.1.1.2.3.2">𝑥</ci><ci id="S2.E4.m3.2.2.1.1.1.2.3.3.cmml" xref="S2.E4.m3.2.2.1.1.1.2.3.3">𝑡</ci></apply></apply><apply id="S2.E4.m3.2.2.1.1.1.3.cmml" xref="S2.E4.m3.2.2.1.1.1.3"><times id="S2.E4.m3.2.2.1.1.1.3.1.cmml" xref="S2.E4.m3.2.2.1.1.1.3.1"></times><apply id="S2.E4.m3.2.2.1.1.1.3.2.cmml" xref="S2.E4.m3.2.2.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E4.m3.2.2.1.1.1.3.2.1.cmml" xref="S2.E4.m3.2.2.1.1.1.3.2">subscript</csymbol><ci id="S2.E4.m3.2.2.1.1.1.3.2.2.cmml" xref="S2.E4.m3.2.2.1.1.1.3.2.2">𝑊</ci><apply id="S2.E4.m3.2.2.1.1.1.3.2.3.cmml" xref="S2.E4.m3.2.2.1.1.1.3.2.3"><times id="S2.E4.m3.2.2.1.1.1.3.2.3.1.cmml" xref="S2.E4.m3.2.2.1.1.1.3.2.3.1"></times><ci id="S2.E4.m3.2.2.1.1.1.3.2.3.2.cmml" xref="S2.E4.m3.2.2.1.1.1.3.2.3.2">ℎ</ci><ci id="S2.E4.m3.2.2.1.1.1.3.2.3.3.cmml" xref="S2.E4.m3.2.2.1.1.1.3.2.3.3">𝑔</ci></apply></apply><apply id="S2.E4.m3.2.2.1.1.1.3.3.cmml" xref="S2.E4.m3.2.2.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E4.m3.2.2.1.1.1.3.3.1.cmml" xref="S2.E4.m3.2.2.1.1.1.3.3">subscript</csymbol><ci id="S2.E4.m3.2.2.1.1.1.3.3.2.cmml" xref="S2.E4.m3.2.2.1.1.1.3.3.2">ℎ</ci><apply id="S2.E4.m3.2.2.1.1.1.3.3.3.cmml" xref="S2.E4.m3.2.2.1.1.1.3.3.3"><minus id="S2.E4.m3.2.2.1.1.1.3.3.3.1.cmml" xref="S2.E4.m3.2.2.1.1.1.3.3.3.1"></minus><ci id="S2.E4.m3.2.2.1.1.1.3.3.3.2.cmml" xref="S2.E4.m3.2.2.1.1.1.3.3.3.2">𝑡</ci><cn type="integer" id="S2.E4.m3.2.2.1.1.1.3.3.3.3.cmml" xref="S2.E4.m3.2.2.1.1.1.3.3.3.3">1</cn></apply></apply></apply><apply id="S2.E4.m3.2.2.1.1.1.4.cmml" xref="S2.E4.m3.2.2.1.1.1.4"><times id="S2.E4.m3.2.2.1.1.1.4.1.cmml" xref="S2.E4.m3.2.2.1.1.1.4.1"></times><apply id="S2.E4.m3.2.2.1.1.1.4.2.cmml" xref="S2.E4.m3.2.2.1.1.1.4.2"><csymbol cd="ambiguous" id="S2.E4.m3.2.2.1.1.1.4.2.1.cmml" xref="S2.E4.m3.2.2.1.1.1.4.2">subscript</csymbol><ci id="S2.E4.m3.2.2.1.1.1.4.2.2.cmml" xref="S2.E4.m3.2.2.1.1.1.4.2.2">𝑊</ci><apply id="S2.E4.m3.2.2.1.1.1.4.2.3.cmml" xref="S2.E4.m3.2.2.1.1.1.4.2.3"><times id="S2.E4.m3.2.2.1.1.1.4.2.3.1.cmml" xref="S2.E4.m3.2.2.1.1.1.4.2.3.1"></times><ci id="S2.E4.m3.2.2.1.1.1.4.2.3.2.cmml" xref="S2.E4.m3.2.2.1.1.1.4.2.3.2">𝑧</ci><ci id="S2.E4.m3.2.2.1.1.1.4.2.3.3.cmml" xref="S2.E4.m3.2.2.1.1.1.4.2.3.3">𝑔</ci></apply></apply><apply id="S2.E4.m3.2.2.1.1.1.4.3.cmml" xref="S2.E4.m3.2.2.1.1.1.4.3"><csymbol cd="ambiguous" id="S2.E4.m3.2.2.1.1.1.4.3.1.cmml" xref="S2.E4.m3.2.2.1.1.1.4.3">subscript</csymbol><ci id="S2.E4.m3.2.2.1.1.1.4.3.2.cmml" xref="S2.E4.m3.2.2.1.1.1.4.3.2">𝑧</ci><ci id="S2.E4.m3.2.2.1.1.1.4.3.3.cmml" xref="S2.E4.m3.2.2.1.1.1.4.3.3">𝑡</ci></apply></apply><apply id="S2.E4.m3.2.2.1.1.1.5.cmml" xref="S2.E4.m3.2.2.1.1.1.5"><csymbol cd="ambiguous" id="S2.E4.m3.2.2.1.1.1.5.1.cmml" xref="S2.E4.m3.2.2.1.1.1.5">subscript</csymbol><ci id="S2.E4.m3.2.2.1.1.1.5.2.cmml" xref="S2.E4.m3.2.2.1.1.1.5.2">𝑏</ci><ci id="S2.E4.m3.2.2.1.1.1.5.3.cmml" xref="S2.E4.m3.2.2.1.1.1.5.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m3.2c">\displaystyle\tanh(W_{xg}x_{t}+W_{hg}h_{t-1}+W_{zg}z_{t}+b_{c})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
<tbody id="S2.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E5.m1.1" class="ltx_Math" alttext="\displaystyle c_{t}" display="inline"><semantics id="S2.E5.m1.1a"><msub id="S2.E5.m1.1.1" xref="S2.E5.m1.1.1.cmml"><mi id="S2.E5.m1.1.1.2" xref="S2.E5.m1.1.1.2.cmml">c</mi><mi id="S2.E5.m1.1.1.3" xref="S2.E5.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.E5.m1.1b"><apply id="S2.E5.m1.1.1.cmml" xref="S2.E5.m1.1.1"><csymbol cd="ambiguous" id="S2.E5.m1.1.1.1.cmml" xref="S2.E5.m1.1.1">subscript</csymbol><ci id="S2.E5.m1.1.1.2.cmml" xref="S2.E5.m1.1.1.2">𝑐</ci><ci id="S2.E5.m1.1.1.3.cmml" xref="S2.E5.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.1c">\displaystyle c_{t}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math id="S2.E5.m2.1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics id="S2.E5.m2.1a"><mo id="S2.E5.m2.1.1" xref="S2.E5.m2.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S2.E5.m2.1b"><eq id="S2.E5.m2.1.1.cmml" xref="S2.E5.m2.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m2.1c">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E5.m3.1" class="ltx_Math" alttext="\displaystyle f_{t}\odot c_{t-1}+i_{t}\odot g_{t}" display="inline"><semantics id="S2.E5.m3.1a"><mrow id="S2.E5.m3.1.1" xref="S2.E5.m3.1.1.cmml"><mrow id="S2.E5.m3.1.1.2" xref="S2.E5.m3.1.1.2.cmml"><msub id="S2.E5.m3.1.1.2.2" xref="S2.E5.m3.1.1.2.2.cmml"><mi id="S2.E5.m3.1.1.2.2.2" xref="S2.E5.m3.1.1.2.2.2.cmml">f</mi><mi id="S2.E5.m3.1.1.2.2.3" xref="S2.E5.m3.1.1.2.2.3.cmml">t</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.E5.m3.1.1.2.1" xref="S2.E5.m3.1.1.2.1.cmml">⊙</mo><msub id="S2.E5.m3.1.1.2.3" xref="S2.E5.m3.1.1.2.3.cmml"><mi id="S2.E5.m3.1.1.2.3.2" xref="S2.E5.m3.1.1.2.3.2.cmml">c</mi><mrow id="S2.E5.m3.1.1.2.3.3" xref="S2.E5.m3.1.1.2.3.3.cmml"><mi id="S2.E5.m3.1.1.2.3.3.2" xref="S2.E5.m3.1.1.2.3.3.2.cmml">t</mi><mo id="S2.E5.m3.1.1.2.3.3.1" xref="S2.E5.m3.1.1.2.3.3.1.cmml">−</mo><mn id="S2.E5.m3.1.1.2.3.3.3" xref="S2.E5.m3.1.1.2.3.3.3.cmml">1</mn></mrow></msub></mrow><mo id="S2.E5.m3.1.1.1" xref="S2.E5.m3.1.1.1.cmml">+</mo><mrow id="S2.E5.m3.1.1.3" xref="S2.E5.m3.1.1.3.cmml"><msub id="S2.E5.m3.1.1.3.2" xref="S2.E5.m3.1.1.3.2.cmml"><mi id="S2.E5.m3.1.1.3.2.2" xref="S2.E5.m3.1.1.3.2.2.cmml">i</mi><mi id="S2.E5.m3.1.1.3.2.3" xref="S2.E5.m3.1.1.3.2.3.cmml">t</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.E5.m3.1.1.3.1" xref="S2.E5.m3.1.1.3.1.cmml">⊙</mo><msub id="S2.E5.m3.1.1.3.3" xref="S2.E5.m3.1.1.3.3.cmml"><mi id="S2.E5.m3.1.1.3.3.2" xref="S2.E5.m3.1.1.3.3.2.cmml">g</mi><mi id="S2.E5.m3.1.1.3.3.3" xref="S2.E5.m3.1.1.3.3.3.cmml">t</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m3.1b"><apply id="S2.E5.m3.1.1.cmml" xref="S2.E5.m3.1.1"><plus id="S2.E5.m3.1.1.1.cmml" xref="S2.E5.m3.1.1.1"></plus><apply id="S2.E5.m3.1.1.2.cmml" xref="S2.E5.m3.1.1.2"><csymbol cd="latexml" id="S2.E5.m3.1.1.2.1.cmml" xref="S2.E5.m3.1.1.2.1">direct-product</csymbol><apply id="S2.E5.m3.1.1.2.2.cmml" xref="S2.E5.m3.1.1.2.2"><csymbol cd="ambiguous" id="S2.E5.m3.1.1.2.2.1.cmml" xref="S2.E5.m3.1.1.2.2">subscript</csymbol><ci id="S2.E5.m3.1.1.2.2.2.cmml" xref="S2.E5.m3.1.1.2.2.2">𝑓</ci><ci id="S2.E5.m3.1.1.2.2.3.cmml" xref="S2.E5.m3.1.1.2.2.3">𝑡</ci></apply><apply id="S2.E5.m3.1.1.2.3.cmml" xref="S2.E5.m3.1.1.2.3"><csymbol cd="ambiguous" id="S2.E5.m3.1.1.2.3.1.cmml" xref="S2.E5.m3.1.1.2.3">subscript</csymbol><ci id="S2.E5.m3.1.1.2.3.2.cmml" xref="S2.E5.m3.1.1.2.3.2">𝑐</ci><apply id="S2.E5.m3.1.1.2.3.3.cmml" xref="S2.E5.m3.1.1.2.3.3"><minus id="S2.E5.m3.1.1.2.3.3.1.cmml" xref="S2.E5.m3.1.1.2.3.3.1"></minus><ci id="S2.E5.m3.1.1.2.3.3.2.cmml" xref="S2.E5.m3.1.1.2.3.3.2">𝑡</ci><cn type="integer" id="S2.E5.m3.1.1.2.3.3.3.cmml" xref="S2.E5.m3.1.1.2.3.3.3">1</cn></apply></apply></apply><apply id="S2.E5.m3.1.1.3.cmml" xref="S2.E5.m3.1.1.3"><csymbol cd="latexml" id="S2.E5.m3.1.1.3.1.cmml" xref="S2.E5.m3.1.1.3.1">direct-product</csymbol><apply id="S2.E5.m3.1.1.3.2.cmml" xref="S2.E5.m3.1.1.3.2"><csymbol cd="ambiguous" id="S2.E5.m3.1.1.3.2.1.cmml" xref="S2.E5.m3.1.1.3.2">subscript</csymbol><ci id="S2.E5.m3.1.1.3.2.2.cmml" xref="S2.E5.m3.1.1.3.2.2">𝑖</ci><ci id="S2.E5.m3.1.1.3.2.3.cmml" xref="S2.E5.m3.1.1.3.2.3">𝑡</ci></apply><apply id="S2.E5.m3.1.1.3.3.cmml" xref="S2.E5.m3.1.1.3.3"><csymbol cd="ambiguous" id="S2.E5.m3.1.1.3.3.1.cmml" xref="S2.E5.m3.1.1.3.3">subscript</csymbol><ci id="S2.E5.m3.1.1.3.3.2.cmml" xref="S2.E5.m3.1.1.3.3.2">𝑔</ci><ci id="S2.E5.m3.1.1.3.3.3.cmml" xref="S2.E5.m3.1.1.3.3.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m3.1c">\displaystyle f_{t}\odot c_{t-1}+i_{t}\odot g_{t}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
<tbody id="S2.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E6.m1.1" class="ltx_Math" alttext="\displaystyle h_{t}" display="inline"><semantics id="S2.E6.m1.1a"><msub id="S2.E6.m1.1.1" xref="S2.E6.m1.1.1.cmml"><mi id="S2.E6.m1.1.1.2" xref="S2.E6.m1.1.1.2.cmml">h</mi><mi id="S2.E6.m1.1.1.3" xref="S2.E6.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.E6.m1.1b"><apply id="S2.E6.m1.1.1.cmml" xref="S2.E6.m1.1.1"><csymbol cd="ambiguous" id="S2.E6.m1.1.1.1.cmml" xref="S2.E6.m1.1.1">subscript</csymbol><ci id="S2.E6.m1.1.1.2.cmml" xref="S2.E6.m1.1.1.2">ℎ</ci><ci id="S2.E6.m1.1.1.3.cmml" xref="S2.E6.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E6.m1.1c">\displaystyle h_{t}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math id="S2.E6.m2.1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics id="S2.E6.m2.1a"><mo id="S2.E6.m2.1.1" xref="S2.E6.m2.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S2.E6.m2.1b"><eq id="S2.E6.m2.1.1.cmml" xref="S2.E6.m2.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S2.E6.m2.1c">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E6.m3.2" class="ltx_Math" alttext="\displaystyle o_{t}\odot\tanh(c_{t})" display="inline"><semantics id="S2.E6.m3.2a"><mrow id="S2.E6.m3.2.2" xref="S2.E6.m3.2.2.cmml"><msub id="S2.E6.m3.2.2.3" xref="S2.E6.m3.2.2.3.cmml"><mi id="S2.E6.m3.2.2.3.2" xref="S2.E6.m3.2.2.3.2.cmml">o</mi><mi id="S2.E6.m3.2.2.3.3" xref="S2.E6.m3.2.2.3.3.cmml">t</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.E6.m3.2.2.2" xref="S2.E6.m3.2.2.2.cmml">⊙</mo><mrow id="S2.E6.m3.2.2.1.1" xref="S2.E6.m3.2.2.1.2.cmml"><mi id="S2.E6.m3.1.1" xref="S2.E6.m3.1.1.cmml">tanh</mi><mo id="S2.E6.m3.2.2.1.1a" xref="S2.E6.m3.2.2.1.2.cmml">⁡</mo><mrow id="S2.E6.m3.2.2.1.1.1" xref="S2.E6.m3.2.2.1.2.cmml"><mo stretchy="false" id="S2.E6.m3.2.2.1.1.1.2" xref="S2.E6.m3.2.2.1.2.cmml">(</mo><msub id="S2.E6.m3.2.2.1.1.1.1" xref="S2.E6.m3.2.2.1.1.1.1.cmml"><mi id="S2.E6.m3.2.2.1.1.1.1.2" xref="S2.E6.m3.2.2.1.1.1.1.2.cmml">c</mi><mi id="S2.E6.m3.2.2.1.1.1.1.3" xref="S2.E6.m3.2.2.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="S2.E6.m3.2.2.1.1.1.3" xref="S2.E6.m3.2.2.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E6.m3.2b"><apply id="S2.E6.m3.2.2.cmml" xref="S2.E6.m3.2.2"><csymbol cd="latexml" id="S2.E6.m3.2.2.2.cmml" xref="S2.E6.m3.2.2.2">direct-product</csymbol><apply id="S2.E6.m3.2.2.3.cmml" xref="S2.E6.m3.2.2.3"><csymbol cd="ambiguous" id="S2.E6.m3.2.2.3.1.cmml" xref="S2.E6.m3.2.2.3">subscript</csymbol><ci id="S2.E6.m3.2.2.3.2.cmml" xref="S2.E6.m3.2.2.3.2">𝑜</ci><ci id="S2.E6.m3.2.2.3.3.cmml" xref="S2.E6.m3.2.2.3.3">𝑡</ci></apply><apply id="S2.E6.m3.2.2.1.2.cmml" xref="S2.E6.m3.2.2.1.1"><tanh id="S2.E6.m3.1.1.cmml" xref="S2.E6.m3.1.1"></tanh><apply id="S2.E6.m3.2.2.1.1.1.1.cmml" xref="S2.E6.m3.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.E6.m3.2.2.1.1.1.1.1.cmml" xref="S2.E6.m3.2.2.1.1.1.1">subscript</csymbol><ci id="S2.E6.m3.2.2.1.1.1.1.2.cmml" xref="S2.E6.m3.2.2.1.1.1.1.2">𝑐</ci><ci id="S2.E6.m3.2.2.1.1.1.1.3.cmml" xref="S2.E6.m3.2.2.1.1.1.1.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E6.m3.2c">\displaystyle o_{t}\odot\tanh(c_{t})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.SSS0.Px2.p1.9" class="ltx_p">where <math id="S2.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S2.SS2.SSS0.Px2.p1.1.m1.1a"><mi id="S2.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.1.m1.1b"><ci id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.1.m1.1c">\sigma</annotation></semantics></math> is a sigmoid nonlinearity, and <math id="S2.SS2.SSS0.Px2.p1.2.m2.4" class="ltx_Math" alttext="i_{t},f_{t},c_{t},o_{t}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p1.2.m2.4a"><mrow id="S2.SS2.SSS0.Px2.p1.2.m2.4.4.4" xref="S2.SS2.SSS0.Px2.p1.2.m2.4.4.5.cmml"><msub id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.1.1" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.2" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.2.cmml">i</mi><mi id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.3" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.3.cmml">t</mi></msub><mo id="S2.SS2.SSS0.Px2.p1.2.m2.4.4.4.5" xref="S2.SS2.SSS0.Px2.p1.2.m2.4.4.5.cmml">,</mo><msub id="S2.SS2.SSS0.Px2.p1.2.m2.2.2.2.2" xref="S2.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.cmml"><mi id="S2.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2" xref="S2.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.cmml">f</mi><mi id="S2.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.3" xref="S2.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.3.cmml">t</mi></msub><mo id="S2.SS2.SSS0.Px2.p1.2.m2.4.4.4.6" xref="S2.SS2.SSS0.Px2.p1.2.m2.4.4.5.cmml">,</mo><msub id="S2.SS2.SSS0.Px2.p1.2.m2.3.3.3.3" xref="S2.SS2.SSS0.Px2.p1.2.m2.3.3.3.3.cmml"><mi id="S2.SS2.SSS0.Px2.p1.2.m2.3.3.3.3.2" xref="S2.SS2.SSS0.Px2.p1.2.m2.3.3.3.3.2.cmml">c</mi><mi id="S2.SS2.SSS0.Px2.p1.2.m2.3.3.3.3.3" xref="S2.SS2.SSS0.Px2.p1.2.m2.3.3.3.3.3.cmml">t</mi></msub><mo id="S2.SS2.SSS0.Px2.p1.2.m2.4.4.4.7" xref="S2.SS2.SSS0.Px2.p1.2.m2.4.4.5.cmml">,</mo><msub id="S2.SS2.SSS0.Px2.p1.2.m2.4.4.4.4" xref="S2.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.cmml"><mi id="S2.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.2" xref="S2.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.2.cmml">o</mi><mi id="S2.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.3" xref="S2.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.2.m2.4b"><list id="S2.SS2.SSS0.Px2.p1.2.m2.4.4.5.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.4.4.4"><apply id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.2">𝑖</ci><ci id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.1.1.3">𝑡</ci></apply><apply id="S2.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.1.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.2.2.2.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.2">𝑓</ci><ci id="S2.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.3.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.2.2.2.2.3">𝑡</ci></apply><apply id="S2.SS2.SSS0.Px2.p1.2.m2.3.3.3.3.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.2.m2.3.3.3.3.1.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.3.3.3.3">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.2.m2.3.3.3.3.2.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.3.3.3.3.2">𝑐</ci><ci id="S2.SS2.SSS0.Px2.p1.2.m2.3.3.3.3.3.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.3.3.3.3.3">𝑡</ci></apply><apply id="S2.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.4.4.4.4"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.1.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.4.4.4.4">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.2.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.2">𝑜</ci><ci id="S2.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.3.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.4.4.4.4.3">𝑡</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.2.m2.4c">i_{t},f_{t},c_{t},o_{t}</annotation></semantics></math> are the input, forget, memory, output state of the LSTM. The various <math id="S2.SS2.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S2.SS2.SSS0.Px2.p1.3.m3.1a"><mi id="S2.SS2.SSS0.Px2.p1.3.m3.1.1" xref="S2.SS2.SSS0.Px2.p1.3.m3.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.3.m3.1b"><ci id="S2.SS2.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.3.m3.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.3.m3.1c">W</annotation></semantics></math> and <math id="S2.SS2.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S2.SS2.SSS0.Px2.p1.4.m4.1a"><mi id="S2.SS2.SSS0.Px2.p1.4.m4.1.1" xref="S2.SS2.SSS0.Px2.p1.4.m4.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.4.m4.1b"><ci id="S2.SS2.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.4.m4.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.4.m4.1c">b</annotation></semantics></math> matrices are trained parameters and <math id="S2.SS2.SSS0.Px2.p1.5.m5.1" class="ltx_Math" alttext="\odot" display="inline"><semantics id="S2.SS2.SSS0.Px2.p1.5.m5.1a"><mo id="S2.SS2.SSS0.Px2.p1.5.m5.1.1" xref="S2.SS2.SSS0.Px2.p1.5.m5.1.1.cmml">⊙</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.5.m5.1b"><csymbol cd="latexml" id="S2.SS2.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.5.m5.1.1">direct-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.5.m5.1c">\odot</annotation></semantics></math> represents element-wise products with gate values. <math id="S2.SS2.SSS0.Px2.p1.6.m6.1" class="ltx_Math" alttext="x_{t}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p1.6.m6.1a"><msub id="S2.SS2.SSS0.Px2.p1.6.m6.1.1" xref="S2.SS2.SSS0.Px2.p1.6.m6.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p1.6.m6.1.1.2" xref="S2.SS2.SSS0.Px2.p1.6.m6.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px2.p1.6.m6.1.1.3" xref="S2.SS2.SSS0.Px2.p1.6.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.6.m6.1b"><apply id="S2.SS2.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.6.m6.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.6.m6.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.6.m6.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px2.p1.6.m6.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p1.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.6.m6.1c">x_{t}</annotation></semantics></math> is the input (<em id="S2.SS2.SSS0.Px2.p1.9.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS2.SSS0.Px2.p1.9.2" class="ltx_text"></span> a word of a question) and <math id="S2.SS2.SSS0.Px2.p1.7.m7.1" class="ltx_Math" alttext="h_{t}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p1.7.m7.1a"><msub id="S2.SS2.SSS0.Px2.p1.7.m7.1.1" xref="S2.SS2.SSS0.Px2.p1.7.m7.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p1.7.m7.1.1.2" xref="S2.SS2.SSS0.Px2.p1.7.m7.1.1.2.cmml">h</mi><mi id="S2.SS2.SSS0.Px2.p1.7.m7.1.1.3" xref="S2.SS2.SSS0.Px2.p1.7.m7.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.7.m7.1b"><apply id="S2.SS2.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.7.m7.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.7.m7.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.7.m7.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.7.m7.1.1.2">ℎ</ci><ci id="S2.SS2.SSS0.Px2.p1.7.m7.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p1.7.m7.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.7.m7.1c">h_{t}</annotation></semantics></math> is the hidden state at time step <math id="S2.SS2.SSS0.Px2.p1.8.m8.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS2.SSS0.Px2.p1.8.m8.1a"><mi id="S2.SS2.SSS0.Px2.p1.8.m8.1.1" xref="S2.SS2.SSS0.Px2.p1.8.m8.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.8.m8.1b"><ci id="S2.SS2.SSS0.Px2.p1.8.m8.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.8.m8.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.8.m8.1c">t</annotation></semantics></math>. The attention mechanism is introduced by the term <math id="S2.SS2.SSS0.Px2.p1.9.m9.1" class="ltx_Math" alttext="z_{t}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p1.9.m9.1a"><msub id="S2.SS2.SSS0.Px2.p1.9.m9.1.1" xref="S2.SS2.SSS0.Px2.p1.9.m9.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p1.9.m9.1.1.2" xref="S2.SS2.SSS0.Px2.p1.9.m9.1.1.2.cmml">z</mi><mi id="S2.SS2.SSS0.Px2.p1.9.m9.1.1.3" xref="S2.SS2.SSS0.Px2.p1.9.m9.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.9.m9.1b"><apply id="S2.SS2.SSS0.Px2.p1.9.m9.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.9.m9.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.9.m9.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.9.m9.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.9.m9.1.1.2">𝑧</ci><ci id="S2.SS2.SSS0.Px2.p1.9.m9.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p1.9.m9.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.9.m9.1c">z_{t}</annotation></semantics></math> , which is a weighted average of convolutional features that depends upon the previous hidden state and the convolutional features:</p>
<table id="Sx1.EGx2" class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table">

<tbody id="S2.E7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E7.m1.1" class="ltx_Math" alttext="\displaystyle e_{t}" display="inline"><semantics id="S2.E7.m1.1a"><msub id="S2.E7.m1.1.1" xref="S2.E7.m1.1.1.cmml"><mi id="S2.E7.m1.1.1.2" xref="S2.E7.m1.1.1.2.cmml">e</mi><mi id="S2.E7.m1.1.1.3" xref="S2.E7.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.E7.m1.1b"><apply id="S2.E7.m1.1.1.cmml" xref="S2.E7.m1.1.1"><csymbol cd="ambiguous" id="S2.E7.m1.1.1.1.cmml" xref="S2.E7.m1.1.1">subscript</csymbol><ci id="S2.E7.m1.1.1.2.cmml" xref="S2.E7.m1.1.1.2">𝑒</ci><ci id="S2.E7.m1.1.1.3.cmml" xref="S2.E7.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E7.m1.1c">\displaystyle e_{t}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math id="S2.E7.m2.1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics id="S2.E7.m2.1a"><mo id="S2.E7.m2.1.1" xref="S2.E7.m2.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S2.E7.m2.1b"><eq id="S2.E7.m2.1.1.cmml" xref="S2.E7.m2.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S2.E7.m2.1c">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E7.m3.3" class="ltx_Math" alttext="\displaystyle w_{a}^{T}\tanh(W_{he}h_{t-1}+W_{ce}C(I))+b_{a}" display="inline"><semantics id="S2.E7.m3.3a"><mrow id="S2.E7.m3.3.3" xref="S2.E7.m3.3.3.cmml"><mrow id="S2.E7.m3.3.3.1" xref="S2.E7.m3.3.3.1.cmml"><msubsup id="S2.E7.m3.3.3.1.3" xref="S2.E7.m3.3.3.1.3.cmml"><mi id="S2.E7.m3.3.3.1.3.2.2" xref="S2.E7.m3.3.3.1.3.2.2.cmml">w</mi><mi id="S2.E7.m3.3.3.1.3.2.3" xref="S2.E7.m3.3.3.1.3.2.3.cmml">a</mi><mi id="S2.E7.m3.3.3.1.3.3" xref="S2.E7.m3.3.3.1.3.3.cmml">T</mi></msubsup><mo lspace="0.167em" rspace="0em" id="S2.E7.m3.3.3.1.2" xref="S2.E7.m3.3.3.1.2.cmml">​</mo><mrow id="S2.E7.m3.3.3.1.1.1" xref="S2.E7.m3.3.3.1.1.2.cmml"><mi id="S2.E7.m3.2.2" xref="S2.E7.m3.2.2.cmml">tanh</mi><mo id="S2.E7.m3.3.3.1.1.1a" xref="S2.E7.m3.3.3.1.1.2.cmml">⁡</mo><mrow id="S2.E7.m3.3.3.1.1.1.1" xref="S2.E7.m3.3.3.1.1.2.cmml"><mo stretchy="false" id="S2.E7.m3.3.3.1.1.1.1.2" xref="S2.E7.m3.3.3.1.1.2.cmml">(</mo><mrow id="S2.E7.m3.3.3.1.1.1.1.1" xref="S2.E7.m3.3.3.1.1.1.1.1.cmml"><mrow id="S2.E7.m3.3.3.1.1.1.1.1.2" xref="S2.E7.m3.3.3.1.1.1.1.1.2.cmml"><msub id="S2.E7.m3.3.3.1.1.1.1.1.2.2" xref="S2.E7.m3.3.3.1.1.1.1.1.2.2.cmml"><mi id="S2.E7.m3.3.3.1.1.1.1.1.2.2.2" xref="S2.E7.m3.3.3.1.1.1.1.1.2.2.2.cmml">W</mi><mrow id="S2.E7.m3.3.3.1.1.1.1.1.2.2.3" xref="S2.E7.m3.3.3.1.1.1.1.1.2.2.3.cmml"><mi id="S2.E7.m3.3.3.1.1.1.1.1.2.2.3.2" xref="S2.E7.m3.3.3.1.1.1.1.1.2.2.3.2.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.E7.m3.3.3.1.1.1.1.1.2.2.3.1" xref="S2.E7.m3.3.3.1.1.1.1.1.2.2.3.1.cmml">​</mo><mi id="S2.E7.m3.3.3.1.1.1.1.1.2.2.3.3" xref="S2.E7.m3.3.3.1.1.1.1.1.2.2.3.3.cmml">e</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E7.m3.3.3.1.1.1.1.1.2.1" xref="S2.E7.m3.3.3.1.1.1.1.1.2.1.cmml">​</mo><msub id="S2.E7.m3.3.3.1.1.1.1.1.2.3" xref="S2.E7.m3.3.3.1.1.1.1.1.2.3.cmml"><mi id="S2.E7.m3.3.3.1.1.1.1.1.2.3.2" xref="S2.E7.m3.3.3.1.1.1.1.1.2.3.2.cmml">h</mi><mrow id="S2.E7.m3.3.3.1.1.1.1.1.2.3.3" xref="S2.E7.m3.3.3.1.1.1.1.1.2.3.3.cmml"><mi id="S2.E7.m3.3.3.1.1.1.1.1.2.3.3.2" xref="S2.E7.m3.3.3.1.1.1.1.1.2.3.3.2.cmml">t</mi><mo id="S2.E7.m3.3.3.1.1.1.1.1.2.3.3.1" xref="S2.E7.m3.3.3.1.1.1.1.1.2.3.3.1.cmml">−</mo><mn id="S2.E7.m3.3.3.1.1.1.1.1.2.3.3.3" xref="S2.E7.m3.3.3.1.1.1.1.1.2.3.3.3.cmml">1</mn></mrow></msub></mrow><mo id="S2.E7.m3.3.3.1.1.1.1.1.1" xref="S2.E7.m3.3.3.1.1.1.1.1.1.cmml">+</mo><mrow id="S2.E7.m3.3.3.1.1.1.1.1.3" xref="S2.E7.m3.3.3.1.1.1.1.1.3.cmml"><msub id="S2.E7.m3.3.3.1.1.1.1.1.3.2" xref="S2.E7.m3.3.3.1.1.1.1.1.3.2.cmml"><mi id="S2.E7.m3.3.3.1.1.1.1.1.3.2.2" xref="S2.E7.m3.3.3.1.1.1.1.1.3.2.2.cmml">W</mi><mrow id="S2.E7.m3.3.3.1.1.1.1.1.3.2.3" xref="S2.E7.m3.3.3.1.1.1.1.1.3.2.3.cmml"><mi id="S2.E7.m3.3.3.1.1.1.1.1.3.2.3.2" xref="S2.E7.m3.3.3.1.1.1.1.1.3.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.E7.m3.3.3.1.1.1.1.1.3.2.3.1" xref="S2.E7.m3.3.3.1.1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S2.E7.m3.3.3.1.1.1.1.1.3.2.3.3" xref="S2.E7.m3.3.3.1.1.1.1.1.3.2.3.3.cmml">e</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E7.m3.3.3.1.1.1.1.1.3.1" xref="S2.E7.m3.3.3.1.1.1.1.1.3.1.cmml">​</mo><mi id="S2.E7.m3.3.3.1.1.1.1.1.3.3" xref="S2.E7.m3.3.3.1.1.1.1.1.3.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.E7.m3.3.3.1.1.1.1.1.3.1a" xref="S2.E7.m3.3.3.1.1.1.1.1.3.1.cmml">​</mo><mrow id="S2.E7.m3.3.3.1.1.1.1.1.3.4.2" xref="S2.E7.m3.3.3.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S2.E7.m3.3.3.1.1.1.1.1.3.4.2.1" xref="S2.E7.m3.3.3.1.1.1.1.1.3.cmml">(</mo><mi id="S2.E7.m3.1.1" xref="S2.E7.m3.1.1.cmml">I</mi><mo stretchy="false" id="S2.E7.m3.3.3.1.1.1.1.1.3.4.2.2" xref="S2.E7.m3.3.3.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S2.E7.m3.3.3.1.1.1.1.3" xref="S2.E7.m3.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E7.m3.3.3.2" xref="S2.E7.m3.3.3.2.cmml">+</mo><msub id="S2.E7.m3.3.3.3" xref="S2.E7.m3.3.3.3.cmml"><mi id="S2.E7.m3.3.3.3.2" xref="S2.E7.m3.3.3.3.2.cmml">b</mi><mi id="S2.E7.m3.3.3.3.3" xref="S2.E7.m3.3.3.3.3.cmml">a</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.E7.m3.3b"><apply id="S2.E7.m3.3.3.cmml" xref="S2.E7.m3.3.3"><plus id="S2.E7.m3.3.3.2.cmml" xref="S2.E7.m3.3.3.2"></plus><apply id="S2.E7.m3.3.3.1.cmml" xref="S2.E7.m3.3.3.1"><times id="S2.E7.m3.3.3.1.2.cmml" xref="S2.E7.m3.3.3.1.2"></times><apply id="S2.E7.m3.3.3.1.3.cmml" xref="S2.E7.m3.3.3.1.3"><csymbol cd="ambiguous" id="S2.E7.m3.3.3.1.3.1.cmml" xref="S2.E7.m3.3.3.1.3">superscript</csymbol><apply id="S2.E7.m3.3.3.1.3.2.cmml" xref="S2.E7.m3.3.3.1.3"><csymbol cd="ambiguous" id="S2.E7.m3.3.3.1.3.2.1.cmml" xref="S2.E7.m3.3.3.1.3">subscript</csymbol><ci id="S2.E7.m3.3.3.1.3.2.2.cmml" xref="S2.E7.m3.3.3.1.3.2.2">𝑤</ci><ci id="S2.E7.m3.3.3.1.3.2.3.cmml" xref="S2.E7.m3.3.3.1.3.2.3">𝑎</ci></apply><ci id="S2.E7.m3.3.3.1.3.3.cmml" xref="S2.E7.m3.3.3.1.3.3">𝑇</ci></apply><apply id="S2.E7.m3.3.3.1.1.2.cmml" xref="S2.E7.m3.3.3.1.1.1"><tanh id="S2.E7.m3.2.2.cmml" xref="S2.E7.m3.2.2"></tanh><apply id="S2.E7.m3.3.3.1.1.1.1.1.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1"><plus id="S2.E7.m3.3.3.1.1.1.1.1.1.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.1"></plus><apply id="S2.E7.m3.3.3.1.1.1.1.1.2.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.2"><times id="S2.E7.m3.3.3.1.1.1.1.1.2.1.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.2.1"></times><apply id="S2.E7.m3.3.3.1.1.1.1.1.2.2.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E7.m3.3.3.1.1.1.1.1.2.2.1.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.2.2">subscript</csymbol><ci id="S2.E7.m3.3.3.1.1.1.1.1.2.2.2.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.2.2.2">𝑊</ci><apply id="S2.E7.m3.3.3.1.1.1.1.1.2.2.3.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.2.2.3"><times id="S2.E7.m3.3.3.1.1.1.1.1.2.2.3.1.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.2.2.3.1"></times><ci id="S2.E7.m3.3.3.1.1.1.1.1.2.2.3.2.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.2.2.3.2">ℎ</ci><ci id="S2.E7.m3.3.3.1.1.1.1.1.2.2.3.3.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.2.2.3.3">𝑒</ci></apply></apply><apply id="S2.E7.m3.3.3.1.1.1.1.1.2.3.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E7.m3.3.3.1.1.1.1.1.2.3.1.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.E7.m3.3.3.1.1.1.1.1.2.3.2.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.2.3.2">ℎ</ci><apply id="S2.E7.m3.3.3.1.1.1.1.1.2.3.3.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.2.3.3"><minus id="S2.E7.m3.3.3.1.1.1.1.1.2.3.3.1.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.2.3.3.1"></minus><ci id="S2.E7.m3.3.3.1.1.1.1.1.2.3.3.2.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.2.3.3.2">𝑡</ci><cn type="integer" id="S2.E7.m3.3.3.1.1.1.1.1.2.3.3.3.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.2.3.3.3">1</cn></apply></apply></apply><apply id="S2.E7.m3.3.3.1.1.1.1.1.3.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.3"><times id="S2.E7.m3.3.3.1.1.1.1.1.3.1.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.3.1"></times><apply id="S2.E7.m3.3.3.1.1.1.1.1.3.2.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E7.m3.3.3.1.1.1.1.1.3.2.1.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E7.m3.3.3.1.1.1.1.1.3.2.2.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.3.2.2">𝑊</ci><apply id="S2.E7.m3.3.3.1.1.1.1.1.3.2.3.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.3.2.3"><times id="S2.E7.m3.3.3.1.1.1.1.1.3.2.3.1.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.3.2.3.1"></times><ci id="S2.E7.m3.3.3.1.1.1.1.1.3.2.3.2.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.3.2.3.2">𝑐</ci><ci id="S2.E7.m3.3.3.1.1.1.1.1.3.2.3.3.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.3.2.3.3">𝑒</ci></apply></apply><ci id="S2.E7.m3.3.3.1.1.1.1.1.3.3.cmml" xref="S2.E7.m3.3.3.1.1.1.1.1.3.3">𝐶</ci><ci id="S2.E7.m3.1.1.cmml" xref="S2.E7.m3.1.1">𝐼</ci></apply></apply></apply></apply><apply id="S2.E7.m3.3.3.3.cmml" xref="S2.E7.m3.3.3.3"><csymbol cd="ambiguous" id="S2.E7.m3.3.3.3.1.cmml" xref="S2.E7.m3.3.3.3">subscript</csymbol><ci id="S2.E7.m3.3.3.3.2.cmml" xref="S2.E7.m3.3.3.3.2">𝑏</ci><ci id="S2.E7.m3.3.3.3.3.cmml" xref="S2.E7.m3.3.3.3.3">𝑎</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E7.m3.3c">\displaystyle w_{a}^{T}\tanh(W_{he}h_{t-1}+W_{ce}C(I))+b_{a}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
<tbody id="S2.E8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E8.m1.1" class="ltx_Math" alttext="\displaystyle a_{t}" display="inline"><semantics id="S2.E8.m1.1a"><msub id="S2.E8.m1.1.1" xref="S2.E8.m1.1.1.cmml"><mi id="S2.E8.m1.1.1.2" xref="S2.E8.m1.1.1.2.cmml">a</mi><mi id="S2.E8.m1.1.1.3" xref="S2.E8.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.E8.m1.1b"><apply id="S2.E8.m1.1.1.cmml" xref="S2.E8.m1.1.1"><csymbol cd="ambiguous" id="S2.E8.m1.1.1.1.cmml" xref="S2.E8.m1.1.1">subscript</csymbol><ci id="S2.E8.m1.1.1.2.cmml" xref="S2.E8.m1.1.1.2">𝑎</ci><ci id="S2.E8.m1.1.1.3.cmml" xref="S2.E8.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E8.m1.1c">\displaystyle a_{t}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math id="S2.E8.m2.1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics id="S2.E8.m2.1a"><mo id="S2.E8.m2.1.1" xref="S2.E8.m2.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S2.E8.m2.1b"><eq id="S2.E8.m2.1.1.cmml" xref="S2.E8.m2.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S2.E8.m2.1c">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E8.m3.1" class="ltx_Math" alttext="\displaystyle{\rm softmax}(e_{t})" display="inline"><semantics id="S2.E8.m3.1a"><mrow id="S2.E8.m3.1.1" xref="S2.E8.m3.1.1.cmml"><mi id="S2.E8.m3.1.1.3" xref="S2.E8.m3.1.1.3.cmml">softmax</mi><mo lspace="0em" rspace="0em" id="S2.E8.m3.1.1.2" xref="S2.E8.m3.1.1.2.cmml">​</mo><mrow id="S2.E8.m3.1.1.1.1" xref="S2.E8.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E8.m3.1.1.1.1.2" xref="S2.E8.m3.1.1.1.1.1.cmml">(</mo><msub id="S2.E8.m3.1.1.1.1.1" xref="S2.E8.m3.1.1.1.1.1.cmml"><mi id="S2.E8.m3.1.1.1.1.1.2" xref="S2.E8.m3.1.1.1.1.1.2.cmml">e</mi><mi id="S2.E8.m3.1.1.1.1.1.3" xref="S2.E8.m3.1.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="S2.E8.m3.1.1.1.1.3" xref="S2.E8.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E8.m3.1b"><apply id="S2.E8.m3.1.1.cmml" xref="S2.E8.m3.1.1"><times id="S2.E8.m3.1.1.2.cmml" xref="S2.E8.m3.1.1.2"></times><ci id="S2.E8.m3.1.1.3.cmml" xref="S2.E8.m3.1.1.3">softmax</ci><apply id="S2.E8.m3.1.1.1.1.1.cmml" xref="S2.E8.m3.1.1.1.1"><csymbol cd="ambiguous" id="S2.E8.m3.1.1.1.1.1.1.cmml" xref="S2.E8.m3.1.1.1.1">subscript</csymbol><ci id="S2.E8.m3.1.1.1.1.1.2.cmml" xref="S2.E8.m3.1.1.1.1.1.2">𝑒</ci><ci id="S2.E8.m3.1.1.1.1.1.3.cmml" xref="S2.E8.m3.1.1.1.1.1.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E8.m3.1c">\displaystyle{\rm softmax}(e_{t})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
<tbody id="S2.E9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E9.m1.1" class="ltx_Math" alttext="\displaystyle z_{t}" display="inline"><semantics id="S2.E9.m1.1a"><msub id="S2.E9.m1.1.1" xref="S2.E9.m1.1.1.cmml"><mi id="S2.E9.m1.1.1.2" xref="S2.E9.m1.1.1.2.cmml">z</mi><mi id="S2.E9.m1.1.1.3" xref="S2.E9.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.E9.m1.1b"><apply id="S2.E9.m1.1.1.cmml" xref="S2.E9.m1.1.1"><csymbol cd="ambiguous" id="S2.E9.m1.1.1.1.cmml" xref="S2.E9.m1.1.1">subscript</csymbol><ci id="S2.E9.m1.1.1.2.cmml" xref="S2.E9.m1.1.1.2">𝑧</ci><ci id="S2.E9.m1.1.1.3.cmml" xref="S2.E9.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E9.m1.1c">\displaystyle z_{t}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math id="S2.E9.m2.1" class="ltx_Math" alttext="\displaystyle=" display="inline"><semantics id="S2.E9.m2.1a"><mo id="S2.E9.m2.1.1" xref="S2.E9.m2.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S2.E9.m2.1b"><eq id="S2.E9.m2.1.1.cmml" xref="S2.E9.m2.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S2.E9.m2.1c">\displaystyle=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E9.m3.1" class="ltx_Math" alttext="\displaystyle a_{t}^{T}C(I)" display="inline"><semantics id="S2.E9.m3.1a"><mrow id="S2.E9.m3.1.2" xref="S2.E9.m3.1.2.cmml"><msubsup id="S2.E9.m3.1.2.2" xref="S2.E9.m3.1.2.2.cmml"><mi id="S2.E9.m3.1.2.2.2.2" xref="S2.E9.m3.1.2.2.2.2.cmml">a</mi><mi id="S2.E9.m3.1.2.2.2.3" xref="S2.E9.m3.1.2.2.2.3.cmml">t</mi><mi id="S2.E9.m3.1.2.2.3" xref="S2.E9.m3.1.2.2.3.cmml">T</mi></msubsup><mo lspace="0em" rspace="0em" id="S2.E9.m3.1.2.1" xref="S2.E9.m3.1.2.1.cmml">​</mo><mi id="S2.E9.m3.1.2.3" xref="S2.E9.m3.1.2.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.E9.m3.1.2.1a" xref="S2.E9.m3.1.2.1.cmml">​</mo><mrow id="S2.E9.m3.1.2.4.2" xref="S2.E9.m3.1.2.cmml"><mo stretchy="false" id="S2.E9.m3.1.2.4.2.1" xref="S2.E9.m3.1.2.cmml">(</mo><mi id="S2.E9.m3.1.1" xref="S2.E9.m3.1.1.cmml">I</mi><mo stretchy="false" id="S2.E9.m3.1.2.4.2.2" xref="S2.E9.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E9.m3.1b"><apply id="S2.E9.m3.1.2.cmml" xref="S2.E9.m3.1.2"><times id="S2.E9.m3.1.2.1.cmml" xref="S2.E9.m3.1.2.1"></times><apply id="S2.E9.m3.1.2.2.cmml" xref="S2.E9.m3.1.2.2"><csymbol cd="ambiguous" id="S2.E9.m3.1.2.2.1.cmml" xref="S2.E9.m3.1.2.2">superscript</csymbol><apply id="S2.E9.m3.1.2.2.2.cmml" xref="S2.E9.m3.1.2.2"><csymbol cd="ambiguous" id="S2.E9.m3.1.2.2.2.1.cmml" xref="S2.E9.m3.1.2.2">subscript</csymbol><ci id="S2.E9.m3.1.2.2.2.2.cmml" xref="S2.E9.m3.1.2.2.2.2">𝑎</ci><ci id="S2.E9.m3.1.2.2.2.3.cmml" xref="S2.E9.m3.1.2.2.2.3">𝑡</ci></apply><ci id="S2.E9.m3.1.2.2.3.cmml" xref="S2.E9.m3.1.2.2.3">𝑇</ci></apply><ci id="S2.E9.m3.1.2.3.cmml" xref="S2.E9.m3.1.2.3">𝐶</ci><ci id="S2.E9.m3.1.1.cmml" xref="S2.E9.m3.1.1">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E9.m3.1c">\displaystyle a_{t}^{T}C(I)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.SSS0.Px2.p1.15" class="ltx_p">where <math id="S2.SS2.SSS0.Px2.p1.10.m1.1" class="ltx_Math" alttext="C(I)" display="inline"><semantics id="S2.SS2.SSS0.Px2.p1.10.m1.1a"><mrow id="S2.SS2.SSS0.Px2.p1.10.m1.1.2" xref="S2.SS2.SSS0.Px2.p1.10.m1.1.2.cmml"><mi id="S2.SS2.SSS0.Px2.p1.10.m1.1.2.2" xref="S2.SS2.SSS0.Px2.p1.10.m1.1.2.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS0.Px2.p1.10.m1.1.2.1" xref="S2.SS2.SSS0.Px2.p1.10.m1.1.2.1.cmml">​</mo><mrow id="S2.SS2.SSS0.Px2.p1.10.m1.1.2.3.2" xref="S2.SS2.SSS0.Px2.p1.10.m1.1.2.cmml"><mo stretchy="false" id="S2.SS2.SSS0.Px2.p1.10.m1.1.2.3.2.1" xref="S2.SS2.SSS0.Px2.p1.10.m1.1.2.cmml">(</mo><mi id="S2.SS2.SSS0.Px2.p1.10.m1.1.1" xref="S2.SS2.SSS0.Px2.p1.10.m1.1.1.cmml">I</mi><mo stretchy="false" id="S2.SS2.SSS0.Px2.p1.10.m1.1.2.3.2.2" xref="S2.SS2.SSS0.Px2.p1.10.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.10.m1.1b"><apply id="S2.SS2.SSS0.Px2.p1.10.m1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.10.m1.1.2"><times id="S2.SS2.SSS0.Px2.p1.10.m1.1.2.1.cmml" xref="S2.SS2.SSS0.Px2.p1.10.m1.1.2.1"></times><ci id="S2.SS2.SSS0.Px2.p1.10.m1.1.2.2.cmml" xref="S2.SS2.SSS0.Px2.p1.10.m1.1.2.2">𝐶</ci><ci id="S2.SS2.SSS0.Px2.p1.10.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.10.m1.1.1">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.10.m1.1c">C(I)</annotation></semantics></math> represents the convolutional feature map of image <math id="S2.SS2.SSS0.Px2.p1.11.m2.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S2.SS2.SSS0.Px2.p1.11.m2.1a"><mi id="S2.SS2.SSS0.Px2.p1.11.m2.1.1" xref="S2.SS2.SSS0.Px2.p1.11.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.11.m2.1b"><ci id="S2.SS2.SSS0.Px2.p1.11.m2.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.11.m2.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.11.m2.1c">I</annotation></semantics></math>. The <span id="S2.SS2.SSS0.Px2.p1.15.1" class="ltx_text ltx_font_italic">attention term</span> <math id="S2.SS2.SSS0.Px2.p1.12.m3.1" class="ltx_Math" alttext="a_{t}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p1.12.m3.1a"><msub id="S2.SS2.SSS0.Px2.p1.12.m3.1.1" xref="S2.SS2.SSS0.Px2.p1.12.m3.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p1.12.m3.1.1.2" xref="S2.SS2.SSS0.Px2.p1.12.m3.1.1.2.cmml">a</mi><mi id="S2.SS2.SSS0.Px2.p1.12.m3.1.1.3" xref="S2.SS2.SSS0.Px2.p1.12.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.12.m3.1b"><apply id="S2.SS2.SSS0.Px2.p1.12.m3.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.12.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.12.m3.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.12.m3.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.12.m3.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.12.m3.1.1.2">𝑎</ci><ci id="S2.SS2.SSS0.Px2.p1.12.m3.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p1.12.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.12.m3.1c">a_{t}</annotation></semantics></math> sets the contribution of each convolutional feature at the <math id="S2.SS2.SSS0.Px2.p1.13.m4.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS2.SSS0.Px2.p1.13.m4.1a"><mi id="S2.SS2.SSS0.Px2.p1.13.m4.1.1" xref="S2.SS2.SSS0.Px2.p1.13.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.13.m4.1b"><ci id="S2.SS2.SSS0.Px2.p1.13.m4.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.13.m4.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.13.m4.1c">t</annotation></semantics></math>-th step. Large values in <math id="S2.SS2.SSS0.Px2.p1.14.m5.1" class="ltx_Math" alttext="a_{t}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p1.14.m5.1a"><msub id="S2.SS2.SSS0.Px2.p1.14.m5.1.1" xref="S2.SS2.SSS0.Px2.p1.14.m5.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p1.14.m5.1.1.2" xref="S2.SS2.SSS0.Px2.p1.14.m5.1.1.2.cmml">a</mi><mi id="S2.SS2.SSS0.Px2.p1.14.m5.1.1.3" xref="S2.SS2.SSS0.Px2.p1.14.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.14.m5.1b"><apply id="S2.SS2.SSS0.Px2.p1.14.m5.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.14.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.14.m5.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.14.m5.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.14.m5.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.14.m5.1.1.2">𝑎</ci><ci id="S2.SS2.SSS0.Px2.p1.14.m5.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p1.14.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.14.m5.1c">a_{t}</annotation></semantics></math> indicate more relevance of the corresponding region to the question. In this formulation, a standard LSTM can be considered as a special case with values in <math id="S2.SS2.SSS0.Px2.p1.15.m6.1" class="ltx_Math" alttext="a_{t}" display="inline"><semantics id="S2.SS2.SSS0.Px2.p1.15.m6.1a"><msub id="S2.SS2.SSS0.Px2.p1.15.m6.1.1" xref="S2.SS2.SSS0.Px2.p1.15.m6.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p1.15.m6.1.1.2" xref="S2.SS2.SSS0.Px2.p1.15.m6.1.1.2.cmml">a</mi><mi id="S2.SS2.SSS0.Px2.p1.15.m6.1.1.3" xref="S2.SS2.SSS0.Px2.p1.15.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.15.m6.1b"><apply id="S2.SS2.SSS0.Px2.p1.15.m6.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.15.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.15.m6.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.15.m6.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.15.m6.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.15.m6.1.1.2">𝑎</ci><ci id="S2.SS2.SSS0.Px2.p1.15.m6.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p1.15.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.15.m6.1c">a_{t}</annotation></semantics></math> set uniformly, <em id="S2.SS2.SSS0.Px2.p1.15.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.SS2.SSS0.Px2.p1.15.3" class="ltx_text"></span> each region contributing equally. A similar mechanism as above was employed by Jiang <em id="S2.SS2.SSS0.Px2.p1.15.4" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.SSS0.Px2.p1.15.5" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
<div id="S2.SS2.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS2.SSS0.Px2.p2.1" class="ltx_p">Chen <em id="S2.SS2.SSS0.Px2.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.SSS0.Px2.p2.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> use a mechanism different from the above word-guided attention. They generate a “question-guided attention map” (QAM) by searching for visual features that correspond to the semantics of the input question in the spatial image feature map. The search is achieved by convolving the visual feature map with a configurable convolutional kernel. This kernel is generated by transforming the question embeddings from the semantic space into the visual space, which contains the visual information determined by the intent of the question. Yang <em id="S2.SS2.SSS0.Px2.p2.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.SSS0.Px2.p2.1.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite> also employ this scheme with “stacked attention networks” (SAN) that infer the answer iteratively. Xu <em id="S2.SS2.SSS0.Px2.p2.1.5" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.SSS0.Px2.p2.1.6" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> propose a “multi-hop image attention scheme” (SMem). The first hop is a word-guided attention, while a second hop is question-guided. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, the authors generate image regions with object proposals and then select regions relevant to the question and possible answer choices. Similarly, Ilievski <em id="S2.SS2.SSS0.Px2.p2.1.7" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.SSS0.Px2.p2.1.8" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> employ off-the-shelf object detectors to identify regions related to the key words of the question and then fuse information from those regions with global features with an LSTM. Lu <em id="S2.SS2.SSS0.Px2.p2.1.9" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.SSS0.Px2.p2.1.10" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> present a “hierarchical co-attention model” (HieCoAtt) that jointly reasons about image and question attention. Whereas the works described above focus only on visual attention, HieCoAtt processes image and question symmetrically, in the sense that the image representation guides attention over the question and vice versa. Most recently, Fukui <em id="S2.SS2.SSS0.Px2.p2.1.11" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.SSS0.Px2.p2.1.12" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> combine the attention mechanism into their “Multimodal Compact Bilinear pooling” (MCB) already mentioned in Section <a href="#S2.SS1" title="2.1 Joint embedding approaches ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>.</p>
</div>
<div id="S2.SS2.SSS0.Px2.p3" class="ltx_para">
<p id="S2.SS2.SSS0.Px2.p3.1" class="ltx_p">Andreas <em id="S2.SS2.SSS0.Px2.p3.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.SSS0.Px2.p3.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> employ attention mechanisms in a different manner. They propose a compositional model that builds a neural network from modules tailored to each question, as described in more details in Section <a href="#S2.SS3.SSS1" title="2.3.1 Neural Module Networks ‣ 2.3 Compositional Models ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3.1</span></a>. Most of these modules operate in the space of attentions, either producing an attention map from an image (<em id="S2.SS2.SSS0.Px2.p3.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.SS2.SSS0.Px2.p3.1.4" class="ltx_text"></span> identifying a salient object), performing unary operations (<em id="S2.SS2.SSS0.Px2.p3.1.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS2.SSS0.Px2.p3.1.6" class="ltx_text"></span> inverting the attention from an object to the context around it), or interactions between attentions (<em id="S2.SS2.SSS0.Px2.p3.1.7" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS2.SSS0.Px2.p3.1.8" class="ltx_text"></span> a subtracting an attention map from another).</p>
</div>
</section>
<section id="S2.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Performance and limitations</h5>

<div id="S2.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px3.p1.1" class="ltx_p">The reported uses of attention mechanisms always improve over models that use global image features. For example, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> show that the attention-enhanced LSTM described above outperforms the “VIS+LSTM” model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> in both “Telling” and “Grounding” tasks of the ‘Visual7W’ dataset (see Section <a href="#S3.SS1" title="3.1 Datasets of natural images ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>). The multiple attention layers of SAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite> bring further improvements over only one layer of attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, especially on the VQA dataset. The HieCoAtt model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> shows benefit from the hierarchical representation of the question and also from the co-attention mechanism (question-guided visual attention and image-guided question attention).</p>
</div>
<div id="S2.SS2.SSS0.Px3.p2" class="ltx_para">
<p id="S2.SS2.SSS0.Px3.p2.1" class="ltx_p">Interestingly, attention mechanisms improve the overall accuracy on all VQA datasets, but closer inspection by question type show little or no benefit on binary (yes/no) questions. One hypothesis is that binary questions typically require longer chains of reasoning, whereas open-ended questions often require identifying and naming only one concept from the image. Therefore, improving on binary questions will likely require other innovations than visual attention. The output in end-to-end joint embedding approaches – regardless of the use of attention – is produced by a simple mapping from the co-embedded visual and textual features to the answer, learned over a large number of training examples. Little insight is available as to how an output answer arises. It can be debated whether any “reasoning” is performed and/or encoded in the mapping. Another important issue is raised by asking whether questions can be answered from the given visual input alone. Oftentimes, they require prior knowledge ranging common sense to subject-specific and even expert-level knowledge. How such information can be provided to VQA systems and incorporated into the reasoning is still an open question (see Section <a href="#S2.SS4" title="2.4 Models using external knowledge bases ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a>).</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_font_italic ltx_title_subsection">2.3   Compositional Models</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The methods discussed so far present limitations related to the monolithic nature of the CNNs and RNNs used to extract representations of images and sentences. An increasingly popular research direction in the design of artificial neural networks is to consider modular architectures. This approach involves connecting distinct modules designed for specific desired capabilities such as memory or specific types of reasoning. A potential advantage is a better use of supervision. On the one hand, it facilitates transfer learning, since a same module can be used and trained within different overall architectures and tasks (see Section <a href="#S2.SS3.SSS1" title="2.3.1 Neural Module Networks ‣ 2.3 Compositional Models ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3.1</span></a>). On the other hand, it allows to use “deep supervision”, <em id="S2.SS3.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.SS3.p1.1.2" class="ltx_text"></span> optimizing an objective that depends on the outputs of internal modules (<em id="S2.SS3.p1.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS3.p1.1.4" class="ltx_text"></span> which supporting facts an attention mechanism should focus on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>). Other models discussed in Section <a href="#S2.SS2" title="2.2 Attention mechanisms ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a> (attention models) and Section <a href="#S2.SS4" title="2.4 Models using external knowledge bases ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a> (connections to knowledge bases) also fall in the category of modular architectures. We focus here on two specific models whose main contribution is in the modular aspect, namely the Neural Module Networks (NMN) and the Dynamic Memory Networks (DMN).</p>
</div>
<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_font_italic ltx_title_subsubsection">2.3.1   Neural Module Networks</h4>

<figure id="S2.F2" class="ltx_figure"><img src="/html/1607.05910/assets/x2.png" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="228" height="78" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The Neural Module Networks (NMN, Section <a href="#S2.SS3.SSS1" title="2.3.1 Neural Module Networks ‣ 2.3 Compositional Models ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3.1</span></a>) leverage the compositional structure of questions, <em id="S2.F2.9.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.F2.10.2" class="ltx_text"></span> here “Is there a red shape above a circle ?” from the <span id="S2.F2.11.3" class="ltx_text ltx_font_italic">Shapes</span> dataset (Section <a href="#S3.SS4.SSS0.Px2" title="Shapes ‣ 3.4 Other datasets ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>). The parsing of the question leads to assembling modules that operate in the space of attentions. Two <span id="S2.F2.12.4" class="ltx_text ltx_font_typewriter">attend</span> modules locate red shapes and circles, <span id="S2.F2.13.5" class="ltx_text ltx_font_typewriter">re-attend[above]</span> shifts the attention above the circles, <span id="S2.F2.14.6" class="ltx_text ltx_font_typewriter">combine</span> computes their intersection, and <span id="S2.F2.15.7" class="ltx_text ltx_font_typewriter">measure[is]</span> inspects the final attention and determines that it is non-empty (figure adapted from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>).</figcaption>
</figure>
<section id="S2.SS3.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Motivation</h5>

<div id="S2.SS3.SSS1.Px1.p1" class="ltx_para">
<p id="S2.SS3.SSS1.Px1.p1.1" class="ltx_p">The Neural Module Networks (NMN) are introduced by Andreas <em id="S2.SS3.SSS1.Px1.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>. in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and extended in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. They are specifically designed for VQA, with the intention of exploiting the compositional linguistic structure of the questions. Questions vary greatly in the level of complexity. For example, <span id="S2.SS3.SSS1.Px1.p1.1.2" class="ltx_text ltx_font_italic">Is this a truck ?</span> only requires retrieving one piece of information from the image, whereas <span id="S2.SS3.SSS1.Px1.p1.1.3" class="ltx_text ltx_font_italic">How many objects are to the left of the toaster ?</span> requires multiple processing steps, such as recognition and counting. NMNs reflect the complexity of a question in a network that is assembled on-the-fly for each instance of the problem. The tactic is related to approaches in textual QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> that use semantic parsers to turn questions into logical expressions. A significant contribution of NMNs is to apply this logical reasoning over continuous visual features, instead of discrete or logical predicates.</p>
</div>
</section>
<section id="S2.SS3.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Method</h5>

<div id="S2.SS3.SSS1.Px2.p1" class="ltx_para">
<p id="S2.SS3.SSS1.Px2.p1.1" class="ltx_p">The method is based on a semantic parsing of the question using a well-known tool in the NLP community. The parse tree is turned into an assembly of modules from a predefined set, which are then used together to answer the question. Crucially, all modules are independent and composable (Figure <a href="#S2.F2" title="Figure 2 ‣ 2.3.1 Neural Module Networks ‣ 2.3 Compositional Models ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). In other words, the computation performed will be different for each problem instance, and a problem instance at test time may use a set of modules that were not seen together during training.</p>
</div>
<div id="S2.SS3.SSS1.Px2.p2" class="ltx_para">
<p id="S2.SS3.SSS1.Px2.p2.1" class="ltx_p">The inputs and outputs of the modules can be of three types: image features, attentions (regions) over images, and labels (classification decisions). A set of possible modules is predefined, each by its type of input and output, but their exact behavior will be acquired through end-to-end training on specific problem instances. The training therefore does not need additional supervision than triples of images, questions, and answers.</p>
</div>
<div id="S2.SS3.SSS1.Px2.p3" class="ltx_para">
<p id="S2.SS3.SSS1.Px2.p3.1" class="ltx_p">The parsing of the question is a crucial step, which is performed with the Standford dependency parser <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> which basically identifies grammatical relations between parts of the sentence. The authors of the NMNs then use ad hoc hand-written rules to deterministically transform parse trees into structured queries, in the form of compositions of modules <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. In their second paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, they additionally learn a ranking function to select the best structures from candidate parses. The whole procedure still uses strong simplifying assumptions about language in the question. The visual features are provided by a fixed, pre-trained VGG CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS3.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Performance and limitations</h5>

<div id="S2.SS3.SSS1.Px3.p1" class="ltx_para">
<p id="S2.SS3.SSS1.Px3.p1.1" class="ltx_p">The Neural Module Networks were evaluated on the VQA benchmark, and shows different strengths and weaknesses than competing approaches. It generally outperforms competitors on questions with a compositional structure, <em id="S2.SS3.SSS1.Px3.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS3.SSS1.Px3.p1.1.2" class="ltx_text"></span> requiring an object to be located and one of its attributes described. However, many of questions in the VQA dataset are quite simple, and require little composition or reasoning. The authors introduced a new dataset, named “Shapes”, of synthetic images (Section <a href="#S3.SS4.SSS0.Px2" title="Shapes ‣ 3.4 Other datasets ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>) paired with complex questions involving spatial relations, set-theoretic reasoning, and shape and attribute recognition.</p>
</div>
<div id="S2.SS3.SSS1.Px3.p2" class="ltx_para">
<p id="S2.SS3.SSS1.Px3.p2.1" class="ltx_p">The limitations of the method are inherent to the bottleneck formed during the parsing of the question. This stage fixes the network structure and errors cannot be recovered from. Moreover, the assembly of modules uses aggressive simplification of the questions that discards some grammatical cues. As a workaround, the authors obtain the final answer by averaging their output with the one from a classical LSTM question encoder.</p>
</div>
<div id="S2.SS3.SSS1.Px3.p3" class="ltx_para">
<p id="S2.SS3.SSS1.Px3.p3.1" class="ltx_p">The potential of the NMNs is dimmed in practice by the lack of truly complex questions in the VQA benchmark. The results reported on this dataset use a restricted subset of possbible modules, presumably to avoid over-fitting. Results on the synthetic Shapes dataset show that semantic structure prediction does improve generalization in deep networks. The overall approach presents the potential of addressing the combinatorial explosion of concepts and relations that can arise in open-world VQA. Finally, note that the general formulation of NMNs can encompass other approaches, including the memory networks presented below, which may be formulated as a composition of “attention” and “classifier” modules.</p>
</div>
</section>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_font_italic ltx_title_subsubsection">2.3.2   Dynamic Memory Networks</h4>

<section id="S2.SS3.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Motivation</h5>

<div id="S2.SS3.SSS2.Px1.p1" class="ltx_para">
<p id="S2.SS3.SSS2.Px1.p1.1" class="ltx_p">The Dynamic Memory Networks (DMN) are neural networks with a particular modular architecture. They were described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, with a number of variants proposed concurrently <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. Most of these were applied to textual QA. We focus here on the work of Xiong <em id="S2.SS3.SSS2.Px1.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS3.SSS2.Px1.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>, who adapted them to VQA. DMNs fall into the broader category of memory-augmented networks, which perform read and write operations on an internal representation of the input. This mechanism, similarly to attention (see Section <a href="#S2.SS2" title="2.2 Attention mechanisms ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>), is designed to address tasks that require complex logical reasoning by modeling interaction between multiple parts of the data over several passes.</p>
</div>
</section>
<section id="S2.SS3.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Method</h5>

<div id="S2.SS3.SSS2.Px2.p1" class="ltx_para">
<p id="S2.SS3.SSS2.Px2.p1.1" class="ltx_p">The dynamic memory networks are composed of 4 main modules <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> that allow independence in their particular implementation. The input module transforms the input data into a set of vectors called “facts”. Its implementation (described below) varies depending on the type of input data. The question module computes a vector representation of the question, using a gated recurrent unit (GRU, a variant of LSTM). The episodic memory module retrieves the facts required to answer the question. They key is to allow the episodic memory module to pass multiple times over the facts to allow transitive reasoning. It incorporates an attention mechanism that selects relevant facts and an update mechanism that generates a new memory representation from interactions between its current state and the retrieved facts. The first state is initialized with the representation from the question module. Finally, the answer module uses the final state of the memory and the question to predict the output with a multinomial classification for single words, or a GRU for datasets where a longer sentence is required.</p>
</div>
<div id="S2.SS3.SSS2.Px2.p2" class="ltx_para">
<p id="S2.SS3.SSS2.Px2.p2.1" class="ltx_p">The input module for VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> extracts image features with a VGG CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> over small image patches. These features are fed to a GRU in the manner of the words of a sentence, traversing the image in a snake-like fashion. This is an ad hoc adaptation of the original input module in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> that used a GRU to process words of sentences. The episodic memory module also includes an attention mechanism to focus on particular image regions.</p>
</div>
<div id="S2.SS3.SSS2.Px2.p3" class="ltx_para">
<p id="S2.SS3.SSS2.Px2.p3.1" class="ltx_p">Let us also mention here the work of Noh <em id="S2.SS3.SSS2.Px2.p3.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS3.SSS2.Px2.p3.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. Their approach has similarities with memory networks in the use of an internal memory-like unit, over which multiple passes are performed. The main novelty is the use of a loss over each of these passes, instead of a single one on the final results. After training, the inference at test time is performed using only one such pass.</p>
</div>
</section>
<section id="S2.SS3.SSS2.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Performance and limitations</h5>

<div id="S2.SS3.SSS2.Px3.p1" class="ltx_para">
<p id="S2.SS3.SSS2.Px3.p1.1" class="ltx_p">The dynamic memory networks were evaluated on the DAQUAR and VQA benchmarks, and show competitive performance for all types of questions. Compared to Neural Module Networks (Section <a href="#S2.SS3.SSS1" title="2.3.1 Neural Module Networks ‣ 2.3 Compositional Models ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3.1</span></a>), they perform similarly on yes/no questions, slightly worse on numerical questions, but markedly better on all other types of questions. The issue with counting likely arises from the limited granularity of the fixed image patches, which may cross object boundaries.</p>
</div>
<div id="S2.SS3.SSS2.Px3.p2" class="ltx_para">
<p id="S2.SS3.SSS2.Px3.p2.1" class="ltx_p">Interestingly, the paper presents competitive results on both VQA and text-based QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> using essentially a same method, except for the input module. The text QA dataset used <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> requires inference over multiple facts, which is a positive indicator of the reasoning capabilities of this model. A potential criticism for applying a same model to text and images stems from the intrinsically different nature of sequences of words, and sequences of image patches. The temporal dimension of a textual narrative is different than relative geometrical positions, although both seem to be handled adequately by GRUs in practice.</p>
</div>
</section>
</section>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_font_italic ltx_title_subsection">2.4   Models using external knowledge bases</h3>

<section id="S2.SS4.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Motivation</h5>

<div id="S2.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS4.SSS0.Px1.p1.1" class="ltx_p">The task of VQA involves understanding the contents of images, but often requires prior non-visual, information, which can range from “common sense” to topic-specific or even encyclopedic knowledge. For example, to answer the question “How many mammals appear in this image ?”, one must understand the word “mammal” and know which animals belong to this category. This observation allows pinpointing two major weaknesses of the joint embedding approaches (Section <a href="#S2.SS1" title="2.1 Joint embedding approaches ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>). First, they can only capture knowledge that is present in the training set, and it is obvious than efforts at scaling up datasets will never reach a complete coverage of the real world. Second, the neural networks trained in such approaches have a limited capacity, which is also inevitably deemed to be surpassed by the amount of information we wish to learn.</p>
</div>
<div id="S2.SS4.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS4.SSS0.Px1.p2.1" class="ltx_p">An alternative is to decouple the reasoning (<em id="S2.SS4.SSS0.Px1.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS4.SSS0.Px1.p2.1.2" class="ltx_text"></span> as a neural network) from the actual storage of data or knowledge. A substantial amount of research has been devoted to structured representations of knowledge. This led to the development of large-scale Knowledge Bases (KB) such as
DBpedia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>,
Freebase <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>,
YAGO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>,
OpenIE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>,
NELL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>,
WebChild <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>,
and ConceptNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>.
These databases store common sense and factual knowledge in a machine readable fashion. Each piece of knowledge, referred to as a fact, is typically represented as a triple <span id="S2.SS4.SSS0.Px1.p2.1.3" class="ltx_text ltx_font_typewriter">(arg1,rel,arg2)</span>, where <span id="S2.SS4.SSS0.Px1.p2.1.4" class="ltx_text ltx_font_typewriter">arg1</span> and <span id="S2.SS4.SSS0.Px1.p2.1.5" class="ltx_text ltx_font_typewriter">arg2</span> represent two concepts and <span id="S2.SS4.SSS0.Px1.p2.1.6" class="ltx_text ltx_font_typewriter">rel</span> represents a relationship between them. The collection of such facts forms a interlinked graph, which is often described according to a Resource Description Framework (RDF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>) specification and can be accessed by query languages such as SPARQL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>. Linking such knowledge bases to VQA methods allows separating the reasoning from the representation of prior knowledge in a practical and scalable manner.</p>
</div>
</section>
<section id="S2.SS4.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Method</h5>

<div id="S2.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS4.SSS0.Px2.p1.1" class="ltx_p">Wang <em id="S2.SS4.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS4.SSS0.Px2.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> propose a VQA framework named “Ahab” that uses DBpedia, one of the largest structured knowledge bases. Visual concepts are first extracted from the given image with CNNs, and they are then associated with nodes from DBpedia that represent similar concepts. Whereas the joint embedding approaches (Section <a href="#S2.SS1" title="2.1 Joint embedding approaches ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>) learn a mapping from images/questions to answers, the authors propose here to learn a mapping images/questions to <em id="S2.SS4.SSS0.Px2.p1.1.3" class="ltx_emph ltx_font_italic">queries</em> over the constructed knowledge graph. The final answer is obtained by summarizing the results of this query. The main limitation of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> is to handle limited types of questions. Although the questions can be provided in natual language, they are parsed using manually designed templates. An improved method named FVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> uses an LSTM and a data-driven approach to learn the mapping of images/questions to queries. This work also uses two additional knowledge bases, ConceptNet and WebChild.</p>
</div>
<div id="S2.SS4.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS4.SSS0.Px2.p2.1" class="ltx_p">An interesting byproduct of the explicit representation of knowledge is that the above methods can indicate how they arrived to the answer by providing the chain of reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> or the supporting facts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> used in the inference process. This contrasts with monolithic neural networks which provide little insight into the computations performed to produce their final answer.</p>
</div>
<div id="S2.SS4.SSS0.Px2.p3" class="ltx_para">
<p id="S2.SS4.SSS0.Px2.p3.1" class="ltx_p">Wu <em id="S2.SS4.SSS0.Px2.p3.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS4.SSS0.Px2.p3.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> proposed a joint embedding approach that also benefits from external knowledge bases. Given an image, they first extract semantic attributes with a CNN. External knowledge related to these attributes is then retrieved from a version of DBpedia containing short descriptions, which are embedded into fixed-size vectors with Doc2Vec. The embedded vectors are fed into an LSTM model that interprets them with the question and finally generates an answer. This method still learns a mapping from questions to answers (as other joint embedding methods) and cannot provide information about the reasoning process.</p>
</div>
</section>
<section id="S2.SS4.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Performance and limitations</h5>

<div id="S2.SS4.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS4.SSS0.Px3.p1.4" class="ltx_p">Both Ahab and FVQA focus specifically on visual questions requiring external knowledge. Most existing VQA datasets include a majority of questions that require little amount of prior knowledge, and performance on these datasets thus poorly reflect the particular capabilities of these methods. The authors of those two methods thus include evaluation on new small-scale datasets (see Section <a href="#S3.SS3" title="3.3 Knowledge base-enhanced datasets ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>). Ahab <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> significantly outperforms joint embedding approaches on its KB-VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> in terms of overall accuracy (<math id="S2.SS4.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="69.6\%" display="inline"><semantics id="S2.SS4.SSS0.Px3.p1.1.m1.1a"><mrow id="S2.SS4.SSS0.Px3.p1.1.m1.1.1" xref="S2.SS4.SSS0.Px3.p1.1.m1.1.1.cmml"><mn id="S2.SS4.SSS0.Px3.p1.1.m1.1.1.2" xref="S2.SS4.SSS0.Px3.p1.1.m1.1.1.2.cmml">69.6</mn><mo id="S2.SS4.SSS0.Px3.p1.1.m1.1.1.1" xref="S2.SS4.SSS0.Px3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px3.p1.1.m1.1b"><apply id="S2.SS4.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="latexml" id="S2.SS4.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S2.SS4.SSS0.Px3.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S2.SS4.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S2.SS4.SSS0.Px3.p1.1.m1.1.1.2">69.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px3.p1.1.m1.1c">69.6\%</annotation></semantics></math> <em id="S2.SS4.SSS0.Px3.p1.4.1" class="ltx_emph ltx_font_italic">vs</em>.<span id="S2.SS4.SSS0.Px3.p1.4.2" class="ltx_text"></span> <math id="S2.SS4.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="44.5\%" display="inline"><semantics id="S2.SS4.SSS0.Px3.p1.2.m2.1a"><mrow id="S2.SS4.SSS0.Px3.p1.2.m2.1.1" xref="S2.SS4.SSS0.Px3.p1.2.m2.1.1.cmml"><mn id="S2.SS4.SSS0.Px3.p1.2.m2.1.1.2" xref="S2.SS4.SSS0.Px3.p1.2.m2.1.1.2.cmml">44.5</mn><mo id="S2.SS4.SSS0.Px3.p1.2.m2.1.1.1" xref="S2.SS4.SSS0.Px3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px3.p1.2.m2.1b"><apply id="S2.SS4.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S2.SS4.SSS0.Px3.p1.2.m2.1.1"><csymbol cd="latexml" id="S2.SS4.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S2.SS4.SSS0.Px3.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S2.SS4.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S2.SS4.SSS0.Px3.p1.2.m2.1.1.2">44.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px3.p1.2.m2.1c">44.5\%</annotation></semantics></math>). In particular, Ahab becomes significantly better than joint embedding approaches on visual questions requiring a higher level of knowledge. Similarly, the FVQA approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> also performs much better than conventional approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> in terms of overall top-1 accuracy (<math id="S2.SS4.SSS0.Px3.p1.3.m3.1" class="ltx_Math" alttext="58.19\%" display="inline"><semantics id="S2.SS4.SSS0.Px3.p1.3.m3.1a"><mrow id="S2.SS4.SSS0.Px3.p1.3.m3.1.1" xref="S2.SS4.SSS0.Px3.p1.3.m3.1.1.cmml"><mn id="S2.SS4.SSS0.Px3.p1.3.m3.1.1.2" xref="S2.SS4.SSS0.Px3.p1.3.m3.1.1.2.cmml">58.19</mn><mo id="S2.SS4.SSS0.Px3.p1.3.m3.1.1.1" xref="S2.SS4.SSS0.Px3.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px3.p1.3.m3.1b"><apply id="S2.SS4.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S2.SS4.SSS0.Px3.p1.3.m3.1.1"><csymbol cd="latexml" id="S2.SS4.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S2.SS4.SSS0.Px3.p1.3.m3.1.1.1">percent</csymbol><cn type="float" id="S2.SS4.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="S2.SS4.SSS0.Px3.p1.3.m3.1.1.2">58.19</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px3.p1.3.m3.1c">58.19\%</annotation></semantics></math> <em id="S2.SS4.SSS0.Px3.p1.4.3" class="ltx_emph ltx_font_italic">vs</em>.<span id="S2.SS4.SSS0.Px3.p1.4.4" class="ltx_text"></span> <math id="S2.SS4.SSS0.Px3.p1.4.m4.1" class="ltx_Math" alttext="23.37\%" display="inline"><semantics id="S2.SS4.SSS0.Px3.p1.4.m4.1a"><mrow id="S2.SS4.SSS0.Px3.p1.4.m4.1.1" xref="S2.SS4.SSS0.Px3.p1.4.m4.1.1.cmml"><mn id="S2.SS4.SSS0.Px3.p1.4.m4.1.1.2" xref="S2.SS4.SSS0.Px3.p1.4.m4.1.1.2.cmml">23.37</mn><mo id="S2.SS4.SSS0.Px3.p1.4.m4.1.1.1" xref="S2.SS4.SSS0.Px3.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS0.Px3.p1.4.m4.1b"><apply id="S2.SS4.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S2.SS4.SSS0.Px3.p1.4.m4.1.1"><csymbol cd="latexml" id="S2.SS4.SSS0.Px3.p1.4.m4.1.1.1.cmml" xref="S2.SS4.SSS0.Px3.p1.4.m4.1.1.1">percent</csymbol><cn type="float" id="S2.SS4.SSS0.Px3.p1.4.m4.1.1.2.cmml" xref="S2.SS4.SSS0.Px3.p1.4.m4.1.1.2">23.37</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS0.Px3.p1.4.m4.1c">23.37\%</annotation></semantics></math>). An issue in the evaluation of both of these methods is the limited number of question types and the small scale of the datasets.</p>
</div>
<div id="S2.SS4.SSS0.Px3.p2" class="ltx_para">
<p id="S2.SS4.SSS0.Px3.p2.1" class="ltx_p">The approach of Wu <em id="S2.SS4.SSS0.Px3.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> is evaluated on the Toronto COCO-QA and VQA datasets, and shows an advantage in using the external KB in terms of average accuracy.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Datasets and evaluation</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">A number of datasets have been proposed specifically for research on VQA. They contain, at the minimum, triples made of an image, a question, and its correct answer. Additional annotations are sometimes provided, such as image captions, image regions supporting the answers, or multiple-choice candidate answers. Datasets and questions within the datasets vary widely in their complexity, the amount of reasoning and of non-visual (<em id="S3.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.p1.1.2" class="ltx_text"></span> “common sense”) information required to infer the correct answer. This section contains a comprehensive comparison of the available datasets and discusses their suitability for evaluating different aspects of VQA systems. We broadly classify dataset according to their type of images (natural, clipart, synthetic). Key characteristics are summarized in Table <a href="#S3.T2" title="Table 2 ‣ 3.1 Datasets of natural images ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. See Figures <a href="#S3.T3" title="Table 3 ‣ Visual Genome and Visual7W ‣ 3.1 Datasets of natural images ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>–<a href="#S3.T5" title="Table 5 ‣ Visual Genome and Visual7W ‣ 3.1 Datasets of natural images ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> for examples from various datasets.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">A given dataset is typically used for both training and evaluating a VQA system. The open-ended nature of the task suggests however that other, large-scale sources of information would be beneficial and likely necessary train practical VQA systems. Some datasets specifically address this aspect through annotations of supporting facts in structured non-visual knowledge bases (Section <a href="#S3.SS3" title="3.3 Knowledge base-enhanced datasets ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Other datasets non-specific to VQA are worth mentioning. They target other tasks involving vision and language, such as image captioning (<em id="S3.p3.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.p3.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>), generating <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> and understanding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> referring expressions for retrieval of images and objects in natural language. Those datasets go beyond the scope of this article (see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> for a review), but are a potential source of additional training data for VQA since they combine images with textual annotations.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_font_italic ltx_title_subsection">3.1   Datasets of natural images</h3>

<div id="S3.T2" class="ltx_table ltx_transformed_outer" style="width:109.7pt;height:489.8pt;vertical-align:-0.0pt;"><div class="ltx_transformed_inner" style="width:489.8pt;transform:translate(-190.09pt,-189.59pt) rotate(-90deg) ;"><figure>
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:108.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-250.1pt,62.7pt) scale(0.464336397632995,0.464336397632995) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Source of</td>
<td id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Number of</td>
<td id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Number of</td>
<td id="S3.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Num. questions /</td>
<td id="S3.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Num. question</td>
<td id="S3.T2.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Question</td>
<td id="S3.T2.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Average quest.</td>
<td id="S3.T2.1.1.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Average ans.</td>
<td id="S3.T2.1.1.1.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Evaluation</td>
</tr>
<tr id="S3.T2.1.1.2" class="ltx_tr">
<td id="S3.T2.1.1.2.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Dataset</td>
<td id="S3.T2.1.1.2.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">images</td>
<td id="S3.T2.1.1.2.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">images</td>
<td id="S3.T2.1.1.2.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">questions</td>
<td id="S3.T2.1.1.2.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">num. images</td>
<td id="S3.T2.1.1.2.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">categories</td>
<td id="S3.T2.1.1.2.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">collection</td>
<td id="S3.T2.1.1.2.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">length</td>
<td id="S3.T2.1.1.2.9" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">length</td>
<td id="S3.T2.1.1.2.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">metrics</td>
</tr>
<tr id="S3.T2.1.1.3" class="ltx_tr">
<td id="S3.T2.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">DAQUAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>
</td>
<td id="S3.T2.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">NYU-Depth V2</td>
<td id="S3.T2.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">1,449</td>
<td id="S3.T2.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">12,468</td>
<td id="S3.T2.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">8.6</td>
<td id="S3.T2.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">4</td>
<td id="S3.T2.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Human</td>
<td id="S3.T2.1.1.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">11.5</td>
<td id="S3.T2.1.1.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">1.2</td>
<td id="S3.T2.1.1.3.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Acc. &amp; WUPS</td>
</tr>
<tr id="S3.T2.1.1.4" class="ltx_tr">
<td id="S3.T2.1.1.4.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">COCO-QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>
</td>
<td id="S3.T2.1.1.4.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">COCO</td>
<td id="S3.T2.1.1.4.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">117,684</td>
<td id="S3.T2.1.1.4.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">117,684</td>
<td id="S3.T2.1.1.4.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">1.0</td>
<td id="S3.T2.1.1.4.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">4</td>
<td id="S3.T2.1.1.4.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Automatic</td>
<td id="S3.T2.1.1.4.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">8.6</td>
<td id="S3.T2.1.1.4.9" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">1.0</td>
<td id="S3.T2.1.1.4.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Acc. &amp; WUPS</td>
</tr>
<tr id="S3.T2.1.1.5" class="ltx_tr">
<td id="S3.T2.1.1.5.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">FM-IQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</td>
<td id="S3.T2.1.1.5.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">COCO</td>
<td id="S3.T2.1.1.5.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">120,360</td>
<td id="S3.T2.1.1.5.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T2.1.1.5.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T2.1.1.5.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T2.1.1.5.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Human</td>
<td id="S3.T2.1.1.5.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T2.1.1.5.9" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T2.1.1.5.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Human</td>
</tr>
<tr id="S3.T2.1.1.6" class="ltx_tr">
<td id="S3.T2.1.1.6.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">VQA-real <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</td>
<td id="S3.T2.1.1.6.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">COCO</td>
<td id="S3.T2.1.1.6.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">204,721</td>
<td id="S3.T2.1.1.6.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">614,163</td>
<td id="S3.T2.1.1.6.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">3.0</td>
<td id="S3.T2.1.1.6.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">20+</td>
<td id="S3.T2.1.1.6.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Human</td>
<td id="S3.T2.1.1.6.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">6.2</td>
<td id="S3.T2.1.1.6.9" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">1.1</td>
<td id="S3.T2.1.1.6.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Acc. against 10 humans</td>
</tr>
<tr id="S3.T2.1.1.7" class="ltx_tr">
<td id="S3.T2.1.1.7.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S3.T2.1.1.7.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">COCO</td>
<td id="S3.T2.1.1.7.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">108,000</td>
<td id="S3.T2.1.1.7.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">1,445,322</td>
<td id="S3.T2.1.1.7.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">13.4</td>
<td id="S3.T2.1.1.7.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">7</td>
<td id="S3.T2.1.1.7.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Human</td>
<td id="S3.T2.1.1.7.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">5.7</td>
<td id="S3.T2.1.1.7.9" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">1.8</td>
<td id="S3.T2.1.1.7.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Acc.</td>
</tr>
<tr id="S3.T2.1.1.8" class="ltx_tr">
<td id="S3.T2.1.1.8.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Visual7W <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite>
</td>
<td id="S3.T2.1.1.8.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">COCO</td>
<td id="S3.T2.1.1.8.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">47,300</td>
<td id="S3.T2.1.1.8.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">327,939</td>
<td id="S3.T2.1.1.8.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">6.9</td>
<td id="S3.T2.1.1.8.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">7</td>
<td id="S3.T2.1.1.8.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Human</td>
<td id="S3.T2.1.1.8.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">6.9</td>
<td id="S3.T2.1.1.8.9" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">1.1</td>
<td id="S3.T2.1.1.8.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Acc.</td>
</tr>
<tr id="S3.T2.1.1.9" class="ltx_tr">
<td id="S3.T2.1.1.9.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Visual Madlibs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>
</td>
<td id="S3.T2.1.1.9.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">COCO</td>
<td id="S3.T2.1.1.9.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">10,738</td>
<td id="S3.T2.1.1.9.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">360,001</td>
<td id="S3.T2.1.1.9.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">33.5</td>
<td id="S3.T2.1.1.9.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">12</td>
<td id="S3.T2.1.1.9.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Human</td>
<td id="S3.T2.1.1.9.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">6.9</td>
<td id="S3.T2.1.1.9.9" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">2.0</td>
<td id="S3.T2.1.1.9.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Acc.</td>
</tr>
<tr id="S3.T2.1.1.10" class="ltx_tr">
<td id="S3.T2.1.1.10.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">VQA-abstract <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</td>
<td id="S3.T2.1.1.10.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Clipart</td>
<td id="S3.T2.1.1.10.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">50,000</td>
<td id="S3.T2.1.1.10.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">150,000</td>
<td id="S3.T2.1.1.10.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">3.0</td>
<td id="S3.T2.1.1.10.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">20+</td>
<td id="S3.T2.1.1.10.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Human</td>
<td id="S3.T2.1.1.10.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">6.2</td>
<td id="S3.T2.1.1.10.9" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">1.1</td>
<td id="S3.T2.1.1.10.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Acc.</td>
</tr>
<tr id="S3.T2.1.1.11" class="ltx_tr">
<td id="S3.T2.1.1.11.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">VQA-balanced <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>
</td>
<td id="S3.T2.1.1.11.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Clipart</td>
<td id="S3.T2.1.1.11.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">15,623</td>
<td id="S3.T2.1.1.11.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">33,379</td>
<td id="S3.T2.1.1.11.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">2.1</td>
<td id="S3.T2.1.1.11.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">1</td>
<td id="S3.T2.1.1.11.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Human</td>
<td id="S3.T2.1.1.11.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">6.2</td>
<td id="S3.T2.1.1.11.9" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">1.0</td>
<td id="S3.T2.1.1.11.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Acc.</td>
</tr>
<tr id="S3.T2.1.1.12" class="ltx_tr">
<td id="S3.T2.1.1.12.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">KB-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>
</td>
<td id="S3.T2.1.1.12.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">COCO</td>
<td id="S3.T2.1.1.12.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">700</td>
<td id="S3.T2.1.1.12.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">2,402</td>
<td id="S3.T2.1.1.12.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">3.4</td>
<td id="S3.T2.1.1.12.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">23</td>
<td id="S3.T2.1.1.12.7" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Human</td>
<td id="S3.T2.1.1.12.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">6.8</td>
<td id="S3.T2.1.1.12.9" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">2.0</td>
<td id="S3.T2.1.1.12.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Human</td>
</tr>
<tr id="S3.T2.1.1.13" class="ltx_tr">
<td id="S3.T2.1.1.13.1" class="ltx_td ltx_align_left ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;">FVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>
</td>
<td id="S3.T2.1.1.13.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;">COCO &amp; ImageNet</td>
<td id="S3.T2.1.1.13.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;">1,906</td>
<td id="S3.T2.1.1.13.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;">4,608</td>
<td id="S3.T2.1.1.13.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;">2.5</td>
<td id="S3.T2.1.1.13.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;">12</td>
<td id="S3.T2.1.1.13.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;">Human</td>
<td id="S3.T2.1.1.13.8" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;">9.7</td>
<td id="S3.T2.1.1.13.9" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;">1.2</td>
<td id="S3.T2.1.1.13.10" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;">Acc.</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Major datasets for VQA and their main characteristics. See Section <a href="#S3.SS1" title="3.1 Datasets of natural images ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> for discussion.</figcaption>
</figure></div></div>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">An early effort at compiling a dataset specifically for VQA was presented by Geman <em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS1.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. The dataset comprises questions generated from templates from a fixed vocabulary of objects, attributes, and relationships between objects. Another early dataset was presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> by Tu <em id="S3.SS1.p1.1.3" class="ltx_emph ltx_font_italic">et al</em>. They study the joint parsing of videos and text to answer queries, and consider two datasets containing 15 video clips each. These two examples are restricted to limited settings and are of relatively small size. We discuss below the open-world large-scale datasets in use today.</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">DAQUAR</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">The first VQA dataset designed as benchmark is the DAQUAR, for DAtaset for QUestion Answering on Real-world images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. It was built with images from the NYU-Depth v2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, which contains 1449 RGBD images of indoor scenes, together with annotated semantic segmentations. The images of DAQUAR are split to 795 training and 654 test images. Two types of question/answer pairs are collected. First, <span id="S3.SS1.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">synthetic questions/answers</span> are generated automatically using 8 predefined templates and the existing annotations of the NYU dataset. Second, <span id="S3.SS1.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">human questions/answers</span> are collected from 5 annotators. They were instructed to focus on basic colors, numbers, objects (894 categories), and sets of those. Overall, 12,468 question/answer pairs were collected, of which 6,794 are to be used for training and 5,674 for testing. The large size of DAQUAR was key to enable the development and training of the early methods for VQA with deep neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. The main disadvantage of DAQUAR is the restriction of answers to a predefined set of 16 colors and 894 object categories. The dataset also presents strong biases showing that humans tend to focus on a few prominent objects, such as tables and chairs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">COCO-QA</h5>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">The COCO-QA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> represents a substantial effort to increase the scale of training data for VQA. This dataset uses images from the Microsoft Common Objects in Context data (COCO) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. COCO-QA includes 123,287 images (72,783 for training and 38,948 for testing) and each image has one question/answer pair. They were automatically generated by turning the image descriptions part of the original COCO dataset into question/answer form. The questions are categorized into four types based on the type of expected answer: object, number, color, and location. A side-effect of the automatic conversion of captions is a high repetition rate of the questions. Among the 38,948 questions of the test set, 9,072 (23.29%) of them also appear as training questions.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">FM-IQA</h5>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">The FM-IQA (Freestyle Multilingual Image Question Answering) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> uses 123,287 images, also sourced from the COCO dataset. The difference with COCO-QA is that the questions/answers are provided here by humans through the Amazon Mechanical Turk crowd-sourcing platform. The annotators were free to give any type of questions, as long as they relate to the contents of each given image. This lead to a much greater diversity of questions than in previously-available datasets. Answering the questions typically requires both understanding the visual contents of the image and incorporating prior “common sense” information. The dataset contains 120,360 images and 250,560 question/answer pairs, which were originally provided in Chinese, then converted into English by human translators.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">VQA-real</h5>

<div id="S3.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px4.p1.1" class="ltx_p">One of the most widely used dataset comes from the VQA team at Virginia Tech, commonly referred to simply as VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. It comprises two parts, one using natural images named VQA-real, and a second one with cartoon images named VQA-abstract, which we will discuss in Section <a href="#S3.SS2" title="3.2 Datasets of clipart images ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>. VQA-real comprises 123,287 training and 81,434 test images, respectively, sourced from COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. Human annotators were encouraged to provide interesting and diverse questions. In contrast to the datasets mentioned above, binary (<em id="S3.SS1.SSS0.Px4.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.SSS0.Px4.p1.1.2" class="ltx_text"></span> yes/no) questions were also allowed. The dataset also allows evaluation in a multiple-choice setting, by providing 17 additional (incorrect) candidate answers for each question. Overall, it contains 614,163 questions, each having 10 answers from 10 different annotators. The authors performed a very detailed analysis of the dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> in terms of questions types, question/answer lengths, <em id="S3.SS1.SSS0.Px4.p1.1.3" class="ltx_emph ltx_font_italic">etc</em>. They also conducted a study to investigate whether questions required prior non-visual knowledge, judged by polling humans. A majority of subjects (at least 6 out of 10) estimated that common sense was required for 18% of the questions. Only 5.5% of the questions were estimated to require adult-level knowledge. These modest figures show that little more than purely visual information is required to answer most questions.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Visual Genome and Visual7W</h5>

<div id="S3.SS1.SSS0.Px5.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px5.p1.1" class="ltx_p">The Visual Genome QA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> is, at the time of this writing, the largest available dataset for VQA, with 1.7 million question/answer pairs. It is built with images from the Visual Genome project <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, which includes unique structured annotations of scene contents in the form of scene graphs. These scene graphs describe the visual elements of the scenes, their attributes, and relationships between them. Human subjects provided questions that must start with one of the ‘seven Ws’, <em id="S3.SS1.SSS0.Px5.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.SSS0.Px5.p1.1.2" class="ltx_text"></span> who, what, where, when, why, how, and which (the ‘which’ questions have not been released at the time of writing this paper). The questions must also relate to the image so as to be clearly answerable if and only if the image is shown. Two types of questions are considered: free-form and region-based. In the free-form setting, the annotator is shown an image and asked to provide 8 question/answer pairs. To encourage diversity, the annotator is forced to use 3 different “Ws” out of the 7 mentioned above. In the region-based setting, the annotator must provide questions/answers related to a specific, given region of the image. The diversity of the answers in the Visual Genome is larger than in VQA-real <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, as shown by the top-1000 most frequent answers only covering about 64% of the correct answers. In VQA-real, the corresponding top-1000 answers cover as much as 80% of the test set answers. A major advantage of the Visual Genome dataset for VQA is the potential for using the structured scene annotations, which we examine further in Section <a href="#S4" title="4 Structured scene annotations for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The use of this information to help designing and training VQA systems is however still an open research question.</p>
</div>
<div id="S3.SS1.SSS0.Px5.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px5.p2.1" class="ltx_p">The Visual7w <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> dataset is a subset of the Visual Genome that contains additional annotations. The questions are evaluated in a multiple-choice setting, each question being provided with 4 candidate answers, of which only one is correct. In addition, all the objects mentioned in the questions are visually grounded, <em id="S3.SS1.SSS0.Px5.p2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.SSS0.Px5.p2.1.2" class="ltx_text"></span> associated with bounding boxes of their depictions in the images.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<div id="S3.T3.12" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:431.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-87.6pt,87.2pt) scale(0.712146186623584,0.712146186623584) ;">
<table id="S3.T3.12.12" class="ltx_tabular ltx_align_middle">
<tr id="S3.T3.3.3.3" class="ltx_tr">
<td id="S3.T3.3.3.3.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">DAQUAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>
</td>
<td id="S3.T3.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.1.1.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/DAQ_1.png" id="S3.T3.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="S3.T3.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.2.2.2.2.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/DAQ_2.png" id="S3.T3.2.2.2.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="S3.T3.3.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.3.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.3.3.3.3.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/DAQ_3.png" id="S3.T3.3.3.3.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="S3.T3.12.12.13" class="ltx_tr">
<td id="S3.T3.12.12.13.1" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T3.12.12.13.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.13.2.1.1" class="ltx_p" style="width:113.8pt;">Q: How many white objects in this picture ?</span>
</span>
</td>
<td id="S3.T3.12.12.13.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.13.3.1.1" class="ltx_p" style="width:113.8pt;">Q: What color is the chair in front of the wall on the left side of the stacked chairs ?</span>
</span>
</td>
<td id="S3.T3.12.12.13.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.13.4.1.1" class="ltx_p" style="width:113.8pt;">Q: What is the largest white object on the left side of the picture ?</span>
</span>
</td>
</tr>
<tr id="S3.T3.12.12.14" class="ltx_tr">
<td id="S3.T3.12.12.14.1" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T3.12.12.14.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.14.2.1.1" class="ltx_p" style="width:113.8pt;">A: 9</span>
</span>
</td>
<td id="S3.T3.12.12.14.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.14.3.1.1" class="ltx_p" style="width:113.8pt;">A: blue</span>
</span>
</td>
<td id="S3.T3.12.12.14.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.14.4.1.1" class="ltx_p" style="width:113.8pt;">A: printer</span>
</span>
</td>
</tr>
<tr id="S3.T3.6.6.6" class="ltx_tr">
<td id="S3.T3.6.6.6.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">COCO-QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>
</td>
<td id="S3.T3.4.4.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.4.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.4.4.4.1.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/CO_1.jpg" id="S3.T3.4.4.4.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="S3.T3.5.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.5.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.5.5.5.2.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/CO_2.jpg" id="S3.T3.5.5.5.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="S3.T3.6.6.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.6.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.6.6.6.3.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/CO_3.jpg" id="S3.T3.6.6.6.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="S3.T3.12.12.15" class="ltx_tr">
<td id="S3.T3.12.12.15.1" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T3.12.12.15.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.15.2.1.1" class="ltx_p" style="width:113.8pt;">Q: How many giraffes walking near a hut in an enclosure ?</span>
</span>
</td>
<td id="S3.T3.12.12.15.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.15.3.1.1" class="ltx_p" style="width:113.8pt;">Q: What is the color of the bus ?</span>
</span>
</td>
<td id="S3.T3.12.12.15.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.15.4.1.1" class="ltx_p" style="width:113.8pt;">Q: What next to darkened display with telltale blue ?</span>
</span>
</td>
</tr>
<tr id="S3.T3.12.12.16" class="ltx_tr">
<td id="S3.T3.12.12.16.1" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T3.12.12.16.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.16.2.1.1" class="ltx_p" style="width:113.8pt;">A: two</span>
</span>
</td>
<td id="S3.T3.12.12.16.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.16.3.1.1" class="ltx_p" style="width:113.8pt;">A: yellow</span>
</span>
</td>
<td id="S3.T3.12.12.16.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.16.4.1.1" class="ltx_p" style="width:113.8pt;">A: keyboard</span>
</span>
</td>
</tr>
<tr id="S3.T3.9.9.9" class="ltx_tr">
<td id="S3.T3.9.9.9.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">VQA-real <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</td>
<td id="S3.T3.7.7.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.7.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.7.7.7.1.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/VQA_1.jpg" id="S3.T3.7.7.7.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="S3.T3.8.8.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.8.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.8.8.8.2.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/VQA_2.jpg" id="S3.T3.8.8.8.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="S3.T3.9.9.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.9.9.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.9.9.9.3.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/VQA_3.jpg" id="S3.T3.9.9.9.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="S3.T3.12.12.17" class="ltx_tr">
<td id="S3.T3.12.12.17.1" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T3.12.12.17.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.17.2.1.1" class="ltx_p" style="width:113.8pt;">Q: What shape is the bench seat ?</span>
</span>
</td>
<td id="S3.T3.12.12.17.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.17.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.17.3.1.1" class="ltx_p" style="width:113.8pt;">Q: What color is the stripe on the train ?</span>
</span>
</td>
<td id="S3.T3.12.12.17.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.17.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.17.4.1.1" class="ltx_p" style="width:113.8pt;">Q: Where are the magazines in this picture ?</span>
</span>
</td>
</tr>
<tr id="S3.T3.12.12.18" class="ltx_tr">
<td id="S3.T3.12.12.18.1" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T3.12.12.18.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.18.2.1.1" class="ltx_p" style="width:113.8pt;">A: oval, semi circle, curved, curved, double curve, banana, curved, wavy, twisting, curved</span>
</span>
</td>
<td id="S3.T3.12.12.18.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.18.3.1.1" class="ltx_p" style="width:113.8pt;">A: white, white, white, white, white, white, white, white, white, white</span>
</span>
</td>
<td id="S3.T3.12.12.18.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.18.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.18.4.1.1" class="ltx_p" style="width:113.8pt;">A: On stool, stool, on stool, on bar stool, on table, stool, on stool, on chair, on bar stool, stool</span>
</span>
</td>
</tr>
<tr id="S3.T3.12.12.12" class="ltx_tr">
<td id="S3.T3.12.12.12.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
<td id="S3.T3.10.10.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.10.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.10.10.10.1.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/VG_1.jpg" id="S3.T3.10.10.10.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="S3.T3.11.11.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.11.11.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.11.11.11.2.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/VG_2.jpg" id="S3.T3.11.11.11.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="S3.T3.12.12.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.12.3.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/VG_3.jpg" id="S3.T3.12.12.12.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="S3.T3.12.12.19" class="ltx_tr">
<td id="S3.T3.12.12.19.1" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T3.12.12.19.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.19.2.1.1" class="ltx_p" style="width:113.8pt;">Q: What color is the clock ?</span>
</span>
</td>
<td id="S3.T3.12.12.19.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.19.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.19.3.1.1" class="ltx_p" style="width:113.8pt;">Q: What is the woman doing ?</span>
</span>
</td>
<td id="S3.T3.12.12.19.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.19.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.19.4.1.1" class="ltx_p" style="width:113.8pt;">Q: How is the ground ?</span>
</span>
</td>
</tr>
<tr id="S3.T3.12.12.20" class="ltx_tr">
<td id="S3.T3.12.12.20.1" class="ltx_td ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T3.12.12.20.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.20.2.1.1" class="ltx_p" style="width:113.8pt;">A: Green</span>
</span>
</td>
<td id="S3.T3.12.12.20.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.20.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.20.3.1.1" class="ltx_p" style="width:113.8pt;">A: Sitting</span>
</span>
</td>
<td id="S3.T3.12.12.20.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T3.12.12.20.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.12.12.20.4.1.1" class="ltx_p" style="width:113.8pt;">A: dry</span>
</span>
</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Examples from datasets of natural images. The questions in different datasets span a wide range of complexity, involving purely visual attributes and single objects, or more complex relations, actions, and global scene structure. Note that in “VQA-real” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, every question is provided with 10 answers, each proposed by a different human annotator.</figcaption>
</figure>
<figure id="S3.T4" class="ltx_table">
<div id="S3.T4.3" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:120.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-68.9pt,19.2pt) scale(0.758754755246848,0.758754755246848) ;">
<table id="S3.T4.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.3.3.3" class="ltx_tr">
<td id="S3.T4.3.3.3.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">VQA-abstract <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</td>
<td id="S3.T4.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T4.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.1.1.1.1.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/VQAA_1.png" id="S3.T4.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="S3.T4.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T4.2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.2.2.2.2.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/VQAA_2.png" id="S3.T4.2.2.2.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="S3.T4.3.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T4.3.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.3.3.3.3.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/VQAA_3.png" id="S3.T4.3.3.3.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="S3.T4.3.3.4" class="ltx_tr">
<td id="S3.T4.3.3.4.1" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T4.3.3.4.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T4.3.3.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.3.3.4.2.1.1" class="ltx_p" style="width:113.8pt;">Q: Who looks happier ?.</span>
</span>
</td>
<td id="S3.T4.3.3.4.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T4.3.3.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.3.3.4.3.1.1" class="ltx_p" style="width:113.8pt;">Q: Where are the flowers ?</span>
</span>
</td>
<td id="S3.T4.3.3.4.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T4.3.3.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.3.3.4.4.1.1" class="ltx_p" style="width:113.8pt;">Q: How many pillows ?</span>
</span>
</td>
</tr>
<tr id="S3.T4.3.3.5" class="ltx_tr">
<td id="S3.T4.3.3.5.1" class="ltx_td ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T4.3.3.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T4.3.3.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.3.3.5.2.1.1" class="ltx_p" style="width:113.8pt;">A: old person, man, man, man, old man, man, man, man, man, grandpa</span>
</span>
</td>
<td id="S3.T4.3.3.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T4.3.3.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.3.3.5.3.1.1" class="ltx_p" style="width:113.8pt;">A: near tree, tree, around tree, tree, by tree, around tree, around tree, grass, beneath tree, base of tree</span>
</span>
</td>
<td id="S3.T4.3.3.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T4.3.3.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T4.3.3.5.4.1.1" class="ltx_p" style="width:113.8pt;">A: 1, 2, 2, 2, 2, 2, 2, 2, 2, 2</span>
</span>
</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Examples from the “VQA-abstract” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> dataset of clip art images. Every question is provided with 10 answers, each proposed by a different human annotator.</figcaption>
</figure>
<figure id="S3.T5" class="ltx_table">
<div id="S3.T5.6" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:231.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-67.0pt,35.8pt) scale(0.763878579150339,0.763878579150339) ;">
<table id="S3.T5.6.6" class="ltx_tabular ltx_align_middle">
<tr id="S3.T5.3.3.3" class="ltx_tr">
<td id="S3.T5.3.3.3.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">KB-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>
</td>
<td id="S3.T5.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T5.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.1.1.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/KB_1.jpg" id="S3.T5.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="S3.T5.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T5.2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.2.2.2.2.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/KB_2.jpg" id="S3.T5.2.2.2.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="S3.T5.3.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T5.3.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.3.3.3.3.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/KB_3.jpg" id="S3.T5.3.3.3.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="S3.T5.6.6.7" class="ltx_tr">
<td id="S3.T5.6.6.7.1" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T5.6.6.7.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T5.6.6.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.6.6.7.2.1.1" class="ltx_p" style="width:113.8pt;">Q: Tell me the common property of the animal in this image and elephant.</span>
</span>
</td>
<td id="S3.T5.6.6.7.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T5.6.6.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.6.6.7.3.1.1" class="ltx_p" style="width:113.8pt;">Q: List all equipment I might use to play this sport.</span>
</span>
</td>
<td id="S3.T5.6.6.7.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T5.6.6.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.6.6.7.4.1.1" class="ltx_p" style="width:113.8pt;">Q: Is the image related to tourism ?</span>
</span>
</td>
</tr>
<tr id="S3.T5.6.6.8" class="ltx_tr">
<td id="S3.T5.6.6.8.1" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T5.6.6.8.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T5.6.6.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.6.6.8.2.1.1" class="ltx_p" style="width:113.8pt;">A: mammal, animals in Africa</span>
</span>
</td>
<td id="S3.T5.6.6.8.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T5.6.6.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.6.6.8.3.1.1" class="ltx_p" style="width:113.8pt;">A: baseball bat, baseball, baseball glove, baseball field</span>
</span>
</td>
<td id="S3.T5.6.6.8.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T5.6.6.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.6.6.8.4.1.1" class="ltx_p" style="width:113.8pt;">A: yes</span>
</span>
</td>
</tr>
<tr id="S3.T5.6.6.6" class="ltx_tr">
<td id="S3.T5.6.6.6.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">FVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>
</td>
<td id="S3.T5.4.4.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T5.4.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.4.4.4.1.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/FV_1.jpg" id="S3.T5.4.4.4.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="S3.T5.5.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T5.5.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.5.5.5.2.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/FV_2.jpg" id="S3.T5.5.5.5.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="S3.T5.6.6.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T5.6.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.6.6.6.3.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1607.05910/assets/FV_3.jpg" id="S3.T5.6.6.6.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="S3.T5.6.6.9" class="ltx_tr">
<td id="S3.T5.6.6.9.1" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T5.6.6.9.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T5.6.6.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.6.6.9.2.1.1" class="ltx_p" style="width:113.8pt;">Q: What things in this image are eatable ?</span>
</span>
</td>
<td id="S3.T5.6.6.9.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T5.6.6.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.6.6.9.3.1.1" class="ltx_p" style="width:113.8pt;">Q: What is the order of the animal described in this image ?</span>
</span>
</td>
<td id="S3.T5.6.6.9.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T5.6.6.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.6.6.9.4.1.1" class="ltx_p" style="width:113.8pt;">Q: What thing in this image is helpful for a romantic dinner ?</span>
</span>
</td>
</tr>
<tr id="S3.T5.6.6.10" class="ltx_tr">
<td id="S3.T5.6.6.10.1" class="ltx_td ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T5.6.6.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T5.6.6.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.6.6.10.2.1.1" class="ltx_p" style="width:113.8pt;">A: Apples</span>
</span>
</td>
<td id="S3.T5.6.6.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T5.6.6.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.6.6.10.3.1.1" class="ltx_p" style="width:113.8pt;">A: Odd toed ungulate</span>
</span>
</td>
<td id="S3.T5.6.6.10.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S3.T5.6.6.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.6.6.10.4.1.1" class="ltx_p" style="width:113.8pt;">A: Wine</span>
</span>
</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Examples from knowledge base-enhanced datasets.</figcaption>
</figure>
</section>
<section id="S3.SS1.SSS0.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Visual Madlibs</h5>

<div id="S3.SS1.SSS0.Px6.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px6.p1.2" class="ltx_p">The Visual Madlibs dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> is designed to evaluate systems on a “fill in the blank” task. The objective is to determine words to complete an affirmation that describes a given image. For example, the description “two <span class="ltx_rule ltx_markedasmath" style="width:17.0pt;height:0.2pt;background:black;display:inline-block;"> </span> are playing <span class="ltx_rule ltx_markedasmath" style="width:17.0pt;height:0.2pt;background:black;display:inline-block;"> </span> in the park”, provided along a corresponding image, may have to be filled in with “men” and “frisbee”. These sentences essentially amount to questions phrased in declarative form, and most VQA systems could be easily adapted to this setting. The dataset comprises 10,738 images from COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> and 360,001 focused natural language descriptions. Incomplete sentences were generated automatically from these descriptions using templates. Both open-ended and multiple-choice evaluation are possible.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px7" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Evaluation Measures</h5>

<div id="S3.SS1.SSS0.Px7.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px7.p1.1" class="ltx_p">The evaluation of computer-generated natural language sentences is an inherently complex task. Both the syntactic (grammatical) and semantic correctness should be taken into account. Comparing generated with ground truth sentences is akin to evaluating paraphrases, which is still an open research problem studied in the NLP community. Most datasets for VQA allow to bypass this issue by restricting answers to single words or short phrases, typically of 1 to 3 words. This allows automatic evaluation, and limits ambiguities during annotation since it forces questions and answers to be more specific.</p>
</div>
<div id="S3.SS1.SSS0.Px7.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px7.p2.1" class="ltx_p">The seminal paper of Malinowski <em id="S3.SS1.SSS0.Px7.p2.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS1.SSS0.Px7.p2.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> proposed two evaluation metrics for VQA. The first is to simply measure the accuracy with respect to the ground truth using string matching. Only exact matches are considered as correct. The second uses the Wu-Palmer similarity (WUPS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite> which evaluates the similarity between their common subsequence in a taxonomy tree. The candidate answer is considered as correct when the similarity between two words exceeds a threshold. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, the metric is evaluated against two thresholds, 0.9 and 0.0. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, Gao <em id="S3.SS1.SSS0.Px7.p2.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS1.SSS0.Px7.p2.1.4" class="ltx_text"></span> conduct an actual <span id="S3.SS1.SSS0.Px7.p2.1.5" class="ltx_text ltx_font_italic">Visual Turing Test</span> using human judges. Subjects are presented with an image, a question and a candidate answer, from either a VQA system or another human. He or she then needs to determine, based on the answer, whether it was more likely to have been generated by a human (<em id="S3.SS1.SSS0.Px7.p2.1.6" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.SSS0.Px7.p2.1.7" class="ltx_text"></span> pass the test) or a machine (<em id="S3.SS1.SSS0.Px7.p2.1.8" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.SSS0.Px7.p2.1.9" class="ltx_text"></span> fail the test). They also rate each candidate answer with a score.</p>
</div>
<div id="S3.SS1.SSS0.Px7.p3" class="ltx_para">
<p id="S3.SS1.SSS0.Px7.p3.1" class="ltx_p">The VQA-real dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> recognize the issue of ambiguous questions and collect, for each question, 10 ground truth answers from 10 different subjects. Evaluation on this dataset must compare a generated answer with these 10 human-generated ones as follows:</p>
<table id="S3.E10" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E10.m1.3" class="ltx_Math" alttext="\text{accuracy}~{}=~{}\min\Big{(}\frac{\text{\# humans provided that answer}}{3},1\Big{)}" display="block"><semantics id="S3.E10.m1.3a"><mrow id="S3.E10.m1.3.4" xref="S3.E10.m1.3.4.cmml"><mtext id="S3.E10.m1.3.4.2" xref="S3.E10.m1.3.4.2a.cmml">accuracy</mtext><mo lspace="0.608em" rspace="0.608em" id="S3.E10.m1.3.4.1" xref="S3.E10.m1.3.4.1.cmml">=</mo><mrow id="S3.E10.m1.3.4.3.2" xref="S3.E10.m1.3.4.3.1.cmml"><mi id="S3.E10.m1.1.1" xref="S3.E10.m1.1.1.cmml">min</mi><mo id="S3.E10.m1.3.4.3.2a" xref="S3.E10.m1.3.4.3.1.cmml">⁡</mo><mrow id="S3.E10.m1.3.4.3.2.1" xref="S3.E10.m1.3.4.3.1.cmml"><mo maxsize="160%" minsize="160%" id="S3.E10.m1.3.4.3.2.1.1" xref="S3.E10.m1.3.4.3.1.cmml">(</mo><mfrac id="S3.E10.m1.2.2" xref="S3.E10.m1.2.2.cmml"><mtext id="S3.E10.m1.2.2.2" xref="S3.E10.m1.2.2.2a.cmml"># humans provided that answer</mtext><mn id="S3.E10.m1.2.2.3" xref="S3.E10.m1.2.2.3.cmml">3</mn></mfrac><mo id="S3.E10.m1.3.4.3.2.1.2" xref="S3.E10.m1.3.4.3.1.cmml">,</mo><mn id="S3.E10.m1.3.3" xref="S3.E10.m1.3.3.cmml">1</mn><mo maxsize="160%" minsize="160%" id="S3.E10.m1.3.4.3.2.1.3" xref="S3.E10.m1.3.4.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E10.m1.3b"><apply id="S3.E10.m1.3.4.cmml" xref="S3.E10.m1.3.4"><eq id="S3.E10.m1.3.4.1.cmml" xref="S3.E10.m1.3.4.1"></eq><ci id="S3.E10.m1.3.4.2a.cmml" xref="S3.E10.m1.3.4.2"><mtext id="S3.E10.m1.3.4.2.cmml" xref="S3.E10.m1.3.4.2">accuracy</mtext></ci><apply id="S3.E10.m1.3.4.3.1.cmml" xref="S3.E10.m1.3.4.3.2"><min id="S3.E10.m1.1.1.cmml" xref="S3.E10.m1.1.1"></min><apply id="S3.E10.m1.2.2.cmml" xref="S3.E10.m1.2.2"><divide id="S3.E10.m1.2.2.1.cmml" xref="S3.E10.m1.2.2"></divide><ci id="S3.E10.m1.2.2.2a.cmml" xref="S3.E10.m1.2.2.2"><mtext id="S3.E10.m1.2.2.2.cmml" xref="S3.E10.m1.2.2.2"># humans provided that answer</mtext></ci><cn type="integer" id="S3.E10.m1.2.2.3.cmml" xref="S3.E10.m1.2.2.3">3</cn></apply><cn type="integer" id="S3.E10.m1.3.3.cmml" xref="S3.E10.m1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E10.m1.3c">\text{accuracy}~{}=~{}\min\Big{(}\frac{\text{\# humans provided that answer}}{3},1\Big{)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS0.Px7.p3.2" class="ltx_p">In other words, an answer is deemed 100% accurate if at least 3 annotators provided that exact answer.</p>
</div>
<figure id="S3.T6" class="ltx_table">
<div id="S3.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:222.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(6.2pt,-3.2pt) scale(1.02943093730179,1.02943093730179) ;">
<table id="S3.T6.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T6.1.1.1" class="ltx_tr">
<td id="S3.T6.1.1.1.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span>
<span id="S3.T6.1.1.1.1.1" class="ltx_text ltx_font_bold">DAQUAR-all</span>
</td>
<td id="S3.T6.1.1.1.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Acc. (%)</td>
<td id="S3.T6.1.1.1.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">WUPS @0.9</td>
<td id="S3.T6.1.1.1.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">WUPS @0.0</td>
</tr>
<tr id="S3.T6.1.1.2" class="ltx_tr">
<td id="S3.T6.1.1.2.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span>
Neural-Image-QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>
</td>
<td id="S3.T6.1.1.2.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">19.43</td>
<td id="S3.T6.1.1.2.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">25.28</td>
<td id="S3.T6.1.1.2.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">62.00</td>
</tr>
<tr id="S3.T6.1.1.3" class="ltx_tr">
<td id="S3.T6.1.1.3.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Multimodal-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S3.T6.1.1.3.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">23.40</td>
<td id="S3.T6.1.1.3.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">29.59</td>
<td id="S3.T6.1.1.3.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">62.95</td>
</tr>
<tr id="S3.T6.1.1.4" class="ltx_tr">
<td id="S3.T6.1.1.4.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Attributes-LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>
</td>
<td id="S3.T6.1.1.4.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">24.27</td>
<td id="S3.T6.1.1.4.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">30.41</td>
<td id="S3.T6.1.1.4.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">62.29</td>
</tr>
<tr id="S3.T6.1.1.5" class="ltx_tr">
<td id="S3.T6.1.1.5.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">QAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S3.T6.1.1.5.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">25.37</td>
<td id="S3.T6.1.1.5.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">31.35</td>
<td id="S3.T6.1.1.5.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">65.89</td>
</tr>
<tr id="S3.T6.1.1.6" class="ltx_tr">
<td id="S3.T6.1.1.6.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">DMN+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>
</td>
<td id="S3.T6.1.1.6.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">28.79</td>
<td id="S3.T6.1.1.6.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T6.1.1.6.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
</tr>
<tr id="S3.T6.1.1.7" class="ltx_tr">
<td id="S3.T6.1.1.7.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Bayesian <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S3.T6.1.1.7.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">28.96</td>
<td id="S3.T6.1.1.7.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">34.74</td>
<td id="S3.T6.1.1.7.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">67.33</td>
</tr>
<tr id="S3.T6.1.1.8" class="ltx_tr">
<td id="S3.T6.1.1.8.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">DPPnet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>
</td>
<td id="S3.T6.1.1.8.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">28.98</td>
<td id="S3.T6.1.1.8.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">34.80</td>
<td id="S3.T6.1.1.8.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">67.81</td>
</tr>
<tr id="S3.T6.1.1.9" class="ltx_tr">
<td id="S3.T6.1.1.9.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">ACK <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>
</td>
<td id="S3.T6.1.1.9.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">29.16</td>
<td id="S3.T6.1.1.9.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">35.30</td>
<td id="S3.T6.1.1.9.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">68.66</td>
</tr>
<tr id="S3.T6.1.1.10" class="ltx_tr">
<td id="S3.T6.1.1.10.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">ACK-S <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>
</td>
<td id="S3.T6.1.1.10.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">29.23</td>
<td id="S3.T6.1.1.10.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">35.37</td>
<td id="S3.T6.1.1.10.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">68.72</td>
</tr>
<tr id="S3.T6.1.1.11" class="ltx_tr">
<td id="S3.T6.1.1.11.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">SAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>
</td>
<td id="S3.T6.1.1.11.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">29.30</td>
<td id="S3.T6.1.1.11.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">35.10</td>
<td id="S3.T6.1.1.11.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">68.60</td>
</tr>
<tr id="S3.T6.1.1.12" class="ltx_tr">
<td id="S3.T6.1.1.12.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span></td>
<td id="S3.T6.1.1.12.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T6.1.1.12.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T6.1.1.12.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Reported results on the DAQUAR-all dataset.</figcaption>
</figure>
<figure id="S3.T7" class="ltx_table">
<div id="S3.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:298.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(7.9pt,-5.4pt) scale(1.03764227008775,1.03764227008775) ;">
<table id="S3.T7.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T7.1.1.1" class="ltx_tr">
<td id="S3.T7.1.1.1.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span>
<span id="S3.T7.1.1.1.1.1" class="ltx_text ltx_font_bold">DAQUAR-reduced</span>
</td>
<td id="S3.T7.1.1.1.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Acc. (%)</td>
<td id="S3.T7.1.1.1.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">WUPS @0.9</td>
<td id="S3.T7.1.1.1.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">WUPS @0.0</td>
</tr>
<tr id="S3.T7.1.1.2" class="ltx_tr">
<td id="S3.T7.1.1.2.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span>
GUESS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>
</td>
<td id="S3.T7.1.1.2.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">18.24</td>
<td id="S3.T7.1.1.2.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">29.65</td>
<td id="S3.T7.1.1.2.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">77.59</td>
</tr>
<tr id="S3.T7.1.1.3" class="ltx_tr">
<td id="S3.T7.1.1.3.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">VIS+BOW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>
</td>
<td id="S3.T7.1.1.3.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">34.17</td>
<td id="S3.T7.1.1.3.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">44.99</td>
<td id="S3.T7.1.1.3.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">81.48</td>
</tr>
<tr id="S3.T7.1.1.4" class="ltx_tr">
<td id="S3.T7.1.1.4.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">VIS+LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>
</td>
<td id="S3.T7.1.1.4.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">34.41</td>
<td id="S3.T7.1.1.4.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">46.05</td>
<td id="S3.T7.1.1.4.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">82.23</td>
</tr>
<tr id="S3.T7.1.1.5" class="ltx_tr">
<td id="S3.T7.1.1.5.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Neural-Image-QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>
</td>
<td id="S3.T7.1.1.5.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">34.68</td>
<td id="S3.T7.1.1.5.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">40.76</td>
<td id="S3.T7.1.1.5.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">79.54</td>
</tr>
<tr id="S3.T7.1.1.6" class="ltx_tr">
<td id="S3.T7.1.1.6.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">2-VIS+BLSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>
</td>
<td id="S3.T7.1.1.6.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">35.78</td>
<td id="S3.T7.1.1.6.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">46.83</td>
<td id="S3.T7.1.1.6.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">82.15</td>
</tr>
<tr id="S3.T7.1.1.7" class="ltx_tr">
<td id="S3.T7.1.1.7.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Multimodal-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S3.T7.1.1.7.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">39.66</td>
<td id="S3.T7.1.1.7.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">44.86</td>
<td id="S3.T7.1.1.7.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">83.06</td>
</tr>
<tr id="S3.T7.1.1.8" class="ltx_tr">
<td id="S3.T7.1.1.8.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">SMem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>
</td>
<td id="S3.T7.1.1.8.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">40.07</td>
<td id="S3.T7.1.1.8.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T7.1.1.8.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
</tr>
<tr id="S3.T7.1.1.9" class="ltx_tr">
<td id="S3.T7.1.1.9.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Attributes-LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>
</td>
<td id="S3.T7.1.1.9.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">40.07</td>
<td id="S3.T7.1.1.9.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">45.43</td>
<td id="S3.T7.1.1.9.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">82.67</td>
</tr>
<tr id="S3.T7.1.1.10" class="ltx_tr">
<td id="S3.T7.1.1.10.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">QAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S3.T7.1.1.10.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">42.76</td>
<td id="S3.T7.1.1.10.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">47.62</td>
<td id="S3.T7.1.1.10.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">83.04</td>
</tr>
<tr id="S3.T7.1.1.11" class="ltx_tr">
<td id="S3.T7.1.1.11.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">DPPnet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>
</td>
<td id="S3.T7.1.1.11.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">44.48</td>
<td id="S3.T7.1.1.11.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">49.56</td>
<td id="S3.T7.1.1.11.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">83.95</td>
</tr>
<tr id="S3.T7.1.1.12" class="ltx_tr">
<td id="S3.T7.1.1.12.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Bayesian <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S3.T7.1.1.12.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">45.17</td>
<td id="S3.T7.1.1.12.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">49.74</td>
<td id="S3.T7.1.1.12.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">85.13</td>
</tr>
<tr id="S3.T7.1.1.13" class="ltx_tr">
<td id="S3.T7.1.1.13.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">SAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>
</td>
<td id="S3.T7.1.1.13.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">45.50</td>
<td id="S3.T7.1.1.13.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">50.20</td>
<td id="S3.T7.1.1.13.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">83.60</td>
</tr>
<tr id="S3.T7.1.1.14" class="ltx_tr">
<td id="S3.T7.1.1.14.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">ACK <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>
</td>
<td id="S3.T7.1.1.14.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">45.79</td>
<td id="S3.T7.1.1.14.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">51.53</td>
<td id="S3.T7.1.1.14.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">83.91</td>
</tr>
<tr id="S3.T7.1.1.15" class="ltx_tr">
<td id="S3.T7.1.1.15.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">ACK-S <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>
</td>
<td id="S3.T7.1.1.15.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">46.13</td>
<td id="S3.T7.1.1.15.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">51.83</td>
<td id="S3.T7.1.1.15.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">83.95</td>
</tr>
<tr id="S3.T7.1.1.16" class="ltx_tr">
<td id="S3.T7.1.1.16.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span></td>
<td id="S3.T7.1.1.16.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T7.1.1.16.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T7.1.1.16.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Reported results on the DAQUAR-reduced dataset.</figcaption>
</figure>
<figure id="S3.T8" class="ltx_table">
<div id="S3.T8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:290.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(15.4pt,-10.3pt) scale(1.07658073256854,1.07658073256854) ;">
<table id="S3.T8.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T8.1.1.1" class="ltx_tr">
<td id="S3.T8.1.1.1.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span>
<span id="S3.T8.1.1.1.1.1" class="ltx_text ltx_font_bold">Toronto COCO-QA</span>
</td>
<td id="S3.T8.1.1.1.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">Acc. (%)</td>
<td id="S3.T8.1.1.1.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">WUPS @0.9</td>
<td id="S3.T8.1.1.1.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">WUPS @0.0</td>
</tr>
<tr id="S3.T8.1.1.2" class="ltx_tr">
<td id="S3.T8.1.1.2.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span>
GUESS<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>
</td>
<td id="S3.T8.1.1.2.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">6.65</td>
<td id="S3.T8.1.1.2.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">17.42</td>
<td id="S3.T8.1.1.2.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">73.44</td>
</tr>
<tr id="S3.T8.1.1.3" class="ltx_tr">
<td id="S3.T8.1.1.3.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">VIS+LSTM<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>
</td>
<td id="S3.T8.1.1.3.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">53.31</td>
<td id="S3.T8.1.1.3.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">63.91</td>
<td id="S3.T8.1.1.3.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">88.25</td>
</tr>
<tr id="S3.T8.1.1.4" class="ltx_tr">
<td id="S3.T8.1.1.4.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Multimodal-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S3.T8.1.1.4.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">54.95</td>
<td id="S3.T8.1.1.4.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">65.36</td>
<td id="S3.T8.1.1.4.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">88.58</td>
</tr>
<tr id="S3.T8.1.1.5" class="ltx_tr">
<td id="S3.T8.1.1.5.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">2-VIS+BLSTM<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>
</td>
<td id="S3.T8.1.1.5.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">55.09</td>
<td id="S3.T8.1.1.5.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">65.34</td>
<td id="S3.T8.1.1.5.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">88.64</td>
</tr>
<tr id="S3.T8.1.1.6" class="ltx_tr">
<td id="S3.T8.1.1.6.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">VIS+BOW<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>
</td>
<td id="S3.T8.1.1.6.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">55.92</td>
<td id="S3.T8.1.1.6.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">66.78</td>
<td id="S3.T8.1.1.6.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">88.99</td>
</tr>
<tr id="S3.T8.1.1.7" class="ltx_tr">
<td id="S3.T8.1.1.7.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">QAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S3.T8.1.1.7.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">58.10</td>
<td id="S3.T8.1.1.7.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">68.44</td>
<td id="S3.T8.1.1.7.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">89.85</td>
</tr>
<tr id="S3.T8.1.1.8" class="ltx_tr">
<td id="S3.T8.1.1.8.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">DPPnet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>
</td>
<td id="S3.T8.1.1.8.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">61.19</td>
<td id="S3.T8.1.1.8.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">70.84</td>
<td id="S3.T8.1.1.8.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">90.61</td>
</tr>
<tr id="S3.T8.1.1.9" class="ltx_tr">
<td id="S3.T8.1.1.9.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Attributes-LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>
</td>
<td id="S3.T8.1.1.9.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">61.38</td>
<td id="S3.T8.1.1.9.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">71.15</td>
<td id="S3.T8.1.1.9.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">91.58</td>
</tr>
<tr id="S3.T8.1.1.10" class="ltx_tr">
<td id="S3.T8.1.1.10.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">SAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>
</td>
<td id="S3.T8.1.1.10.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">61.60</td>
<td id="S3.T8.1.1.10.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">71.60</td>
<td id="S3.T8.1.1.10.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">90.90</td>
</tr>
<tr id="S3.T8.1.1.11" class="ltx_tr">
<td id="S3.T8.1.1.11.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Bayesian <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S3.T8.1.1.11.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">63.18</td>
<td id="S3.T8.1.1.11.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">73.14</td>
<td id="S3.T8.1.1.11.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">91.32</td>
</tr>
<tr id="S3.T8.1.1.12" class="ltx_tr">
<td id="S3.T8.1.1.12.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">HieCoAtt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
<td id="S3.T8.1.1.12.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">65.40</td>
<td id="S3.T8.1.1.12.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">75.10</td>
<td id="S3.T8.1.1.12.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">92.00</td>
</tr>
<tr id="S3.T8.1.1.13" class="ltx_tr">
<td id="S3.T8.1.1.13.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">ACK <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>
</td>
<td id="S3.T8.1.1.13.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">69.73</td>
<td id="S3.T8.1.1.13.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">77.14</td>
<td id="S3.T8.1.1.13.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">92.50</td>
</tr>
<tr id="S3.T8.1.1.14" class="ltx_tr">
<td id="S3.T8.1.1.14.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">ACK-S <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>
</td>
<td id="S3.T8.1.1.14.2" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">70.98</td>
<td id="S3.T8.1.1.14.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">78.35</td>
<td id="S3.T8.1.1.14.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">92.87</td>
</tr>
<tr id="S3.T8.1.1.15" class="ltx_tr">
<td id="S3.T8.1.1.15.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span></td>
<td id="S3.T8.1.1.15.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T8.1.1.15.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T8.1.1.15.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Reported results on the COCO-QA dataset.</figcaption>
</figure>
<div id="S3.SS1.SSS0.Px7.p4" class="ltx_para">
<p id="S3.SS1.SSS0.Px7.p4.1" class="ltx_p">Other datasets such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> simply measure accuracy through the ratio of exact matches between predictions and answers, which is sensible when answers are short and therefore mostly unambiguous. Evaluation in a multiple-choice setting (<em id="S3.SS1.SSS0.Px7.p4.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS1.SSS0.Px7.p4.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>) is straightforward. It makes the task of VQA easier by constraining the output space to a few discrete points, and it eliminates any artifact in the evaluation that could arise from a chosen metric.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px8" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Results of existing methods</h5>

<div id="S3.SS1.SSS0.Px8.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px8.p1.1" class="ltx_p">Most modern methods for VQA have been evaluated on the VQA-real <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, DAQUAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, and COCO-QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> datasets. We summarize results on these three main datasets in Tables <a href="#S3.T6" title="Table 6 ‣ Evaluation Measures ‣ 3.1 Datasets of natural images ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, <a href="#S3.T7" title="Table 7 ‣ Evaluation Measures ‣ 3.1 Datasets of natural images ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, <a href="#S3.T8" title="Table 8 ‣ Evaluation Measures ‣ 3.1 Datasets of natural images ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> and <a href="#S3.T9" title="Table 9 ‣ Results of existing methods ‣ 3.1 Datasets of natural images ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure id="S3.T9" class="ltx_table">
<div id="S3.T9.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:336.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-107.8pt,83.7pt) scale(0.667905752394383,0.667905752394383) ;">
<table id="S3.T9.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T9.1.1.1" class="ltx_tr">
<td id="S3.T9.1.1.1.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span></td>
<td id="S3.T9.1.1.1.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.1.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;" colspan="6">Test-dev</td>
<td id="S3.T9.1.1.1.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.1.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;" colspan="6">Test-standard</td>
</tr>
<tr id="S3.T9.1.1.2" class="ltx_tr">
<td id="S3.T9.1.1.2.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Method</td>
<td id="S3.T9.1.1.2.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;" colspan="4">Open-ended</td>
<td id="S3.T9.1.1.2.4" class="ltx_td ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">M.C.</td>
<td id="S3.T9.1.1.2.6" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;" colspan="4">Open-ended</td>
<td id="S3.T9.1.1.2.8" class="ltx_td ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">M.C.</td>
</tr>
<tr id="S3.T9.1.1.3" class="ltx_tr">
<td id="S3.T9.1.1.3.1" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.3.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Y/N</td>
<td id="S3.T9.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Num.</td>
<td id="S3.T9.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Other</td>
<td id="S3.T9.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">All</td>
<td id="S3.T9.1.1.3.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">All</td>
<td id="S3.T9.1.1.3.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.3.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Y/N</td>
<td id="S3.T9.1.1.3.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Num.</td>
<td id="S3.T9.1.1.3.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">Other</td>
<td id="S3.T9.1.1.3.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">All</td>
<td id="S3.T9.1.1.3.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.3.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">All</td>
</tr>
<tr id="S3.T9.1.1.4" class="ltx_tr">
<td id="S3.T9.1.1.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span>
Com-Mem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</td>
<td id="S3.T9.1.1.4.2" class="ltx_td ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">78.3</td>
<td id="S3.T9.1.1.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">35.9</td>
<td id="S3.T9.1.1.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">34.5</td>
<td id="S3.T9.1.1.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">52.6</td>
<td id="S3.T9.1.1.4.7" class="ltx_td ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.4.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.4.9" class="ltx_td ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.4.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.4.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.4.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.4.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.4.14" class="ltx_td ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.4.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
</tr>
<tr id="S3.T9.1.1.5" class="ltx_tr">
<td id="S3.T9.1.1.5.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Attributes-LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>
</td>
<td id="S3.T9.1.1.5.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.5.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">79.8</td>
<td id="S3.T9.1.1.5.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">36.1</td>
<td id="S3.T9.1.1.5.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">43.1</td>
<td id="S3.T9.1.1.5.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">55.6</td>
<td id="S3.T9.1.1.5.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.5.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.5.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.5.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">78.7</td>
<td id="S3.T9.1.1.5.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">36.0</td>
<td id="S3.T9.1.1.5.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">43.4</td>
<td id="S3.T9.1.1.5.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">55.8</td>
<td id="S3.T9.1.1.5.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.5.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
</tr>
<tr id="S3.T9.1.1.6" class="ltx_tr">
<td id="S3.T9.1.1.6.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">iBOWING <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>
</td>
<td id="S3.T9.1.1.6.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.6.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">76.5</td>
<td id="S3.T9.1.1.6.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">35.0</td>
<td id="S3.T9.1.1.6.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">42.6</td>
<td id="S3.T9.1.1.6.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">55.7</td>
<td id="S3.T9.1.1.6.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.6.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.6.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.6.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">76.8</td>
<td id="S3.T9.1.1.6.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">35.0</td>
<td id="S3.T9.1.1.6.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">42.6</td>
<td id="S3.T9.1.1.6.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">55.9</td>
<td id="S3.T9.1.1.6.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.6.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">62.0</td>
</tr>
<tr id="S3.T9.1.1.7" class="ltx_tr">
<td id="S3.T9.1.1.7.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Region-Sel <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>
</td>
<td id="S3.T9.1.1.7.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.7.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.7.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.7.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.7.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.7.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.7.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">62.4</td>
<td id="S3.T9.1.1.7.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.7.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.7.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.7.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.7.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.7.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.7.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">62.4</td>
</tr>
<tr id="S3.T9.1.1.8" class="ltx_tr">
<td id="S3.T9.1.1.8.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">DPPnet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>
</td>
<td id="S3.T9.1.1.8.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.8.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">80.7</td>
<td id="S3.T9.1.1.8.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">37.2</td>
<td id="S3.T9.1.1.8.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">41.7</td>
<td id="S3.T9.1.1.8.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">57.2</td>
<td id="S3.T9.1.1.8.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.8.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.8.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.8.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">80.3</td>
<td id="S3.T9.1.1.8.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">36.9</td>
<td id="S3.T9.1.1.8.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">42.2</td>
<td id="S3.T9.1.1.8.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">57.4</td>
<td id="S3.T9.1.1.8.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.8.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
</tr>
<tr id="S3.T9.1.1.9" class="ltx_tr">
<td id="S3.T9.1.1.9.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">VQA team <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</td>
<td id="S3.T9.1.1.9.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.9.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">80.5</td>
<td id="S3.T9.1.1.9.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">36.8</td>
<td id="S3.T9.1.1.9.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">43.1</td>
<td id="S3.T9.1.1.9.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">57.8</td>
<td id="S3.T9.1.1.9.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.9.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">62.7</td>
<td id="S3.T9.1.1.9.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.9.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">80.6</td>
<td id="S3.T9.1.1.9.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">36.5</td>
<td id="S3.T9.1.1.9.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">43.7</td>
<td id="S3.T9.1.1.9.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">58.2</td>
<td id="S3.T9.1.1.9.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.9.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">63.1</td>
</tr>
<tr id="S3.T9.1.1.10" class="ltx_tr">
<td id="S3.T9.1.1.10.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">MLP-AQI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="S3.T9.1.1.10.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.10.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.10.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.10.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.10.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.10.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.10.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.10.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.10.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.10.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.10.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.10.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.10.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.10.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">65.2</td>
</tr>
<tr id="S3.T9.1.1.11" class="ltx_tr">
<td id="S3.T9.1.1.11.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">SMem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>
</td>
<td id="S3.T9.1.1.11.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.11.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">80.9</td>
<td id="S3.T9.1.1.11.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">37.3</td>
<td id="S3.T9.1.1.11.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">43.1</td>
<td id="S3.T9.1.1.11.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">58.0</td>
<td id="S3.T9.1.1.11.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.11.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.11.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.11.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">80.9</td>
<td id="S3.T9.1.1.11.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">37.5</td>
<td id="S3.T9.1.1.11.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">43.5</td>
<td id="S3.T9.1.1.11.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">58.2</td>
<td id="S3.T9.1.1.11.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.11.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
</tr>
<tr id="S3.T9.1.1.12" class="ltx_tr">
<td id="S3.T9.1.1.12.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Neural-Image-QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>
</td>
<td id="S3.T9.1.1.12.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.12.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">78.4</td>
<td id="S3.T9.1.1.12.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">36.4</td>
<td id="S3.T9.1.1.12.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">46.3</td>
<td id="S3.T9.1.1.12.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">58.4</td>
<td id="S3.T9.1.1.12.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.12.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.12.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.12.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">78.2</td>
<td id="S3.T9.1.1.12.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">36.3</td>
<td id="S3.T9.1.1.12.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">46.3</td>
<td id="S3.T9.1.1.12.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">58.4</td>
<td id="S3.T9.1.1.12.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.12.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
</tr>
<tr id="S3.T9.1.1.13" class="ltx_tr">
<td id="S3.T9.1.1.13.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">NMN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</td>
<td id="S3.T9.1.1.13.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.13.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">81.2</td>
<td id="S3.T9.1.1.13.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">38.0</td>
<td id="S3.T9.1.1.13.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">44.0</td>
<td id="S3.T9.1.1.13.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">58.6</td>
<td id="S3.T9.1.1.13.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.13.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.13.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.13.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">81.2</td>
<td id="S3.T9.1.1.13.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">37.7</td>
<td id="S3.T9.1.1.13.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">44.0</td>
<td id="S3.T9.1.1.13.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">58.7</td>
<td id="S3.T9.1.1.13.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.13.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
</tr>
<tr id="S3.T9.1.1.14" class="ltx_tr">
<td id="S3.T9.1.1.14.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">SAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>
</td>
<td id="S3.T9.1.1.14.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.14.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">79.3</td>
<td id="S3.T9.1.1.14.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">36.6</td>
<td id="S3.T9.1.1.14.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">46.1</td>
<td id="S3.T9.1.1.14.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">58.7</td>
<td id="S3.T9.1.1.14.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.14.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.14.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.14.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.14.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.14.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.14.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">58.9</td>
<td id="S3.T9.1.1.14.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.14.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
</tr>
<tr id="S3.T9.1.1.15" class="ltx_tr">
<td id="S3.T9.1.1.15.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">ACK <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>
</td>
<td id="S3.T9.1.1.15.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.15.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">81.0</td>
<td id="S3.T9.1.1.15.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">38.4</td>
<td id="S3.T9.1.1.15.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">45.2</td>
<td id="S3.T9.1.1.15.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">59.2</td>
<td id="S3.T9.1.1.15.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.15.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.15.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.15.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">81.1</td>
<td id="S3.T9.1.1.15.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">37.1</td>
<td id="S3.T9.1.1.15.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">45.8</td>
<td id="S3.T9.1.1.15.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">59.4</td>
<td id="S3.T9.1.1.15.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.15.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
</tr>
<tr id="S3.T9.1.1.16" class="ltx_tr">
<td id="S3.T9.1.1.16.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">DNMN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
<td id="S3.T9.1.1.16.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.16.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">81.1</td>
<td id="S3.T9.1.1.16.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">38.6</td>
<td id="S3.T9.1.1.16.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">45.5</td>
<td id="S3.T9.1.1.16.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">59.4</td>
<td id="S3.T9.1.1.16.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.16.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.16.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.16.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.16.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.16.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.16.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">59.4</td>
<td id="S3.T9.1.1.16.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.16.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
</tr>
<tr id="S3.T9.1.1.17" class="ltx_tr">
<td id="S3.T9.1.1.17.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">FDA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S3.T9.1.1.17.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.17.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">81.1</td>
<td id="S3.T9.1.1.17.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">36.2</td>
<td id="S3.T9.1.1.17.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">45.8</td>
<td id="S3.T9.1.1.17.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">59.2</td>
<td id="S3.T9.1.1.17.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.17.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.17.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.17.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.17.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.17.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.17.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">59.5</td>
<td id="S3.T9.1.1.17.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.17.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
</tr>
<tr id="S3.T9.1.1.18" class="ltx_tr">
<td id="S3.T9.1.1.18.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">ACK-S <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>
</td>
<td id="S3.T9.1.1.18.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.18.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">81.0</td>
<td id="S3.T9.1.1.18.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">38.5</td>
<td id="S3.T9.1.1.18.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">45.3</td>
<td id="S3.T9.1.1.18.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">59.2</td>
<td id="S3.T9.1.1.18.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.18.8" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.18.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.18.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">81.1</td>
<td id="S3.T9.1.1.18.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">37.2</td>
<td id="S3.T9.1.1.18.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">45.9</td>
<td id="S3.T9.1.1.18.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">59.5</td>
<td id="S3.T9.1.1.18.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.18.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
</tr>
<tr id="S3.T9.1.1.19" class="ltx_tr">
<td id="S3.T9.1.1.19.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Bayesian <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S3.T9.1.1.19.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.19.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">80.5</td>
<td id="S3.T9.1.1.19.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">37.5</td>
<td id="S3.T9.1.1.19.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">46.7</td>
<td id="S3.T9.1.1.19.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">59.6</td>
<td id="S3.T9.1.1.19.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.19.8" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.19.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.19.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">80.3</td>
<td id="S3.T9.1.1.19.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">37.8</td>
<td id="S3.T9.1.1.19.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">47.6</td>
<td id="S3.T9.1.1.19.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">60.1</td>
<td id="S3.T9.1.1.19.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.19.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
</tr>
<tr id="S3.T9.1.1.20" class="ltx_tr">
<td id="S3.T9.1.1.20.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">DMN+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>
</td>
<td id="S3.T9.1.1.20.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.20.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">80.5</td>
<td id="S3.T9.1.1.20.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">36.8</td>
<td id="S3.T9.1.1.20.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">48.3</td>
<td id="S3.T9.1.1.20.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">60.3</td>
<td id="S3.T9.1.1.20.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.20.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.20.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.20.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.20.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.20.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.20.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">60.4</td>
<td id="S3.T9.1.1.20.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.20.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
</tr>
<tr id="S3.T9.1.1.21" class="ltx_tr">
<td id="S3.T9.1.1.21.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">MCB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</td>
<td id="S3.T9.1.1.21.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.21.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">81.7</td>
<td id="S3.T9.1.1.21.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">36.9</td>
<td id="S3.T9.1.1.21.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">49.0</td>
<td id="S3.T9.1.1.21.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">61.1</td>
<td id="S3.T9.1.1.21.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.21.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.21.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.21.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.21.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.21.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.21.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.21.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.21.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
</tr>
<tr id="S3.T9.1.1.22" class="ltx_tr">
<td id="S3.T9.1.1.22.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">DualNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>
</td>
<td id="S3.T9.1.1.22.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.22.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">82.0</td>
<td id="S3.T9.1.1.22.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">37.9</td>
<td id="S3.T9.1.1.22.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">49.2</td>
<td id="S3.T9.1.1.22.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">61.5</td>
<td id="S3.T9.1.1.22.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.22.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">66.7</td>
<td id="S3.T9.1.1.22.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.22.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">81.9</td>
<td id="S3.T9.1.1.22.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">37.8</td>
<td id="S3.T9.1.1.22.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">49.7</td>
<td id="S3.T9.1.1.22.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">61.7</td>
<td id="S3.T9.1.1.22.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.22.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">66.7</td>
</tr>
<tr id="S3.T9.1.1.23" class="ltx_tr">
<td id="S3.T9.1.1.23.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">MRN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S3.T9.1.1.23.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.23.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">82.3</td>
<td id="S3.T9.1.1.23.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">39.1</td>
<td id="S3.T9.1.1.23.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">48.8</td>
<td id="S3.T9.1.1.23.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">61.5</td>
<td id="S3.T9.1.1.23.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.23.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">66.3</td>
<td id="S3.T9.1.1.23.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.23.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">82.4</td>
<td id="S3.T9.1.1.23.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">38.2</td>
<td id="S3.T9.1.1.23.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">49.4</td>
<td id="S3.T9.1.1.23.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">61.8</td>
<td id="S3.T9.1.1.23.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.23.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">66.3</td>
</tr>
<tr id="S3.T9.1.1.24" class="ltx_tr">
<td id="S3.T9.1.1.24.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">HieCoAtt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
<td id="S3.T9.1.1.24.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.24.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">79.7</td>
<td id="S3.T9.1.1.24.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">38.7</td>
<td id="S3.T9.1.1.24.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">51.7</td>
<td id="S3.T9.1.1.24.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">61.8</td>
<td id="S3.T9.1.1.24.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.24.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">65.8</td>
<td id="S3.T9.1.1.24.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.24.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.24.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.24.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.24.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">62.1</td>
<td id="S3.T9.1.1.24.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.24.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">66.1</td>
</tr>
<tr id="S3.T9.1.1.25" class="ltx_tr">
<td id="S3.T9.1.1.25.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">MCB-Att <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</td>
<td id="S3.T9.1.1.25.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.25.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">82.7</td>
<td id="S3.T9.1.1.25.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">37.7</td>
<td id="S3.T9.1.1.25.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">54.8</td>
<td id="S3.T9.1.1.25.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">64.2</td>
<td id="S3.T9.1.1.25.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.25.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.25.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.25.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.25.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.25.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.25.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
<td id="S3.T9.1.1.25.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.25.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">-</td>
</tr>
<tr id="S3.T9.1.1.26" class="ltx_tr">
<td id="S3.T9.1.1.26.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Joint-Loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>
</td>
<td id="S3.T9.1.1.26.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.26.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">81.9</td>
<td id="S3.T9.1.1.26.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">39.0</td>
<td id="S3.T9.1.1.26.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">53.0</td>
<td id="S3.T9.1.1.26.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">63.3</td>
<td id="S3.T9.1.1.26.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.26.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">67.7</td>
<td id="S3.T9.1.1.26.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.26.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">81.7</td>
<td id="S3.T9.1.1.26.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">38.2</td>
<td id="S3.T9.1.1.26.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">52.8</td>
<td id="S3.T9.1.1.26.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">63.2</td>
<td id="S3.T9.1.1.26.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.26.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">67.3</td>
</tr>
<tr id="S3.T9.1.1.27" class="ltx_tr">
<td id="S3.T9.1.1.27.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;">Ensemble of 7 models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</td>
<td id="S3.T9.1.1.27.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.27.3" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">83.4</td>
<td id="S3.T9.1.1.27.4" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">39.8</td>
<td id="S3.T9.1.1.27.5" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">58.5</td>
<td id="S3.T9.1.1.27.6" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">66.7</td>
<td id="S3.T9.1.1.27.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.27.8" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">70.2</td>
<td id="S3.T9.1.1.27.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.27.10" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">83.2</td>
<td id="S3.T9.1.1.27.11" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">39.5</td>
<td id="S3.T9.1.1.27.12" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">58.0</td>
<td id="S3.T9.1.1.27.13" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">66.5</td>
<td id="S3.T9.1.1.27.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.27.15" class="ltx_td ltx_align_center" style="padding-top:0.75pt;padding-bottom:0.75pt;">70.1</td>
</tr>
<tr id="S3.T9.1.1.28" class="ltx_tr">
<td id="S3.T9.1.1.28.1" class="ltx_td ltx_align_left" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span></td>
<td id="S3.T9.1.1.28.2" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.28.3" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.28.4" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.28.5" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.28.6" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.28.7" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.28.8" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.28.9" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.28.10" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.28.11" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.28.12" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.28.13" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.28.14" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
<td id="S3.T9.1.1.28.15" class="ltx_td" style="padding-top:0.75pt;padding-bottom:0.75pt;"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Reported results on the VQA-real test set in the open-ended and multiple-choice (M.C.) settings.</figcaption>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_font_italic ltx_title_subsection">3.2   Datasets of clipart images</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">This section discusses datasets of synthetic images created manually from clipart illustrations. They are often referred to as “abstract scenes” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, although this denomination is confusing since they supposedly depict <em id="S3.SS2.p1.1.1" class="ltx_emph ltx_font_italic">realistic</em> situations, albeit in minimalistic representations. Such “cartoon” images allow studying connections between vision and language by focusing on high-level semantics rather than on the visual recognition. This type of images has been used before for capturing common sense <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>, learning models of interactions between people <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, generating scenes from natural language descriptions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite>, and learning the semantic relevance of visual features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>, <a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">VQA abstract scenes</h5>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">The VQA benchmark (Section <a href="#S3.SS1" title="3.1 Datasets of natural images ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>) contains clipart scenes with questions/answer pairs as a separate and complimentary set to the real images. The aim is to enable research focused on high-level reasoning, removing the need to parse real images. As such, the scenes are provided as structured (XML) descriptions, in addition to the actual images. The scenes were created manually. Annotators were instructed to represent realistic situations through a drag-and-drop interface. Two types of scenes are possible, indoor and outdoor, each allowing a different set of elements, including animals, objects, and humans with adjustable poses. A total of 50,000 scenes were generated, and 3 questions per scene (<em id="S3.SS2.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS2.SSS0.Px1.p1.1.2" class="ltx_text"></span> a total of 150,000 questions) were collected, in a similar manner as for the real images of the VQA dataset (Section <a href="#S3.SS1" title="3.1 Datasets of natural images ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>). Each question was answered by 10 subjects who also provided a confidence score. Questions are labeled with an answer type: “yes/no”, “number”, and “other”. Interestingly, the distribution of question lengths and question types (based on the first four words of the questions) is similar to those of real images. However, the number of unique one-word answers is significantly lower (3,770 <span id="S3.SS2.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_italic">vs</span> 23,234), reflecting the smaller variations and limited set of objects in the scenes. Ambiguity in the ground truth answers is also lower with abstract scenes, as reflected by a better inter-human agreement (87.5% <span id="S3.SS2.SSS0.Px1.p1.1.4" class="ltx_text ltx_font_italic">vs</span> 83.3%). Results on these abstract scenes have so far only reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Balanced dataset</h5>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">Another version of the dataset discussed above is presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>. Most VQA datasets present strong biases such that a language-only “blind” model (<em id="S3.SS2.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS2.SSS0.Px2.p1.1.2" class="ltx_text"></span> using no visual input) can often guess correct answers. This seriously hampers the original objective of VQA of acting as a proxy to evaluate deep image understanding. Synthetic scenes allow better control over the distribution in the dataset. The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite> balance the existing abstract binary VQA dataset (discussed above) with additional complementary scenes so that each question has both “yes” and “no” answers for two very similar scenes.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p2.1" class="ltx_p">As examples on strong biases can be in the VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, any question starting with “What sport is” can be answered correctly with “tennis” 41% of the time. Similarly, “What color are the” is answered correctly with “white” 23% of the time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>. Overall, half of all questions can be answered correctly by a blind neural network, <em id="S3.SS2.SSS0.Px2.p2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS2.SSS0.Px2.p2.1.2" class="ltx_text"></span> using the question alone. This rises to more than 78% for the binary questions.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p3" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p3.1" class="ltx_p">The resulting balanced dataset contains 10,295 and 5,328 pairs of complementary scenes for the training and test set respectively. Evaluation should use the VQA evaluation metric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Results were reported using combinations of balanced and unbalanced training and test sets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>, of which we summarize the interesting observations. First, when testing on unbalanced data (<em id="S3.SS2.SSS0.Px2.p3.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS2.SSS0.Px2.p3.1.2" class="ltx_text"></span> the setting of prior work), it is better to train on similarly unbalanced, so as to learn and exploit dataset biases (<em id="S3.SS2.SSS0.Px2.p3.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS2.SSS0.Px2.p3.1.4" class="ltx_text"></span> that 69% of answers are “yes”). Second, testing on the new balanced data, it is now better to train on similarly balanced data. It forces models to use visual information, being unable to exploit language biases in the training set. In this setting, blind models perform, as expected, close to chance. The particular model evaluated is a method for visual verification that relies on language parsing and a number of hand designed rules. The authors also provide results in an even harder form, where a prediction is considered correct only when the model can answer correctly both versions (with yes and no answers) of a scene. In this setting, a language-only model gives zero performance, and this arguably constitutes one of the most rigorous metrics to quantify actual deep scene understanding.</p>
</div>
<div id="S3.SS2.SSS0.Px2.p4" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p4.1" class="ltx_p">One criticism of forcing the removal of biases in a dataset supposedly depicting realistic scenes is that it artificially shifts the distribution away from the real world. Statistical biases that appear in datasets reflect those inherent to the world, and it is arguable how much these should enter the learning process of a general VQA system.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_font_italic ltx_title_subsection">3.3   Knowledge base-enhanced datasets</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The datasets discussed above contain various ratio of purely visual questions, and questions that require external knowledge. For example, most questions in DAQUAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> are purely visual in nature, referring to colors, numbers, and physical locations of objects. In the COCO-QA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>, questions are generated automatically from image captions which describe the major visual elements of the image. In the VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, 5.5% of questions require “adult-level common sense”, but none require “knowledge base-level” knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>. We discussed in Section <a href="#S2.SS4" title="2.4 Models using external knowledge bases ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a> methods for VQA that make use of external knowledge bases. The authors of two such methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> proposed two datasets that allow highlighting this particular capability. The scope of these datasets is different than the general-purpose VQA datasets discussed above, and they are also smaller in scale.</p>
</div>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">KB-VQA</h5>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p1.1" class="ltx_p">The KB-VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> was constructed to evaluate the performance of the Ahab VQA system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>. It contains questions requiring topic-specific knowledge that is present in DBpedia. 700 images were selected from the COCO image dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> and 3 to 5 question/answer pairs were collected for each, for a total of 2,402 questions. Each question follows one of 23 predefined templates. The questions require different levels of knowledge, from common sense to encyclopedic knowledge.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">FVQA</h5>

<div id="S3.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px2.p1.1" class="ltx_p">The FVQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> contains only questions which involve external (non-visual) information. It was designed to include additional annotations to ease the supervised training of methods using knowledge bases. In contrast with most VQA datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> which only provide question/answer pairs, FVQA includes, with each question/answer, a supporting fact. These facts are represented as triple <span id="S3.SS3.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_typewriter">(arg1,rel,arg2)</span>. For example, consider the question/answer “Why are these people wearing yellow jackets ? For Safety”. It will include the supporting fact <span id="S3.SS3.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_typewriter">(wearing bright clothes,aids,safety)</span>. To collect this dataset, a large number of such facts (triples) related to visual concepts were extracted from the knowledge bases DBpedia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, ConceptNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, and Webchild <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>. Annotators chose an image and a visual element of the image, and then had to select one of those pre-extracted supporting facts related to the visual concept. They finally had to propose a question/answer that specifically involves the selected supporting fact. The dataset contains 193,005 candidate supporting facts related to 580 visual concepts (234 objects, 205 scenes and 141 attributes) for a total of 4,608 questions.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_font_italic ltx_title_subsection">3.4   Other datasets</h3>

<section id="S3.SS4.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Diagrams</h5>

<div id="S3.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS4.SSS0.Px1.p1.1" class="ltx_p">Kembhavi <em id="S3.SS4.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS4.SSS0.Px1.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> propose a dataset for VQA on diagrams, named as AI2 Diagrams (AI2D). It comprises more than 5,000 diagrams representing grade school science topics, such as the water cycle and the digestive system. Each diagram is annotated with segmentations and relationships between graphical elements. The dataset includes more than 15,000 multiple-choice questions and answers. In the same paper, the authors propose a method specifically designed to infer correct answers on this dataset. The method builds structured representations, named diagram parse graphs (DPG) with techniques specifically tailored to diagrams, <em id="S3.SS4.SSS0.Px1.p1.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS4.SSS0.Px1.p1.1.4" class="ltx_text"></span> for recognizing arrows or text with OCR. The DPGs are then used to infer correct answers. In comparison with VQA on natural images, the visual parsing of diagrams remains challenging and the questions often require a high level of reasoning, which make the task very challenging overall.</p>
</div>
</section>
<section id="S3.SS4.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Shapes</h5>

<div id="S3.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS4.SSS0.Px2.p1.1" class="ltx_p">Andreas <em id="S3.SS4.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S3.SS4.SSS0.Px2.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> propose a dataset of synthetic images. It is complimentary to datasets of natural image as it provides different challenges, by emphasizing the understanding of spatial and logical relations among multiple objects. The dataset consists of complex questions about arrangements of colored shapes. The questions are built around compositions of concepts and relations, <em id="S3.SS4.SSS0.Px2.p1.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS4.SSS0.Px2.p1.1.4" class="ltx_text"></span> <span id="S3.SS4.SSS0.Px2.p1.1.5" class="ltx_text ltx_font_italic">Is there a red shape above a circle ?</span> or <span id="S3.SS4.SSS0.Px2.p1.1.6" class="ltx_text ltx_font_italic">Is a red shape blue ?</span>. This allowed the authors to highlight the capabilities of the Neural Module Networks (see Section <a href="#S2.SS3.SSS1" title="2.3.1 Neural Module Networks ‣ 2.3 Compositional Models ‣ 2 Methods for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3.1</span></a>). Questions contain between two and four attributes, object types, or relationships. There are 244 questions and 15,616 images in total, with all questions having a yes and no answer (and corresponding supporting image). This eliminates the risk of learning biases, as discussed in Section <a href="#S3.SS2" title="3.2 Datasets of clipart images ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>. The authors provide results of their own method and of a reimplementation of a joint embedding baseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>. No other results have been reported so far. Note that this dataset is similar in spirit to the synthetic “bAbI” dataset used in textual QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Structured scene annotations for VQA</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> is currently the largest dataset available for VQA. It provides the unique advantage of human-generated structured annotations for each image in the form of scene graphs. In summary, a scene graph is formed of nodes representing visual elements of the scene, which can be objects, attributes, actions, <em id="S4.p1.1.1" class="ltx_emph ltx_font_italic">etc</em>. Nodes are linked with directed edges that represent relationships between them (see Figure <a href="#S4.F3" title="Figure 3 ‣ 4 Structured scene annotations for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for an example). A detailed description is available in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/1607.05910/assets/x3.png" id="S4.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="452" height="258" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example of a scene graph (structured annotation of an image) provided in the Visual Genome dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.</figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The inclusion of scene graphs with images is a significant step toward rich and more comprehensive annotations, compared to the more typical object-level and image-level annotations. In this section, we investigate whether scene graphs could be used to directly answer related visual questions. In other words, if we assume a perfect vision system capable of recovering the same scene graph of the input image as the one annotated by a human, could the answer be trivially obtained from it ? We check a prerequisite for such a hypothesis which is whether the answer actually appears as an element of the graph. Practically, we first build a vocabulary for each image based on its corresponding scene graph. Words in the vocabulary are formed from all node labels of the graph. Then, for each question, we check whether its answer can be matched with words or the combination of words from the vocabulary of its image (Figure <a href="#S4.F4" title="Figure 4 ‣ 4 Structured scene annotations for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>)</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/1607.05910/assets/x4.png" id="S4.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="453" height="276" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>We verify whether the answer to each question in the Visual Genome dataset can be found from the corresponding scene graph of the scene. Therefore, we first build the vocabulary of each image from the labels of all nodes and edges of its scene graph. Then, for each question, we check whether its answer can be found within the words or combination of words in the vocabulary of the corresponding image.</figcaption>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">We apply the above procedure on all images and questions of the Visual Genome dataset. We find that only 40.02% of the answers can be directly found in the scene graph, <em id="S4.p3.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.p3.1.2" class="ltx_text"></span> only 40.02% of the questions could be directly answered using the scene graph representation. Another 7% of answers are numbers (<em id="S4.p3.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.p3.1.4" class="ltx_text"></span> counting questions) which we choose to leave aside from the rest of our analysis. There remains 53% of questions can not be directly answered from the scene graph. This ratio is surprisingly high considering the apparent level of detail of the descriptions provided as scene graphs.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">To characterize the remaining 53% of questions, we examine question types using their first few words (Figure <a href="#S4.F5" title="Figure 5 ‣ 4 Structured scene annotations for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. A large number of questions starting with “what” are among those that cannot be directly answered by scene graphs. Note that the overall distribution of question types over the whole dataset (including those that could be answered directly from the scene graph) differs significantly (Figure <a href="#S4.F6" title="Figure 6 ‣ 4 Structured scene annotations for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). We report in Figure <a href="#S4.F7" title="Figure 7 ‣ 4 Structured scene annotations for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> the number of questions that cannot be answered from the scene graph as a fraction of all questions of each type separately. We find that a large fraction of the questions starting with “when”, “why” and “how” have answers not be found in scene graphs. Indeed, answering such questions often involves information that does not correspond to specific visual entities, thus not represented by nodes in the scene graphs. It may be possible however to recover these answers using common sense or object-specific knowledge.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/1607.05910/assets/x5.png" id="S4.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="451" height="236" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Number of answers that cannot be found in the scene graph for each question type.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/1607.05910/assets/x6.png" id="S4.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="451" height="228" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Total number of questions of each type.</figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/1607.05910/assets/x7.png" id="S4.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="452" height="300" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Answers that can not be found in the scene graph or the knowledge-expanded scene graph, measured as a fraction of all questions of each type separately.</figcaption>
</figure>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">We recorded answers that could not be found in scene graphs and ranked them by frequency of occurrence (Table <a href="#S4.T10" title="Table 10 ‣ 4 Structured scene annotations for VQA ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>). Answers to “what” questions that can not be found in the scene graph are mainly attributes like colours and materials, which are probably considered too fine-grained by annotators for inclusion in the scene graphs. The missing answers to the “where” questions are mostly global scene labels such as bedroom, street, zoo, <em id="S4.p5.1.1" class="ltx_emph ltx_font_italic">etc</em>. The missing answers to “when” questions are, for 90% of them, daytime, night, and variants thereof. These labels are seldom represented in the scene graph, and a large number of synonyms can represent a similar semantic concept. Finally, the “why” and “how” questions lead to higher-level concepts such as actions, reasons, weather types, <em id="S4.p5.1.2" class="ltx_emph ltx_font_italic">etc</em>.</p>
</div>
<figure id="S4.T10" class="ltx_table">
<table id="S4.T10.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T10.1.1" class="ltx_tr">
<td id="S4.T10.1.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T10.1.1.1.1" class="ltx_text ltx_font_italic">what</span></td>
<td id="S4.T10.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S4.T10.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T10.1.1.2.1.1" class="ltx_p" style="width:369.9pt;">white, green, brown, black, blue, wood, red, gray, yellow, grey, black and white, metal, silver, orange, tree, male, tan, round, sunny, brick, grass, skateboarding, cloud, daytime, surfing, right, skiing, left, pink, dirt, female, standing, water, …</span>
</span>
</td>
</tr>
<tr id="S4.T10.1.2" class="ltx_tr">
<td id="S4.T10.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T10.1.2.1.1" class="ltx_text ltx_font_italic">where</span></td>
<td id="S4.T10.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S4.T10.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T10.1.2.2.1.1" class="ltx_p" style="width:369.9pt;">ground, air, street, table, field, road, sidewalk, zoo, left, sky, right, water, beach, wall, kitchen, background, restaurant, park, bathroom, living room, ocean, in distance, tennis court, plate, airport, bedroom, city, baseball field, …</span>
</span>
</td>
</tr>
<tr id="S4.T10.1.3" class="ltx_tr">
<td id="S4.T10.1.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T10.1.3.1.1" class="ltx_text ltx_font_italic">when</span></td>
<td id="S4.T10.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S4.T10.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T10.1.3.2.1.1" class="ltx_p" style="width:369.9pt;">daytime, during day, day time, during daytime, in daytime, afternoon, at night, night time, winter, now, nighttime, during day time, night, morning, outside during day time, during daylight hour, evening, daylight, sunny day, daylight hour, …</span>
</span>
</td>
</tr>
<tr id="S4.T10.1.4" class="ltx_tr">
<td id="S4.T10.1.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T10.1.4.1.1" class="ltx_text ltx_font_italic">who</span></td>
<td id="S4.T10.1.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S4.T10.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T10.1.4.2.1.1" class="ltx_p" style="width:369.9pt;">no one, nobody, man, person, woman, photographer, boy, lady, girl, pilot, surfer, child, man on right, man on left, skateboarder, tennis player, no person, little girl, spectator, skier, conductor,guy, passenger, young man, man and woman, …</span>
</span>
</td>
</tr>
<tr id="S4.T10.1.5" class="ltx_tr">
<td id="S4.T10.1.5.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T10.1.5.1.1" class="ltx_text ltx_font_italic">why</span></td>
<td id="S4.T10.1.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S4.T10.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T10.1.5.2.1.1" class="ltx_p" style="width:369.9pt;">sunny, to hit ball, to eat, sunlight, raining, safety, daytime, to be eaten, during day, remembrance, for fun, for balance, cold, to surf, resting, to play tennis, protection, sun, balance, winter, to catch frisbee, decoration, to play, …</span>
</span>
</td>
</tr>
<tr id="S4.T10.1.6" class="ltx_tr">
<td id="S4.T10.1.6.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;"><span id="S4.T10.1.6.1.1" class="ltx_text ltx_font_italic">how</span></td>
<td id="S4.T10.1.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" style="padding-top:0.75pt;padding-bottom:0.75pt;">
<span id="S4.T10.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T10.1.6.2.1.1" class="ltx_p" style="width:369.9pt;">clear, none, sunny, cloudy, overcast, calm, open, good, closed, white, clear blue, happy, standing, short, partly cloudy, rainy, green, blue, cold, wet, black, brown, in motion, long, down, blurry, dirty, small, up, large, clean, gray, upside down, sliced, …</span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Frequent answers that cannot be found in scene graphs of the input image, for each question type, ranked by rate of occurrence.</figcaption>
</figure>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">The above analysis leads to the conclusion that the current scene graphs are rich intermediate abstractions of the scenes, but they are not comprehensive enough to capture <em id="S4.p6.1.1" class="ltx_emph ltx_font_italic">all</em> elements required for VQA. For example, low-level visual attributes such as colour and material are lost in this representation and one therefore needs to access the image to answer questions involving them. Global scene attributes such as location, weather, or time of day are seldom labeled but they are often involved in “where” and “when” questions. It remains debatable whether more human effort should be invested to obtain comprehensive annotations. Another solution is to combine visual datasets with large-scale knowledge bases (KBs) that provide common sense information about visual and non-visual concepts. The type of information in those KBs is complementary to visual annotations like scene graphs. For example, although “daytime” may not be annotated for a particular scene, the annotation of a “clear blue sky” may lead to reason about daytime given some common sense knowledge. Similar reasoning could <em id="S4.p6.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.p6.1.3" class="ltx_text"></span> associate “oven” to “kitchen”, “food” to “eat”, and “snow” to “cold”.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p">We perform an additional experiment to estimate the potential of connecting scene graphs with a general purpose KB.</p>
</div>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.1" class="ltx_p">For each question, we examine whether the correct answer can be found in a first-order extension of the scene graph using relations from relations in the KB.</p>
</div>
<div id="S4.p9" class="ltx_para">
<p id="S4.p9.1" class="ltx_p">More specifically, we use the labels of all nodes of the scene graph to query 3 large KBs (DBpedia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, WebChild <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>, and ConceptNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>). The triples resulting from the query are used to expand the scene graph and its vocabulary. For example, a scene graph node labeled “cat” may return the fact <span id="S4.p9.1.1" class="ltx_text ltx_font_typewriter">&lt;cat,isa,mammal&gt;</span>, which will be appended to the “cat” node of the scene graph. This simple procedure effectively completes the scene-specific graph with general, relevant knowledge. Similarly as above, we then examine whether questions could potentially be answered from this representation alone, checking for matches between the correct answer and words or combination of words from the expanded vocabulary. We find that this is the case for 79.58% of the questions, which is nearly the double of the same experiment without the KB expansion (40.02%). This clearly shows the potential for complementing the interpretation of visual contents with information from general-purpose KBs.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and future directions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The introduction of the task of VQA is relatively recent and it sparked significant interest and accelerating developments in just a few years. VQA is a complex task and it was initially encouraged by a certain level of maturity reached in the fundamental tasks of computer vision such as image recognition. VQA is particularly attractive because it constitutes an AI complete task in its ultimate form, <em id="S5.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.p1.1.2" class="ltx_text"></span> considering open-world free-form questions and answers. Recent results, although encouraging, should however not fool us, as this ultimate goal is indisputably a long way from any current technique. Reduced and limited forms of VQA, <em id="S5.p1.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.p1.1.4" class="ltx_text"></span> multiple-choice format, short answer lengths, limited types of questions, <em id="S5.p1.1.5" class="ltx_emph ltx_font_italic">etc</em>.<span id="S5.p1.1.6" class="ltx_text"></span>, are reasonable intermediate objectives that seem attainable. Their evaluation is practically easier and may be more representative of our actual progress.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Our review of datasets (Section <a href="#S3" title="3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) showed a diversity of protocols for collecting data (from human annotators or semi-automatically from image captions) and imposing certain constraints (<em id="S5.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.p2.1.2" class="ltx_text"></span> focusing on certain image regions, objects, or types of questions). These choices influence the collected questions and answers in many ways. First, they impact the level of complexity and number of facts involved, <em id="S5.p2.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.p2.1.4" class="ltx_text"></span> whether the correct answers can be inferred after recognizing a single item/relation/attribute, or requires inference over multiple elements and characteristics of the scene. Second, they influence the ratio of visual <span id="S5.p2.1.5" class="ltx_text ltx_font_italic">vs</span> textual understanding required. One extreme example is the synthetic “Shapes” dataset (Section <a href="#S3.SS4.SSS0.Px2" title="Shapes ‣ 3.4 Other datasets ‣ 3 Datasets and evaluation ‣ Visual Question Answering: A Survey of Methods and Datasets" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>) which only requires recognizing a handful of shapes and colors by their names, and rather places the emphasis on the reasoning over relationships between such elements. Third, they influence the amount of prior external knowledge required. External is to be understood in the sense of not inferable from the given visual and textual input. This information may however be visual in nature, <em id="S5.p2.1.6" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.p2.1.7" class="ltx_text"></span> bright blue skies occur during daytime, or yellow jackets are usually worn for safety.</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">External knowledge</h5>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">As mentioned above, VQA constitutes an AI-complete challenge since most tasks in AI can be formulated as questions over images. Note however that these questions will often require external knowledge to be answered. This is a reason for the recent interest in methods connecting VQA with structured knowledge bases, and in specific datasets of questions requiring such mechanisms. One may argue that such complex questions are a distraction from the purely visual questions that should be tackled first. We believe that both paths can be explored in parallel. Unfortunately, the current approaches that use knowledge bases for VQA present serious limitations. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> can only handle a limited number of question types predefined by hand-coded templates. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> encode retrieved information using Doc2Vec, but the encoding process is question-independent and may include information irrelevant to the question. The concept of memory-augmented neural networks could offer a suitable and scalable framework for incorporating and adaptively selecting relevant external knowledge for VQA. This avenue has not been explored yet to our knowledge. On the dataset front, questions involving significant external knowledge are unevenly represented. Specific datasets have been proposed with such questions and additional annotations of supporting facts, but they are limited in scale. Efforts on datasets will likely stimulate research in this direction and help training suitable models. Note that these efforts could simply involve additional annotations of existing datasets.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Textual question answering</h5>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">The task of textual question answering predates its visual counterpart by several decades and has produced a substantial amount of work. Two distinct types of approaches have traditionally been proposed: information retrieval, and semantic parsing coupled with knowledge bases. On the one hand, information retrieval approaches use unstructured collections of documents, in which the key words of the question are looked for to identify relevant passages and sentences <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. A ranking function then sorts these candidates, and the answer is extracted from one or several top matches. This approach can be compared to the basic “joint embedding with attention” method of VQA, where features describing each image region are compared to a representation of the question, identifying the region(s) to focus on, then extracting the answer. A key concept is the prediction of answer type from the question (<em id="S5.SS0.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.SS0.SSS0.Px2.p1.1.2" class="ltx_text"></span> a colour, a date, a person, <em id="S5.SS0.SSS0.Px2.p1.1.3" class="ltx_emph ltx_font_italic">etc</em>.<span id="S5.SS0.SSS0.Px2.p1.1.4" class="ltx_text"></span>) to facilitate the final extraction of the answer from candidate passages. This very concept of answer-type prediction was recently brought back to VQA by Kafle and Kanan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. On the other hand, the semantic parsing approaches focus on a better understanding of the question, using more sophisticated language models and parsers to turn the question into structured queries <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>. These queries can then be executed on domain-specific databases or general purpose structured knowledge bases. A similar process is used in the VQA methods of Wang <em id="S5.SS0.SSS0.Px2.p1.1.5" class="ltx_emph ltx_font_italic">et al</em>.<span id="S5.SS0.SSS0.Px2.p1.1.6" class="ltx_text"></span> for querying external knowledge bases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>.</p>
</div>
<div id="S5.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p2.1" class="ltx_p">As we stated before, interest in VQA grew from the maturity of deep learning on tasks of image recognition (of objects, activities, scenes, <em id="S5.SS0.SSS0.Px2.p2.1.1" class="ltx_emph ltx_font_italic">etc</em>.<span id="S5.SS0.SSS0.Px2.p2.1.2" class="ltx_text"></span>). Most current work on VQA is therefore built with tools and methods from the computer vision community. Textual question answering has traditionally been addressed in the natural language processing community, with different approaches and algorithms. A number of concepts have permeated from NLP to recent efforts on VQA, for example word embeddings, sentence representations, processing with recurrent neural networks, <em id="S5.SS0.SSS0.Px2.p2.1.3" class="ltx_emph ltx_font_italic">etc</em>. Some notable successes are attributable to joint efforts from both fields (<em id="S5.SS0.SSS0.Px2.p2.1.4" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.SS0.SSS0.Px2.p2.1.5" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>). We believe that there still exists potential for better use of concepts from NLP for addressing challenges in VQA. Language models are trainable on large amounts of minimally-labeled text, independently from visual data. They can then be used in the output stage of VQA systems to generate long answers in natural language. Similarly, syntactic parsers may be pre-trained on text alone and be reused for a more principled processing of input questions. The understanding of the question does not have to be trained end-to-end as most VQA systems currently do. The interpretation of text queries into logical representations has been studied in NLP in its own right (<em id="S5.SS0.SSS0.Px2.p2.1.6" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.SS0.SSS0.Px2.p2.1.7" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>).</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This article presented a comprehensive review of the state-of-the-art on visual question answering. We reviewed the most popular approach that maps questions and images to vector representations in a common feature space. We described additional improvements that build up on this concept, namely attention mechanisms, modular and memory-augmented architectures. We reviewed the growing number of datasets available for training and evaluating VQA methods, highlighting differences in the type and difficulty of questions that they include. In addition to a descriptive review, we pinpointed a number of promising directions for future research. In particular, we suggest to scale up the inclusion of additional external knowledge from structured knowledge bases, as well as a continued exploration of the potential of natural language processing tools. We believe that the ongoing and future work on these particular points will benefit the specific task of VQA as well as the general objective of visual scene understanding.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This research was in part supported by the Data to Decisions Cooperative Research Centre funded by the Australian Government.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Andreas, M. Rohrbach, T. Darrell, and D. Klein.

</span>
<span class="ltx_bibblock">Learning to compose neural networks for question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Annual Conference of the North American Chapter of the
Association for Computational Linguistics</span>, 2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Andreas, M. Rohrbach, T. Darrell, and D. Klein.

</span>
<span class="ltx_bibblock">Neural Module Networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</span>, 2016.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and
D. Parikh.

</span>
<span class="ltx_bibblock">VQA: Visual Question Answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Int. Conf. Comp. Vis.</span>, 2015.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S. Antol, C. L. Zitnick, and D. Parikh.

</span>
<span class="ltx_bibblock">Zero-shot learning via visual abstraction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Proc. Eur. Conf. Comp. Vis.</span>, 2014.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, and Z. Ives.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">DBpedia: A nucleus for a web of open data</span>.

</span>
<span class="ltx_bibblock">Springer, 2007.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni.

</span>
<span class="ltx_bibblock">Open information extraction for the web.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proc. Int. Joint Conf. Artificial Intell.</span>, 2007.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor.

</span>
<span class="ltx_bibblock">Freebase: a collaboratively created graph database for structuring
human knowledge.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">ACM SIGMOD International Conference on Management of Data</span>,
pages 1247–1250. ACM, 2008.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. Bordes, N. Usunier, S. Chopra, and J. Weston.

</span>
<span class="ltx_bibblock">Large-scale simple question answering with memory networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1506.02075</span>, 2015.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
R. Cantrell, M. Scheutz, P. Schermerhorn, and X. Wu.

</span>
<span class="ltx_bibblock">Robust spoken instruction understanding for hri.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Human-Robot Interaction (HRI), 2010 5th ACM/IEEE
International Conference on</span>, pages 275–282. IEEE, 2010.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Carlson, J. Betteridge, B. Kisiel, and B. Settles.

</span>
<span class="ltx_bibblock">Toward an Architecture for Never-Ending Language Learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proc. Conf. AAAI</span>, 2010.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
K. Chen, J. Wang, L.-C. Chen, H. Gao, W. Xu, and R. Nevatia.

</span>
<span class="ltx_bibblock">ABC-CNN: An Attention Based Convolutional Neural Network for Visual
Question Answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.05960</span>, 2015.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dollar, and C. L.
Zitnick.

</span>
<span class="ltx_bibblock">Microsoft COCO captions: Data collection and evaluation server.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1504.00325</span>, 2015.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M.-C. de Marneffe and C. D. Manning.

</span>
<span class="ltx_bibblock">The stanford typed dependencies representation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">COLING Workshop on Cross-framework and Cross-domain Parser
Evaluation</span>, 2008.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</span>, 2009.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan,
K. Saenko, and T. Darrell.

</span>
<span class="ltx_bibblock">Long-term recurrent convolutional networks for visual recognition
and description.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</span>, 2015.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
L. Dong and M. Lapata.

</span>
<span class="ltx_bibblock">Language to logical form with neural attention.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proc. Conf. Association for Computational Linguistics</span>,
2016.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
O. Etzioni, A. Fader, J. Christensen, S. Soderland, and M. Mausam.

</span>
<span class="ltx_bibblock">Open Information Extraction: The Second Generation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proc. Int. Joint Conf. Artificial Intell.</span>, 2011.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
A. Fader, S. Soderland, and O. Etzioni.

</span>
<span class="ltx_bibblock">Identifying relations for open information extraction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proc. Conf. Empirical Methods in Natural Language
Processing</span>, 2011.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
F. Ferraro, N. Mostafazadeh, T.-H. Huang, L. Vanderwende, J. Devlin, M. Galley,
and M. Mitchell.

</span>
<span class="ltx_bibblock">A survey of current datasets for vision and language research.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Conference on Empirical Methods in Natural Language
Processing</span>, pages 207–213. Association for Computational Linguistics, 2015.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
D. F. Fouhey and C. Zitnick.

</span>
<span class="ltx_bibblock">Predicting object dynamics in scenes.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</span>, 2014.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and M. Rohrbach.

</span>
<span class="ltx_bibblock">Multimodal compact bilinear pooling for visual question answering and
visual grounding.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.01847</span>, 2016.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu.

</span>
<span class="ltx_bibblock">Are You Talking to a Machine? Dataset and Methods for Multilingual
Image Question Answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proc. Advances in Neural Inf. Process. Syst.</span>, 2015.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
D. Geman, S. Geman, N. Hallonquist, and L. Younes.

</span>
<span class="ltx_bibblock">Visual Turing test for computer vision systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proceedings of the National Academy of Sciences</span>,
112(12):3618–3623, 2015.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
R. W. Group et al.

</span>
<span class="ltx_bibblock">Resource description framework, 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://www.w3.org/standards/techs/rdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.w3.org/standards/techs/rdf</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</span>, 2016.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
M. Hodosh, P. Young, and J. Hockenmaier.

</span>
<span class="ltx_bibblock">Framing image description as a ranking task: Data, models and
evaluation metrics.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">JAIR</span>, pages 853–899, 2013.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
J. Hoffart, F. M. Suchanek, K. Berberich, and G. Weikum.

</span>
<span class="ltx_bibblock">YAGO2: A spatially and temporally enhanced knowledge base from
Wikipedia.

</span>
<span class="ltx_bibblock">In <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Proc. Int. Joint Conf. Artificial Intell.</span>, 2013.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Darrell.

</span>
<span class="ltx_bibblock">Natural language object retrieval.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</span>, 2016.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
I. Ilievski, S. Yan, and J. Feng.

</span>
<span class="ltx_bibblock">A focused dynamic attention model for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1604.01485</span>, 2016.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
M. Iyyer, J. Boyd-Graber, L. Claudino, R. Socher, and H. Daumé III.

</span>
<span class="ltx_bibblock">A neural network for factoid question answering over paragraphs.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Empirical Methods in Natural Language Processing</span>, 2014.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A. Jabri, A. Joulin, and L. van der Maaten.

</span>
<span class="ltx_bibblock">Revisiting visual question answering baselines.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.08390</span>, 2016.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
A. Jiang, F. Wang, F. Porikli, and Y. Li.

</span>
<span class="ltx_bibblock">Compositional Memory for Visual Question Answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.05676</span>, 2015.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
D. Jurafsky and J. H. Martin.

</span>
<span class="ltx_bibblock">Question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Speech and Language Processing: An Introduction to Natural
Language Processing, Computational Linguistics, and Speech Recognition</span>,
chapter 28. Prentice Hall PTR, 2000.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
K. Kafle and C. Kanan.

</span>
<span class="ltx_bibblock">Answer-type prediction for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</span>, 2016.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
A. Karpathy, A. Joulin, and F. F. Li.

</span>
<span class="ltx_bibblock">Deep fragment embeddings for bidirectional image sentence mapping.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Proc. Advances in Neural Inf. Process. Syst.</span>, 2014.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
S. Kazemzadeh, V. Ordonez, M. Matten, and T. L. Berg.

</span>
<span class="ltx_bibblock">Referitgame: Referring to objects in photographs of natural scenes.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Conference on Empirical Methods in Natural Language
Processing</span>, pages 787–798, 2014.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
A. Kembhavi, M. Salvato, E. Kolve, M. J. Seo, H. Hajishirzi, and A. Farhadi.

</span>
<span class="ltx_bibblock">A diagram is worth a dozen images.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1603.07396</span>, 2016.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
J.-H. Kim, S.-W. Lee, D.-H. Kwak, M.-O. Heo, J. Kim, J.-W. Ha, and B.-T. Zhang.

</span>
<span class="ltx_bibblock">Multimodal residual learning for visual qa.

</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.01455</span>, 2016.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
T. Kollar, J. Krishnamurthy, and G. P. Strimel.

</span>
<span class="ltx_bibblock">Toward interactive grounded language acqusition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">Robotics: Science and Systems</span>, 2013.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
O. Kolomiyets and M.-F. Moens.

</span>
<span class="ltx_bibblock">A survey on question answering technology from an information
retrieval perspective.

</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">Information Sciences</span>, 181(24):5412 – 5434, 2011.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
Y. Kalantidis, L.-J. Li, D. A. Shamma, M. Bernstein, and L. Fei-Fei.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1602.07332</span>, 2016.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
A. Kumar, O. Irsoy, J. Su, J. Bradbury, R. English, B. Pierce, P. Ondruska,
I. Gulrajani, and R. Socher.

</span>
<span class="ltx_bibblock">Ask me anything: Dynamic memory networks for natural language
processing.

</span>
<span class="ltx_bibblock">In <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">Proc. Int. Conf. Mach. Learn.</span>, 2016.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
S. Li, G. Kulkarni, T. L. Berg, A. C. Berg, and Y. Choi.

</span>
<span class="ltx_bibblock">Composing simple image descriptions using web-scale n-grams.

</span>
<span class="ltx_bibblock">In <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">The SIGNLL Conference on Computational Natural Language
Learning</span>, 2011.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
P. Liang, M. I. Jordan, and D. Klein.

</span>
<span class="ltx_bibblock">Learning dependency-based compositional semantics.

</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">Computational Linguistics</span>, 39(2):389–446, 2013.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick.

</span>
<span class="ltx_bibblock">Microsoft COCO: Common objects in context.

</span>
<span class="ltx_bibblock">In <span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">Proc. Eur. Conf. Comp. Vis.</span>, 2014.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
X. Lin and D. Parikh.

</span>
<span class="ltx_bibblock">Don’t just listen, use your imagination: Leveraging visual common
sense for non-visual tasks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</span>, 2015.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
H. Liu and P. Singh.

</span>
<span class="ltx_bibblock">ConceptNet - A practical commonsense reasoning toolkit.

</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">BT technology journal</span>, 22(4):211–226, 2004.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
J. Lu, J. Yang, D. Batra, and D. Parikh.

</span>
<span class="ltx_bibblock">Hierarchical question-image co-attention for visual question
answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.00061</span>, 2016.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
L. Ma, Z. Lu, and H. Li.

</span>
<span class="ltx_bibblock">Learning to Answer Questions From Image using Convolutional Neural
Network.

</span>
<span class="ltx_bibblock">In <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">Proc. Conf. AAAI</span>, 2016.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
F. Mahdisoltani, J. Biega, and F. Suchanek.

</span>
<span class="ltx_bibblock">YAGO3: A knowledge base from multilingual Wikipedias.

</span>
<span class="ltx_bibblock">In <span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">CIDR</span>, 2015.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
M. Malinowski and M. Fritz.

</span>
<span class="ltx_bibblock">A multi-world approach to question answering about real-world scenes
based on uncertain input.

</span>
<span class="ltx_bibblock">In <span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">Proc. Advances in Neural Inf. Process. Syst.</span>, pages
1682–1690, 2014.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
M. Malinowski, M. Rohrbach, and M. Fritz.

</span>
<span class="ltx_bibblock">Ask Your Neurons: A Neural-based Approach to Answering Questions
about Images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Int. Conf. Comp. Vis.</span>, 2015.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
J. Mao, H. Jonathan, A. Toshev, O. Camburu, A. Yuille, and K. Murphy.

</span>
<span class="ltx_bibblock">Generation and comprehension of unambiguous object descriptions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</span>, 2016.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
J. Mao, W. Xu, Y. Yang, J. Wang, and A. Yuille.

</span>
<span class="ltx_bibblock">Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN).

</span>
<span class="ltx_bibblock">In <span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">Proc. Int. Conf. Learn. Representations</span>, 2015.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
C. Matuszek, N. FitzGerald, L. Zettlemoyer, L. Bo, and D. Fox.

</span>
<span class="ltx_bibblock">A joint model of language and perception for grounded attribute
learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">Proc. Int. Conf. Mach. Learn.</span>, 2012.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
T. Mikolov, K. Chen, G. Corrado, and J. Dean.

</span>
<span class="ltx_bibblock">Efficient estimation of word representations in vector space.

</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1301.3781</span>, 2013.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
H. Noh and B. Han.

</span>
<span class="ltx_bibblock">Training recurrent answering units with joint loss minimization for
vqa.

</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.03647</span>, 2016.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
H. Noh, P. H. Seo, and B. Han.

</span>
<span class="ltx_bibblock">Image Question Answering using Convolutional Neural Network with
Dynamic Parameter Prediction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</span>, 2016.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
B. Peng, Z. Lu, H. Li, and K. Wong.

</span>
<span class="ltx_bibblock">Towards neural network-based reasoning.

</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1508.05508</span>, 2015.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
B. Peng and K. Yao.

</span>
<span class="ltx_bibblock">Recurrent neural networks with external memory for language
understanding.

</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1506.00195</span>, 2015.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
J. Pennington, R. Socher, and C. Manning.

</span>
<span class="ltx_bibblock">Glove: Global Vectors for Word Representation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">Conference on Empirical Methods in Natural Language
Processing</span>, Doha, Qatar, 2014.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
E. Prud’Hommeaux, A. Seaborne, et al.

</span>
<span class="ltx_bibblock">SPARQL query language for RDF.

</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">W3C recommendation</span>, 15, 2008.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
M. Ren, R. Kiros, and R. Zemel.

</span>
<span class="ltx_bibblock">Image Question Answering: A Visual Semantic Embedding Model and a
New Dataset.

</span>
<span class="ltx_bibblock">In <span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">Proc. Advances in Neural Inf. Process. Syst.</span>, 2015.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
D. Roy, K.-Y. Hsiao, and N. Mavridis.

</span>
<span class="ltx_bibblock">Conversational robots: building blocks for grounding word meaning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">HLT-NAACL Workshop on Learning word meaning from
non-linguistic data</span>, pages 70–77. Association for Computational
Linguistics, 2003.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
K. Saito, A. Shin, Y. Ushiku, and T. Harada.

</span>
<span class="ltx_bibblock">Dualnet: Domain-invariant network for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.06108</span>, 2016.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
K. J. Shih, S. Singh, and D. Hoiem.

</span>
<span class="ltx_bibblock">Where to look: Focus regions for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</span>, 2016.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
N. Silberman, D. Hoiem, P. Kohli, and R. Fergus.

</span>
<span class="ltx_bibblock">Indoor segmentation and support inference from rgbd images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">Proc. Eur. Conf. Comp. Vis.</span>, 2012.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
K. Simonyan and A. Zisserman.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">arXiv: Comp. Res. Repository</span>, abs/1409.1556, 2014.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
V. Singh and S. K. Dwivedi.

</span>
<span class="ltx_bibblock">Question answering: A survey of research, techniques and issues.

</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">Int. J. Inf. Retr. Res.</span>, 4(3), 2014.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus.

</span>
<span class="ltx_bibblock">Weakly supervised memory networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1503.08895</span>, 2015.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich.

</span>
<span class="ltx_bibblock">Going deeper with convolutions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib71.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</span>, 2015.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
N. Tandon, G. de Melo, F. Suchanek, and G. Weikum.

</span>
<span class="ltx_bibblock">Webchild: Harvesting and organizing commonsense knowledge from the
web.

</span>
<span class="ltx_bibblock">In <span id="bib.bib72.1.1" class="ltx_text ltx_font_italic">International Conference on Web Search and Data Mining</span>.
ACM, 2014.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
N. Tandon, G. De Melo, and G. Weikum.

</span>
<span class="ltx_bibblock">Acquiring Comparative Commonsense Knowledge from the Web.

</span>
<span class="ltx_bibblock">In <span id="bib.bib73.1.1" class="ltx_text ltx_font_italic">Proc. Conf. AAAI</span>, 2014.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
K. Tu, M. Meng, M. W. Lee, T. E. Choe, and S.-C. Zhu.

</span>
<span class="ltx_bibblock">Joint video and text parsing for understanding events and answering
queries.

</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Multimedia</span>, 21(2):42–70, 2014.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
R. Vedantam, X. Lin, T. Batra, C. L. Zitnick, and D. Parikh.

</span>
<span class="ltx_bibblock">Learning common sense through visual abstraction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib75.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Int. Conf. Comp. Vis.</span>, 2015.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
R. Vedantam, C. L. Zitnick, and D. Parikh.

</span>
<span class="ltx_bibblock">CIDEr: Consensus-based Image Description Evaluation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib76.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</span>, 2015.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
O. Vinyals, A. Toshev, S. Bengio, and D. Erhan.

</span>
<span class="ltx_bibblock">Show and tell: A neural image caption generator.

</span>
<span class="ltx_bibblock">In <span id="bib.bib77.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</span>, 2014.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
P. Wang, Q. Wu, C. Shen, A. v. d. Hengel, and A. Dick.

</span>
<span class="ltx_bibblock">Explicit knowledge-based reasoning for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib78.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.02570</span>, 2015.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
P. Wang, Q. Wu, C. Shen, A. v. d. Hengel, and A. Dick.

</span>
<span class="ltx_bibblock">Fvqa: Fact-based visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib79.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.05433</span>, 2016.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
P. Wang, Q. Wu, C. Shen, A. van den Hengel, and A. Dick.

</span>
<span class="ltx_bibblock">FVQA: Fact-based visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib80.1.1" class="ltx_text ltx_font_italic">arXiv:1606.05433</span>, 2016.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Y. Wen-tau, C. Ming-Wei, H. Xiaodong, and G. Jianfeng.

</span>
<span class="ltx_bibblock">Semantic parsing via staged query graph generation: Question
answering with knowledge base.

</span>
<span class="ltx_bibblock">In <span id="bib.bib81.1.1" class="ltx_text ltx_font_italic">International Joint Conference on Natural Language Processing
of the AFNLP</span>. ACL, July 2015.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
J. Weston, A. Bordes, S. Chopra, and T. Mikolov.

</span>
<span class="ltx_bibblock">Towards ai-complete question answering: A set of prerequisite toy
tasks.

</span>
<span class="ltx_bibblock"><span id="bib.bib82.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1502.05698</span>, 2015.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
J. Weston, S. Chopra, and A. Bordes.

</span>
<span class="ltx_bibblock">Memory networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib83.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1410.3916</span>, 2014.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
T. Winograd.

</span>
<span class="ltx_bibblock">Understanding natural language.

</span>
<span class="ltx_bibblock"><span id="bib.bib84.1.1" class="ltx_text ltx_font_italic">Cognitive psychology</span>, 3(1):1–191, 1972.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Q. Wu, C. Shen, A. v. d. Hengel, L. Liu, and A. Dick.

</span>
<span class="ltx_bibblock">What Value Do Explicit High Level Concepts Have in Vision to
Language Problems?

</span>
<span class="ltx_bibblock">In <span id="bib.bib85.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</span>, 2016.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Q. Wu, C. Shen, A. v. d. Hengel, P. Wang, and A. Dick.

</span>
<span class="ltx_bibblock">Image captioning and visual question answering based on attributes
and their related external knowledge.

</span>
<span class="ltx_bibblock"><span id="bib.bib86.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1603.02814</span>, 2016.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Q. Wu, P. Wang, C. Shen, A. Dick, and A. v. d. Hengel.

</span>
<span class="ltx_bibblock">Ask Me Anything: Free-form Visual Question Answering Based on
Knowledge from External Sources.

</span>
<span class="ltx_bibblock">In <span id="bib.bib87.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</span>, 2016.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Z. Wu and M. Palmer.

</span>
<span class="ltx_bibblock">Verbs semantics and lexical selection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib88.1.1" class="ltx_text ltx_font_italic">Proc. Conf. Association for Computational Linguistics</span>,
1994.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
C. Xiong, S. Merity, and R. Socher.

</span>
<span class="ltx_bibblock">Dynamic memory networks for visual and textual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib89.1.1" class="ltx_text ltx_font_italic">Proc. Int. Conf. Mach. Learn.</span>, 2016.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
H. Xu and K. Saenko.

</span>
<span class="ltx_bibblock">Ask, Attend and Answer: Exploring Question-Guided Spatial Attention
for Visual Question Answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib90.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.05234</span>, 2015.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov, R. Zemel, and
Y. Bengio.

</span>
<span class="ltx_bibblock">Show, Attend and Tell: Neural Image Caption Generation with Visual
Attention.

</span>
<span class="ltx_bibblock">In <span id="bib.bib91.1.1" class="ltx_text ltx_font_italic">Proc. Int. Conf. Mach. Learn.</span>, 2015.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Z. Yang, X. He, J. Gao, L. Deng, and A. Smola.

</span>
<span class="ltx_bibblock">Stacked Attention Networks for Image Question Answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib92.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</span>, 2016.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle, and A. Courville.

</span>
<span class="ltx_bibblock">Describing videos by exploiting temporal structure.

</span>
<span class="ltx_bibblock">In <span id="bib.bib93.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Int. Conf. Comp. Vis.</span>, 2015.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
P. Young, A. Lai, M. Hodosh, and J. Hockenmaier.

</span>
<span class="ltx_bibblock">From image descriptions to visual denotations: New similarity
metrics for semantic inference over event descriptions.

</span>
<span class="ltx_bibblock"><span id="bib.bib94.1.1" class="ltx_text ltx_font_italic">Proc. Conf. Association for Computational Linguistics</span>, 2,
2014.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
L. Yu, E. Park, A. C. Berg, and T. L. Berg.

</span>
<span class="ltx_bibblock">Visual madlibs: Fill in the blank image generation and question
answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib95.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Int. Conf. Comp. Vis.</span>, 2015.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
M. D. Zeiler and R. Fergus.

</span>
<span class="ltx_bibblock">Visualizing and understanding convolutional networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib96.1.1" class="ltx_text ltx_font_italic">Proc. Eur. Conf. Comp. Vis.</span>, 2014.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
L. S. Zettlemoyer and M. Collins.

</span>
<span class="ltx_bibblock">Online learning of relaxed ccg grammars for parsing to logical form.

</span>
<span class="ltx_bibblock">In <span id="bib.bib97.1.1" class="ltx_text ltx_font_italic">Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning</span>, 2007.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
P. Zhang, Y. Goyal, D. Summers-Stay, D. Batra, and D. Parikh.

</span>
<span class="ltx_bibblock">Yin and yang: Balancing and answering binary visual questions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib98.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</span>, 2016.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
B. Zhou, Y. Tian, S. Sukhbaatar, A. Szlam, and R. Fergus.

</span>
<span class="ltx_bibblock">Simple baseline for visual question answering.

</span>
<span class="ltx_bibblock"><span id="bib.bib99.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1512.02167</span>, 2015.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Y. Zhu, O. Groth, M. Bernstein, and L. Fei-Fei.

</span>
<span class="ltx_bibblock">Visual7W: Grounded Question Answering in Images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib100.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</span>, 2016.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
Y. Zhu, C. Zhang, C. Ré, and L. Fei-Fei.

</span>
<span class="ltx_bibblock">Building a Large-scale Multimodal Knowledge Base System for
Answering Visual Queries.

</span>
<span class="ltx_bibblock"><span id="bib.bib101.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1507.05670</span>, 2015.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
C. L. Zitnick and D. Parikh.

</span>
<span class="ltx_bibblock">Bringing semantics into focus using visual abstraction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib102.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Conf. Comp. Vis. Patt. Recogn.</span>, 2013.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
C. L. Zitnick, D. Parikh, and L. Vanderwende.

</span>
<span class="ltx_bibblock">Learning the visual interpretation of sentences.

</span>
<span class="ltx_bibblock">In <span id="bib.bib103.1.1" class="ltx_text ltx_font_italic">Proc. IEEE Int. Conf. Comp. Vis.</span>, 2013.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
C. L. Zitnick, R. Vedantam, and D. Parikh.

</span>
<span class="ltx_bibblock">Adopting abstract images for semantic scene understanding.

</span>
<span class="ltx_bibblock"><span id="bib.bib104.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Pattern Anal. Mach. Intell.</span>, 38(4):627–638,
2016.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1607.05909" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1607.05910" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1607.05910">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1607.05910" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1607.05915" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar  3 12:06:07 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
