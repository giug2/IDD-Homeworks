<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition</title>
<!--Generated on Tue Sep 17 18:16:37 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.08345v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S1" title="In SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S2" title="In SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S2.SS1" title="In 2 Related Works ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Face Recognition Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S2.SS2" title="In 2 Related Works ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Deep Face Recognition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S2.SS3" title="In 2 Related Works ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Synthetic Image Generation &amp; Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S2.SS4" title="In 2 Related Works ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>ControlNets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S3" title="In SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Synthetic Identity Generation Pipeline (SIG)</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S3.SS1" title="In 3 Synthetic Identity Generation Pipeline (SIG) ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Architecture of the SIG Pipeline</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S3.SS1.SSS1" title="In 3.1 Architecture of the SIG Pipeline ‣ 3 Synthetic Identity Generation Pipeline (SIG) ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Prompt Builder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S3.SS1.SSS2" title="In 3.1 Architecture of the SIG Pipeline ‣ 3 Synthetic Identity Generation Pipeline (SIG) ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Image Generator</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S4" title="In SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>ControlFace10k</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S5" title="In SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S5.SS1" title="In 5 Analysis ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Similarity Score Distributions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S5.SS2" title="In 5 Analysis ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Synthetic Identity Verification</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S6" title="In SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S7" title="In SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#A1" title="In SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix: Supplementary Material</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#A2" title="In SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Average Similarity Scores by Race</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#A3" title="In SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Countries Used to Generate Names for Each Race</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kassi Nzalasse, Rishav Raj, Eli Laird, Corey Clark 
<br class="ltx_break"/>Intelligent Systems and Bias Examination Lab at Southern Methodist University
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1" style="font-size:90%;">ejlaird@smu.edu</span>, <span class="ltx_text ltx_font_typewriter" id="id2.2.id2" style="font-size:90%;">knzalasse@smu.edu</span>, <span class="ltx_text ltx_font_typewriter" id="id3.3.id3" style="font-size:90%;">rishavr@smu.edu</span>, <span class="ltx_text ltx_font_typewriter" id="id4.4.id4" style="font-size:90%;">coreyc@smu.edu</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">As Artificial Intelligence applications expand, the evaluation of models faces heightened scrutiny. Ensuring public readiness requires evaluation datasets, which differ from training data by being disjoint and ethically sourced in compliance with privacy regulations. The performance and fairness of face recognition systems depend significantly on the quality and representativeness of these evaluation datasets. This data is sometimes scraped from the internet without user’s consent, causing ethical concerns that can prohibit its use without proper releases. In rare cases, data is collected in a controlled environment with consent, however, this process is time-consuming, expensive, and logistically difficult to execute. This creates a barrier for those unable to conjure the immense resources required to gather ethically sourced evaluation datasets. To address these challenges, we introduce the Synthetic Identity Generation pipeline, or SIG, that allows for the targeted creation of ethical, balanced datasets for face recognition evaluation. Our proposed and demonstrated pipeline generates high-quality images of synthetic identities with controllable pose, facial features, and demographic attributes, such as race, gender, and age. We also release an open-source evaluation dataset named ControlFace10k, consisting of 10,008 face images of 3,336 unique synthetic identities balanced across race, gender, and age, generated using the proposed SIG pipeline. We analyze ControlFace10k along with a non-synthetic BUPT dataset using state-of-the-art face recognition algorithms to demonstrate its effectiveness as an evaluation tool. This analysis highlights the dataset’s characteristics and its utility in assessing algorithmic bias across different demographic groups.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Face recognition systems are increasingly being deployed worldwide in airports <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib54" title="">54</a>]</cite>, sports stadiums <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib12" title="">12</a>]</cite>, borders <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib23" title="">23</a>]</cite>, and more <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib31" title="">31</a>]</cite>. This growth, fueled by the rapid development of deep learning-based computer vision systems, is largely due to the growing need for fast, reliable, and fair identity verification systems in environments where security is paramount. Critical to the development and deployment of these systems is not only the data used for training, but crucially, the datasets used for evaluating the performance and fairness of face recognition models.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Ideally, evaluation face datasets should include a large number of faces of unique identities with varying features including pose, lighting, facial expressions, skin color, gender, age, and more. These datasets are essential for assessing the accuracy, robustness, and potential biases of face recognition algorithms across diverse demographic groups and conditions. However, gathering such datasets is often time-consuming, expensive, and logistically challenging.
Bias in face recognition systems has been demonstrated in most, if not all, commercial face recognition systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib26" title="">26</a>]</cite>, and is often attributed to imbalanced datasets used for both training and evaluation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib5" title="">5</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib29" title="">29</a>]</cite>. This persistent bias issue underscores the critical need for balanced and comprehensive evaluation datasets.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">The difficulties in collecting suitable evaluation data have led to problematic practices like internet scraping, raising ethical concerns and prompting new privacy legislation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib17" title="">17</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib18" title="">18</a>]</cite>. Synthetic data has emerged as a potential solution for assessing machine learning models, including face recognition systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib30" title="">30</a>]</cite>, but most approaches lack fine-grained control over demographic features like as race, gender, age, and pose without specific training.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we introduce the Synthetic Identity Generation Pipeline (SIG), designed to create balanced, ethical, and controllable evaluation datasets for face recognition systems. SIG generates high-quality images of synthetic identities with precise control over pose, facial features, and demographic attributes using crafted prompt template.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Using our SIG pipeline, we generated ControlFace10k, a new evaluation dataset for face recognition systems comprising 3,336 unique synthetic identities, balanced across race, gender, and age attributes. ControlFace10k is open-source and freely available on <a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/HuMInGameLab/ControlFace10K" title="">Hugging face</a>, for researchers to use.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Our contributions are as follows:</p>
</div>
<div class="ltx_para" id="S1.p7">
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Demonstration of the Synthetic Identity Generation Pipeline and its ability to generate high-quality synthetic identities with controllable pose, race, gender, and age.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Release of the ControlFace10k synthetic face dataset, a demographically balanced evaluation dataset with 3,336 unique synthetic identities with varying pose.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">An analysis of ControlFace10k alongside a non-synthetic face dataset using similarity score distributions from state-of-the-art face recognition systems, demonstrating its effectiveness as an evaluation tool.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Face Recognition Datasets</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.11">The development and evaluation of face recognition systems are significantly dependent on the availability of face image datasets. Several prominent datasets exist, each distinguished by unique characteristics and tailored for specific applications. One of the most commonly used datasets is the Labeled Faces in the Wild (LFW) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib27" title="">27</a>]</cite>, containing over <math alttext="13,000" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.2"><semantics id="S2.SS1.p1.1.m1.2a"><mrow id="S2.SS1.p1.1.m1.2.3.2" xref="S2.SS1.p1.1.m1.2.3.1.cmml"><mn id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">13</mn><mo id="S2.SS1.p1.1.m1.2.3.2.1" xref="S2.SS1.p1.1.m1.2.3.1.cmml">,</mo><mn id="S2.SS1.p1.1.m1.2.2" xref="S2.SS1.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.2b"><list id="S2.SS1.p1.1.m1.2.3.1.cmml" xref="S2.SS1.p1.1.m1.2.3.2"><cn id="S2.SS1.p1.1.m1.1.1.cmml" type="integer" xref="S2.SS1.p1.1.m1.1.1">13</cn><cn id="S2.SS1.p1.1.m1.2.2.cmml" type="integer" xref="S2.SS1.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.2c">13,000</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.2d">13 , 000</annotation></semantics></math> images of public figures collected from the web. The CelebA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib35" title="">35</a>]</cite>, provides more than <math alttext="200,000" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.2"><semantics id="S2.SS1.p1.2.m2.2a"><mrow id="S2.SS1.p1.2.m2.2.3.2" xref="S2.SS1.p1.2.m2.2.3.1.cmml"><mn id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">200</mn><mo id="S2.SS1.p1.2.m2.2.3.2.1" xref="S2.SS1.p1.2.m2.2.3.1.cmml">,</mo><mn id="S2.SS1.p1.2.m2.2.2" xref="S2.SS1.p1.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.2b"><list id="S2.SS1.p1.2.m2.2.3.1.cmml" xref="S2.SS1.p1.2.m2.2.3.2"><cn id="S2.SS1.p1.2.m2.1.1.cmml" type="integer" xref="S2.SS1.p1.2.m2.1.1">200</cn><cn id="S2.SS1.p1.2.m2.2.2.cmml" type="integer" xref="S2.SS1.p1.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.2c">200,000</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.2d">200 , 000</annotation></semantics></math> celebrity images, spanning over <math alttext="10,177" class="ltx_Math" display="inline" id="S2.SS1.p1.3.m3.2"><semantics id="S2.SS1.p1.3.m3.2a"><mrow id="S2.SS1.p1.3.m3.2.3.2" xref="S2.SS1.p1.3.m3.2.3.1.cmml"><mn id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">10</mn><mo id="S2.SS1.p1.3.m3.2.3.2.1" xref="S2.SS1.p1.3.m3.2.3.1.cmml">,</mo><mn id="S2.SS1.p1.3.m3.2.2" xref="S2.SS1.p1.3.m3.2.2.cmml">177</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.2b"><list id="S2.SS1.p1.3.m3.2.3.1.cmml" xref="S2.SS1.p1.3.m3.2.3.2"><cn id="S2.SS1.p1.3.m3.1.1.cmml" type="integer" xref="S2.SS1.p1.3.m3.1.1">10</cn><cn id="S2.SS1.p1.3.m3.2.2.cmml" type="integer" xref="S2.SS1.p1.3.m3.2.2">177</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.2c">10,177</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.3.m3.2d">10 , 177</annotation></semantics></math> identities. There is CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib58" title="">58</a>]</cite> which contains around half a million face images of <math alttext="10,575" class="ltx_Math" display="inline" id="S2.SS1.p1.4.m4.2"><semantics id="S2.SS1.p1.4.m4.2a"><mrow id="S2.SS1.p1.4.m4.2.3.2" xref="S2.SS1.p1.4.m4.2.3.1.cmml"><mn id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">10</mn><mo id="S2.SS1.p1.4.m4.2.3.2.1" xref="S2.SS1.p1.4.m4.2.3.1.cmml">,</mo><mn id="S2.SS1.p1.4.m4.2.2" xref="S2.SS1.p1.4.m4.2.2.cmml">575</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.2b"><list id="S2.SS1.p1.4.m4.2.3.1.cmml" xref="S2.SS1.p1.4.m4.2.3.2"><cn id="S2.SS1.p1.4.m4.1.1.cmml" type="integer" xref="S2.SS1.p1.4.m4.1.1">10</cn><cn id="S2.SS1.p1.4.m4.2.2.cmml" type="integer" xref="S2.SS1.p1.4.m4.2.2">575</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.2c">10,575</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.4.m4.2d">10 , 575</annotation></semantics></math> real identities. VGG Face <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib42" title="">42</a>]</cite> is a dataset of around <math alttext="2.6" class="ltx_Math" display="inline" id="S2.SS1.p1.5.m5.1"><semantics id="S2.SS1.p1.5.m5.1a"><mn id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">2.6</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><cn id="S2.SS1.p1.5.m5.1.1.cmml" type="float" xref="S2.SS1.p1.5.m5.1.1">2.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">2.6</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.5.m5.1d">2.6</annotation></semantics></math> million face images of <math alttext="2622" class="ltx_Math" display="inline" id="S2.SS1.p1.6.m6.1"><semantics id="S2.SS1.p1.6.m6.1a"><mn id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml">2622</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><cn id="S2.SS1.p1.6.m6.1.1.cmml" type="integer" xref="S2.SS1.p1.6.m6.1.1">2622</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">2622</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.6.m6.1d">2622</annotation></semantics></math> people. Additionally, there is MS-Celeb-1M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib22" title="">22</a>]</cite>, which is a large scale face recognition dataset containing over 10 million face images of nearly 100,000 individuals. Notably, MS-Celeb-1M has since been withdrawn due to ethical concerns surrounding nefarious applications of face recognition and the ethical collection of data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib24" title="">24</a>]</cite>. This withdrawal underscores the critical need for carefully curated and ethically sourced evaluation datasets in the field of facial recognition and image processing. Glint360K <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib2" title="">2</a>]</cite> is considered the “largest and cleanest face recognition dataset” containing over <math alttext="17" class="ltx_Math" display="inline" id="S2.SS1.p1.7.m7.1"><semantics id="S2.SS1.p1.7.m7.1a"><mn id="S2.SS1.p1.7.m7.1.1" xref="S2.SS1.p1.7.m7.1.1.cmml">17</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><cn id="S2.SS1.p1.7.m7.1.1.cmml" type="integer" xref="S2.SS1.p1.7.m7.1.1">17</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">17</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.7.m7.1d">17</annotation></semantics></math> million faces of <math alttext="360,232" class="ltx_Math" display="inline" id="S2.SS1.p1.8.m8.2"><semantics id="S2.SS1.p1.8.m8.2a"><mrow id="S2.SS1.p1.8.m8.2.3.2" xref="S2.SS1.p1.8.m8.2.3.1.cmml"><mn id="S2.SS1.p1.8.m8.1.1" xref="S2.SS1.p1.8.m8.1.1.cmml">360</mn><mo id="S2.SS1.p1.8.m8.2.3.2.1" xref="S2.SS1.p1.8.m8.2.3.1.cmml">,</mo><mn id="S2.SS1.p1.8.m8.2.2" xref="S2.SS1.p1.8.m8.2.2.cmml">232</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m8.2b"><list id="S2.SS1.p1.8.m8.2.3.1.cmml" xref="S2.SS1.p1.8.m8.2.3.2"><cn id="S2.SS1.p1.8.m8.1.1.cmml" type="integer" xref="S2.SS1.p1.8.m8.1.1">360</cn><cn id="S2.SS1.p1.8.m8.2.2.cmml" type="integer" xref="S2.SS1.p1.8.m8.2.2">232</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m8.2c">360,232</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.8.m8.2d">360 , 232</annotation></semantics></math> individuals. BUPT-Balancedface dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib56" title="">56</a>]</cite> contains <math alttext="1.3" class="ltx_Math" display="inline" id="S2.SS1.p1.9.m9.1"><semantics id="S2.SS1.p1.9.m9.1a"><mn id="S2.SS1.p1.9.m9.1.1" xref="S2.SS1.p1.9.m9.1.1.cmml">1.3</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.9.m9.1b"><cn id="S2.SS1.p1.9.m9.1.1.cmml" type="float" xref="S2.SS1.p1.9.m9.1.1">1.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.9.m9.1c">1.3</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.9.m9.1d">1.3</annotation></semantics></math> million images of <math alttext="28,000" class="ltx_Math" display="inline" id="S2.SS1.p1.10.m10.2"><semantics id="S2.SS1.p1.10.m10.2a"><mrow id="S2.SS1.p1.10.m10.2.3.2" xref="S2.SS1.p1.10.m10.2.3.1.cmml"><mn id="S2.SS1.p1.10.m10.1.1" xref="S2.SS1.p1.10.m10.1.1.cmml">28</mn><mo id="S2.SS1.p1.10.m10.2.3.2.1" xref="S2.SS1.p1.10.m10.2.3.1.cmml">,</mo><mn id="S2.SS1.p1.10.m10.2.2" xref="S2.SS1.p1.10.m10.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.10.m10.2b"><list id="S2.SS1.p1.10.m10.2.3.1.cmml" xref="S2.SS1.p1.10.m10.2.3.2"><cn id="S2.SS1.p1.10.m10.1.1.cmml" type="integer" xref="S2.SS1.p1.10.m10.1.1">28</cn><cn id="S2.SS1.p1.10.m10.2.2.cmml" type="integer" xref="S2.SS1.p1.10.m10.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.10.m10.2c">28,000</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.10.m10.2d">28 , 000</annotation></semantics></math> individuals divided into four race categories: African, Asian, Caucasian, and Indian. Each race group contains <math alttext="7000" class="ltx_Math" display="inline" id="S2.SS1.p1.11.m11.1"><semantics id="S2.SS1.p1.11.m11.1a"><mn id="S2.SS1.p1.11.m11.1.1" xref="S2.SS1.p1.11.m11.1.1.cmml">7000</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.11.m11.1b"><cn id="S2.SS1.p1.11.m11.1.1.cmml" type="integer" xref="S2.SS1.p1.11.m11.1.1">7000</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.11.m11.1c">7000</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.11.m11.1d">7000</annotation></semantics></math> individuals, making it the only dataset which balances face images based on race. 
<br class="ltx_break"/>The LFW dataset presents many limitations that contribute to overly optimistic performance in models, including a lack of sufficient pose variance and minimal variation in age among the subjects. While attempts have been made to address these issues, they still fall short of providing a comprehensive solution. Cross-Pose LFW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib61" title="">61</a>]</cite>, a renovation of LFW, seeks to address the challenge of comparing faces with different poses to increase intra-class variance. However, this dataset was created through crowd-sourcing efforts to find images in LFW with larger pose differences, rather than generating controlled pose variations. This approach limits the dataset’s ability to systematically evaluate pose-invariant face recognition across a wide range of controlled angles. Similarly, Cross-Age LFW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib62" title="">62</a>]</cite> attempts to address the challenge of age variations, but it too is constrained by the availability of images in the original LFW dataset, lacking fine-grained control over age progression.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">VGGFace2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib7" title="">7</a>]</cite> offers 3.31 million face images of 9,131 individuals with variations in age, pose, illumination, ethnicity, and profession. While more comprehensive, it still suffers from several limitations that highlight the need for a more controlled approach to dataset generation. The dataset’s ethnic diversity is limited by the distribution of celebrities and public figures, potentially underrepresenting certain populations. Its reliance on web-collected images introduces variations in image quality and conditions that could impact recognition performance. Moreover, the dataset exhibits a slight gender imbalance (59.3% male) and varying numbers of images per identity (ranging from 80 to 843), which could potentially affect model training and evaluation. The web-collected nature of the images also means there is limited control over specific image conditions, potentially introducing unintended biases or variations. While efforts were made to clean the dataset, the potential for label noise remains, though it appears to be minimal.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Despite these efforts, it is important to note that there are no public face recognition datasets that comprehensively label and control for race, gender, pose, and age simultaneously. This limitation makes it challenging to evaluate face recognition algorithms across all of these attributes in a controlled and systematic manner. The shortcomings of existing datasets underscore the critical need for a more flexible, controllable, and systematic approach to dataset generation for face recognition evaluation. Such an approach would allow for precise control over demographic attributes, pose variations, and image conditions, enabling more rigorous and unbiased assessment of face recognition algorithms across diverse populations and scenarios.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Deep Face Recognition</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The field of face recognition has seen significant advancements with the development of deep learning-based systems. However, each major contribution has had limitations that underscore the need for more robust and unbiased approaches.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">DeepFace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib50" title="">50</a>]</cite> and FaceNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib49" title="">49</a>]</cite> pioneered the use of deep neural networks for face recognition tasks. While groundbreaking, these early systems were limited by their reliance on large, uncontrolled datasets, potentially introducing biases and reducing generalizability across diverse populations.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">SphereFace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib34" title="">34</a>]</cite> introduced the Angular Softmax loss, allowing CNNs to learn angularly discriminative features. Despite achieving state-of-the-art performance when trained on the CASIA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib58" title="">58</a>]</cite>, SphereFace was constrained by the dataset’s lack of demographic diversity and controlled variations in pose and age.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">ArcFace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib14" title="">14</a>]</cite> improved upon SphereFace with its Additive Margin loss function. However, its reliance on internet-scraped datasets like CASIA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib58" title="">58</a>]</cite>, VGG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib42" title="">42</a>]</cite>, and MS1MV3 (a refined version of Celeb-1M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib22" title="">22</a>]</cite>) introduced potential biases due to the uncontrolled nature of these data sources and their focus on celebrity images.</p>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">MagFace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib37" title="">37</a>]</cite> introduced an adaptive mechanism for better within-class feature distribution. While innovative, its use of the MS1MV2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib14" title="">14</a>]</cite> perpetuated the limitations of internet-scraped data, lacking systematic control over demographic representation and image conditions.</p>
</div>
<div class="ltx_para" id="S2.SS2.p6">
<p class="ltx_p" id="S2.SS2.p6.1">Most recently, GhostFaceNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib1" title="">1</a>]</cite> achieved state-of-the-art performance with a lightweight model. Despite its computational efficiency, GhostFaceNet’s training on MS1MV2 and MS1MV3 datasets inherited the biases and limitations of these celebrity-focused, web-scraped collections.</p>
</div>
<div class="ltx_para" id="S2.SS2.p7">
<p class="ltx_p" id="S2.SS2.p7.1">A critical limitation shared by all these systems is their reliance on datasets that lack sufficient diversity and balance in terms of race, gender, age, and pose. This deficiency stems from the common practice of training on internet-scraped images, which are subsequently cleaned but not systematically controlled. Consequently, these models may exhibit biases in downstream tasks, particularly when applied to diverse, real-world populations.</p>
</div>
<div class="ltx_para" id="S2.SS2.p8">
<p class="ltx_p" id="S2.SS2.p8.1">This persistent issue underscores the critical need for a more controlled and systematic approach to dataset generation for face recognition. Such an approach would enable the development and evaluation of face recognition systems that are more robust, unbiased, and generalizable across diverse demographics and imaging conditions.</p>
</div>
<figure class="ltx_figure" id="S2.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="150" id="S2.F1.sf1.g1" src="extracted/5861389/right-front-left-ref-char-sheet.png" width="449"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S2.F1.sf1.3.2" style="font-size:90%;">Example reference image featuring diverse poses.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="150" id="S2.F1.sf2.g1" src="extracted/5861389/openpose_processed_character_sheet.png" width="449"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S2.F1.sf2.3.2" style="font-size:90%;">Pose and facial landmarks extracted by ‘OpenPose’ controlNet.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">(a) Example reference image featuring diverse poses, (b) Pose information extracted by the ‘OpenPose’ ControlNet</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Synthetic Image Generation &amp; Datasets</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The development of synthetic generation systems has grown drastically since the introduction of Generative Adversarial Networks (GANs) in 2014. Goodfellow <em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.1">et al</em>.<span class="ltx_text" id="S2.SS3.p1.1.2"></span> introduced Generative Adversarial Networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib20" title="">20</a>]</cite>, which train two models simultaneously: a generator and a discriminator. GANs have found widespread application in synthetic image generation across various domains, including time-series medical records <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib3" title="">3</a>]</cite>, image-to-image translation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib28" title="">28</a>]</cite>, and more. Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib46" title="">46</a>]</cite> uses a diffusion process to generate new images from latent space representations. It involves adding noise to the latent space iteratively, and learning to predict as well as remove that noise, which is guided by a text prompt.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">There has been significant work done to build synthetic face image generators. DeepFace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib9" title="">9</a>]</cite> generated face images, by firstly finetuning a pre-trained model i.e VGG-Face net, to classify facial and image characteristics, then, secondly, generating faces given some description using a custom Gaussian Mixture Model. DCGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib44" title="">44</a>]</cite> generates high-quality face images, but fails to preserve the same identity across multiple images. Microsoft’s DiscoFaceGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib16" title="">16</a>]</cite> aims to generate realistic face images of virtual people, with varied pose, expression, and illumination. SynFace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib43" title="">43</a>]</cite> proposes to integrate an identity mixup module to DiscoFaceGAN. It is able to generate high-quality face images but for mostly frontal-view poses. DigiFace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib4" title="">4</a>]</cite> introduces a large-scale synthetic dataset for face recognition training, comprising approximately 100,000 identities generated through a computer graphics pipeline. However, the dataset features synthetic images with unrealistic texture artifacts. DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib30" title="">30</a>]</cite> introduces the Dual Condition Face Generator, a diffusion-based model designed to consistently generate face images of the same identity in various styles with precise control, producing 10,000 identities. However, DCFace requires the training on existing datasets for every attribute of interest, such as style, facial expression, pose, and more. Importantly, all of these methods lack mechanisms to control factors such as race, gender, age, or pose without specifically training on these features. It is crucial to highlight that there currently are no public synthetic datasets that provides comprehensive attribute labels for race, gender, pose, and age, which is one of the core reasons we have developed ControlFace10K to address this significant gap in the field.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>ControlNets</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">ControlNets represent a relatively recent advancement in directing text-to-image diffusion models, particularly with Stable Diffusion, a large pre-trained diffusion model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib47" title="">47</a>]</cite>. ControlNets enhance the control over the image generation process by freezing the original model’s weights and creating a trainable copy, which receives an additional vector as input. This input helps guide the generative process, which traditionally relied solely on text prompts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib59" title="">59</a>]</cite>. A distinguishing innovation of ControlNets is their ability to generate this trainable copy without altering the foundational diffusion model’s learned parameters, thereby facilitating the training and application of various ControlNets using the same unmodified base model.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">The pioneering work with ControlNets has paved the way for a variety of conditional inputs to direct Stable Diffusion. The initial research incorporated a range of conditioning types, from Canny Edge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib6" title="">6</a>]</cite> and Depth Map <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib45" title="">45</a>]</cite> to Normal Map <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib53" title="">53</a>]</cite>, M-LSD straight line detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib21" title="">21</a>]</cite>, HED boundaries <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib57" title="">57</a>]</cite>, OpenPose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib8" title="">8</a>]</cite>, and image segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib63" title="">63</a>]</cite>. This work, in particular, builds directly upon the original ControlNet implementation, utilizing the OpenPose ControlNet along with a variant of the HED boundaries, termed the LineArt ControlNet.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Synthetic Identity Generation Pipeline (SIG)</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this work, we propose the Synthetic Identity Generation (SIG) pipeline, a Stable Diffusion-based image generation pipeline. Stable Diffusion is a generative model capable of producing high-quality synthetic images from textual descriptions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib47" title="">47</a>]</cite>. SIG leverages Absolute Reality, a finetune of Stable Diffusion, optimized for producing hyper-realistic imagery freely available on CivitAI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib11" title="">11</a>]</cite>. The SIG pipeline utilizes this version of Stable Diffusion alongside two ControlNets to produce hyper-realistic synthetic identities with fine-grained control on identity features and pose generation.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">With the implementation of meticulously crafted prompt templates, SIG is capable of generating a diverse array of unique identities, offering rich variations in demographic characteristics such as age, race, and gender.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Furthermore, the SIG pipeline is particularly effective in generating datasets for scenarios where data collection is typically challenging or even impossible. For instance, SIG can produce detailed datasets that capture a variety of human poses, enabling precise studies on posture and movement and their impact on the performance of face recognition systems—scenarios often difficult to consistently replicate in the real world due to natural human variability. Additionally, SIG can simulate facial expressions and head poses under controlled lighting conditions, as well as varying apparent ages, which are challenging to authentically capture due to the subtle nature of these factors. Unlike Generative Adversarial Networks (GANs), which rely on existing datasets to generate new data, our pipeline can synthesize identities de novo. This capability enables SIG to construct comprehensive datasets from text prompts, thereby eliminating the dependence on pre-existing imagery.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Architecture of the SIG Pipeline</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">SIG is made of two main systems: The Prompt Builder and Image Generator systems.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Prompt Builder</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">The Prompt Builder initiates the process of creating synthetic identities - unique generated images representing individuals with consistent facial attributes across poses and lighting conditions. It creates prompts for the Image Generator system, discussed in subsequent sections, guiding the production of multiple face images per identity. These prompts include attributes such as race, background, hairstyle, facial expression, age, and gender.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1">To enhance diversity, the Prompt Builder selects culturally diverse names within targeted racial demographics. We compiled 15,900 names using GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib41" title="">41</a>]</cite>, including 100 names (50 per gender) from 139 countries and 2,000 Indian names, evenly distributed among four racial groups. For details, see Supplementary Material.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.2">We employ ‘keyword blending,’ to create synthetic identities within specific demographic categories, using the syntax: <math alttext="[\text{Name 1}|\text{Name 2}|\text{Name 3}]" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.1.m1.2"><semantics id="S3.SS1.SSS1.p3.1.m1.2a"><mrow id="S3.SS1.SSS1.p3.1.m1.2.2.1" xref="S3.SS1.SSS1.p3.1.m1.2.2.2.cmml"><mo id="S3.SS1.SSS1.p3.1.m1.2.2.1.2" stretchy="false" xref="S3.SS1.SSS1.p3.1.m1.2.2.2.1.cmml">[</mo><mrow id="S3.SS1.SSS1.p3.1.m1.2.2.1.1" xref="S3.SS1.SSS1.p3.1.m1.2.2.1.1.cmml"><mtext id="S3.SS1.SSS1.p3.1.m1.2.2.1.1.2" xref="S3.SS1.SSS1.p3.1.m1.2.2.1.1.2a.cmml">Name 1</mtext><mo id="S3.SS1.SSS1.p3.1.m1.2.2.1.1.1" xref="S3.SS1.SSS1.p3.1.m1.2.2.1.1.1.cmml">⁢</mo><mrow id="S3.SS1.SSS1.p3.1.m1.2.2.1.1.3.2" xref="S3.SS1.SSS1.p3.1.m1.2.2.1.1.3.1.cmml"><mo id="S3.SS1.SSS1.p3.1.m1.2.2.1.1.3.2.1" stretchy="false" xref="S3.SS1.SSS1.p3.1.m1.2.2.1.1.3.1.1.cmml">|</mo><mtext id="S3.SS1.SSS1.p3.1.m1.1.1" xref="S3.SS1.SSS1.p3.1.m1.1.1a.cmml">Name 2</mtext><mo id="S3.SS1.SSS1.p3.1.m1.2.2.1.1.3.2.2" stretchy="false" xref="S3.SS1.SSS1.p3.1.m1.2.2.1.1.3.1.1.cmml">|</mo></mrow><mo id="S3.SS1.SSS1.p3.1.m1.2.2.1.1.1a" xref="S3.SS1.SSS1.p3.1.m1.2.2.1.1.1.cmml">⁢</mo><mtext id="S3.SS1.SSS1.p3.1.m1.2.2.1.1.4" xref="S3.SS1.SSS1.p3.1.m1.2.2.1.1.4a.cmml">Name 3</mtext></mrow><mo id="S3.SS1.SSS1.p3.1.m1.2.2.1.3" stretchy="false" xref="S3.SS1.SSS1.p3.1.m1.2.2.2.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.1.m1.2b"><apply id="S3.SS1.SSS1.p3.1.m1.2.2.2.cmml" xref="S3.SS1.SSS1.p3.1.m1.2.2.1"><csymbol cd="latexml" id="S3.SS1.SSS1.p3.1.m1.2.2.2.1.cmml" xref="S3.SS1.SSS1.p3.1.m1.2.2.1.2">delimited-[]</csymbol><apply id="S3.SS1.SSS1.p3.1.m1.2.2.1.1.cmml" xref="S3.SS1.SSS1.p3.1.m1.2.2.1.1"><times id="S3.SS1.SSS1.p3.1.m1.2.2.1.1.1.cmml" xref="S3.SS1.SSS1.p3.1.m1.2.2.1.1.1"></times><ci id="S3.SS1.SSS1.p3.1.m1.2.2.1.1.2a.cmml" xref="S3.SS1.SSS1.p3.1.m1.2.2.1.1.2"><mtext id="S3.SS1.SSS1.p3.1.m1.2.2.1.1.2.cmml" xref="S3.SS1.SSS1.p3.1.m1.2.2.1.1.2">Name 1</mtext></ci><apply id="S3.SS1.SSS1.p3.1.m1.2.2.1.1.3.1.cmml" xref="S3.SS1.SSS1.p3.1.m1.2.2.1.1.3.2"><abs id="S3.SS1.SSS1.p3.1.m1.2.2.1.1.3.1.1.cmml" xref="S3.SS1.SSS1.p3.1.m1.2.2.1.1.3.2.1"></abs><ci id="S3.SS1.SSS1.p3.1.m1.1.1a.cmml" xref="S3.SS1.SSS1.p3.1.m1.1.1"><mtext id="S3.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p3.1.m1.1.1">Name 2</mtext></ci></apply><ci id="S3.SS1.SSS1.p3.1.m1.2.2.1.1.4a.cmml" xref="S3.SS1.SSS1.p3.1.m1.2.2.1.1.4"><mtext id="S3.SS1.SSS1.p3.1.m1.2.2.1.1.4.cmml" xref="S3.SS1.SSS1.p3.1.m1.2.2.1.1.4">Name 3</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.1.m1.2c">[\text{Name 1}|\text{Name 2}|\text{Name 3}]</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.1.m1.2d">[ Name 1 | Name 2 | Name 3 ]</annotation></semantics></math>. This technique associates each unique name triplet with a ’synthetic identity.’ With 3,975 names per racial group, we can theoretically generate up to 10,460,015,075 unique name triplets per race, represented as <math alttext="\binom{3975}{3}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.2.m2.2"><semantics id="S3.SS1.SSS1.p3.2.m2.2a"><mrow id="S3.SS1.SSS1.p3.2.m2.2.2.4" xref="S3.SS1.SSS1.p3.2.m2.2.2.3.cmml"><mo id="S3.SS1.SSS1.p3.2.m2.2.2.4.1" xref="S3.SS1.SSS1.p3.2.m2.2.2.3.1.cmml">(</mo><mfrac id="S3.SS1.SSS1.p3.2.m2.2.2.2.2" linethickness="0pt" xref="S3.SS1.SSS1.p3.2.m2.2.2.3.cmml"><mn id="S3.SS1.SSS1.p3.2.m2.1.1.1.1.1.1" xref="S3.SS1.SSS1.p3.2.m2.1.1.1.1.1.1.cmml">3975</mn><mn id="S3.SS1.SSS1.p3.2.m2.2.2.2.2.2.1" xref="S3.SS1.SSS1.p3.2.m2.2.2.2.2.2.1.cmml">3</mn></mfrac><mo id="S3.SS1.SSS1.p3.2.m2.2.2.4.2" xref="S3.SS1.SSS1.p3.2.m2.2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.2.m2.2b"><apply id="S3.SS1.SSS1.p3.2.m2.2.2.3.cmml" xref="S3.SS1.SSS1.p3.2.m2.2.2.4"><csymbol cd="latexml" id="S3.SS1.SSS1.p3.2.m2.2.2.3.1.cmml" xref="S3.SS1.SSS1.p3.2.m2.2.2.4.1">binomial</csymbol><cn id="S3.SS1.SSS1.p3.2.m2.1.1.1.1.1.1.cmml" type="integer" xref="S3.SS1.SSS1.p3.2.m2.1.1.1.1.1.1">3975</cn><cn id="S3.SS1.SSS1.p3.2.m2.2.2.2.2.2.1.cmml" type="integer" xref="S3.SS1.SSS1.p3.2.m2.2.2.2.2.2.1">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.2.m2.2c">\binom{3975}{3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.2.m2.2d">( FRACOP start_ARG 3975 end_ARG start_ARG 3 end_ARG )</annotation></semantics></math>, demonstrating the vast potential of the synthetic identity space.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p4">
<p class="ltx_p" id="S3.SS1.SSS1.p4.1">To further enhance diversity, its possible to introduce more synthetic names or blend names across racial groups. For instance, increasing names per race to 5,000 could elevate possibilities to approximately 20.8 billion, showcasing our approach’s scalability.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p5">
<p class="ltx_p" id="S3.SS1.SSS1.p5.1">The ‘keyword blending’ technique not only facilitates vast identity generation but also encourages the model to merge facial features associated with each name, broadening the array of unique faces produced. Our framework, as illustrated in ControlFace10K generation, provides a foundation for future research in synthetic identity generation, emphasizing flexibility and extensibility.</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F2.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="77" id="S3.F2.sf1.g1" src="extracted/5861389/conditoning-face-mask.png" width="77"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F2.sf1.3.2" style="font-size:90%;">Conditioning Mask</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F2.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="77" id="S3.F2.sf2.g1" src="extracted/5861389/generated-image-0.png" width="77"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F2.sf2.3.2" style="font-size:90%;">Generated face using conditioning mask.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">Face generated by Stable Diffusion 2.1 base utilizing the trained “pose” ControlNet, demonstrating the model’s ability to capture one head orientation</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Image Generator</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">The‘Image Generator’ consumes prompts from the ‘Prompt Builder’ to generate the images. The core of this component is the <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS2.p1.1.1">StableDiffusionControlNetPipeline</span>, from Hugging Face’s Diffusers library <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib55" title="">55</a>]</cite> which facilitates the integration of ControlNets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib60" title="">60</a>]</cite> within the Image Generator to guide image synthesis. For instance, the ‘OpenPose’ ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib59" title="">59</a>]</cite> captures the desired poses for synthetic identities. It estimates pose information from a reference image featuring a person in various orientations <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S2.F1.sf1" title="In Figure 1 ‣ 2.2 Deep Face Recognition ‣ 2 Related Works ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1(a)</span></a>. The estimated pose information <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S2.F1.sf2" title="In Figure 1 ‣ 2.2 Deep Face Recognition ‣ 2 Related Works ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1(b)</span></a> is reused across generations to condition the poses featured by synthetic identities.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1">The Image Generator’s design facilitates the integration of ControlNets, enabling face generation with controlled poses. It can operate with models hosted on Hugging Face or locally, offering deployment flexibility. Upon initialization, the Image Generator loads the previously described ControlNet and reference images. For each prompt, it uses the ‘OpenPose’ ControlNet to capture pose information from the character reference image, then proceeds with generating the images using the available Stable Diffusion model.</p>
</div>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g1" src="extracted/5861389/r0_g1_a65_o3_DUGZxN.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g2" src="extracted/5861389/r0_g1_a65_o4_OvtukM.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g3" src="extracted/5861389/r0_g1_a65_o5_rNOVxQ.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g4" src="extracted/5861389/r0_g0_a65_o3_XYNHdp.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g5" src="extracted/5861389/r0_g0_a65_o4_KtVUil.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g6" src="extracted/5861389/r0_g0_a65_o5_BWeBnH.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g7" src="extracted/5861389/r1_g1_a25_o3_GYUvTg.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g8" src="extracted/5861389/r1_g1_a25_o4_IlLFRq.png" width="79"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g9" src="extracted/5861389/r1_g1_a25_o5_tLtict.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g10" src="extracted/5861389/r1_g0_a65_o3_DdijVZ.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g11" src="extracted/5861389/r1_g0_a65_o4_Edffxv.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g12" src="extracted/5861389/r1_g0_a65_o5_mavakj.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g13" src="extracted/5861389/r2_g1_a65_o3_IsXxLL.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g14" src="extracted/5861389/r2_g1_a65_o4_ZvuNfo.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g15" src="extracted/5861389/r2_g1_a65_o5_McNWdH.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g16" src="extracted/5861389/r2_g0_a65_o3_dTESEb.png" width="79"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g17" src="extracted/5861389/r2_g0_a65_o4_QsrRJs.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g18" src="extracted/5861389/r2_g0_a65_o5_XOimdb.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g19" src="extracted/5861389/r3_g1_a50_o3_sjHWla.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g20" src="extracted/5861389/r3_g1_a50_o4_NnXXfE.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g21" src="extracted/5861389/r3_g1_a50_o5_jgUsye.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g22" src="extracted/5861389/r3_g0_a65_o3_dWwlYH.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g23" src="extracted/5861389/r3_g0_a65_o4_EWnYgM.png" width="79"/></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="79" id="S3.F3.g24" src="extracted/5861389/r3_g0_a65_o5_pyERNS.png" width="79"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Synthetic identities generated with SIG for each race in the ControlFace10k dataset. Each row displays images from left to right: male right-facing, male front-facing, male left-facing, female right-facing, female front-facing, female left-facing, for the races African, Asian, Caucasian, and Indian respectively. The generated identities look realistic, with no irregular textures.</span></figcaption>
</figure>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.2.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.1.1">Characteristic</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.2.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.2.1">Category</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.2.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.2.1.1.3.1">Number of Identities</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.2.2.1" rowspan="3"><span class="ltx_text" id="S3.T1.2.2.2.1.1">Age Group</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.2.2.2">25</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.2.2.2.3">1112</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.3.3">
<td class="ltx_td ltx_align_center" id="S3.T1.2.3.3.1">50</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.2.3.3.2">1112</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.4.4">
<td class="ltx_td ltx_align_center" id="S3.T1.2.4.4.1">65</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.2.4.4.2">1112</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.5.5">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.5.5.1" rowspan="2"><span class="ltx_text" id="S3.T1.2.5.5.1.1">Gender</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.5.5.2">Male</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.2.5.5.3">1668</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.6.6">
<td class="ltx_td ltx_align_center" id="S3.T1.2.6.6.1">Female</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.2.6.6.2">1668</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.7.7">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.2.7.7.1" rowspan="4"><span class="ltx_text" id="S3.T1.2.7.7.1.1">Ethnicity</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.7.7.2">Indian</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T1.2.7.7.3">834</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.8.8">
<td class="ltx_td ltx_align_center" id="S3.T1.2.8.8.1">Caucasian</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.2.8.8.2">834</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.9.9">
<td class="ltx_td ltx_align_center" id="S3.T1.2.9.9.1">African</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.2.9.9.2">834</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.10.10">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.10.10.1">Asian</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T1.2.10.10.2">834</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.4.2" style="font-size:90%;">Summary of ControlFace10k Characteristics</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="532" id="S3.F4.sf1.g1" src="extracted/5861389/density-curve-african.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F4.sf1.3.2" style="font-size:90%;">African</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="532" id="S3.F4.sf2.g1" src="extracted/5861389/density-curve-asian.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F4.sf2.3.2" style="font-size:90%;">Asian</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="532" id="S3.F4.sf3.g1" src="extracted/5861389/density-curve-caucasian.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S3.F4.sf3.3.2" style="font-size:90%;">Caucasian</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="532" id="S3.F4.sf4.g1" src="extracted/5861389/density-curve-indian.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.sf4.2.1.1" style="font-size:90%;">(d)</span> </span><span class="ltx_text" id="S3.F4.sf4.3.2" style="font-size:90%;">Indian</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">Score distributions for non-mated pairs across different racial groups using ArcFace and GhostFaceNet models on BUPT and ControlFace10k datasets. Panels (a) African, (b) Asian, (c) Caucasian, (d) Indian illustrate that the ideal similarity score centered around 0.5 signifies orthogonal embeddings between different identities. The distributions typically show scores centering around 0.5, indicating effective model performance. Notably, the overlap between the synthetic data and the BUPT dataset demonstrates that the synthetic data’s scores follow the behavior of non-synthetic data, which is the desired result. However, variances such as the higher scores for African identities by ArcFace compared to GhostFaceNet suggest potential biases in the algorithms. Such observations underscore the utility of synthetic data with race labels in assessing the fairness and accuracy of facial recognition technologies.</span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>ControlFace10k</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.3">Using the SIG pipeline described in <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S3" title="3 Synthetic Identity Generation Pipeline (SIG) ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3</span></a>, we generated ControlFace10k, an evaluation face dataset containing <math alttext="10,008" class="ltx_Math" display="inline" id="S4.p1.1.m1.2"><semantics id="S4.p1.1.m1.2a"><mrow id="S4.p1.1.m1.2.3.2" xref="S4.p1.1.m1.2.3.1.cmml"><mn id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">10</mn><mo id="S4.p1.1.m1.2.3.2.1" xref="S4.p1.1.m1.2.3.1.cmml">,</mo><mn id="S4.p1.1.m1.2.2" xref="S4.p1.1.m1.2.2.cmml">008</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.2b"><list id="S4.p1.1.m1.2.3.1.cmml" xref="S4.p1.1.m1.2.3.2"><cn id="S4.p1.1.m1.1.1.cmml" type="integer" xref="S4.p1.1.m1.1.1">10</cn><cn id="S4.p1.1.m1.2.2.cmml" type="integer" xref="S4.p1.1.m1.2.2">008</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.2c">10,008</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.2d">10 , 008</annotation></semantics></math> images of <math alttext="3,336" class="ltx_Math" display="inline" id="S4.p1.2.m2.2"><semantics id="S4.p1.2.m2.2a"><mrow id="S4.p1.2.m2.2.3.2" xref="S4.p1.2.m2.2.3.1.cmml"><mn id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">3</mn><mo id="S4.p1.2.m2.2.3.2.1" xref="S4.p1.2.m2.2.3.1.cmml">,</mo><mn id="S4.p1.2.m2.2.2" xref="S4.p1.2.m2.2.2.cmml">336</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.2b"><list id="S4.p1.2.m2.2.3.1.cmml" xref="S4.p1.2.m2.2.3.2"><cn id="S4.p1.2.m2.1.1.cmml" type="integer" xref="S4.p1.2.m2.1.1">3</cn><cn id="S4.p1.2.m2.2.2.cmml" type="integer" xref="S4.p1.2.m2.2.2">336</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.2c">3,336</annotation><annotation encoding="application/x-llamapun" id="S4.p1.2.m2.2d">3 , 336</annotation></semantics></math> synthetic identities balanced across race, gender, age and pose, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S3.T1" title="In 3.1.2 Image Generator ‣ 3.1 Architecture of the SIG Pipeline ‣ 3 Synthetic Identity Generation Pipeline (SIG) ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>. This dataset is specifically designed for the evaluation and testing of face recognition systems, not for training purposes. ControlFace10k is stratified into four race groups: African, Asian, Caucasian, and Indian, with <math alttext="834" class="ltx_Math" display="inline" id="S4.p1.3.m3.1"><semantics id="S4.p1.3.m3.1a"><mn id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">834</mn><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><cn id="S4.p1.3.m3.1.1.cmml" type="integer" xref="S4.p1.3.m3.1.1">834</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">834</annotation><annotation encoding="application/x-llamapun" id="S4.p1.3.m3.1d">834</annotation></semantics></math> identities in each group. Each synthetic identity contains images featuring right-facing, front-facing, and left-facing poses. Synthetic identities featured in this evaluation dataset are also equally distributed across three predefined age groups of 25, 50, and 65 years. Samples of male and female synthetic identities for each race group are shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S3.F3" title="In 3.1.2 Image Generator ‣ 3.1 Architecture of the SIG Pipeline ‣ 3 Synthetic Identity Generation Pipeline (SIG) ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">It’s important to note that while these synthetic images are hyper-realistic, featuring ideal lighting, focus, and contrast, our primary goal was to generate unique identities with controllable features such as pose, race, gender, and age for comprehensive evaluation. These demographic attributes are typically more challenging to generate in a controlled manner and are crucial for our evaluation purposes. Moreover, starting with high-quality images provides the flexibility to simulate lower quality scenarios through subsequent image processing, if needed.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">ControlFace10k provides a balanced and controlled environment for assessing face recognition systems across various demographic groups and poses. Despite the ideal conditions of our dataset, our findings in <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S5.SS2" title="5.2 Synthetic Identity Verification ‣ 5 Analysis ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.2</span></a> demonstrate that models may still struggle to correctly match images across varying poses when evaluated using ControlFace10k. These discrepancies highlight the importance of having robust evaluation datasets like ControlFace10k to effectively identify and measure biases, demonstrating false positives and negatives that vary across demographic groups.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">While our current focus is on controlling critical demographic attributes, it’s worth noting that additional image augmentations to simulate various real-world conditions can be easily applied to the dataset. However, the primary challenge and contribution of ControlFace10k lies in its precise control over race, gender, pose, and age attributes, which are fundamental to comprehensive and unbiased evaluation of face recognition systems.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we compare the similarity score distributions between a sample of the BUPT Balancedface dataset, referred to here as BUPT, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib56" title="">56</a>]</cite> and the ControlFace10k dataset. The BUPT sample includes 3,200 identities with an equal distribution across African, Asian, Caucasian, and Indian race groups. Our primary goal is to assess how the similarity scores for synthetic identities, generated using two state-of-the-art face recognition models, ArcFace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib14" title="">14</a>]</cite> and GhostFaceNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib1" title="">1</a>]</cite>, compare to those from real data, accounting for model performance variances. This analysis also explores intra-race comparisons among synthetic identities within the ‘ControlFace10k’ dataset to identify potential biases introduced by the models or synthetic data generation techniques like Stable Diffusion.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We note that there exist potential overlap between BUPT and MS1MV3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib15" title="">15</a>]</cite>, the training dataset for ArcFace and GhostFaceNet, since both are independently gathered sets of 1 million celebrities. Additionally, BUPT is not controlled for pose. These factors, along with the potential familiarity of the models with some identities, may influence the similarity scores. Despite these considerations, we proceed with the analysis as BUPT remains the only freely available dataset with balanced race labels.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Similarity Score Distributions</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">To understand the distribution of non-mated similarity scores in ‘ControlFace10k’, we analyze the density curves for each race group within the dataset and compare them with the corresponding curves from the BUPT sample. This comparison provides a direct visual measure of how synthetic identities generated by each model align with real-world data. To accurately assess the similarity between images without being influenced by the different poses in the dataset, we focus on non-mated comparisons between images that are all facing forward (i.e., frontal pose).</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.3">In <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S3.F4" title="In 3.1.2 Image Generator ‣ 3.1 Architecture of the SIG Pipeline ‣ 3 Synthetic Identity Generation Pipeline (SIG) ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a> we observe the Arcface and GhostFaceNet score distributions for each race group in ControlFace10k compared to BUPT. For all race groups, we notice that the BUPT scores for both models center around <math alttext="0.5" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.1"><semantics id="S5.SS1.p2.1.m1.1a"><mn id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><cn id="S5.SS1.p2.1.m1.1.1.cmml" type="float" xref="S5.SS1.p2.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.1.m1.1d">0.5</annotation></semantics></math>. The distributions for ControlFace10k are shifted slightly to the right of the BUPT scores, but the difference is relatively small. We do note a clear difference in ControlFace10k’s distributions across race groups particularly for the Arcface model. For example, in <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S3.F4.sf1" title="In Figure 4 ‣ 3.1.2 Image Generator ‣ 3.1 Architecture of the SIG Pipeline ‣ 3 Synthetic Identity Generation Pipeline (SIG) ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4(a)</span></a> most of the mass of the Arcface distribution for African identities is above <math alttext="0.6" class="ltx_Math" display="inline" id="S5.SS1.p2.2.m2.1"><semantics id="S5.SS1.p2.2.m2.1a"><mn id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">0.6</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><cn id="S5.SS1.p2.2.m2.1.1.cmml" type="float" xref="S5.SS1.p2.2.m2.1.1">0.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">0.6</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.2.m2.1d">0.6</annotation></semantics></math>, compared to the GhostFaceNet scores for ControlFace10k and both sets for BUPT overlapping in the <math alttext="0.5" class="ltx_Math" display="inline" id="S5.SS1.p2.3.m3.1"><semantics id="S5.SS1.p2.3.m3.1a"><mn id="S5.SS1.p2.3.m3.1.1" xref="S5.SS1.p2.3.m3.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><cn id="S5.SS1.p2.3.m3.1.1.cmml" type="float" xref="S5.SS1.p2.3.m3.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.3.m3.1d">0.5</annotation></semantics></math> range. A similar pattern is seen for Caucasian group though less pronounced, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S3.F4.sf3" title="In Figure 4 ‣ 3.1.2 Image Generator ‣ 3.1 Architecture of the SIG Pipeline ‣ 3 Synthetic Identity Generation Pipeline (SIG) ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4(c)</span></a>. This shift in distributions suggests that Arcface finds these identities slightly more similar that those in the BUPT dataset. The largest shift is seen in the Arcface scores for the Indian group <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S3.F4.sf4" title="In Figure 4 ‣ 3.1.2 Image Generator ‣ 3.1 Architecture of the SIG Pipeline ‣ 3 Synthetic Identity Generation Pipeline (SIG) ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4(d)</span></a>, where a larger tail in the distribution suggests higher similarity for Indian identities.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">The similarity distributions computed using GhostFaceNet overlap more with the scores for BUPT than with Arcface, uncovering a ‘model-effect’ in the distributions. This model-effect is potentially explained by GhostFaceNet’s improved ability to generate distinct embeddings for synthetic identities compared to Arcface.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">Notably, both models show significant overlap with the BUPT distributions for the Asian group, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S3.F4.sf2" title="In Figure 4 ‣ 3.1.2 Image Generator ‣ 3.1 Architecture of the SIG Pipeline ‣ 3 Synthetic Identity Generation Pipeline (SIG) ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4(b)</span></a>. This level of overlap for the Asian group could have two potential explanations: the Arcface and GhostFaceNet models are better at distinguishing between Asian identities, and
<br class="ltx_break"/>or the SIG pipeline is better at generating more unique identities of Asians. Both of these explanations suggest some bias in the training data of the face recognition models and/or the Stable Diffusion model.</p>
</div>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1">Overall, we see that the similarity scores produced for ControlFace10k behave similarly to the real world BUPT dataset, with a slight tail of similar identities for each race group. We also note a dependence on the face recognition model, where Arcface embeddings exhibited a slight increase in scores compared to those computed using GhostFaceNet. We also find that scores for Asian identities showed more overlap with the BUPT scores. We hypothesize that these differences in score distributions can be explained by bias in both the face recognition model and the image generation model.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Synthetic Identity Verification</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">To demonstrate the consistency of images within synthetic identities generated by SIG, we generated an additional sample of 3360 synthetic identities, all featuring a frontal pose (SIG Frontal sample). Utilizing embedding vectors from state-of-the-art face recognition models, ArcFace and GhostFaceNet, we calculated similarity scores between <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.1">distinct</span> images of each synthetic identity. Our goal was twofold: firstly, to validate that images belonging to the same synthetic identity maintain consistent facial attributes across different poses, as per our definition of a ‘synthetic identity’ in <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S3.SS1.SSS1" title="3.1.1 Prompt Builder ‣ 3.1 Architecture of the SIG Pipeline ‣ 3 Synthetic Identity Generation Pipeline (SIG) ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1.1</span></a>; and secondly, to provide an insight into the performance of current face recognition systems. We achieved this by constructing a density curve illustrating the distribution of similarity scores for synthetic identity within ControlFace10k. The resulting distributions for both models are shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#S5.F5" title="In 5.2 Synthetic Identity Verification ‣ 5 Analysis ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a>. The comparison between the distributions of similarity scores for ControlFace10k, with varying pose, and images from the same synthetic identity, with frontal pose, indicates that while current face recognition systems perform well on frontal poses, there is room for improvement in recognizing faces across different poses. These differences can be explained by the poorer performance for both ArcFace and GhostFaceNet on non-frontal face images, as observed in their original papers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#bib.bib1" title="">1</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="266" id="S5.F5.g1" src="extracted/5861389/same-identity-comparison.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S5.F5.3.2" style="font-size:90%;">Facial similarity score distributions for synthetic identities using (A) ArcFace and (B) GhostFaceNet models. Each density curve represents similarity scores among three images of the same synthetic identity. Higher scores indicate greater similarity. The SIG Frontal Sample’s sharp peak demonstrates high similarity when generating three frontal images per identity. In contrast, the broader distributions for BUPT (three different poses) and ControlFace10k (one frontal, one left, one right pose) datasets indicate lower similarity scores despite representing the same identity. This suggests face recognition models may be less consistent when comparing varied pose angles of the same individual.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Synthetic data facilitates the study of specific scenarios that are difficult or expensive to capture in real-world data collection. For instance, it can enable the creation of multiple doppelganger identities across different age groups, which can be useful for examining the age-invariance of face recognition algorithms. By controlling features such as age, pose, and lighting, researchers can generate data sets that specifically target edge cases where existing datasets are sparse. This approach addresses specific challenges that might otherwise be overlooked due to the difficulties associated with collecting data in controlled environments. Synthetic data can also be used as a supplement to existing face datasets by demographically balancing them in order to reduce bias in the datasets and the training of the models. Another advantage of synthetic data lies in its capacity to uphold privacy and security standards. Synthetic data offers a solution that mimics real-world data distributions while ensuring that individual privacy is maintained. This is particularly relevant under new regulations, where the use of real data can be fraught with legal and ethical challenges.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this work, we propose and demonstrate the Synthetic Identity Generation (SIG) pipeline, which generates high-quality images of synthetic identities with controllable pose, facial features, and demographic attributes,
such as race, gender, and age. Using SIG, we generated a demographically and pose balanced dataset, ControlFace10k, for use in evaluating face recognition systems.
Through the analysis of similarity scores for the ControlFace10k dataset, we demonstrate SIG’s ability to generate unique synthetic identities and discuss in detail the architecture behind our approach. To facilitate further research, we have made ControlFace10k open-source and publicly available on <a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/HuMInGameLab/ControlFace10K" title="">Hugging Face</a>. We encourage researchers and practitioners to utilize this resource for evaluating and improving face recognition systems.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">Going further, we plan to scale SIG to generate much larger datasets, consisting of more unique identities and increase the controllable features in the data. We also plan to use SIG to mitigate biases in face recognition systems through targeted training of underrepresented attributes. We hope that our contributions provide new opportunities for the evaluation and development of more fair and performant face recognition systems.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
M. Alansari, O. A. Hay, S. Javed, A. Shoufan, Y. Zweiri, and N. Werghi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.2.1" style="font-size:90%;">Ghostfacenets: Lightweight face recognition model from cheap
operations.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.3.1" style="font-size:90%;">IEEE Access</span><span class="ltx_text" id="bib.bib1.4.2" style="font-size:90%;">, 11:35429–35446, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
X. An, X. Zhu, Y. Xiao, L. Wu, M. Zhang, Y. Gao, B. Qin, D. Zhang, and Y. Fu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">Partial fc: Training 10 million identities on a single machine.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.3.1" style="font-size:90%;">2021 IEEE/CVF International Conference on Computer Vision
Workshops (ICCVW)</span><span class="ltx_text" id="bib.bib2.4.2" style="font-size:90%;">, pages 1445–1449, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
N. Ashrafi, V. Schmitt, R. P. Spang, S. Möller, and J.-N. Voigt-Antons.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.2.1" style="font-size:90%;">Protect and extend – using gans for synthetic data generation of
time-series medical records, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
G. Bae, M. de La Gorce, T. Baltrusaitis, C. Hewitt, D. Chen, J. Valentin,
R. Cipolla, and J. Shen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">Digiface-1m: 1 million digital face images for face recognition,
2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
J. Buolamwini and T. Gebru.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">Gender shades: Intersectional accuracy disparities in commercial
gender classification.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib5.4.2" style="font-size:90%;">FAT</span><span class="ltx_text" id="bib.bib5.5.3" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
J. Canny.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">A computational approach to edge detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.3.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="ltx_text" id="bib.bib6.4.2" style="font-size:90%;">,
PAMI-8(6):679–698, 1986.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">Vggface2: A dataset for recognising faces across pose and age.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.3.1" style="font-size:90%;">2018 13th IEEE International Conference on Automatic Face &amp;
Gesture Recognition (FG 2018)</span><span class="ltx_text" id="bib.bib7.4.2" style="font-size:90%;">, pages 67–74, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">Realtime multi-person 2d pose estimation using part affinity fields,
2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
H. Cate, F. Dalvi, and Z. Hussain.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">Deepface: Face generation using deep learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.3.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib9.4.2" style="font-size:90%;">, abs/1701.01876, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
C. Chung.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">Facial Recognition: Coming Soon to an Airport Near You.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.3.1" style="font-size:90%;">The New York Times</span><span class="ltx_text" id="bib.bib10.4.2" style="font-size:90%;">, Feb. 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
Civitai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.2.1" style="font-size:90%;">Absolutereality - stable diffusion checkpoint.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://civitai.com/models/81458/absolutereality" style="font-size:90%;" title="">https://civitai.com/models/81458/absolutereality</a><span class="ltx_text" id="bib.bib11.3.1" style="font-size:90%;">, 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.4.1" style="font-size:90%;">Accessed: 2024-03-20.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
A. Cohen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">Facial recognition in sports: The biometrics technology shaping
ticketing, payments, security, more, Feb. 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
S. Cosentino and A. Baden.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">A New Type of Face Painting - The Use of Facial
Recognition Technology in Sports Venues: Stinson LLP Law
Firm, Oct. 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
J. Deng, J. Guo, J. Yang, N. Xue, I. Kotsia, and S. Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">Arcface: Additive angular margin loss for deep face recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.3.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="ltx_text" id="bib.bib14.4.2" style="font-size:90%;">,
44:5962–5979, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
J. Deng, J. Guo, D. Zhang, Y. Deng, X. Lu, and S. Shi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">Lightweight face recognition challenge.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib15.4.2" style="font-size:90%;">2019 IEEE/CVF International Conference on Computer Vision
Workshop (ICCVW)</span><span class="ltx_text" id="bib.bib15.5.3" style="font-size:90%;">, pages 2638–2646, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
Y. Deng, J. Yang, D. Chen, F. Wen, and X. Tong.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">Disentangled and controllable face image generation via 3d
imitative-contrastive learning, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
G. Drenik.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">Data Privacy And Ownership To Remain Key Concerns In
Web Scraping Industry Next Year, Dec. 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
European Parliament.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">Artificial intelligence act.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.3.1" style="font-size:90%;">Legislative Resolution, 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.4.1" style="font-size:90%;">Adopted April 2024, Accessed: April 26, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
G. Gee.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">Here Are the Stadiums That Are Keeping Track of Your
Face.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.3.1" style="font-size:90%;">Slate</span><span class="ltx_text" id="bib.bib19.4.2" style="font-size:90%;">, Mar. 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, and Y. Bengio.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.2.1" style="font-size:90%;">Generative adversarial networks, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
G. Gu, B. Ko, S. Go, S.-H. Lee, J. Lee, and M. Shin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.2.1" style="font-size:90%;">Towards light-weight and real-time line segment detection, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.2.1" style="font-size:90%;">Ms-celeb-1m: A dataset and benchmark for large-scale face
recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.3.1" style="font-size:90%;">ArXiv</span><span class="ltx_text" id="bib.bib22.4.2" style="font-size:90%;">, abs/1607.08221, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
R. I. Gutman-Argemí, Clara.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.2.1" style="font-size:90%;">How Technology Is Changing Immigration Lines, May 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
J. Harvey, Adam. LaPlace.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.2.1" style="font-size:90%;">Exposing.ai, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
J. J. Howard, Y. B. Sirotin, J. L. Tipton, and A. R. Vemury.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.2.1" style="font-size:90%;">Quantifying the extent to which race and gender features determine
identity in commercial face recognition algorithms.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.3.1" style="font-size:90%;">ArXiv</span><span class="ltx_text" id="bib.bib25.4.2" style="font-size:90%;">, abs/2010.07979, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
J. J. Howard, Y. B. Sirotin, and A. R. Vemury.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.2.1" style="font-size:90%;">The effect of broad and specific demographic homogeneity on the
imposter distributions and false match rates in face recognition algorithm
performance.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.3.1" style="font-size:90%;">2019 IEEE 10th International Conference on Biometrics Theory,
Applications and Systems (BTAS)</span><span class="ltx_text" id="bib.bib26.4.2" style="font-size:90%;">, pages 1–8, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
G. B. Huang, M. A. Mattar, T. L. Berg, and E. Learned-Miller.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.2.1" style="font-size:90%;">Labeled faces in the wild: A database forstudying face recognition in
unconstrained environments.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.3.1" style="font-size:90%;">International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib27.4.2" style="font-size:90%;">, 2008.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="font-size:90%;">
P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.2.1" style="font-size:90%;">Image-to-image translation with conditional adversarial networks,
2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="font-size:90%;">
T. L. J. Johnson, Natasha N.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.2.1" style="font-size:90%;">Police Facial Recognition Technology Can’t Tell Black
People Apart.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="font-size:90%;">
M. Kim, F. Liu, A. Jain, and X. Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.2.1" style="font-size:90%;">Dcface: Synthetic face generation with dual condition diffusion
model, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="font-size:90%;">
T. Klosowski.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.2.1" style="font-size:90%;">Facial Recognition Is Everywhere. Here’s What We Can
Do About It., July 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.1.1" style="font-size:90%;">
C. S. Legislature.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.2.1" style="font-size:90%;">California consumer privacy act.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.3.1" style="font-size:90%;">California Civil Code, 2018.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.4.1" style="font-size:90%;">Division 3, Part 4, Title 1.81.5, Accessed: April 26, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.1.1" style="font-size:90%;">
I. S. Legislature.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.2.1" style="font-size:90%;">Biometric information privacy act.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.3.1" style="font-size:90%;">Illinois Compiled Statutes, 2008.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.4.1" style="font-size:90%;">740 ILCS 14/, Accessed: April 26, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.1.1" style="font-size:90%;">
W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.2.1" style="font-size:90%;">Sphereface: Deep hypersphere embedding for face recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.3.1" style="font-size:90%;">2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span class="ltx_text" id="bib.bib34.4.2" style="font-size:90%;">, pages 6738–6746, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.1.1" style="font-size:90%;">
Z. Liu, P. Luo, X. Wang, and X. Tang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.2.1" style="font-size:90%;">Deep learning face attributes in the wild.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib35.4.2" style="font-size:90%;">Proceedings of International Conference on Computer Vision
(ICCV)</span><span class="ltx_text" id="bib.bib35.5.3" style="font-size:90%;">, December 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.1.1" style="font-size:90%;">
L. Marshall.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.2.1" style="font-size:90%;">Why new facial-recognition airport screenings are raising concerns,
July 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.1.1" style="font-size:90%;">
Q. Meng, S. Zhao, Z. Huang, and F. Zhou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.2.1" style="font-size:90%;">Magface: A universal representation for face recognition and quality
assessment.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.3.1" style="font-size:90%;">2021 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span class="ltx_text" id="bib.bib37.4.2" style="font-size:90%;">, pages 14220–14229, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.1.1" style="font-size:90%;">
U. D. of Homeland Security.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.2.1" style="font-size:90%;">Airports | CBP Biometrics | U.S. Customs
and Border Protection.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.1.1" style="font-size:90%;">
U. D. of Homeland Security.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.2.1" style="font-size:90%;">Biometrics | U.S. Customs and Border Protection.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.1.1" style="font-size:90%;">
U. S. G. A. Office.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.2.1" style="font-size:90%;">Facial Recognition Technology: CBP Traveler Identity
Verification and Efforts to Address Privacy Issues |
U.S. GAO.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.1.1" style="font-size:90%;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.2.1" style="font-size:90%;">ChatGPT.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chat.openai.com/chat" style="font-size:90%;" title="">https://chat.openai.com/chat</a><span class="ltx_text" id="bib.bib41.3.1" style="font-size:90%;">, May 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.4.1" style="font-size:90%;">Version 30.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.1.1" style="font-size:90%;">
O. M. Parkhi, A. Vedaldi, and A. Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.2.1" style="font-size:90%;">Deep face recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib42.4.2" style="font-size:90%;">British Machine Vision Conference</span><span class="ltx_text" id="bib.bib42.5.3" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.1.1" style="font-size:90%;">
H. Qiu, B. Yu, D. Gong, Z. Li, W. Liu, and D. Tao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.2.1" style="font-size:90%;">Synface: Face recognition with synthetic data, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.1.1" style="font-size:90%;">
A. Radford, L. Metz, and S. Chintala.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.2.1" style="font-size:90%;">Unsupervised representation learning with deep convolutional
generative adversarial networks, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.1.1" style="font-size:90%;">
R. Ranftl, K. Lasinger, D. Hafner, K. Schindler, and V. Koltun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.2.1" style="font-size:90%;">Towards robust monocular depth estimation: Mixing datasets for
zero-shot cross-dataset transfer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.3.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="ltx_text" id="bib.bib45.4.2" style="font-size:90%;">,
44(03):1623–1637, mar 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.1.1" style="font-size:90%;">
R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.2.1" style="font-size:90%;">High-resolution image synthesis with latent diffusion models, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.1.1" style="font-size:90%;">
R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.2.1" style="font-size:90%;">High-resolution image synthesis with latent diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib47.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span class="ltx_text" id="bib.bib47.5.3" style="font-size:90%;">, pages 10684–10695, June 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.1.1" style="font-size:90%;">
R. Saravanan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.2.1" style="font-size:90%;">Facial Recognition on Campus: Benefits &amp; Security Risks
| Ellucian.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.1.1" style="font-size:90%;">
F. Schroff, D. Kalenichenko, and J. Philbin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.2.1" style="font-size:90%;">Facenet: A unified embedding for face recognition and clustering.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib49.3.1" style="font-size:90%;">2015 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span class="ltx_text" id="bib.bib49.4.2" style="font-size:90%;">, pages 815–823, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.1.1" style="font-size:90%;">
Y. Taigman, M. Yang, M. Ranzato, and L. Wolf.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.2.1" style="font-size:90%;">Deepface: Closing the gap to human-level performance in face
verification.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib50.4.2" style="font-size:90%;">2014 IEEE Conference on Computer Vision and Pattern
Recognition</span><span class="ltx_text" id="bib.bib50.5.3" style="font-size:90%;">, pages 1701–1708, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.1.1" style="font-size:90%;">
Thales.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.2.1" style="font-size:90%;">11 use cases for facial recognition, June 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.1.1" style="font-size:90%;">
U.S. Department of Homeland Security.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.2.1" style="font-size:90%;">Facial recognition technology — transportation security
administration.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.1.1" style="font-size:90%;">
I. Vasiljevic, N. Kolkin, S. Zhang, R. Luo, H. Wang, F. Z. Dai, A. F. Daniele,
M. Mostajabi, S. Basart, M. R. Walter, and G. Shakhnarovich.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.2.1" style="font-size:90%;">Diode: A dense indoor and outdoor depth dataset, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.1.1" style="font-size:90%;">
J. Vermes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.2.1" style="font-size:90%;">Airports want to scan your face to make travelling easier. Privacy
experts caution it’s not ready for takeoff.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib54.3.1" style="font-size:90%;">CBC Radio</span><span class="ltx_text" id="bib.bib54.4.2" style="font-size:90%;">, Mar. 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.1.1" style="font-size:90%;">
P. von Platen, S. Patil, A. Lozhkov, P. Cuenca, N. Lambert, K. Rasul,
M. Davaadorj, D. Nair, S. Paul, W. Berman, Y. Xu, S. Liu, and T. Wolf.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.2.1" style="font-size:90%;">Diffusers: State-of-the-art diffusion models.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/diffusers" style="font-size:90%;" title="">https://github.com/huggingface/diffusers</a><span class="ltx_text" id="bib.bib55.3.1" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.1.1" style="font-size:90%;">
M. Wang, Y. Zhang, and W. Deng.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.2.1" style="font-size:90%;">Meta balanced network for fair face recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib56.3.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="ltx_text" id="bib.bib56.4.2" style="font-size:90%;">,
44:8433–8448, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.1.1" style="font-size:90%;">
S. Xie and Z. Tu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.2.1" style="font-size:90%;">Holistically-nested edge detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib57.4.2" style="font-size:90%;">2015 IEEE International Conference on Computer Vision
(ICCV)</span><span class="ltx_text" id="bib.bib57.5.3" style="font-size:90%;">, pages 1395–1403, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.1.1" style="font-size:90%;">
D. Yi, Z. Lei, S. Liao, and S. Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.2.1" style="font-size:90%;">Learning face representation from scratch.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib58.3.1" style="font-size:90%;">ArXiv</span><span class="ltx_text" id="bib.bib58.4.2" style="font-size:90%;">, abs/1411.7923, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.1.1" style="font-size:90%;">
L. Zhang, A. Rao, and M. Agrawala.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.2.1" style="font-size:90%;">Adding conditional control to text-to-image diffusion models, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.1.1" style="font-size:90%;">
L. Zhang, A. Rao, and M. Agrawala.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.2.1" style="font-size:90%;">Adding conditional control to text-to-image diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib60.3.1" style="font-size:90%;">2023 IEEE/CVF International Conference on Computer Vision
(ICCV)</span><span class="ltx_text" id="bib.bib60.4.2" style="font-size:90%;">, pages 3813–3824, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.1.1" style="font-size:90%;">
T. Zheng and W. Deng.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.2.1" style="font-size:90%;">Cross-pose lfw: A database for studying cross-pose face recognition
in unconstrained environments.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.3.1" style="font-size:90%;">Technical Report 18-01, Beijing University of Posts and
Telecommunications, February 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.1.1" style="font-size:90%;">
T. Zheng, W. Deng, and J. Hu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.2.1" style="font-size:90%;">Cross-age lfw: A database for studying cross-age face recognition in
unconstrained environments.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib62.3.1" style="font-size:90%;">ArXiv</span><span class="ltx_text" id="bib.bib62.4.2" style="font-size:90%;">, abs/1708.08197, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.1.1" style="font-size:90%;">
B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.2.1" style="font-size:90%;">Scene parsing through ade20k dataset.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib63.3.1" style="font-size:90%;">2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span class="ltx_text" id="bib.bib63.4.2" style="font-size:90%;">, pages 5122–5130, 2017.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix: Supplementary Material</h2>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Average Similarity Scores by Race</h2>
<figure class="ltx_figure" id="A2.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="149" id="A2.F6.g1" src="extracted/5861389/controlface10k-avg-sim-scores-per-race.png" width="329"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="A2.F6.3.2" style="font-size:90%;">Heatmap comparison illustrating average similarity scores by race group within the ‘ControlFace10k’ dataset, contrasted across different facial recognition embeddings: (A) Utilizing ArcFace embeddings; (B) Utilizing GhostFaceNet embeddings</span></figcaption>
</figure>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">We present the average similarity scores for synthetic identities across diverse racial groups within the ControlFace10k dataset. These scores, derived from the Arcface and GhostFaceNet models. In the provided heatmaps depicting average similarity scores from the ControlFace10k dataset, there is a noticeable contrast when comparing the outcomes of ArcFace and GhostFaceNet embeddings. It can be observed that the ArcFace-derived scores are consistently higher across all racial groups than those from GhostFaceNet, hinting at a model-intrinsic leniency in scoring identities. ArcFace scores indicate a closer resemblance within synthetic identities of the Indian group, while GhostFaceNet scores imply a similar closeness among African identities. These patterns may stem from the models’ differential sensitivity to skin tone variations, which range from very dark to brown in African identities and from fair to dark brown in Indian identities.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Countries Used to Generate Names for Each Race</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">SIG’s Prompt Builder module generates a prompt for each desired synthetic identity. To foster the uniqueness of these prompts and enhance the diversity of the resulting portraits, the builder selects names that reflect diverse cultural or linguistic backgrounds within the targeted racial demographic. These names, synthetically generated, represent common names from specific racial groups. To compile these names, we initially identify countries where the racial group is predominantly found.</p>
</div>
<div class="ltx_para" id="A3.p2">
<p class="ltx_p" id="A3.p2.1">The GPT-4 large language model is then utilized to produce names that are found in the identified countries, see <a class="ltx_ref" href="https://arxiv.org/html/2409.08345v2#A3.T2" title="In Appendix C Countries Used to Generate Names for Each Race ‣ SIG: A Synthetic Identity Generation Pipeline for Generating Evaluation Datasets for Face Recognition"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a> below, with a target of 50 names per gender per country. From this effort, we gathered 100 names (50 per gender) from 139 countries, plus an additional 2,000 names specifically for Indian demographics, resulting in a total of 15,900 names. These are evenly distributed among four racial groups, providing approximately 3,975 names per group.</p>
</div>
<figure class="ltx_table" id="A3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A3.T2.2.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="A3.T2.3.2" style="font-size:90%;">Countries used to generate names for each race.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.T2.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T2.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A3.T2.4.1.1.1">African</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A3.T2.4.1.1.2">Asian</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A3.T2.4.1.1.3">Caucasian</th>
<th class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A3.T2.4.1.1.4">Indian</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T2.4.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T2.4.2.1.1">Algeria</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T2.4.2.1.2">Afghanistan</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T2.4.2.1.3">United States</td>
<td class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t" id="A3.T2.4.2.1.4">India</td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.3.2">
<td class="ltx_td ltx_align_left" id="A3.T2.4.3.2.1">Angola</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.3.2.2">Armenia</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.3.2.3">Canada</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.3.2.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.4.3">
<td class="ltx_td ltx_align_left" id="A3.T2.4.4.3.1">Benin</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.4.3.2">Azerbaijan</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.4.3.3">United Kingdom</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.4.3.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.5.4">
<td class="ltx_td ltx_align_left" id="A3.T2.4.5.4.1">Botswana</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.5.4.2">Bahrain</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.5.4.3">France</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.5.4.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.6.5">
<td class="ltx_td ltx_align_left" id="A3.T2.4.6.5.1">Burkina Faso</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.6.5.2">Bangladesh</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.6.5.3">Germany</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.6.5.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.7.6">
<td class="ltx_td ltx_align_left" id="A3.T2.4.7.6.1">Burundi</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.7.6.2">Bhutan</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.7.6.3">Italy</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.7.6.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.8.7">
<td class="ltx_td ltx_align_left" id="A3.T2.4.8.7.1">Cabo Verde</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.8.7.2">Brunei</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.8.7.3">Spain</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.8.7.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.9.8">
<td class="ltx_td ltx_align_left" id="A3.T2.4.9.8.1">Cameroon</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.9.8.2">Cambodia</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.9.8.3">Russia</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.9.8.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.10.9">
<td class="ltx_td ltx_align_left" id="A3.T2.4.10.9.1">Central African Republic</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.10.9.2">China</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.10.9.3">Australia</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.10.9.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.11.10">
<td class="ltx_td ltx_align_left" id="A3.T2.4.11.10.1">Chad</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.11.10.2">Cyprus</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.11.10.3">New Zealand</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.11.10.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.12.11">
<td class="ltx_td ltx_align_left" id="A3.T2.4.12.11.1">Comoros</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.12.11.2">Georgia</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.12.11.3">Sweden</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.12.11.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.13.12">
<td class="ltx_td ltx_align_left" id="A3.T2.4.13.12.1">Democratic Republic of the Congo</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.13.12.2">India</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.13.12.3">Norway</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.13.12.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.14.13">
<td class="ltx_td ltx_align_left" id="A3.T2.4.14.13.1">Republic of the Congo</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.14.13.2">Indonesia</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.14.13.3">Denmark</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.14.13.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.15.14">
<td class="ltx_td ltx_align_left" id="A3.T2.4.15.14.1">Djibouti</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.15.14.2">Iran</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.15.14.3">Finland</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.15.14.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.16.15">
<td class="ltx_td ltx_align_left" id="A3.T2.4.16.15.1">Egypt</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.16.15.2">Iraq</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.16.15.3">Netherlands</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.16.15.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.17.16">
<td class="ltx_td ltx_align_left" id="A3.T2.4.17.16.1">Equatorial Guinea</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.17.16.2">Israel</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.17.16.3">Belgium</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.17.16.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.18.17">
<td class="ltx_td ltx_align_left" id="A3.T2.4.18.17.1">Eritrea</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.18.17.2">Japan</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.18.17.3">Austria</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.18.17.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.19.18">
<td class="ltx_td ltx_align_left" id="A3.T2.4.19.18.1">Eswatini</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.19.18.2">Jordan</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.19.18.3">Switzerland</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.19.18.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.20.19">
<td class="ltx_td ltx_align_left" id="A3.T2.4.20.19.1">Ethiopia</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.20.19.2">Kazakhstan</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.20.19.3">Ireland</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.20.19.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.21.20">
<td class="ltx_td ltx_align_left" id="A3.T2.4.21.20.1">Gabon</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.21.20.2">Kuwait</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.21.20.3">Portugal</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.21.20.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.22.21">
<td class="ltx_td ltx_align_left" id="A3.T2.4.22.21.1">Gambia</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.22.21.2">Kyrgyzstan</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.22.21.3">Iceland</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.22.21.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.23.22">
<td class="ltx_td ltx_align_left" id="A3.T2.4.23.22.1">Ghana</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.23.22.2">Laos</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.23.22.3">Greece</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.23.22.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.24.23">
<td class="ltx_td ltx_align_left" id="A3.T2.4.24.23.1">Guinea</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.24.23.2">Lebanon</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.24.23.3">Poland</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.24.23.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.25.24">
<td class="ltx_td ltx_align_left" id="A3.T2.4.25.24.1">Guinea-Bissau</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.25.24.2">Malaysia</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.25.24.3">Czech Republic</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.25.24.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.26.25">
<td class="ltx_td ltx_align_left" id="A3.T2.4.26.25.1">Ivory Coast</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.26.25.2">Maldives</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.26.25.3">Hungary</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.26.25.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.27.26">
<td class="ltx_td ltx_align_left" id="A3.T2.4.27.26.1">Kenya</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.27.26.2">Mongolia</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.27.26.3">Slovakia</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.27.26.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.28.27">
<td class="ltx_td ltx_align_left" id="A3.T2.4.28.27.1">Lesotho</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.28.27.2">Myanmar</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.28.27.3">Croatia</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.28.27.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.29.28">
<td class="ltx_td ltx_align_left" id="A3.T2.4.29.28.1">Liberia</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.29.28.2">Nepal</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.29.28.3">Slovenia</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.29.28.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.30.29">
<td class="ltx_td ltx_align_left" id="A3.T2.4.30.29.1">Libya</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.30.29.2">North Korea</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.30.29.3">Bulgaria</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.30.29.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.31.30">
<td class="ltx_td ltx_align_left" id="A3.T2.4.31.30.1">Madagascar</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.31.30.2">Oman</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.31.30.3">Romania</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.31.30.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.32.31">
<td class="ltx_td ltx_align_left" id="A3.T2.4.32.31.1">Malawi</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.32.31.2">Pakistan</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.32.31.3">Estonia</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.32.31.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.33.32">
<td class="ltx_td ltx_align_left" id="A3.T2.4.33.32.1">Mali</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.33.32.2">Palestine</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.33.32.3">Latvia</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.33.32.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.34.33">
<td class="ltx_td ltx_align_left" id="A3.T2.4.34.33.1">Mauritania</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.34.33.2">Philippines</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.34.33.3">Lithuania</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.34.33.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.35.34">
<td class="ltx_td ltx_align_left" id="A3.T2.4.35.34.1">Mauritius</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.35.34.2">Qatar</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.35.34.3">Luxembourg</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.35.34.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.36.35">
<td class="ltx_td ltx_align_left" id="A3.T2.4.36.35.1">Morocco</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.36.35.2">Russia</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.36.35.3">Malta</td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.36.35.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.37.36">
<td class="ltx_td ltx_align_left" id="A3.T2.4.37.36.1">Mozambique</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.37.36.2">Saudi Arabia</td>
<td class="ltx_td" id="A3.T2.4.37.36.3"></td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.37.36.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.38.37">
<td class="ltx_td ltx_align_left" id="A3.T2.4.38.37.1">Namibia</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.38.37.2">Singapore</td>
<td class="ltx_td" id="A3.T2.4.38.37.3"></td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.38.37.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.39.38">
<td class="ltx_td ltx_align_left" id="A3.T2.4.39.38.1">Niger</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.39.38.2">South Korea</td>
<td class="ltx_td" id="A3.T2.4.39.38.3"></td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.39.38.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.40.39">
<td class="ltx_td ltx_align_left" id="A3.T2.4.40.39.1">Nigeria</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.40.39.2">Sri Lanka</td>
<td class="ltx_td" id="A3.T2.4.40.39.3"></td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.40.39.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.41.40">
<td class="ltx_td ltx_align_left" id="A3.T2.4.41.40.1">Rwanda</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.41.40.2">Syria</td>
<td class="ltx_td" id="A3.T2.4.41.40.3"></td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.41.40.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.42.41">
<td class="ltx_td ltx_align_left" id="A3.T2.4.42.41.1">São Tomé and Príncipe</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.42.41.2">Taiwan</td>
<td class="ltx_td" id="A3.T2.4.42.41.3"></td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.42.41.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.43.42">
<td class="ltx_td ltx_align_left" id="A3.T2.4.43.42.1">Senegal</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.43.42.2">Tajikistan</td>
<td class="ltx_td" id="A3.T2.4.43.42.3"></td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.43.42.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.44.43">
<td class="ltx_td ltx_align_left" id="A3.T2.4.44.43.1">Seychelles</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.44.43.2">Thailand</td>
<td class="ltx_td" id="A3.T2.4.44.43.3"></td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.44.43.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.45.44">
<td class="ltx_td ltx_align_left" id="A3.T2.4.45.44.1">Sierra Leone</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.45.44.2">Timor-Leste</td>
<td class="ltx_td" id="A3.T2.4.45.44.3"></td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.45.44.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.46.45">
<td class="ltx_td ltx_align_left" id="A3.T2.4.46.45.1">Somalia</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.46.45.2">Turkey</td>
<td class="ltx_td" id="A3.T2.4.46.45.3"></td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.46.45.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.47.46">
<td class="ltx_td ltx_align_left" id="A3.T2.4.47.46.1">South Africa</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.47.46.2">Turkmenistan</td>
<td class="ltx_td" id="A3.T2.4.47.46.3"></td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.47.46.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.48.47">
<td class="ltx_td ltx_align_left" id="A3.T2.4.48.47.1">South Sudan</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.48.47.2">United Arab Emirates</td>
<td class="ltx_td" id="A3.T2.4.48.47.3"></td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.48.47.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.49.48">
<td class="ltx_td ltx_align_left" id="A3.T2.4.49.48.1">Sudan</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.49.48.2">Uzbekistan</td>
<td class="ltx_td" id="A3.T2.4.49.48.3"></td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.49.48.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.50.49">
<td class="ltx_td ltx_align_left" id="A3.T2.4.50.49.1">Tanzania</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.50.49.2">Vietnam</td>
<td class="ltx_td" id="A3.T2.4.50.49.3"></td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.50.49.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.51.50">
<td class="ltx_td ltx_align_left" id="A3.T2.4.51.50.1">Togo</td>
<td class="ltx_td ltx_align_left" id="A3.T2.4.51.50.2">Yemen</td>
<td class="ltx_td" id="A3.T2.4.51.50.3"></td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.51.50.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.52.51">
<td class="ltx_td ltx_align_left" id="A3.T2.4.52.51.1">Tunisia</td>
<td class="ltx_td" id="A3.T2.4.52.51.2"></td>
<td class="ltx_td" id="A3.T2.4.52.51.3"></td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.52.51.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.53.52">
<td class="ltx_td ltx_align_left" id="A3.T2.4.53.52.1">Uganda</td>
<td class="ltx_td" id="A3.T2.4.53.52.2"></td>
<td class="ltx_td" id="A3.T2.4.53.52.3"></td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.53.52.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.54.53">
<td class="ltx_td ltx_align_left" id="A3.T2.4.54.53.1">Zambia</td>
<td class="ltx_td" id="A3.T2.4.54.53.2"></td>
<td class="ltx_td" id="A3.T2.4.54.53.3"></td>
<td class="ltx_td ltx_nopad_r" id="A3.T2.4.54.53.4"></td>
</tr>
<tr class="ltx_tr" id="A3.T2.4.55.54">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T2.4.55.54.1">Zimbabwe</td>
<td class="ltx_td ltx_border_bb" id="A3.T2.4.55.54.2"></td>
<td class="ltx_td ltx_border_bb" id="A3.T2.4.55.54.3"></td>
<td class="ltx_td ltx_nopad_r ltx_border_bb" id="A3.T2.4.55.54.4"></td>
</tr>
</tbody>
</table>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 17 18:16:37 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
