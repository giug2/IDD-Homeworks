<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>CAD: Memory Efficient Convolutional Adapter for Segment Anything</title>
<!--Generated on Tue Sep 24 08:55:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.15889v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#S1" title="In CAD: Memory Efficient Convolutional Adapter for Segment Anything"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#S2" title="In CAD: Memory Efficient Convolutional Adapter for Segment Anything"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#S2.SS1" title="In 2 Related Works ‣ CAD: Memory Efficient Convolutional Adapter for Segment Anything"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Segment Anything</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#S2.SS2" title="In 2 Related Works ‣ CAD: Memory Efficient Convolutional Adapter for Segment Anything"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Adapters for PEFT</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#S3" title="In CAD: Memory Efficient Convolutional Adapter for Segment Anything"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#S3.SS1" title="In 3 Method ‣ CAD: Memory Efficient Convolutional Adapter for Segment Anything"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>High Frequency Components (HFC)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#S3.SS2" title="In 3 Method ‣ CAD: Memory Efficient Convolutional Adapter for Segment Anything"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Convolutional Adapter</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#S3.SS3" title="In 3 Method ‣ CAD: Memory Efficient Convolutional Adapter for Segment Anything"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Output Scaling</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#S4" title="In CAD: Memory Efficient Convolutional Adapter for Segment Anything"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#S4.SS1" title="In 4 Experiments ‣ CAD: Memory Efficient Convolutional Adapter for Segment Anything"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#S4.SS2" title="In 4 Experiments ‣ CAD: Memory Efficient Convolutional Adapter for Segment Anything"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#S4.SS3" title="In 4 Experiments ‣ CAD: Memory Efficient Convolutional Adapter for Segment Anything"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Comparative Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#S5" title="In CAD: Memory Efficient Convolutional Adapter for Segment Anything"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#S6" title="In CAD: Memory Efficient Convolutional Adapter for Segment Anything"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document">CAD: Memory Efficient Convolutional Adapter for Segment Anything</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Joohyeok Kim, Joonhyeon Song, Seohwan Yun, Seongho Yoon, Sangmin Lee 
<br class="ltx_break"/>School of Information Convergence, Kwangwoon University
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">The Foundation model for image segmentation, Segment Anything (SAM), has been actively researched in various fields since its proposal. Various researches have been proposed to adapt SAM to specific domains, with one notable approach involving the addition and training of lightweight adapter modules. While adapter-based fine-tuning approaches have reported parameter efficiency and significant performance improvements, they face a often overlooked issue: the excessive consumption of GPU memory relative to the number of trainable parameters. Addressing this issue, this paper proposes a memory-efficient parallel convolutional adapter architecture. This architecture connects in parallel with SAM’s image encoder, eliminating the need to store activations and gradients of the image encoder during model training. Our proposed architecture demonstrated competitive experimental results while using less than half the GPU memory compared to SAM Adapter, indicating its value as an alternative to simple decoder fine-tuning when hardware limitations preclude adapter-based learning. Our code implementation is available at <a class="ltx_ref ltx_href" href="https://github.com/Kyyle2114/Convolutional-Adapter-for-Segment-Anything" title="">our github</a>.</p>
</div>
<div class="ltx_para" id="p1">
<br class="ltx_break"/>
</div>
<section class="ltx_section ltx_indent_first" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In 2023, a foundation model for image segmentation called Segment Anything (SAM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#bib.bib1" title="">1</a>]</cite> was introduced. Befitting its status as a foundation model trained on large-scale segmentation datasets, SAM demonstrates excellent zero-shot performance, leading to its active research across various domains. Although SAM demonstrates impressive zero-shot performance, it still faces limitations as a foundation model. Consequently, model fine-tuning is essential to achieve optimal performance in novel domains.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To efficiently train heavy models, various Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed. One such approach is the adapter-based method, which involves adding new modules to the existing heavy model. This method freezes the weights of the heavy backbone network to prevent them from being trained, while adding lightweight modules within the backbone network. It then focuses on training only the necessary components, such as the internal lightweight modules and classification head.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Adapter modules possess a small number of parameters, yet training with these additional parameters alone can yield performance comparable to or surpassing that of training whole model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#bib.bib2" title="">2</a>]</cite>. This method of incorporating adapter module has been applied to various architectures with transformer backbones, such as SegFormer, SETR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#bib.bib3" title="">3</a>]</cite> and SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#bib.bib5" title="">5</a>]</cite>. These applications have reported performance improvements across diverse domains.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">While fine-tuning with adapters is undoubtedly parameter-efficient, there is a significant issue that often goes unemphasized: GPU memory constraints. Currently, most deep learning models are trained on GPUs. To train a deep learning model on a GPU, it is necessary to load various elements onto GPU memory, including batch data, model parameters, activations, and gradients. As the model capacity and complexity increase, GPU memory usage increases correspondingly. Consequently, heavy models like transformers may require the use of very small batch sizes on standard GPUs, or may not be trainable at all.</p>
</div>
<figure class="ltx_table" id="S1.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S1.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S1.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T1.1.1.1.1">Module</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T1.1.1.1.2">Trainable Parameters</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T1.1.1.1.3">GPU Memory Usage</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.2.1.1">SAM Decoder</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.2.1.2">4,058,340</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.2.1.3">14,113 MiB</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.3.2.1">SAM Adapter</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.3.2.2">4,788,740</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.3.2.3">36,871 MiB</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1.1.4.3.1">SAM Conv Adapter</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1.1.4.3.2">5,831,908</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1.1.4.3.3">17,697 MiB</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>GPU memory usage for each module (ViT-B, batch size = 4).</figcaption>
</figure>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Due to the nature of adapters, lightweight modules are inserted within the transformer backbone. To train these adapters using backpropagation, it is necessary to store the transformer’s activations and gradients in GPU memory. This scenario demands a disproportionately large amount of GPU memory relative to the number of trainable parameters. Table 1, 2 illustrates the number of trainable parameters and GPU memory usage for each module during model training. SAM adapter trains both the adapter within the transformer and the SAM’s mask decoder. SAM Conv Adapter refers to the architecture proposed in this paper.</p>
</div>
<figure class="ltx_table" id="S1.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S1.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S1.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T2.1.1.1.1">Module</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T2.1.1.1.2">Trainable Parameters</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T2.1.1.1.3">GPU Memory Usage</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T2.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T2.1.2.1.1">SAM Decoder</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T2.1.2.1.2">4,058,340</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T2.1.2.1.3">14,329 MiB</td>
</tr>
<tr class="ltx_tr" id="S1.T2.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T2.1.3.2.1">SAM Adapter</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T2.1.3.2.2">6,091,908</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T2.1.3.2.3">42,623 MiB</td>
</tr>
<tr class="ltx_tr" id="S1.T2.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T2.1.4.3.1">SAM Conv Adapter</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T2.1.4.3.2">5,831,908</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T2.1.4.3.3">15,457 MiB</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>GPU memory usage for each module (ViT-H, batch size = 2).</figcaption>
</figure>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">To address this issue, we propose a parallel convolutional adapter architecture for SAM, capable of reducing GPU memory usage. The proposed architecture connects parallelly to the transformer backbone and does not require the gradients or activations of the transformer during training, thus dramatically reducing the GPU memory requirements for training.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Following previous studies that applied adapters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#bib.bib4" title="">4</a>]</cite>, we utilized two challenging tasks: Shadow detection and Camouflaged object detection. We trained the proposed architecture alongside SAM Adapter and SAM Decoder on these datasets, evaluating and reporting their respective performances.</p>
</div>
</section>
<section class="ltx_section ltx_indent_first" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<section class="ltx_subsection ltx_indent_first" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Segment Anything</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Segment Anything (SAM), proposed in 2023, is a foundation model for image segmentation. The model comprises an image encoder utilizing a Vision Transformer (ViT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#bib.bib6" title="">6</a>]</cite>, a prompt encoder, and a mask decoder, and was trained on its proprietary dataset, SA-1B.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">The image encoder processes input images to compute image embeddings. Based on various user-input prompts such as points, boxes, masks, or text, the mask decoder makes the segmentation masks.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">The SAM model types can be categorized into three variants based on the vision transformer(image encoder) used: ViT-B(Base), ViT-L(Large), and ViT-H(Huge). The smallest model, ViT-B, has approximately 90 million parameters, while the largest, ViT-H, contains about 600 million parameters.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Adapters for PEFT</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">As the capacity of deep learning models such as transformers continues to increase, various methods are being proposed to efficiently train these heavy architectures. These methods are commonly referred to as Parameter-Efficient Fine-Tuning (PEFT).</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">One such approach is the adapter-based method. This method involves adding lightweight modules within the transformer, freezing the transformer’s weights, and training only the necessary components such as the adapters and classification head. Notable examples include LoRA (Low Rank Adaptation) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#bib.bib7" title="">7</a>]</cite>, proposed for training Large Language Models (LLMs) as shown in Figure 1, and QLoRA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#bib.bib8" title="">8</a>]</cite>, which quantizes the transformer’s weights with adding LoRA adapters.</p>
</div>
<figure class="ltx_figure" id="S2.F1">
<p class="ltx_p ltx_align_center" id="S2.F1.1"><span class="ltx_text" id="S2.F1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="230" id="S2.F1.1.1.g1" src="extracted/5875603/figures/lora.png" width="314"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>LoRA architecture.</figcaption>
</figure>
<figure class="ltx_figure" id="S2.F2">
<p class="ltx_p ltx_align_center" id="S2.F2.1"><span class="ltx_text" id="S2.F2.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="316" id="S2.F2.1.1.g1" src="extracted/5875603/figures/sam_sa.png" width="589"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>SAM Adapter architecture.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">In addition to the parallel adapter addition shown in Figure 1, there are also studies that sum the outputs of the transformer block and the adapter. For instance, EVP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#bib.bib3" title="">3</a>]</cite> combines patch embedding tuning with the extraction of high-frequency components, which are then input into an lightweight MLP adapter module. The output of this MLP is added to each transformer block. In EVP, the information input to the MLP is termed “task-specific information”.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">The SAM adapter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#bib.bib4" title="">4</a>]</cite>, which adds adapters to the SAM image encoder as shown in Figure 2, incorporating lightweight MLPs within the ViT along with the use of task-specific information. Besides this, various adapter architectures have been proposed, such as Med-SA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#bib.bib2" title="">2</a>]</cite> and R-SAM-Seg <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#bib.bib5" title="">5</a>]</cite>, which add MLPs after self-attention operations or connect them in parallel to the MLP as shown in Figure 3.</p>
</div>
<figure class="ltx_figure" id="S2.F3">
<p class="ltx_p ltx_align_center" id="S2.F3.1"><span class="ltx_text" id="S2.F3.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="310" id="S2.F3.1.1.g1" src="extracted/5875603/figures/block_sa.png" width="314"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Adapters in transformer block.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">These adapter module, with their small number of trainable parameters, demonstrate remarkable performance improvements relative to the number of trainable parameters, enabling parameter-efficient fine-tuning. However, due to the adapter’s position within the transformer block, training it through backpropagation requires storing the activations and gradients of the transformer block. As evident in Table 1, 2 above, this necessitates an disproportionately large amount of GPU memory relative to the number of parameters, potentially rendering training infeasible on standard GPUs.</p>
</div>
</section>
</section>
<section class="ltx_section ltx_indent_first" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<figure class="ltx_figure" id="S3.F4">
<p class="ltx_p ltx_align_center" id="S3.F4.1"><span class="ltx_text" id="S3.F4.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="289" id="S3.F4.1.1.g1" src="extracted/5875603/figures/cad.png" width="589"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>CAD architecture.</figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Figure 4 illustrates our parallel convolutional adapter (CAD) architecture.</p>
</div>
<section class="ltx_subsection ltx_indent_first" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>High Frequency Components (HFC)</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">High-frequency components (HFC) are known to be effective in image segmentation, and previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15889v1#bib.bib4" title="">4</a>]</cite> have utilized HFC for segmentation tasks. Following these precedents, we extract and use the HFC from the input image.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">We employ Fast Fourier Transform (FFT) to convert the image into the frequency domain. A zero mask is then applied to the central portion of the transformed image, retaining only the high-frequency components.</p>
</div>
<figure class="ltx_figure" id="S3.F5">
<p class="ltx_p ltx_align_center" id="S3.F5.1"><span class="ltx_text" id="S3.F5.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="149" id="S3.F5.1.1.g1" src="extracted/5875603/figures/fft.png" width="393"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The process to generate high-frequency components.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Inverse Fast Fourier Transform (IFFT) is subsequently applied to this high-frequency image to extract the HFC of the input image. These values are then concatenated along the channel dimension of the input image. Consequently, the input to the CAD has dimensions of (batch size, 6, height, width). The value 6 represents the sum of the original image’s 3 RGB channels and the 3 RGB channels of the HFC.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Convolutional Adapter</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The output of the CAD is designed to have the same dimensions as the image embedding from the image encoder, and this output is subsequently added to the image embedding vector.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Initially, we normalize each channel by dividing it by its respective maximum value, ensuring all six channels have a distribution within the range [0.0, 1.0]. Subsequently, we adjust the channel dimension using pointwise convolution operations. Then, through three Conv block operations and adaptive average pooling, we adjust the size of the feature map to match that of the image embedding.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">A single Conv block consists of a 3x3 convolution, leaky ReLU activation, batch normalization, and average pooling. This block is designed to halve the size of the feature map with each operation.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Output Scaling</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The range of values for image embeddings is approximately [-1.0, 1.0], and the subsequent mask decoder is expected to operate on this input distribution. Considering this, we employed the tanh activation function to constrain the output distribution of the CAD to [-1.0, 1.0]. Additionally, to ensure that the adapter’s output does not induce excessive changes to the image embedding, we scaled its magnitude by multiplying it by 0.1.</p>
</div>
</section>
</section>
<section class="ltx_section ltx_indent_first" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We trained the model on publicly available shadow detection and camouflaged object detection datasets. Along with our proposed convolutional adapter, we also trained the SAM mask decoder and SAM-adapter to evaluate the performance of each model.</p>
</div>
<section class="ltx_subsection ltx_indent_first" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Implementation Details</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">All models were trained using BCE loss and IoU loss, with the AdamW optimizer. The experimental conditions were set based on those used in the SAM-adapter. For segmentation evaluation metrics, we employed Dice and IoU metrics.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Experiments were conducted using models based on both ViT-B and ViT-H architectures. For ViT-B, a batch size of 4 was employed, while for ViT-H, a batch size of 2 was used. All models were trained for 20 epochs. All the experiments are performed on a single NVIDIA A100 with 80GB memory.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">For SAM, we loaded the model using pre-trained checkpoints and then trained only the parameters of specific module such as the adapter and mask decoder. For input prompts, we used box of the same size as the input image. In the case of SAM adapter, we used a model that we implemented ourselves, referencing the official implementation.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Datasets</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">For shadow detection, we utilized the ISTD dataset, while for camouflaged object detection, we employed the CAMO dataset and the COD10K dataset.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">The ISTD dataset comprises 1,330 training images and 540 test images. The CAMO dataset contains 1,000 training images and 250 test images, while the Cod10k dataset includes 3,040 training images and 2,026 test images. As the image sizes varied across these datasets, we resized all images to 512x512 before inputting them into the model.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparative Results</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Tables 3 and 4 summarize the experimental results, Figures 6 and 7 illustrate the GPU memory usage and Dice scores for each model across the respective datasets. “SAM” refers to the zero-shot performance without any training on the respective datasets. “Decoder” indicates results where only the mask decoder of SAM was trained, while “SA” denotes the SAM Adapter.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S4.T3.1.1.1.2">ISTD</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S4.T3.1.1.1.3">COD10K</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S4.T3.1.1.1.4">CAMO</th>
</tr>
<tr class="ltx_tr" id="S4.T3.1.2.2">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T3.1.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.2">Dice</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.1.2.2.3">IoU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.4">Dice</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.1.2.2.5">IoU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.6">Dice</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.2.2.7">IoU</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.3.1.1">SAM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.2">0.1495</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.3.1.3">0.0889</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.4">0.0601</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.3.1.5">0.0347</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.6">0.1637</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.3.1.7">0.1025</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.4.2.1">Decoder</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.2">0.7773</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.4.2.3">0.6889</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.4">0.6764</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.4.2.5">0.5738</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.6">0.6673</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.2.7">0.5484</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.5.3.1">SA</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.2">0.9039</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.5.3.3">0.8567</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.4">0.7343</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.5.3.5">0.6400</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.6">0.7393</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.3.7">0.6281</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S4.T3.1.6.4.1">CAD (Ours)</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.6.4.2">0.8681</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T3.1.6.4.3">0.8086</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.6.4.4">0.6489</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T3.1.6.4.5">0.5409</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.6.4.6">0.6847</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.6.4.7">0.5616</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Quantitative result with ViT-B.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S4.T4.1.1.1.2">ISTD</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S4.T4.1.1.1.3">COD10K</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S4.T4.1.1.1.4">CAMO</th>
</tr>
<tr class="ltx_tr" id="S4.T4.1.2.2">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T4.1.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.2.2.2">Dice</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.1.2.2.3">IoU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.2.2.4">Dice</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.1.2.2.5">IoU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.2.2.6">Dice</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T4.1.2.2.7">IoU</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.1.3.1.1">SAM</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.2">0.2153</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.3.1.3">0.1307</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.4">0.0498</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.3.1.5">0.0294</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.6">0.1387</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.1.7">0.0879</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.4.2.1">Decoder</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.2">0.7544</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.4.2.3">0.6638</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.4">0.7420</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.4.2.5">0.6579</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.6">0.7690</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.2.7">0.6730</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.5.3.1">SA</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3.2">0.9516</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.5.3.3">0.9220</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3.4">0.8600</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.5.3.5">0.7890</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3.6">0.8177</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3.7">0.7292</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.6.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S4.T4.1.6.4.1">CAD (Ours)</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.6.4.2">0.8616</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T4.1.6.4.3">0.8069</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.6.4.4">0.7464</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T4.1.6.4.5">0.6613</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.6.4.6">0.7578</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.6.4.7">0.6595</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Quantitative result with ViT-H.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F6">
<p class="ltx_p ltx_align_center" id="S4.F6.1"><span class="ltx_text" id="S4.F6.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="364" id="S4.F6.1.1.g1" src="extracted/5875603/figures/vitb.png" width="589"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The dice-memory trade-off for SAM Decoder, SAM Adapter, SAM Conv Adapter with ViT-B.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F7">
<p class="ltx_p ltx_align_center" id="S4.F7.1"><span class="ltx_text" id="S4.F7.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="364" id="S4.F7.1.1.g1" src="extracted/5875603/figures/vith.png" width="589"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The dice-memory trade-off for SAM Decoder, SAM Adapter, SAM Conv Adapter with ViT-H.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Across all evaluated datasets, the zero-shot performance of SAM, without any task-specific training, consistently yielded the lowest performance metrics. This observation held true even when employing the higher-capacity ViT-H image encoder. The significant performance gap between zero-shot and fine-tuned models underscores the inherent limitations of foundation models when applied to specialized tasks without adaptation. This finding emphasizes the critical need for domain-specific fine-tuning to achieve optimal performance in targeted applications. A notable observation was the substantial performance enhancement achieved through the fine-tuning of SAM’s mask decoder. This improvement suggests that even minimal task-specific adaptation can yield significant gains in segmentation accuracy.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">The SAM Adapter consistently demonstrated superior performance metrics across all model variants and evaluated datasets, suggesting its robust adaptability to diverse segmentation tasks. The transition from ViT-B to ViT-H architecture resulted in notable performance enhancements, indicating a positive correlation between model capacity and segmentation accuracy. In ISTD dataset, the ViT-H-based SAM Adapter achieved a near-perfect Dice score, demonstrating excellent segmentation performance.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">Despite its impressive performance, the ViT-H-based model presents significant computational challenges. As illustrated in Figure 7, its training process demands in excess of 40GB of GPU memory. This substantial resource requirement introduces practical limitations: a) Training infeasibility on GPUs with less than 40GB memory, b) Necessity for extremely small batch sizes such as 1 on hardware with limited memory capacity.</p>
</div>
<figure class="ltx_figure" id="S4.F8">
<p class="ltx_p ltx_align_center" id="S4.F8.1"><span class="ltx_text" id="S4.F8.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="504" id="S4.F8.1.1.g1" src="extracted/5875603/figures/infers.png" width="550"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The visualization results comparing the outputs of each model.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1">With the exception of two specific cases (COD10K dataset with ViT-B and CAMO dataset with ViT-H), CAD consistently outperformed the baseline approach of simple mask decoder fine-tuning. This trend was particularly pronounced in the ISTD dataset, where a substantial performance increment was observed. As illustrated in Figures 6 and 7, CAD achieved these performance improvements while maintaining a memory footprint comparable to that of mask decoder training.</p>
</div>
<div class="ltx_para" id="S4.SS3.p6">
<p class="ltx_p" id="S4.SS3.p6.1">The experimental outcomes position CAD as an attractive intermediate solution in the spectrum of adaptation strategies. It offers enhanced performance over simple mask decoder fine-tuning while remaining computationally feasible in scenarios where more resource-intensive approaches like SAM Adapter are impractical due to hardware constraints.</p>
</div>
<div class="ltx_para" id="S4.SS3.p7">
<p class="ltx_p" id="S4.SS3.p7.1">Figure 8 illustrates the output results from each model. It is evident that the vanilla SAM, which was not trained on these specific datasets, either fails to accurately identify the target objects or incorrectly inverts the detection of foreground (object) and background. This observation demonstrates that models, as exemplified by Decoder, SA, and CAD, require dataset-specific training to at least avoid confusing foreground and background elements. While the models may not achieve perfect object identification due to the inherent difficulty of the tasks, it is noteworthy that SA and CAD consistently detect regions that closely resemble the GT Mask in the most cases.</p>
</div>
</section>
</section>
<section class="ltx_section ltx_indent_first" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussions</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Up to this point, we conducted experiments using the same batch size for both CAD and SAM Adapter to ensure fair experimental conditions. However, given the same GPU memory usage, CAD can actually utilize a batch size nearly twice as large as SAM Adapter. While there isn’t a linear relationship between increasing batch size and model performance, using extremely small batch sizes such as 1 or 2 can lead to training instability due to noisy samples, potentially making model convergence impossible.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">When using heavier transformer backbones such as ViT-H instead of ViT-B, the number of inserted adapter modules increases. This could potentially lead to a greater performance improvement in SAM Adapter, potentially widening the performance gap between CAD and SAM Adapter. However, employing larger backbones like ViT-H results in a substantial increase in GPU memory usage. Such high memory requirements make training on standard GPUs challenging. Even when training is feasible, it necessitates the use of extremely small batch sizes. Conversely, parallel architecture like the CAD do not significantly increase GPU memory usage even with heavier backbones. The memory usage only increases by the amount of parameters in the enlarged backbone.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Due to the parallel connection of CAD with the image encoder, the learning process only requires the image embedding output by the image encoder, not the image encoder itself. Leveraging this characteristic, one could pre-compute image embeddings for training images and utilize them during the learning process. This approach would eliminate the need to load the image encoder into memory, resulting in significantly reduced GPU memory usage and faster model training in the same environment.</p>
</div>
</section>
<section class="ltx_section ltx_indent_first" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we introduce the GPU memory issues that are not typically discussed in the fine-tuning process using Adapters, and propose a parallel convolutional adapter architecture to address the memory constraints in the training process of Segment Anything. We conducted experiments with the proposed architecture on shadow detection and camouflaged object detection tasks, which are introduced as challenging tasks for Segment Anything. The proposed architecture, CAD, was able to achieve superior performance in most cases compared to simple mask decoder fine-tuning. When more accurate models are required than those obtained through simple mask decoder training, but the addition of adapters is not feasible due to hardware limitations, we anticipate that the CAD proposed in this paper may serve as a viable alternative worth exploring.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.

</span>
<span class="ltx_bibblock">Segment anything.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 4015–4026, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Junde Wu, Wei Ji, Yuanpei Liu, Huazhu Fu, Min Xu, Yanwu Xu, and Yueming Jin.

</span>
<span class="ltx_bibblock">Medical sam adapter: Adapting segment anything model for medical image segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2304.12620</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Weihuang Liu, Xi Shen, Chi-Man Pun, and Xiaodong Cun.

</span>
<span class="ltx_bibblock">Explicit visual prompting for low-level structure segmentations.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 19434–19445, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Tianrun Chen, Lanyun Zhu, Chaotao Deng, Runlong Cao, Yan Wang, Shangzhan Zhang, Zejian Li, Lingyun Sun, Ying Zang, and Papa Mao.

</span>
<span class="ltx_bibblock">Sam-adapter: Adapting segment anything in underperformed scenes.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 3367–3375, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Jie Zhang, Xubing Yang, Rui Jiang, Wei Shao, and Li Zhang.

</span>
<span class="ltx_bibblock">Rsam-seg: A sam-based approach with prior knowledge integration for remote sensing image semantic segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2402.19004</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2010.11929</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2106.09685</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Qlora: Efficient finetuning of quantized llms.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 24 08:55:44 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
