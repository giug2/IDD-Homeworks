<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Understanding Fairness in Recommender Systems: A Healthcare Perspective</title>
<!--Generated on Mon Sep  9 07:47:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="fairness,  algorithmic fairness,  healthcare,  understanding,  decision-making,  demographic parity,  equal accuracy,  equalized odds,  positive predicted value,  ethical artificial intelligence,  decision-making" lang="en" name="keywords"/>
<base href="/html/2409.03893v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#S1" title="In Understanding Fairness in Recommender Systems: A Healthcare Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#S2" title="In Understanding Fairness in Recommender Systems: A Healthcare Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#S3" title="In Understanding Fairness in Recommender Systems: A Healthcare Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Study Design</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#S4" title="In Understanding Fairness in Recommender Systems: A Healthcare Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#S5" title="In Understanding Fairness in Recommender Systems: A Healthcare Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results and Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#S6" title="In Understanding Fairness in Recommender Systems: A Healthcare Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#S7" title="In Understanding Fairness in Recommender Systems: A Healthcare Perspective"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusions</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Understanding Fairness in Recommender Systems:
<br class="ltx_break"/>A Healthcare Perspective</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Veronica Kecki
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:verokecki@gmail.com">verokecki@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-6229-7792" title="ORCID identifier">0000-0001-6229-7792</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">University of Gothenburg</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Gothenburg</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">Sweden</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alan Said
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:alan@gu.se">alan@gu.se</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-2929-0529" title="ORCID identifier">0000-0002-2929-0529</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">University of Gothenburg</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Gothenburg</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">Sweden</span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id7.id1">Fairness in AI-driven decision-making systems has become a critical concern, especially when these systems directly affect human lives. This paper explores the public’s comprehension of fairness in healthcare recommendations. We conducted a survey where participants selected from four fairness metrics – Demographic Parity, Equal Accuracy, Equalized Odds, and Positive Predictive Value – across different healthcare scenarios to assess their understanding of these concepts. Our findings reveal that fairness is a complex and often misunderstood concept, with a generally low level of public understanding regarding fairness metrics in recommender systems. This study highlights the need for enhanced information and education on algorithmic fairness to support informed decision-making in using these systems. Furthermore, the results suggest that a one-size-fits-all approach to fairness may be insufficient, pointing to the importance of context-sensitive designs in developing equitable AI systems.</p>
</div>
<div class="ltx_keywords">fairness, algorithmic fairness, healthcare, understanding, decision-making, demographic parity, equal accuracy, equalized odds, positive predicted value, ethical artificial intelligence, decision-making
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>rightsretained</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>18th ACM Conference on Recommender Systems; October 14–18, 2024; Bari, Italy</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>18th ACM Conference on Recommender Systems (RecSys ’24), October 14–18, 2024, Bari, Italy</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3640457.3691711</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0505-2/24/10</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Decision support systems</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Recommender systems</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id9"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Applied computing Health informatics</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id10"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Social and professional topics User studies</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The rapid advancements in AI have significantly transformed various sectors, including healthcare, finance, and education. These systems have the potential to enhance decision-making processes, improve efficiency, and provide personalized recommendations. However, integrating AI into critical decision-making roles raises significant ethical concerns. Algorithmic fairness, in particular, has become a major issue, especially when these systems affect individuals’ lives and well-being <cite class="ltx_cite ltx_citemacro_citep">(Schäfer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib13" title="">2017</a>; Hauptmann et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib4" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In healthcare, the stakes of algorithmic decisions are exceptionally high, as they can influence patient outcomes, resource allocation, and overall quality of care <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib16" title="">2020</a>)</cite>. Ensuring that these systems operate fairly and do not perpetuate biases is crucial. Various fairness metrics have been proposed to address these concerns, including Demographic Parity, Equal Accuracy, Equalized Odds, and Positive Predictive Value. Each metric offers a different approach to fairness, highlighting the complexity and contextual nature of defining and achieving fairness in AI systems. The computer science literature has amassed more than twenty different notions of fairness, making it infeasible to create a “catch-all” metric <cite class="ltx_cite ltx_citemacro_citep">(Abu Elyounes, <a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib2" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In recommendation and decision-making, any machine learning model will inevitably alleviate one condition while simultaneously deteriorating another <cite class="ltx_cite ltx_citemacro_citep">(Hiller, <a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib5" title="">2020</a>)</cite>, a phenomenon known as competing notions of fairness, or “trade-offs” <cite class="ltx_cite ltx_citemacro_citep">(Samuel, <a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib10" title="">2022</a>)</cite>. It is impossible to definitively state that one fairness metric is “fairer” than another in a general sense. A metric may be fairer in one aspect and less fair in another. For instance, a model could be optimized for either equality or accuracy, but it can be challenging to determine which is more appropriate in a given context. This raises the question: who decides what is fair?</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">This study investigates the public’s understanding and perception of fairness in healthcare recommender systems. By conducting a survey in which participants chose among commonly used fairness metrics in various healthcare scenarios, we aimed to uncover how people perceive and understand fairness and how these perceptions change based on the context of the decision-making situation. Our study reveals a low level of public understanding of fairness and significant influence from the specific context of the decision. Additionally, there is generally a lack of consensus on what fairness means in different scenarios.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We argue that improving knowledge about algorithmic fairness is essential for making informed choices when using AI-driven decision support systems.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Fairness in algorithmic decision-making is crucial, particularly in recommender systems that impact critical areas like healthcare <cite class="ltx_cite ltx_citemacro_citep">(Schäfer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib13" title="">2017</a>)</cite>. Various fairness metrics, such as Demographic Parity, Equalized Odds, and Positive Predictive Value, address different aspects of fairness. For instance, Demographic Parity focuses on equal representation, while Equalized Odds ensures equal error rates across groups. These metrics highlight the trade-offs between fairness and accuracy, especially in high-stakes scenarios like healthcare.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">A central aspect in the discussion of algorithmic fairness which helps to understand the difference between different metrics and the involved trade-offs is individual vs. group fairness, with accuracy typically representing the former and parity addressing the latter. A study by <cite class="ltx_cite ltx_citemacro_citet">Schlicker et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib12" title="">2021</a>)</cite>, for instance, contrasted people’s perceptions of fairness between human and automated agents pointing to a higher degree of consistency in automated decision-making. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Köchling et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib6" title="">2021</a>)</cite> also found that algorithms tend to carry out more accurate decisions than human agents; at the same time, the authors of this study highlighted the fact that despite the high degree of accuracy, there is simultaneously the tendency to perpetuate biases. There are quite a few studies which emphasize the need for optimizing the algorithms in such a way that protect vulnerable populations (<cite class="ltx_cite ltx_citemacro_citet">Rajkomar et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib9" title="">2018</a>)</cite>, for instance, propose concrete machine learning solutions to that effect), but it should be kept in mind that such strategies always come at the cost of accuracy on an individual level. In other words, the inherent conflict of algorithmic decision-making is that automated decisions are either likely to be accurate for individuals while enforcing biases on a group level or if group biases are mitigated, it will lower the degree of accuracy on an individual level.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">While the literature on people’s fairness preferences in general socio-economic contexts is quite extensive, it still remains rather scarce when it comes to either the healthcare context or more specifically algorithmic decision-making. The former can be exemplified by the study by <cite class="ltx_cite ltx_citemacro_citet">Li et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib7" title="">2019</a>)</cite> which found that public preference tends to lean towards accuracy over demographic parity when patient outcomes are at stake. As for the latter, <cite class="ltx_cite ltx_citemacro_citet">Srivastava et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib14" title="">2019</a>)</cite> noted a general preference for demographic parity, but with a shift towards accuracy in high-stakes situations. This suggests that fairness is highly contextual and should align with societal values to ensure equitable outcomes.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Public perception significantly influences the acceptance of recommendation systems which underscores the need for transparency and public involvement. While fairness has been extensively researched in recommender systems, e.g., <cite class="ltx_cite ltx_citemacro_citet">Li et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib8" title="">2021</a>)</cite> focusing on foundations and algorithms for fairness, <cite class="ltx_cite ltx_citemacro_citet">Elahi et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib3" title="">2021</a>)</cite> investigating evaluation from a beyond algorithm perspective, and <cite class="ltx_cite ltx_citemacro_citet">Wu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib17" title="">2023</a>)</cite> studying evaluation approaches and assurance strategies for fairness, the concept of fairness understanding among the end-users of recommender and decision support systems remains understudied <cite class="ltx_cite ltx_citemacro_citet">Wang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib15" title="">2023</a>)</cite>. It is also worth noting that some of the studies that have explored people’s fairness preferences in the context of automated decision-making and were able to identify a clear preference were based on case studies where the stakes were not strongly contrasted (see <cite class="ltx_cite ltx_citemacro_citet">Saxena et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib11" title="">2019</a>)</cite>).</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T1.2.1.1" style="font-size:90%;">Table 1</span>. </span><span class="ltx_text" id="S2.T1.3.2" style="font-size:90%;">The answer options available (A through E) in the study, including the metric and descriptions, as presented to the study participants in the high-stake scenario.</span></figcaption>
<table class="ltx_tabular ltx_align_middle" id="S2.T1.4">
<tr class="ltx_tr" id="S2.T1.4.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.4.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.4.1.1.1">Option</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.4.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.4.1.2.1">Metric</span></td>
<td class="ltx_td ltx_align_justify ltx_border_tt" id="S2.T1.4.1.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.1.3.1">
<span class="ltx_p" id="S2.T1.4.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.4.1.3.1.1.1">Description</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.2.1">A</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.2.2">Demographic Parity</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.4.2.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.2.3.1">
<span class="ltx_p" id="S2.T1.4.2.3.1.1">The same percentage of patients from both groups, regardless of whether they actually need a transfer or not (i.e., if they are qualified), get transferred to the ICU—but the algorithm may not be equally accurate across the two groups (i.e., it may happen that the accuracy is much lower for either of the groups)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.3.1">B</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.3.2">Equal Accuracy</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.3.3.1">
<span class="ltx_p" id="S2.T1.4.3.3.1.1">The model is equally accurate for both groups (i.e., same percentage of True Negatives + True Positives)—but it may be so that a smaller portion of patients from either of the groups overall ends up getting placed at the ICU (in this case, irrespective of being qualified)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.4.1">C</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.4.2">Equalized Odds</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.4.4.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.4.3.1">
<span class="ltx_p" id="S2.T1.4.4.3.1.1">The same percentage of patients actually requiring a transfer (i.e., those who qualify) end up getting one across both groups—but it could happen that a much smaller overall portion of patients from either group get the allocation</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.5.1">D</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.4.5.2">Positive Predicted Value</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.4.5.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.5.3.1">
<span class="ltx_p" id="S2.T1.4.5.3.1.1">Out of all the patients who end up getting a spot at the ICU (i.e. True Positive + False Positive), the same portion is identified correctly by the algorithm across both groups— but that could include a much lower portion of patients from one of the groups overall, as well as it may imply lower accuracy for one of the groups</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.6">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S2.T1.4.6.1">E</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S2.T1.4.6.2">N/A</td>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="S2.T1.4.6.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.6.3.1">
<span class="ltx_p" id="S2.T1.4.6.3.1.1">I do not understand the options</span>
</span>
</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Study Design</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We conducted a survey where participants were tasked with choosing between four different algorithmic fairness metrics in a healthcare setting.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The survey comprised three parts: an introductory section with background information, two main questions, and several supporting questions. This description primarily focuses on the two main questions. It should be noted that we accounted for the possibility that respondents might have little to no knowledge of the subject. Therefore, the descriptions and questions gradually introduced the topic over the course of the survey.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">The survey began with a short background introducing the issue of competing notions of fairness and briefly explaining the options presented in the two main questions.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">The specific task was presented to the participants as follows:
<br class="ltx_break"/><em class="ltx_emph ltx_font_italic" id="S3.p4.1.1">You will be presented with two different scenarios taking place in the healthcare context. An AI system needs to make a decision resulting in the allocation of resources across 2 equally-sized groups: a high-income patient group, and a low-income one. The choice needs to be made between 4 different algorithmic fairness metrics which prioritize different conditions.</em>
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">Following this description, the participants were presented with two scenarios, each correlating to a hypothetical medical situation, i.e., one high-stake and one low-stake.
Respondents were asked to choose between four different options (each optimizing a different fairness metric) that they felt achieved the best overall fairness in these two scenarios: 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_indent" id="S3.p6">
<p class="ltx_p" id="S3.p6.1"><span class="ltx_text ltx_font_bold" id="S3.p6.1.1">Scenario 1 (high-stake):</span> Imagine that a hospital is using an AI-based monitoring system to warn the rapid response team about patients at a high risk for deterioration, requiring their transfer to an intensive care unit within 6 hours.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S3.p6.1.2">Scenario 2 (low-stake):</span> Imagine that a hospital is using an AI tool that allows to identify patients who are likely to develop vitamin deficiencies from the existing patient health data, and may subsequently recommend such patients to get a blood test.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S3.p7">
<p class="ltx_p" id="S3.p7.1">The high-stake condition in Scenario 1 was identical to <cite class="ltx_cite ltx_citemacro_citep">(Rajkomar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib9" title="">2018</a>)</cite>, whereas the condition in Scenario 2 was adapted for the survey and converted into a low-stake condition. The fairness options presented under both scenarios were kept identical with the aim of contrasting these two conditions to be able to see <em class="ltx_emph ltx_font_italic" id="S3.p7.1.1">1)</em> if there is any difference at all between participants’ preferences depending on the context (i.e., gravity of situation), and <em class="ltx_emph ltx_font_italic" id="S3.p7.1.2">2)</em> which factors it could possibly be attributed to if detected. Furthermore, the study also contrasted between two different groups of patients: a high-income patient group and a low-income one, in order to compare fairness between both groups. For the sake of simplicity, the study presumes that the groups are equally-sized.</p>
</div>
<div class="ltx_para" id="S3.p8">
<p class="ltx_p" id="S3.p8.1">The four algorithmic fairness metrics chosen for the study were <em class="ltx_emph ltx_font_italic" id="S3.p8.1.1">Demographic Parity</em>, <em class="ltx_emph ltx_font_italic" id="S3.p8.1.2">Equal Accuracy</em>, <em class="ltx_emph ltx_font_italic" id="S3.p8.1.3">Equal Odds</em>, and <em class="ltx_emph ltx_font_italic" id="S3.p8.1.4">Positive Predictive Value</em> <cite class="ltx_cite ltx_citemacro_citep">(Rajkomar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib9" title="">2018</a>)</cite>. Each of these metrics alleviates at least one condition and deteriorates one other. Demographic Parity is achieved if two populations are equally represented in the outcome, independent of the size of the populations - in this scenario, demographic parity is achieved if patients from the high-income and low-income groups are equally represented in the outcome. Equal Accuracy, also referred to as accuracy parity, requires the model to perform equivalently (in terms of, e.g., prediction accuracy) within the populations in order to attain equal accuracy. Equalized odds refers to the notion of two equivalently qualified data points in two populations having the same probability of being selected, independent of population sizes. Finally, Positive predictive value, or precision, specifies the fraction of predicted values being correct. The specific formulations used in the high-stake scenario in survey are shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#S2.T1" title="In 2. Related Work ‣ Understanding Fairness in Recommender Systems: A Healthcare Perspective"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>. Similar descriptions were also presented for the low-stake scenario, although worded so as to reflect the low-stake of that scenario.</p>
</div>
<div class="ltx_para" id="S3.p9">
<p class="ltx_p" id="S3.p9.1">The difference between the response options for both scenarios is that the first scenario involves a high-stake condition (i.e., life/death situation), and the second question involves a low-stake condition (i.e., “nice-to-have” situation).</p>
</div>
<div class="ltx_para" id="S3.p10">
<p class="ltx_p" id="S3.p10.1">The rationale behind the questions corresponding to different conditions (high-stake/low-stake) is to see if a difference in preference for a particular option can be detected depending on the context. As seen in <a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#S2.T1" title="In 2. Related Work ‣ Understanding Fairness in Recommender Systems: A Healthcare Perspective"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>, an additional option (E) was included, for the survey participants to be able to indicate that they do not fully understand the four model options (whether individually or the difference between them).</p>
</div>
<div class="ltx_para" id="S3.p11">
<p class="ltx_p" id="S3.p11.1">Responses were collected through an online survey (Google Forms). The link to the survey was circulated on social media from the personal and professional profiles of a number of people and organizations within the higher education sector, reaching approximately 10, 000 followers combined.
The participants had no knowledge about the specific algorithmic models prior to the study, although many expressed familiarity with the general subject matter.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Data</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">A total of 131 survey responses were collected, representing a diverse cross-section of the population. Slightly more than half of the respondents identified as male, slightly fewer than half as female, and a small percentage as non-binary or preferred not to say. The respondents’ ages ranged from 19 to 59 years old, with an average age of 32 and a median age of 30. Over 60% of respondents worked full-time, while nearly 40% were students, indicating a broad mix of professional and educational backgrounds.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">The majority of respondents (76%) were from Sweden, the country of the paper’s origin, 19% were from the European Union, and the remaining 5% were from other countries. This geographic distribution reflects the channels through which the survey was distributed, primarily targeting individuals in the higher education sector. The diversity in age, gender, and geographic location among respondents provides a context for interpreting the survey results, particularly in understanding the varying perceptions of fairness in healthcare recommendation systems.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">The survey’s design and distribution aimed to capture a wide range of perspectives, which is critical for exploring public comprehension of fairness metrics across different healthcare scenarios.</p>
</div>
<figure class="ltx_figure" id="S4.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F1.sf1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="286" id="S4.F1.sf1.g1" src="x1.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F1.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F1.sf1.3.2" style="font-size:90%;">Scenario 1</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.F1.sf1.4">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S4.F1.sf1.5">A pie chart containing the answer percentage for each answer option for Scenario 1</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F1.sf2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="291" id="S4.F1.sf2.g1" src="x2.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F1.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F1.sf2.3.2" style="font-size:90%;">Scenario 2</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.F1.sf2.4">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S4.F1.sf2.5">A pie chart containing the answer percentage for each answer option for Scenario 2</p>
</div>
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F1.2.1.1" style="font-size:90%;">Figure 1</span>. </span><span class="ltx_text" id="S4.F1.3.2" style="font-size:90%;">The percentages of respondents’ preferred fairness metrics for Scenarios 1 and 2</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Results and Analysis</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The results of the user study are presented in <a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#S4.F1.sf1" title="In Figure 1 ‣ 4. Data ‣ Understanding Fairness in Recommender Systems: A Healthcare Perspective"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1(a)</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#S4.F1.sf2" title="In Figure 1 ‣ 4. Data ‣ Understanding Fairness in Recommender Systems: A Healthcare Perspective"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1(b)</span></a> for Scenario 1 and Scenario 2 respectively (cf. <a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#S3" title="3. Study Design ‣ Understanding Fairness in Recommender Systems: A Healthcare Perspective"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#S4.F1.sf1" title="In Figure 1 ‣ 4. Data ‣ Understanding Fairness in Recommender Systems: A Healthcare Perspective"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1(a)</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#S4.F1.sf2" title="In Figure 1 ‣ 4. Data ‣ Understanding Fairness in Recommender Systems: A Healthcare Perspective"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1(b)</span></a> show an overview of participants’ responses to Scenario 1 and 2. Each option (i.e., answers A through E as presented in <a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#S2.T1" title="In 2. Related Work ‣ Understanding Fairness in Recommender Systems: A Healthcare Perspective"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>) is represented. There is a clear preference for specific algorithmic fairness metrics in each case: Option C (Equalized Odds) was preferred for Scenario 1 (34% of responses), and Option B (Equal Accuracy) for Scenario 2 (35% of responses). In other words, the most common response among participants was the Equal Odds metric in a high-stake scenario and the Equal Accuracy metric in a low-stake one.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">It is important to note that only one of the four metrics used in this study can be classified as an “equality metric,” namely, Option A, Demographic Parity. The remaining metrics – Equal Accuracy, Equal Odds, and Positive Predictive Parity – are variations of “equity” metrics <cite class="ltx_cite ltx_citemacro_citep">(Abu Elyounes, <a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib2" title="">2019</a>)</cite>. These metrics prioritize accuracy, but do so in different ways.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">The results indicate a trend of participants preferring accuracy over equality, regardless of the scenario’s stakes. This preference suggests that when it comes to health, accuracy takes precedence, even in less critical situations. This finding aligns with previous studies, such as <cite class="ltx_cite ltx_citemacro_citet">Li et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib7" title="">2019</a>)</cite>, which concluded that individuals prefer accuracy when lives are at stake, as opposed to equality when monetary resources are involved, and <cite class="ltx_cite ltx_citemacro_citet">Srivastava et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2409.03893v2#bib.bib14" title="">2019</a>)</cite> which showed a tendency to prefer the Demographic Parity model in most contexts except for healthcare where accuracy is preferred.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">While accuracy is favored in both scenarios, the specific preferences for Equal Accuracy and Equalized Odds reflect nuanced differences in how participants perceive fairness in healthcare contexts. Equalized Odds emphasizes ensuring that individuals who genuinely need assistance receive it, making it more appropriate for high-stake scenarios where the consequences of misallocation are severe. Conversely, Equal Accuracy focuses on maintaining accuracy across all groups, which might be considered more suitable in low-stake situations where resource efficiency is important. This distinction could explain the different preferences observed between the two scenarios.</p>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">Additionally, the emphasis on accuracy in healthcare, even in low-stake scenarios, may be influenced by the intrinsic value placed on health outcomes. This prioritization of accuracy suggests that participants perceive healthcare decisions as inherently high-stakes, where the potential impact on individuals’ well-being outweighs considerations of equality. The consistent preference for accuracy, regardless of the stakes, highlights the complexity of defining fairness in contexts where human health is involved.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Discussion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The results of this study reveal a certain divergence in public perceptions of fairness within healthcare recommender systems. Even when presented with scenarios that could be perceived as having objectively fair alternatives, participants displayed different preferences across several fairness metrics they were presented with. This divergence underscores a fundamental challenge in designing recommender systems that are perceived as fair by all users.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">While approximately one-third of respondents favored Equalized Odds for high-stakes scenarios, whereas a similar fraction preferred Equal Accuracy for low-stakes scenarios, the remaining participants expressed varied opinions in both scenarios. This lack of consensus suggests that subjective interpretations of fairness can vary widely, even when certain metrics may appear to offer clear solutions. Factors contributing to these variations include individual values, personal experiences, and differing understandings of the implications of each of the fairness metrics.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">One notable observation is that the preference for Equal Accuracy and Equalized Odds highlights a tension between prioritizing overall accuracy and ensuring fairness in outcomes. Equal Accuracy aims to provide consistent accuracy across different groups, while Equalized Odds focuses on equalizing the chances of receiving a positive outcome for all qualified individuals. These metrics, while both emphasizing aspects of fairness, address it from different angles, illustrating the complexity of balancing fairness in practice. The disagreement among participants suggests that no single metric can be universally accepted as the most fair, as individuals weigh the importance of different fairness aspects differently.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">This disparity in perceptions also points to a broader issue in the deployment of recommender systems: the communication and understanding of algorithmic decisions. The study revealed that a significant portion of participants felt unsure about the fairness metrics, as indicated by the notable selection of the “N/A” option. This highlights a critical need for clear and accessible explanations of how algorithms function and the trade-offs they entail. Users should understand not only the mechanics of the algorithms but also the ethical considerations involved in choosing one fairness metric over another.</p>
</div>
<div class="ltx_para" id="S6.p5">
<p class="ltx_p" id="S6.p5.1">Furthermore, the findings suggest that fairness is inherently subjective and context-dependent. For instance, in high-stakes healthcare scenarios, the focus on accuracy may be driven by the critical need to correctly identify patients requiring urgent care. In contrast, in low-stakes scenarios, participants may prioritize fairness in terms of equitable access to resources or recommendations, even if it means accepting some level of inaccuracy. This indicates that public perception of what constitutes fairness can shift based on the stakes involved, further complicating the task of developing universally fair systems. This indicates that public perception of what constitutes fairness can shift based on the stakes involved which also points to unfeasibility of developing universally fair systems.</p>
</div>
<div class="ltx_para" id="S6.p6">
<p class="ltx_p" id="S6.p6.1">The apparent lack of a universally accepted definition of fairness in these contexts poses a challenge for developers and policymakers. It calls for a more nuanced approach to designing recommender systems, one that considers the diverse perspectives of users and strives to accommodate them. This might involve implementing adaptive systems capable of adjusting fairness metrics based on real-time context or user preferences. Additionally, engaging users in the design process through participatory methods could help align system outcomes with user expectations, thereby enhancing the perceived fairness and acceptance of the system.</p>
</div>
<div class="ltx_para" id="S6.p7">
<p class="ltx_p" id="S6.p7.1">This study underscores the complexity of achieving consensus on fairness in healthcare recommender systems. The varied range of opinions among participants reflects the diverse nature of fairness perceptions, which are influenced by individual and contextual factors. Addressing these differences requires thoughtful consideration of communication strategies, system design, and user engagement to create recommender systems that are both effective and perceived as fair by a broad user base.</p>
</div>
<div class="ltx_para" id="S6.p8">
<p class="ltx_p" id="S6.p8.1">It is worth mentioning that there remain certain practical challenges in realizing that ambition. Improving the general knowledge on the subject among the general public required to ensure proper understanding of the presented questions through educational curricula or information campaigns can take years, and until then, quite extensive background information is going to be necessary to be included into every study of such kind. That, in turn, adds to the cognitive load of participants and may affect participation rates and quality of results. It is also difficult to ensure genuine understanding of the presented options by participants. Interactive forms of data collection such as interviews or focus groups are much more likely to provide higher quality results but will fail to achieve a scale needed to be representative of the overall public opinion. Surveys, on the other hand, have the capacity to achieve that scale but offer lower guarantees of proper understanding of the questions.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusions</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This study highlights the complexities and challenges inherent in achieving fairness in healthcare recommendation systems. The diverse preferences observed among participants for different fairness metrics, such as Equal Accuracy and Equalized Odds, underscore the subjective nature of fairness and its context-dependent interpretation. Despite the presence of seemingly objective criteria, public perceptions of fairness vary widely, influenced by individual values and the specific context of decision-making scenarios.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">The findings suggest that a one-size-fits-all approach to fairness is impractical in the design of recommendation systems. Instead, there is a need for flexible and adaptive models that can dynamically adjust fairness criteria based on the context and user preferences. This flexibility is essential for accommodating the broad spectrum of fairness perceptions and ensuring that systems are not only technically fair but also perceived as fair by their users.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">Moreover, the study highlights the importance of transparency and user engagement in the development of recommendation systems. Clear communication about how algorithms work and the trade-offs involved in different fairness metrics is crucial for building trust and understanding among users. Engaging users in the design process through participatory methods can help align system functionalities with the diverse values and expectations of the user base, enhancing both fairness and acceptance.</p>
</div>
<div class="ltx_para" id="S7.p4">
<p class="ltx_p" id="S7.p4.1">One of the primary limitations of this study is the reliance on self-reported data from a relatively small and potentially non-representative sample of participants. The survey’s limited reach, primarily targeting individuals connected to the higher education sector, may not fully capture the diversity of opinions and experiences found in the broader population. Additionally, the scenarios presented in the survey, while designed to represent high-stake and low-stake situations, may not encompass the full range of complexities and nuances encountered in real-world healthcare settings. The predefined fairness metrics provided to participants may also constrain their understanding and responses, as they do not account for all possible interpretations of fairness. Furthermore, the cross-sectional nature of the study means it captures participants’ perceptions at a single point in time, without considering how these perceptions might evolve with increased knowledge or changing societal values. These limitations suggest that the findings should be interpreted with caution and underscore the need for further research to validate and expand upon these initial insights.</p>
</div>
<div class="ltx_para" id="S7.p5">
<p class="ltx_p" id="S7.p5.1">In conclusion, the pursuit of fairness in healthcare recommendation systems requires a nuanced approach that acknowledges the complexity of fairness perceptions. By considering the varied perspectives of users and the specificities of different decision-making contexts, developers can create more equitable and trusted systems. Future research should continue exploring the balance between fairness and other essential metrics, such as accuracy, and investigate methods for effectively communicating these complexities to the public. This ongoing effort is vital for the responsible and ethical deployment of AI-driven recommender systems in healthcare and beyond.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abu Elyounes (2019)</span>
<span class="ltx_bibblock">
Doaa Abu Elyounes. 2019.

</span>
<span class="ltx_bibblock">Contextual Fairness: A Legal and Policy Analysis of Algorithmic Fairness.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.2139/ssrn.3478296" title="">https://doi.org/10.2139/ssrn.3478296</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elahi et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Mehdi Elahi, Himan Abdollahpouri, Masoud Mansoury, and Helma Torkamaan. 2021.

</span>
<span class="ltx_bibblock">Beyond Algorithmic Fairness in Recommender Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Adjunct Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization</em> <em class="ltx_emph ltx_font_italic" id="bib.bib3.4.2">(UMAP ’21)</em>. Association for Computing Machinery, New York, NY, USA, 41–46.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3450614.3461685" title="">https://doi.org/10.1145/3450614.3461685</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hauptmann et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Hanna Hauptmann, Alan Said, and Christoph Trattner. 2022.

</span>
<span class="ltx_bibblock">Research directions in recommender systems for health and well-being.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">User Modeling and User-Adapted Interaction</em> 32, 5 (Nov. 2022), 781–786.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s11257-022-09349-4" title="">https://doi.org/10.1007/s11257-022-09349-4</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hiller (2020)</span>
<span class="ltx_bibblock">
Janine S. Hiller. 2020.

</span>
<span class="ltx_bibblock">Fairness in the Eyes of the Beholder: AI; Fairness; and Alternative Credit Scoring.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">West Virginia Law Review</em> 123, 3 (2020), 907–936.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://heinonline.org/HOL/P?h=hein.journals/wvb123&amp;i=937" title="">https://heinonline.org/HOL/P?h=hein.journals/wvb123&amp;i=937</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Köchling et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Alina Köchling, Shirin Riazy, Marius Claus Wehner, and Katharina Simbeck. 2021.

</span>
<span class="ltx_bibblock">Highly Accurate, But Still Discriminatory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Business &amp; Information Systems Engineering</em> 63, 1 (Feb. 2021), 39–54.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s12599-020-00673-w" title="">https://doi.org/10.1007/s12599-020-00673-w</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Meng Li, Helen A. Colby, and Philip Fernbach. 2019.

</span>
<span class="ltx_bibblock">Efficiency for Lives, Equality for Everything Else: How Allocation Preference Shifts Across Domains.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Social Psychological and Personality Science</em> 10, 5 (2019), 697–707.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yunqi Li, Yingqiang Ge, and Yongfeng Zhang. 2021.

</span>
<span class="ltx_bibblock">Tutorial on Fairness of Machine Learning in Recommender Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</em> <em class="ltx_emph ltx_font_italic" id="bib.bib8.4.2">(SIGIR ’21)</em>. Association for Computing Machinery, New York, NY, USA, 2654–2657.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3404835.3462814" title="">https://doi.org/10.1145/3404835.3462814</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajkomar et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Alvin Rajkomar, Michaela Hardt, Michael D. Howell, Greg Corrado, and Marshall H. Chin. 2018.

</span>
<span class="ltx_bibblock">Ensuring Fairness in Machine Learning to Advance Health Equity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Annals of Internal Medicine</em> 169, 12 (Dec. 2018), 866–872.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.7326/M18-1990" title="">https://doi.org/10.7326/M18-1990</a>
</span>
<span class="ltx_bibblock">Publisher: American College of Physicians.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Samuel (2022)</span>
<span class="ltx_bibblock">
Sigal Samuel. 2022.

</span>
<span class="ltx_bibblock">Why it’s so damn hard to make AI fair and unbiased.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.vox.com/future-perfect/22916602/ai-bias-fairness-tradeoffs-artificial-intelligence" title="">https://www.vox.com/future-perfect/22916602/ai-bias-fairness-tradeoffs-artificial-intelligence</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saxena et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Nripsuta Ani Saxena, Karen Huang, Evan DeFilippis, Goran Radanovic, David C. Parkes, and Yang Liu. 2019.

</span>
<span class="ltx_bibblock">How Do Fairness Definitions Fare? Examining Public Attitudes Towards Algorithmic Definitions of Fairness. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society</em> <em class="ltx_emph ltx_font_italic" id="bib.bib11.4.2">(AIES ’19)</em>. Association for Computing Machinery, New York, NY, USA, 99–106.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3306618.3314248" title="">https://doi.org/10.1145/3306618.3314248</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schlicker et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Nadine Schlicker, Markus Langer, Sonja K. Ötting, Kevin Baum, Cornelius J. König, and Dieter Wallach. 2021.

</span>
<span class="ltx_bibblock">What to expect from opening up ‘black boxes’? Comparing perceptions of justice between human and automated agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">Computers in Human Behavior</em> 122 (2021), 106837.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.chb.2021.106837" title="">https://doi.org/10.1016/j.chb.2021.106837</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schäfer et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Hanna Schäfer, Santiago Hors-Fraile, Raghav Pavan Karumur, André Calero Valdez, Alan Said, Helma Torkamaan, Tom Ulmer, and Christoph Trattner. 2017.

</span>
<span class="ltx_bibblock">Towards Health (Aware) Recommender Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">Proceedings of the 2017 International Conference on Digital Health</em> <em class="ltx_emph ltx_font_italic" id="bib.bib13.4.2">(DH ’17)</em>. Association for Computing Machinery, New York, NY, USA, 157–161.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3079452.3079499" title="">https://doi.org/10.1145/3079452.3079499</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Megha Srivastava, Hoda Heidari, and Andreas Krause. 2019.

</span>
<span class="ltx_bibblock">Mathematical Notions vs. Human Perception of Fairness: A Descriptive Approach to Fairness for Machine Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em> <em class="ltx_emph ltx_font_italic" id="bib.bib14.4.2">(KDD ’19)</em>. Association for Computing Machinery, New York, NY, USA, 2459–2468.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3292500.3330664" title="">https://doi.org/10.1145/3292500.3330664</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yifan Wang, Weizhi Ma, Min Zhang, Yiqun Liu, and Shaoping Ma. 2023.

</span>
<span class="ltx_bibblock">A Survey on the Fairness of Recommender Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">ACM Trans. Inf. Syst.</em> 41, 3 (Feb. 2023), 52:1–52:43.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3547333" title="">https://doi.org/10.1145/3547333</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Guangyao Wu, Pei Yang, Yuanliang Xie, Henry C. Woodruff, Xiangang Rao, Julien Guiot, Anne-Noelle Frix, Renaud Louis, Michel Moutschen, Jiawei Li, Jing Li, Chenggong Yan, Dan Du, Shengchao Zhao, Yi Ding, Bin Liu, Wenwu Sun, Fabrizio Albarello, Alessandra D’Abramo, Vincenzo Schininà, Emanuele Nicastri, Mariaelena Occhipinti, Giovanni Barisione, Emanuela Barisione, Iva Halilaj, Pierre Lovinfosse, Xiang Wang, Jianlin Wu, and Philippe Lambin. 2020.

</span>
<span class="ltx_bibblock">Development of a clinical decision support system for severity risk prediction and triage of COVID-19 patients at hospital admission: an international multicentre study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">European Respiratory Journal</em> 56, 2 (Aug. 2020).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1183/13993003.01104-2020" title="">https://doi.org/10.1183/13993003.01104-2020</a>
</span>
<span class="ltx_bibblock">Publisher: European Respiratory Society Section: Original Articles.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yao Wu, Jian Cao, and Guandong Xu. 2023.

</span>
<span class="ltx_bibblock">Fairness in Recommender Systems: Evaluation Approaches and Assurance Strategies.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">ACM Trans. Knowl. Discov. Data</em> 18, 1 (Aug. 2023), 10:1–10:37.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3604558" title="">https://doi.org/10.1145/3604558</a>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep  9 07:47:01 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
