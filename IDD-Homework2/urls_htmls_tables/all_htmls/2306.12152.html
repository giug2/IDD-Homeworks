<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.12152] Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario</title><meta property="og:description" content="In this paper, we tackle the problem of Egocentric Human-Object Interaction (EHOI) detection in an industrial setting. To overcome the lack of public datasets in this context, we propose a pipeline and a tool for gener…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.12152">

<!--Generated on Wed Feb 28 23:03:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_fleqn">
<h1 class="ltx_title ltx_title_document">Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rosario Leonardi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:rosario.leonardi@phd.unict.com">rosario.leonardi@phd.unict.com</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Francesco Ragusa
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Antonino Furnari
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Giovanni Maria Farinella
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">FPV@IPLAB, Department of Mathematics and Computer Science - University of Catania, Catania 95125, Italy
</span>
<span class="ltx_contact ltx_role_address">Next Vision s.r.l, Spinoff of the University of Catania - Viale Andrea Doria 6, Catania 95125, Italy
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">In this paper, we tackle the problem of Egocentric Human-Object Interaction (EHOI) detection in an industrial setting. To overcome the lack of public datasets in this context, we propose a pipeline and a tool for generating synthetic images of EHOIs paired with several annotations and data signals (e.g., depth maps or instance segmentation masks). Using the proposed pipeline, we present <span id="id1.id1.1" class="ltx_text ltx_font_italic">EgoISM-HOI</span> a new multimodal dataset composed of synthetic EHOI images in an industrial environment with rich annotations of hands and objects. To demonstrate the utility and effectiveness of synthetic EHOI data produced by the proposed tool, we designed a new method that predicts and combines different multimodal signals to detect EHOIs in RGB images. Our study shows that exploiting synthetic data to pre-train the proposed method significantly improves performance when tested on real-world data. Moreover, the proposed approach outperforms state-of-the-art class-agnostic methods. To support research in this field, we publicly release the datasets, source code, and pre-trained models at <a target="_blank" href="https://iplab.dmi.unict.it/egoism-hoi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://iplab.dmi.unict.it/egoism-hoi</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, wearable devices have become increasingly popular as they offer a first-person perspective of how users interact with the world around them. One of the advantages of wearable devices is that they allow the collection and processing of visual information without requiring users to hold any devices with their hands, enabling them to perform their activities in a natural way. Intelligent systems can analyze this visual information to provide services to support humans in different domains such as activities of daily living <cite class="ltx_cite ltx_citemacro_citep">(Damen et al., <a href="#bib.bib9" title="" class="ltx_ref">2014</a>, <a href="#bib.bib8" title="" class="ltx_ref">2018</a>; Grauman et al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>, cultural sites <cite class="ltx_cite ltx_citemacro_citep">(Farinella et al., <a href="#bib.bib13" title="" class="ltx_ref">2019</a>)</cite> and industrial scenarios <cite class="ltx_cite ltx_citemacro_citep">(Sener et al., <a href="#bib.bib44" title="" class="ltx_ref">2022</a>; Mazzamuto et al., <a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite>. In particular, egocentric vision can be adopted in the industrial context to understand workers’ behavior, improve workplace safety, and increase overall productivity. For example, by detecting the hands of the workers and determining which objects they are interacting with, it is possible to monitor object usage, provide information on the procedures to be carried out, and improve the safety of workers by issuing reminders when dangerous objects are manipulated.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Previous works have investigated the problem of Human-Object Interaction detection (HOI) considering either third-person <cite class="ltx_cite ltx_citemacro_citep">(Gkioxari et al., <a href="#bib.bib16" title="" class="ltx_ref">2018</a>; Liao et al., <a href="#bib.bib28" title="" class="ltx_ref">2020</a>)</cite> or first-person <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib31" title="" class="ltx_ref">2022</a>; Zhang et al., <a href="#bib.bib54" title="" class="ltx_ref">2022b</a>)</cite> points of view. While these works have considered generic scenarios (e.g., COCO objects) or class-agnostic settings <cite class="ltx_cite ltx_citemacro_citep">(Shan et al., <a href="#bib.bib45" title="" class="ltx_ref">2020</a>)</cite>, their use in industrial contexts is still understudied due to the limited availability of public datasets <cite class="ltx_cite ltx_citemacro_citep">(Ragusa et al., <a href="#bib.bib39" title="" class="ltx_ref">2021</a>; Sener et al., <a href="#bib.bib44" title="" class="ltx_ref">2022</a>)</cite>. To develop a system capable of detecting Egocentric Human-Object Interactions (EHOI) in this context, it is generally required to collect and label large amounts of domain-specific data, which could be expensive in terms of costs and time and not always possible due to privacy constraints and industrial secrets <cite class="ltx_cite ltx_citemacro_citep">(Ragusa et al., <a href="#bib.bib39" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2306.12152/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="400" height="97" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 1: </span>Synthetic EHOI images generation pipeline. (a) We use 3D scanners to acquire 3D models of the objects and environment. (b) We hence use the proposed data generation tool to create the synthetic dataset.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we investigate whether the use of synthetic data in first-person vision can mitigate the need for labeled real domain-specific data in model training, which would greatly reduce the cost of gathering a suitable dataset for model development. We propose a pipeline (see Fig. <a href="#S1.F1" title="Fig. 1 ‣ 1 Introduction ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) and a tool that, leveraging 3D models of the target environment and objects, produces a large number of synthetic EHOI image examples, automatically labeled with several annotations, such as hand-object 2D-3D bounding boxes, object categories, hand information (i.e., hand side, contact state, and associated active objects) as well as multimodal signals such as depth maps and instance segmentation masks.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Exploiting the proposed pipeline, we present <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">EgoISM-HOI</span> (Egocentric Industrial Synthetic Multimodal dataset for Human-Object Interaction detection), a new photo-realistic dataset of EHOIs in an industrial scenario with rich annotations of hands, objects, and active objects (i.e., the objects the user is interacting with), including class labels, depth maps, and instance segmentation masks (see Fig. <a href="#S1.F1" title="Fig. 1 ‣ 1 Introduction ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (b)). To assess the suitability of the synthetic data generated with the proposed protocol to tackle the EHOI detection task on target real data, we further acquired and labeled 42 real egocentric videos in an industrial laboratory in which different subjects perform test and repair operations on electrical boards<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Note that both real and synthetic data were acquired in the same environment and with the same objects</span></span></span>. We annotated all EHOIs instances of the images identifying the frames in which interactions occur and all active objects with a bounding box associated with the related object class. In addition, we labeled the hands and all the objects in the images.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We investigated the potential of using the generated synthetic multimodal data, including depth maps and instance segmentation masks, to improve the performance of EHOI detection methods. Specifically, we designed an EHOI detection approach based on the method proposed in <cite class="ltx_cite ltx_citemacro_citet">Shan et al. (<a href="#bib.bib45" title="" class="ltx_ref">2020</a>)</cite> which makes use of the different multimodal signals available within our dataset. Experiments show that the proposed method outperforms baseline approaches based on the exploitation of class-agnostic models trained on out-of-domain real-world data. Indeed, the proposed method achieves good performance when trained with our synthetic data and a very small amount of real-world data. Additional experiments show that, by leveraging multimodal signals, the accuracy and robustness of our EHOI detection system increased.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The contributions of this study are the following: 1) we propose a pipeline that exploits 3D models of real objects and environments to generate thousands of domain-specific synthetic egocentric human-object interaction images paired with several labels and modalities; 2) we present <span id="S1.p6.1.1" class="ltx_text ltx_font_italic">EgoISM-HOI</span>, a new multimodal dataset of synthetic EHOIs in an industrial scenario with rich annotations of hands and objects. To test the ability of models to generalize to real-world data, we acquire and manually labeled real-world images of EHOIs in the target environment; 3) we design a new method for EHOI detection that exploits additional modalities, such as depth maps and instance segmentation maps to enhance the performance of classic HOI detection approaches; 4) we perform extensive evaluations to highlight the benefit of using synthetic data to pre-train EHOI detection methods, mainly when a limited set of real data is available, and report improvements of our approach over classical class-agnostic state-of-the-art methods; 5) we release the dataset and code publicly at the following link: <a target="_blank" href="https://iplab.dmi.unict.it/egoism-hoi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://iplab.dmi.unict.it/egoism-hoi</a>.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The remainder of this paper is organized as follows. Section <a href="#S2" title="2 Related Work ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides a detailed summary of the related work. Section <a href="#S3" title="3 Proposed EHOI Generation Pipeline ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> details the proposed data generation pipeline. Section <a href="#S4" title="4 EgoISM-HOI dataset ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> describes the proposed dataset. Section <a href="#S5" title="5 Proposed approach ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> introduces our multimodal EHOI detection method. Section <a href="#S6" title="6 Experimental results ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> reports and discusses the performed experiments and ablation studies. Finally, Section <a href="#S7" title="7 Conclusion ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> concludes the paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this Section, we discuss datasets and state-of-the-art methods for detecting human-object interactions from images and videos acquired from both third (TPV) and first-person vision (FPV).</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Datasets for Human-Object Interaction Detection from Third Person View</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Previous works have proposed benchmark datasets to study human-object interactions from a third-person vision. <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">PASCAL VOC</span> <cite class="ltx_cite ltx_citemacro_citep">(Everingham et al., <a href="#bib.bib12" title="" class="ltx_ref">2009</a>)</cite> was one of the first datasets focusing on understanding human behavior from images. This dataset has been used for several tasks related to human-object interaction understanding, such as object classification, object detection, and static action classification. <cite class="ltx_cite ltx_citemacro_citet">Gupta and Malik (<a href="#bib.bib18" title="" class="ltx_ref">2015</a>)</cite> introduced <span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_italic">The Verbs in COCO</span> (V-COCO) dataset, an extension of the <span id="S2.SS1.p1.1.3" class="ltx_text ltx_font_italic">COCO</span> dataset <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib30" title="" class="ltx_ref">2014</a>)</cite> that includes 26 verbs classes, along with bounding box annotations of human and objects involved in interactions. <cite class="ltx_cite ltx_citemacro_citet">Chao et al. (<a href="#bib.bib6" title="" class="ltx_ref">2015</a>)</cite> presented <span id="S2.SS1.p1.1.4" class="ltx_text ltx_font_italic">Humans Interacting with COmmon Objects</span> (HICO), a dataset for detecting human-object interactions that comprises more than 600 categories of human-object interactions across 117 activities and 80 common objects. <cite class="ltx_cite ltx_citemacro_citet">Chao et al. (<a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite> extended <span id="S2.SS1.p1.1.5" class="ltx_text ltx_font_italic">HICO</span> to <span id="S2.SS1.p1.1.6" class="ltx_text ltx_font_italic">HICO-DET</span>, adding more than 150,000 annotated instances of human-object interaction pairs. <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite> proposed <span id="S2.SS1.p1.1.7" class="ltx_text ltx_font_italic">AmbiguousHOI</span>, a benchmark that includes hard ambiguous images of HOI instances selected from existing datasets such as <span id="S2.SS1.p1.1.8" class="ltx_text ltx_font_italic">HICO-DET</span> <cite class="ltx_cite ltx_citemacro_citep">(Chao et al., <a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite>, <span id="S2.SS1.p1.1.9" class="ltx_text ltx_font_italic">V-COCO</span> <cite class="ltx_cite ltx_citemacro_citep">(Gupta and Malik, <a href="#bib.bib18" title="" class="ltx_ref">2015</a>)</cite>, and <span id="S2.SS1.p1.1.10" class="ltx_text ltx_font_italic">OpenImage</span> <cite class="ltx_cite ltx_citemacro_citep">(Kuznetsova et al., <a href="#bib.bib25" title="" class="ltx_ref">2020</a>)</cite>. The <span id="S2.SS1.p1.1.11" class="ltx_text ltx_font_italic">Human-Object Interaction for Application</span> (HOI-A) dataset has been proposed by <cite class="ltx_cite ltx_citemacro_citet">Liao et al. (<a href="#bib.bib28" title="" class="ltx_ref">2020</a>)</cite>. It includes 38,668 annotated images with 11 different types of objects, and 10 action categories, comprising 43,820 human instances, 60,438 object instances, and 96,160 interaction instances. Recently, the human-object interaction detection task has been studied by exploiting multimodal signals. The <span id="S2.SS1.p1.1.12" class="ltx_text ltx_font_italic">BEHAVE</span> dataset <cite class="ltx_cite ltx_citemacro_citep">(Bhatnagar et al., <a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite> is a multi-view RGB-D dataset of human-object interactions acquired in natural environments, which contains 3D human and object annotations, instance segmentation masks, and contact-state labels. Most related to our study, <span id="S2.SS1.p1.1.13" class="ltx_text ltx_font_italic">100 Days of Hands</span> <cite class="ltx_cite ltx_citemacro_citep">(Shan et al., <a href="#bib.bib45" title="" class="ltx_ref">2020</a>)</cite> is a large-scale dataset of human-object interactions containing more than 131 days of video footage acquired from both third and first-person points of view. The authors extracted 100K frames and annotated with bounding boxes 189.6K hands and 110.1K objects involved in interactions. Moreover, for each hand, they annotated the contact state considering five different classes (i.e., <span id="S2.SS1.p1.1.14" class="ltx_text ltx_font_italic">none, self, other-person, non-portable object</span>, and <span id="S2.SS1.p1.1.15" class="ltx_text ltx_font_italic">portable object</span>).</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The aforementioned works focused mostly on a third-person point of view. Our study focuses on understanding human-object interactions from a first-person point of view.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Datasets for Human-Object Interaction Detection from First Person View</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Owing to the aforementioned vantage point given by wearable cameras, previous works have proposed datasets to study human-object interactions from first-person vision. <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">EgoHands</span> <cite class="ltx_cite ltx_citemacro_citep">(Bambach et al., <a href="#bib.bib1" title="" class="ltx_ref">2015</a>)</cite> is a dataset composed of egocentric video pairs of people interacting with their hands in different daily-life contexts, where they are involved in four social situations (i.e., playing cards, playing chess, solving puzzles, and playing Jenga). It is composed of 130,000 frames and 4,800 pixel-level segmentation masks of hands. <span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_italic">EPIC-KITCHENS-100</span> <cite class="ltx_cite ltx_citemacro_citep">(Damen et al., <a href="#bib.bib7" title="" class="ltx_ref">2021</a>)</cite> contains over 100 hours, 20 million frames, and 90,000 actions in 700 variable-length videos of unscripted activities in 45 kitchen environments. The authors provide spatial annotations of (1) instance segmentations masks using Mask R-CNN <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite> and (2) hand and active object bounding boxes labeled with the system introduced in <cite class="ltx_cite ltx_citemacro_citet">Shan et al. (<a href="#bib.bib45" title="" class="ltx_ref">2020</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Darkhalil et al. (<a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite> proposed <span id="S2.SS2.p1.1.3" class="ltx_text ltx_font_italic">VISOR</span>, an extension of <span id="S2.SS2.p1.1.4" class="ltx_text ltx_font_italic">EPIC-KITCHENS-100</span>, which comprises pixel annotations and a benchmark suite for segmenting hands and active objects in egocentric videos. It contains 272,000 manual segmented semantic masks of 257 object classes, 9.9 million interpolated dense masks, and 67,000 hand-object relations. <span id="S2.SS2.p1.1.5" class="ltx_text ltx_font_italic">EGTEA Gaze+</span> <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite> contains more than 28 hours of egocentric video acquired by subjects performing different meal preparation tasks. The authors provide several annotations, including binocular gaze tracking data, frame-level action annotations, and 15K hand segmentation masks. Recognizing EHOIs could be particularly useful in industrial scenarios, for example, to optimize production processes or to increase workplace safety. <span id="S2.SS2.p1.1.6" class="ltx_text ltx_font_italic">MECCANO</span> <cite class="ltx_cite ltx_citemacro_citep">(Ragusa et al., <a href="#bib.bib39" title="" class="ltx_ref">2021</a>, <a href="#bib.bib38" title="" class="ltx_ref">2022</a>)</cite> is a multimodal dataset of FPV videos for human behavior understanding collected in an industrial-like scenario. It includes gaze signals, depth maps, and several annotations. MECCANO has been explicitly annotated to study EHOIs with bounding boxes around the hands and active objects, and verbs that describe the interactions. <span id="S2.SS2.p1.1.7" class="ltx_text ltx_font_italic">Assembly101</span> <cite class="ltx_cite ltx_citemacro_citep">(Sener et al., <a href="#bib.bib44" title="" class="ltx_ref">2022</a>)</cite> is a multi-view action dataset of people assembling and disassembling 101 toy vehicles. It contains 4321 video sequences acquired simultaneously from 8 TPV and 4 FPV cameras, 1M fine-grained action segments, and 18 million 3D hand poses. <span id="S2.SS2.p1.1.8" class="ltx_text ltx_font_italic">Ego4D</span> <cite class="ltx_cite ltx_citemacro_citep">(Grauman et al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite> is a multimodal video dataset to study egocentric perception. The dataset contains more than 3,500 video hours of daily life activity captured by 931 subjects and additional modalities such as eye gaze data, audio, and 3D mesh of environments. EGO4D has been annotated with bounding boxes around the hands and objects involved in the interactions. <span id="S2.SS2.p1.1.9" class="ltx_text ltx_font_italic">HOI4D</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib31" title="" class="ltx_ref">2022</a>)</cite> is a large-scale 4D egocentric dataset for human-object interaction detection. <span id="S2.SS2.p1.1.10" class="ltx_text ltx_font_italic">HOI4D</span> contains more than 2 million RGB-D egocentric video frames in different indoor environments of people interacting with 800 object instances.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Unlike these works, we aim to study the usefulness of synthetic data for training models which need to be deployed in a specific environment. To this aim, we provide <span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_italic">EgoISM-HOI</span>, a photo-realistic multimodal dataset of synthetic images for understanding human-object interactions acquired in an industrial scenario, paired with labeled real-world images of egocentric human-object interactions in the same target environment. Our dataset contains RGB-D images and rich automatically labeled annotations of hands, objects, and active objects, including bounding boxes, object categories, instance segmentation masks, and interaction information (i.e., hand contact state, hand side, and hand-active object relationships).</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Human-Object Interaction simulators and synthetic datasets</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">This line of research focused on providing 3D simulators which are able to generate automatically labeled synthetic data <cite class="ltx_cite ltx_citemacro_citep">(Kolve et al., <a href="#bib.bib24" title="" class="ltx_ref">2017</a>; Savva et al., <a href="#bib.bib43" title="" class="ltx_ref">2019</a>; Xia et al., <a href="#bib.bib50" title="" class="ltx_ref">2020</a>; Hwang et al., <a href="#bib.bib23" title="" class="ltx_ref">2020</a>; Quattrocchi et al., <a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>. While these tools allow simulating an agent that navigates in an indoor environment, there are fewer choices for simulating object interaction. <cite class="ltx_cite ltx_citemacro_citet">Mueller et al. (<a href="#bib.bib36" title="" class="ltx_ref">2017</a>)</cite> proposed a data generation framework that tracks and combines real human hands with virtual objects to generate photorealistic images of hand-object interactions. Using the proposed tool, the authors introduced <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">SynthHands</span>, a dataset that contains around 200K RGB-D images of hand-object interactions acquired from 5 FPV virtual cameras. <span id="S2.SS3.p1.1.2" class="ltx_text ltx_font_italic">ManipulaTHOR</span> <cite class="ltx_cite ltx_citemacro_citep">(Ehsani et al., <a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite> is an extension of the <span id="S2.SS3.p1.1.3" class="ltx_text ltx_font_italic">AI2-THOR</span> framework <cite class="ltx_cite ltx_citemacro_citep">(Kolve et al., <a href="#bib.bib24" title="" class="ltx_ref">2017</a>)</cite> that adds a robotic arm to virtual agents, enabling the interaction with objects. Thanks to this framework, the authors introduced the <span id="S2.SS3.p1.1.4" class="ltx_text ltx_font_italic">Arm POINTNAV</span> dataset, which contains interactions in 30 kitchen scenes, 150 object categories, and 12 graspable object categories. <cite class="ltx_cite ltx_citemacro_citet">Hasson et al. (<a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite> introduced the <span id="S2.SS3.p1.1.5" class="ltx_text ltx_font_italic">ObMan</span> dataset, a large-scale synthetic image dataset of hand-object interactions. The peculiarity of this work is that the authors used the <span id="S2.SS3.p1.1.6" class="ltx_text ltx_font_italic">GraspIt</span> software <cite class="ltx_cite ltx_citemacro_citep">(Miller and Allen, <a href="#bib.bib35" title="" class="ltx_ref">2004</a>)</cite> to improve the photo-realism of the generated interactions. The generated dataset contains more than 20,000 hand-object interactions in which the background is randomized by choosing images from the <span id="S2.SS3.p1.1.7" class="ltx_text ltx_font_italic">LSUN</span> <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a href="#bib.bib52" title="" class="ltx_ref">2015</a>)</cite> and <span id="S2.SS3.p1.1.8" class="ltx_text ltx_font_italic">ImageNet</span> <cite class="ltx_cite ltx_citemacro_citep">(Russakovsky et al., <a href="#bib.bib42" title="" class="ltx_ref">2015</a>)</cite> datasets. <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a href="#bib.bib48" title="" class="ltx_ref">2022</a>)</cite> introduced <span id="S2.SS3.p1.1.9" class="ltx_text ltx_font_italic">DexGraspNet</span>, a large-scale synthetic dataset for robotic dexterous grasping containing 1.32M grasps of 5355 objects among 133 object categories. <cite class="ltx_cite ltx_citemacro_citet">Ye et al. (<a href="#bib.bib51" title="" class="ltx_ref">2023</a>)</cite> proposed an approach for synthesizing virtual human hands interacting with real-world objects from RGB images.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Differently from these works, our generation pipeline has been specifically designed to obtain accurate 3D reconstructions of a target environment and the objects it contains. 3D models of the target environment and objects are used by our tool to generate realistic egocentric hand-object interactions that integrate coherently with the surrounding environment. Moreover, our tool allows the customization of several parameters of the virtual scene, for example, by randomizing the light points, the position of the virtual object in the environment, or the virtual agent’s clothing. In addition, the proposed tool is able to output several annotations automatically labeled and data signals, such as 2D-3D bounding boxes, hand labels (i.e., hand contact state and hand side), instance segmentation masks, and depth maps. Another difference with respect to the aforementioned works is that our tool is designed to automatically generate interactions from a first-person point of view without using any additional real-world data or specific hardware devices other than 3D models.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Methods for Detecting Human-Object Interactions</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.9" class="ltx_p">In the past year, the human-object interaction detection task has been studied from the third-person point of view <cite class="ltx_cite ltx_citemacro_citep">(Gupta and Malik, <a href="#bib.bib18" title="" class="ltx_ref">2015</a>; Chao et al., <a href="#bib.bib6" title="" class="ltx_ref">2015</a>, <a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Gkioxari et al. (<a href="#bib.bib16" title="" class="ltx_ref">2018</a>)</cite> proposed a method for detecting human-object interactions in the form of <math id="S2.SS4.p1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S2.SS4.p1.1.m1.1a"><mo id="S2.SS4.p1.1.m1.1.1" xref="S2.SS4.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.1.m1.1b"><lt id="S2.SS4.p1.1.m1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.1.m1.1c">&lt;</annotation></semantics></math><span id="S2.SS4.p1.2.1" class="ltx_text ltx_font_italic">human, verb, object<math id="S2.SS4.p1.2.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.SS4.p1.2.1.m1.1a"><mo id="S2.SS4.p1.2.1.m1.1.1" xref="S2.SS4.p1.2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.2.1.m1.1b"><gt id="S2.SS4.p1.2.1.m1.1.1.cmml" xref="S2.SS4.p1.2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.2.1.m1.1c">&gt;</annotation></semantics></math></span> triplets, where bounding boxes around objects and humans are also predicted. Specifically, they extended the state-of-the-art object detector Faster R-CNN <cite class="ltx_cite ltx_citemacro_citep">(Ren et al., <a href="#bib.bib41" title="" class="ltx_ref">2015</a>)</cite> with an additional human-centric branch that uses the features extracted by the backbone to predict a score for candidate human-object pairs and an action class. <cite class="ltx_cite ltx_citemacro_citet">Liao et al. (<a href="#bib.bib28" title="" class="ltx_ref">2020</a>)</cite> proposed a method called <span id="S2.SS4.p1.9.3" class="ltx_text ltx_font_italic">PPDM</span> (Parallel Point Detection and Matching) that defines an HOI as a triplet <math id="S2.SS4.p1.3.m2.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S2.SS4.p1.3.m2.1a"><mo id="S2.SS4.p1.3.m2.1.1" xref="S2.SS4.p1.3.m2.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.3.m2.1b"><lt id="S2.SS4.p1.3.m2.1.1.cmml" xref="S2.SS4.p1.3.m2.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.3.m2.1c">&lt;</annotation></semantics></math><span id="S2.SS4.p1.4.2" class="ltx_text ltx_font_italic">human point, interaction point, object point<math id="S2.SS4.p1.4.2.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S2.SS4.p1.4.2.m1.1a"><mo id="S2.SS4.p1.4.2.m1.1.1" xref="S2.SS4.p1.4.2.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.4.2.m1.1b"><gt id="S2.SS4.p1.4.2.m1.1.1.cmml" xref="S2.SS4.p1.4.2.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.4.2.m1.1c">&gt;</annotation></semantics></math></span> composed of three points associated with the human, the active object and the interaction location. Recently, several works figured out the HOI detection task by proposing transformer-based models. <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a href="#bib.bib53" title="" class="ltx_ref">2022a</a>)</cite> proposed a new two-stage detector based on a transformer architecture to detect interactions. <cite class="ltx_cite ltx_citemacro_citet">Wu et al. (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite> proposed an approach for learning a body-part saliency map, which contains informative cues of the person involved in the interaction and other persons in the image, in order to boost HOI detection methods <cite class="ltx_cite ltx_citemacro_citep">(Chao et al., <a href="#bib.bib5" title="" class="ltx_ref">2018</a>; Gao et al., <a href="#bib.bib15" title="" class="ltx_ref">2018</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Ma et al. (<a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite> introduced a transformer-based human-object interaction detector that uses a multi-scale feature extractor and a multi-scale sampling strategy to predict the HOI instances from images with noisy backgrounds in the form of <math id="S2.SS4.p1.5.m3.1" class="ltx_math_unparsed" alttext="&lt;b_{h},b_{o},c_{o},c_{v}&gt;" display="inline"><semantics id="S2.SS4.p1.5.m3.1a"><mrow id="S2.SS4.p1.5.m3.1b"><mo id="S2.SS4.p1.5.m3.1.1">&lt;</mo><msub id="S2.SS4.p1.5.m3.1.2"><mi id="S2.SS4.p1.5.m3.1.2.2">b</mi><mi id="S2.SS4.p1.5.m3.1.2.3">h</mi></msub><mo id="S2.SS4.p1.5.m3.1.3">,</mo><msub id="S2.SS4.p1.5.m3.1.4"><mi id="S2.SS4.p1.5.m3.1.4.2">b</mi><mi id="S2.SS4.p1.5.m3.1.4.3">o</mi></msub><mo id="S2.SS4.p1.5.m3.1.5">,</mo><msub id="S2.SS4.p1.5.m3.1.6"><mi id="S2.SS4.p1.5.m3.1.6.2">c</mi><mi id="S2.SS4.p1.5.m3.1.6.3">o</mi></msub><mo id="S2.SS4.p1.5.m3.1.7">,</mo><msub id="S2.SS4.p1.5.m3.1.8"><mi id="S2.SS4.p1.5.m3.1.8.2">c</mi><mi id="S2.SS4.p1.5.m3.1.8.3">v</mi></msub><mo id="S2.SS4.p1.5.m3.1.9">&gt;</mo></mrow><annotation encoding="application/x-tex" id="S2.SS4.p1.5.m3.1c">&lt;b_{h},b_{o},c_{o},c_{v}&gt;</annotation></semantics></math> quadruplet, where <math id="S2.SS4.p1.6.m4.1" class="ltx_Math" alttext="b_{h}" display="inline"><semantics id="S2.SS4.p1.6.m4.1a"><msub id="S2.SS4.p1.6.m4.1.1" xref="S2.SS4.p1.6.m4.1.1.cmml"><mi id="S2.SS4.p1.6.m4.1.1.2" xref="S2.SS4.p1.6.m4.1.1.2.cmml">b</mi><mi id="S2.SS4.p1.6.m4.1.1.3" xref="S2.SS4.p1.6.m4.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.6.m4.1b"><apply id="S2.SS4.p1.6.m4.1.1.cmml" xref="S2.SS4.p1.6.m4.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.6.m4.1.1.1.cmml" xref="S2.SS4.p1.6.m4.1.1">subscript</csymbol><ci id="S2.SS4.p1.6.m4.1.1.2.cmml" xref="S2.SS4.p1.6.m4.1.1.2">𝑏</ci><ci id="S2.SS4.p1.6.m4.1.1.3.cmml" xref="S2.SS4.p1.6.m4.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.6.m4.1c">b_{h}</annotation></semantics></math> and <math id="S2.SS4.p1.7.m5.1" class="ltx_Math" alttext="b_{o}" display="inline"><semantics id="S2.SS4.p1.7.m5.1a"><msub id="S2.SS4.p1.7.m5.1.1" xref="S2.SS4.p1.7.m5.1.1.cmml"><mi id="S2.SS4.p1.7.m5.1.1.2" xref="S2.SS4.p1.7.m5.1.1.2.cmml">b</mi><mi id="S2.SS4.p1.7.m5.1.1.3" xref="S2.SS4.p1.7.m5.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.7.m5.1b"><apply id="S2.SS4.p1.7.m5.1.1.cmml" xref="S2.SS4.p1.7.m5.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.7.m5.1.1.1.cmml" xref="S2.SS4.p1.7.m5.1.1">subscript</csymbol><ci id="S2.SS4.p1.7.m5.1.1.2.cmml" xref="S2.SS4.p1.7.m5.1.1.2">𝑏</ci><ci id="S2.SS4.p1.7.m5.1.1.3.cmml" xref="S2.SS4.p1.7.m5.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.7.m5.1c">b_{o}</annotation></semantics></math> represent the human and object boxes, and <math id="S2.SS4.p1.8.m6.1" class="ltx_Math" alttext="c_{o}" display="inline"><semantics id="S2.SS4.p1.8.m6.1a"><msub id="S2.SS4.p1.8.m6.1.1" xref="S2.SS4.p1.8.m6.1.1.cmml"><mi id="S2.SS4.p1.8.m6.1.1.2" xref="S2.SS4.p1.8.m6.1.1.2.cmml">c</mi><mi id="S2.SS4.p1.8.m6.1.1.3" xref="S2.SS4.p1.8.m6.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.8.m6.1b"><apply id="S2.SS4.p1.8.m6.1.1.cmml" xref="S2.SS4.p1.8.m6.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.8.m6.1.1.1.cmml" xref="S2.SS4.p1.8.m6.1.1">subscript</csymbol><ci id="S2.SS4.p1.8.m6.1.1.2.cmml" xref="S2.SS4.p1.8.m6.1.1.2">𝑐</ci><ci id="S2.SS4.p1.8.m6.1.1.3.cmml" xref="S2.SS4.p1.8.m6.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.8.m6.1c">c_{o}</annotation></semantics></math> and <math id="S2.SS4.p1.9.m7.1" class="ltx_Math" alttext="c_{v}" display="inline"><semantics id="S2.SS4.p1.9.m7.1a"><msub id="S2.SS4.p1.9.m7.1.1" xref="S2.SS4.p1.9.m7.1.1.cmml"><mi id="S2.SS4.p1.9.m7.1.1.2" xref="S2.SS4.p1.9.m7.1.1.2.cmml">c</mi><mi id="S2.SS4.p1.9.m7.1.1.3" xref="S2.SS4.p1.9.m7.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.9.m7.1b"><apply id="S2.SS4.p1.9.m7.1.1.cmml" xref="S2.SS4.p1.9.m7.1.1"><csymbol cd="ambiguous" id="S2.SS4.p1.9.m7.1.1.1.cmml" xref="S2.SS4.p1.9.m7.1.1">subscript</csymbol><ci id="S2.SS4.p1.9.m7.1.1.2.cmml" xref="S2.SS4.p1.9.m7.1.1.2">𝑐</ci><ci id="S2.SS4.p1.9.m7.1.1.3.cmml" xref="S2.SS4.p1.9.m7.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.9.m7.1c">c_{v}</annotation></semantics></math> the object class and the verb class. While previous works all addressed the HOI modeling detecting a bounding box around the human, <cite class="ltx_cite ltx_citemacro_citet">Shan et al. (<a href="#bib.bib45" title="" class="ltx_ref">2020</a>)</cite> addressed the HOI detection task by predicting information about human hands, such as hand location, side, contact state, and, in case of an interaction, a box around the object touched by the hand. <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a href="#bib.bib54" title="" class="ltx_ref">2022b</a>)</cite> proposed to use a contact boundary, i.e. the contact region between the hand and the interacting object, to model the interaction relationship between hands and objects. <cite class="ltx_cite ltx_citemacro_citet">Fu et al. (<a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite> designed an approach for HOI detection that introduced a new pixel-wise voting function for improving the active object bounding box estimation. <cite class="ltx_cite ltx_citemacro_citet">Benavent-Lledo et al. (<a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite> proposed an architecture for human-object interaction detection estimation based on two YOLOv4 object detectors <cite class="ltx_cite ltx_citemacro_citep">(Bochkovskiy et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite> and an attention-based method. Recently, some work investigated the use of additional modalities, such as 6DOF hand poses or semantic segmentation masks, to learn more robust representations of human-object interactions. <cite class="ltx_cite ltx_citemacro_citet">Lu and Mayol-Cuevas (<a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite> introduced an approach that uses contextual information, i.e. hand pose, hand mask, and object mask, to improve the performance of HOI detection systems.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">In this work, we focused on detecting human-object interactions from FPV, where, in most cases, the hands are the only portion of the body visible in the images. To this aim, we designed an approach for detecting egocentric human-object interactions using different multimodal signals available within our <span id="S2.SS4.p2.1.1" class="ltx_text ltx_font_italic">EgoISM-HOI</span> dataset. Similar to <cite class="ltx_cite ltx_citemacro_citet">Shan et al. (<a href="#bib.bib45" title="" class="ltx_ref">2020</a>)</cite>, our method detects hands from RGB images using a two-stage object detector and predicts some attributes of the latter, such as hands side, hands contact state, and the objects involved in the interactions. Additionally, our approach is able to detect all objects present in the image and infer their category. Similar to <cite class="ltx_cite ltx_citemacro_citet">Lu and Mayol-Cuevas (<a href="#bib.bib32" title="" class="ltx_ref">2021</a>); Zhang et al. (<a href="#bib.bib54" title="" class="ltx_ref">2022b</a>)</cite>, we exploit multimodal signals (i.e., depth maps and hand segmentation masks) to predict the hand contact state.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed EHOI Generation Pipeline</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2306.12152/assets/imgs/fig_industrial_lab.jpg" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1920" height="937" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 2: </span>A picture of the ENIGMA Lab.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To study the egocentric human-object interaction detection task in a realistic industrial scenario, we have set up a laboratory called <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">ENIGMA Lab</span> (Figure <a href="#S3.F2" title="Fig. 2 ‣ 3 Proposed EHOI Generation Pipeline ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) that contains different types of work tools and equipment. Specifically, we considered the following 19 object categories: <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">power supply, oscilloscope, welder station, electric screwdriver, screwdriver, pliers, welder probe tip, oscilloscope probe tip, low voltage board, high voltage board, register, electric screwdriver battery, working area, welder base, socket, left red button, left green button, right red button</span>, and <span id="S3.p1.1.3" class="ltx_text ltx_font_italic">right green button</span>. Figure <a href="#S3.F3" title="Fig. 3 ‣ 3 Proposed EHOI Generation Pipeline ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the acquired 3D models of all the objects considered for the experiments. Note that the categories <span id="S3.p1.1.4" class="ltx_text ltx_font_italic">left red button, left green button, right red button</span>, and <span id="S3.p1.1.5" class="ltx_text ltx_font_italic">right green button</span>, refer to each button of the electrical panel shown in the bottom-left corner of Figure <a href="#S3.F3" title="Fig. 3 ‣ 3 Proposed EHOI Generation Pipeline ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2306.12152/assets/imgs/fig_object_models.jpg" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="3592" height="3591" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 3: </span>3D models of the 19 objects considered for the experiments.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">We propose a pipeline for generating and labeling synthetic human-object interactions from a first-person point of view using 3D models of the target environment and objects, which can be cheaply collected using commercial scanners. Figure <a href="#S1.F1" title="Fig. 1 ‣ 1 Introduction ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the overall scheme of our EHOI data generation pipeline, which consists of two main phases: 1) the collection of the 3D models, and 2) the generation of EHOI synthetic images using the proposed tool.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">In our study, we noted that high-quality object reconstructions are necessary to generate realistic EHOIs, while high accuracy is not required for environment reconstruction. We used two different 3D scanners to create 3D models. Specifically, we used the structured-light 3D scanner <span id="S3.p3.1.1" class="ltx_text ltx_font_italic">Artec Eva<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span id="footnote2.1.1.1" class="ltx_text ltx_font_upright">2</span></span><a target="_blank" href="https://www.artec3d.com/portable-3d-scanners/artec-eva-v2" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_upright">https://www.artec3d.com/portable-3d-scanners/artec-eva-v2</a></span></span></span></span> for scanning the objects, and a <span id="S3.p3.1.2" class="ltx_text ltx_font_italic">MatterPort<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note"><span id="footnote3.1.1.1" class="ltx_text ltx_font_upright">3</span></span><a target="_blank" href="https://matterport.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_upright">https://matterport.com/</a></span></span></span></span> device for the environment.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">We developed a tool based on the Unity<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://unity.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://unity.com/</a></span></span></span> engine which exploits 3D models of the objects and the environment to generate synthetic egocentric human-object interaction images together with the following data: 1) RGB images (see Fig. <a href="#S3.F4" title="Fig. 4 ‣ 3 Proposed EHOI Generation Pipeline ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> - left), 2) depth maps (see Fig. <a href="#S3.F4" title="Fig. 4 ‣ 3 Proposed EHOI Generation Pipeline ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> - right), 3) instance segmentation masks (see Fig. <a href="#S3.F4" title="Fig. 4 ‣ 3 Proposed EHOI Generation Pipeline ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> - center), 4) bounding boxes for hands and objects including the object categories, 5) EHOI’s metadata, such as information about associations between hands and objects in contact (which hand is in contact with which object), and hand attributes (i.e., hand side, and hand contact state).</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2306.12152/assets/imgs/fig_egoism_synth_anns.jpg" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="3959" height="2968" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 4: </span>Examples of synthetic images (left) with the corresponding annotations (center) and depth maps (right) generated with the proposed tool.</figcaption>
</figure>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">Our system exploits the <span id="S3.p5.1.1" class="ltx_text ltx_font_italic">Unity Perception package</span> <cite class="ltx_cite ltx_citemacro_citep">(Unity Technologies, <a href="#bib.bib47" title="" class="ltx_ref">2020</a>)</cite>, which offers different tools for generating large-scale synthetic datasets. This package allows to randomize some aspects of the virtual scene, such as the intensity and the color of the lights, the object textures, the presence and amount of motion blur, as well as visual effects like noise, to make the virtual scene more realistic, and adds further diversity to the generated dataset, making it more representative of the real-world environment. In addition, to include different randomized aspects, we created the following randomizers:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">SurfaceObjectPlacementRandomizer</span>: Randomizes the position of a group of objects on a flat surface;</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">CustomRotationRandomizer</span>: Randomizes object rotation by respecting the constraints of each rotation axis;</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">PlayerPlacementRandomizer</span>: Randomizes the location of the virtual agent in the environment;</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">TextureShirtRandomizer</span>: Randomizes the texture and color of the virtual agent’s shirt;</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p"><span id="S3.I1.i5.p1.1.1" class="ltx_text ltx_font_italic">CameraRandomizer</span>: Randomizes the observed point of the FPV camera;</p>
</div>
</li>
</ul>
<p id="S3.p5.2" class="ltx_p">Examples of randomization are shown in Figure <a href="#S3.F5" title="Fig. 5 ‣ 3 Proposed EHOI Generation Pipeline ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2306.12152/assets/imgs/fig_examples_randomization.jpg" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1647" height="941" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 5: </span>Our tool is able to randomize different aspects of the virtual scene, such as the camera and user positions or the shirt’s texture and color.</figcaption>
</figure>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">The Unity perception package provides a component called <span id="S3.p6.1.1" class="ltx_text ltx_font_italic">Scenario</span> which allows to control the execution flow of the simulation by setting standard simulation parameters, such as the number of iterations, the seed of the randomizers, and the number of frames to acquire for each iteration. We have extended the basic <span id="S3.p6.1.2" class="ltx_text ltx_font_italic">Scenario</span> by adding the following parameters: 1) the probability that an interaction will occur in the current iteration, 2) the target object with which the virtual agent will interact in the current interaction (chosen randomly from a list of objects), 3) the probability that two hands are visible from the camera at the same time, and 4) the hand that will interact with the object (right or left).</p>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p">Moreover, we used a Unity asset called <span id="S3.p7.1.1" class="ltx_text ltx_font_italic">Auto Hand - VR Physics Interaction<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note"><span id="footnote5.1.1.1" class="ltx_text ltx_font_upright">5</span></span><a target="_blank" href="https://assetstore.unity.com/packages/tools/game-toolkits/auto-hand-vr-physics-interaction-165323" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_upright">https://assetstore.unity.com/packages/tools/game-toolkits/auto-hand-vr-physics-interaction-165323</a></span></span></span></span> to improve the physics of the agent when it interacts with the objects. This asset provides a Virtual Reality (VR) interaction system that automatically determines an appropriate hand pose during object manipulation. We have integrated this system into our virtual agent by extending it to automate the grabbing process and adding special types of interactions, such as pressing buttons. Examples of the generated images and poses are reported in Figure <a href="#S3.F4" title="Fig. 4 ‣ 3 Proposed EHOI Generation Pipeline ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>EgoISM-HOI dataset</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We present a new multimodal dataset of EHOIs in the aforementioned industrial scenario called <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">EgoISM-HOI</span>. It is composed of two parts: 1) a generated synthetic set of images, and 2) a real-world set of data. Henceforth, we will refer to the synthetic set as <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">EgoISM-HOI-Synth</span>, whereas we refer to the real-world data as <span id="S4.p1.1.3" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span>.</p>
</div>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">EgoISM-HOI-Synth</h5>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">We adopted our EHOI generation pipeline to generate <span id="S4.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">EgoISM-HOI-Synth</span>. It contains a total of 23,356 images with associated depth maps and instance segmentation masks, 35,672 hand instances of which 18,884 are involved in an interaction, and 148,024 object instances across the 19 object categories reported in Figure <a href="#S3.F3" title="Fig. 3 ‣ 3 Proposed EHOI Generation Pipeline ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Examples of the data which composes the dataset are reported in Figure <a href="#S3.F4" title="Fig. 4 ‣ 3 Proposed EHOI Generation Pipeline ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, while Table <a href="#S4.T1" title="Table 1 ‣ EgoISM-HOI-Synth ‣ 4 EgoISM-HOI dataset ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> reports statistics about the dataset, including the total number of images, hands, objects, and EHOIs.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Statistics of <span id="S4.T1.2.1" class="ltx_text ltx_font_italic">EgoISM-HOI-Synth</span>.</figcaption>
<div id="S4.T1.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:84.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(32.0pt,-6.2pt) scale(1.17344155738511,1.17344155738511) ;">
<table id="S4.T1.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.3.1.1.1" class="ltx_tr">
<th id="S4.T1.3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Set</th>
<th id="S4.T1.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#images</th>
<th id="S4.T1.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#hands</th>
<th id="S4.T1.3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#EHOIs</th>
<th id="S4.T1.3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#left hands</th>
<th id="S4.T1.3.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#right hands</th>
<th id="S4.T1.3.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#objects</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.3.1.2.1" class="ltx_tr">
<th id="S4.T1.3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Train</th>
<td id="S4.T1.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">20,788</td>
<td id="S4.T1.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">31,790</td>
<td id="S4.T1.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">16,786</td>
<td id="S4.T1.3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">16,019</td>
<td id="S4.T1.3.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">15,771</td>
<td id="S4.T1.3.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">131,968</td>
</tr>
<tr id="S4.T1.3.1.3.2" class="ltx_tr">
<th id="S4.T1.3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Val</th>
<td id="S4.T1.3.1.3.2.2" class="ltx_td ltx_align_center">2,568</td>
<td id="S4.T1.3.1.3.2.3" class="ltx_td ltx_align_center">3,912</td>
<td id="S4.T1.3.1.3.2.4" class="ltx_td ltx_align_center">2,098</td>
<td id="S4.T1.3.1.3.2.5" class="ltx_td ltx_align_center">1,989</td>
<td id="S4.T1.3.1.3.2.6" class="ltx_td ltx_align_center">1,923</td>
<td id="S4.T1.3.1.3.2.7" class="ltx_td ltx_align_center">16,056</td>
</tr>
<tr id="S4.T1.3.1.4.3" class="ltx_tr">
<th id="S4.T1.3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">Total</th>
<td id="S4.T1.3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b">23,356</td>
<td id="S4.T1.3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b">35,672</td>
<td id="S4.T1.3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_b">18,884</td>
<td id="S4.T1.3.1.4.3.5" class="ltx_td ltx_align_center ltx_border_b">18,008</td>
<td id="S4.T1.3.1.4.3.6" class="ltx_td ltx_align_center ltx_border_b">17,694</td>
<td id="S4.T1.3.1.4.3.7" class="ltx_td ltx_align_center ltx_border_b">148,024</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">EgoISM-HOI-Real</h5>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p">For <span id="S4.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span>, we collected and labeled 42 real egocentric videos in the ENIGMA Laboratory. In these videos, subjects performed testing and repairing operations on electrical boards using laboratory tools. To simplify data collection and to make it more consistent, we developed an application for Microsoft Hololens 2<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://www.microsoft.com/hololens" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.microsoft.com/hololens</a></span></span></span> that guides the users through audio and images, suggesting the next steps to perform during the acquisition. We defined 8 procedures composed of several steps, in which we vary the tools and electrical boards interacted by the users. Nineteen subjects participated in the data collection. Two were women and seventeen were men. For privacy reasons, we made sure that no other people are visible in the videos, and all the subjects removed any personal object that might reveal their identities (e.g., rings or wristwatches). We acquired 18 hours, 48 minutes, and 13 seconds of video recordings, with an average duration of 26 minutes and 51 seconds, at a resolution of 2272x1278 pixels and a framerate of 30fps. Table <a href="#S4.T2" title="Table 2 ‣ EgoISM-HOI-Real ‣ 4 EgoISM-HOI dataset ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes statistics about the collected data.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Statistics of <span id="S4.T2.2.1" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> data. Since we mainly want to use synthetic data to train models, we used most of the real-world data for testing.</figcaption>
<div id="S4.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:59.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-113.4pt,15.5pt) scale(0.656639593322165,0.656639593322165) ;">
<table id="S4.T2.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.3.1.1.1" class="ltx_tr">
<th id="S4.T2.3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Set</th>
<th id="S4.T2.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#videos</th>
<th id="S4.T2.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#subjects</th>
<th id="S4.T2.3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#procedures</th>
<th id="S4.T2.3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">cumulative videos length</th>
<th id="S4.T2.3.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#images</th>
<th id="S4.T2.3.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#hands</th>
<th id="S4.T2.3.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#EHOIs</th>
<th id="S4.T2.3.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#left hands</th>
<th id="S4.T2.3.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#right hands</th>
<th id="S4.T2.3.1.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">#objects</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.3.1.2.1" class="ltx_tr">
<td id="S4.T2.3.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">Train</td>
<td id="S4.T2.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">2</td>
<td id="S4.T2.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="S4.T2.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">2</td>
<td id="S4.T2.3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">1h:00m:52s</td>
<td id="S4.T2.3.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">1,010</td>
<td id="S4.T2.3.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">1,686</td>
<td id="S4.T2.3.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t">1,262</td>
<td id="S4.T2.3.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t">758</td>
<td id="S4.T2.3.1.2.1.10" class="ltx_td ltx_align_center ltx_border_t">928</td>
<td id="S4.T2.3.1.2.1.11" class="ltx_td ltx_align_center ltx_border_t">6,689</td>
</tr>
<tr id="S4.T2.3.1.3.2" class="ltx_tr">
<td id="S4.T2.3.1.3.2.1" class="ltx_td ltx_align_left">Val</td>
<td id="S4.T2.3.1.3.2.2" class="ltx_td ltx_align_center">10</td>
<td id="S4.T2.3.1.3.2.3" class="ltx_td ltx_align_center">7</td>
<td id="S4.T2.3.1.3.2.4" class="ltx_td ltx_align_center">6</td>
<td id="S4.T2.3.1.3.2.5" class="ltx_td ltx_align_center">4h:35m:28s</td>
<td id="S4.T2.3.1.3.2.6" class="ltx_td ltx_align_center">3,717</td>
<td id="S4.T2.3.1.3.2.7" class="ltx_td ltx_align_center">5,622</td>
<td id="S4.T2.3.1.3.2.8" class="ltx_td ltx_align_center">3,867</td>
<td id="S4.T2.3.1.3.2.9" class="ltx_td ltx_align_center">2,577</td>
<td id="S4.T2.3.1.3.2.10" class="ltx_td ltx_align_center">3,045</td>
<td id="S4.T2.3.1.3.2.11" class="ltx_td ltx_align_center">20,916</td>
</tr>
<tr id="S4.T2.3.1.4.3" class="ltx_tr">
<td id="S4.T2.3.1.4.3.1" class="ltx_td ltx_align_left">Test</td>
<td id="S4.T2.3.1.4.3.2" class="ltx_td ltx_align_center">30</td>
<td id="S4.T2.3.1.4.3.3" class="ltx_td ltx_align_center">15</td>
<td id="S4.T2.3.1.4.3.4" class="ltx_td ltx_align_center">8</td>
<td id="S4.T2.3.1.4.3.5" class="ltx_td ltx_align_center">13h:11m:51s</td>
<td id="S4.T2.3.1.4.3.6" class="ltx_td ltx_align_center">11,221</td>
<td id="S4.T2.3.1.4.3.7" class="ltx_td ltx_align_center">16,850</td>
<td id="S4.T2.3.1.4.3.8" class="ltx_td ltx_align_center">11,403</td>
<td id="S4.T2.3.1.4.3.9" class="ltx_td ltx_align_center">7,743</td>
<td id="S4.T2.3.1.4.3.10" class="ltx_td ltx_align_center">9,107</td>
<td id="S4.T2.3.1.4.3.11" class="ltx_td ltx_align_center">62,356</td>
</tr>
<tr id="S4.T2.3.1.5.4" class="ltx_tr">
<td id="S4.T2.3.1.5.4.1" class="ltx_td ltx_align_left ltx_border_b">Total</td>
<td id="S4.T2.3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b">42</td>
<td id="S4.T2.3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b">19</td>
<td id="S4.T2.3.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b">8</td>
<td id="S4.T2.3.1.5.4.5" class="ltx_td ltx_align_center ltx_border_b">18h:48m:13s</td>
<td id="S4.T2.3.1.5.4.6" class="ltx_td ltx_align_center ltx_border_b">15,948</td>
<td id="S4.T2.3.1.5.4.7" class="ltx_td ltx_align_center ltx_border_b">24,158</td>
<td id="S4.T2.3.1.5.4.8" class="ltx_td ltx_align_center ltx_border_b">16,532</td>
<td id="S4.T2.3.1.5.4.9" class="ltx_td ltx_align_center ltx_border_b">11,078</td>
<td id="S4.T2.3.1.5.4.10" class="ltx_td ltx_align_center ltx_border_b">13,080</td>
<td id="S4.T2.3.1.5.4.11" class="ltx_td ltx_align_center ltx_border_b">89,961</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p2.1" class="ltx_p">From these videos, we manually annotated 15,948 images following this strategy: 1) we annotated the first frame in which the hand touches an object (i.e., contact frame), and 2) we annotated the first frame after the hand released the object (i.e., end of contact frame). Finally, we assigned the following attributes: 1) hands and objects bounding boxes, 2) hand side (Left/Right), 3) hand contact state (Contact/No contact), 4) hand-object relationships (e.g., hand <span id="S4.SS0.SSS0.Px2.p2.1.1" class="ltx_text ltx_font_italic">x</span> touches object <span id="S4.SS0.SSS0.Px2.p2.1.2" class="ltx_text ltx_font_italic">y</span>), and 5) object categories. Figure <a href="#S4.F6" title="Fig. 6 ‣ EgoISM-HOI-Real ‣ 4 EgoISM-HOI dataset ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows some images from this set of data along with the related annotations.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2306.12152/assets/imgs/fig_examples_real.jpg" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="2904" height="1631" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 6: </span>Examples of <span id="S4.F6.2.1" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> images with the corresponding EHOI annotations.</figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2306.12152/assets/x2.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="400" height="104" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 7: </span>Overall architecture of the proposed Multimodal EHOI detection system. First, the <span id="S4.F7.11.1" class="ltx_text ltx_font_italic">backbone</span> extracts image features from the input RGB image. Then, the <span id="S4.F7.12.2" class="ltx_text ltx_font_italic">object detector branch</span> and the <span id="S4.F7.13.3" class="ltx_text ltx_font_italic">instance segmentation branch</span> detect and generate segmentation masks for all hands and objects in the image. At the same time, the <span id="S4.F7.14.4" class="ltx_text ltx_font_italic">monocular depth estimation branch</span> predicts a depth map of the scene. Next, the hand feature vectors obtained through <span id="S4.F7.15.5" class="ltx_text ltx_font_italic">RoI Pooling</span> are sent to the following modules for predicting hand attributes: 1) the <span id="S4.F7.16.6" class="ltx_text ltx_font_italic">hand side classifier</span>, 2) <span id="S4.F7.17.7" class="ltx_text ltx_font_italic">hand state classifier</span>, and 3) <span id="S4.F7.18.8" class="ltx_text ltx_font_italic">offset vector regressor</span>. Simultaneously, the RGB image, depth map, and instance segmentation mask of each hand are combined and passed to the <span id="S4.F7.19.9" class="ltx_text ltx_font_italic">multimodal hand state classifier</span> module to predict the hand contact state. Finally, the outputs from the previous components are combined and passed to a <span id="S4.F7.20.10" class="ltx_text ltx_font_italic">matching algorithm</span> to predict EHOIs.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Proposed approach</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Inspired by <cite class="ltx_cite ltx_citemacro_citet">Shan et al. (<a href="#bib.bib45" title="" class="ltx_ref">2020</a>)</cite>, our system extends a two-stage object detector with additional modules specialized to recognize human-object interactions. The proposed method is able to exploit different data signals, such as instance segmentation maps and depth maps, to improve the performance of classic HOI detection approaches. Moreover, our method is able to recognize the class of all the objects in the scene. We believe that this knowledge could be used for other downstream tasks.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Figure <a href="#S4.F7" title="Fig. 7 ‣ EgoISM-HOI-Real ‣ 4 EgoISM-HOI dataset ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows a diagram of the overall architecture of the method. Firstly, the input RGB image is passed to the <span id="S5.p2.1.1" class="ltx_text ltx_font_italic">backbone</span> component to extract the image features. These features are used by the <span id="S5.p2.1.2" class="ltx_text ltx_font_italic">object detector branch</span> and the <span id="S5.p2.1.3" class="ltx_text ltx_font_italic">instance segmentation branch</span> to detect, recognize and generate segmentation masks of all the objects and hands in the image. Simultaneously, the <span id="S5.p2.1.4" class="ltx_text ltx_font_italic">monocular depth estimation branch</span> predicts a depth map of the scene from the RGB image. Then, using the hand boxes predicted by the <span id="S5.p2.1.5" class="ltx_text ltx_font_italic">object detector branch</span> and the features map produced by the backbone, the hand feature vectors are extracted with <span id="S5.p2.1.6" class="ltx_text ltx_font_italic">RoI pooling</span> and sent to the following modules: 1) the <span id="S5.p2.1.7" class="ltx_text ltx_font_italic">hand side classifier</span>, 2) <span id="S5.p2.1.8" class="ltx_text ltx_font_italic">hand state classifier</span>, and 3) <span id="S5.p2.1.9" class="ltx_text ltx_font_italic">offset vector regressor</span>. These modules predict several hand attributes that will be detailed later. Furthermore, the RGB image, the depth map, and the instance segmentation mask of each hand are combined using an early fusion strategy and passed to the <span id="S5.p2.1.10" class="ltx_text ltx_font_italic">multimodal hand state classifier</span> component to predict the hand contact state. As the last step, the resulting outputs of the previous modules are combined and passed to a <span id="S5.p2.1.11" class="ltx_text ltx_font_italic">matching algorithm</span> to predict EHOIs in the form of <span id="S5.p2.1.12" class="ltx_text ltx_font_italic">&lt;hand, contact state, active object&gt;</span> triplets. The various modules composing our system are described in detail in the following.</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2306.12152/assets/x3.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="393" height="187" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 8: </span>Comparison of the depth maps predicted by our <span id="S5.F8.2.1" class="ltx_text ltx_font_italic">monocular depth estimation branch</span>. The first row shows RGB video frames, while the second and third rows contain depth maps predicted by two different models fine-tuned, respectively, by using the losses described in <cite class="ltx_cite ltx_citemacro_citet">Ranftl et al. (<a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite> and the proposed one in Equation (<a href="#S5.E1" title="In monocular depth estimation branch ‣ 5 Proposed approach ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). The results of the third row are more uniform, while the predicted depth values of the second row vary considerably between similar frames (e.g., the background of (3) and (4) or the object in contact with the left-hand of (1) and (2)).</figcaption>
</figure>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">backbone</h5>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">This component consists of a ResNet-101 backbone <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib21" title="" class="ltx_ref">2016a</a>)</cite> with a Feature Pyramid Network (FPN) <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib29" title="" class="ltx_ref">2017</a>)</cite>. It takes an RGB image as input and returns a feature map.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">object detector branch</h5>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">We used Faster-RCNN <cite class="ltx_cite ltx_citemacro_citep">(Ren et al., <a href="#bib.bib41" title="" class="ltx_ref">2015</a>)</cite><span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>We used the following implementation: <a target="_blank" href="https://github.com/facebookresearch/detectron2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/detectron2</a></span></span></span>, which uses two branches that take as input the features extracted by a backbone to detect and recognize objects and hands in the image.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">instance segmentation branch</h5>

<div id="S5.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p1.1" class="ltx_p">We followed Mask-RCNN <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite> and add a branch to predict instance segmentation masks from the features extracted by a backbone.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">monocular depth estimation branch</h5>

<div id="S5.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px4.p1.13" class="ltx_p">We used the system presented in <cite class="ltx_cite ltx_citemacro_citep">(Ranftl et al., <a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite>, called <span id="S5.SS0.SSS0.Px4.p1.13.1" class="ltx_text ltx_font_italic">MiDaS</span>, to build the monocular depth estimation branch. Given a single RGB image as input, this component estimates the 3D distance to the camera of each pixel. To make the prediction scale of the depth values uniform in our domain, we fine-tuned <span id="S5.SS0.SSS0.Px4.p1.13.2" class="ltx_text ltx_font_italic">MiDaS<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note"><span id="footnote8.1.1.1" class="ltx_text ltx_font_upright">8</span></span><span id="footnote8.5" class="ltx_text ltx_font_upright">We used the model </span><span id="footnote8.6" class="ltx_text">midas_v21_384</span><span id="footnote8.7" class="ltx_text ltx_font_upright"> available in the following repository: </span><a target="_blank" href="https://github.com/isl-org/MiDaS" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_upright">https://github.com/isl-org/MiDaS</a></span></span></span></span> redefining the loss function as follows:</p>
<table id="S5.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S5.E1.m1.8" class="ltx_Math" alttext="\mathcal{L}_{depth}(d,d^{*})=\alpha\mathcal{L}_{ssim}(e,e^{*})+\beta\mathcal{L}_{ssim}(d,d^{*})+\gamma\mathcal{L}_{l1}(d,d^{*})" display="block"><semantics id="S5.E1.m1.8a"><mrow id="S5.E1.m1.8.8" xref="S5.E1.m1.8.8.cmml"><mrow id="S5.E1.m1.5.5.1" xref="S5.E1.m1.5.5.1.cmml"><msub id="S5.E1.m1.5.5.1.3" xref="S5.E1.m1.5.5.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.E1.m1.5.5.1.3.2" xref="S5.E1.m1.5.5.1.3.2.cmml">ℒ</mi><mrow id="S5.E1.m1.5.5.1.3.3" xref="S5.E1.m1.5.5.1.3.3.cmml"><mi id="S5.E1.m1.5.5.1.3.3.2" xref="S5.E1.m1.5.5.1.3.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.5.5.1.3.3.1" xref="S5.E1.m1.5.5.1.3.3.1.cmml">​</mo><mi id="S5.E1.m1.5.5.1.3.3.3" xref="S5.E1.m1.5.5.1.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.5.5.1.3.3.1a" xref="S5.E1.m1.5.5.1.3.3.1.cmml">​</mo><mi id="S5.E1.m1.5.5.1.3.3.4" xref="S5.E1.m1.5.5.1.3.3.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.5.5.1.3.3.1b" xref="S5.E1.m1.5.5.1.3.3.1.cmml">​</mo><mi id="S5.E1.m1.5.5.1.3.3.5" xref="S5.E1.m1.5.5.1.3.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.5.5.1.3.3.1c" xref="S5.E1.m1.5.5.1.3.3.1.cmml">​</mo><mi id="S5.E1.m1.5.5.1.3.3.6" xref="S5.E1.m1.5.5.1.3.3.6.cmml">h</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S5.E1.m1.5.5.1.2" xref="S5.E1.m1.5.5.1.2.cmml">​</mo><mrow id="S5.E1.m1.5.5.1.1.1" xref="S5.E1.m1.5.5.1.1.2.cmml"><mo stretchy="false" id="S5.E1.m1.5.5.1.1.1.2" xref="S5.E1.m1.5.5.1.1.2.cmml">(</mo><mi id="S5.E1.m1.1.1" xref="S5.E1.m1.1.1.cmml">d</mi><mo id="S5.E1.m1.5.5.1.1.1.3" xref="S5.E1.m1.5.5.1.1.2.cmml">,</mo><msup id="S5.E1.m1.5.5.1.1.1.1" xref="S5.E1.m1.5.5.1.1.1.1.cmml"><mi id="S5.E1.m1.5.5.1.1.1.1.2" xref="S5.E1.m1.5.5.1.1.1.1.2.cmml">d</mi><mo id="S5.E1.m1.5.5.1.1.1.1.3" xref="S5.E1.m1.5.5.1.1.1.1.3.cmml">∗</mo></msup><mo stretchy="false" id="S5.E1.m1.5.5.1.1.1.4" xref="S5.E1.m1.5.5.1.1.2.cmml">)</mo></mrow></mrow><mo id="S5.E1.m1.8.8.5" xref="S5.E1.m1.8.8.5.cmml">=</mo><mrow id="S5.E1.m1.8.8.4" xref="S5.E1.m1.8.8.4.cmml"><mrow id="S5.E1.m1.6.6.2.1" xref="S5.E1.m1.6.6.2.1.cmml"><mi id="S5.E1.m1.6.6.2.1.3" xref="S5.E1.m1.6.6.2.1.3.cmml">α</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.6.6.2.1.2" xref="S5.E1.m1.6.6.2.1.2.cmml">​</mo><msub id="S5.E1.m1.6.6.2.1.4" xref="S5.E1.m1.6.6.2.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.E1.m1.6.6.2.1.4.2" xref="S5.E1.m1.6.6.2.1.4.2.cmml">ℒ</mi><mrow id="S5.E1.m1.6.6.2.1.4.3" xref="S5.E1.m1.6.6.2.1.4.3.cmml"><mi id="S5.E1.m1.6.6.2.1.4.3.2" xref="S5.E1.m1.6.6.2.1.4.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.6.6.2.1.4.3.1" xref="S5.E1.m1.6.6.2.1.4.3.1.cmml">​</mo><mi id="S5.E1.m1.6.6.2.1.4.3.3" xref="S5.E1.m1.6.6.2.1.4.3.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.6.6.2.1.4.3.1a" xref="S5.E1.m1.6.6.2.1.4.3.1.cmml">​</mo><mi id="S5.E1.m1.6.6.2.1.4.3.4" xref="S5.E1.m1.6.6.2.1.4.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.6.6.2.1.4.3.1b" xref="S5.E1.m1.6.6.2.1.4.3.1.cmml">​</mo><mi id="S5.E1.m1.6.6.2.1.4.3.5" xref="S5.E1.m1.6.6.2.1.4.3.5.cmml">m</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S5.E1.m1.6.6.2.1.2a" xref="S5.E1.m1.6.6.2.1.2.cmml">​</mo><mrow id="S5.E1.m1.6.6.2.1.1.1" xref="S5.E1.m1.6.6.2.1.1.2.cmml"><mo stretchy="false" id="S5.E1.m1.6.6.2.1.1.1.2" xref="S5.E1.m1.6.6.2.1.1.2.cmml">(</mo><mi id="S5.E1.m1.2.2" xref="S5.E1.m1.2.2.cmml">e</mi><mo id="S5.E1.m1.6.6.2.1.1.1.3" xref="S5.E1.m1.6.6.2.1.1.2.cmml">,</mo><msup id="S5.E1.m1.6.6.2.1.1.1.1" xref="S5.E1.m1.6.6.2.1.1.1.1.cmml"><mi id="S5.E1.m1.6.6.2.1.1.1.1.2" xref="S5.E1.m1.6.6.2.1.1.1.1.2.cmml">e</mi><mo id="S5.E1.m1.6.6.2.1.1.1.1.3" xref="S5.E1.m1.6.6.2.1.1.1.1.3.cmml">∗</mo></msup><mo stretchy="false" id="S5.E1.m1.6.6.2.1.1.1.4" xref="S5.E1.m1.6.6.2.1.1.2.cmml">)</mo></mrow></mrow><mo id="S5.E1.m1.8.8.4.4" xref="S5.E1.m1.8.8.4.4.cmml">+</mo><mrow id="S5.E1.m1.7.7.3.2" xref="S5.E1.m1.7.7.3.2.cmml"><mi id="S5.E1.m1.7.7.3.2.3" xref="S5.E1.m1.7.7.3.2.3.cmml">β</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.7.7.3.2.2" xref="S5.E1.m1.7.7.3.2.2.cmml">​</mo><msub id="S5.E1.m1.7.7.3.2.4" xref="S5.E1.m1.7.7.3.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.E1.m1.7.7.3.2.4.2" xref="S5.E1.m1.7.7.3.2.4.2.cmml">ℒ</mi><mrow id="S5.E1.m1.7.7.3.2.4.3" xref="S5.E1.m1.7.7.3.2.4.3.cmml"><mi id="S5.E1.m1.7.7.3.2.4.3.2" xref="S5.E1.m1.7.7.3.2.4.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.7.7.3.2.4.3.1" xref="S5.E1.m1.7.7.3.2.4.3.1.cmml">​</mo><mi id="S5.E1.m1.7.7.3.2.4.3.3" xref="S5.E1.m1.7.7.3.2.4.3.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.7.7.3.2.4.3.1a" xref="S5.E1.m1.7.7.3.2.4.3.1.cmml">​</mo><mi id="S5.E1.m1.7.7.3.2.4.3.4" xref="S5.E1.m1.7.7.3.2.4.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.7.7.3.2.4.3.1b" xref="S5.E1.m1.7.7.3.2.4.3.1.cmml">​</mo><mi id="S5.E1.m1.7.7.3.2.4.3.5" xref="S5.E1.m1.7.7.3.2.4.3.5.cmml">m</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S5.E1.m1.7.7.3.2.2a" xref="S5.E1.m1.7.7.3.2.2.cmml">​</mo><mrow id="S5.E1.m1.7.7.3.2.1.1" xref="S5.E1.m1.7.7.3.2.1.2.cmml"><mo stretchy="false" id="S5.E1.m1.7.7.3.2.1.1.2" xref="S5.E1.m1.7.7.3.2.1.2.cmml">(</mo><mi id="S5.E1.m1.3.3" xref="S5.E1.m1.3.3.cmml">d</mi><mo id="S5.E1.m1.7.7.3.2.1.1.3" xref="S5.E1.m1.7.7.3.2.1.2.cmml">,</mo><msup id="S5.E1.m1.7.7.3.2.1.1.1" xref="S5.E1.m1.7.7.3.2.1.1.1.cmml"><mi id="S5.E1.m1.7.7.3.2.1.1.1.2" xref="S5.E1.m1.7.7.3.2.1.1.1.2.cmml">d</mi><mo id="S5.E1.m1.7.7.3.2.1.1.1.3" xref="S5.E1.m1.7.7.3.2.1.1.1.3.cmml">∗</mo></msup><mo stretchy="false" id="S5.E1.m1.7.7.3.2.1.1.4" xref="S5.E1.m1.7.7.3.2.1.2.cmml">)</mo></mrow></mrow><mo id="S5.E1.m1.8.8.4.4a" xref="S5.E1.m1.8.8.4.4.cmml">+</mo><mrow id="S5.E1.m1.8.8.4.3" xref="S5.E1.m1.8.8.4.3.cmml"><mi id="S5.E1.m1.8.8.4.3.3" xref="S5.E1.m1.8.8.4.3.3.cmml">γ</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.8.8.4.3.2" xref="S5.E1.m1.8.8.4.3.2.cmml">​</mo><msub id="S5.E1.m1.8.8.4.3.4" xref="S5.E1.m1.8.8.4.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.E1.m1.8.8.4.3.4.2" xref="S5.E1.m1.8.8.4.3.4.2.cmml">ℒ</mi><mrow id="S5.E1.m1.8.8.4.3.4.3" xref="S5.E1.m1.8.8.4.3.4.3.cmml"><mi id="S5.E1.m1.8.8.4.3.4.3.2" xref="S5.E1.m1.8.8.4.3.4.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.E1.m1.8.8.4.3.4.3.1" xref="S5.E1.m1.8.8.4.3.4.3.1.cmml">​</mo><mn id="S5.E1.m1.8.8.4.3.4.3.3" xref="S5.E1.m1.8.8.4.3.4.3.3.cmml">1</mn></mrow></msub><mo lspace="0em" rspace="0em" id="S5.E1.m1.8.8.4.3.2a" xref="S5.E1.m1.8.8.4.3.2.cmml">​</mo><mrow id="S5.E1.m1.8.8.4.3.1.1" xref="S5.E1.m1.8.8.4.3.1.2.cmml"><mo stretchy="false" id="S5.E1.m1.8.8.4.3.1.1.2" xref="S5.E1.m1.8.8.4.3.1.2.cmml">(</mo><mi id="S5.E1.m1.4.4" xref="S5.E1.m1.4.4.cmml">d</mi><mo id="S5.E1.m1.8.8.4.3.1.1.3" xref="S5.E1.m1.8.8.4.3.1.2.cmml">,</mo><msup id="S5.E1.m1.8.8.4.3.1.1.1" xref="S5.E1.m1.8.8.4.3.1.1.1.cmml"><mi id="S5.E1.m1.8.8.4.3.1.1.1.2" xref="S5.E1.m1.8.8.4.3.1.1.1.2.cmml">d</mi><mo id="S5.E1.m1.8.8.4.3.1.1.1.3" xref="S5.E1.m1.8.8.4.3.1.1.1.3.cmml">∗</mo></msup><mo stretchy="false" id="S5.E1.m1.8.8.4.3.1.1.4" xref="S5.E1.m1.8.8.4.3.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E1.m1.8b"><apply id="S5.E1.m1.8.8.cmml" xref="S5.E1.m1.8.8"><eq id="S5.E1.m1.8.8.5.cmml" xref="S5.E1.m1.8.8.5"></eq><apply id="S5.E1.m1.5.5.1.cmml" xref="S5.E1.m1.5.5.1"><times id="S5.E1.m1.5.5.1.2.cmml" xref="S5.E1.m1.5.5.1.2"></times><apply id="S5.E1.m1.5.5.1.3.cmml" xref="S5.E1.m1.5.5.1.3"><csymbol cd="ambiguous" id="S5.E1.m1.5.5.1.3.1.cmml" xref="S5.E1.m1.5.5.1.3">subscript</csymbol><ci id="S5.E1.m1.5.5.1.3.2.cmml" xref="S5.E1.m1.5.5.1.3.2">ℒ</ci><apply id="S5.E1.m1.5.5.1.3.3.cmml" xref="S5.E1.m1.5.5.1.3.3"><times id="S5.E1.m1.5.5.1.3.3.1.cmml" xref="S5.E1.m1.5.5.1.3.3.1"></times><ci id="S5.E1.m1.5.5.1.3.3.2.cmml" xref="S5.E1.m1.5.5.1.3.3.2">𝑑</ci><ci id="S5.E1.m1.5.5.1.3.3.3.cmml" xref="S5.E1.m1.5.5.1.3.3.3">𝑒</ci><ci id="S5.E1.m1.5.5.1.3.3.4.cmml" xref="S5.E1.m1.5.5.1.3.3.4">𝑝</ci><ci id="S5.E1.m1.5.5.1.3.3.5.cmml" xref="S5.E1.m1.5.5.1.3.3.5">𝑡</ci><ci id="S5.E1.m1.5.5.1.3.3.6.cmml" xref="S5.E1.m1.5.5.1.3.3.6">ℎ</ci></apply></apply><interval closure="open" id="S5.E1.m1.5.5.1.1.2.cmml" xref="S5.E1.m1.5.5.1.1.1"><ci id="S5.E1.m1.1.1.cmml" xref="S5.E1.m1.1.1">𝑑</ci><apply id="S5.E1.m1.5.5.1.1.1.1.cmml" xref="S5.E1.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S5.E1.m1.5.5.1.1.1.1.1.cmml" xref="S5.E1.m1.5.5.1.1.1.1">superscript</csymbol><ci id="S5.E1.m1.5.5.1.1.1.1.2.cmml" xref="S5.E1.m1.5.5.1.1.1.1.2">𝑑</ci><times id="S5.E1.m1.5.5.1.1.1.1.3.cmml" xref="S5.E1.m1.5.5.1.1.1.1.3"></times></apply></interval></apply><apply id="S5.E1.m1.8.8.4.cmml" xref="S5.E1.m1.8.8.4"><plus id="S5.E1.m1.8.8.4.4.cmml" xref="S5.E1.m1.8.8.4.4"></plus><apply id="S5.E1.m1.6.6.2.1.cmml" xref="S5.E1.m1.6.6.2.1"><times id="S5.E1.m1.6.6.2.1.2.cmml" xref="S5.E1.m1.6.6.2.1.2"></times><ci id="S5.E1.m1.6.6.2.1.3.cmml" xref="S5.E1.m1.6.6.2.1.3">𝛼</ci><apply id="S5.E1.m1.6.6.2.1.4.cmml" xref="S5.E1.m1.6.6.2.1.4"><csymbol cd="ambiguous" id="S5.E1.m1.6.6.2.1.4.1.cmml" xref="S5.E1.m1.6.6.2.1.4">subscript</csymbol><ci id="S5.E1.m1.6.6.2.1.4.2.cmml" xref="S5.E1.m1.6.6.2.1.4.2">ℒ</ci><apply id="S5.E1.m1.6.6.2.1.4.3.cmml" xref="S5.E1.m1.6.6.2.1.4.3"><times id="S5.E1.m1.6.6.2.1.4.3.1.cmml" xref="S5.E1.m1.6.6.2.1.4.3.1"></times><ci id="S5.E1.m1.6.6.2.1.4.3.2.cmml" xref="S5.E1.m1.6.6.2.1.4.3.2">𝑠</ci><ci id="S5.E1.m1.6.6.2.1.4.3.3.cmml" xref="S5.E1.m1.6.6.2.1.4.3.3">𝑠</ci><ci id="S5.E1.m1.6.6.2.1.4.3.4.cmml" xref="S5.E1.m1.6.6.2.1.4.3.4">𝑖</ci><ci id="S5.E1.m1.6.6.2.1.4.3.5.cmml" xref="S5.E1.m1.6.6.2.1.4.3.5">𝑚</ci></apply></apply><interval closure="open" id="S5.E1.m1.6.6.2.1.1.2.cmml" xref="S5.E1.m1.6.6.2.1.1.1"><ci id="S5.E1.m1.2.2.cmml" xref="S5.E1.m1.2.2">𝑒</ci><apply id="S5.E1.m1.6.6.2.1.1.1.1.cmml" xref="S5.E1.m1.6.6.2.1.1.1.1"><csymbol cd="ambiguous" id="S5.E1.m1.6.6.2.1.1.1.1.1.cmml" xref="S5.E1.m1.6.6.2.1.1.1.1">superscript</csymbol><ci id="S5.E1.m1.6.6.2.1.1.1.1.2.cmml" xref="S5.E1.m1.6.6.2.1.1.1.1.2">𝑒</ci><times id="S5.E1.m1.6.6.2.1.1.1.1.3.cmml" xref="S5.E1.m1.6.6.2.1.1.1.1.3"></times></apply></interval></apply><apply id="S5.E1.m1.7.7.3.2.cmml" xref="S5.E1.m1.7.7.3.2"><times id="S5.E1.m1.7.7.3.2.2.cmml" xref="S5.E1.m1.7.7.3.2.2"></times><ci id="S5.E1.m1.7.7.3.2.3.cmml" xref="S5.E1.m1.7.7.3.2.3">𝛽</ci><apply id="S5.E1.m1.7.7.3.2.4.cmml" xref="S5.E1.m1.7.7.3.2.4"><csymbol cd="ambiguous" id="S5.E1.m1.7.7.3.2.4.1.cmml" xref="S5.E1.m1.7.7.3.2.4">subscript</csymbol><ci id="S5.E1.m1.7.7.3.2.4.2.cmml" xref="S5.E1.m1.7.7.3.2.4.2">ℒ</ci><apply id="S5.E1.m1.7.7.3.2.4.3.cmml" xref="S5.E1.m1.7.7.3.2.4.3"><times id="S5.E1.m1.7.7.3.2.4.3.1.cmml" xref="S5.E1.m1.7.7.3.2.4.3.1"></times><ci id="S5.E1.m1.7.7.3.2.4.3.2.cmml" xref="S5.E1.m1.7.7.3.2.4.3.2">𝑠</ci><ci id="S5.E1.m1.7.7.3.2.4.3.3.cmml" xref="S5.E1.m1.7.7.3.2.4.3.3">𝑠</ci><ci id="S5.E1.m1.7.7.3.2.4.3.4.cmml" xref="S5.E1.m1.7.7.3.2.4.3.4">𝑖</ci><ci id="S5.E1.m1.7.7.3.2.4.3.5.cmml" xref="S5.E1.m1.7.7.3.2.4.3.5">𝑚</ci></apply></apply><interval closure="open" id="S5.E1.m1.7.7.3.2.1.2.cmml" xref="S5.E1.m1.7.7.3.2.1.1"><ci id="S5.E1.m1.3.3.cmml" xref="S5.E1.m1.3.3">𝑑</ci><apply id="S5.E1.m1.7.7.3.2.1.1.1.cmml" xref="S5.E1.m1.7.7.3.2.1.1.1"><csymbol cd="ambiguous" id="S5.E1.m1.7.7.3.2.1.1.1.1.cmml" xref="S5.E1.m1.7.7.3.2.1.1.1">superscript</csymbol><ci id="S5.E1.m1.7.7.3.2.1.1.1.2.cmml" xref="S5.E1.m1.7.7.3.2.1.1.1.2">𝑑</ci><times id="S5.E1.m1.7.7.3.2.1.1.1.3.cmml" xref="S5.E1.m1.7.7.3.2.1.1.1.3"></times></apply></interval></apply><apply id="S5.E1.m1.8.8.4.3.cmml" xref="S5.E1.m1.8.8.4.3"><times id="S5.E1.m1.8.8.4.3.2.cmml" xref="S5.E1.m1.8.8.4.3.2"></times><ci id="S5.E1.m1.8.8.4.3.3.cmml" xref="S5.E1.m1.8.8.4.3.3">𝛾</ci><apply id="S5.E1.m1.8.8.4.3.4.cmml" xref="S5.E1.m1.8.8.4.3.4"><csymbol cd="ambiguous" id="S5.E1.m1.8.8.4.3.4.1.cmml" xref="S5.E1.m1.8.8.4.3.4">subscript</csymbol><ci id="S5.E1.m1.8.8.4.3.4.2.cmml" xref="S5.E1.m1.8.8.4.3.4.2">ℒ</ci><apply id="S5.E1.m1.8.8.4.3.4.3.cmml" xref="S5.E1.m1.8.8.4.3.4.3"><times id="S5.E1.m1.8.8.4.3.4.3.1.cmml" xref="S5.E1.m1.8.8.4.3.4.3.1"></times><ci id="S5.E1.m1.8.8.4.3.4.3.2.cmml" xref="S5.E1.m1.8.8.4.3.4.3.2">𝑙</ci><cn type="integer" id="S5.E1.m1.8.8.4.3.4.3.3.cmml" xref="S5.E1.m1.8.8.4.3.4.3.3">1</cn></apply></apply><interval closure="open" id="S5.E1.m1.8.8.4.3.1.2.cmml" xref="S5.E1.m1.8.8.4.3.1.1"><ci id="S5.E1.m1.4.4.cmml" xref="S5.E1.m1.4.4">𝑑</ci><apply id="S5.E1.m1.8.8.4.3.1.1.1.cmml" xref="S5.E1.m1.8.8.4.3.1.1.1"><csymbol cd="ambiguous" id="S5.E1.m1.8.8.4.3.1.1.1.1.cmml" xref="S5.E1.m1.8.8.4.3.1.1.1">superscript</csymbol><ci id="S5.E1.m1.8.8.4.3.1.1.1.2.cmml" xref="S5.E1.m1.8.8.4.3.1.1.1.2">𝑑</ci><times id="S5.E1.m1.8.8.4.3.1.1.1.3.cmml" xref="S5.E1.m1.8.8.4.3.1.1.1.3"></times></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E1.m1.8c">\mathcal{L}_{depth}(d,d^{*})=\alpha\mathcal{L}_{ssim}(e,e^{*})+\beta\mathcal{L}_{ssim}(d,d^{*})+\gamma\mathcal{L}_{l1}(d,d^{*})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S5.SS0.SSS0.Px4.p1.12" class="ltx_p">where <math id="S5.SS0.SSS0.Px4.p1.1.m1.2" class="ltx_Math" alttext="d,d^{*}" display="inline"><semantics id="S5.SS0.SSS0.Px4.p1.1.m1.2a"><mrow id="S5.SS0.SSS0.Px4.p1.1.m1.2.2.1" xref="S5.SS0.SSS0.Px4.p1.1.m1.2.2.2.cmml"><mi id="S5.SS0.SSS0.Px4.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px4.p1.1.m1.1.1.cmml">d</mi><mo id="S5.SS0.SSS0.Px4.p1.1.m1.2.2.1.2" xref="S5.SS0.SSS0.Px4.p1.1.m1.2.2.2.cmml">,</mo><msup id="S5.SS0.SSS0.Px4.p1.1.m1.2.2.1.1" xref="S5.SS0.SSS0.Px4.p1.1.m1.2.2.1.1.cmml"><mi id="S5.SS0.SSS0.Px4.p1.1.m1.2.2.1.1.2" xref="S5.SS0.SSS0.Px4.p1.1.m1.2.2.1.1.2.cmml">d</mi><mo id="S5.SS0.SSS0.Px4.p1.1.m1.2.2.1.1.3" xref="S5.SS0.SSS0.Px4.p1.1.m1.2.2.1.1.3.cmml">∗</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px4.p1.1.m1.2b"><list id="S5.SS0.SSS0.Px4.p1.1.m1.2.2.2.cmml" xref="S5.SS0.SSS0.Px4.p1.1.m1.2.2.1"><ci id="S5.SS0.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.1.m1.1.1">𝑑</ci><apply id="S5.SS0.SSS0.Px4.p1.1.m1.2.2.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px4.p1.1.m1.2.2.1.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.1.m1.2.2.1.1">superscript</csymbol><ci id="S5.SS0.SSS0.Px4.p1.1.m1.2.2.1.1.2.cmml" xref="S5.SS0.SSS0.Px4.p1.1.m1.2.2.1.1.2">𝑑</ci><times id="S5.SS0.SSS0.Px4.p1.1.m1.2.2.1.1.3.cmml" xref="S5.SS0.SSS0.Px4.p1.1.m1.2.2.1.1.3"></times></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px4.p1.1.m1.2c">d,d^{*}</annotation></semantics></math> are the prediction and ground truth depth maps, and <math id="S5.SS0.SSS0.Px4.p1.2.m2.2" class="ltx_Math" alttext="e,e^{*}" display="inline"><semantics id="S5.SS0.SSS0.Px4.p1.2.m2.2a"><mrow id="S5.SS0.SSS0.Px4.p1.2.m2.2.2.1" xref="S5.SS0.SSS0.Px4.p1.2.m2.2.2.2.cmml"><mi id="S5.SS0.SSS0.Px4.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px4.p1.2.m2.1.1.cmml">e</mi><mo id="S5.SS0.SSS0.Px4.p1.2.m2.2.2.1.2" xref="S5.SS0.SSS0.Px4.p1.2.m2.2.2.2.cmml">,</mo><msup id="S5.SS0.SSS0.Px4.p1.2.m2.2.2.1.1" xref="S5.SS0.SSS0.Px4.p1.2.m2.2.2.1.1.cmml"><mi id="S5.SS0.SSS0.Px4.p1.2.m2.2.2.1.1.2" xref="S5.SS0.SSS0.Px4.p1.2.m2.2.2.1.1.2.cmml">e</mi><mo id="S5.SS0.SSS0.Px4.p1.2.m2.2.2.1.1.3" xref="S5.SS0.SSS0.Px4.p1.2.m2.2.2.1.1.3.cmml">∗</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px4.p1.2.m2.2b"><list id="S5.SS0.SSS0.Px4.p1.2.m2.2.2.2.cmml" xref="S5.SS0.SSS0.Px4.p1.2.m2.2.2.1"><ci id="S5.SS0.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.2.m2.1.1">𝑒</ci><apply id="S5.SS0.SSS0.Px4.p1.2.m2.2.2.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px4.p1.2.m2.2.2.1.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.2.m2.2.2.1.1">superscript</csymbol><ci id="S5.SS0.SSS0.Px4.p1.2.m2.2.2.1.1.2.cmml" xref="S5.SS0.SSS0.Px4.p1.2.m2.2.2.1.1.2">𝑒</ci><times id="S5.SS0.SSS0.Px4.p1.2.m2.2.2.1.1.3.cmml" xref="S5.SS0.SSS0.Px4.p1.2.m2.2.2.1.1.3"></times></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px4.p1.2.m2.2c">e,e^{*}</annotation></semantics></math> represent the edge maps of <math id="S5.SS0.SSS0.Px4.p1.3.m3.2" class="ltx_Math" alttext="d,d^{*}" display="inline"><semantics id="S5.SS0.SSS0.Px4.p1.3.m3.2a"><mrow id="S5.SS0.SSS0.Px4.p1.3.m3.2.2.1" xref="S5.SS0.SSS0.Px4.p1.3.m3.2.2.2.cmml"><mi id="S5.SS0.SSS0.Px4.p1.3.m3.1.1" xref="S5.SS0.SSS0.Px4.p1.3.m3.1.1.cmml">d</mi><mo id="S5.SS0.SSS0.Px4.p1.3.m3.2.2.1.2" xref="S5.SS0.SSS0.Px4.p1.3.m3.2.2.2.cmml">,</mo><msup id="S5.SS0.SSS0.Px4.p1.3.m3.2.2.1.1" xref="S5.SS0.SSS0.Px4.p1.3.m3.2.2.1.1.cmml"><mi id="S5.SS0.SSS0.Px4.p1.3.m3.2.2.1.1.2" xref="S5.SS0.SSS0.Px4.p1.3.m3.2.2.1.1.2.cmml">d</mi><mo id="S5.SS0.SSS0.Px4.p1.3.m3.2.2.1.1.3" xref="S5.SS0.SSS0.Px4.p1.3.m3.2.2.1.1.3.cmml">∗</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px4.p1.3.m3.2b"><list id="S5.SS0.SSS0.Px4.p1.3.m3.2.2.2.cmml" xref="S5.SS0.SSS0.Px4.p1.3.m3.2.2.1"><ci id="S5.SS0.SSS0.Px4.p1.3.m3.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.3.m3.1.1">𝑑</ci><apply id="S5.SS0.SSS0.Px4.p1.3.m3.2.2.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px4.p1.3.m3.2.2.1.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.3.m3.2.2.1.1">superscript</csymbol><ci id="S5.SS0.SSS0.Px4.p1.3.m3.2.2.1.1.2.cmml" xref="S5.SS0.SSS0.Px4.p1.3.m3.2.2.1.1.2">𝑑</ci><times id="S5.SS0.SSS0.Px4.p1.3.m3.2.2.1.1.3.cmml" xref="S5.SS0.SSS0.Px4.p1.3.m3.2.2.1.1.3"></times></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px4.p1.3.m3.2c">d,d^{*}</annotation></semantics></math>. <math id="S5.SS0.SSS0.Px4.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{L}_{ssim}" display="inline"><semantics id="S5.SS0.SSS0.Px4.p1.4.m4.1a"><msub id="S5.SS0.SSS0.Px4.p1.4.m4.1.1" xref="S5.SS0.SSS0.Px4.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS0.SSS0.Px4.p1.4.m4.1.1.2" xref="S5.SS0.SSS0.Px4.p1.4.m4.1.1.2.cmml">ℒ</mi><mrow id="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3" xref="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.cmml"><mi id="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.2" xref="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.1" xref="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.3" xref="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.1a" xref="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.4" xref="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.1b" xref="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.5" xref="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.5.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px4.p1.4.m4.1b"><apply id="S5.SS0.SSS0.Px4.p1.4.m4.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px4.p1.4.m4.1.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.4.m4.1.1">subscript</csymbol><ci id="S5.SS0.SSS0.Px4.p1.4.m4.1.1.2.cmml" xref="S5.SS0.SSS0.Px4.p1.4.m4.1.1.2">ℒ</ci><apply id="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.cmml" xref="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3"><times id="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.1.cmml" xref="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.1"></times><ci id="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.2.cmml" xref="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.2">𝑠</ci><ci id="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.3.cmml" xref="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.3">𝑠</ci><ci id="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.4.cmml" xref="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.4">𝑖</ci><ci id="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.5.cmml" xref="S5.SS0.SSS0.Px4.p1.4.m4.1.1.3.5">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px4.p1.4.m4.1c">\mathcal{L}_{ssim}</annotation></semantics></math> denotes the <span id="S5.SS0.SSS0.Px4.p1.12.1" class="ltx_text ltx_font_italic">SSIM loss function</span>, which is used to learn the structure of the depth map, and <math id="S5.SS0.SSS0.Px4.p1.5.m5.1" class="ltx_Math" alttext="\mathcal{L}_{l1}" display="inline"><semantics id="S5.SS0.SSS0.Px4.p1.5.m5.1a"><msub id="S5.SS0.SSS0.Px4.p1.5.m5.1.1" xref="S5.SS0.SSS0.Px4.p1.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS0.SSS0.Px4.p1.5.m5.1.1.2" xref="S5.SS0.SSS0.Px4.p1.5.m5.1.1.2.cmml">ℒ</mi><mrow id="S5.SS0.SSS0.Px4.p1.5.m5.1.1.3" xref="S5.SS0.SSS0.Px4.p1.5.m5.1.1.3.cmml"><mi id="S5.SS0.SSS0.Px4.p1.5.m5.1.1.3.2" xref="S5.SS0.SSS0.Px4.p1.5.m5.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px4.p1.5.m5.1.1.3.1" xref="S5.SS0.SSS0.Px4.p1.5.m5.1.1.3.1.cmml">​</mo><mn id="S5.SS0.SSS0.Px4.p1.5.m5.1.1.3.3" xref="S5.SS0.SSS0.Px4.p1.5.m5.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px4.p1.5.m5.1b"><apply id="S5.SS0.SSS0.Px4.p1.5.m5.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px4.p1.5.m5.1.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.5.m5.1.1">subscript</csymbol><ci id="S5.SS0.SSS0.Px4.p1.5.m5.1.1.2.cmml" xref="S5.SS0.SSS0.Px4.p1.5.m5.1.1.2">ℒ</ci><apply id="S5.SS0.SSS0.Px4.p1.5.m5.1.1.3.cmml" xref="S5.SS0.SSS0.Px4.p1.5.m5.1.1.3"><times id="S5.SS0.SSS0.Px4.p1.5.m5.1.1.3.1.cmml" xref="S5.SS0.SSS0.Px4.p1.5.m5.1.1.3.1"></times><ci id="S5.SS0.SSS0.Px4.p1.5.m5.1.1.3.2.cmml" xref="S5.SS0.SSS0.Px4.p1.5.m5.1.1.3.2">𝑙</ci><cn type="integer" id="S5.SS0.SSS0.Px4.p1.5.m5.1.1.3.3.cmml" xref="S5.SS0.SSS0.Px4.p1.5.m5.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px4.p1.5.m5.1c">\mathcal{L}_{l1}</annotation></semantics></math> is the standard <span id="S5.SS0.SSS0.Px4.p1.12.2" class="ltx_text ltx_font_italic">L1 Loss function</span> used to learn the depth values of each pixel. Finally, the factors <math id="S5.SS0.SSS0.Px4.p1.6.m6.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.SS0.SSS0.Px4.p1.6.m6.1a"><mi id="S5.SS0.SSS0.Px4.p1.6.m6.1.1" xref="S5.SS0.SSS0.Px4.p1.6.m6.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px4.p1.6.m6.1b"><ci id="S5.SS0.SSS0.Px4.p1.6.m6.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.6.m6.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px4.p1.6.m6.1c">\alpha</annotation></semantics></math>, <math id="S5.SS0.SSS0.Px4.p1.7.m7.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S5.SS0.SSS0.Px4.p1.7.m7.1a"><mi id="S5.SS0.SSS0.Px4.p1.7.m7.1.1" xref="S5.SS0.SSS0.Px4.p1.7.m7.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px4.p1.7.m7.1b"><ci id="S5.SS0.SSS0.Px4.p1.7.m7.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.7.m7.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px4.p1.7.m7.1c">\beta</annotation></semantics></math>, and <math id="S5.SS0.SSS0.Px4.p1.8.m8.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S5.SS0.SSS0.Px4.p1.8.m8.1a"><mi id="S5.SS0.SSS0.Px4.p1.8.m8.1.1" xref="S5.SS0.SSS0.Px4.p1.8.m8.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px4.p1.8.m8.1b"><ci id="S5.SS0.SSS0.Px4.p1.8.m8.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.8.m8.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px4.p1.8.m8.1c">\gamma</annotation></semantics></math> are used to regulate the scale of the <math id="S5.SS0.SSS0.Px4.p1.9.m9.1" class="ltx_Math" alttext="\mathcal{L}_{depth}" display="inline"><semantics id="S5.SS0.SSS0.Px4.p1.9.m9.1a"><msub id="S5.SS0.SSS0.Px4.p1.9.m9.1.1" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.2" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1.2.cmml">ℒ</mi><mrow id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.cmml"><mi id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.2" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.1" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.3" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.1a" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.4" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.1b" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.5" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.1c" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.6" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.6.cmml">h</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px4.p1.9.m9.1b"><apply id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1">subscript</csymbol><ci id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.2.cmml" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1.2">ℒ</ci><apply id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.cmml" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3"><times id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.1.cmml" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.1"></times><ci id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.2.cmml" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.2">𝑑</ci><ci id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.3.cmml" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.3">𝑒</ci><ci id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.4.cmml" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.4">𝑝</ci><ci id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.5.cmml" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.5">𝑡</ci><ci id="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.6.cmml" xref="S5.SS0.SSS0.Px4.p1.9.m9.1.1.3.6">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px4.p1.9.m9.1c">\mathcal{L}_{depth}</annotation></semantics></math> components. During our experiments, we set these factors as follows: <math id="S5.SS0.SSS0.Px4.p1.10.m10.1" class="ltx_Math" alttext="\alpha=0.85" display="inline"><semantics id="S5.SS0.SSS0.Px4.p1.10.m10.1a"><mrow id="S5.SS0.SSS0.Px4.p1.10.m10.1.1" xref="S5.SS0.SSS0.Px4.p1.10.m10.1.1.cmml"><mi id="S5.SS0.SSS0.Px4.p1.10.m10.1.1.2" xref="S5.SS0.SSS0.Px4.p1.10.m10.1.1.2.cmml">α</mi><mo id="S5.SS0.SSS0.Px4.p1.10.m10.1.1.1" xref="S5.SS0.SSS0.Px4.p1.10.m10.1.1.1.cmml">=</mo><mn id="S5.SS0.SSS0.Px4.p1.10.m10.1.1.3" xref="S5.SS0.SSS0.Px4.p1.10.m10.1.1.3.cmml">0.85</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px4.p1.10.m10.1b"><apply id="S5.SS0.SSS0.Px4.p1.10.m10.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.10.m10.1.1"><eq id="S5.SS0.SSS0.Px4.p1.10.m10.1.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.10.m10.1.1.1"></eq><ci id="S5.SS0.SSS0.Px4.p1.10.m10.1.1.2.cmml" xref="S5.SS0.SSS0.Px4.p1.10.m10.1.1.2">𝛼</ci><cn type="float" id="S5.SS0.SSS0.Px4.p1.10.m10.1.1.3.cmml" xref="S5.SS0.SSS0.Px4.p1.10.m10.1.1.3">0.85</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px4.p1.10.m10.1c">\alpha=0.85</annotation></semantics></math>, <math id="S5.SS0.SSS0.Px4.p1.11.m11.1" class="ltx_Math" alttext="\beta=0.9" display="inline"><semantics id="S5.SS0.SSS0.Px4.p1.11.m11.1a"><mrow id="S5.SS0.SSS0.Px4.p1.11.m11.1.1" xref="S5.SS0.SSS0.Px4.p1.11.m11.1.1.cmml"><mi id="S5.SS0.SSS0.Px4.p1.11.m11.1.1.2" xref="S5.SS0.SSS0.Px4.p1.11.m11.1.1.2.cmml">β</mi><mo id="S5.SS0.SSS0.Px4.p1.11.m11.1.1.1" xref="S5.SS0.SSS0.Px4.p1.11.m11.1.1.1.cmml">=</mo><mn id="S5.SS0.SSS0.Px4.p1.11.m11.1.1.3" xref="S5.SS0.SSS0.Px4.p1.11.m11.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px4.p1.11.m11.1b"><apply id="S5.SS0.SSS0.Px4.p1.11.m11.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.11.m11.1.1"><eq id="S5.SS0.SSS0.Px4.p1.11.m11.1.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.11.m11.1.1.1"></eq><ci id="S5.SS0.SSS0.Px4.p1.11.m11.1.1.2.cmml" xref="S5.SS0.SSS0.Px4.p1.11.m11.1.1.2">𝛽</ci><cn type="float" id="S5.SS0.SSS0.Px4.p1.11.m11.1.1.3.cmml" xref="S5.SS0.SSS0.Px4.p1.11.m11.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px4.p1.11.m11.1c">\beta=0.9</annotation></semantics></math>, and <math id="S5.SS0.SSS0.Px4.p1.12.m12.1" class="ltx_Math" alttext="\gamma=0.9" display="inline"><semantics id="S5.SS0.SSS0.Px4.p1.12.m12.1a"><mrow id="S5.SS0.SSS0.Px4.p1.12.m12.1.1" xref="S5.SS0.SSS0.Px4.p1.12.m12.1.1.cmml"><mi id="S5.SS0.SSS0.Px4.p1.12.m12.1.1.2" xref="S5.SS0.SSS0.Px4.p1.12.m12.1.1.2.cmml">γ</mi><mo id="S5.SS0.SSS0.Px4.p1.12.m12.1.1.1" xref="S5.SS0.SSS0.Px4.p1.12.m12.1.1.1.cmml">=</mo><mn id="S5.SS0.SSS0.Px4.p1.12.m12.1.1.3" xref="S5.SS0.SSS0.Px4.p1.12.m12.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px4.p1.12.m12.1b"><apply id="S5.SS0.SSS0.Px4.p1.12.m12.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.12.m12.1.1"><eq id="S5.SS0.SSS0.Px4.p1.12.m12.1.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.12.m12.1.1.1"></eq><ci id="S5.SS0.SSS0.Px4.p1.12.m12.1.1.2.cmml" xref="S5.SS0.SSS0.Px4.p1.12.m12.1.1.2">𝛾</ci><cn type="float" id="S5.SS0.SSS0.Px4.p1.12.m12.1.1.3.cmml" xref="S5.SS0.SSS0.Px4.p1.12.m12.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px4.p1.12.m12.1c">\gamma=0.9</annotation></semantics></math>.</p>
</div>
<div id="S5.SS0.SSS0.Px4.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px4.p2.1" class="ltx_p">Differently from the loss proposed in <cite class="ltx_cite ltx_citemacro_citep">(Ranftl et al., <a href="#bib.bib40" title="" class="ltx_ref">2022</a>)</cite>, which standardizes the scale of the depth maps for various datasets, the loss in <a href="#S5.E1" title="In monocular depth estimation branch ‣ 5 Proposed approach ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> allows the prediction of values convertible into a real 3d distance. Some examples of the considered depth maps are reported in Figure <a href="#S5.F8" title="Fig. 8 ‣ 5 Proposed approach ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results of the proposed approach on <span id="S5.T3.5.1" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> test data. The <span id="S5.T3.6.2" class="ltx_text ltx_font_italic">EgoISM-HOI-Synth</span> column indicates whether the <span id="S5.T3.7.3" class="ltx_text ltx_font_italic">EgoISM-HOI-Synth</span> training set was used for pre-training models. The <span id="S5.T3.8.4" class="ltx_text ltx_font_italic">EgoISM-HOI-Real %</span> column shows the percentage of real-world data used for fine-tuning.</figcaption>
<div id="S5.T3.9" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:145.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-51.9pt,17.4pt) scale(0.806983620666595,0.806983620666595) ;">
<table id="S5.T3.9.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.9.1.1.1" class="ltx_tr">
<th id="S5.T3.9.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">EgoISM-HOI-Synth</th>
<th id="S5.T3.9.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">EgoISM-HOI-Real %</th>
<th id="S5.T3.9.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP Hand</th>
<th id="S5.T3.9.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP H.+Side</th>
<th id="S5.T3.9.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP H.+State</th>
<th id="S5.T3.9.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP H.+Obj</th>
<th id="S5.T3.9.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP H.+All</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.9.1.2.1" class="ltx_tr">
<th id="S5.T3.9.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Yes</th>
<th id="S5.T3.9.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">0</th>
<td id="S5.T3.9.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">90.02</td>
<td id="S5.T3.9.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">84.72</td>
<td id="S5.T3.9.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">31.85</td>
<td id="S5.T3.9.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">23.92</td>
<td id="S5.T3.9.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">23.28</td>
</tr>
<tr id="S5.T3.9.1.3.2" class="ltx_tr">
<th id="S5.T3.9.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Yes</th>
<th id="S5.T3.9.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">10</th>
<td id="S5.T3.9.1.3.2.3" class="ltx_td ltx_align_center">90.53</td>
<td id="S5.T3.9.1.3.2.4" class="ltx_td ltx_align_center">89.34</td>
<td id="S5.T3.9.1.3.2.5" class="ltx_td ltx_align_center">46.64</td>
<td id="S5.T3.9.1.3.2.6" class="ltx_td ltx_align_center">30.90</td>
<td id="S5.T3.9.1.3.2.7" class="ltx_td ltx_align_center">30.65</td>
</tr>
<tr id="S5.T3.9.1.4.3" class="ltx_tr">
<th id="S5.T3.9.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Yes</th>
<th id="S5.T3.9.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">25</th>
<td id="S5.T3.9.1.4.3.3" class="ltx_td ltx_align_center">90.66</td>
<td id="S5.T3.9.1.4.3.4" class="ltx_td ltx_align_center">89.71</td>
<td id="S5.T3.9.1.4.3.5" class="ltx_td ltx_align_center">48.31</td>
<td id="S5.T3.9.1.4.3.6" class="ltx_td ltx_align_center">31.76</td>
<td id="S5.T3.9.1.4.3.7" class="ltx_td ltx_align_center">31.33</td>
</tr>
<tr id="S5.T3.9.1.5.4" class="ltx_tr">
<th id="S5.T3.9.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Yes</th>
<th id="S5.T3.9.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">50</th>
<td id="S5.T3.9.1.5.4.3" class="ltx_td ltx_align_center"><span id="S5.T3.9.1.5.4.3.1" class="ltx_text ltx_framed ltx_framed_underline">90.69</span></td>
<td id="S5.T3.9.1.5.4.4" class="ltx_td ltx_align_center"><span id="S5.T3.9.1.5.4.4.1" class="ltx_text ltx_framed ltx_framed_underline">90.00</span></td>
<td id="S5.T3.9.1.5.4.5" class="ltx_td ltx_align_center">54.79</td>
<td id="S5.T3.9.1.5.4.6" class="ltx_td ltx_align_center"><span id="S5.T3.9.1.5.4.6.1" class="ltx_text ltx_framed ltx_framed_underline">34.12</span></td>
<td id="S5.T3.9.1.5.4.7" class="ltx_td ltx_align_center"><span id="S5.T3.9.1.5.4.7.1" class="ltx_text ltx_framed ltx_framed_underline">33.12</span></td>
</tr>
<tr id="S5.T3.9.1.6.5" class="ltx_tr">
<th id="S5.T3.9.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Yes</th>
<th id="S5.T3.9.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">100</th>
<td id="S5.T3.9.1.6.5.3" class="ltx_td ltx_align_center"><span id="S5.T3.9.1.6.5.3.1" class="ltx_text ltx_font_bold">90.73</span></td>
<td id="S5.T3.9.1.6.5.4" class="ltx_td ltx_align_center">89.99</td>
<td id="S5.T3.9.1.6.5.5" class="ltx_td ltx_align_center"><span id="S5.T3.9.1.6.5.5.1" class="ltx_text ltx_font_bold">56.88</span></td>
<td id="S5.T3.9.1.6.5.6" class="ltx_td ltx_align_center"><span id="S5.T3.9.1.6.5.6.1" class="ltx_text ltx_font_bold">35.94</span></td>
<td id="S5.T3.9.1.6.5.7" class="ltx_td ltx_align_center"><span id="S5.T3.9.1.6.5.7.1" class="ltx_text ltx_font_bold">35.47</span></td>
</tr>
<tr id="S5.T3.9.1.7.6" class="ltx_tr">
<th id="S5.T3.9.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">No</th>
<th id="S5.T3.9.1.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">10</th>
<td id="S5.T3.9.1.7.6.3" class="ltx_td ltx_align_center">90.08</td>
<td id="S5.T3.9.1.7.6.4" class="ltx_td ltx_align_center">88.57</td>
<td id="S5.T3.9.1.7.6.5" class="ltx_td ltx_align_center">45.69</td>
<td id="S5.T3.9.1.7.6.6" class="ltx_td ltx_align_center">18.19</td>
<td id="S5.T3.9.1.7.6.7" class="ltx_td ltx_align_center">17.48</td>
</tr>
<tr id="S5.T3.9.1.8.7" class="ltx_tr">
<th id="S5.T3.9.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">No</th>
<th id="S5.T3.9.1.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">25</th>
<td id="S5.T3.9.1.8.7.3" class="ltx_td ltx_align_center">90.43</td>
<td id="S5.T3.9.1.8.7.4" class="ltx_td ltx_align_center">89.45</td>
<td id="S5.T3.9.1.8.7.5" class="ltx_td ltx_align_center">43.73</td>
<td id="S5.T3.9.1.8.7.6" class="ltx_td ltx_align_center">18.72</td>
<td id="S5.T3.9.1.8.7.7" class="ltx_td ltx_align_center">18.31</td>
</tr>
<tr id="S5.T3.9.1.9.8" class="ltx_tr">
<th id="S5.T3.9.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">No</th>
<th id="S5.T3.9.1.9.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">50</th>
<td id="S5.T3.9.1.9.8.3" class="ltx_td ltx_align_center">90.43</td>
<td id="S5.T3.9.1.9.8.4" class="ltx_td ltx_align_center">89.57</td>
<td id="S5.T3.9.1.9.8.5" class="ltx_td ltx_align_center">52.74</td>
<td id="S5.T3.9.1.9.8.6" class="ltx_td ltx_align_center">19.17</td>
<td id="S5.T3.9.1.9.8.7" class="ltx_td ltx_align_center">19.06</td>
</tr>
<tr id="S5.T3.9.1.10.9" class="ltx_tr">
<th id="S5.T3.9.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">No</th>
<th id="S5.T3.9.1.10.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">100</th>
<td id="S5.T3.9.1.10.9.3" class="ltx_td ltx_align_center ltx_border_b">90.54</td>
<td id="S5.T3.9.1.10.9.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T3.9.1.10.9.4.1" class="ltx_text ltx_font_bold">90.06</span></td>
<td id="S5.T3.9.1.10.9.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T3.9.1.10.9.5.1" class="ltx_text ltx_framed ltx_framed_underline">56.34</span></td>
<td id="S5.T3.9.1.10.9.6" class="ltx_td ltx_align_center ltx_border_b">22.31</td>
<td id="S5.T3.9.1.10.9.7" class="ltx_td ltx_align_center ltx_border_b">21.76</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS0.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">hand side classifier</h5>

<div id="S5.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px5.p1.1" class="ltx_p">A Multi-Layer Perceptron (MLP) with a hidden fully connected layer that takes as input an ROI-pooled feature vector of the hand crop to predict the hand side (<span id="S5.SS0.SSS0.Px5.p1.1.1" class="ltx_text ltx_font_italic">left/right</span>).</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">hand state classifier</h5>

<div id="S5.SS0.SSS0.Px6.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px6.p1.1" class="ltx_p">This module classifies the contact state of the detected hands through an additional MLP with a hidden fully connected layer. It takes as input the hand features vector, enlarged by 30% to include information about the surrounding context (e.g., nearby objects), and predicts the hand contact state (<span id="S5.SS0.SSS0.Px6.p1.1.1" class="ltx_text ltx_font_italic">no contact/in contact</span>).</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px7" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">multimodal hand state classifier</h5>

<div id="S5.SS0.SSS0.Px7.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px7.p1.1" class="ltx_p">This component is based on the EfficientNetV2 architecture <cite class="ltx_cite ltx_citemacro_citep">(Tan and Le, <a href="#bib.bib46" title="" class="ltx_ref">2021</a>)</cite>. It takes as input a combination of RGB, depth map (inferred by the <span id="S5.SS0.SSS0.Px7.p1.1.1" class="ltx_text ltx_font_italic">monocular depth estimation branch</span>), and instance segmentation mask (predicted by the <span id="S5.SS0.SSS0.Px7.p1.1.2" class="ltx_text ltx_font_italic">instance segmentation branch</span>) of each hand to estimate the hand contact state. The output of this module is combined with the output of the <span id="S5.SS0.SSS0.Px7.p1.1.3" class="ltx_text ltx_font_italic">hand state classifier</span> to obtain the final prediction of the hand contact state.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px8" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">offset vector regressor</h5>

<div id="S5.SS0.SSS0.Px8.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px8.p1.4" class="ltx_p">This module infers a vector that links the center of the bounding box of each hand to the center of the bounding box of the candidate active object (i.e., the object touched by the hand). This module consists of an MLP which takes as input the ROI-pooled feature vectors of the hands to predict <span id="S5.SS0.SSS0.Px8.p1.2.2" class="ltx_text ltx_font_italic">&lt;<math id="S5.SS0.SSS0.Px8.p1.1.1.m1.1" class="ltx_Math" alttext="v_{x}" display="inline"><semantics id="S5.SS0.SSS0.Px8.p1.1.1.m1.1a"><msub id="S5.SS0.SSS0.Px8.p1.1.1.m1.1.1" xref="S5.SS0.SSS0.Px8.p1.1.1.m1.1.1.cmml"><mi id="S5.SS0.SSS0.Px8.p1.1.1.m1.1.1.2" xref="S5.SS0.SSS0.Px8.p1.1.1.m1.1.1.2.cmml">v</mi><mi id="S5.SS0.SSS0.Px8.p1.1.1.m1.1.1.3" xref="S5.SS0.SSS0.Px8.p1.1.1.m1.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px8.p1.1.1.m1.1b"><apply id="S5.SS0.SSS0.Px8.p1.1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px8.p1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px8.p1.1.1.m1.1.1.1.cmml" xref="S5.SS0.SSS0.Px8.p1.1.1.m1.1.1">subscript</csymbol><ci id="S5.SS0.SSS0.Px8.p1.1.1.m1.1.1.2.cmml" xref="S5.SS0.SSS0.Px8.p1.1.1.m1.1.1.2">𝑣</ci><ci id="S5.SS0.SSS0.Px8.p1.1.1.m1.1.1.3.cmml" xref="S5.SS0.SSS0.Px8.p1.1.1.m1.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px8.p1.1.1.m1.1c">v_{x}</annotation></semantics></math>, <math id="S5.SS0.SSS0.Px8.p1.2.2.m2.1" class="ltx_Math" alttext="v_{y}" display="inline"><semantics id="S5.SS0.SSS0.Px8.p1.2.2.m2.1a"><msub id="S5.SS0.SSS0.Px8.p1.2.2.m2.1.1" xref="S5.SS0.SSS0.Px8.p1.2.2.m2.1.1.cmml"><mi id="S5.SS0.SSS0.Px8.p1.2.2.m2.1.1.2" xref="S5.SS0.SSS0.Px8.p1.2.2.m2.1.1.2.cmml">v</mi><mi id="S5.SS0.SSS0.Px8.p1.2.2.m2.1.1.3" xref="S5.SS0.SSS0.Px8.p1.2.2.m2.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px8.p1.2.2.m2.1b"><apply id="S5.SS0.SSS0.Px8.p1.2.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px8.p1.2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px8.p1.2.2.m2.1.1.1.cmml" xref="S5.SS0.SSS0.Px8.p1.2.2.m2.1.1">subscript</csymbol><ci id="S5.SS0.SSS0.Px8.p1.2.2.m2.1.1.2.cmml" xref="S5.SS0.SSS0.Px8.p1.2.2.m2.1.1.2">𝑣</ci><ci id="S5.SS0.SSS0.Px8.p1.2.2.m2.1.1.3.cmml" xref="S5.SS0.SSS0.Px8.p1.2.2.m2.1.1.3">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px8.p1.2.2.m2.1c">v_{y}</annotation></semantics></math>, m&gt;</span> triplets, where (<math id="S5.SS0.SSS0.Px8.p1.3.m1.2" class="ltx_Math" alttext="v_{x},v_{y}" display="inline"><semantics id="S5.SS0.SSS0.Px8.p1.3.m1.2a"><mrow id="S5.SS0.SSS0.Px8.p1.3.m1.2.2.2" xref="S5.SS0.SSS0.Px8.p1.3.m1.2.2.3.cmml"><msub id="S5.SS0.SSS0.Px8.p1.3.m1.1.1.1.1" xref="S5.SS0.SSS0.Px8.p1.3.m1.1.1.1.1.cmml"><mi id="S5.SS0.SSS0.Px8.p1.3.m1.1.1.1.1.2" xref="S5.SS0.SSS0.Px8.p1.3.m1.1.1.1.1.2.cmml">v</mi><mi id="S5.SS0.SSS0.Px8.p1.3.m1.1.1.1.1.3" xref="S5.SS0.SSS0.Px8.p1.3.m1.1.1.1.1.3.cmml">x</mi></msub><mo id="S5.SS0.SSS0.Px8.p1.3.m1.2.2.2.3" xref="S5.SS0.SSS0.Px8.p1.3.m1.2.2.3.cmml">,</mo><msub id="S5.SS0.SSS0.Px8.p1.3.m1.2.2.2.2" xref="S5.SS0.SSS0.Px8.p1.3.m1.2.2.2.2.cmml"><mi id="S5.SS0.SSS0.Px8.p1.3.m1.2.2.2.2.2" xref="S5.SS0.SSS0.Px8.p1.3.m1.2.2.2.2.2.cmml">v</mi><mi id="S5.SS0.SSS0.Px8.p1.3.m1.2.2.2.2.3" xref="S5.SS0.SSS0.Px8.p1.3.m1.2.2.2.2.3.cmml">y</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px8.p1.3.m1.2b"><list id="S5.SS0.SSS0.Px8.p1.3.m1.2.2.3.cmml" xref="S5.SS0.SSS0.Px8.p1.3.m1.2.2.2"><apply id="S5.SS0.SSS0.Px8.p1.3.m1.1.1.1.1.cmml" xref="S5.SS0.SSS0.Px8.p1.3.m1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px8.p1.3.m1.1.1.1.1.1.cmml" xref="S5.SS0.SSS0.Px8.p1.3.m1.1.1.1.1">subscript</csymbol><ci id="S5.SS0.SSS0.Px8.p1.3.m1.1.1.1.1.2.cmml" xref="S5.SS0.SSS0.Px8.p1.3.m1.1.1.1.1.2">𝑣</ci><ci id="S5.SS0.SSS0.Px8.p1.3.m1.1.1.1.1.3.cmml" xref="S5.SS0.SSS0.Px8.p1.3.m1.1.1.1.1.3">𝑥</ci></apply><apply id="S5.SS0.SSS0.Px8.p1.3.m1.2.2.2.2.cmml" xref="S5.SS0.SSS0.Px8.p1.3.m1.2.2.2.2"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px8.p1.3.m1.2.2.2.2.1.cmml" xref="S5.SS0.SSS0.Px8.p1.3.m1.2.2.2.2">subscript</csymbol><ci id="S5.SS0.SSS0.Px8.p1.3.m1.2.2.2.2.2.cmml" xref="S5.SS0.SSS0.Px8.p1.3.m1.2.2.2.2.2">𝑣</ci><ci id="S5.SS0.SSS0.Px8.p1.3.m1.2.2.2.2.3.cmml" xref="S5.SS0.SSS0.Px8.p1.3.m1.2.2.2.2.3">𝑦</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px8.p1.3.m1.2c">v_{x},v_{y}</annotation></semantics></math>) represent the direction of the vector and <math id="S5.SS0.SSS0.Px8.p1.4.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S5.SS0.SSS0.Px8.p1.4.m2.1a"><mi id="S5.SS0.SSS0.Px8.p1.4.m2.1.1" xref="S5.SS0.SSS0.Px8.p1.4.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px8.p1.4.m2.1b"><ci id="S5.SS0.SSS0.Px8.p1.4.m2.1.1.cmml" xref="S5.SS0.SSS0.Px8.p1.4.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px8.p1.4.m2.1c">m</annotation></semantics></math> its magnitude.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px9" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">matching algorithm</h5>

<div id="S5.SS0.SSS0.Px9.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px9.p1.3" class="ltx_p">The final module of our system is a matching algorithm that exploits the outputs of the previous modules to predict EHOIs as <span id="S5.SS0.SSS0.Px9.p1.3.1" class="ltx_text ltx_font_italic">&lt;hand, contact state, active object&gt;</span> triplets. For each detected hand, the algorithm calculates an interaction point (<math id="S5.SS0.SSS0.Px9.p1.1.m1.1" class="ltx_Math" alttext="p_{ehoi}" display="inline"><semantics id="S5.SS0.SSS0.Px9.p1.1.m1.1a"><msub id="S5.SS0.SSS0.Px9.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px9.p1.1.m1.1.1.cmml"><mi id="S5.SS0.SSS0.Px9.p1.1.m1.1.1.2" xref="S5.SS0.SSS0.Px9.p1.1.m1.1.1.2.cmml">p</mi><mrow id="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3" xref="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.cmml"><mi id="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.2" xref="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.1" xref="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.3" xref="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.1a" xref="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.4" xref="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.1b" xref="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.5" xref="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.5.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px9.p1.1.m1.1b"><apply id="S5.SS0.SSS0.Px9.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px9.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px9.p1.1.m1.1.1.1.cmml" xref="S5.SS0.SSS0.Px9.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS0.SSS0.Px9.p1.1.m1.1.1.2.cmml" xref="S5.SS0.SSS0.Px9.p1.1.m1.1.1.2">𝑝</ci><apply id="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.cmml" xref="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3"><times id="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.1.cmml" xref="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.1"></times><ci id="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.2.cmml" xref="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.2">𝑒</ci><ci id="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.3.cmml" xref="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.3">ℎ</ci><ci id="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.4.cmml" xref="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.4">𝑜</ci><ci id="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.5.cmml" xref="S5.SS0.SSS0.Px9.p1.1.m1.1.1.3.5">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px9.p1.1.m1.1c">p_{ehoi}</annotation></semantics></math>) using the bounding box center of the hand and the corresponding offset vector. <math id="S5.SS0.SSS0.Px9.p1.2.m2.1" class="ltx_Math" alttext="p_{ehoi}" display="inline"><semantics id="S5.SS0.SSS0.Px9.p1.2.m2.1a"><msub id="S5.SS0.SSS0.Px9.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px9.p1.2.m2.1.1.cmml"><mi id="S5.SS0.SSS0.Px9.p1.2.m2.1.1.2" xref="S5.SS0.SSS0.Px9.p1.2.m2.1.1.2.cmml">p</mi><mrow id="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3" xref="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.cmml"><mi id="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.2" xref="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.1" xref="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.3" xref="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.1a" xref="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.4" xref="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.1b" xref="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.5" xref="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.5.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px9.p1.2.m2.1b"><apply id="S5.SS0.SSS0.Px9.p1.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px9.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px9.p1.2.m2.1.1.1.cmml" xref="S5.SS0.SSS0.Px9.p1.2.m2.1.1">subscript</csymbol><ci id="S5.SS0.SSS0.Px9.p1.2.m2.1.1.2.cmml" xref="S5.SS0.SSS0.Px9.p1.2.m2.1.1.2">𝑝</ci><apply id="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.cmml" xref="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3"><times id="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.1.cmml" xref="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.1"></times><ci id="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.2.cmml" xref="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.2">𝑒</ci><ci id="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.3.cmml" xref="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.3">ℎ</ci><ci id="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.4.cmml" xref="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.4">𝑜</ci><ci id="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.5.cmml" xref="S5.SS0.SSS0.Px9.p1.2.m2.1.1.3.5">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px9.p1.2.m2.1c">p_{ehoi}</annotation></semantics></math> represents the prediction of the bounding box center of the candidate active object. Finally, the object whose center is closest to <math id="S5.SS0.SSS0.Px9.p1.3.m3.1" class="ltx_Math" alttext="p_{ehoi}" display="inline"><semantics id="S5.SS0.SSS0.Px9.p1.3.m3.1a"><msub id="S5.SS0.SSS0.Px9.p1.3.m3.1.1" xref="S5.SS0.SSS0.Px9.p1.3.m3.1.1.cmml"><mi id="S5.SS0.SSS0.Px9.p1.3.m3.1.1.2" xref="S5.SS0.SSS0.Px9.p1.3.m3.1.1.2.cmml">p</mi><mrow id="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3" xref="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.cmml"><mi id="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.2" xref="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.1" xref="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.3" xref="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.1a" xref="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.4" xref="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.1b" xref="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.5" xref="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.5.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px9.p1.3.m3.1b"><apply id="S5.SS0.SSS0.Px9.p1.3.m3.1.1.cmml" xref="S5.SS0.SSS0.Px9.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px9.p1.3.m3.1.1.1.cmml" xref="S5.SS0.SSS0.Px9.p1.3.m3.1.1">subscript</csymbol><ci id="S5.SS0.SSS0.Px9.p1.3.m3.1.1.2.cmml" xref="S5.SS0.SSS0.Px9.p1.3.m3.1.1.2">𝑝</ci><apply id="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.cmml" xref="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3"><times id="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.1.cmml" xref="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.1"></times><ci id="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.2.cmml" xref="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.2">𝑒</ci><ci id="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.3.cmml" xref="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.3">ℎ</ci><ci id="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.4.cmml" xref="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.4">𝑜</ci><ci id="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.5.cmml" xref="S5.SS0.SSS0.Px9.p1.3.m3.1.1.3.5">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px9.p1.3.m3.1c">p_{ehoi}</annotation></semantics></math> is chosen as the active object. 
<br class="ltx_break"></p>
</div>
<div id="S5.SS0.SSS0.Px9.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px9.p2.6" class="ltx_p">To optimize our system during the training phase, we used the standard Faster R-CNN loss <cite class="ltx_cite ltx_citemacro_citep">(Ren et al., <a href="#bib.bib41" title="" class="ltx_ref">2015</a>)</cite> for the <span id="S5.SS0.SSS0.Px9.p2.6.1" class="ltx_text ltx_font_italic">object detector branch</span>, while we utilized the definition of <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite> for the <span id="S5.SS0.SSS0.Px9.p2.6.2" class="ltx_text ltx_font_italic">instance segmentation branch</span>. As previously discussed, to optimize the <span id="S5.SS0.SSS0.Px9.p2.6.3" class="ltx_text ltx_font_italic">monocular depth estimation branch</span> we exploited the loss function in Equation (<a href="#S5.E1" title="In monocular depth estimation branch ‣ 5 Proposed approach ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). We used the standard <span id="S5.SS0.SSS0.Px9.p2.6.4" class="ltx_text ltx_font_italic">binary cross-entropy loss</span> for the <span id="S5.SS0.SSS0.Px9.p2.6.5" class="ltx_text ltx_font_italic">hand side classifier</span>, whereas for <span id="S5.SS0.SSS0.Px9.p2.6.6" class="ltx_text ltx_font_italic">offset vector regressor</span> we used the <span id="S5.SS0.SSS0.Px9.p2.6.7" class="ltx_text ltx_font_italic">mean squared error loss</span>. We optimized the <span id="S5.SS0.SSS0.Px9.p2.6.8" class="ltx_text ltx_font_italic">hand state classifier</span> and <span id="S5.SS0.SSS0.Px9.p2.6.9" class="ltx_text ltx_font_italic">multimodal hand state classifier</span> according to the following equation:</p>
<table id="S5.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left">
<div id="S5.E2.m1.1.1.1" class="ltx_inline-block ltx_markedasmath ltx_transformed_outer" style="width:411.9pt;height:18.2pt;vertical-align:-5.2pt;"><span class="ltx_transformed_inner" style="transform:translate(87.0pt,-2.7pt) scale(1.73059265453106,1.73059265453106) ;">
<p id="S5.E2.m1.1.1.1.1" class="ltx_p"><math id="S5.E2.m1.1.1.1.1.m1.8" class="ltx_Math" alttext="\mathcal{L}_{cs}(cs,cs^{*})=\mathcal{L}_{bce}(cs_{rgb},cs^{*})+\mathcal{L}_{bce}(cs_{mm},cs^{*})+\mathcal{L}_{bce}(cs_{lf},cs^{*})" display="inline"><semantics id="S5.E2.m1.1.1.1.1.m1.8a"><mrow id="S5.E2.m1.1.1.1.1.m1.8.8" xref="S5.E2.m1.1.1.1.1.m1.8.8.cmml"><mrow id="S5.E2.m1.1.1.1.1.m1.2.2.2" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.cmml"><msub id="S5.E2.m1.1.1.1.1.m1.2.2.2.4" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.E2.m1.1.1.1.1.m1.2.2.2.4.2" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.4.2.cmml">ℒ</mi><mrow id="S5.E2.m1.1.1.1.1.m1.2.2.2.4.3" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.4.3.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.2.2.2.4.3.2" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.4.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.2.2.2.4.3.1" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.4.3.1.cmml">​</mo><mi id="S5.E2.m1.1.1.1.1.m1.2.2.2.4.3.3" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.4.3.3.cmml">s</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.2.2.2.3" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.3.cmml">​</mo><mrow id="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.3" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.2.3.cmml">(</mo><mrow id="S5.E2.m1.1.1.1.1.m1.1.1.1.1.1.1" xref="S5.E2.m1.1.1.1.1.m1.1.1.1.1.1.1.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.1.1.1.1.1.1.2" xref="S5.E2.m1.1.1.1.1.m1.1.1.1.1.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.1.1.1.1.1.1.1" xref="S5.E2.m1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S5.E2.m1.1.1.1.1.m1.1.1.1.1.1.1.3" xref="S5.E2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.cmml">s</mi></mrow><mo id="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.4" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.2.3.cmml">,</mo><mrow id="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.2" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.1" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.1.cmml">​</mo><msup id="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.3" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.3.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.3.2" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.3.2.cmml">s</mi><mo id="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.3.3" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.3.3.cmml">∗</mo></msup></mrow><mo stretchy="false" id="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.5" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S5.E2.m1.1.1.1.1.m1.8.8.9" xref="S5.E2.m1.1.1.1.1.m1.8.8.9.cmml">=</mo><mrow id="S5.E2.m1.1.1.1.1.m1.8.8.8" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.cmml"><mrow id="S5.E2.m1.1.1.1.1.m1.4.4.4.2" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.cmml"><msub id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.2" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.2.cmml">ℒ</mi><mrow id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3.2" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3.1" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3.1.cmml">​</mo><mi id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3.3" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3.1a" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3.1.cmml">​</mo><mi id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3.4" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3.4.cmml">e</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.3" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.3.cmml">​</mo><mrow id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.3.cmml"><mo stretchy="false" id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.3" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.3.cmml">(</mo><mrow id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.2" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.1" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.1.cmml">​</mo><msub id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.2" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.2.cmml">s</mi><mrow id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3.2" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3.1" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3.1.cmml">​</mo><mi id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3.3" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3.1a" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3.1.cmml">​</mo><mi id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3.4" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3.4.cmml">b</mi></mrow></msub></mrow><mo id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.4" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.3.cmml">,</mo><mrow id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.2" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.1" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.1.cmml">​</mo><msup id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.3" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.3.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.3.2" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.3.2.cmml">s</mi><mo id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.3.3" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.3.3.cmml">∗</mo></msup></mrow><mo stretchy="false" id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.5" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.3.cmml">)</mo></mrow></mrow><mo id="S5.E2.m1.1.1.1.1.m1.8.8.8.7" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.7.cmml">+</mo><mrow id="S5.E2.m1.1.1.1.1.m1.6.6.6.4" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.cmml"><msub id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.2" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.2.cmml">ℒ</mi><mrow id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3.2" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3.1" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3.1.cmml">​</mo><mi id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3.3" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3.1a" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3.1.cmml">​</mo><mi id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3.4" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3.4.cmml">e</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.3" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.3.cmml">​</mo><mrow id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.3.cmml"><mo stretchy="false" id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.3" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.3.cmml">(</mo><mrow id="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1" xref="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.2" xref="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.1" xref="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.1.cmml">​</mo><msub id="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3" xref="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.2" xref="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.2.cmml">s</mi><mrow id="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.3" xref="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.3.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.3.2" xref="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.3.1" xref="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.3.1.cmml">​</mo><mi id="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.3.3" xref="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.3.3.cmml">m</mi></mrow></msub></mrow><mo id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.4" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.3.cmml">,</mo><mrow id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.2" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.1" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.1.cmml">​</mo><msup id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.3" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.3.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.3.2" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.3.2.cmml">s</mi><mo id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.3.3" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.3.3.cmml">∗</mo></msup></mrow><mo stretchy="false" id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.5" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.3.cmml">)</mo></mrow></mrow><mo id="S5.E2.m1.1.1.1.1.m1.8.8.8.7a" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.7.cmml">+</mo><mrow id="S5.E2.m1.1.1.1.1.m1.8.8.8.6" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.cmml"><msub id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.2" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.2.cmml">ℒ</mi><mrow id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3.2" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3.1" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3.1.cmml">​</mo><mi id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3.3" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3.1a" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3.1.cmml">​</mo><mi id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3.4" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3.4.cmml">e</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.3" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.3.cmml">​</mo><mrow id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.3.cmml"><mo stretchy="false" id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.3" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.3.cmml">(</mo><mrow id="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1" xref="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.2" xref="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.1" xref="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.1.cmml">​</mo><msub id="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3" xref="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.2" xref="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.2.cmml">s</mi><mrow id="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.3" xref="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.3.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.3.2" xref="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.3.1" xref="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.3.1.cmml">​</mo><mi id="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.3.3" xref="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.3.3.cmml">f</mi></mrow></msub></mrow><mo id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.4" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.3.cmml">,</mo><mrow id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.2" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.1" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.1.cmml">​</mo><msup id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.3" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.3.cmml"><mi id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.3.2" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.3.2.cmml">s</mi><mo id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.3.3" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.3.3.cmml">∗</mo></msup></mrow><mo stretchy="false" id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.5" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E2.m1.1.1.1.1.m1.8b"><apply id="S5.E2.m1.1.1.1.1.m1.8.8.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8"><eq id="S5.E2.m1.1.1.1.1.m1.8.8.9.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.9"></eq><apply id="S5.E2.m1.1.1.1.1.m1.2.2.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.2.2.2"><times id="S5.E2.m1.1.1.1.1.m1.2.2.2.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.3"></times><apply id="S5.E2.m1.1.1.1.1.m1.2.2.2.4.cmml" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.4"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.1.1.m1.2.2.2.4.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.4">subscript</csymbol><ci id="S5.E2.m1.1.1.1.1.m1.2.2.2.4.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.4.2">ℒ</ci><apply id="S5.E2.m1.1.1.1.1.m1.2.2.2.4.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.4.3"><times id="S5.E2.m1.1.1.1.1.m1.2.2.2.4.3.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.4.3.1"></times><ci id="S5.E2.m1.1.1.1.1.m1.2.2.2.4.3.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.4.3.2">𝑐</ci><ci id="S5.E2.m1.1.1.1.1.m1.2.2.2.4.3.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.4.3.3">𝑠</ci></apply></apply><interval closure="open" id="S5.E2.m1.1.1.1.1.m1.2.2.2.2.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2"><apply id="S5.E2.m1.1.1.1.1.m1.1.1.1.1.1.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.1.1.1.1.1.1"><times id="S5.E2.m1.1.1.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.1.1.1.1.1.1.1"></times><ci id="S5.E2.m1.1.1.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.1.1.1.1.1.1.2">𝑐</ci><ci id="S5.E2.m1.1.1.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.1.1.1.1.1.1.3">𝑠</ci></apply><apply id="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2"><times id="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.1"></times><ci id="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.2">𝑐</ci><apply id="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.3.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.3">superscript</csymbol><ci id="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.3.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.3.2">𝑠</ci><times id="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.3.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.2.2.2.2.2.2.3.3"></times></apply></apply></interval></apply><apply id="S5.E2.m1.1.1.1.1.m1.8.8.8.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.8"><plus id="S5.E2.m1.1.1.1.1.m1.8.8.8.7.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.7"></plus><apply id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2"><times id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.3"></times><apply id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.cmml" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4">subscript</csymbol><ci id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.2">ℒ</ci><apply id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3"><times id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3.1"></times><ci id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3.2">𝑏</ci><ci id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3.3">𝑐</ci><ci id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3.4.cmml" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.4.3.4">𝑒</ci></apply></apply><interval closure="open" id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2"><apply id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1"><times id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.1"></times><ci id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.2">𝑐</ci><apply id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3">subscript</csymbol><ci id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.2">𝑠</ci><apply id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3"><times id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3.1"></times><ci id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3.2">𝑟</ci><ci id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3.3">𝑔</ci><ci id="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3.4.cmml" xref="S5.E2.m1.1.1.1.1.m1.3.3.3.1.1.1.1.3.3.4">𝑏</ci></apply></apply></apply><apply id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2"><times id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.1"></times><ci id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.2">𝑐</ci><apply id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.3"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.3.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.3">superscript</csymbol><ci id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.3.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.3.2">𝑠</ci><times id="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.3.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.4.4.4.2.2.2.2.3.3"></times></apply></apply></interval></apply><apply id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.cmml" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4"><times id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.3"></times><apply id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.cmml" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4">subscript</csymbol><ci id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.2">ℒ</ci><apply id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3"><times id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3.1"></times><ci id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3.2">𝑏</ci><ci id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3.3">𝑐</ci><ci id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3.4.cmml" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.4.3.4">𝑒</ci></apply></apply><interval closure="open" id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2"><apply id="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1"><times id="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.1"></times><ci id="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.2">𝑐</ci><apply id="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3">subscript</csymbol><ci id="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.2">𝑠</ci><apply id="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.3"><times id="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.3.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.3.1"></times><ci id="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.3.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.3.2">𝑚</ci><ci id="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.3.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.5.5.5.3.1.1.1.3.3.3">𝑚</ci></apply></apply></apply><apply id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2"><times id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.1"></times><ci id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.2">𝑐</ci><apply id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.3"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.3.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.3">superscript</csymbol><ci id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.3.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.3.2">𝑠</ci><times id="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.3.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.6.6.6.4.2.2.2.3.3"></times></apply></apply></interval></apply><apply id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6"><times id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.3"></times><apply id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4">subscript</csymbol><ci id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.2">ℒ</ci><apply id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3"><times id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3.1"></times><ci id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3.2">𝑏</ci><ci id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3.3">𝑐</ci><ci id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3.4.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.4.3.4">𝑒</ci></apply></apply><interval closure="open" id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2"><apply id="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1"><times id="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.1"></times><ci id="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.2">𝑐</ci><apply id="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3">subscript</csymbol><ci id="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.2">𝑠</ci><apply id="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.3"><times id="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.3.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.3.1"></times><ci id="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.3.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.3.2">𝑙</ci><ci id="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.3.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.7.7.7.5.1.1.1.3.3.3">𝑓</ci></apply></apply></apply><apply id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2"><times id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.1"></times><ci id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.2">𝑐</ci><apply id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.3"><csymbol cd="ambiguous" id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.3.1.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.3">superscript</csymbol><ci id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.3.2.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.3.2">𝑠</ci><times id="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.3.3.cmml" xref="S5.E2.m1.1.1.1.1.m1.8.8.8.6.2.2.2.3.3"></times></apply></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E2.m1.1.1.1.1.m1.8c">\mathcal{L}_{cs}(cs,cs^{*})=\mathcal{L}_{bce}(cs_{rgb},cs^{*})+\mathcal{L}_{bce}(cs_{mm},cs^{*})+\mathcal{L}_{bce}(cs_{lf},cs^{*})</annotation></semantics></math></p>
</span></div>
</td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="0" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S5.SS0.SSS0.Px9.p2.5" class="ltx_p">where <math id="S5.SS0.SSS0.Px9.p2.1.m1.2" class="ltx_Math" alttext="cs,cs^{*}" display="inline"><semantics id="S5.SS0.SSS0.Px9.p2.1.m1.2a"><mrow id="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2" xref="S5.SS0.SSS0.Px9.p2.1.m1.2.2.3.cmml"><mrow id="S5.SS0.SSS0.Px9.p2.1.m1.1.1.1.1" xref="S5.SS0.SSS0.Px9.p2.1.m1.1.1.1.1.cmml"><mi id="S5.SS0.SSS0.Px9.p2.1.m1.1.1.1.1.2" xref="S5.SS0.SSS0.Px9.p2.1.m1.1.1.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px9.p2.1.m1.1.1.1.1.1" xref="S5.SS0.SSS0.Px9.p2.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px9.p2.1.m1.1.1.1.1.3" xref="S5.SS0.SSS0.Px9.p2.1.m1.1.1.1.1.3.cmml">s</mi></mrow><mo id="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.3" xref="S5.SS0.SSS0.Px9.p2.1.m1.2.2.3.cmml">,</mo><mrow id="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2" xref="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.cmml"><mi id="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.2" xref="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.1" xref="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.1.cmml">​</mo><msup id="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.3" xref="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.3.cmml"><mi id="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.3.2" xref="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.3.2.cmml">s</mi><mo id="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.3.3" xref="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.3.3.cmml">∗</mo></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px9.p2.1.m1.2b"><list id="S5.SS0.SSS0.Px9.p2.1.m1.2.2.3.cmml" xref="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2"><apply id="S5.SS0.SSS0.Px9.p2.1.m1.1.1.1.1.cmml" xref="S5.SS0.SSS0.Px9.p2.1.m1.1.1.1.1"><times id="S5.SS0.SSS0.Px9.p2.1.m1.1.1.1.1.1.cmml" xref="S5.SS0.SSS0.Px9.p2.1.m1.1.1.1.1.1"></times><ci id="S5.SS0.SSS0.Px9.p2.1.m1.1.1.1.1.2.cmml" xref="S5.SS0.SSS0.Px9.p2.1.m1.1.1.1.1.2">𝑐</ci><ci id="S5.SS0.SSS0.Px9.p2.1.m1.1.1.1.1.3.cmml" xref="S5.SS0.SSS0.Px9.p2.1.m1.1.1.1.1.3">𝑠</ci></apply><apply id="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.cmml" xref="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2"><times id="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.1.cmml" xref="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.1"></times><ci id="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.2.cmml" xref="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.2">𝑐</ci><apply id="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.3.cmml" xref="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.3"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.3.1.cmml" xref="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.3">superscript</csymbol><ci id="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.3.2.cmml" xref="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.3.2">𝑠</ci><times id="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.3.3.cmml" xref="S5.SS0.SSS0.Px9.p2.1.m1.2.2.2.2.3.3"></times></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px9.p2.1.m1.2c">cs,cs^{*}</annotation></semantics></math> are the prediction and ground truth hand contact states, <math id="S5.SS0.SSS0.Px9.p2.2.m2.1" class="ltx_Math" alttext="cs_{rgb}" display="inline"><semantics id="S5.SS0.SSS0.Px9.p2.2.m2.1a"><mrow id="S5.SS0.SSS0.Px9.p2.2.m2.1.1" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.cmml"><mi id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.2" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.1" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.1.cmml">​</mo><msub id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.cmml"><mi id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.2" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.2.cmml">s</mi><mrow id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3.cmml"><mi id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3.2" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3.1" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3.3" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3.1a" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3.4" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3.4.cmml">b</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px9.p2.2.m2.1b"><apply id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1"><times id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.1.cmml" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.1"></times><ci id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.2.cmml" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.2">𝑐</ci><apply id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.cmml" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.1.cmml" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3">subscript</csymbol><ci id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.2.cmml" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.2">𝑠</ci><apply id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3.cmml" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3"><times id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3.1.cmml" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3.1"></times><ci id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3.2.cmml" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3.2">𝑟</ci><ci id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3.3.cmml" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3.3">𝑔</ci><ci id="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3.4.cmml" xref="S5.SS0.SSS0.Px9.p2.2.m2.1.1.3.3.4">𝑏</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px9.p2.2.m2.1c">cs_{rgb}</annotation></semantics></math>, <math id="S5.SS0.SSS0.Px9.p2.3.m3.1" class="ltx_Math" alttext="cs_{mm}" display="inline"><semantics id="S5.SS0.SSS0.Px9.p2.3.m3.1a"><mrow id="S5.SS0.SSS0.Px9.p2.3.m3.1.1" xref="S5.SS0.SSS0.Px9.p2.3.m3.1.1.cmml"><mi id="S5.SS0.SSS0.Px9.p2.3.m3.1.1.2" xref="S5.SS0.SSS0.Px9.p2.3.m3.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px9.p2.3.m3.1.1.1" xref="S5.SS0.SSS0.Px9.p2.3.m3.1.1.1.cmml">​</mo><msub id="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3" xref="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.cmml"><mi id="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.2" xref="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.2.cmml">s</mi><mrow id="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.3" xref="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.3.cmml"><mi id="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.3.2" xref="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.3.1" xref="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.3.3" xref="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.3.3.cmml">m</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px9.p2.3.m3.1b"><apply id="S5.SS0.SSS0.Px9.p2.3.m3.1.1.cmml" xref="S5.SS0.SSS0.Px9.p2.3.m3.1.1"><times id="S5.SS0.SSS0.Px9.p2.3.m3.1.1.1.cmml" xref="S5.SS0.SSS0.Px9.p2.3.m3.1.1.1"></times><ci id="S5.SS0.SSS0.Px9.p2.3.m3.1.1.2.cmml" xref="S5.SS0.SSS0.Px9.p2.3.m3.1.1.2">𝑐</ci><apply id="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.cmml" xref="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.1.cmml" xref="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3">subscript</csymbol><ci id="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.2.cmml" xref="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.2">𝑠</ci><apply id="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.3.cmml" xref="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.3"><times id="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.3.1.cmml" xref="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.3.1"></times><ci id="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.3.2.cmml" xref="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.3.2">𝑚</ci><ci id="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.3.3.cmml" xref="S5.SS0.SSS0.Px9.p2.3.m3.1.1.3.3.3">𝑚</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px9.p2.3.m3.1c">cs_{mm}</annotation></semantics></math>, and <math id="S5.SS0.SSS0.Px9.p2.4.m4.1" class="ltx_Math" alttext="cs_{lf}" display="inline"><semantics id="S5.SS0.SSS0.Px9.p2.4.m4.1a"><mrow id="S5.SS0.SSS0.Px9.p2.4.m4.1.1" xref="S5.SS0.SSS0.Px9.p2.4.m4.1.1.cmml"><mi id="S5.SS0.SSS0.Px9.p2.4.m4.1.1.2" xref="S5.SS0.SSS0.Px9.p2.4.m4.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px9.p2.4.m4.1.1.1" xref="S5.SS0.SSS0.Px9.p2.4.m4.1.1.1.cmml">​</mo><msub id="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3" xref="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.cmml"><mi id="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.2" xref="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.2.cmml">s</mi><mrow id="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.3" xref="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.3.cmml"><mi id="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.3.2" xref="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.3.1" xref="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.3.3" xref="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.3.3.cmml">f</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px9.p2.4.m4.1b"><apply id="S5.SS0.SSS0.Px9.p2.4.m4.1.1.cmml" xref="S5.SS0.SSS0.Px9.p2.4.m4.1.1"><times id="S5.SS0.SSS0.Px9.p2.4.m4.1.1.1.cmml" xref="S5.SS0.SSS0.Px9.p2.4.m4.1.1.1"></times><ci id="S5.SS0.SSS0.Px9.p2.4.m4.1.1.2.cmml" xref="S5.SS0.SSS0.Px9.p2.4.m4.1.1.2">𝑐</ci><apply id="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.cmml" xref="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.1.cmml" xref="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3">subscript</csymbol><ci id="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.2.cmml" xref="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.2">𝑠</ci><apply id="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.3.cmml" xref="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.3"><times id="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.3.1.cmml" xref="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.3.1"></times><ci id="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.3.2.cmml" xref="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.3.2">𝑙</ci><ci id="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.3.3.cmml" xref="S5.SS0.SSS0.Px9.p2.4.m4.1.1.3.3.3">𝑓</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px9.p2.4.m4.1c">cs_{lf}</annotation></semantics></math> denotes, respectively, the predictions of the hand contact states of the <span id="S5.SS0.SSS0.Px9.p2.5.1" class="ltx_text ltx_font_italic">hand state classifier</span>, <span id="S5.SS0.SSS0.Px9.p2.5.2" class="ltx_text ltx_font_italic">multimodal hand state classifier</span> and the combined predictions of these modules. <math id="S5.SS0.SSS0.Px9.p2.5.m5.1" class="ltx_Math" alttext="\mathcal{L}_{bce}" display="inline"><semantics id="S5.SS0.SSS0.Px9.p2.5.m5.1a"><msub id="S5.SS0.SSS0.Px9.p2.5.m5.1.1" xref="S5.SS0.SSS0.Px9.p2.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS0.SSS0.Px9.p2.5.m5.1.1.2" xref="S5.SS0.SSS0.Px9.p2.5.m5.1.1.2.cmml">ℒ</mi><mrow id="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3" xref="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3.cmml"><mi id="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3.2" xref="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3.1" xref="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3.3" xref="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3.1a" xref="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3.1.cmml">​</mo><mi id="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3.4" xref="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3.4.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px9.p2.5.m5.1b"><apply id="S5.SS0.SSS0.Px9.p2.5.m5.1.1.cmml" xref="S5.SS0.SSS0.Px9.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px9.p2.5.m5.1.1.1.cmml" xref="S5.SS0.SSS0.Px9.p2.5.m5.1.1">subscript</csymbol><ci id="S5.SS0.SSS0.Px9.p2.5.m5.1.1.2.cmml" xref="S5.SS0.SSS0.Px9.p2.5.m5.1.1.2">ℒ</ci><apply id="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3.cmml" xref="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3"><times id="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3.1.cmml" xref="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3.1"></times><ci id="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3.2.cmml" xref="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3.2">𝑏</ci><ci id="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3.3.cmml" xref="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3.3">𝑐</ci><ci id="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3.4.cmml" xref="S5.SS0.SSS0.Px9.p2.5.m5.1.1.3.4">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px9.p2.5.m5.1c">\mathcal{L}_{bce}</annotation></semantics></math> denotes the standard <span id="S5.SS0.SSS0.Px9.p2.5.3" class="ltx_text ltx_font_italic">binary cross-entropy loss</span>. The final loss of our system is the sum of all the aforementioned losses.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Experimental results</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We conducted a series of experiments to
1) assess how much the generated synthetic data are useful in training models able to generalize to the real-world domain (Section <a href="#S6.SS2" title="6.2 The Impact of Synthetic Data on System Performance ‣ 6 Experimental results ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2</span></a>),
2) highlight the contribution of multimodal signals to tackle the EHOI detection task (Section <a href="#S6.SS3" title="6.3 Impact of Multimodal training ‣ 6 Experimental results ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3</span></a>), and
3) compare the proposed method with a set of baselines based of state-of-the-art class-agnostic approaches (Section <a href="#S6.SS4" title="6.4 Comparison with class-agnostic baselines ‣ 6 Experimental results ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.4</span></a>). Section <a href="#S6.SS5" title="6.5 Additional results ‣ 6 Experimental results ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.5</span></a> further reports additional results on pre-training our method with external data and improvements obtained by our approach on the object detection task.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Experimental Settings</h3>

<section id="S6.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Dataset</h5>

<div id="S6.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS1.SSS0.Px1.p1.1" class="ltx_p">We performed experiments on the proposed <span id="S6.SS1.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">EgoISM-HOI</span> dataset. Since we want to exploit synthetic data to train models to detect EHOIs when few or zero real-world data are available, we used the splits reported in Table <a href="#S4.T1" title="Table 1 ‣ EgoISM-HOI-Synth ‣ 4 EgoISM-HOI dataset ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and Table <a href="#S4.T2" title="Table 2 ‣ EgoISM-HOI-Real ‣ 4 EgoISM-HOI dataset ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for the synthetic and real data respectively.</p>
</div>
</section>
<section id="S6.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Evaluation Metrics</h5>

<div id="S6.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS1.SSS0.Px2.p1.1" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_citet">Shan et al. (<a href="#bib.bib45" title="" class="ltx_ref">2020</a>)</cite>, we evaluated our method using metrics based on standard <span id="S6.SS1.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">Average Precision</span>, which assess the models’ ability to detect hands and objects as well as the correctness of some attributes such as the hand state, the hand side, and whether an object is active (i.e., it is involved in an interaction). In addition, since our model predicts active object classes, we computed the <span id="S6.SS1.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">mean Average Precision</span> (mAP) to consider the correctness of the predicted object classes. Specifically, we used the following metrics: 1) <span id="S6.SS1.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_italic">AP Hand</span>: <span id="S6.SS1.SSS0.Px2.p1.1.4" class="ltx_text ltx_font_italic">Average Precision</span> of the hand detections, 2) <span id="S6.SS1.SSS0.Px2.p1.1.5" class="ltx_text ltx_font_italic">AP Hand+Side</span>: <span id="S6.SS1.SSS0.Px2.p1.1.6" class="ltx_text ltx_font_italic">Average Precision</span> of the hand detections considering the correctness of the hand side, 3) <span id="S6.SS1.SSS0.Px2.p1.1.7" class="ltx_text ltx_font_italic">AP Hand+State</span>: <span id="S6.SS1.SSS0.Px2.p1.1.8" class="ltx_text ltx_font_italic">Average Precision</span> of the hand detections considering the correctness of the hand state, 4) <span id="S6.SS1.SSS0.Px2.p1.1.9" class="ltx_text ltx_font_italic">mAP Hand+Obj</span>: <span id="S6.SS1.SSS0.Px2.p1.1.10" class="ltx_text ltx_font_italic">mean Average Precision</span> of the <span id="S6.SS1.SSS0.Px2.p1.1.11" class="ltx_text ltx_font_italic">&lt;hand, active object&gt;</span> detected pairs, and 5) <span id="S6.SS1.SSS0.Px2.p1.1.12" class="ltx_text ltx_font_italic">mAP Hand+All</span>: combinations of <span id="S6.SS1.SSS0.Px2.p1.1.13" class="ltx_text ltx_font_italic">AP Hand+Side</span>, <span id="S6.SS1.SSS0.Px2.p1.1.14" class="ltx_text ltx_font_italic">AP Hand+State</span>, and <span id="S6.SS1.SSS0.Px2.p1.1.15" class="ltx_text ltx_font_italic">mAP Hand+Obj</span> metrics.</p>
</div>
</section>
<section id="S6.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Training Details</h5>

<div id="S6.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S6.SS1.SSS0.Px3.p1.1" class="ltx_p">To perform all the experiments we used a machine with a single <span id="S6.SS1.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">NVIDIA A30</span> GPU and an <span id="S6.SS1.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_italic">Intel Xeon Silver 4310</span> CPU. We scaled images for both the training and inference phases to a resolution of 1280x720 pixels. We trained models on <span id="S6.SS1.SSS0.Px3.p1.1.3" class="ltx_text ltx_font_italic">EgoISM-HOI-Synth</span> with <span id="S6.SS1.SSS0.Px3.p1.1.4" class="ltx_text ltx_font_italic">Stochastic Gradient Descent</span> (SGD) for 80,000 iterations with an initial learning rate equal to 0.001, which is decreased by a factor of 10 after 40,000 and 60,000 iterations, and a minibatch size of 4 images. Instead, to fine-tune the models with <span id="S6.SS1.SSS0.Px3.p1.1.5" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> training data, we froze the <span id="S6.SS1.SSS0.Px3.p1.1.6" class="ltx_text ltx_font_italic">monocular depth estimation branch</span> and <span id="S6.SS1.SSS0.Px3.p1.1.7" class="ltx_text ltx_font_italic">instance segmentation branch</span> modules. Finally, we trained the models for 20,000 iterations and decreased the initial learning rate (0.001) by a factor of 10 after 12,500 and 15,000 iterations.</p>
</div>
</section>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>The Impact of Synthetic Data on System Performance</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">The goal of this set of experiments is to show the ability of a model trained with synthetic data to generalize to real-world data. Specifically, we want to demonstrate how the synthetic data generated by the proposed tool can be used to represent realistic human-object interactions.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">We compared models pre-trained on the <span id="S6.SS2.p2.1.1" class="ltx_text ltx_font_italic">EgoISM-HOI-Synth</span> training split and fine-tuned using different amounts of <span id="S6.SS2.p2.1.2" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> training data (i.e., 0%, 10%, 25%, 50%, and 100%) with models trained only with <span id="S6.SS2.p2.1.3" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> data. Since the <span id="S6.SS2.p2.1.4" class="ltx_text ltx_font_italic">multimodal hand state classifier</span>, <span id="S6.SS2.p2.1.5" class="ltx_text ltx_font_italic">monocular depth estimation branch</span>, and <span id="S6.SS2.p2.1.6" class="ltx_text ltx_font_italic">instance segmentation branch</span> modules need to be trained with labels available only on synthetic data, we deactivated these components in all the models in this set of experiments for a fair comparison.</p>
</div>
<figure id="S6.F9" class="ltx_figure"><img src="/html/2306.12152/assets/x4.png" id="S6.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="192" height="118" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 9: </span>Performance comparison of the proposed system on our <span id="S6.F9.5.1" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> test data in terms of <span id="S6.F9.6.2" class="ltx_text ltx_font_italic">mAP Hand+All</span>. The blue curve reports the results of the models pre-trained on <span id="S6.F9.7.3" class="ltx_text ltx_font_italic">EgoISM-HOI-Synth</span> and fine-tuned at different percentages of the <span id="S6.F9.8.4" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> training set, while the red curve reports the results of the models trained on real-world data only.</figcaption>
</figure>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p">Table <a href="#S5.T3" title="Table 3 ‣ monocular depth estimation branch ‣ 5 Proposed approach ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> reports EHOI detection results on the <span id="S6.SS2.p3.1.1" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> test set. Models pre-trained with <span id="S6.SS2.p3.1.2" class="ltx_text ltx_font_italic">EgoISM-HOI-Synth</span> data (rows 1-5) outperform all the corresponding models trained using only <span id="S6.SS2.p3.1.3" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> data (rows 6-9) by consistent margins according to all evaluation metrics, except for the <span id="S6.SS2.p3.1.4" class="ltx_text ltx_font_italic">AP Hand+Side</span> measure, in which they obtain the second-best result (row 4). Considering the two models fine-tuned using the 100% of the real-world training set (rows 5 and 9), the improvements of the model pre-trained with <span id="S6.SS2.p3.1.5" class="ltx_text ltx_font_italic">EgoISM-HOI-Synth</span> data are significant in the metrics affected by active objects, obtaining +13,63% (35.94 vs 22.31) for the <span id="S6.SS2.p3.1.6" class="ltx_text ltx_font_italic">mAP Hand+Obj</span> and +13,71% (35.47 vs 21.76) for the <span id="S6.SS2.p3.1.7" class="ltx_text ltx_font_italic">mAP Hand+All</span>. These improvements are also evident if we compare the models trained with smaller portions of the real-world data. However, for the metrics <span id="S6.SS2.p3.1.8" class="ltx_text ltx_font_italic">AP Hand</span> and <span id="S6.SS2.p3.1.9" class="ltx_text ltx_font_italic">AP Hand+State</span>, there is only a small boost in the performance of the model pre-trained on <span id="S6.SS2.p3.1.10" class="ltx_text ltx_font_italic">EgoISM-HOI-Synth</span> (row 5) compared to the model trained using only <span id="S6.SS2.p3.1.11" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> data (row 9), i.e., +0.19% (90.73 vs 90.54) and +0.54% (56.88 vs 56.34). These results suggest that using synthetic data for pre-training models enhances the method’s ability to detect active objects which are susceptible to frequent occlusions by the hands. In addition, it is worth noting that the model trained using only the <span id="S6.SS2.p3.1.12" class="ltx_text ltx_font_italic">EgoISM-HOI-Synth</span> data (row 1) outperforms the best model that used only the real-world data for the evaluation measures influenced by the active objects, obtaining +1,61% (23.92 vs 22.31) and +1,52% (23.28 vs 21.76) for the <span id="S6.SS2.p3.1.13" class="ltx_text ltx_font_italic">mAP Hand+Obj</span> and <span id="S6.SS2.p3.1.14" class="ltx_text ltx_font_italic">mAP Hand+All</span> metrics, respectively. Figure <a href="#S6.F9" title="Fig. 9 ‣ 6.2 The Impact of Synthetic Data on System Performance ‣ 6 Experimental results ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> further illustrates the results in terms of <span id="S6.SS2.p3.1.15" class="ltx_text ltx_font_italic">mAP Hand+All</span> considering different amounts of <span id="S6.SS2.p3.1.16" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> training data in the fine-tuning.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Impact of Multimodal training</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">This set of experiments aims to highlight the contribution of the different modalities involved in our approach.
For these experiments, we consider the full architecture illustrated in Figure <a href="#S4.F7" title="Fig. 7 ‣ EgoISM-HOI-Real ‣ 4 EgoISM-HOI dataset ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> comprising the <span id="S6.SS3.p1.1.1" class="ltx_text ltx_font_italic">backbone</span>, the <span id="S6.SS3.p1.1.2" class="ltx_text ltx_font_italic">object detector branch</span>, the <span id="S6.SS3.p1.1.3" class="ltx_text ltx_font_italic">instance segmentation branch</span>, the <span id="S6.SS3.p1.1.4" class="ltx_text ltx_font_italic">monocular depth estimation branch</span>, and the <span id="S6.SS3.p1.1.5" class="ltx_text ltx_font_italic">multimodal hand state classifier</span>.
As a baseline, we considered a model trained by deactivating the <span id="S6.SS3.p1.1.6" class="ltx_text ltx_font_italic">multimodal hand state classifier</span>, <span id="S6.SS3.p1.1.7" class="ltx_text ltx_font_italic">monocular depth estimation branch</span>, and <span id="S6.SS3.p1.1.8" class="ltx_text ltx_font_italic">instance segmentation branch</span> modules. We compare this baseline with several versions of the proposed architecture in which the <span id="S6.SS3.p1.1.9" class="ltx_text ltx_font_italic">hand contact state</span> is estimated using different subsets of modalities (i.e., RGB, Depth, and Mask) and modules (i.e., <span id="S6.SS3.p1.1.10" class="ltx_text ltx_font_italic">multimodal hand state classifier</span>, and <span id="S6.SS3.p1.1.11" class="ltx_text ltx_font_italic">hand state classifier</span>). As these modules only affect the prediction of hand contact state, Table <a href="#S6.T4" title="Table 4 ‣ 6.3 Impact of Multimodal training ‣ 6 Experimental results ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> reports only the metrics affected by these predictions (i.e., <span id="S6.SS3.p1.1.12" class="ltx_text ltx_font_italic">AP Hand+State</span> and <span id="S6.SS3.p1.1.13" class="ltx_text ltx_font_italic">mAP Hand+All</span>). Note that all the models used in this experiment were pre-trained using <span id="S6.SS3.p1.1.14" class="ltx_text ltx_font_italic">EgoISM-HOI-Synth</span> and then fine-tuned using 100% of the <span id="S6.SS3.p1.1.15" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> training set.</p>
</div>
<figure id="S6.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Experiments to evaluate the impact on system performance of the different modalities and components involved in our architecture. The <span id="S6.T4.7.1" class="ltx_text ltx_font_italic">Contact state</span> column indicates the branches used to predict the <span id="S6.T4.8.2" class="ltx_text ltx_font_italic">hand contact states</span>, i.e., <span id="S6.T4.9.3" class="ltx_text ltx_font_italic">multimodal hand state classifier</span> (MHS), and <span id="S6.T4.10.4" class="ltx_text ltx_font_italic">Hand state classifier</span> (HS). While the <span id="S6.T4.11.5" class="ltx_text ltx_font_italic">MHS Input Modalities</span> column indicates the modalities passed in input to the <span id="S6.T4.12.6" class="ltx_text ltx_font_italic">multimodal hand state classifier</span>. The best results are highlighted in bold, whereas the second-best results are underlined.</figcaption>
<div id="S6.T4.13" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:168.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(54.8pt,-21.3pt) scale(1.33792955127459,1.33792955127459) ;">
<table id="S6.T4.13.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T4.13.1.1.1" class="ltx_tr">
<th id="S6.T4.13.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Contact state</th>
<th id="S6.T4.13.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">MHS Input Modalities</th>
<th id="S6.T4.13.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP H+State</th>
<th id="S6.T4.13.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP H+All</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T4.13.1.2.1" class="ltx_tr">
<th id="S6.T4.13.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">HS</th>
<td id="S6.T4.13.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S6.T4.13.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">56.88</td>
<td id="S6.T4.13.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">35.47</td>
</tr>
<tr id="S6.T4.13.1.3.2" class="ltx_tr">
<th id="S6.T4.13.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">HS+MHS</th>
<td id="S6.T4.13.1.3.2.2" class="ltx_td ltx_align_center">RGB</td>
<td id="S6.T4.13.1.3.2.3" class="ltx_td ltx_align_center">58.29</td>
<td id="S6.T4.13.1.3.2.4" class="ltx_td ltx_align_center">35.71</td>
</tr>
<tr id="S6.T4.13.1.4.3" class="ltx_tr">
<th id="S6.T4.13.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">HS+MHS</th>
<td id="S6.T4.13.1.4.3.2" class="ltx_td ltx_align_center">RGB+DEPTH</td>
<td id="S6.T4.13.1.4.3.3" class="ltx_td ltx_align_center"><span id="S6.T4.13.1.4.3.3.1" class="ltx_text ltx_framed ltx_framed_underline">58.37</span></td>
<td id="S6.T4.13.1.4.3.4" class="ltx_td ltx_align_center"><span id="S6.T4.13.1.4.3.4.1" class="ltx_text ltx_framed ltx_framed_underline">35.92</span></td>
</tr>
<tr id="S6.T4.13.1.5.4" class="ltx_tr">
<th id="S6.T4.13.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">HS+MHS</th>
<td id="S6.T4.13.1.5.4.2" class="ltx_td ltx_align_center">RGB+MASK</td>
<td id="S6.T4.13.1.5.4.3" class="ltx_td ltx_align_center">58.30</td>
<td id="S6.T4.13.1.5.4.4" class="ltx_td ltx_align_center">35.34</td>
</tr>
<tr id="S6.T4.13.1.6.5" class="ltx_tr">
<th id="S6.T4.13.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">HS+MHS</th>
<td id="S6.T4.13.1.6.5.2" class="ltx_td ltx_align_center">RGB+DEPTH+MASK</td>
<td id="S6.T4.13.1.6.5.3" class="ltx_td ltx_align_center"><span id="S6.T4.13.1.6.5.3.1" class="ltx_text ltx_font_bold">58.40</span></td>
<td id="S6.T4.13.1.6.5.4" class="ltx_td ltx_align_center"><span id="S6.T4.13.1.6.5.4.1" class="ltx_text ltx_font_bold">36.51</span></td>
</tr>
<tr id="S6.T4.13.1.7.6" class="ltx_tr">
<th id="S6.T4.13.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">MHS</th>
<td id="S6.T4.13.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b">RGB+DEPTH+MASK</td>
<td id="S6.T4.13.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b">57.56</td>
<td id="S6.T4.13.1.7.6.4" class="ltx_td ltx_align_center ltx_border_b">35.81</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">Combining the predictions of the <span id="S6.SS3.p2.1.1" class="ltx_text ltx_font_italic">multimodal hand state classifier</span> and <span id="S6.SS3.p2.1.2" class="ltx_text ltx_font_italic">hand state classifier</span> modules (rows 2-5) leads to general improvements in the system performance over the models that use only a single branch to predict the <span id="S6.SS3.p2.1.3" class="ltx_text ltx_font_italic">hand contact state</span> (rows 1 and 6), with maximum improvements over the baseline (rows 5 vs 1) of +1,52% (58.40 vs 56.88) for the <span id="S6.SS3.p2.1.4" class="ltx_text ltx_font_italic">AP Hand+State</span> and +1.04% (36.51 vs 35.47) for the <span id="S6.SS3.p2.1.5" class="ltx_text ltx_font_italic">mAP Hand+All</span>. Fusing RGB with Depth signals (row 3) brings a small improvement of +0.21% (35.92 vs 35.71) for the <span id="S6.SS3.p2.1.6" class="ltx_text ltx_font_italic">mAP Hand+All</span> over the model which uses only the RGB signal (row 2). Interestingly, combining RGB with Mask (row 4) improves the result of +1.42% (58.30 vs 56.88) over the baseline (row 1) in terms of <span id="S6.SS3.p2.1.7" class="ltx_text ltx_font_italic">AP Hand+State</span> but leads to a worsening performance of -0.13% (35.34 vs 35.47) considering the <span id="S6.SS3.p2.1.8" class="ltx_text ltx_font_italic">mAP Hand+All</span> measure. This suggests that the method is unable to benefit from segmentation masks in the absence of the depth signal. Finally, fusing all the modalities (row 5) leads to the best performance, bringing an improvement over the second-best result (RGB+DEPTH, row 3) of +0.59% (36.51 vs 35.92) for the <span id="S6.SS3.p2.1.9" class="ltx_text ltx_font_italic">mAP Hand+All</span> metric. Figure <a href="#S6.F10" title="Fig. 10 ‣ 6.3 Impact of Multimodal training ‣ 6 Experimental results ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> shows some qualitative results obtained with the full proposed architecture.</p>
</div>
<figure id="S6.F10" class="ltx_figure"><img src="/html/2306.12152/assets/imgs/fig_qualitative_examples.jpg" id="S6.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="2986" height="1680" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Fig. 10: </span>Qualitative results of the proposed multimodal EHOI detection system on the <span id="S6.F10.2.1" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> test data.</figcaption>
</figure>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Comparison with class-agnostic baselines</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">Table <a href="#S6.T5" title="Table 5 ‣ 6.4 Comparison with class-agnostic baselines ‣ 6 Experimental results ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> compares our system with different instances of the class-agnostic method introduced in <cite class="ltx_cite ltx_citemacro_citet">Shan et al. (<a href="#bib.bib45" title="" class="ltx_ref">2020</a>)</cite>. Henceforth, we will refer to this method as <span id="S6.SS4.p1.1.1" class="ltx_text ltx_font_italic">Hands In Contact</span> (HIC). Since HIC is class agnostic, to compare our method with it, we extend it to recognize the active object classes following two different approaches. In the first approach, we used a Resnet-18 CNN <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib22" title="" class="ltx_ref">2016b</a>)</cite> to classify image patches extracted from the active object bounding boxes. We trained the classifier with four different sets of data: 1) <span id="S6.SS4.p1.1.2" class="ltx_text ltx_font_italic">BS1</span>: we sampled 20,000 frames from 19 videos where a single object of each class is shot at a time. This collection provides a minimal training set that can be collected with a modest labeling effort (comparable with the time needed for acquiring 3D models of the objects in our pipeline); 2) <span id="S6.SS4.p1.1.3" class="ltx_text ltx_font_italic">BS2</span>: we used images from the proposed <span id="S6.SS4.p1.1.4" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> training set; 3) <span id="S6.SS4.p1.1.5" class="ltx_text ltx_font_italic">BS3</span>: we used images from the proposed <span id="S6.SS4.p1.1.6" class="ltx_text ltx_font_italic">EgoISM-HOI-Synth</span> training set; 4) <span id="S6.SS4.p1.1.7" class="ltx_text ltx_font_italic">BS4</span>: we used all <span id="S6.SS4.p1.1.8" class="ltx_text ltx_font_italic">EgoISM-HOI</span> data. The second approach (<span id="S6.SS4.p1.1.9" class="ltx_text ltx_font_italic">BS5</span>) exploits a YOLOv5<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>YOLOv5: <a target="_blank" href="https://github.com/ultralytics/yolov5" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ultralytics/yolov5</a></span></span></span> object detector, trained to recognize the considered objects (see Fig. <a href="#S3.F3" title="Fig. 3 ‣ 3 Proposed EHOI Generation Pipeline ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), to assign a label to the active objects predicted by HIC. Specifically, for each active object prediction, we select the class of the object with the highest <span id="S6.SS4.p1.1.10" class="ltx_text ltx_font_italic">IoU</span> among those predicted by the YOLOv5 object detector or discard the proposal if there are no box intersections. It is worth noting that HIC was pre-trained on the large-scale dataset <span id="S6.SS4.p1.1.11" class="ltx_text ltx_font_italic">100DOH</span>, which contains over 100K labeled frames of HOIs.</p>
</div>
<figure id="S6.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison between the proposed system and different baseline approaches based on HIC <cite class="ltx_cite ltx_citemacro_citep">(Shan et al., <a href="#bib.bib45" title="" class="ltx_ref">2020</a>)</cite>.</figcaption>
<div id="S6.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:177.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(18.8pt,-7.7pt) scale(1.09484436031899,1.09484436031899) ;">
<table id="S6.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T5.1.1.1.1" class="ltx_tr">
<th id="S6.T5.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Method</th>
<th id="S6.T5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S6.T5.1.1.1.1.2.1" class="ltx_text ltx_font_italic">EgoISM-HOI-Synth</span></th>
<th id="S6.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">EgoISM-HOI-Real %</th>
<th id="S6.T5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP Hand+All</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T5.1.1.2.1" class="ltx_tr">
<td id="S6.T5.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">Proposed (Base)</td>
<td id="S6.T5.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">Yes</td>
<td id="S6.T5.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S6.T5.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">23.28</td>
</tr>
<tr id="S6.T5.1.1.3.2" class="ltx_tr">
<td id="S6.T5.1.1.3.2.1" class="ltx_td ltx_align_left">Proposed (Base)</td>
<td id="S6.T5.1.1.3.2.2" class="ltx_td ltx_align_center">Yes</td>
<td id="S6.T5.1.1.3.2.3" class="ltx_td ltx_align_center">10</td>
<td id="S6.T5.1.1.3.2.4" class="ltx_td ltx_align_center"><span id="S6.T5.1.1.3.2.4.1" class="ltx_text ltx_framed ltx_framed_underline">30.65</span></td>
</tr>
<tr id="S6.T5.1.1.4.3" class="ltx_tr">
<td id="S6.T5.1.1.4.3.1" class="ltx_td ltx_align_left">Proposed (Full)</td>
<td id="S6.T5.1.1.4.3.2" class="ltx_td ltx_align_center">Yes</td>
<td id="S6.T5.1.1.4.3.3" class="ltx_td ltx_align_center">100</td>
<td id="S6.T5.1.1.4.3.4" class="ltx_td ltx_align_center"><span id="S6.T5.1.1.4.3.4.1" class="ltx_text ltx_font_bold">36.51</span></td>
</tr>
<tr id="S6.T5.1.1.5.4" class="ltx_tr">
<td id="S6.T5.1.1.5.4.1" class="ltx_td ltx_align_left">HIC+RESNET (BS1)</td>
<td id="S6.T5.1.1.5.4.2" class="ltx_td ltx_align_center">No</td>
<td id="S6.T5.1.1.5.4.3" class="ltx_td ltx_align_center">100*</td>
<td id="S6.T5.1.1.5.4.4" class="ltx_td ltx_align_center">09.92</td>
</tr>
<tr id="S6.T5.1.1.6.5" class="ltx_tr">
<td id="S6.T5.1.1.6.5.1" class="ltx_td ltx_align_left">HIC+RESNET (BS2)</td>
<td id="S6.T5.1.1.6.5.2" class="ltx_td ltx_align_center">No</td>
<td id="S6.T5.1.1.6.5.3" class="ltx_td ltx_align_center">100</td>
<td id="S6.T5.1.1.6.5.4" class="ltx_td ltx_align_center">22.18</td>
</tr>
<tr id="S6.T5.1.1.7.6" class="ltx_tr">
<td id="S6.T5.1.1.7.6.1" class="ltx_td ltx_align_left">HIC+RESNET (BS3)</td>
<td id="S6.T5.1.1.7.6.2" class="ltx_td ltx_align_center">Yes</td>
<td id="S6.T5.1.1.7.6.3" class="ltx_td ltx_align_center">0</td>
<td id="S6.T5.1.1.7.6.4" class="ltx_td ltx_align_center">16.39</td>
</tr>
<tr id="S6.T5.1.1.8.7" class="ltx_tr">
<td id="S6.T5.1.1.8.7.1" class="ltx_td ltx_align_left">HIC+RESNET (BS4)</td>
<td id="S6.T5.1.1.8.7.2" class="ltx_td ltx_align_center">Yes</td>
<td id="S6.T5.1.1.8.7.3" class="ltx_td ltx_align_center">100</td>
<td id="S6.T5.1.1.8.7.4" class="ltx_td ltx_align_center">23.59</td>
</tr>
<tr id="S6.T5.1.1.9.8" class="ltx_tr">
<td id="S6.T5.1.1.9.8.1" class="ltx_td ltx_align_left ltx_border_b">HIC+YOLOv5 (BS5)</td>
<td id="S6.T5.1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_b">Yes</td>
<td id="S6.T5.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_b">100</td>
<td id="S6.T5.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_b">20.62</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S6.SS4.p2" class="ltx_para">
<p id="S6.SS4.p2.1" class="ltx_p">The best model of the proposed EHOI detection method (row 3) outperforms all the baselines (rows 4-8) with significant improvements ranging from +12.92% (36.51 vs 23.59) to +26,59% (36.51 vs 9.92). The approach based on Resnet-18 (rows 4-7) leads to better performance compared to the method based on the YOLOv5 object detector (row 8). Indeed, considering only the baselines (rows 4-8), the best result is achieved by BS4 (row 7), which was pre-trained using synthetic and real-world <span id="S6.SS4.p2.1.1" class="ltx_text ltx_font_italic">EgoISM-HOI</span> data, with an improvement of +2.97% (23.59 vs 20.62) over the BS5 (row 8). Interestingly, even the BS2 (row 5), which did not use synthetic data during training, obtained a higher result of +1.56% (22.18 vs 20.62) than the BS5 (row 8). These results suggest the limits of this simple approach. In addition, it is worth noting that the model pre-trained on <span id="S6.SS4.p2.1.2" class="ltx_text ltx_font_italic">EgoISM-HOI-Synth</span> and fine-tuned using 10% of the <span id="S6.SS4.p2.1.3" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> training set (row 2) outperforms all the baseline approaches (rows 4-8), with an improvement of +7,06% (30.65 vs 23.59) over the BS4 (row 7). Worth mentioning that the model trained only on <span id="S6.SS4.p2.1.4" class="ltx_text ltx_font_italic">EgoISM-HOI-Synth</span> (row 1) achieves comparable results to the best baseline approach (row 7).</p>
</div>
</section>
<section id="S6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Additional results</h3>

<div id="S6.SS5.p1" class="ltx_para">
<p id="S6.SS5.p1.1" class="ltx_p">In this section, we show an additional set of experiments with the aim of 1) demonstrating how using domain-specific synthetic data improves the performance of a system pre-trained on an out-of-domain large-scale dataset (Section <a href="#S6.SS5.SSS1" title="6.5.1 Pre-training on 100 Days Of Hands ‣ 6.5 Additional results ‣ 6 Experimental results ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.5.1</span></a>), 2) showing the potential of using synthetic data for the related task of <span id="S6.SS5.p1.1.1" class="ltx_text ltx_font_italic">Object Detection</span> (Section <a href="#S6.SS5.SSS2" title="6.5.2 Object Detection ‣ 6.5 Additional results ‣ 6 Experimental results ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.5.2</span></a>). Similar to the set of experiments in Section <a href="#S6.SS2" title="6.2 The Impact of Synthetic Data on System Performance ‣ 6 Experimental results ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2</span></a>, we deactivated the <span id="S6.SS5.p1.1.2" class="ltx_text ltx_font_italic">multimodal hand state classifier</span>, <span id="S6.SS5.p1.1.3" class="ltx_text ltx_font_italic">monocular depth estimation branch</span>, and <span id="S6.SS5.p1.1.4" class="ltx_text ltx_font_italic">instance segmentation branch</span> modules for a fair comparison.</p>
</div>
<section id="S6.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.5.1 </span>Pre-training on 100 Days Of Hands</h4>

<div id="S6.SS5.SSS1.p1" class="ltx_para">
<p id="S6.SS5.SSS1.p1.1" class="ltx_p">To further demonstrate the utility of synthetic data, we performed an additional experiment in which we pre-trained different models on the large-scale 100DOH dataset and then fine-tuned them on our EgoISM-HOI dataset. The goal of this experiment is to demonstrate how the use of domain-specific synthetic data further increases the performance of a system pre-trained on a large amount of out-of-domain real data.</p>
</div>
<figure id="S6.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>EHOI detection results on <span id="S6.T6.2.1" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> test data of models pre-trained on 100DOH dataset.</figcaption>
<div id="S6.T6.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:65pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-26.8pt,4.0pt) scale(0.889931644822395,0.889931644822395) ;">
<table id="S6.T6.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T6.3.1.1.1" class="ltx_tr">
<th id="S6.T6.3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Pre-training</th>
<th id="S6.T6.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Fine-tuning</th>
<th id="S6.T6.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP Hand</th>
<th id="S6.T6.3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP H+Side</th>
<th id="S6.T6.3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AP H+State</th>
<th id="S6.T6.3.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP H+Obj</th>
<th id="S6.T6.3.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP H+All</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T6.3.1.2.1" class="ltx_tr">
<th id="S6.T6.3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">100DOH</th>
<td id="S6.T6.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">EgoISM-HOI-Synth</td>
<td id="S6.T6.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">90.78</td>
<td id="S6.T6.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">89.88</td>
<td id="S6.T6.3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">35.46</td>
<td id="S6.T6.3.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T6.3.1.2.1.6.1" class="ltx_text ltx_framed ltx_framed_underline">23.47</span></td>
<td id="S6.T6.3.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T6.3.1.2.1.7.1" class="ltx_text ltx_framed ltx_framed_underline">23.19</span></td>
</tr>
<tr id="S6.T6.3.1.3.2" class="ltx_tr">
<th id="S6.T6.3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">100DOH</th>
<td id="S6.T6.3.1.3.2.2" class="ltx_td ltx_align_center">EgoISM-HOI-Real</td>
<td id="S6.T6.3.1.3.2.3" class="ltx_td ltx_align_center"><span id="S6.T6.3.1.3.2.3.1" class="ltx_text ltx_font_bold">90.87</span></td>
<td id="S6.T6.3.1.3.2.4" class="ltx_td ltx_align_center"><span id="S6.T6.3.1.3.2.4.1" class="ltx_text ltx_framed ltx_framed_underline">90.44</span></td>
<td id="S6.T6.3.1.3.2.5" class="ltx_td ltx_align_center"><span id="S6.T6.3.1.3.2.5.1" class="ltx_text ltx_font_bold">59.25</span></td>
<td id="S6.T6.3.1.3.2.6" class="ltx_td ltx_align_center">18.10</td>
<td id="S6.T6.3.1.3.2.7" class="ltx_td ltx_align_center">17.69</td>
</tr>
<tr id="S6.T6.3.1.4.3" class="ltx_tr">
<th id="S6.T6.3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">100DOH</th>
<td id="S6.T6.3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b">EgoISM-HOI</td>
<td id="S6.T6.3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S6.T6.3.1.4.3.3.1" class="ltx_text ltx_framed ltx_framed_underline">90.86</span></td>
<td id="S6.T6.3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S6.T6.3.1.4.3.4.1" class="ltx_text ltx_font_bold">90.51</span></td>
<td id="S6.T6.3.1.4.3.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S6.T6.3.1.4.3.5.1" class="ltx_text ltx_framed ltx_framed_underline">58.87</span></td>
<td id="S6.T6.3.1.4.3.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S6.T6.3.1.4.3.6.1" class="ltx_text ltx_font_bold">38.54</span></td>
<td id="S6.T6.3.1.4.3.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S6.T6.3.1.4.3.7.1" class="ltx_text ltx_font_bold">37.37</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S6.SS5.SSS1.p2" class="ltx_para">
<p id="S6.SS5.SSS1.p2.1" class="ltx_p">Using synthetic and real-world training data (row 3) leads to the best or second-best results for all the evaluation metrics. In particular, the improvements over the model which uses only <span id="S6.SS5.SSS1.p2.1.1" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> data (rows 2) are significant in the metrics affected by the active objects with a +20,44% (38.54 vs 18.10) for the <span id="S6.SS5.SSS1.p2.1.2" class="ltx_text ltx_font_italic">mAP Hand+Obj</span> and +19,68 (37.37 vs 17.69) for the <span id="S6.SS5.SSS1.p2.1.3" class="ltx_text ltx_font_italic">mAP Hand+All</span>. Considering the <span id="S6.SS5.SSS1.p2.1.4" class="ltx_text ltx_font_italic">mAP Hand+All</span> metric, it is worth noting that the model trained only on <span id="S6.SS5.SSS1.p2.1.5" class="ltx_text ltx_font_italic">EgoISM-HOI-Synth</span> (row 1) surpasses the model trained on the <span id="S6.SS5.SSS1.p2.1.6" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> training data (row 2) with an improvement of +5,5% (23.19 vs 17.69).</p>
</div>
</section>
<section id="S6.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.5.2 </span>Object Detection</h4>

<div id="S6.SS5.SSS2.p1" class="ltx_para">
<p id="S6.SS5.SSS2.p1.1" class="ltx_p">We performed an additional experiment to assess the utility of using synthetic data for the related task of <span id="S6.SS5.SSS2.p1.1.1" class="ltx_text ltx_font_italic">Object Detection</span>. The <span id="S6.SS5.SSS2.p1.1.2" class="ltx_text ltx_font_italic">mean Average Precision metric<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note"><span id="footnote10.1.1.1" class="ltx_text ltx_font_upright">10</span></span><span id="footnote10.5" class="ltx_text ltx_font_upright">We used the following implementation: </span><a target="_blank" href="https://github.com/cocodataset/cocoapi" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_upright">https://github.com/cocodataset/cocoapi</a></span></span></span></span> with an <span id="S6.SS5.SSS2.p1.1.3" class="ltx_text ltx_font_italic">IoU</span> threshold of <math id="S6.SS5.SSS2.p1.1.m1.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S6.SS5.SSS2.p1.1.m1.1a"><mn id="S6.SS5.SSS2.p1.1.m1.1.1" xref="S6.SS5.SSS2.p1.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S6.SS5.SSS2.p1.1.m1.1b"><cn type="float" id="S6.SS5.SSS2.p1.1.m1.1.1.cmml" xref="S6.SS5.SSS2.p1.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS5.SSS2.p1.1.m1.1c">0.5</annotation></semantics></math> (<span id="S6.SS5.SSS2.p1.1.4" class="ltx_text ltx_font_italic">mAP@50</span>) was used as the evaluation criterion.</p>
</div>
<figure id="S6.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Object detection results on the <span id="S6.T7.2.1" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> test data.</figcaption>
<div id="S6.T7.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:293.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(83.7pt,-56.6pt) scale(1.62844676634836,1.62844676634836) ;">
<table id="S6.T7.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T7.3.1.1.1" class="ltx_tr">
<th id="S6.T7.3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">EgoISM-HOI Synth</th>
<th id="S6.T7.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">EgoISM-HOI Real%</th>
<th id="S6.T7.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mAP@50%</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T7.3.1.2.1" class="ltx_tr">
<th id="S6.T7.3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Yes</th>
<td id="S6.T7.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S6.T7.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">66.58</td>
</tr>
<tr id="S6.T7.3.1.3.2" class="ltx_tr">
<th id="S6.T7.3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Yes</th>
<td id="S6.T7.3.1.3.2.2" class="ltx_td ltx_align_center">10</td>
<td id="S6.T7.3.1.3.2.3" class="ltx_td ltx_align_center">76.29</td>
</tr>
<tr id="S6.T7.3.1.4.3" class="ltx_tr">
<th id="S6.T7.3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Yes</th>
<td id="S6.T7.3.1.4.3.2" class="ltx_td ltx_align_center">25</td>
<td id="S6.T7.3.1.4.3.3" class="ltx_td ltx_align_center">78.48</td>
</tr>
<tr id="S6.T7.3.1.5.4" class="ltx_tr">
<th id="S6.T7.3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Yes</th>
<td id="S6.T7.3.1.5.4.2" class="ltx_td ltx_align_center">50</td>
<td id="S6.T7.3.1.5.4.3" class="ltx_td ltx_align_center"><span id="S6.T7.3.1.5.4.3.1" class="ltx_text ltx_framed ltx_framed_underline">79.68</span></td>
</tr>
<tr id="S6.T7.3.1.6.5" class="ltx_tr">
<th id="S6.T7.3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Yes</th>
<td id="S6.T7.3.1.6.5.2" class="ltx_td ltx_align_center">100</td>
<td id="S6.T7.3.1.6.5.3" class="ltx_td ltx_align_center"><span id="S6.T7.3.1.6.5.3.1" class="ltx_text ltx_font_bold">81.06</span></td>
</tr>
<tr id="S6.T7.3.1.7.6" class="ltx_tr">
<th id="S6.T7.3.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">No</th>
<td id="S6.T7.3.1.7.6.2" class="ltx_td ltx_align_center">10</td>
<td id="S6.T7.3.1.7.6.3" class="ltx_td ltx_align_center">68.41</td>
</tr>
<tr id="S6.T7.3.1.8.7" class="ltx_tr">
<th id="S6.T7.3.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">No</th>
<td id="S6.T7.3.1.8.7.2" class="ltx_td ltx_align_center">25</td>
<td id="S6.T7.3.1.8.7.3" class="ltx_td ltx_align_center">71.59</td>
</tr>
<tr id="S6.T7.3.1.9.8" class="ltx_tr">
<th id="S6.T7.3.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">No</th>
<td id="S6.T7.3.1.9.8.2" class="ltx_td ltx_align_center">50</td>
<td id="S6.T7.3.1.9.8.3" class="ltx_td ltx_align_center">73.33</td>
</tr>
<tr id="S6.T7.3.1.10.9" class="ltx_tr">
<th id="S6.T7.3.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">No</th>
<td id="S6.T7.3.1.10.9.2" class="ltx_td ltx_align_center ltx_border_b">100</td>
<td id="S6.T7.3.1.10.9.3" class="ltx_td ltx_align_center ltx_border_b">72.97</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S6.SS5.SSS2.p2" class="ltx_para">
<p id="S6.SS5.SSS2.p2.1" class="ltx_p">The results are shown in Table <a href="#S6.T7" title="Table 7 ‣ 6.5.2 Object Detection ‣ 6.5 Additional results ‣ 6 Experimental results ‣ Exploiting Multimodal Synthetic Data for Egocentric Human-Object Interaction Detection in an Industrial Scenario" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. The models trained using synthetic and real-world data (rows 1-5) outperform all the corresponding models trained only on the real-world training set (rows 6-9). In particular, the best result of 81.06% was obtained by the model pre-trained on <span id="S6.SS5.SSS2.p2.1.1" class="ltx_text ltx_font_italic">EgoISM-HOI-Synth</span> training set and fine-tuned with 100% of <span id="S6.SS5.SSS2.p2.1.2" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> training data (row 5), with an improvement of +7.73% (81.06 vs 73.33) over the model which obtains the best results among the ones trained only on <span id="S6.SS5.SSS2.p2.1.3" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> (row 8). Furthermore, it is worth noting that the model pre-trained using <span id="S6.SS5.SSS2.p2.1.4" class="ltx_text ltx_font_italic">EgoISM-HOI-Synth</span> and fine-tuned with only 10% of the <span id="S6.SS5.SSS2.p2.1.5" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span> training set (row 2) surpasses all the models fine-tuned using only <span id="S6.SS5.SSS2.p2.1.6" class="ltx_text ltx_font_italic">EgoISM-HOI-Real</span>.</p>
</div>
</section>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We studied egocentric human-object interactions in an industrial domain. Due to the expensiveness of collecting and labeling real in-domain data in the considered context, we proposed a pipeline and a tool that leverages 3D models of the objects and the considered environment to generate synthetic images of EHOIs automatically labeled and additional data signals, such as depth maps and instance segmentation masks. Exploiting our pipeline, we presented <span id="S7.p1.1.1" class="ltx_text ltx_font_italic">EgoISM-HOI</span>, a new multimodal dataset of synthetic and real EHOI images in an industrial scenario with rich annotations of hands and objects. We investigated the potential of using multimodal synthetic data to pre-train an EHOI detection system and demonstrated that our proposed method outperforms class-agnostic baselines based on the state-of-the-art method of <cite class="ltx_cite ltx_citemacro_citet">Shan et al. (<a href="#bib.bib45" title="" class="ltx_ref">2020</a>)</cite>. Future work will investigate how the knowledge inferred by our method can be valuable for other related tasks such as next active object detection or action recognition. To encourage research on the topic, we publicly released the datasets and the source code of the proposed system, together with pre-trained models, on our project web page: <a target="_blank" href="https://iplab.dmi.unict.it/egoism-hoi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://iplab.dmi.unict.it/egoism-hoi</a>.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This research is supported by Next Vision<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>Next Vision: https://www.nextvisionlab.it/</span></span></span> s.r.l., by MISE - PON I&amp;C 2014-2020 - Progetto ENIGMA - Prog n. F/190050/02/X44 – CUP: B61B19000520008, and by the project Future Artificial Intelligence Research (FAIR) – PNRR MUR Cod. PE0000013 - CUP: E63C22001940006.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bambach et al. (2015)</span>
<span class="ltx_bibblock">
Bambach, S., Lee, S.,
Crandall, D.J., Yu, C.,
2015.

</span>
<span class="ltx_bibblock">Lending a hand: Detecting hands and recognizing
activities in complex egocentric interactions, in:
International Conference on Computer Vision, pp.
1949–1957.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Benavent-Lledo et al. (2022)</span>
<span class="ltx_bibblock">
Benavent-Lledo, M., Oprea, S.,
Castro-Vargas, J.A., Mulero-Perez, D.,
Garcia-Rodriguez, J., 2022.

</span>
<span class="ltx_bibblock">Predicting human-object interactions in egocentric
videos, in: International Joint Conference on Neural
Networks, pp. 1–7.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhatnagar et al. (2022)</span>
<span class="ltx_bibblock">
Bhatnagar, B.L., Xie, X.,
Petrov, I., Sminchisescu, C.,
Theobalt, C., Pons-Moll, G.,
2022.

</span>
<span class="ltx_bibblock">Behave: Dataset and method for tracking human object
interactions, in: Conference on Computer Vision and
Pattern Recognition, pp. 15935–15946.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bochkovskiy et al. (2020)</span>
<span class="ltx_bibblock">
Bochkovskiy, A., Wang, C.Y.,
Liao, H.Y.M., 2020.

</span>
<span class="ltx_bibblock">Yolov4: Optimal speed and accuracy of object
detection.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://arxiv.org/abs/2004.10934" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2004.10934</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chao et al. (2018)</span>
<span class="ltx_bibblock">
Chao, Y.W., Liu, Y., Liu,
X., Zeng, H., Deng, J.,
2018.

</span>
<span class="ltx_bibblock">Learning to detect human-object interactions, in:
Winter Conference on Applications of Computer Vision,
pp. 381–389.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chao et al. (2015)</span>
<span class="ltx_bibblock">
Chao, Y.W., Wang, Z., He,
Y., Wang, J., Deng, J.,
2015.

</span>
<span class="ltx_bibblock">Hico: A benchmark for recognizing human-object
interactions in images, in: International Conference on
Computer Vision, pp. 1017–1025.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Damen et al. (2021)</span>
<span class="ltx_bibblock">
Damen, D., Doughty, H.,
Farinella, G.M., , Furnari, A.,
Ma, J., Kazakos, E.,
Moltisanti, D., Munro, J.,
Perrett, T., Price, W.,
Wray, M., 2021.

</span>
<span class="ltx_bibblock">Rescaling egocentric vision: Collection, pipeline and
challenges for epic-kitchens-100.

</span>
<span class="ltx_bibblock">International Journal of Computer Vision ,
1–23.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Damen et al. (2018)</span>
<span class="ltx_bibblock">
Damen, D., Doughty, H.,
Farinella, G.M., Fidler, S.,
Furnari, A., Kazakos, E.,
Moltisanti, D., Munro, J.,
Perrett, T., Price, W.,
Wray, M., 2018.

</span>
<span class="ltx_bibblock">Scaling egocentric vision: The epic-kitchens
dataset, in: European Conference on Computer Vision,
pp. 720–736.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Damen et al. (2014)</span>
<span class="ltx_bibblock">
Damen, D., Leelasawassuk, T.,
Haines, O., Calway, A.,
Mayol-Cuevas, W.W., 2014.

</span>
<span class="ltx_bibblock">You-do, i-learn: Discovering task relevant objects
and their modes of interaction from multi-user egocentric video., in:
Proceedings of the British Machine Vision Conference,
p. 3.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Darkhalil et al. (2022)</span>
<span class="ltx_bibblock">
Darkhalil, A., Shan, D.,
Zhu, B., Ma, J., Kar,
A., Higgins, R., Fidler, S.,
Fouhey, D., Damen, D.,
2022.

</span>
<span class="ltx_bibblock">Epic-kitchens visor benchmark: Video segmentations
and object relations, in: Conference on Neural
Information Processing Systems Datasets and Benchmarks Track.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ehsani et al. (2021)</span>
<span class="ltx_bibblock">
Ehsani, K., Han, W.,
Herrasti, A., VanderBilt, E.,
Weihs, L., Kolve, E.,
Kembhavi, A., Mottaghi, R.,
2021.

</span>
<span class="ltx_bibblock">Manipulathor: A framework for visual object
manipulation, in: Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pp. 4497–4506.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Everingham et al. (2009)</span>
<span class="ltx_bibblock">
Everingham, M., Van Gool, L.,
Williams, C.K., Winn, J.,
Zisserman, A., 2009.

</span>
<span class="ltx_bibblock">The pascal visual object classes (voc) challenge.

</span>
<span class="ltx_bibblock">International journal of computer vision
88, 303–308.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Farinella et al. (2019)</span>
<span class="ltx_bibblock">
Farinella, G.M., Signorello, G.,
Battiato, S., Furnari, A.,
Ragusa, F., Leonardi, R.,
Ragusa, E., Scuderi, E.,
Lopes, A., Santo, L., et al.,
2019.

</span>
<span class="ltx_bibblock">Vedi: Vision exploitation for data interpretation,
in: Image Analysis and Processing–ICIAP 2019: 20th
International Conference, Trento, Italy, September 9–13, 2019, Proceedings,
Part II 20, pp. 753–763.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al. (2022)</span>
<span class="ltx_bibblock">
Fu, Q., Liu, X., Kitani,
K.M., 2022.

</span>
<span class="ltx_bibblock">Sequential voting with relational box fields for
active object detection, in: Conference on Computer
Vision and Pattern Recognition, pp. 2374–2383.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2018)</span>
<span class="ltx_bibblock">
Gao, C., Zou, Y., Huang,
J.B., 2018.

</span>
<span class="ltx_bibblock">ican: Instance-centric attention network for
human-object interaction detection, in: British Machine
Vision Conference.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gkioxari et al. (2018)</span>
<span class="ltx_bibblock">
Gkioxari, G., Girshick, R.,
Dollár, P., He, K.,
2018.

</span>
<span class="ltx_bibblock">Detecting and recognizing human-object interactions,
in: Conference on Computer Vision and Pattern
Recognition, pp. 8359–8367.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grauman et al. (2021)</span>
<span class="ltx_bibblock">
Grauman, K., Westbury, A.,
Byrne, E., Chavis, Z.Q.,
Furnari, A., Girdhar, R.,
Hamburger, J., Jiang, H.,
Liu, M., Liu, X.,
Martin, M., Nagarajan, T.,
Radosavovic, I., Ramakrishnan, S.K.,
Ryan, F., Sharma, J.,
Wray, M., Xu, M., Xu,
E.Z., Zhao, C., Bansal, S.,
Batra, D., Cartillier, V.,
Crane, S., Do, T.,
Doulaty, M., Erapalli, A.,
Feichtenhofer, C., Fragomeni, A.,
Fu, Q., Fuegen, C.,
Gebreselasie, A., González, C.,
Hillis, J.M., Huang, X.,
Huang, Y., Jia, W.,
Khoo, W.Y.H., Kolár, J.,
Kottur, S., Kumar, A.,
Landini, F., Li, C., Li,
Y., Li, Z., Mangalam, K.,
Modhugu, R., Munro, J.,
Murrell, T., Nishiyasu, T.,
Price, W., Puentes, P.R.,
Ramazanova, M., Sari, L.,
Somasundaram, K.K., Southerland, A.,
Sugano, Y., Tao, R., Vo,
M., Wang, Y., Wu, X.,
Yagi, T., Zhu, Y.,
Arbeláez, P., Crandall, D.J.,
Damen, D., Farinella, G.M.,
Ghanem, B., Ithapu, V.K.,
Jawahar, C.V., Joo, H.,
Kitani, K., Li, H.,
Newcombe, R.A., Oliva, A.,
Park, H.S., Rehg, J.M.,
Sato, Y., Shi, J., Shou,
M.Z., Torralba, A., Torresani, L.,
Yan, M., Malik, J., 2021.

</span>
<span class="ltx_bibblock">Ego4d: Around the world in 3,000 hours of egocentric
video, in: Conference on Computer Vision and Pattern
Recognition, pp. 18995–19012.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta and Malik (2015)</span>
<span class="ltx_bibblock">
Gupta, S., Malik, J., 2015.

</span>
<span class="ltx_bibblock">Visual semantic role labeling.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://arxiv.org/abs/1505.04474" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1505.04474</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hasson et al. (2019)</span>
<span class="ltx_bibblock">
Hasson, Y., Varol, G.,
Tzionas, D., Kalevatykh, I.,
Black, M., Laptev, I.,
Schmid, C., 2019.

</span>
<span class="ltx_bibblock">Learning joint reconstruction of hands and
manipulated objects, in: Conference on Computer Vision
and Pattern Recognition, pp. 11807–11816.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2017)</span>
<span class="ltx_bibblock">
He, K., Gkioxari, G.,
Dollár, P., Girshick, R.,
2017.

</span>
<span class="ltx_bibblock">Mask r-cnn, in: International
Conference on Computer Vision, pp. 2961–2969.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016a)</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren,
S., Sun, J., 2016a.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition, in:
Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 770–778.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016b)</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren,
S., Sun, J., 2016b.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition, in:
Conference on Computer Vision and Pattern Recognition,
pp. 770–778.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hwang et al. (2020)</span>
<span class="ltx_bibblock">
Hwang, H., Jang, C., Park,
G., Cho, J., Kim, I.J.,
2020.

</span>
<span class="ltx_bibblock">Eldersim: A synthetic data generation platform for
human action recognition in eldercare applications.

</span>
<span class="ltx_bibblock">IEEE Access .

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kolve et al. (2017)</span>
<span class="ltx_bibblock">
Kolve, E., Mottaghi, R.,
Han, W., VanderBilt, E.,
Weihs, L., Herrasti, A.,
Gordon, D., Zhu, Y.,
Gupta, A., Farhadi, A.,
2017.

</span>
<span class="ltx_bibblock">Ai2-thor: An interactive 3d environment for visual
ai.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://arxiv.org/abs/1712.05474" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1712.05474</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuznetsova et al. (2020)</span>
<span class="ltx_bibblock">
Kuznetsova, A., Rom, H.,
Alldrin, N., Uijlings, J.,
Krasin, I., Pont-Tuset, J.,
Kamali, S., Popov, S.,
Malloci, M., Kolesnikov, A., et al.,
2020.

</span>
<span class="ltx_bibblock">The open images dataset v4: Unified image
classification, object detection, and visual relationship detection at
scale.

</span>
<span class="ltx_bibblock">International Journal of Computer Vision
128, 1956–1981.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021)</span>
<span class="ltx_bibblock">
Li, Y., Liu, M., Rehg,
J.M., 2021.

</span>
<span class="ltx_bibblock">In the eye of the beholder: Gaze and actions in first
person video.

</span>
<span class="ltx_bibblock">IEEE transactions on pattern analysis and machine
intelligence .

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020)</span>
<span class="ltx_bibblock">
Li, Y.L., Liu, X., Lu,
H., Wang, S., Liu, J.,
Li, J., Lu, C., 2020.

</span>
<span class="ltx_bibblock">Detailed 2d-3d joint representation for human-object
interaction, in: Conference on Computer Vision and
Pattern Recognition, pp. 10166–10175.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et al. (2020)</span>
<span class="ltx_bibblock">
Liao, Y., Liu, S., Wang,
F., Chen, Y., Feng, J.,
2020.

</span>
<span class="ltx_bibblock">Ppdm: Parallel point detection and matching for
real-time human-object interaction detection, in:
Conference on Computer Vision and Pattern Recognition,
pp. 479–487.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2017)</span>
<span class="ltx_bibblock">
Lin, T.Y., Dollár, P.,
Girshick, R., He, K.,
Hariharan, B., Belongie, S.,
2017.

</span>
<span class="ltx_bibblock">Feature pyramid networks for object detection, in:
Conference on Computer Vision and Pattern Recognition,
pp. 2117–2125.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Lin, T.Y., Maire, M.,
Belongie, S.J., Hays, J.,
Perona, P., Ramanan, D.,
Dollár, P., Zitnick, C.L.,
2014.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context, in:
European Conference on Computer Vision, pp.
740–755.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022)</span>
<span class="ltx_bibblock">
Liu, Y., Liu, Y., Jiang,
C., Lyu, K., Wan, W.,
Shen, H., Liang, B., Fu,
Z., Wang, H., Yi, L.,
2022.

</span>
<span class="ltx_bibblock">Hoi4d: A 4d egocentric dataset for category-level
human-object interaction, in: Conference on Computer
Vision and Pattern Recognition, pp. 21013–21022.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu and Mayol-Cuevas (2021)</span>
<span class="ltx_bibblock">
Lu, Y., Mayol-Cuevas, W.W.,
2021.

</span>
<span class="ltx_bibblock">Egocentric hand-object interaction detection and
application.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://arxiv.org/abs/2109.14734" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2109.14734</a>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2023)</span>
<span class="ltx_bibblock">
Ma, S., Wang, Y., Wang,
S., Wei, Y., 2023.

</span>
<span class="ltx_bibblock">Fgahoi: Fine-grained anchors for human-object
interaction detection.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://arxiv.org/abs/2301.04019" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2301.04019</a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mazzamuto et al. (2023)</span>
<span class="ltx_bibblock">
Mazzamuto, M., Ragusa, F.,
Resta, A., Farinella, G.M.,
Furnari, A., 2023.

</span>
<span class="ltx_bibblock">A wearable device application for human-object
interactions detection, in: International Conference on
Computer Vision Theory and Applications, pp. 664–671.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller and Allen (2004)</span>
<span class="ltx_bibblock">
Miller, A.T., Allen, P.K.,
2004.

</span>
<span class="ltx_bibblock">Graspit! a versatile simulator for robotic grasping.

</span>
<span class="ltx_bibblock">IEEE Robotics &amp; Automation Magazine
11, 110–122.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mueller et al. (2017)</span>
<span class="ltx_bibblock">
Mueller, F., Mehta, D.,
Sotnychenko, O., Sridhar, S.,
Casas, D., Theobalt, C.,
2017.

</span>
<span class="ltx_bibblock">Real-time hand tracking under occlusion from an
egocentric rgb-d sensor, in: International Conference on
Computer Vision, pp. 1154–1163.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Quattrocchi et al. (2023)</span>
<span class="ltx_bibblock">
Quattrocchi, C., Mauro, D.D.,
Furnari, A., Lopes, A.,
Moltisanti, M., Farinella, G.M.,
2023.

</span>
<span class="ltx_bibblock">Put your ppe on: A tool for synthetic data generation
and related benchmark in construction site scenarios, in:
International Conference on Computer Vision Theory and
Applications, pp. 656–663.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ragusa et al. (2022)</span>
<span class="ltx_bibblock">
Ragusa, F., Furnari, A.,
Farinella, G.M., 2022.

</span>
<span class="ltx_bibblock">Meccano: A multimodal egocentric dataset for humans
behavior understanding in the industrial-like domain.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://arxiv.org/abs/2209.08691" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2209.08691</a>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ragusa et al. (2021)</span>
<span class="ltx_bibblock">
Ragusa, F., Furnari, A.,
Livatino, S., Farinella, G.M.,
2021.

</span>
<span class="ltx_bibblock">The meccano dataset: Understanding human-object
interactions from egocentric videos in an industrial-like domain, in:
Winter Conference on Applications of Computer Vision,
pp. 1569–1578.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ranftl et al. (2022)</span>
<span class="ltx_bibblock">
Ranftl, R., Lasinger, K.,
Hafner, D., Schindler, K.,
Koltun, V., 2022.

</span>
<span class="ltx_bibblock">Towards robust monocular depth estimation: Mixing
datasets for zero-shot cross-dataset transfer.

</span>
<span class="ltx_bibblock">IEEE Transactions on Pattern Analysis and Machine
Intelligence 44, 1623–1637.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2015)</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick,
R., Sun, J., 2015.

</span>
<span class="ltx_bibblock">Faster R-CNN: Towards real-time object detection
with region proposal networks.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems
28.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Russakovsky et al. (2015)</span>
<span class="ltx_bibblock">
Russakovsky, O., Deng, J.,
Su, H., Krause, J.,
Satheesh, S., Ma, S.,
Huang, Z., Karpathy, A.,
Khosla, A., Bernstein, M., et al.,
2015.

</span>
<span class="ltx_bibblock">Imagenet large scale visual recognition challenge.

</span>
<span class="ltx_bibblock">International journal of computer vision
115, 211–252.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Savva et al. (2019)</span>
<span class="ltx_bibblock">
Savva, M., Kadian, A.,
Maksymets, O., Zhao, Y.,
Wijmans, E., Jain, B.,
Straub, J., Liu, J.,
Koltun, V., Malik, J., et al.,
2019.

</span>
<span class="ltx_bibblock">Habitat: A platform for embodied ai research, in:
International Conference on Computer Vision, pp.
9339–9347.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sener et al. (2022)</span>
<span class="ltx_bibblock">
Sener, F., Chatterjee, D.,
Shelepov, D., He, K.,
Singhania, D., Wang, R.,
Yao, A., 2022.

</span>
<span class="ltx_bibblock">Assembly101: A large-scale multi-view video dataset
for understanding procedural activities, in: Conference
on Computer Vision and Pattern Recognition, pp.
21096–21106.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shan et al. (2020)</span>
<span class="ltx_bibblock">
Shan, D., Geng, J., Shu,
M., Fouhey, D.F., 2020.

</span>
<span class="ltx_bibblock">Understanding human hands in contact at internet
scale, in: Conference on Computer Vision and Pattern
Recognition, pp. 9869–9878.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Le (2021)</span>
<span class="ltx_bibblock">
Tan, M., Le, Q.V., 2021.

</span>
<span class="ltx_bibblock">Efficientnetv2: Smaller models and faster training,
in: International Conference on Machine Learning, pp.
10096–10106.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Unity Technologies (2020)</span>
<span class="ltx_bibblock">
Unity Technologies, 2020.

</span>
<span class="ltx_bibblock">Unity Perception package.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/Unity-Technologies/com.unity.perception" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Unity-Technologies/com.unity.perception</a>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Wang, R., Zhang, J., Chen,
J., Xu, Y., Li, P.,
Liu, T., Wang, H., 2022.

</span>
<span class="ltx_bibblock">Dexgraspnet: A large-scale robotic dexterous grasp
dataset for general objects based on simulation.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://arxiv.org/abs/2210.02697" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2210.02697</a>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2022)</span>
<span class="ltx_bibblock">
Wu, X., Li, Y.L., Liu,
X., Zhang, J., Wu, Y.,
Lu, C., 2022.

</span>
<span class="ltx_bibblock">Mining cross-person cues for body-part
interactiveness learning in hoi detection, in: European
Conference on Computer Vision, pp. 121–136.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al. (2020)</span>
<span class="ltx_bibblock">
Xia, F., Shen, W.B., Li,
C., Kasimbeg, P., Tchapmi, M.E.,
Toshev, A., Martín-Martín, R.,
Savarese, S., 2020.

</span>
<span class="ltx_bibblock">Interactive gibson benchmark: A benchmark for
interactive navigation in cluttered environments.

</span>
<span class="ltx_bibblock">IEEE Robotics and Automation Letters
5, 713–720.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2023)</span>
<span class="ltx_bibblock">
Ye, Y., Li, X., Gupta,
A., Mello, S.D., Birchfield, S.,
Song, J., Tulsiani, S.,
Liu, S., 2023.

</span>
<span class="ltx_bibblock">Affordance diffusion: Synthesizing hand-object
interactions, in: Conference on Computer Vision and
Pattern Recognition.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2015)</span>
<span class="ltx_bibblock">
Yu, F., Seff, A., Zhang,
Y., Song, S., Funkhouser, T.,
Xiao, J., 2015.

</span>
<span class="ltx_bibblock">Lsun: Construction of a large-scale image dataset
using deep learning with humans in the loop.

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://arxiv.org/abs/1506.03365" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1506.03365</a>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022a)</span>
<span class="ltx_bibblock">
Zhang, F.Z., Campbell, D.,
Gould, S., 2022a.

</span>
<span class="ltx_bibblock">Efficient two-stage detection of human-object
interactions with a novel unary-pairwise transformer, in:
Conference on Computer Vision and Pattern Recognition,
pp. 20104–20112.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022b)</span>
<span class="ltx_bibblock">
Zhang, L., Zhou, S.,
Stent, S., Shi, J.,
2022b.

</span>
<span class="ltx_bibblock">Fine-grained egocentric hand-object segmentation:
Dataset, model, and applications, in: European
Conference on Computer Vision, pp. 127–145.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.12151" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.12152" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.12152">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.12152" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.12153" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 23:03:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
