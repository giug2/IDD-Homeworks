<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation</title>
<!--Generated on Thu Aug 29 12:24:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.11512v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#S1" title="In IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#S2" title="In IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Pre-trained LLM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#S3" title="In IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Tokenizer Efficiency</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#S4" title="In IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#S4.SS1" title="In 4 Experiments ‣ IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Continuous Pre-training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#S4.SS2" title="In 4 Experiments ‣ IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Subsequent Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#S4.SS3" title="In 4 Experiments ‣ IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#S5" title="In IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">IKUN for WMT24 General MT Task: 
<br class="ltx_break"/>LLMs Are here for Multilingual Machine Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Baohao Liao<sup class="ltx_sup" id="id7.7.id1"><span class="ltx_text ltx_font_italic" id="id7.7.id1.1">1,2</span></sup>     Christian Herold<sup class="ltx_sup" id="id8.8.id2"><span class="ltx_text ltx_font_italic" id="id8.8.id2.1">2</span></sup>     Shahram Khadivi<sup class="ltx_sup" id="id9.9.id3"><span class="ltx_text ltx_font_italic" id="id9.9.id3.1">2</span></sup>     Christof Monz<sup class="ltx_sup" id="id10.10.id4"><span class="ltx_text ltx_font_italic" id="id10.10.id4.1">1</span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id11.11.id5"><span class="ltx_text ltx_font_italic" id="id11.11.id5.1">1</span></sup>Language Technology Lab, University of Amsterdam 
<br class="ltx_break"/><sup class="ltx_sup" id="id12.12.id6"><span class="ltx_text ltx_font_italic" id="id12.12.id6.1">2</span></sup>eBay Inc., Aachen, Germany 
<br class="ltx_break"/><a class="ltx_ref ltx_href ltx_font_typewriter" href="mailto:b.liao@uva.nl" title="">b.liao@uva.nl</a>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id13.id1">This paper introduces two multilingual systems, <span class="ltx_text ltx_font_italic" id="id13.id1.1">IKUN</span> and <span class="ltx_text ltx_font_italic" id="id13.id1.2">IKUN-C</span>, developed for the general machine translation task in WMT24. IKUN and IKUN-C represent an <span class="ltx_text ltx_font_italic" id="id13.id1.3">open system</span> and a <span class="ltx_text ltx_font_italic" id="id13.id1.4">constrained system</span>, respectively, built on Llama-3-8b and Mistral-7B-v0.3. Both systems are designed to handle all 11 language directions using a single model. According to automatic evaluation metrics, <span class="ltx_text ltx_font_bold" id="id13.id1.5">IKUN-C achieved 6 first-place and 3 second-place finishes among all constrained systems, while IKUN secured 1 first-place and 2 second-place finishes across both open and constrained systems</span>. These encouraging results suggest that large language models (LLMs) are nearing the level of proficiency required for effective multilingual machine translation. The systems are based on a two-stage approach: first, continuous pre-training on monolingual data in 10 languages, followed by fine-tuning on high-quality parallel data for 11 language directions. The primary difference between IKUN and IKUN-C lies in their monolingual pre-training strategy. IKUN-C is pre-trained using constrained monolingual data, whereas IKUN leverages monolingual data from the OSCAR dataset. In the second phase, both systems are fine-tuned on parallel data sourced from NTREX, Flores, and WMT16-23 for all 11 language pairs.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large language models (LLMs) <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib22" title="">2023</a>); Dubey et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib8" title="">2024</a>); Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib12" title="">2023</a>); OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib18" title="">2023</a>)</cite> serve as a crucial foundation for a wide range of applications. One significant advantage of LLMs is their ability to be applied across various tasks, thereby simplifying deployment processes. However, the application of LLMs to multilingual machine translation (MT) presents several challenges:</p>
</div>
<div class="ltx_para" id="S1.p2">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Most LLMs are pre-trained on one or a few dominant languages, making direct fine-tuning on multilingual data insufficient for ensuring optimal performance, particularly for low-resource languages, which are often underrepresented in the training data.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">It remains unclear whether these LLMs, primarily pre-trained on a limited number of languages, effectively facilitate transfer learning across different languages <cite class="ltx_cite ltx_citemacro_cite">Tan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib21" title="">2024</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">The large-scale nature of most LLMs presents significant challenges for efficient fine-tuning, particularly for researchers and practitioners with limited computational resources.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In the WMT24 general MT task <cite class="ltx_cite ltx_citemacro_cite">Kocmi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib14" title="">2024</a>)</cite>, our objective is to assess the capability of LLMs for multilingual MT, as an alternative to training bilingual systems from scratch <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib24" title="">2023</a>)</cite>. This paper provides a detailed account of how we developed our final multilingual system using LLMs for both the constrained and open tracks.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Firstly, we identified that certain LLMs exhibit inefficiencies in tokenizing sentences from languages that are underrepresented in the pre-training data. To address this, we extended the existing vocabulary to reduce the tokenized sentence length, thereby enhancing training efficiency. Secondly, we enriched the LLMs with knowledge across the 10 target languages through continued pre-training. This step is particularly crucial for underrepresented languages, as it facilitates transfer learning. Finally, we fine-tuned the models using high-quality parallel datasets across all 11 pairs.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Through this streamlined approach, our constrained multilingual system, IKUN-C, secured 6 first-place and 3 second-place rankings in the automatic evaluation. Our open multilingual system, IKUN, achieved 1 first-place and 2 second-place rankings across the open and constrained tracks. These encouraging results demonstrate that LLMs can be effectively adapted for multilingual MT, broadening access to speakers of diverse languages.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="173" id="S1.F1.g1" src="extracted/5821192/figures/bpe_length_ratio.png" width="586"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Tokenizer efficiency for various LLMs and languages. The larger the length ratio is, the less efficient the tokenizer is. We add new sub-words, from Chinese, Japanese, Hindi and Icelandic, to the Mistral-v0.3 vocabulary to construct the IKUN-C vocabulary. IKUN uses the Llama-3 tokenizer without any modification.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Pre-trained LLM</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">LLMs are pre-trained on extensive web-scale data, encompassing a vast repository of general knowledge applicable to various tasks. Previous studies <cite class="ltx_cite ltx_citemacro_cite">Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib25" title="">2024a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib26" title="">b</a>)</cite> have demonstrated that LLMs can substantially enhance the performance of multilingual MT. Building on this insight, we adopt a pre-trained LLM as the foundation for our system.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">IKUN</span> is an open system developed with meticulous consideration of available resources and system capabilities. For this purpose, we selected Llama-3 <cite class="ltx_cite ltx_citemacro_cite">Dubey et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib8" title="">2024</a>)</cite>, one of the most advanced open-source LLMs available at the time of this competition. Due to constraints on computational resources, we opted for the 8B version<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_href" href="https://huggingface.co/meta-llama/Meta-Llama-3-8B" title="">https://huggingface.co/meta-llama/Meta-Llama-3-8B</a></span></span></span> instead of the 70B version. A significant factor in our choice of Llama-3 was its strong support for multilingual applications, as evidenced by the efficiency with which its tokenizer handles all 11 languages involved in this competition (See §<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#S3" title="3 Tokenizer Efficiency ‣ IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>). We also tried the instruct version, but it is worse than the pre-trained version.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">IKUN-C</span> is a constrained system based on Mistral-7B-v0.3 <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib12" title="">2023</a>)</cite>, one of the three LLMs permitted for the constrained track. Prior to selecting Mistral-7B-v0.3<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_href" href="https://huggingface.co/mistralai/Mistral-7B-v0.3" title="">https://huggingface.co/mistralai/Mistral-7B-v0.3</a></span></span></span>, we conducted continuous pre-training on all three allowed LLMs — namely, Llama-2-7B, Llama-2-13B <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib22" title="">2023</a>)</cite>, and Mistral-7B — using a subset of our monolingual data (approximately 1B tokens). The pre-training loss demonstrated that Mistral-7B outperformed Llama-2-7B and performed comparably to Llama-2-13B, leading us to select it as our architecture of choice.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Tokenizer Efficiency</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">A significant challenge in applying LLMs to multilingual MT lies in the efficiency of their tokenizers. These models are typically pre-trained on one or a few dominant languages, and when their tokenizers are applied to low-resource languages, they produce disproportionately long sequences of sub-words. This inefficiency leads to excessive GPU memory consumption during training.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.3">To evaluate the efficiency of the tokenizer, we focus on comparing the length differences between tokenized English sentences and their corresponding non-English counterparts. Specifically, we define the length ratio as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{length}\ \ \mathrm{ratio}=\frac{\mathrm{len(tokenizer(}x\mathrm{))}}{%
\mathrm{len(tokenizer(}y\mathrm{))}}" class="ltx_Math" display="block" id="S3.Ex1.m1.6"><semantics id="S3.Ex1.m1.6a"><mrow id="S3.Ex1.m1.6.7" xref="S3.Ex1.m1.6.7.cmml"><mrow id="S3.Ex1.m1.6.7.2.2" xref="S3.Ex1.m1.6.7.2.1.cmml"><mi id="S3.Ex1.m1.5.5" xref="S3.Ex1.m1.5.5.cmml">length</mi><mspace id="S3.Ex1.m1.6.7.2.2.1" width="1em" xref="S3.Ex1.m1.6.7.2.1.cmml"></mspace><mi id="S3.Ex1.m1.6.6" xref="S3.Ex1.m1.6.6.cmml">ratio</mi></mrow><mo id="S3.Ex1.m1.6.7.1" xref="S3.Ex1.m1.6.7.1.cmml">=</mo><mfrac id="S3.Ex1.m1.4.4" xref="S3.Ex1.m1.4.4.cmml"><mrow id="S3.Ex1.m1.2.2.2" xref="S3.Ex1.m1.2.2.2.cmml"><mi id="S3.Ex1.m1.2.2.2.4" xref="S3.Ex1.m1.2.2.2.4.cmml">len</mi><mo id="S3.Ex1.m1.2.2.2.3" xref="S3.Ex1.m1.2.2.2.3.cmml">⁢</mo><mrow id="S3.Ex1.m1.2.2.2.2.1" xref="S3.Ex1.m1.2.2.2.2.1.1.cmml"><mo id="S3.Ex1.m1.2.2.2.2.1.2" stretchy="false" xref="S3.Ex1.m1.2.2.2.2.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.2.2.2.2.1.1" xref="S3.Ex1.m1.2.2.2.2.1.1.cmml"><mi id="S3.Ex1.m1.2.2.2.2.1.1.2" xref="S3.Ex1.m1.2.2.2.2.1.1.2.cmml">tokenizer</mi><mo id="S3.Ex1.m1.2.2.2.2.1.1.1" xref="S3.Ex1.m1.2.2.2.2.1.1.1.cmml">⁢</mo><mrow id="S3.Ex1.m1.2.2.2.2.1.1.3.2" xref="S3.Ex1.m1.2.2.2.2.1.1.cmml"><mo id="S3.Ex1.m1.2.2.2.2.1.1.3.2.1" stretchy="false" xref="S3.Ex1.m1.2.2.2.2.1.1.cmml">(</mo><mi id="S3.Ex1.m1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml">x</mi><mo id="S3.Ex1.m1.2.2.2.2.1.1.3.2.2" stretchy="false" xref="S3.Ex1.m1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.2.2.2.2.1.3" stretchy="false" xref="S3.Ex1.m1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow><mrow id="S3.Ex1.m1.4.4.4" xref="S3.Ex1.m1.4.4.4.cmml"><mi id="S3.Ex1.m1.4.4.4.4" xref="S3.Ex1.m1.4.4.4.4.cmml">len</mi><mo id="S3.Ex1.m1.4.4.4.3" xref="S3.Ex1.m1.4.4.4.3.cmml">⁢</mo><mrow id="S3.Ex1.m1.4.4.4.2.1" xref="S3.Ex1.m1.4.4.4.2.1.1.cmml"><mo id="S3.Ex1.m1.4.4.4.2.1.2" stretchy="false" xref="S3.Ex1.m1.4.4.4.2.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.4.4.4.2.1.1" xref="S3.Ex1.m1.4.4.4.2.1.1.cmml"><mi id="S3.Ex1.m1.4.4.4.2.1.1.2" xref="S3.Ex1.m1.4.4.4.2.1.1.2.cmml">tokenizer</mi><mo id="S3.Ex1.m1.4.4.4.2.1.1.1" xref="S3.Ex1.m1.4.4.4.2.1.1.1.cmml">⁢</mo><mrow id="S3.Ex1.m1.4.4.4.2.1.1.3.2" xref="S3.Ex1.m1.4.4.4.2.1.1.cmml"><mo id="S3.Ex1.m1.4.4.4.2.1.1.3.2.1" stretchy="false" xref="S3.Ex1.m1.4.4.4.2.1.1.cmml">(</mo><mi id="S3.Ex1.m1.3.3.3.1" xref="S3.Ex1.m1.3.3.3.1.cmml">y</mi><mo id="S3.Ex1.m1.4.4.4.2.1.1.3.2.2" stretchy="false" xref="S3.Ex1.m1.4.4.4.2.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.4.4.4.2.1.3" stretchy="false" xref="S3.Ex1.m1.4.4.4.2.1.1.cmml">)</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.6b"><apply id="S3.Ex1.m1.6.7.cmml" xref="S3.Ex1.m1.6.7"><eq id="S3.Ex1.m1.6.7.1.cmml" xref="S3.Ex1.m1.6.7.1"></eq><list id="S3.Ex1.m1.6.7.2.1.cmml" xref="S3.Ex1.m1.6.7.2.2"><ci id="S3.Ex1.m1.5.5.cmml" xref="S3.Ex1.m1.5.5">length</ci><ci id="S3.Ex1.m1.6.6.cmml" xref="S3.Ex1.m1.6.6">ratio</ci></list><apply id="S3.Ex1.m1.4.4.cmml" xref="S3.Ex1.m1.4.4"><divide id="S3.Ex1.m1.4.4.5.cmml" xref="S3.Ex1.m1.4.4"></divide><apply id="S3.Ex1.m1.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2"><times id="S3.Ex1.m1.2.2.2.3.cmml" xref="S3.Ex1.m1.2.2.2.3"></times><ci id="S3.Ex1.m1.2.2.2.4.cmml" xref="S3.Ex1.m1.2.2.2.4">len</ci><apply id="S3.Ex1.m1.2.2.2.2.1.1.cmml" xref="S3.Ex1.m1.2.2.2.2.1"><times id="S3.Ex1.m1.2.2.2.2.1.1.1.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.1"></times><ci id="S3.Ex1.m1.2.2.2.2.1.1.2.cmml" xref="S3.Ex1.m1.2.2.2.2.1.1.2">tokenizer</ci><ci id="S3.Ex1.m1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1">𝑥</ci></apply></apply><apply id="S3.Ex1.m1.4.4.4.cmml" xref="S3.Ex1.m1.4.4.4"><times id="S3.Ex1.m1.4.4.4.3.cmml" xref="S3.Ex1.m1.4.4.4.3"></times><ci id="S3.Ex1.m1.4.4.4.4.cmml" xref="S3.Ex1.m1.4.4.4.4">len</ci><apply id="S3.Ex1.m1.4.4.4.2.1.1.cmml" xref="S3.Ex1.m1.4.4.4.2.1"><times id="S3.Ex1.m1.4.4.4.2.1.1.1.cmml" xref="S3.Ex1.m1.4.4.4.2.1.1.1"></times><ci id="S3.Ex1.m1.4.4.4.2.1.1.2.cmml" xref="S3.Ex1.m1.4.4.4.2.1.1.2">tokenizer</ci><ci id="S3.Ex1.m1.3.3.3.1.cmml" xref="S3.Ex1.m1.3.3.3.1">𝑦</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.6c">\mathrm{length}\ \ \mathrm{ratio}=\frac{\mathrm{len(tokenizer(}x\mathrm{))}}{%
\mathrm{len(tokenizer(}y\mathrm{))}}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.6d">roman_length roman_ratio = divide start_ARG roman_len ( roman_tokenizer ( italic_x ) ) end_ARG start_ARG roman_len ( roman_tokenizer ( italic_y ) ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p2.2">where <math alttext="y" class="ltx_Math" display="inline" id="S3.p2.1.m1.1"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">y</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.1d">italic_y</annotation></semantics></math> represents the English sentence, and <math alttext="x" class="ltx_Math" display="inline" id="S3.p2.2.m2.1"><semantics id="S3.p2.2.m2.1a"><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.p2.2.m2.1d">italic_x</annotation></semantics></math> denotes the paired non-English sentence. A smaller length ratio (close to 1) is desired, since it means that the tokenizer can encode the non-English sentence as efficient as the English sentence.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">To facilitate a comparison of length ratios across different languages, English-centric multilingual data is essential. Fortunately, the FLoRes-200 dataset <cite class="ltx_cite ltx_citemacro_cite">Costa-jussà et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib7" title="">2022</a>)</cite> possesses this characteristic. In the devtest and test sets of FLoRes-200, every English sentence is paired with translations in various other languages. We concatenate all sentences from the devtest set and compute the length ratio for each language, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>. We also include NLLB’s tokenizer <cite class="ltx_cite ltx_citemacro_cite">Costa-jussà et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib7" title="">2022</a>)</cite> for a comparison.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">We can observe: (1) NLLB consistently exhibits the smallest length ratio across all languages, likely due to its extensive optimization for hundreds of languages, thus serving as a lower bound in this context. (2) Mistral-v0.3 and Llama-3 demonstrate a notably high length ratio for Hindi, suggesting that Hindi is underrepresented in the pre-training data. (3) Compared to NLLB, the tokenizer of Mistral-v0.3 is significantly less efficient for Chinese, Japanese, Hindi, and Icelandic.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">We opted to expand the vocabulary by incorporating new sub-words to reduce the length of tokenized sentences, thereby enhancing training efficiency. However, this approach introduces a trade-off between the addition of new sub-words and training performance. The embeddings for the newly introduced sub-words are initially untrained, and a substantial increase in sub-words may necessitate additional iterations of continuous pre-training. Consequently, our strategy for adding sub-words prioritizes those from languages with higher length ratios.</p>
</div>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.1">For our open system, IKUN, we didn’t modify its tokenizer, since Llama-3 tokenizer is already efficient enough, only except for Hindi. For our constrained system, IKUN-C, we expanded its vocabulary for Chinese, Japanese, Hindi, and Icelandic through the following steps: (1) Generate a new vocabulary of 12K sub-words using monolingual data from the past two years for these four languages from News Crawl<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_href" href="https://data.statmt.org/news-crawl/" title="">https://data.statmt.org/news-crawl/</a></span></span></span>; and (2) Merge the new vocabulary with the original. The efficiency of the resulted IKUN-C tokenizer is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, demonstrating more efficiency for these four languages, especially for Hindi.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1">Language pair</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1">Num.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.1">Language pair</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S3.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.4.1">Num.</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.1">cs-uk</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.2.2.2">8768</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.3">uk-cs</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.2.2.4">8768</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.3">
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.3.1">ja-zh</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.3.3.2">12858</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.3.3">zh-ja</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.3.3.4">12858</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.4">
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.4.1">en-zh</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.4.4.2">36647</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.4.3">zh-en</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.4.4.4">34650</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.5">
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.1">en-cs</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.5.5.2">30120</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.3">cs-en</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.5.5.4">28123</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.6">
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.1">en-de</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.6.6.2">35564</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.3">de-en</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.6.6.4">33567</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.7">
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.7.1">en-hi</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.7.7.2">11020</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.7.3">hi-en</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.7.7.4">9023</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.8.8">
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.8.1">en-is</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.8.8.2">8010</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.8.3">is-en</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.8.8.4">6013</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.9.9">
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.9.1">en-ja</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.9.9.2">18113</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.9.3">ja-en</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.9.9.4">16116</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.10.10">
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.10.1">en-ru</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.10.10.2">32840</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.10.3">ru-en</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.10.10.4">30843</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.11.11">
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.11.1">en-es</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.11.11.2">13808</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.11.3">es-en</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.11.11.4">11811</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.12.12">
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.12.1">en-uk</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.12.12.2">11961</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.12.3">uk-en</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.12.12.4">9964</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.13.13">
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.13.1">en-fr</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.13.13.2">4006</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.13.3">fr-en</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.13.13.4">4006</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.14.14">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.14.14.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.14.14.1.1">Total:</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" colspan="3" id="S3.T1.1.14.14.2">429457</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Number of parallel sentences.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.1.2.1.1.1">Hyper-parameter</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.2.1.2.1">Continuous pre-training</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S3.T2.1.2.1.3.1">Finetuning</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.3.2.1">sampling probability</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.3.2.2">cs,de,en,es,hi,is,ja,ru,uk,zh =</td>
<td class="ltx_td ltx_border_t" id="S3.T2.1.3.2.3"></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.3">
<td class="ltx_td" id="S3.T2.1.4.3.1"></td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.4.3.2">0.1,0.13,0.1,0.13,0.08,0.05,0.08,0.13,0.08,0.12</td>
<td class="ltx_td" id="S3.T2.1.4.3.3"></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5.4">
<td class="ltx_td ltx_align_left" id="S3.T2.1.5.4.1">duration</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.5.4.2">60K steps</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.5.4.3">1 epoch</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.6.5">
<td class="ltx_td ltx_align_left" id="S3.T2.1.6.5.1">batch size</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.6.5.2">64</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.6.5.3">128</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.7.6">
<td class="ltx_td ltx_align_left" id="S3.T2.1.7.6.1">sequence length</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.7.6.2">2048</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.7.6.3">max source length=512, max target length=512</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.8.7">
<td class="ltx_td ltx_align_left" id="S3.T2.1.8.7.1">learning rate (lr)</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.8.7.2">2e-5</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.8.7.3">2e-4</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.9.8">
<td class="ltx_td ltx_align_left" id="S3.T2.1.9.8.1">warmup ratio</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.9.8.2">0</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.9.8.3">0.01</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.10.9">
<td class="ltx_td ltx_align_left" id="S3.T2.1.10.9.1">weight decay</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.10.9.2">0.01</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.10.9.3">0.01</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.11.10">
<td class="ltx_td ltx_align_left" id="S3.T2.1.11.10.1">lr scheduler</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.11.10.2">cosine</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.11.10.3">inverse_sqrt</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.1.1.2">training type</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.1.1.3">full finetuning</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.1.1.1">LoRA <math alttext="r=64" class="ltx_Math" display="inline" id="S3.T2.1.1.1.m1.1"><semantics id="S3.T2.1.1.1.m1.1a"><mrow id="S3.T2.1.1.1.m1.1.1" xref="S3.T2.1.1.1.m1.1.1.cmml"><mi id="S3.T2.1.1.1.m1.1.1.2" xref="S3.T2.1.1.1.m1.1.1.2.cmml">r</mi><mo id="S3.T2.1.1.1.m1.1.1.1" xref="S3.T2.1.1.1.m1.1.1.1.cmml">=</mo><mn id="S3.T2.1.1.1.m1.1.1.3" xref="S3.T2.1.1.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.m1.1b"><apply id="S3.T2.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.m1.1.1"><eq id="S3.T2.1.1.1.m1.1.1.1.cmml" xref="S3.T2.1.1.1.m1.1.1.1"></eq><ci id="S3.T2.1.1.1.m1.1.1.2.cmml" xref="S3.T2.1.1.1.m1.1.1.2">𝑟</ci><cn id="S3.T2.1.1.1.m1.1.1.3.cmml" type="integer" xref="S3.T2.1.1.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.m1.1c">r=64</annotation><annotation encoding="application/x-llamapun" id="S3.T2.1.1.1.m1.1d">italic_r = 64</annotation></semantics></math> for all layers</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Experimental setting for continuous pre-training and subsequent finetuning.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T3.8">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.8.9.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S3.T3.8.9.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.8.9.1.1.1">System</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S3.T3.8.9.1.2"><span class="ltx_text ltx_font_bold" id="S3.T3.8.9.1.2.1">Metric</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.8.9.1.3"><span class="ltx_text ltx_font_bold" id="S3.T3.8.9.1.3.1">cs-uk</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.8.9.1.4"><span class="ltx_text ltx_font_bold" id="S3.T3.8.9.1.4.1">en-cs</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.8.9.1.5"><span class="ltx_text ltx_font_bold" id="S3.T3.8.9.1.5.1">en-de</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.8.9.1.6"><span class="ltx_text ltx_font_bold" id="S3.T3.8.9.1.6.1">en-es</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.8.9.1.7"><span class="ltx_text ltx_font_bold" id="S3.T3.8.9.1.7.1">en-hi</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.8.9.1.8"><span class="ltx_text ltx_font_bold" id="S3.T3.8.9.1.8.1">en-is</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.8.9.1.9"><span class="ltx_text ltx_font_bold" id="S3.T3.8.9.1.9.1">en-ja</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.8.9.1.10"><span class="ltx_text ltx_font_bold" id="S3.T3.8.9.1.10.1">en-ru</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.8.9.1.11"><span class="ltx_text ltx_font_bold" id="S3.T3.8.9.1.11.1">en-uk</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.8.9.1.12"><span class="ltx_text ltx_font_bold" id="S3.T3.8.9.1.12.1">en-zh</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.8.9.1.13"><span class="ltx_text ltx_font_bold" id="S3.T3.8.9.1.13.1">ja-zh</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.2"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.1">MetricX <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T3.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.m1.1a"><mo id="S3.T3.1.1.1.m1.1.1" stretchy="false" xref="S3.T3.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.m1.1b"><ci id="S3.T3.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.m1.1d">↓</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.3">2.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.4">4.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.5">2.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.6">3.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.7">7.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.8">4.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.9">4.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.10">4.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.11">4.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.12">4.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.13">6.2</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T3.2.2.2"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.2.2.1">CometKiwi <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T3.2.2.1.m1.1"><semantics id="S3.T3.2.2.1.m1.1a"><mo id="S3.T3.2.2.1.m1.1.1" stretchy="false" xref="S3.T3.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.1.m1.1b"><ci id="S3.T3.2.2.1.m1.1.1.cmml" xref="S3.T3.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.2.2.1.m1.1d">↑</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.3">0.648</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.4">0.618</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.5">0.641</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.6">0.666</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.7">0.499</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.8">0.657</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.9">0.669</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.10">0.649</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.11">0.622</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.12">0.624</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.2.13">0.519</td>
</tr>
<tr class="ltx_tr" id="S3.T3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.3.3.2">IKUN-C</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.3.3.1">AutoRank <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T3.3.3.1.m1.1"><semantics id="S3.T3.3.3.1.m1.1a"><mo id="S3.T3.3.3.1.m1.1.1" stretchy="false" xref="S3.T3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.1.m1.1b"><ci id="S3.T3.3.3.1.m1.1.1.cmml" xref="S3.T3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.3.3.1.m1.1d">↓</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.3">3.0</td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.4">4.7</td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.5">3.8</td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.6">3.4</td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.7">5.5</td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.8">3.7</td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.9">3.9</td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.10">3.9</td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.11">3.9</td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.12">3.5</td>
<td class="ltx_td ltx_align_center" id="S3.T3.3.3.13">5.5</td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.4">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T3.4.4.2"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.4.4.1">Place in constrained <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T3.4.4.1.m1.1"><semantics id="S3.T3.4.4.1.m1.1a"><mo id="S3.T3.4.4.1.m1.1.1" stretchy="false" xref="S3.T3.4.4.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T3.4.4.1.m1.1b"><ci id="S3.T3.4.4.1.m1.1.1.cmml" xref="S3.T3.4.4.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.4.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.4.4.1.m1.1d">↓</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.4.3">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.4.4">5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.4.5">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.4.6">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.4.7">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.4.8">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.4.9">3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.4.10">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.4.11">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.4.12">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.4.13">2</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.5">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S3.T3.5.5.2"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.5.5.1">MetricX <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T3.5.5.1.m1.1"><semantics id="S3.T3.5.5.1.m1.1a"><mo id="S3.T3.5.5.1.m1.1.1" stretchy="false" xref="S3.T3.5.5.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T3.5.5.1.m1.1b"><ci id="S3.T3.5.5.1.m1.1.1.cmml" xref="S3.T3.5.5.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.5.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.5.5.1.m1.1d">↓</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.5.3">1.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.5.4">3.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.5.5">1.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.5.6">3.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.5.7">9.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.5.8">4.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.5.9">3.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.5.10">4.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.5.11">3.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.5.12">4.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.5.13">5.4</td>
</tr>
<tr class="ltx_tr" id="S3.T3.6.6">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T3.6.6.2"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.6.6.1">CometKiwi <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T3.6.6.1.m1.1"><semantics id="S3.T3.6.6.1.m1.1a"><mo id="S3.T3.6.6.1.m1.1.1" stretchy="false" xref="S3.T3.6.6.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T3.6.6.1.m1.1b"><ci id="S3.T3.6.6.1.m1.1.1.cmml" xref="S3.T3.6.6.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.6.6.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.6.6.1.m1.1d">↑</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.3">0.664</td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.4">0.638</td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.5">0.668</td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.6">0.687</td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.7">0.428</td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.8">0.666</td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.9">0.696</td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.10">0.675</td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.11">0.661</td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.12">0.646</td>
<td class="ltx_td ltx_align_center" id="S3.T3.6.6.13">0.544</td>
</tr>
<tr class="ltx_tr" id="S3.T3.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.7.7.2">IKUN</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.7.7.1">AutoRank <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T3.7.7.1.m1.1"><semantics id="S3.T3.7.7.1.m1.1a"><mo id="S3.T3.7.7.1.m1.1.1" stretchy="false" xref="S3.T3.7.7.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T3.7.7.1.m1.1b"><ci id="S3.T3.7.7.1.m1.1.1.cmml" xref="S3.T3.7.7.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.7.7.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.7.7.1.m1.1d">↓</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.3">2.3</td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.4">3.9</td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.5">3.0</td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.6">2.8</td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.7">7.7</td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.8">3.2</td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.9">3.1</td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.10">3.2</td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.11">2.8</td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.12">3.1</td>
<td class="ltx_td ltx_align_center" id="S3.T3.7.7.13">4.4</td>
</tr>
<tr class="ltx_tr" id="S3.T3.8.8">
<th class="ltx_td ltx_th ltx_th_row ltx_border_bb" id="S3.T3.8.8.2"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T3.8.8.1">Place in constrained&amp;open <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T3.8.8.1.m1.1"><semantics id="S3.T3.8.8.1.m1.1a"><mo id="S3.T3.8.8.1.m1.1.1" stretchy="false" xref="S3.T3.8.8.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T3.8.8.1.m1.1b"><ci id="S3.T3.8.8.1.m1.1.1.cmml" xref="S3.T3.8.8.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.8.8.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.8.8.1.m1.1d">↓</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.8.8.3">2</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.8.8.4">5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.8.8.5">4</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.8.8.6">3</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.8.8.7">5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.8.8.8">1</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.8.8.9">6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.8.8.10">3</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.8.8.11">2</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.8.8.12">5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.8.8.13">6</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Preliminary results of our systems on the WMT24 test sets, taken from <cite class="ltx_cite ltx_citemacro_citet">Kocmi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib14" title="">2024</a>)</cite>. The final human evaluation results are not released yet. “-” here means <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.T3.11.m1.1"><semantics id="S3.T3.11.m1.1b"><mo id="S3.T3.11.m1.1.1" stretchy="false" xref="S3.T3.11.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T3.11.m1.1c"><ci id="S3.T3.11.m1.1.1.cmml" xref="S3.T3.11.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.11.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.11.m1.1e">→</annotation></semantics></math>. I.e. cs-uk is cs<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S3.T3.12.m2.1"><semantics id="S3.T3.12.m2.1b"><mo id="S3.T3.12.m2.1.1" stretchy="false" xref="S3.T3.12.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T3.12.m2.1c"><ci id="S3.T3.12.m2.1.1.cmml" xref="S3.T3.12.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.12.m2.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.12.m2.1e">→</annotation></semantics></math>uk. </figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We mainly follow the pipeline from <cite class="ltx_cite ltx_citemacro_citet">Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib25" title="">2024a</a>)</cite>, i.e. continuous pre-training on monolingual data for all 10 languages, and followed by fine-tuning on parallel data for all 11 language pairs.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Continuous Pre-training</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Given that our selected LLMs, specifically Llama-3-8B and Mistral-7B-v0.3, are primarily pre-trained on English, it is necessary to incorporate knowledge from other languages through further pre-training, with particular emphasis on low-resource languages. Additionally, the word embeddings for the newly introduced sub-words in IKUN-C must also undergo training.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.5">For the open system IKUN, we utilize monolingual data from the Oscar dataset <cite class="ltx_cite ltx_citemacro_cite">Suárez et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib20" title="">2020</a>)</cite>, covering all 10 target languages. We adopt the sampling strategy outlined by <cite class="ltx_cite ltx_citemacro_citet">Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib25" title="">2024a</a>)</cite>, described as:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P(l)\propto(\frac{D_{l}}{\sum_{l^{\prime}\in L}D_{l^{\prime}}})^{\frac{1}{T}}%
\quad s.t.\quad\sum_{l^{\prime}\in L}P(l^{\prime})=\frac{9}{10}" class="ltx_math_unparsed" display="block" id="S4.Ex2.m1.4"><semantics id="S4.Ex2.m1.4a"><mrow id="S4.Ex2.m1.4b"><mi id="S4.Ex2.m1.4.5">P</mi><mrow id="S4.Ex2.m1.4.6"><mo id="S4.Ex2.m1.4.6.1" stretchy="false">(</mo><mi id="S4.Ex2.m1.1.1">l</mi><mo id="S4.Ex2.m1.4.6.2" stretchy="false">)</mo></mrow><mo id="S4.Ex2.m1.4.7">∝</mo><msup id="S4.Ex2.m1.4.8"><mrow id="S4.Ex2.m1.4.8.2"><mo id="S4.Ex2.m1.4.8.2.1" stretchy="false">(</mo><mfrac id="S4.Ex2.m1.2.2"><msub id="S4.Ex2.m1.2.2.2"><mi id="S4.Ex2.m1.2.2.2.2">D</mi><mi id="S4.Ex2.m1.2.2.2.3">l</mi></msub><mrow id="S4.Ex2.m1.2.2.3"><msub id="S4.Ex2.m1.2.2.3.1"><mo id="S4.Ex2.m1.2.2.3.1.2">∑</mo><mrow id="S4.Ex2.m1.2.2.3.1.3"><msup id="S4.Ex2.m1.2.2.3.1.3.2"><mi id="S4.Ex2.m1.2.2.3.1.3.2.2">l</mi><mo id="S4.Ex2.m1.2.2.3.1.3.2.3">′</mo></msup><mo id="S4.Ex2.m1.2.2.3.1.3.1">∈</mo><mi id="S4.Ex2.m1.2.2.3.1.3.3">L</mi></mrow></msub><msub id="S4.Ex2.m1.2.2.3.2"><mi id="S4.Ex2.m1.2.2.3.2.2">D</mi><msup id="S4.Ex2.m1.2.2.3.2.3"><mi id="S4.Ex2.m1.2.2.3.2.3.2">l</mi><mo id="S4.Ex2.m1.2.2.3.2.3.3">′</mo></msup></msub></mrow></mfrac><mo id="S4.Ex2.m1.4.8.2.2" stretchy="false">)</mo></mrow><mfrac id="S4.Ex2.m1.4.8.3"><mn id="S4.Ex2.m1.4.8.3.2">1</mn><mi id="S4.Ex2.m1.4.8.3.3">T</mi></mfrac></msup><mspace id="S4.Ex2.m1.4.9" width="1em"></mspace><mi id="S4.Ex2.m1.3.3">s</mi><mo id="S4.Ex2.m1.4.10" lspace="0em" rspace="0.167em">.</mo><mi id="S4.Ex2.m1.4.4">t</mi><mo id="S4.Ex2.m1.4.11" lspace="0em">.</mo><mspace id="S4.Ex2.m1.4.12" width="1.167em"></mspace><munder id="S4.Ex2.m1.4.13"><mo id="S4.Ex2.m1.4.13.2" movablelimits="false">∑</mo><mrow id="S4.Ex2.m1.4.13.3"><msup id="S4.Ex2.m1.4.13.3.2"><mi id="S4.Ex2.m1.4.13.3.2.2">l</mi><mo id="S4.Ex2.m1.4.13.3.2.3">′</mo></msup><mo id="S4.Ex2.m1.4.13.3.1">∈</mo><mi id="S4.Ex2.m1.4.13.3.3">L</mi></mrow></munder><mi id="S4.Ex2.m1.4.14">P</mi><mrow id="S4.Ex2.m1.4.15"><mo id="S4.Ex2.m1.4.15.1" stretchy="false">(</mo><msup id="S4.Ex2.m1.4.15.2"><mi id="S4.Ex2.m1.4.15.2.2">l</mi><mo id="S4.Ex2.m1.4.15.2.3">′</mo></msup><mo id="S4.Ex2.m1.4.15.3" stretchy="false">)</mo></mrow><mo id="S4.Ex2.m1.4.16">=</mo><mfrac id="S4.Ex2.m1.4.17"><mn id="S4.Ex2.m1.4.17.2">9</mn><mn id="S4.Ex2.m1.4.17.3">10</mn></mfrac></mrow><annotation encoding="application/x-tex" id="S4.Ex2.m1.4c">P(l)\propto(\frac{D_{l}}{\sum_{l^{\prime}\in L}D_{l^{\prime}}})^{\frac{1}{T}}%
\quad s.t.\quad\sum_{l^{\prime}\in L}P(l^{\prime})=\frac{9}{10}</annotation><annotation encoding="application/x-llamapun" id="S4.Ex2.m1.4d">italic_P ( italic_l ) ∝ ( divide start_ARG italic_D start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_l start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∈ italic_L end_POSTSUBSCRIPT italic_D start_POSTSUBSCRIPT italic_l start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT end_ARG ) start_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG italic_T end_ARG end_POSTSUPERSCRIPT italic_s . italic_t . ∑ start_POSTSUBSCRIPT italic_l start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∈ italic_L end_POSTSUBSCRIPT italic_P ( italic_l start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) = divide start_ARG 9 end_ARG start_ARG 10 end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS1.p2.4">where <math alttext="D_{l}" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><msub id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">D</mi><mi id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">𝐷</ci><ci id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">D_{l}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">italic_D start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math> represents the number of words in language <math alttext="l" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.1"><semantics id="S4.SS1.p2.2.m2.1a"><mi id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><ci id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">l</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.2.m2.1d">italic_l</annotation></semantics></math><span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_href" href="https://huggingface.co/datasets/oscar-corpus/OSCAR-2301" title="">https://huggingface.co/datasets/oscar-corpus/OSCAR-2301</a></span></span></span>, <math alttext="T" class="ltx_Math" display="inline" id="S4.SS1.p2.3.m3.1"><semantics id="S4.SS1.p2.3.m3.1a"><mi id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><ci id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.3.m3.1d">italic_T</annotation></semantics></math> is the temperature parameter (set to 6), and <math alttext="L" class="ltx_Math" display="inline" id="S4.SS1.p2.4.m4.1"><semantics id="S4.SS1.p2.4.m4.1a"><mi id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><ci id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">L</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.4.m4.1d">italic_L</annotation></semantics></math> denotes the set of all languages except English. The sampling probability for English is fixed at 1/10. The experimental settings for continuous pre-training are detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#S3.T2" title="Table 2 ‣ 3 Tokenizer Efficiency ‣ IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>. We approximately pre-trained IKUN on an additional 8B tokens.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">In the constrained system, only the provided data sources are permitted for use<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_href" href="https://www2.statmt.org/wmt24/mtdata/" title="">https://www2.statmt.org/wmt24/mtdata/</a></span></span></span>. The IKUN-C system utilizes monolingual data from the News Crawl dataset for 9 languages, with the exception of Spanish, as the use of Spanish monolingual data from News Crawl is restricted. For Spanish, we incorporate monolingual data from the Leipzig Corpora <cite class="ltx_cite ltx_citemacro_cite">Goldhahn et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib11" title="">2012</a>)</cite>. Additionally, for Hindi, we augment the dataset with monolingual data from News Commentary due to the relatively limited amount of Hindi data available in the News Crawl dataset. This adjustment is crucial because Hindi is underrepresented in the pre-training of Mistral-7B-v0.3. Our experimental settings closely align with those detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#S3.T2" title="Table 2 ‣ 3 Tokenizer Efficiency ‣ IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Subsequent Fine-tuning</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Previous studies <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib23" title="">2024</a>); Zhou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib27" title="">2023</a>)</cite> have demonstrated that the quality of fine-tuning data is a critical factor in achieving optimal performance. <cite class="ltx_cite ltx_citemacro_citet">Liao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib17" title="">2021</a>)</cite> further indicates that increasing the amount of back-translation data does not necessarily lead to better outcomes. In light of this, we exclusively utilize high-quality parallel data for the fine-tuning phase.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">The high-quality parallel data is primarily sourced from FloRes-200 <cite class="ltx_cite ltx_citemacro_cite">Costa-jussà et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib7" title="">2022</a>)</cite>, NTREX-128 <cite class="ltx_cite ltx_citemacro_cite">Federmann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib9" title="">2022</a>)</cite>, and previous WMT16-23 general MT/news tasks <cite class="ltx_cite ltx_citemacro_cite">Kocmi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib15" title="">2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib16" title="">2022</a>); Akhbardeh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib1" title="">2021</a>); Barrault et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib2" title="">2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib3" title="">2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib4" title="">2018</a>); Bojar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib5" title="">2017</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib6" title="">2016</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">FloRes-200:</span> As the FloRes-200 dataset provides parallel sentences across multiple languages, we leverage all 11 translation directions from the devtest and test sets. Importantly, our fine-tuning approach is not limited to the required directions listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#S3.T3" title="Table 3 ‣ 3 Tokenizer Efficiency ‣ IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>; instead, we fine-tune the model on both translation directions, e.g., en<math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mo id="S4.SS2.p3.1.m1.1.1" stretchy="false" xref="S4.SS2.p3.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">↔</annotation></semantics></math>de, to facilitate broader applicability of the final model.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.2"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.2.1">NTREX-128:</span> We also incorporate parallel sentences from NTREX-128 for from-English translation directions, i.e. en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS2.p4.1.m1.1"><semantics id="S4.SS2.p4.1.m1.1a"><mo id="S4.SS2.p4.1.m1.1.1" stretchy="false" xref="S4.SS2.p4.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><ci id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.1.m1.1d">→</annotation></semantics></math>XX. In accordance with <cite class="ltx_cite ltx_citemacro_citet">Federmann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib9" title="">2022</a>)</cite>, which recommends using the en<math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS2.p4.2.m2.1"><semantics id="S4.SS2.p4.2.m2.1a"><mo id="S4.SS2.p4.2.m2.1.1" stretchy="false" xref="S4.SS2.p4.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><ci id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.2.m2.1d">→</annotation></semantics></math>XX translation direction, our fine-tuning is confined to these directions rather than adopting a bidirectional approach. An exception is made for the en-fr pair, where bidirectional fine-tuning is applied due to the limited availability of parallel data for this pair (absent in previous WMTs).</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p5.1.1">Past WMTs:</span> Additionally, we extract parallel sentences from the development and test sets of the WMT16-23 general MT tasks, provided they contain the necessary translation directions. For these sentences, we employ a bidirectional fine-tuning strategy.</p>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.1">The statistics for all parallel sentences are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#S3.T1" title="Table 1 ‣ 3 Tokenizer Efficiency ‣ IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>. Notably, all systems are fine-tuned at the sentence level. Given that WMT development and test sets are at the document level, models could alternatively be fine-tuned at the document level or reformatted into a conversational structure for fine-tuning. This latter approach might be more effective for context-aware translation, as WMT24 applies context-based human evaluations. We reserve this exploration for future work. The fine-tuning setting is listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#S3.T2" title="Table 2 ‣ 3 Tokenizer Efficiency ‣ IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We present the preliminary results reported by <cite class="ltx_cite ltx_citemacro_citet">Kocmi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib14" title="">2024</a>)</cite> in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#S3.T3" title="Table 3 ‣ 3 Tokenizer Efficiency ‣ IKUN for WMT24 General MT Task: LLMs Are here for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>, which includes four evaluation metrics. Both MetricX-23-XL <cite class="ltx_cite ltx_citemacro_cite">Juraska et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib13" title="">2023</a>)</cite> and CometKiwi-DA-XL <cite class="ltx_cite ltx_citemacro_cite">Rei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib19" title="">2023</a>)</cite> have demonstrated strong correlations with human evaluation <cite class="ltx_cite ltx_citemacro_cite">Freitag et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib10" title="">2023</a>)</cite>. AutoRank <cite class="ltx_cite ltx_citemacro_cite">Kocmi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11512v2#bib.bib14" title="">2024</a>)</cite>, a normalized composite metric derived from MetricX and CometKiwi, scales the scores of each metric linearly to span the range from 1 to the total number of systems in a given language pair. The final automatic ranking is obtained by averaging these normalized scores. AutoRank can thus be considered a measure of the overall rank across all systems and tracks. Additionally, we report the rankings of our systems across various tracks.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">It is noteworthy that both of our systems are multilingual, designed to handle all language pairs. The IKUN-C system, in particular, demonstrates promising performance in the constrained track, achieving 6 first-place and 3 second-place finishes. In both the open and constrained tracks, IKUN maintains strong performance, securing 1 first-place and 2 second-place positions, even when compared to systems that may leverage additional open-source data or specialize in a limited set of language pairs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we present a methodology for effectively adapting pre-trained LLMs to the task of multilingual machine translation. Our approach involves three primary steps: (1) expanding the vocabulary to accommodate languages that are underrepresented in the pre-training data, when necessary; (2) continuing pre-training the LLM on monolingual data to enhance its knowledge of underrepresented languages and to train the embeddings of newly introduced sub-words; and (3) fine-tuning the LLM on high-quality parallel data. Our experimental results demonstrate the efficacy of this straightforward pipeline, with IKUN-C securing 6 first-place finishes in the constrained track, and IKUN achieving 1 first-place ranking in both the open and constrained tracks.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We thank eBay Inc. for the computation support. This research was funded in part by the Netherlands Organization for Scientific Research (NWO) under project number VI.C.192.080.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akhbardeh et al. (2021)</span>
<span class="ltx_bibblock">
Farhad Akhbardeh, Arkady Arkhangorodsky, Magdalena Biesialska, Ondrej Bojar, Rajen Chatterjee, Vishrav Chaudhary, Marta R. Costa-jussà, Cristina España-Bonet, Angela Fan, Christian Federmann, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Leonie Harter, Kenneth Heafield, Christopher Homan, Matthias Huck, Kwabena Amponsah-Kaakyire, Jungo Kasai, Daniel Khashabi, Kevin Knight, Tom Kocmi, Philipp Koehn, Nicholas Lourie, Christof Monz, Makoto Morishita, Masaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa, Matteo Negri, Santanu Pal, Allahsera Auguste Tapo, Marco Turchi, Valentin Vydrin, and Marcos Zampieri. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.wmt-1.1" title="">Findings of the 2021 conference on machine translation (WMT21)</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the Sixth Conference on Machine Translation, WMT@EMNLP 2021, Online Event, November 10-11, 2021</em>, pages 1–88. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrault et al. (2020)</span>
<span class="ltx_bibblock">
Loïc Barrault, Magdalena Biesialska, Ondrej Bojar, Marta R. Costa-jussà, Christian Federmann, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Matthias Huck, Eric Joanis, Tom Kocmi, Philipp Koehn, Chi-kiu Lo, Nikola Ljubesic, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Santanu Pal, Matt Post, and Marcos Zampieri. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.wmt-1.1/" title="">Findings of the 2020 conference on machine translation (WMT20)</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the Fifth Conference on Machine Translation, WMT@EMNLP 2020, Online, November 19-20, 2020</em>, pages 1–55. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrault et al. (2019)</span>
<span class="ltx_bibblock">
Loïc Barrault, Ondrej Bojar, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias Müller, Santanu Pal, Matt Post, and Marcos Zampieri. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/W19-5301" title="">Findings of the 2019 conference on machine translation (WMT19)</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the Fourth Conference on Machine Translation, WMT 2019, Florence, Italy, August 1-2, 2019 - Volume 2: Shared Task Papers, Day 1</em>, pages 1–61. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrault et al. (2018)</span>
<span class="ltx_bibblock">
Loïc Barrault, Fethi Bougares, Lucia Specia, Chiraag Lala, Desmond Elliott, and Stella Frank. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/W18-6402" title="">Findings of the third shared task on multimodal machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the Third Conference on Machine Translation: Shared Task Papers, WMT 2018, Belgium, Brussels, October 31 - November 1, 2018</em>, pages 304–323. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bojar et al. (2017)</span>
<span class="ltx_bibblock">
Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/W17-4717" title="">Findings of the 2017 conference on machine translation (WMT17)</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the Second Conference on Machine Translation, WMT 2017, Copenhagen, Denmark, September 7-8, 2017</em>, pages 169–214. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bojar et al. (2016)</span>
<span class="ltx_bibblock">
Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno-Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana L. Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Marcos Zampieri. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/W16-2301" title="">Findings of the 2016 conference on machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the First Conference on Machine Translation, WMT 2016, colocated with ACL 2016, August 11-12, Berlin, Germany</em>, pages 131–198. The Association for Computer Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costa-jussà et al. (2022)</span>
<span class="ltx_bibblock">
Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, and et al. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2207.04672" title="">No language left behind: Scaling human-centered machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">CoRR</em>, abs/2207.04672.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et al. (2024)</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, and et al. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2407.21783" title="">The llama 3 herd of models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Federmann et al. (2022)</span>
<span class="ltx_bibblock">
Christian Federmann, Tom Kocmi, and Ying Xin. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.sumeval-1.4" title="">NTREX-128 – news test references for MT evaluation of 128 languages</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the First Workshop on Scaling Up Multilingual Evaluation</em>, pages 21–24, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag et al. (2023)</span>
<span class="ltx_bibblock">
Markus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frédéric Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, and George F. Foster. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.WMT-1.51" title="">Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the Eighth Conference on Machine Translation, WMT 2023, Singapore, December 6-7, 2023</em>, pages 578–628. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldhahn et al. (2012)</span>
<span class="ltx_bibblock">
Dirk Goldhahn, Thomas Eckart, and Uwe Quasthoff. 2012.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://www.lrec-conf.org/proceedings/lrec2012/summaries/327.html" title="">Building large monolingual dictionaries at the leipzig corpora collection: From 100 to 200 languages</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the Eighth International Conference on Language Resources and Evaluation, LREC 2012, Istanbul, Turkey, May 23-25, 2012</em>, pages 759–765. European Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023)</span>
<span class="ltx_bibblock">
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, and et al. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.06825" title="">Mistral 7b</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">CoRR</em>, abs/2310.06825.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Juraska et al. (2023)</span>
<span class="ltx_bibblock">
Juraj Juraska, Mara Finkelstein, Daniel Deutsch, Aditya Siddhant, Mehdi Mirzazadeh, and Markus Freitag. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.wmt-1.63" title="">MetricX-23: The Google submission to the WMT 2023 metrics shared task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the Eighth Conference on Machine Translation</em>, pages 756–767, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocmi et al. (2024)</span>
<span class="ltx_bibblock">
Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Marzena Karpinska, Philipp Koehn, Benjamin Marie, Kenton Murray, Masaaki Nagata, Martin Popel, Maja Popovic, Mariya Shmatova, Steinþór Steingrímsson, and Vilém Zouhar. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2407.19884" title="">Preliminary wmt24 ranking of general mt systems and llms</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocmi et al. (2023)</span>
<span class="ltx_bibblock">
Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Makoto Nagata, Toshiaki Nakazawa, Martin Popel, Maja Popovic, and Mariya Shmatova. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.WMT-1.1" title="">Findings of the 2023 conference on machine translation (WMT23): llms are here but not quite there yet</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the Eighth Conference on Machine Translation, WMT 2023, Singapore, December 6-7, 2023</em>, pages 1–42. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocmi et al. (2022)</span>
<span class="ltx_bibblock">
Tom Kocmi, Rachel Bawden, Ondrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Rebecca Knowles, Philipp Koehn, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Michal Novák, Martin Popel, and Maja Popovic. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.1" title="">Findings of the 2022 conference on machine translation (WMT22)</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the Seventh Conference on Machine Translation, WMT 2022, Abu Dhabi, United Arab Emirates (Hybrid), December 7-8, 2022</em>, pages 1–45. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et al. (2021)</span>
<span class="ltx_bibblock">
Baohao Liao, Shahram Khadivi, and Sanjika Hewavitharana. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.wmt-1.50" title="">Back-translation for large-scale multilingual machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the Sixth Conference on Machine Translation, WMT@EMNLP 2021, Online Event, November 10-11, 2021</em>, pages 418–424. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2303.08774" title="">GPT-4 technical report</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">CoRR</em>, abs/2303.08774.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al. (2023)</span>
<span class="ltx_bibblock">
Ricardo Rei, Nuno Miguel Guerreiro, José Pombal, Daan van Stigt, Marcos V. Treviso, Luísa Coheur, José G. C. de Souza, and André F. T. Martins. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.WMT-1.73" title="">Scaling up cometkiwi: Unbabel-ist 2023 submission for the quality estimation shared task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the Eighth Conference on Machine Translation, WMT 2023, Singapore, December 6-7, 2023</em>, pages 841–848. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suárez et al. (2020)</span>
<span class="ltx_bibblock">
Pedro Javier Ortiz Suárez, Laurent Romary, and Benoît Sagot. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2020.ACL-MAIN.156" title="">A monolingual approach to contextualized word embeddings for mid-resource languages</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020</em>, pages 1703–1714. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al. (2024)</span>
<span class="ltx_bibblock">
Shaomu Tan, Di Wu, and Christof Monz. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2404.11201" title="">Neuron specialization: Leveraging intrinsic task modularity for multilingual machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">CoRR</em>, abs/2404.11201.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, and et al. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2307.09288" title="">Llama 2: Open foundation and fine-tuned chat models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">CoRR</em>, abs/2307.09288.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2024)</span>
<span class="ltx_bibblock">
Di Wu, Shaomu Tan, Yan Meng, David Stap, and Christof Monz. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2401.12413" title="">How far can 100 samples go? unlocking overall zero-shot multilingual translation via tiny multi-parallel data</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">CoRR</em>, abs/2401.12413.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023)</span>
<span class="ltx_bibblock">
Di Wu, Shaomu Tan, David Stap, Ali Araabi, and Christof Monz. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2310.09946" title="">Uva-mt’s participation in the WMT23 general translation shared task</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">CoRR</em>, abs/2310.09946.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024a)</span>
<span class="ltx_bibblock">
Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2024a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=farT6XXntP" title="">A paradigm shift in machine translation: Boosting translation performance of large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024b)</span>
<span class="ltx_bibblock">
Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. 2024b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2401.08417" title="">Contrastive preference optimization: Pushing the boundaries of LLM performance in machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">CoRR</em>, abs/2401.08417.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023)</span>
<span class="ltx_bibblock">
Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://papers.nips.cc/paper_files/paper/2023/hash/ac662d74829e4407ce1d126477f4a03a-Abstract-Conference.html" title="">LIMA: less is more for alignment</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Aug 29 12:24:46 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
