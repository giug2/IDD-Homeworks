<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2307.07146] Federated Learning-Empowered AI-Generated Content in Wireless Networks</title><meta property="og:description" content="Artificial intelligence generated content (AIGC) has emerged as a promising technology to improve the efficiency, quality, diversity and flexibility of the content creation process by adopting a variety of generative A…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Learning-Empowered AI-Generated Content in Wireless Networks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Learning-Empowered AI-Generated Content in Wireless Networks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2307.07146">

<!--Generated on Wed Feb 28 18:06:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated learning,  AIGC,  wireless networks,  deep learning,  stable diffusion.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated Learning-Empowered AI-Generated Content in Wireless Networks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Xumin Huang, Peichun Li, Hongyang Du, Jiawen Kang, <em id="id1.1.id1" class="ltx_emph ltx_font_italic">Member, IEEE</em>
<br class="ltx_break">Dusit Niyato, <em id="id2.2.id2" class="ltx_emph ltx_font_italic">Fellow, IEEE</em>, Dong In Kim, <em id="id3.3.id3" class="ltx_emph ltx_font_italic">Fellow, IEEE</em>, and Yuan Wu, <em id="id4.4.id4" class="ltx_emph ltx_font_italic">Senior Member, IEEE</em>


</span><span class="ltx_author_notes">
X. Huang and P. Li are with State Key Laboratory of Internet of Things for Smart City, University of Macau, Taipa, Macau, China (e-mail: huangxu_min@163.com; peichunli@um.edu.mo).
H. Du, and D. Niyato are with the School of Computer Science and Engineering, Nanyang Technological University, Singapore (e-mail: hongyang001@e.ntu.edu.sg; dniyato@ntu.edu.sg). J. Kang is with School of Automation, Guangdong University of Technology, Guangzhou 510006, China (e-mail: kavinkang@gdut.edu.cn). D. I. Kim is with the Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon 16419, South Korea (e-mail: dikim@skku.ac.kr). Y. Wu is with State Key Laboratory of Internet of Things for Smart City, University of Macau, Taipa, Macau, China, and also with Department of Computer and Information Science, University of Macau, Taipa, Macau, China (e-mail: yuanwu@um.edu.mo). </span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Artificial intelligence generated content (AIGC) has emerged as a promising technology to improve the efficiency, quality, diversity and flexibility of the content creation process by adopting a variety of generative AI models. Deploying AIGC services in wireless networks has been expected to enhance the user experience. However, the existing AIGC service provision suffers from several limitations, e.g., the centralized training in the pre-training, fine-tuning and inference processes, especially their implementations in wireless networks with privacy preservation. Federated learning (FL), as a collaborative learning framework where the model training is distributed to cooperative data owners without the need for data sharing, can be leveraged to simultaneously improve learning efficiency and achieve privacy protection for AIGC. To this end, we present FL-based techniques for empowering AIGC, and aim to enable users to generate diverse, personalized, and high-quality content. Furthermore, we conduct a case study of FL-aided AIGC fine-tuning by using the state-of-the-art AIGC model, i.e., stable diffusion model. Numerical results show that our scheme achieves advantages in effectively reducing the communication cost and training latency and privacy protection. Finally, we highlight several major research directions and open issues for the convergence of FL and AIGC.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated learning, AIGC, wireless networks, deep learning, stable diffusion.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">To generate a vast amount of high-quality digital content for Web 3.0 applications such as Metaverse, artificial intelligence generated content (AIGC) has emerged as a promising technology to adopt a variety of generative AI models for producing, handling and modifying diverse data, e.g., text, image and audio. Due to the outstanding capability of content generation, AIGC achieves great potential in changing the lifestyle of humans and making tremendous progress in different domains. Furthermore, deploying AIGC services in wireless networks has been envisioned to enable users to have a real-time interactive environment, context awareness support, personalized and engaging experience <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. However, AIGC service provision is still facing several challenging issues. On one hand, both model size and dataset size of a large-scale AIGC models are extremely large and cause critical difficulties to pre-train, fine-tune and infer the AIGC model among resource-constrained devices. For example, Chat Generative Pre-trained Transformer (ChatGPT) is one of the latest large language models trained on a huge amount of text data, i.e., 300 billion words (570 GB) and with over 175 billion parameters. On the other hand, data privacy has become a growing concern for users in the open wireless distributed computing environment when they join AIGC services by using their local data.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Despite its great benefits and advantages, there have been several vital challenges in the AIGC processes, including pre-training, fine-tuning, and inference as follows.</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Since AIGC pre-training necessitates enormous computing power and time, most of the devices with the limited resource capacity and power supply are difficult to join this process. Their data cannot be fully utilized.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">AIGC fine-tuning transfers a pre-trained model to a wide range of downstream tasks. Fine-tuning aims to apply the pre-trained model’s knowledge to new problems, i.e., downstream tasks. But the traditional fine-tuning process relies on continuous data collection from the users, which violates individual data privacy of users.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Optimizations for AIGC inference suffer from imperfect information and uncertainties of network environment, where deep reinforcement learning (DRL) can provide a promising solution. For example, DRL was adopted in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> to orchestrate the multi-user AIGC inference. We further consider a cooperative multi-agent learning environment without centralizing the training data of all agents.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Facing the above challenges, federated learning (FL), as a most prevalent distributed training and collaborative learning framework, provides an effective approach to empower AIGC. FL enables distributed users/clients to collaboratively train a shared model while keeping all training data on the local storage. In particular, FL provides a promising scheme for enabling the AIGC pre-training and fine-tuning in a distributed manner. Compared with the centralized AIGC via cloud, distributive AIGC via FL promotes collaboration among multiple clients and has advantages in improving resource utilization efficiency, collecting fresh data for model training, reducing notable AIGC service delay and mitigating certain security and privacy threats. Thanks to FL, both efficiency and privacy of AIGC can be effectively improved. More specifically,</p>
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p">FL can be leveraged for AIGC pre-training by delegating a proper training task to a number of clients. Clients can join the pre-training process and contribute their data.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p">FL can facilitate AIGC fine-tuning such that the clients can distributively fine-tune a pre-trained AIGC model in a privacy-preserving manner.</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p">Federated reinforcement learning enables each agent to only share the learning experience with other agents and accelerate the training process, which is helpful to save the time and energy consumption for solving the multi-user AIGC inference problem.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">However, the integration of FL and AIGC is not straightforward and poses several challenging issues. Accounting for the huge scale of an AIGC model with billions of parameters, it is prohibitive for a conventional client to perform the pre-training of an entire AIGC mode or fine-tuning all model parameters. In addition, multiple iterations of local training and global aggregation in FL cause high resource consumption to AIGC services, which further aggravates challenges of deploying AIGC services in wireless networks. In federated pre-training, each client is suitable to train a small part of the AIGC model. In federated fine-tuning, we should limit the number of parameters which are fine-tuned by each client such that we can reduce the data communication of interacting with the parameter server. In federated fine-tuning and federated reinforcement learning for inference, proper FL designs can be discussed to accelerate the training convergence. The challenges motivate us to investigate how to exploit FL and its variants for empowering AIGC in different processes.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this article, we focus on leveraging FL for AIGC, and study how to decentralize the model training procedures regarding AIGC pre-training, fine-tuning and inference. We discuss the challenges of the existing AIGC, and propose instrumental FL-based techniques to empower AIGC and implement the FL procedure in different approaches, i.e., parallel, split and sequential, according to different application conditions. We compare the techniques and reveal the remarkable benefits of FL to AIGC.
Besides, we present a case study of FL-aided AIGC fine-tuning, where we consider the stable diffusion model, one of the state-of-the-art AIGC models, and study how to fine-tune the stable diffusion model on a neural style transfer task under the FL setting. Finally, we provide extensive discussions about opportunities and challenges of FL-empowered AIGC.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The main contributions of this article can be summarized as follows.</p>
<ul id="S1.I3" class="ltx_itemize">
<li id="S1.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I3.i1.p1" class="ltx_para">
<p id="S1.I3.i1.p1.1" class="ltx_p">We propose a FL-empowered approach to introduce the FL-based techniques for the AIGC processes. To the best of our knowledge, this is the first work that considers the integration of FL and AIGC in wireless environment.</p>
</div>
</li>
<li id="S1.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I3.i2.p1" class="ltx_para">
<p id="S1.I3.i2.p1.1" class="ltx_p">We present a case study to show the potential of FL for fine-tuning the stable diffusion model. This case study can be straightforwardly extended to other AIGC fine-tuning scenarios. Compared with a traditional scheme, our scheme reduces both the communication cost and training latency while achieving the identical convergence performance to it.</p>
</div>
</li>
<li id="S1.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I3.i3.p1" class="ltx_para">
<p id="S1.I3.i3.p1.1" class="ltx_p">We outline research challenges and present potential solutions to the FL-empowered AIGC. We discuss open directions on how economic theories and emerging technologies such as semantic communication, blockchain and edge intelligence can be leveraged for the convergence of FL and AIGC.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Deep Generative Models and AIGC Processes</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we present a short overview of FL-empowered AIGC, as shown in Fig. <a href="#S2.F1" title="Figure 1 ‣ II-A Fundamentals of Deep Generative Models ‣ II Deep Generative Models and AIGC Processes ‣ Federated Learning-Empowered AI-Generated Content in Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We introduce the deep generative models and AIGC processes. The up-to-date research works of applying FL for the AIGC processes are also discussed.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Fundamentals of Deep Generative Models</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">As a key enabling technology for AIGC, deep generative model learns to generate predictions in the identical modality as the input data with one modality. The rapid development of deep generative models has witnessed several promising techniques and we pay attention to Generative Adversarial Network (GAN), Variational Autoencode (VAE), diffusion model and Transformer. We summarize technique details of the deep generative models and their comparisons in Table I.</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><em id="S2.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">GAN</em>: As a unsupervised machine learning algorithm, GAN aims to generate new data from the same space of the original data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. GAN consists of two neural networks named by generator and discriminator, which are trained together in an adversarial manner. However, GAN cannot support multi-modal generation.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><em id="S2.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">VAE</em>: As a probabilistic generative model, VAE aims to maximize the probability of the generated output with respect to the input data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. VAE is an adaptation from a standard autoencoder, transforms the data from a higher-dimensional to a lower-dimensional space and further tackles the critical problem of non-regularized latent space in the traditional autoencoder. Both GAN and VAE are two popular approaches that have been widely commonly used for generative modeling. Compared with GAN, VAE learns from labeled data and is easier to train.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><em id="S2.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">Diffusion model</em>: Also known as diffusion probabilistic model, diffusion model is inspired by non-equilibrium thermodynamics proposed by Sohl-Dickstein et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Diffusion model destructs training data by gradually adding Gaussian noise in the forward process, then reverses this process to generate the desired data from the noise. Compared with VAE and GAN, diffusion model can achieve a better learning performance but requires more training data, computational cost and time.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p"><em id="S2.I1.i4.p1.1.1" class="ltx_emph ltx_font_italic">Transformer</em>: Following an encoder-decoder structure, Transformer relies on the adaption of self-attention layers in both the encoder and decoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. By capturing the relations among elements of the sequential data, Transformer is suited to process sequence-to-sequence prediction tasks, and enables parallel training for both the data and model. Compared with the above generative models, Transformer achieves an advantage in performing the large-scale training.</p>
</div>
</li>
</ul>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2307.07146/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An overview of federated learning-empowered AIGC</figcaption>
</figure>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Deep generative models for federated learning-aided AIGC</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.1.1.1" class="ltx_p" style="width:32.5pt;"><span id="S2.T1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></span>
</span>
</th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.2.1.1" class="ltx_p" style="width:54.2pt;"><span id="S2.T1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Component</span></span>
</span>
</th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.3.1.1" class="ltx_p" style="width:97.6pt;"><span id="S2.T1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Feature</span></span>
</span>
</th>
<th id="S2.T1.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.4.1.1" class="ltx_p" style="width:97.6pt;"><span id="S2.T1.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Comparison</span></span>
</span>
</th>
<th id="S2.T1.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.5.1.1" class="ltx_p" style="width:97.6pt;"><span id="S2.T1.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold">Application</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<td id="S2.T1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.1.1.1" class="ltx_p" style="width:32.5pt;">GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite></span>
</span>
</td>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.2.1.1" class="ltx_p" style="width:54.2pt;">Generator and discriminator</span>
</span>
</td>
<td id="S2.T1.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.3.1.1" class="ltx_p" style="width:97.6pt;">Two models are trained together in an adversarial manner in terms of a zero-sum non-cooperative game.</span>
</span>
</td>
<td id="S2.T1.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.4.1.1" class="ltx_p" style="width:97.6pt;">GAN adopts unsupervised learning and is sensitive to hyperparameter settings.</span>
</span>
</td>
<td id="S2.T1.1.2.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.5.1.1" class="ltx_p" style="width:97.6pt;">Image generation, restoration and manipulation, super resolution and 3D shape reconstruction.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<td id="S2.T1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.1.1.1" class="ltx_p" style="width:32.5pt;">VAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite></span>
</span>
</td>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.2.1.1" class="ltx_p" style="width:54.2pt;">Encoder-decoder model</span>
</span>
</td>
<td id="S2.T1.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.3.1.1" class="ltx_p" style="width:97.6pt;">VAE tries to create latent representations of training data in a probabilistic manner.</span>
</span>
</td>
<td id="S2.T1.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.4.1.1" class="ltx_p" style="width:97.6pt;">VAE utilizes supervised learning and is easier to train and generate more coherent data.</span>
</span>
</td>
<td id="S2.T1.1.3.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.5.1.1" class="ltx_p" style="width:97.6pt;">Image denoising, image generation and audio synthesis.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<td id="S2.T1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.1.1.1" class="ltx_p" style="width:32.5pt;">Diffusion model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite></span>
</span>
</td>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.2.1.1" class="ltx_p" style="width:54.2pt;">Likelihood-based model</span>
</span>
</td>
<td id="S2.T1.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.3.1.1" class="ltx_p" style="width:97.6pt;">Add random noise to input data in the forward process and recover the data by removing the noise in the reverse process.</span>
</span>
</td>
<td id="S2.T1.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.4.1.1" class="ltx_p" style="width:97.6pt;">Diffusion model has the better learning performance but requires more training data, computational cost and time.</span>
</span>
</td>
<td id="S2.T1.1.4.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.5.1.1" class="ltx_p" style="width:97.6pt;">Computer version, natural language generation, and multi-modal generation.</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.5.4" class="ltx_tr">
<td id="S2.T1.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.1.1.1" class="ltx_p" style="width:32.5pt;">Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite></span>
</span>
</td>
<td id="S2.T1.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.2.1.1" class="ltx_p" style="width:54.2pt;">Sequence-to-sequence encoder-decoder model</span>
</span>
</td>
<td id="S2.T1.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.3.1.1" class="ltx_p" style="width:97.6pt;">There are self-attention layers in both the encoder and decoder and each layer performs a multi-head attention mechanism.</span>
</span>
</td>
<td id="S2.T1.1.5.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.4.1.1" class="ltx_p" style="width:97.6pt;">Transformer enables multiple attention heads to work in parallel, and achieves an advantage in the scalability to large-scale datasets.</span>
</span>
</td>
<td id="S2.T1.1.5.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" style="padding:5pt 5.0pt;">
<span id="S2.T1.1.5.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.5.1.1" class="ltx_p" style="width:97.6pt;">Natural language processing and computer vision.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">To enable the convenient use of AIGC, pre-trained large models, i.e., foundation models, are trained on large, unlabeled datasets and developed to adapt to a wide range of downstream tasks. In recent years, leading AI industries have presented a variety of pre-trained large models for AIGC, including the models for natural language processing such as BERT by Google and GPT by OpenAI, the models for computer vision such as Florence by Microsoft, the models for multimodal training such as Dall-E 2 by OpenAI, Imagen by Google, and stable diffusion by Stability AI. We provide more details of GPT and stable diffusion as follows.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Many foundation models in the natural language processing domain rely on the Transformer-based architecture for training, due to the remarkable learning ability and parallelism of Transformer.
In this regard, GPT refers to a series of generative pre-Trained Transformer models released by OpenAI and has evolved from the first version GPT-1 in 2018 to the latest model GPT-4 in 2023. GPT 3.5 is an intermediate version between GPT-3 and GPT-4. As a popular AI chatbot based on GPT-3.5, ChatGPT is developed as a variant of GPT with chatbot functionality, and is fine-tuned by jointly utilizing supervised learning and reinforcement learning to improve the conversational abilities of the chatbot<span id="footnotex1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://openai.com/blog/chatgpt</span></span></span>. Due to the fast-rising popularity, ChatGPT reportedly has over 100 million active users globally. In addition, diffusion model provides a cutting-edge approach for text-conditioned image generation and stable diffusion as one of the well-known foundation models have attracted extensive attention. Stable diffusion is an open-source visual generative foundation model released by Stability AI in 2022, and has been commonly used for generating high-quality images in massive applications. Stable diffusion is one of the most flexible AI image generators to create AI-generated images and modify the images on demand, e.g., inpainting and super-resolution, according to the text prompts<span id="footnotex2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://stability.ai/blog/stable-diffusion-v2-release</span></span></span>. Besides, running a stable diffusion model also requires relatively lower memory, e.g., 16 GB of DDR4 or DDR5 RAM, and GPU usage, e.g, 10 GB VRAM.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">AIGC Processes</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In the following, we discuss the challenging issues and current solutions of AIGC processes, including pre-training, fine-tuning and inference.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS1.4.1.1" class="ltx_text">II-B</span>1 </span>Pre-training</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">AIGC depends on large-scale pre-trained models with several billions of parameters. Existing pre-training methods require an available large-scale dataset and a huge investment in hardware (e.g., thousands of GPUs) and time (e.g., several days). For example, training a GPT-3 model with over 175 billion parameters requires more than 4 months with 1,000 GPUs and the training cost is estimated from 4.6 to 12 million US dollars for a single training run. The big companies like Google and OpenAI can afford but individuals and small companies are difficult to help pre-train a large model although they have available training data. Effective collaboration among different parties should be explored to facilitate the pre-training process and improve the diversity of pre-training data.</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.1" class="ltx_p">To improve the pre-training performance, effective schemes are provided to alleviate the unbearablel computational time and expense. Take the Transformer-based foundation models as an example. Proper pre-training designs are introduced to make well use of the inherent parallelism in Transformers. Model architecture change by applying the switchable transformer blocks to drop some Transformer layers for each mini-batch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> is proposed to train the models at a faster rate.
FL has been leveraged to enhance the pre-training processes of BERT and ImageNet. The authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> combine FL with split learning to prevent collecting the tremendous training data and permit resource-constrained clients to join the BERT pre-training. Split learning enables each client to train a computation-lightweight part of the model as a client-side model and FL further enables all clients to collaboratively train the shared client-side model in a privacy-preserving manner. Federated self-supervised learning is integrated with the existing masked image modeling methods to facilitate the ImageNet pre-training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Comparison of different fine-tuning methods for federated learning-aided AIGC</figcaption>
<table id="S2.T2.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.8.9.1" class="ltx_tr">
<th id="S2.T2.8.9.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S2.T2.8.9.1.1.1" class="ltx_text">Method</span></th>
<th id="S2.T2.8.9.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" rowspan="2">
<span id="S2.T2.8.9.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.8.9.1.2.1.1" class="ltx_p">Description</span>
</span>
</th>
<th id="S2.T2.8.9.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" rowspan="2">
<span id="S2.T2.8.9.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.8.9.1.3.1.1" class="ltx_p">Feature</span>
</span>
</th>
<th id="S2.T2.8.9.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Combining with federated learning</th>
</tr>
<tr id="S2.T2.8.10.2" class="ltx_tr">
<th id="S2.T2.8.10.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Pros</th>
<th id="S2.T2.8.10.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Cons</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.1.1" class="ltx_tr">
<td id="S2.T2.1.1.2" class="ltx_td ltx_align_left ltx_border_t">Traditional</td>
<td id="S2.T2.1.1.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T2.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.1.3.1.1" class="ltx_p">Fine-tune the full or partial weights of a pre-trained model.</span>
</span>
</td>
<td id="S2.T2.1.1.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.1.1.1.1" class="ltx_p">Very large gradient size for model update (1<math id="S2.T2.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.T2.1.1.1.1.1.m1.1a"><mo id="S2.T2.1.1.1.1.1.m1.1.1" xref="S2.T2.1.1.1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S2.T2.1.1.1.1.1.m1.1.1.cmml" xref="S2.T2.1.1.1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.1.1.1.1.1.m1.1c">\sim</annotation></semantics></math>10GB)</span>
</span>
</td>
<td id="S2.T2.1.1.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T2.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.1.4.1.1" class="ltx_p">Simple and effective</span>
</span>
</td>
<td id="S2.T2.1.1.5" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T2.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.1.1.5.1.1" class="ltx_p">Likely causing over-fitting when learning with few training samples</span>
</span>
</td>
</tr>
<tr id="S2.T2.3.3" class="ltx_tr">
<td id="S2.T2.3.3.3" class="ltx_td ltx_align_left ltx_border_t">LoRA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</td>
<td id="S2.T2.3.3.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T2.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.3.4.1.1" class="ltx_p">Add extra bypass layers with few parameters for low-rank adaption.</span>
</span>
</td>
<td id="S2.T2.3.3.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T2.3.3.2.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.3.2.2.2" class="ltx_p">Moderate gradient size (<math id="S2.T2.2.2.1.1.1.m1.1" class="ltx_Math" alttext="\sim 100" display="inline"><semantics id="S2.T2.2.2.1.1.1.m1.1a"><mrow id="S2.T2.2.2.1.1.1.m1.1.1" xref="S2.T2.2.2.1.1.1.m1.1.1.cmml"><mi id="S2.T2.2.2.1.1.1.m1.1.1.2" xref="S2.T2.2.2.1.1.1.m1.1.1.2.cmml"></mi><mo id="S2.T2.2.2.1.1.1.m1.1.1.1" xref="S2.T2.2.2.1.1.1.m1.1.1.1.cmml">∼</mo><mn id="S2.T2.2.2.1.1.1.m1.1.1.3" xref="S2.T2.2.2.1.1.1.m1.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.2.2.1.1.1.m1.1b"><apply id="S2.T2.2.2.1.1.1.m1.1.1.cmml" xref="S2.T2.2.2.1.1.1.m1.1.1"><csymbol cd="latexml" id="S2.T2.2.2.1.1.1.m1.1.1.1.cmml" xref="S2.T2.2.2.1.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S2.T2.2.2.1.1.1.m1.1.1.2.cmml" xref="S2.T2.2.2.1.1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S2.T2.2.2.1.1.1.m1.1.1.3.cmml" xref="S2.T2.2.2.1.1.1.m1.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.2.2.1.1.1.m1.1c">\sim 100</annotation></semantics></math> MB), moderate training data (<math id="S2.T2.3.3.2.2.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.T2.3.3.2.2.2.m2.1a"><mo id="S2.T2.3.3.2.2.2.m2.1.1" xref="S2.T2.3.3.2.2.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.T2.3.3.2.2.2.m2.1b"><csymbol cd="latexml" id="S2.T2.3.3.2.2.2.m2.1.1.cmml" xref="S2.T2.3.3.2.2.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.3.3.2.2.2.m2.1c">\sim</annotation></semantics></math> 30 samples)</span>
</span>
</td>
<td id="S2.T2.3.3.5" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T2.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.3.5.1.1" class="ltx_p">Exploiting decomposition matrices for <span id="S2.T2.3.3.5.1.1.1" class="ltx_text ltx_font_italic">communication and data efficient</span> tuning</span>
</span>
</td>
<td id="S2.T2.3.3.6" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T2.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.3.3.6.1.1" class="ltx_p">Deteriorate the diversity of the generated content</span>
</span>
</td>
</tr>
<tr id="S2.T2.5.5" class="ltx_tr">
<td id="S2.T2.5.5.3" class="ltx_td ltx_align_left ltx_border_t">DreamBooth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</td>
<td id="S2.T2.5.5.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T2.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.5.5.4.1.1" class="ltx_p">Create a personalized model for a specific subject under various contexts.</span>
</span>
</td>
<td id="S2.T2.5.5.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T2.5.5.2.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.5.5.2.2.2" class="ltx_p">Large gradient size (<math id="S2.T2.4.4.1.1.1.m1.1" class="ltx_Math" alttext="\sim 2" display="inline"><semantics id="S2.T2.4.4.1.1.1.m1.1a"><mrow id="S2.T2.4.4.1.1.1.m1.1.1" xref="S2.T2.4.4.1.1.1.m1.1.1.cmml"><mi id="S2.T2.4.4.1.1.1.m1.1.1.2" xref="S2.T2.4.4.1.1.1.m1.1.1.2.cmml"></mi><mo id="S2.T2.4.4.1.1.1.m1.1.1.1" xref="S2.T2.4.4.1.1.1.m1.1.1.1.cmml">∼</mo><mn id="S2.T2.4.4.1.1.1.m1.1.1.3" xref="S2.T2.4.4.1.1.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.4.4.1.1.1.m1.1b"><apply id="S2.T2.4.4.1.1.1.m1.1.1.cmml" xref="S2.T2.4.4.1.1.1.m1.1.1"><csymbol cd="latexml" id="S2.T2.4.4.1.1.1.m1.1.1.1.cmml" xref="S2.T2.4.4.1.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S2.T2.4.4.1.1.1.m1.1.1.2.cmml" xref="S2.T2.4.4.1.1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S2.T2.4.4.1.1.1.m1.1.1.3.cmml" xref="S2.T2.4.4.1.1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.4.4.1.1.1.m1.1c">\sim 2</annotation></semantics></math> GB), very little training data (<math id="S2.T2.5.5.2.2.2.m2.1" class="ltx_Math" alttext="3\sim 5" display="inline"><semantics id="S2.T2.5.5.2.2.2.m2.1a"><mrow id="S2.T2.5.5.2.2.2.m2.1.1" xref="S2.T2.5.5.2.2.2.m2.1.1.cmml"><mn id="S2.T2.5.5.2.2.2.m2.1.1.2" xref="S2.T2.5.5.2.2.2.m2.1.1.2.cmml">3</mn><mo id="S2.T2.5.5.2.2.2.m2.1.1.1" xref="S2.T2.5.5.2.2.2.m2.1.1.1.cmml">∼</mo><mn id="S2.T2.5.5.2.2.2.m2.1.1.3" xref="S2.T2.5.5.2.2.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.5.5.2.2.2.m2.1b"><apply id="S2.T2.5.5.2.2.2.m2.1.1.cmml" xref="S2.T2.5.5.2.2.2.m2.1.1"><csymbol cd="latexml" id="S2.T2.5.5.2.2.2.m2.1.1.1.cmml" xref="S2.T2.5.5.2.2.2.m2.1.1.1">similar-to</csymbol><cn type="integer" id="S2.T2.5.5.2.2.2.m2.1.1.2.cmml" xref="S2.T2.5.5.2.2.2.m2.1.1.2">3</cn><cn type="integer" id="S2.T2.5.5.2.2.2.m2.1.1.3.cmml" xref="S2.T2.5.5.2.2.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.5.5.2.2.2.m2.1c">3\sim 5</annotation></semantics></math> samples)</span>
</span>
</td>
<td id="S2.T2.5.5.5" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T2.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.5.5.5.1.1" class="ltx_p">Leverage the semantic prior for <span id="S2.T2.5.5.5.1.1.1" class="ltx_text ltx_font_italic">data-efficient</span> training</span>
</span>
</td>
<td id="S2.T2.5.5.6" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T2.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.5.5.6.1.1" class="ltx_p">Communication-intensive fine-tuning for each single subject</span>
</span>
</td>
</tr>
<tr id="S2.T2.7.7" class="ltx_tr">
<td id="S2.T2.7.7.3" class="ltx_td ltx_align_left ltx_border_t">Textual inversion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S2.T2.7.7.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T2.7.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.7.7.4.1.1" class="ltx_p">Capture novel concepts for personalized image generation via embedding tuning.</span>
</span>
</td>
<td id="S2.T2.7.7.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T2.7.7.2.2" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.7.7.2.2.2" class="ltx_p">Small embedding vector (<math id="S2.T2.6.6.1.1.1.m1.1" class="ltx_Math" alttext="\sim 1" display="inline"><semantics id="S2.T2.6.6.1.1.1.m1.1a"><mrow id="S2.T2.6.6.1.1.1.m1.1.1" xref="S2.T2.6.6.1.1.1.m1.1.1.cmml"><mi id="S2.T2.6.6.1.1.1.m1.1.1.2" xref="S2.T2.6.6.1.1.1.m1.1.1.2.cmml"></mi><mo id="S2.T2.6.6.1.1.1.m1.1.1.1" xref="S2.T2.6.6.1.1.1.m1.1.1.1.cmml">∼</mo><mn id="S2.T2.6.6.1.1.1.m1.1.1.3" xref="S2.T2.6.6.1.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.6.6.1.1.1.m1.1b"><apply id="S2.T2.6.6.1.1.1.m1.1.1.cmml" xref="S2.T2.6.6.1.1.1.m1.1.1"><csymbol cd="latexml" id="S2.T2.6.6.1.1.1.m1.1.1.1.cmml" xref="S2.T2.6.6.1.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S2.T2.6.6.1.1.1.m1.1.1.2.cmml" xref="S2.T2.6.6.1.1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S2.T2.6.6.1.1.1.m1.1.1.3.cmml" xref="S2.T2.6.6.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.6.6.1.1.1.m1.1c">\sim 1</annotation></semantics></math> KB), very little training data (<math id="S2.T2.7.7.2.2.2.m2.1" class="ltx_Math" alttext="3\sim 5" display="inline"><semantics id="S2.T2.7.7.2.2.2.m2.1a"><mrow id="S2.T2.7.7.2.2.2.m2.1.1" xref="S2.T2.7.7.2.2.2.m2.1.1.cmml"><mn id="S2.T2.7.7.2.2.2.m2.1.1.2" xref="S2.T2.7.7.2.2.2.m2.1.1.2.cmml">3</mn><mo id="S2.T2.7.7.2.2.2.m2.1.1.1" xref="S2.T2.7.7.2.2.2.m2.1.1.1.cmml">∼</mo><mn id="S2.T2.7.7.2.2.2.m2.1.1.3" xref="S2.T2.7.7.2.2.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.7.7.2.2.2.m2.1b"><apply id="S2.T2.7.7.2.2.2.m2.1.1.cmml" xref="S2.T2.7.7.2.2.2.m2.1.1"><csymbol cd="latexml" id="S2.T2.7.7.2.2.2.m2.1.1.1.cmml" xref="S2.T2.7.7.2.2.2.m2.1.1.1">similar-to</csymbol><cn type="integer" id="S2.T2.7.7.2.2.2.m2.1.1.2.cmml" xref="S2.T2.7.7.2.2.2.m2.1.1.2">3</cn><cn type="integer" id="S2.T2.7.7.2.2.2.m2.1.1.3.cmml" xref="S2.T2.7.7.2.2.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.7.7.2.2.2.m2.1c">3\sim 5</annotation></semantics></math> samples)</span>
</span>
</td>
<td id="S2.T2.7.7.5" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T2.7.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.7.7.5.1.1" class="ltx_p">Manipulate the word embedding for <span id="S2.T2.7.7.5.1.1.1" class="ltx_text ltx_font_italic">communication and data efficient</span> fine-tuning</span>
</span>
</td>
<td id="S2.T2.7.7.6" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T2.7.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.7.7.6.1.1" class="ltx_p">Computation-intensive fine-tuning for each word embedding</span>
</span>
</td>
</tr>
<tr id="S2.T2.8.8" class="ltx_tr">
<td id="S2.T2.8.8.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S2.T2.8.8.3" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t">
<span id="S2.T2.8.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.8.8.3.1.1" class="ltx_p">Introduce extra conditional input for controlling the generated content.</span>
</span>
</td>
<td id="S2.T2.8.8.1" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t">
<span id="S2.T2.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.8.8.1.1.1" class="ltx_p">Large gradient size (<math id="S2.T2.8.8.1.1.1.m1.1" class="ltx_Math" alttext="\sim 1" display="inline"><semantics id="S2.T2.8.8.1.1.1.m1.1a"><mrow id="S2.T2.8.8.1.1.1.m1.1.1" xref="S2.T2.8.8.1.1.1.m1.1.1.cmml"><mi id="S2.T2.8.8.1.1.1.m1.1.1.2" xref="S2.T2.8.8.1.1.1.m1.1.1.2.cmml"></mi><mo id="S2.T2.8.8.1.1.1.m1.1.1.1" xref="S2.T2.8.8.1.1.1.m1.1.1.1.cmml">∼</mo><mn id="S2.T2.8.8.1.1.1.m1.1.1.3" xref="S2.T2.8.8.1.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.8.8.1.1.1.m1.1b"><apply id="S2.T2.8.8.1.1.1.m1.1.1.cmml" xref="S2.T2.8.8.1.1.1.m1.1.1"><csymbol cd="latexml" id="S2.T2.8.8.1.1.1.m1.1.1.1.cmml" xref="S2.T2.8.8.1.1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S2.T2.8.8.1.1.1.m1.1.1.2.cmml" xref="S2.T2.8.8.1.1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S2.T2.8.8.1.1.1.m1.1.1.3.cmml" xref="S2.T2.8.8.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.8.8.1.1.1.m1.1c">\sim 1</annotation></semantics></math> GB), extra inference cost</span>
</span>
</td>
<td id="S2.T2.8.8.4" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t">
<span id="S2.T2.8.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.8.8.4.1.1" class="ltx_p">Improve the <span id="S2.T2.8.8.4.1.1.1" class="ltx_text ltx_font_italic">scalability</span> of the based model from diverse data over edge networks</span>
</span>
</td>
<td id="S2.T2.8.8.5" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t">
<span id="S2.T2.8.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.8.8.5.1.1" class="ltx_p">Communication-intensive; incompatible with joint training of multiple controllers</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS2.4.1.1" class="ltx_text">II-B</span>2 </span>Fine-tuning</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">Before the practical use of a pre-trained AIGC model, fine-tuning is performed to properly enhance the model’s performance on a downstream task and adjust the model’s parameters to suit the new data, e.g., in a new domain.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">We specifically consider the diffusion-based text-to-image generative model and introduce baseline methods for AIGC fine-tuning. The traditional fine-tuning method is to directly fine-tune the full or partial weights of a pre-trained model. Many advanced fine-tuning methods have been presented to overcome the limitations of the traditional method. Low-rank adaptation (LoRA) fine-tuning is proposed by Microsoft to explore low-rank adaptation to facilitate the fine-tuning process in a storage and computation efficient manner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. In Section IV, we apply this method to fine-tune a stable diffusion model on a customized dataset. DreamBooth fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> is proposed by Google to bind a unique identifier with an interesting subject in a few (typically 3-5) input images, and embed the subject within the pre-trained diffusion model’s output domain to generate new images of the subject. Textual inversion fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> is proposed by NVIDIA to find new pseudo-words that capture high-level semantic information and fine-grained visual details in the textual embedding space of a frozen latent diffusion model. ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> adds an adapter model to a frozen pre-trained diffusion model, and train the adapter model to let the diffusion model support additional input conditions. We have a brief introduction of the above fine-tuning methods and compare their gradient and training data sizes in the first three columns of Table II.</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS3.4.1.1" class="ltx_text">II-B</span>3 </span>Inference</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">A fine-tuned AIGC model is deployed in wireless networks to provide AIGC inference services for users. Several works have been presented to improve the inference efficiency and provide considerable payments for the inference services. To incentivize AIGC service providers, a diffusion-based contract theory is employed to tackle the problem on how a user designs proper contracts for different AIGC service providers with different types <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Deep reinforcement learning is adopted to tackle the inference service matching problem under incomplete information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Reinforcement learning algorithms show great potential to overcome the incomplete information condition when the inference services have insufficient prior knowledge. Furthermore, we need to prevent each agent from directly sharing any private information in the cooperative multi-agent learning environment.</p>
</div>
<div id="S2.SS2.SSS3.p2" class="ltx_para">
<p id="S2.SS2.SSS3.p2.1" class="ltx_p">According to the overview of AIGC, we realize that FL has begun to be applied for decentralizing the model training procedures regrading the AIGC processes. However, the integration of FL and AIGC is not straightforward. We take fine-tuning an AIGC model in a federated manner as an example. For the privacy protection, the federated fine-tuning methods are motivated but technical challenges remain unresolved. For example, the combination of FL and LoRA is helpful to exploit the decomposition matrices for achieving the communication and data-efficient fine-tuning while this may deteriorate the diversity of the generated content. We summarize the pros and cons of combining the fine-tuning methods with FL for AIGC in the last two columns of Table II.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Federated Learning-Aided AIGC</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we introduce features, technique details and discussions of FL-aided AIGC.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Features of Federated Learning Designs for AIGC</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We discuss the necessitated features of the FL designs for AIGC fine-tuning when both the FL and AIGC are deployed in wireless networks.</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><em id="S3.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">Data-efficient</em>: For local model training of each client, the amount of downstream data is properly decided to overcome the overfitting problem and restrict the number of local training iterations.
</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><em id="S3.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">Hardware-friendly</em>: Fine-tuning methods are selected and adjusted to reduce the GPU VRAM usage and make GPU-based computation more efficient.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><em id="S3.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">Parameter-efficient</em>: Parameter-efficient fine-tuning techniques aim to train a small portion of the model parameters while keeping the rest frozen to cut down the computation workloads, time and communication cost. </p>
</div>
</li>
</ul>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2307.07146/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="368" height="361" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Federated learning-based techniques for AIGC</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Federated Learning-Based Techniques for AIGC</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We propose FL and the variants to empower AIGC, as shown in Fig. <a href="#S3.F2" title="Figure 2 ‣ III-A Features of Federated Learning Designs for AIGC ‣ III Federated Learning-Aided AIGC ‣ Federated Learning-Empowered AI-Generated Content in Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. In the conventional FL, all clients train an entire learning model in a parallel approach. This may hinder the participation willingness of lightweight clients that are difficult to train the computation-intensive model, and degrade the convergence rate when a large number of clients with heterogeneous data sizes and computing capability participate in FL. Thus, we propose to implement FL in other approaches, i.e., split and sequential, according to the specific application scenarios. We aim to extend the adoption of FL in the AIGC processes, and make FL designs better suited for different application purposes. We provide more details as follows.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS1.4.1.1" class="ltx_text">III-B</span>1 </span>FL in a parallel approach</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">: The implementation of the conventional FL is shown in Fig. 2(a). In each global round (i.e., communication round), Steps 2-5 are performed and the steps repeat until the global model converges.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS2.4.1.1" class="ltx_text">III-B</span>2 </span>FL in a split approach</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.3" class="ltx_p">: The implementation depends on the joint utilization of FL and split learning. The entire AIGC model is split into two sub-models: client-side model <math id="S3.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\bf{C}" display="inline"><semantics id="S3.SS2.SSS2.p1.1.m1.1a"><mi id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml">𝐂</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.1b"><ci id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1">𝐂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.1c">\bf{C}</annotation></semantics></math> and server-side model <math id="S3.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="\bf{S}" display="inline"><semantics id="S3.SS2.SSS2.p1.2.m2.1a"><mi id="S3.SS2.SSS2.p1.2.m2.1.1" xref="S3.SS2.SSS2.p1.2.m2.1.1.cmml">𝐒</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.2.m2.1b"><ci id="S3.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1">𝐒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.2.m2.1c">\bf{S}</annotation></semantics></math>. The split approach provides an opportunity for general clients to collaboratively train a large AIGC model since the client-side model is computation-lightweight and acceptable for the general clients with limited computing capability. In a global round of FL, each client receives the initialized weights of the client-side model and trains it by using the local data. The parameter server has dual responsibilities: i) helps each client update its client-side model and updates its server-side model by using split learning, ii) aggregates all client-side models by using the FedAvg algorithm, which assigns higher weights to the clients with more local data. Here, output of the last layer of a client-side model is named by smash data <math id="S3.SS2.SSS2.p1.3.m3.1" class="ltx_Math" alttext="\bf{D}" display="inline"><semantics id="S3.SS2.SSS2.p1.3.m3.1a"><mi id="S3.SS2.SSS2.p1.3.m3.1.1" xref="S3.SS2.SSS2.p1.3.m3.1.1.cmml">𝐃</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.3.m3.1b"><ci id="S3.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1">𝐃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.3.m3.1c">\bf{D}</annotation></semantics></math>. We provide more details of the global round in Fig. 2(b).</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS3.4.1.1" class="ltx_text">III-B</span>3 </span>FL in a sequential approach</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">: The implementation depends on device-to-device (D2D) based model propagation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> among all clients. All local model training tasks are performed among the clients one by one. A client takes a turn to play as a relay training node, which receives a local model from its previous client, train the local model by using the local data, and pass the updated local model to the next client. The model propagation between any two adjacent clients is quickly performed by using D2D communications to improve spectral and energy efficiency of cellular networks. We provide more details of the global round in Fig. 2(c).</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Discussions on Federated Learning-Empowered AIGC</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">FL can be performed in different approaches to empower AIGC. The parallel approach is easily applied and can resist a certain proportion of security threats such as model poisoning attacks through the gradient averaging operation. However, it presents a relatively higher computational requirement for the participating clients since each client needs to train the entire AIGC model. The convergence performance of the original FL cannot be guaranteed with the increasing number of the clients. The split approach prevents the clients from undertaking the heavy computation workloads, and enables the resource-constrained clients to join the FL procedure by letting them only train a computational-lightweight part of the model. The split approach is suitable for training the models with Transformer-based architectures since the models are conveniently split into two parts with different computation workloads. However, the split approach may be inapplicable for training some special AIGC models since the parameter server could simultaneously require the input data and label data of each client to perform the backward propagation for updating the client-side model. For example, FL in a split approach is not compatible with the training of a conventional denoising diffusion probabilistic model, which necessitates the original image data of the clients for each training iteration. At this time, sharing the local data of the clients with the parameter server will violate the original privacy-preserving rule of FL. The sequential approach is convinced to greatly improve the convergence rate of FL. However, the sequential approach could aggravate some security threats to the FL procedure. Due to the model propagation among the clients, the sequential approach makes the FL procedure more susceptible to model poisoning attacks and gradient leakage attacks.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The proposed FL-based techniques are helpful to facilitate the development and deployment of the AIGC models with more data utilization for pre-training, less communication cost for fine-tuning and higher-level optimizations for inference. We summarize the benefits of FL to AIGC as follows.</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><em id="S3.I2.i1.p1.1.1" class="ltx_emph ltx_font_italic">Data diversity for AIGC pre-training</em>: The diversity of pre-training data increases
after FL enables the clients to help pre-train a large AIGC model. </p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><em id="S3.I2.i2.p1.1.1" class="ltx_emph ltx_font_italic">Communication saving for AIGC fine-tuning</em>: FL and the parameter-efficient fine-tuning methods work together to improve the communication efficiency in the AIGC fine-tuning.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p"><em id="S3.I2.i3.p1.1.1" class="ltx_emph ltx_font_italic">Knowledge transferring for AIGC inference</em>: Federated reinforcement learning encourages the agents to use the knowledge of other agents, which is beneficial to accelerate the training process of the multi-agent learning.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Case Study: Federated Fine-tuning for a Stable Diffusion Model</span>
</h2>

<figure id="S4.F3" class="ltx_figure"><img src="/html/2307.07146/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="227" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Federated fine-tuning for a stable diffusion model</figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we present a detailed case study of federated fine-tuning for a stable diffusion model. We perform the FL procedure in a sequential approach for fine-tuning the stable diffusion model in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. We aim to generate traditional Chinese ink paintings by leveraging the LoRA-based neural style transfer technique. We design a federated fine-tuning framework where multiple clients collaboratively customize the general image synthesis model for creating a unique artistic style and D2D based model propagation is utilized to accelerate the training convergence for them.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.4" class="ltx_p">We consider a training scenario with a set of clients and a parameter server.
Let <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="\bm{w}" display="inline"><semantics id="S4.p2.1.m1.1a"><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">𝒘</mi><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">𝒘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">\bm{w}</annotation></semantics></math> denote the weight of the foundation stable diffusion model, and <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="\bm{v}" display="inline"><semantics id="S4.p2.2.m2.1a"><mi id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">𝒗</mi><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><ci id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">𝒗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">\bm{v}</annotation></semantics></math> denote the weight of the additional bypass introduced by LoRA. During the fine-tuning process, we fix the base weight <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="\bm{w}" display="inline"><semantics id="S4.p2.3.m3.1a"><mi id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml">𝒘</mi><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><ci id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1">𝒘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">\bm{w}</annotation></semantics></math> and only train the additional weight <math id="S4.p2.4.m4.1" class="ltx_Math" alttext="\bm{v}" display="inline"><semantics id="S4.p2.4.m4.1a"><mi id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml">𝒗</mi><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><ci id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1">𝒗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">\bm{v}</annotation></semantics></math>. The training procedure of the federated fine-tuning with the D2D communications is described as follows.</p>
</div>
<div id="S4.p3" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.5" class="ltx_p">Step 1: The parameter server initializes an index set <math id="S4.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="{\cal S}" display="inline"><semantics id="S4.I1.i1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.I1.i1.p1.1.m1.1.1" xref="S4.I1.i1.p1.1.m1.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.1.m1.1b"><ci id="S4.I1.i1.p1.1.m1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.1.m1.1c">{\cal S}</annotation></semantics></math> that equalizes the original set of the clients, and randomly selects a client <math id="S4.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="i\in{\cal S}" display="inline"><semantics id="S4.I1.i1.p1.2.m2.1a"><mrow id="S4.I1.i1.p1.2.m2.1.1" xref="S4.I1.i1.p1.2.m2.1.1.cmml"><mi id="S4.I1.i1.p1.2.m2.1.1.2" xref="S4.I1.i1.p1.2.m2.1.1.2.cmml">i</mi><mo id="S4.I1.i1.p1.2.m2.1.1.1" xref="S4.I1.i1.p1.2.m2.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S4.I1.i1.p1.2.m2.1.1.3" xref="S4.I1.i1.p1.2.m2.1.1.3.cmml">𝒮</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.2.m2.1b"><apply id="S4.I1.i1.p1.2.m2.1.1.cmml" xref="S4.I1.i1.p1.2.m2.1.1"><in id="S4.I1.i1.p1.2.m2.1.1.1.cmml" xref="S4.I1.i1.p1.2.m2.1.1.1"></in><ci id="S4.I1.i1.p1.2.m2.1.1.2.cmml" xref="S4.I1.i1.p1.2.m2.1.1.2">𝑖</ci><ci id="S4.I1.i1.p1.2.m2.1.1.3.cmml" xref="S4.I1.i1.p1.2.m2.1.1.3">𝒮</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.2.m2.1c">i\in{\cal S}</annotation></semantics></math>. In the global round <math id="S4.I1.i1.p1.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.I1.i1.p1.3.m3.1a"><mi id="S4.I1.i1.p1.3.m3.1.1" xref="S4.I1.i1.p1.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.3.m3.1b"><ci id="S4.I1.i1.p1.3.m3.1.1.cmml" xref="S4.I1.i1.p1.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.3.m3.1c">t</annotation></semantics></math>, the parameter server transmits the bypass weights <math id="S4.I1.i1.p1.4.m4.1" class="ltx_Math" alttext="\bm{v}_{t}" display="inline"><semantics id="S4.I1.i1.p1.4.m4.1a"><msub id="S4.I1.i1.p1.4.m4.1.1" xref="S4.I1.i1.p1.4.m4.1.1.cmml"><mi id="S4.I1.i1.p1.4.m4.1.1.2" xref="S4.I1.i1.p1.4.m4.1.1.2.cmml">𝒗</mi><mi id="S4.I1.i1.p1.4.m4.1.1.3" xref="S4.I1.i1.p1.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.4.m4.1b"><apply id="S4.I1.i1.p1.4.m4.1.1.cmml" xref="S4.I1.i1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.I1.i1.p1.4.m4.1.1.1.cmml" xref="S4.I1.i1.p1.4.m4.1.1">subscript</csymbol><ci id="S4.I1.i1.p1.4.m4.1.1.2.cmml" xref="S4.I1.i1.p1.4.m4.1.1.2">𝒗</ci><ci id="S4.I1.i1.p1.4.m4.1.1.3.cmml" xref="S4.I1.i1.p1.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.4.m4.1c">\bm{v}_{t}</annotation></semantics></math> to client <math id="S4.I1.i1.p1.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.I1.i1.p1.5.m5.1a"><mi id="S4.I1.i1.p1.5.m5.1.1" xref="S4.I1.i1.p1.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.5.m5.1b"><ci id="S4.I1.i1.p1.5.m5.1.1.cmml" xref="S4.I1.i1.p1.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.5.m5.1c">i</annotation></semantics></math>.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.4" class="ltx_p">Step 2: The selected client <math id="S4.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.I1.i2.p1.1.m1.1a"><mi id="S4.I1.i2.p1.1.m1.1.1" xref="S4.I1.i2.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.1.m1.1b"><ci id="S4.I1.i2.p1.1.m1.1.1.cmml" xref="S4.I1.i2.p1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.1.m1.1c">i</annotation></semantics></math> performs a local model training task with local training samples and produces the new weights for the bypass model <math id="S4.I1.i2.p1.2.m2.2" class="ltx_Math" alttext="\bm{v}_{t,i}" display="inline"><semantics id="S4.I1.i2.p1.2.m2.2a"><msub id="S4.I1.i2.p1.2.m2.2.3" xref="S4.I1.i2.p1.2.m2.2.3.cmml"><mi id="S4.I1.i2.p1.2.m2.2.3.2" xref="S4.I1.i2.p1.2.m2.2.3.2.cmml">𝒗</mi><mrow id="S4.I1.i2.p1.2.m2.2.2.2.4" xref="S4.I1.i2.p1.2.m2.2.2.2.3.cmml"><mi id="S4.I1.i2.p1.2.m2.1.1.1.1" xref="S4.I1.i2.p1.2.m2.1.1.1.1.cmml">t</mi><mo id="S4.I1.i2.p1.2.m2.2.2.2.4.1" xref="S4.I1.i2.p1.2.m2.2.2.2.3.cmml">,</mo><mi id="S4.I1.i2.p1.2.m2.2.2.2.2" xref="S4.I1.i2.p1.2.m2.2.2.2.2.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.2.m2.2b"><apply id="S4.I1.i2.p1.2.m2.2.3.cmml" xref="S4.I1.i2.p1.2.m2.2.3"><csymbol cd="ambiguous" id="S4.I1.i2.p1.2.m2.2.3.1.cmml" xref="S4.I1.i2.p1.2.m2.2.3">subscript</csymbol><ci id="S4.I1.i2.p1.2.m2.2.3.2.cmml" xref="S4.I1.i2.p1.2.m2.2.3.2">𝒗</ci><list id="S4.I1.i2.p1.2.m2.2.2.2.3.cmml" xref="S4.I1.i2.p1.2.m2.2.2.2.4"><ci id="S4.I1.i2.p1.2.m2.1.1.1.1.cmml" xref="S4.I1.i2.p1.2.m2.1.1.1.1">𝑡</ci><ci id="S4.I1.i2.p1.2.m2.2.2.2.2.cmml" xref="S4.I1.i2.p1.2.m2.2.2.2.2">𝑖</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.2.m2.2c">\bm{v}_{t,i}</annotation></semantics></math>. After that, client <math id="S4.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.I1.i2.p1.3.m3.1a"><mi id="S4.I1.i2.p1.3.m3.1.1" xref="S4.I1.i2.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.3.m3.1b"><ci id="S4.I1.i2.p1.3.m3.1.1.cmml" xref="S4.I1.i2.p1.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.3.m3.1c">i</annotation></semantics></math> is split from the index set, <math id="S4.I1.i2.p1.4.m4.1" class="ltx_Math" alttext="{\mathcal{S}}={\mathcal{S}}-{i}" display="inline"><semantics id="S4.I1.i2.p1.4.m4.1a"><mrow id="S4.I1.i2.p1.4.m4.1.1" xref="S4.I1.i2.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.I1.i2.p1.4.m4.1.1.2" xref="S4.I1.i2.p1.4.m4.1.1.2.cmml">𝒮</mi><mo id="S4.I1.i2.p1.4.m4.1.1.1" xref="S4.I1.i2.p1.4.m4.1.1.1.cmml">=</mo><mrow id="S4.I1.i2.p1.4.m4.1.1.3" xref="S4.I1.i2.p1.4.m4.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.I1.i2.p1.4.m4.1.1.3.2" xref="S4.I1.i2.p1.4.m4.1.1.3.2.cmml">𝒮</mi><mo id="S4.I1.i2.p1.4.m4.1.1.3.1" xref="S4.I1.i2.p1.4.m4.1.1.3.1.cmml">−</mo><mi id="S4.I1.i2.p1.4.m4.1.1.3.3" xref="S4.I1.i2.p1.4.m4.1.1.3.3.cmml">i</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.4.m4.1b"><apply id="S4.I1.i2.p1.4.m4.1.1.cmml" xref="S4.I1.i2.p1.4.m4.1.1"><eq id="S4.I1.i2.p1.4.m4.1.1.1.cmml" xref="S4.I1.i2.p1.4.m4.1.1.1"></eq><ci id="S4.I1.i2.p1.4.m4.1.1.2.cmml" xref="S4.I1.i2.p1.4.m4.1.1.2">𝒮</ci><apply id="S4.I1.i2.p1.4.m4.1.1.3.cmml" xref="S4.I1.i2.p1.4.m4.1.1.3"><minus id="S4.I1.i2.p1.4.m4.1.1.3.1.cmml" xref="S4.I1.i2.p1.4.m4.1.1.3.1"></minus><ci id="S4.I1.i2.p1.4.m4.1.1.3.2.cmml" xref="S4.I1.i2.p1.4.m4.1.1.3.2">𝒮</ci><ci id="S4.I1.i2.p1.4.m4.1.1.3.3.cmml" xref="S4.I1.i2.p1.4.m4.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.4.m4.1c">{\mathcal{S}}={\mathcal{S}}-{i}</annotation></semantics></math>.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.8" class="ltx_p">Step 3: The current global round terminates if <math id="S4.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="{\cal S}=\varnothing" display="inline"><semantics id="S4.I1.i3.p1.1.m1.1a"><mrow id="S4.I1.i3.p1.1.m1.1.1" xref="S4.I1.i3.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.I1.i3.p1.1.m1.1.1.2" xref="S4.I1.i3.p1.1.m1.1.1.2.cmml">𝒮</mi><mo id="S4.I1.i3.p1.1.m1.1.1.1" xref="S4.I1.i3.p1.1.m1.1.1.1.cmml">=</mo><mi mathvariant="normal" id="S4.I1.i3.p1.1.m1.1.1.3" xref="S4.I1.i3.p1.1.m1.1.1.3.cmml">∅</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.1.m1.1b"><apply id="S4.I1.i3.p1.1.m1.1.1.cmml" xref="S4.I1.i3.p1.1.m1.1.1"><eq id="S4.I1.i3.p1.1.m1.1.1.1.cmml" xref="S4.I1.i3.p1.1.m1.1.1.1"></eq><ci id="S4.I1.i3.p1.1.m1.1.1.2.cmml" xref="S4.I1.i3.p1.1.m1.1.1.2">𝒮</ci><emptyset id="S4.I1.i3.p1.1.m1.1.1.3.cmml" xref="S4.I1.i3.p1.1.m1.1.1.3"></emptyset></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.1.m1.1c">{\cal S}=\varnothing</annotation></semantics></math> and go to the next global round <math id="S4.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="t=t+1" display="inline"><semantics id="S4.I1.i3.p1.2.m2.1a"><mrow id="S4.I1.i3.p1.2.m2.1.1" xref="S4.I1.i3.p1.2.m2.1.1.cmml"><mi id="S4.I1.i3.p1.2.m2.1.1.2" xref="S4.I1.i3.p1.2.m2.1.1.2.cmml">t</mi><mo id="S4.I1.i3.p1.2.m2.1.1.1" xref="S4.I1.i3.p1.2.m2.1.1.1.cmml">=</mo><mrow id="S4.I1.i3.p1.2.m2.1.1.3" xref="S4.I1.i3.p1.2.m2.1.1.3.cmml"><mi id="S4.I1.i3.p1.2.m2.1.1.3.2" xref="S4.I1.i3.p1.2.m2.1.1.3.2.cmml">t</mi><mo id="S4.I1.i3.p1.2.m2.1.1.3.1" xref="S4.I1.i3.p1.2.m2.1.1.3.1.cmml">+</mo><mn id="S4.I1.i3.p1.2.m2.1.1.3.3" xref="S4.I1.i3.p1.2.m2.1.1.3.3.cmml">1</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.2.m2.1b"><apply id="S4.I1.i3.p1.2.m2.1.1.cmml" xref="S4.I1.i3.p1.2.m2.1.1"><eq id="S4.I1.i3.p1.2.m2.1.1.1.cmml" xref="S4.I1.i3.p1.2.m2.1.1.1"></eq><ci id="S4.I1.i3.p1.2.m2.1.1.2.cmml" xref="S4.I1.i3.p1.2.m2.1.1.2">𝑡</ci><apply id="S4.I1.i3.p1.2.m2.1.1.3.cmml" xref="S4.I1.i3.p1.2.m2.1.1.3"><plus id="S4.I1.i3.p1.2.m2.1.1.3.1.cmml" xref="S4.I1.i3.p1.2.m2.1.1.3.1"></plus><ci id="S4.I1.i3.p1.2.m2.1.1.3.2.cmml" xref="S4.I1.i3.p1.2.m2.1.1.3.2">𝑡</ci><cn type="integer" id="S4.I1.i3.p1.2.m2.1.1.3.3.cmml" xref="S4.I1.i3.p1.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.2.m2.1c">t=t+1</annotation></semantics></math>. Otherwise, client <math id="S4.I1.i3.p1.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.I1.i3.p1.3.m3.1a"><mi id="S4.I1.i3.p1.3.m3.1.1" xref="S4.I1.i3.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.3.m3.1b"><ci id="S4.I1.i3.p1.3.m3.1.1.cmml" xref="S4.I1.i3.p1.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.3.m3.1c">i</annotation></semantics></math> selects a new client <math id="S4.I1.i3.p1.4.m4.1" class="ltx_Math" alttext="i^{\prime}\in{\cal S}" display="inline"><semantics id="S4.I1.i3.p1.4.m4.1a"><mrow id="S4.I1.i3.p1.4.m4.1.1" xref="S4.I1.i3.p1.4.m4.1.1.cmml"><msup id="S4.I1.i3.p1.4.m4.1.1.2" xref="S4.I1.i3.p1.4.m4.1.1.2.cmml"><mi id="S4.I1.i3.p1.4.m4.1.1.2.2" xref="S4.I1.i3.p1.4.m4.1.1.2.2.cmml">i</mi><mo id="S4.I1.i3.p1.4.m4.1.1.2.3" xref="S4.I1.i3.p1.4.m4.1.1.2.3.cmml">′</mo></msup><mo id="S4.I1.i3.p1.4.m4.1.1.1" xref="S4.I1.i3.p1.4.m4.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S4.I1.i3.p1.4.m4.1.1.3" xref="S4.I1.i3.p1.4.m4.1.1.3.cmml">𝒮</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.4.m4.1b"><apply id="S4.I1.i3.p1.4.m4.1.1.cmml" xref="S4.I1.i3.p1.4.m4.1.1"><in id="S4.I1.i3.p1.4.m4.1.1.1.cmml" xref="S4.I1.i3.p1.4.m4.1.1.1"></in><apply id="S4.I1.i3.p1.4.m4.1.1.2.cmml" xref="S4.I1.i3.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S4.I1.i3.p1.4.m4.1.1.2.1.cmml" xref="S4.I1.i3.p1.4.m4.1.1.2">superscript</csymbol><ci id="S4.I1.i3.p1.4.m4.1.1.2.2.cmml" xref="S4.I1.i3.p1.4.m4.1.1.2.2">𝑖</ci><ci id="S4.I1.i3.p1.4.m4.1.1.2.3.cmml" xref="S4.I1.i3.p1.4.m4.1.1.2.3">′</ci></apply><ci id="S4.I1.i3.p1.4.m4.1.1.3.cmml" xref="S4.I1.i3.p1.4.m4.1.1.3">𝒮</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.4.m4.1c">i^{\prime}\in{\cal S}</annotation></semantics></math> as the next relay training node. When the local weights <math id="S4.I1.i3.p1.5.m5.2" class="ltx_Math" alttext="\bm{v}_{t,i}" display="inline"><semantics id="S4.I1.i3.p1.5.m5.2a"><msub id="S4.I1.i3.p1.5.m5.2.3" xref="S4.I1.i3.p1.5.m5.2.3.cmml"><mi id="S4.I1.i3.p1.5.m5.2.3.2" xref="S4.I1.i3.p1.5.m5.2.3.2.cmml">𝒗</mi><mrow id="S4.I1.i3.p1.5.m5.2.2.2.4" xref="S4.I1.i3.p1.5.m5.2.2.2.3.cmml"><mi id="S4.I1.i3.p1.5.m5.1.1.1.1" xref="S4.I1.i3.p1.5.m5.1.1.1.1.cmml">t</mi><mo id="S4.I1.i3.p1.5.m5.2.2.2.4.1" xref="S4.I1.i3.p1.5.m5.2.2.2.3.cmml">,</mo><mi id="S4.I1.i3.p1.5.m5.2.2.2.2" xref="S4.I1.i3.p1.5.m5.2.2.2.2.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.5.m5.2b"><apply id="S4.I1.i3.p1.5.m5.2.3.cmml" xref="S4.I1.i3.p1.5.m5.2.3"><csymbol cd="ambiguous" id="S4.I1.i3.p1.5.m5.2.3.1.cmml" xref="S4.I1.i3.p1.5.m5.2.3">subscript</csymbol><ci id="S4.I1.i3.p1.5.m5.2.3.2.cmml" xref="S4.I1.i3.p1.5.m5.2.3.2">𝒗</ci><list id="S4.I1.i3.p1.5.m5.2.2.2.3.cmml" xref="S4.I1.i3.p1.5.m5.2.2.2.4"><ci id="S4.I1.i3.p1.5.m5.1.1.1.1.cmml" xref="S4.I1.i3.p1.5.m5.1.1.1.1">𝑡</ci><ci id="S4.I1.i3.p1.5.m5.2.2.2.2.cmml" xref="S4.I1.i3.p1.5.m5.2.2.2.2">𝑖</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.5.m5.2c">\bm{v}_{t,i}</annotation></semantics></math> is transmitted from client <math id="S4.I1.i3.p1.6.m6.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.I1.i3.p1.6.m6.1a"><mi id="S4.I1.i3.p1.6.m6.1.1" xref="S4.I1.i3.p1.6.m6.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.6.m6.1b"><ci id="S4.I1.i3.p1.6.m6.1.1.cmml" xref="S4.I1.i3.p1.6.m6.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.6.m6.1c">i</annotation></semantics></math> to client <math id="S4.I1.i3.p1.7.m7.1" class="ltx_Math" alttext="i^{\prime}" display="inline"><semantics id="S4.I1.i3.p1.7.m7.1a"><msup id="S4.I1.i3.p1.7.m7.1.1" xref="S4.I1.i3.p1.7.m7.1.1.cmml"><mi id="S4.I1.i3.p1.7.m7.1.1.2" xref="S4.I1.i3.p1.7.m7.1.1.2.cmml">i</mi><mo id="S4.I1.i3.p1.7.m7.1.1.3" xref="S4.I1.i3.p1.7.m7.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.7.m7.1b"><apply id="S4.I1.i3.p1.7.m7.1.1.cmml" xref="S4.I1.i3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S4.I1.i3.p1.7.m7.1.1.1.cmml" xref="S4.I1.i3.p1.7.m7.1.1">superscript</csymbol><ci id="S4.I1.i3.p1.7.m7.1.1.2.cmml" xref="S4.I1.i3.p1.7.m7.1.1.2">𝑖</ci><ci id="S4.I1.i3.p1.7.m7.1.1.3.cmml" xref="S4.I1.i3.p1.7.m7.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.7.m7.1c">i^{\prime}</annotation></semantics></math>, we assign <math id="S4.I1.i3.p1.8.m8.1" class="ltx_Math" alttext="i=i^{\prime}" display="inline"><semantics id="S4.I1.i3.p1.8.m8.1a"><mrow id="S4.I1.i3.p1.8.m8.1.1" xref="S4.I1.i3.p1.8.m8.1.1.cmml"><mi id="S4.I1.i3.p1.8.m8.1.1.2" xref="S4.I1.i3.p1.8.m8.1.1.2.cmml">i</mi><mo id="S4.I1.i3.p1.8.m8.1.1.1" xref="S4.I1.i3.p1.8.m8.1.1.1.cmml">=</mo><msup id="S4.I1.i3.p1.8.m8.1.1.3" xref="S4.I1.i3.p1.8.m8.1.1.3.cmml"><mi id="S4.I1.i3.p1.8.m8.1.1.3.2" xref="S4.I1.i3.p1.8.m8.1.1.3.2.cmml">i</mi><mo id="S4.I1.i3.p1.8.m8.1.1.3.3" xref="S4.I1.i3.p1.8.m8.1.1.3.3.cmml">′</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.8.m8.1b"><apply id="S4.I1.i3.p1.8.m8.1.1.cmml" xref="S4.I1.i3.p1.8.m8.1.1"><eq id="S4.I1.i3.p1.8.m8.1.1.1.cmml" xref="S4.I1.i3.p1.8.m8.1.1.1"></eq><ci id="S4.I1.i3.p1.8.m8.1.1.2.cmml" xref="S4.I1.i3.p1.8.m8.1.1.2">𝑖</ci><apply id="S4.I1.i3.p1.8.m8.1.1.3.cmml" xref="S4.I1.i3.p1.8.m8.1.1.3"><csymbol cd="ambiguous" id="S4.I1.i3.p1.8.m8.1.1.3.1.cmml" xref="S4.I1.i3.p1.8.m8.1.1.3">superscript</csymbol><ci id="S4.I1.i3.p1.8.m8.1.1.3.2.cmml" xref="S4.I1.i3.p1.8.m8.1.1.3.2">𝑖</ci><ci id="S4.I1.i3.p1.8.m8.1.1.3.3.cmml" xref="S4.I1.i3.p1.8.m8.1.1.3.3">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.8.m8.1c">i=i^{\prime}</annotation></semantics></math> and jump to Step 2.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.7" class="ltx_p">Step 4: The above steps 1-3 are repeated <math id="S4.I1.i4.p1.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.I1.i4.p1.1.m1.1a"><mi id="S4.I1.i4.p1.1.m1.1.1" xref="S4.I1.i4.p1.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i4.p1.1.m1.1b"><ci id="S4.I1.i4.p1.1.m1.1.1.cmml" xref="S4.I1.i4.p1.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i4.p1.1.m1.1c">T</annotation></semantics></math> global rounds when <math id="S4.I1.i4.p1.2.m2.1" class="ltx_Math" alttext="t&gt;T" display="inline"><semantics id="S4.I1.i4.p1.2.m2.1a"><mrow id="S4.I1.i4.p1.2.m2.1.1" xref="S4.I1.i4.p1.2.m2.1.1.cmml"><mi id="S4.I1.i4.p1.2.m2.1.1.2" xref="S4.I1.i4.p1.2.m2.1.1.2.cmml">t</mi><mo id="S4.I1.i4.p1.2.m2.1.1.1" xref="S4.I1.i4.p1.2.m2.1.1.1.cmml">&gt;</mo><mi id="S4.I1.i4.p1.2.m2.1.1.3" xref="S4.I1.i4.p1.2.m2.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i4.p1.2.m2.1b"><apply id="S4.I1.i4.p1.2.m2.1.1.cmml" xref="S4.I1.i4.p1.2.m2.1.1"><gt id="S4.I1.i4.p1.2.m2.1.1.1.cmml" xref="S4.I1.i4.p1.2.m2.1.1.1"></gt><ci id="S4.I1.i4.p1.2.m2.1.1.2.cmml" xref="S4.I1.i4.p1.2.m2.1.1.2">𝑡</ci><ci id="S4.I1.i4.p1.2.m2.1.1.3.cmml" xref="S4.I1.i4.p1.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i4.p1.2.m2.1c">t&gt;T</annotation></semantics></math>. Then the weights of the base model <math id="S4.I1.i4.p1.3.m3.1" class="ltx_Math" alttext="\bm{w}" display="inline"><semantics id="S4.I1.i4.p1.3.m3.1a"><mi id="S4.I1.i4.p1.3.m3.1.1" xref="S4.I1.i4.p1.3.m3.1.1.cmml">𝒘</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i4.p1.3.m3.1b"><ci id="S4.I1.i4.p1.3.m3.1.1.cmml" xref="S4.I1.i4.p1.3.m3.1.1">𝒘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i4.p1.3.m3.1c">\bm{w}</annotation></semantics></math> and additional bypass <math id="S4.I1.i4.p1.4.m4.1" class="ltx_Math" alttext="\bm{v}_{T}" display="inline"><semantics id="S4.I1.i4.p1.4.m4.1a"><msub id="S4.I1.i4.p1.4.m4.1.1" xref="S4.I1.i4.p1.4.m4.1.1.cmml"><mi id="S4.I1.i4.p1.4.m4.1.1.2" xref="S4.I1.i4.p1.4.m4.1.1.2.cmml">𝒗</mi><mi id="S4.I1.i4.p1.4.m4.1.1.3" xref="S4.I1.i4.p1.4.m4.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S4.I1.i4.p1.4.m4.1b"><apply id="S4.I1.i4.p1.4.m4.1.1.cmml" xref="S4.I1.i4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.I1.i4.p1.4.m4.1.1.1.cmml" xref="S4.I1.i4.p1.4.m4.1.1">subscript</csymbol><ci id="S4.I1.i4.p1.4.m4.1.1.2.cmml" xref="S4.I1.i4.p1.4.m4.1.1.2">𝒗</ci><ci id="S4.I1.i4.p1.4.m4.1.1.3.cmml" xref="S4.I1.i4.p1.4.m4.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i4.p1.4.m4.1c">\bm{v}_{T}</annotation></semantics></math> are merged together to form the weights of the fine-tuned personalized model <math id="S4.I1.i4.p1.5.m5.1" class="ltx_Math" alttext="\bm{w}^{\prime}" display="inline"><semantics id="S4.I1.i4.p1.5.m5.1a"><msup id="S4.I1.i4.p1.5.m5.1.1" xref="S4.I1.i4.p1.5.m5.1.1.cmml"><mi id="S4.I1.i4.p1.5.m5.1.1.2" xref="S4.I1.i4.p1.5.m5.1.1.2.cmml">𝒘</mi><mo id="S4.I1.i4.p1.5.m5.1.1.3" xref="S4.I1.i4.p1.5.m5.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.I1.i4.p1.5.m5.1b"><apply id="S4.I1.i4.p1.5.m5.1.1.cmml" xref="S4.I1.i4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.I1.i4.p1.5.m5.1.1.1.cmml" xref="S4.I1.i4.p1.5.m5.1.1">superscript</csymbol><ci id="S4.I1.i4.p1.5.m5.1.1.2.cmml" xref="S4.I1.i4.p1.5.m5.1.1.2">𝒘</ci><ci id="S4.I1.i4.p1.5.m5.1.1.3.cmml" xref="S4.I1.i4.p1.5.m5.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i4.p1.5.m5.1c">\bm{w}^{\prime}</annotation></semantics></math>, which is utilized to generate the customized content. Note that the inference cost of <math id="S4.I1.i4.p1.6.m6.1" class="ltx_Math" alttext="\bm{w}^{\prime}" display="inline"><semantics id="S4.I1.i4.p1.6.m6.1a"><msup id="S4.I1.i4.p1.6.m6.1.1" xref="S4.I1.i4.p1.6.m6.1.1.cmml"><mi id="S4.I1.i4.p1.6.m6.1.1.2" xref="S4.I1.i4.p1.6.m6.1.1.2.cmml">𝒘</mi><mo id="S4.I1.i4.p1.6.m6.1.1.3" xref="S4.I1.i4.p1.6.m6.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.I1.i4.p1.6.m6.1b"><apply id="S4.I1.i4.p1.6.m6.1.1.cmml" xref="S4.I1.i4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.I1.i4.p1.6.m6.1.1.1.cmml" xref="S4.I1.i4.p1.6.m6.1.1">superscript</csymbol><ci id="S4.I1.i4.p1.6.m6.1.1.2.cmml" xref="S4.I1.i4.p1.6.m6.1.1.2">𝒘</ci><ci id="S4.I1.i4.p1.6.m6.1.1.3.cmml" xref="S4.I1.i4.p1.6.m6.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i4.p1.6.m6.1c">\bm{w}^{\prime}</annotation></semantics></math> is identical to that of <math id="S4.I1.i4.p1.7.m7.1" class="ltx_Math" alttext="\bm{w}" display="inline"><semantics id="S4.I1.i4.p1.7.m7.1a"><mi id="S4.I1.i4.p1.7.m7.1.1" xref="S4.I1.i4.p1.7.m7.1.1.cmml">𝒘</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i4.p1.7.m7.1b"><ci id="S4.I1.i4.p1.7.m7.1.1.cmml" xref="S4.I1.i4.p1.7.m7.1.1">𝒘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i4.p1.7.m7.1c">\bm{w}</annotation></semantics></math>.</p>
</div>
</li>
</ul>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.2" class="ltx_p">To evaluate the performance of our proposed federated fine-tuning scheme, we collected 20 pieces of ancient Chinese ink paintings by different masters and cropped them to a resolution of <math id="S4.p4.1.m1.1" class="ltx_Math" alttext="512\times 512" display="inline"><semantics id="S4.p4.1.m1.1a"><mrow id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml"><mn id="S4.p4.1.m1.1.1.2" xref="S4.p4.1.m1.1.1.2.cmml">512</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p4.1.m1.1.1.1" xref="S4.p4.1.m1.1.1.1.cmml">×</mo><mn id="S4.p4.1.m1.1.1.3" xref="S4.p4.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><apply id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1"><times id="S4.p4.1.m1.1.1.1.cmml" xref="S4.p4.1.m1.1.1.1"></times><cn type="integer" id="S4.p4.1.m1.1.1.2.cmml" xref="S4.p4.1.m1.1.1.2">512</cn><cn type="integer" id="S4.p4.1.m1.1.1.3.cmml" xref="S4.p4.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">512\times 512</annotation></semantics></math> pixels. For simplicity, the total number of clients is 5 and each client has 4 training samples. For the hyper-parameter setting, we use a number of low-rank dimensions of 8 and let the total number of global rounds <math id="S4.p4.2.m2.1" class="ltx_Math" alttext="T=100" display="inline"><semantics id="S4.p4.2.m2.1a"><mrow id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml"><mi id="S4.p4.2.m2.1.1.2" xref="S4.p4.2.m2.1.1.2.cmml">T</mi><mo id="S4.p4.2.m2.1.1.1" xref="S4.p4.2.m2.1.1.1.cmml">=</mo><mn id="S4.p4.2.m2.1.1.3" xref="S4.p4.2.m2.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><apply id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1"><eq id="S4.p4.2.m2.1.1.1.cmml" xref="S4.p4.2.m2.1.1.1"></eq><ci id="S4.p4.2.m2.1.1.2.cmml" xref="S4.p4.2.m2.1.1.2">𝑇</ci><cn type="integer" id="S4.p4.2.m2.1.1.3.cmml" xref="S4.p4.2.m2.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">T=100</annotation></semantics></math>. We compare the performance between our scheme and a traditional fine-tuning scheme, which trains all parameters of the U-Net in the stable diffusion model. To reduce the training cost, we apply the half-precision floating-point for the parameter representation. Specifically, the required data sizes for D2D parameter transmission of the proposed and traditional schemes are 9.1 and 1,720 megabytes, respectively.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">Figure <a href="#S4.F3" title="Figure 3 ‣ IV Case Study: Federated Fine-tuning for a Stable Diffusion Model ‣ Federated Learning-Empowered AI-Generated Content in Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(a) shows the training loss over the global round between these two schemes. We observe that the traditional scheme converges faster than the our scheme in the first 60 rounds and our scheme achieves comparable training loss to the traditional scheme after 100 global rounds. During the training, we periodically sample and record the output image of the generator every 20 rounds with the same prompt. The traditional scheme acquires the ability to generate Chinese ink-style images after approximately 20 global rounds while our scheme requires about 40 rounds to attain the same proficiency.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">However, our scheme achieves a remarkable advantage in reducing the system cost in terms of the communication cost and time consumption for the model convergence, as shown in Fig. <a href="#S4.F3" title="Figure 3 ‣ IV Case Study: Federated Fine-tuning for a Stable Diffusion Model ‣ Federated Learning-Empowered AI-Generated Content in Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(b).
To achieve an identical training loss of 0.02, the traditional and our schemes consume about 189 gigabytes and 1.3 gigabytes of data traffic, respectively. To achieve a training loss of 0.08, the traditional and our schemes take about 131 minutes and 2.9 minutes, respectively. Compared with the traditional scheme, our scheme can reduce both the communication costs and training latency by up to two orders of magnitude. Fig. <a href="#S4.F3" title="Figure 3 ‣ IV Case Study: Federated Fine-tuning for a Stable Diffusion Model ‣ Federated Learning-Empowered AI-Generated Content in Wireless Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(c) shows the image samples generated by different fine-tuned models on the client side. The results show that our scheme successfully enables a client to obtain the ability to generate Chinese ink-style paintings.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Future Research Directions</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">To achieve the benefits of FL-empowered AIGC, there still exist several open and challenging issues.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Incentive Mechanism Design for Federated Learning</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Incentive mechanisms are necessitated to provide effective incentives for clients, which are willing to join the federated-learning-empowered AIGC. Mathematical tools of economic theory such as game theory, auction theory, and contract theory can be adopted. Furthermore, the incentive mechanism design should coordinate with other FL designs to achieve a centralized objective. For example, the incentive mechanism design for the clients and split point selection of an AIGC model are jointly optimized to enhance the FL in a split approach. Facing the FL in a sequential approach, the decision-maker considers a joint optimization scheme to design proper incentives to the clients while assigning their processing orders.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Blockchain Assisted Decentralized AIGC Services</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">As a decentralized ledger, blockchain has been envisioned as the underlying technology to provide data security and transparent management for FL-aided AIGC.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">On one hand, blockchain supports decentralized and secure model storage for reliable FL by maintaining immutable records of all submitted local model updates and making them tamper-proof. Blockchain-based incentive mechanisms have been designed to stimulate honest clients to contribute data and join the FL procedure. On the other hand, blockchain provides a valuable solution to protection of sensitive data of users and effective management of AIGC products. Blockchain can prevent the leakage of the information such as chat records of ChatGPT. Blockchain also establishes a decentralized platform for the users to freely distribute, authentic the ownership, and trade their content in the totally trustless environment.
</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Green AIGC Enabled By Semantic Communication</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">AIGC has been applied to generate digital content to render graphics for Metaverse. A Metaverse service provider collects sensing data of a user and performs an AIGC inference based rendering task for the user. Semantic communication is further exploited to extract available semantic information from the raw data and only transmit the semantic information with a small data size, thereby saving the data transmission time and communication overheads in the system.
In particular, FL can be leveraged to decentralize the semantic encoder/decoder model training while improving the accuracy of the semantic information extraction by using more training data.
</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS4.4.1.1" class="ltx_text">V-D</span> </span><span id="S5.SS4.5.2" class="ltx_text ltx_font_italic">Personalized AIGC Based on Edge Intelligence</span>
</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Edge intelligence relies on exploiting the AI based decision-making capability for promoting the convergence of caching, computing and communication at the network edge and serves different users in the AIGC inference services.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">Many research efforts have been conducted to apply the edge intelligence to tackle the tradeoff between the model performance and resource consumption when multiple clients join the FL procedure. When a part of AIGC models are cached, the multi-user inference service delivery aims to improve the overall user satisfaction while reducing the inference latency and resource consumption. The existing optimization schemes for FL can be modified to take into account the personalized requirements for the quality of the AIGC inference services. However, it is rather difficult to quantitatively evaluate the impacts of the above optimization decisions on the performance metrics of the AIGC inference services, e.g., quality of the generated content, latency and reliability of content delivery and model hit ratio.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The centralized training in the AIGC processes raises a challenging issue for deploying AIGC services in wireless networks. To address this issue, we proposed FL to decentralize the model training procedures regarding AIGC, which can improve learning efficiency and achieve privacy protection for AIGC. We analyzed the issues remained on the integration of FL and AIGC, and then proposed instrumental FL-based techniques to implement the FL procedure including the parallel, split and sequential approaches. As a case study, we adopted FL in a sequential approach to perform the federated fine-tuning for a stable diffusion model. Numerical results verified the superiority of the proposed scheme. Finally, we outlined the open research issues of FL-empowered AIGC.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
H. Du, Z. Li, D. Niyato, J. Kang, Z. Xiong, D. I. Kim, <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Enabling
ai-generated content (aigc) services in wireless edge networks,” <span id="bib.bib1.2.2" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:2301.03220</span>, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, and Y. Bengio, “Generative adversarial networks,” <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Communications of the ACM</span>, vol. 63, no. 11, pp. 139–144, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
D. P. Kingma, M. Welling, <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">et al.</span>, “An introduction to variational
autoencoders,” <span id="bib.bib3.2.2" class="ltx_text ltx_font_italic">Foundations and Trends® in Machine
Learning</span>, vol. 12, no. 4, pp. 307–392, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, “Deep
unsupervised learning using nonequilibrium thermodynamics,” in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pp. 2256–2265, PMLR, 2015.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Advances
in neural information processing systems</span>, vol. 30, 2017.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
M. Zhang and Y. He, “Accelerating training of transformer-based language
models with progressive layer dropping,” <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information
Processing Systems</span>, vol. 33, pp. 14011–14023, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y. Tian, Y. Wan, L. Lyu, D. Yao, H. Jin, and L. Sun, “Fedbert: when federated
learning meets pre-training,” <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Intelligent Systems
and Technology (TIST)</span>, vol. 13, no. 4, pp. 1–26, 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
R. Yan, L. Qu, Q. Wei, S.-C. Huang, L. Shen, D. Rubin, L. Xing, and Y. Zhou,
“Label-efficient self-supervised federated learning for tackling data
heterogeneity in medical imaging,” <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Medical
Imaging</span>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
W. Chen, “Lora: Low-rank adaptation of large language models,” <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:2106.09685</span>, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman,
“Dreambooth: Fine tuning text-to-image diffusion models for subject-driven
generation,” <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2208.12242</span>, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
R. Gal, Y. Alaluf, Y. Atzmon, O. Patashnik, A. H. Bermano, G. Chechik, and
D. Cohen-Or, “An image is worth one word: Personalizing text-to-image
generation using textual inversion,” <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2208.01618</span>,
2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
L. Zhang and M. Agrawala, “Adding conditional control to text-to-image
diffusion models,” <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.05543</span>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Y. Liu, H. Du, D. Niyato, J. Kang, Z. Xiong, D. I. Kim, and A. Jamalipour,
“Deep generative model and its applications in efficient wireless network
management: A tutorial and case study,” <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2303.17114</span>, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
S. Hosseinalipour, S. S. Azam, C. G. Brinton, N. Michelusi, V. Aggarwal, D. J.
Love, and H. Dai, “Multi-stage hybrid federated learning over large-scale
d2d-enabled fog networks,” <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">IEEE/ACM Transactions on Networking</span>,
vol. 30, no. 4, pp. 1569–1584, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution
image synthesis with latent diffusion models,” in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>,
pp. 10684–10695, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2307.07145" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2307.07146" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2307.07146">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2307.07146" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2307.07147" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 18:06:13 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
