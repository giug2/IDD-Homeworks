<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2204.04950] Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data</title><meta property="og:description" content="Transfer learning for GANs successfully improves generation performance under low-shot regimes. However, existing studies show that the pretrained model using a single benchmark dataset is not generalized to various taâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2204.04950">

<!--Generated on Mon Mar 11 12:06:15 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Commonality in Natural Images Rescues GANs:
<br class="ltx_break">Pretraining GANs with Generic and Privacy-free Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kyungjune Baek, Â Hyunjung Shim
<br class="ltx_break">Yonsei University
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{bkjbkj12, kateshim}@yonsei.ac.kr</span>
</span><span class="ltx_author_notes">Hyunjung Shim is a corresponding author.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Transfer learning for GANs successfully improves generation performance under low-shot regimes. However, existing studies show that the pretrained model using a single benchmark dataset is not generalized to various target datasets. More importantly, the pretrained model can be vulnerable to copyright or privacy risks as membership inference attack advances. To resolve both issues, we propose an effective and unbiased data synthesizer, namely <span id="id2.id1.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span>, inspired by the generic characteristics of natural images. Specifically, we utilize 1) the generic statistics on the frequency magnitude spectrum, 2) the elementary shape (i.e., image composition via elementary shapes) for representing the structure information, and 3) the existence of saliency as prior. Since our synthesizer only considers the generic properties of natural images, the single model pretrained on our dataset can be consistently transferred to various target datasets, and even outperforms the previous methods pretrained with the natural images in terms of FrÃ©chet inception distance. Extensive analysis, ablation study, and evaluations demonstrate that each component of our data synthesizer is effective, and provide insights on the desirable nature of the pretrained model for the transferability of GANs.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Generative adversarial networks (GANs)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> are a powerful generative model that can synthesize complex data by learning the implicit density distribution with adversarial training. Thanks to the impressive generation quality, particularly in image generation tasksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, GANs have been widely used in various downstream tasks in computer vision, such as data augmentationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, super-resolutionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, image translationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, and image synthesis with primitive representationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. Despite the remarkable quality, GANs require at least several thousand, mostly several hundred thousand images for training. This requirement for data collection is often infeasible in practical applications (e.g., many pictures of a treasure, endangered species, or the medical images of rare disease).</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The idea of transfer learning has been recently introduced to GAN trainingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> for resolving the real-world generation problem. Following the common practice, the general framework of GAN transfer learning 1) pretrains GANs on a publicly available large-scale source dataset (e.g., FFHQ and ImageNet) and then 2) finetunes GANs with a relatively small target dataset. As a result, developing GANs with transfer learning clearly improves the generation quality and diversity over the models trained from scratch only with the target dataset.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2204.04950/assets/images/synthetic_a.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="147" height="49" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2204.04950/assets/images/synthetic_b.png" id="S1.F1.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="147" height="49" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S1.F1.2" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S1.F1.2.1" class="ltx_text ltx_inline-block" style="width:106.2pt;">(a) <span id="S1.F1.2.1.1" class="ltx_text ltx_font_typewriter">PinkNoise</span></span>
<span id="S1.F1.2.2" class="ltx_text ltx_inline-block" style="width:106.2pt;">(b) <span id="S1.F1.2.2.1" class="ltx_text ltx_font_typewriter">Primitives</span></span> 
<br class="ltx_break">
<img src="/html/2204.04950/assets/images/synthetic_c.png" id="S1.F1.1.g1" class="ltx_graphics ltx_img_landscape" width="147" height="49" alt="Refer to caption">
<img src="/html/2204.04950/assets/images/synthetic_d.png" id="S1.F1.2.g2" class="ltx_graphics ltx_img_landscape" width="147" height="49" alt="Refer to caption"> 
<br class="ltx_break"><span id="S1.F1.2.3" class="ltx_text ltx_inline-block" style="width:106.2pt;">(c) <span id="S1.F1.2.3.1" class="ltx_text ltx_font_typewriter">Primitives-S</span></span>
<span id="S1.F1.2.4" class="ltx_text ltx_inline-block" style="width:106.2pt;">(d) <span id="S1.F1.2.4.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span></span>
<br class="ltx_break"></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.5.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.6.2" class="ltx_text" style="font-size:90%;">Visualization of our synthetic datasets. We visualize four variants of our synthetic datasets and <span id="S1.F1.6.2.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span> is finally chosen for the best performance. Example images are resized in three different scales.</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Unfortunately, the effectiveness of transfer learning for GANs highly depends on how similar the source dataset is to the target dataset. According to TransferGANÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, transfer learning can achieve the best performance when the source shares common characteristics with the target. For example, when LFWÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> is the target dataset, the best performance is achieved with the source dataset of CelebAÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> as both are face datasets. For FlowerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> or KitchensÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, utilizing CelebA as the source dataset does not significantly improve the performance. Thus, it is required to search the best source dataset for a given target dataset by measuring the similarity between two datasets (e.g., FID score). Because exploring the best source dataset and then acquiring its pretrained model is ad-hoc, the search result does not guarantee the best pair for transfer learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. Moreover, none of the existing source datasets can sufficiently fit the target dataset in real-world applications.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Other than the performance issue, we argue that the current pretrained models can be vulnerable to copyright (see the supplementary 7 for potential copyright issues of large-scale datasets) and privacy issuesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>.
Even for public benchmark datasets, employing them for commercial purposes is not always permitted.
For examples, ImageNet-1K having 1M images, the copyright issue might not be feasible to handle. When targeting the commercial use of a dataset, the developer should negotiate with the author of each sample. For this reason, one might compose her own dataset via web crawling, but filtering out the copyrighted samples is practically difficult. Besides, unresolved copyright and privacy issues might cause legal issuesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Recent studiesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> also show that the deep generative models are vulnerable to membership inference attacks, implying that privacy issues still remains beyond the copyright issues. An adversary can reconstruct a face even without additional prior informationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. That is, we can reveal individual training samples by attacking the trained model. As the network capacity of GANs increases rapidly to improve performance, the risk of memorization also grows quickly. Memorization effects make GANs more vulnerable to membership inference attacksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Since we consider transfer learning, someone might argue that the membership inference on the source (e.g., pretraining) dataset should not be a critical issue. However, Zou et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> reported that the membership inference of the source dataset could be conducted even after the transfer learning (see the supplementary 7 for empirical evidence).</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In this work, we dive into tackling the two undiscovered but critical issues of transfer learning for GANs: 1) the lack of generalization for the pretrained model and 2) the copyright or privacy issue of the pretraining dataset. To this end, we devise a synthetic data generation strategy for acquiring pretrained GANs. Since our pretrained model is newly computed with a synthetic dataset, it is inherently free from copyright and privacy issues. Besides, the learned features of existing pretrained models encode the inductive bias of a training dataset, exhibiting lower transferabilityÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. Learned from this lesson, we ensure that our synthetic data should be unbiased to any datasets and free from expert knowledge or specific domain prior.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Towards this goal, we adopt the generic property of the natural images in the frequency spectrum and structure. We develop our data generation strategy, namely <span id="S1.p7.1.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span>, inspired by the analysis and observations on natural images from previous studiesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Our design philosophy is built upon three aspects: 1) considering the power spectrum distribution of the natural images as in <a href="#S1.F1" title="In 1 Introduction â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>(a), 2) reflecting the structural property of the natural images as illustrated in <a href="#S1.F1" title="In 1 Introduction â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>(b), and 3) utilizing the existence of saliency in images (<a href="#S1.F1" title="In 1 Introduction â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>(c) shows the synthetic data generated by applying both 2) and 3).) Finally, we combine all three aspects and develop our final data synthesizer <span id="S1.p7.1.2" class="ltx_text ltx_font_typewriter">Primitives-PS</span>, as visualized in <a href="#S1.F1" title="In 1 Introduction â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>(d). We pretrain GANs using the synthetic dataset generated by our data synthesizer. Then, the effectiveness of the proposed method is evaluated by repurposing the pretrained model to various low-shot datasets.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">Extensive evaluations and analysis confirm that this single pretrained network 1) can be effectively transferred to various low-shot datasets and 2) improve the generation performance and the convergence time. Interestingly, the model pretrained with our dataset outperforms the model pretrained with the natural images when transferred to several datasets. Our empirical study shows that the bias from a specific dataset for pretraining GANs is harmful to the generalization performance of transfer learning. Finally, our analysis of learned filters provides insight into what makes the pretrained model transferable. The code is available at <a target="_blank" href="https://github.com/FriedRonaldo/Primitives-PS" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:80%;">https://github.com/FriedRonaldo/Primitives-PS</a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Utilizing synthetic datasets</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The samples and labels of synthetic datasets can be generated automatically and unlimitedly by a pre-defined process. Since generating synthetic data can bypass the cumbersome data crawling and pruning for data collection, previous works have utilized synthetic datasets for training the model and then achieved performance improvement on real datasetsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. Domain randomizationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> used various illuminations, color, noise, and texture to reduce the performance gap between the simulated and real samples. By doing so, a model trained with a synthetic dataset helps improve the performance on the real dataset. Fourier domain adaptationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> proposed swapping the low-frequency components of the synthetic and real samples to reduce the domain gap in the texture.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Although the previous methods improved the performance of the model on the real dataset, generating such synthetic datasets requires expertise in domain knowledge or a specific software (e.g., GTA-5 game engineÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>). To handle the issue, Kataoka et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> utilized the iterated function system to generate fractals and used the fractals as a pretraining dataset for classification. As a concurrent work, Baradad et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> observe that the unsupervised representation learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> trains the model using patches, and these patches are visually similar to the noise patches (from the noise generation model) or the patches drawn from GANs. Based on the observation, they generate synthetic datasets and conduct self-supervised learning for an image classification task. However, none of the existing studies have investigated synthetic data generation for training GANs.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Transfer learning in GANs</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">GANs involve a unique architecture and a training strategy; consisting of a discriminator and generator trained via adversarial competition. Therefore, the GAN transfer learning method should be developed by considering the unique characteristics of GANsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. TransferGANÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> trains GANs with a small number of samples by transferring the weights trained on a relatively large dataset. TransferGAN also shows that the performance of the transferred model depends on the relationship between the source and target datasets. Noguchi and HaradaÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> proposed to update only the statistics of the batch normalization layer for transferring GANs. This strategy prevents GANs from overfitting so that the model can generate diverse images even with a small number of samples. FreezeDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> fixes several layers of the discriminator and then finetunes the remaining layers. FreezeD improved the generation performance of transferring from the FFHQ pretrained model to various animals. Despite the improvement in GAN transfer learning, the model still requires a large-scale pretraining dataset. Consequently, they commonly suffer from copyright issues, and their performance is sensitive to the relationship between the source and target dataset. In contrast, our goal is to tackle both issues simultaneously by introducing an effective data synthesizer.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Low-shot learning in GANs</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">For high-quality image generation, GANs require a large-scale dataset, and such a requirement can limit the practical use of GANs. To reduce the number of samples for training, several recent studies have introduced data augmentation for training the discriminator<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Then, the generator can produce images with a small number of samples without reflecting an unwanted transformation such as cutoutÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> in the results (i.e., augmentation leakageÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>). Recently, ReMixÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> utilizes interpolation in the style space to reduce the required images to train an image-to-image translation model. In this work, we tackle low-shot generation using GANs via transfer learning; GANs are trained with a small number of samples by transferring a pretrained network into a low-shot dataset.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Towards an effective data synthesizer</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this work, our primary goal is to develop an unbiased and effective data synthesizer. The synthetic dataset secured by our synthesizer is then used to pretrain GANs, which facilitates low-shot data generation. To accomplish unbiased data generation, we only consider the generic properties of natural images because the inductive bias in a pretraining dataset is harmful to transfer learning of GANs. In the following, we introduce three design philosophies of our data synthesizer inspired by the common characteristics of natural images: 1) learning the power spectrum of natural images, 2) exploiting the shape primitives from natural images, and 3) adopting the existence of saliency in images.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Learning the power spectrum of natural images</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.7" class="ltx_p">Several previous works reported the magnitude of natural images in the frequency domain Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> roughly obeys <math id="S3.SS1.p1.1.m1.2" class="ltx_Math" alttext="w_{m}=\frac{1}{|f_{x}|^{a}+|f_{y}|^{a}}" display="inline"><semantics id="S3.SS1.p1.1.m1.2a"><mrow id="S3.SS1.p1.1.m1.2.3" xref="S3.SS1.p1.1.m1.2.3.cmml"><msub id="S3.SS1.p1.1.m1.2.3.2" xref="S3.SS1.p1.1.m1.2.3.2.cmml"><mi id="S3.SS1.p1.1.m1.2.3.2.2" xref="S3.SS1.p1.1.m1.2.3.2.2.cmml">w</mi><mi id="S3.SS1.p1.1.m1.2.3.2.3" xref="S3.SS1.p1.1.m1.2.3.2.3.cmml">m</mi></msub><mo id="S3.SS1.p1.1.m1.2.3.1" xref="S3.SS1.p1.1.m1.2.3.1.cmml">=</mo><mfrac id="S3.SS1.p1.1.m1.2.2" xref="S3.SS1.p1.1.m1.2.2.cmml"><mn id="S3.SS1.p1.1.m1.2.2.4" xref="S3.SS1.p1.1.m1.2.2.4.cmml">1</mn><mrow id="S3.SS1.p1.1.m1.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.cmml"><msup id="S3.SS1.p1.1.m1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.cmml"><mrow id="S3.SS1.p1.1.m1.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.1.m1.1.1.1.1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2.1.cmml">|</mo><msub id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.cmml">f</mi><mi id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.3.cmml">x</mi></msub><mo stretchy="false" id="S3.SS1.p1.1.m1.1.1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.1.1.2.1.cmml">|</mo></mrow><mi id="S3.SS1.p1.1.m1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.1.3.cmml">a</mi></msup><mo id="S3.SS1.p1.1.m1.2.2.2.3" xref="S3.SS1.p1.1.m1.2.2.2.3.cmml">+</mo><msup id="S3.SS1.p1.1.m1.2.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.2.cmml"><mrow id="S3.SS1.p1.1.m1.2.2.2.2.1.1" xref="S3.SS1.p1.1.m1.2.2.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.1.m1.2.2.2.2.1.1.2" xref="S3.SS1.p1.1.m1.2.2.2.2.1.2.1.cmml">|</mo><msub id="S3.SS1.p1.1.m1.2.2.2.2.1.1.1" xref="S3.SS1.p1.1.m1.2.2.2.2.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.2.2.2.2.1.1.1.2" xref="S3.SS1.p1.1.m1.2.2.2.2.1.1.1.2.cmml">f</mi><mi id="S3.SS1.p1.1.m1.2.2.2.2.1.1.1.3" xref="S3.SS1.p1.1.m1.2.2.2.2.1.1.1.3.cmml">y</mi></msub><mo stretchy="false" id="S3.SS1.p1.1.m1.2.2.2.2.1.1.3" xref="S3.SS1.p1.1.m1.2.2.2.2.1.2.1.cmml">|</mo></mrow><mi id="S3.SS1.p1.1.m1.2.2.2.2.3" xref="S3.SS1.p1.1.m1.2.2.2.2.3.cmml">a</mi></msup></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.2b"><apply id="S3.SS1.p1.1.m1.2.3.cmml" xref="S3.SS1.p1.1.m1.2.3"><eq id="S3.SS1.p1.1.m1.2.3.1.cmml" xref="S3.SS1.p1.1.m1.2.3.1"></eq><apply id="S3.SS1.p1.1.m1.2.3.2.cmml" xref="S3.SS1.p1.1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.3.2.1.cmml" xref="S3.SS1.p1.1.m1.2.3.2">subscript</csymbol><ci id="S3.SS1.p1.1.m1.2.3.2.2.cmml" xref="S3.SS1.p1.1.m1.2.3.2.2">ğ‘¤</ci><ci id="S3.SS1.p1.1.m1.2.3.2.3.cmml" xref="S3.SS1.p1.1.m1.2.3.2.3">ğ‘š</ci></apply><apply id="S3.SS1.p1.1.m1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2"><divide id="S3.SS1.p1.1.m1.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2"></divide><cn type="integer" id="S3.SS1.p1.1.m1.2.2.4.cmml" xref="S3.SS1.p1.1.m1.2.2.4">1</cn><apply id="S3.SS1.p1.1.m1.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2"><plus id="S3.SS1.p1.1.m1.2.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2.3"></plus><apply id="S3.SS1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1">superscript</csymbol><apply id="S3.SS1.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1"><abs id="S3.SS1.p1.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.2"></abs><apply id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.2">ğ‘“</ci><ci id="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1.1.1.3">ğ‘¥</ci></apply></apply><ci id="S3.SS1.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.3">ğ‘</ci></apply><apply id="S3.SS1.p1.1.m1.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2">superscript</csymbol><apply id="S3.SS1.p1.1.m1.2.2.2.2.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.1.1"><abs id="S3.SS1.p1.1.m1.2.2.2.2.1.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.1.1.2"></abs><apply id="S3.SS1.p1.1.m1.2.2.2.2.1.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.2.2.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.2.2.2.2.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.1.1.1.2">ğ‘“</ci><ci id="S3.SS1.p1.1.m1.2.2.2.2.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.1.1.1.3">ğ‘¦</ci></apply></apply><ci id="S3.SS1.p1.1.m1.2.2.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.3">ğ‘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.2c">w_{m}=\frac{1}{|f_{x}|^{a}+|f_{y}|^{a}}</annotation></semantics></math> where <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">a</annotation></semantics></math> is a constant, well approximated to one. Inspired by this finding, we generate synthetic images by randomly drawing <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">a</annotation></semantics></math> from the uniform distribution of <math id="S3.SS1.p1.4.m4.2" class="ltx_Math" alttext="\mathcal{U}(0.5,3.5)" display="inline"><semantics id="S3.SS1.p1.4.m4.2a"><mrow id="S3.SS1.p1.4.m4.2.3" xref="S3.SS1.p1.4.m4.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.4.m4.2.3.2" xref="S3.SS1.p1.4.m4.2.3.2.cmml">ğ’°</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.4.m4.2.3.1" xref="S3.SS1.p1.4.m4.2.3.1.cmml">â€‹</mo><mrow id="S3.SS1.p1.4.m4.2.3.3.2" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p1.4.m4.2.3.3.2.1" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml">(</mo><mn id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">0.5</mn><mo id="S3.SS1.p1.4.m4.2.3.3.2.2" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml">,</mo><mn id="S3.SS1.p1.4.m4.2.2" xref="S3.SS1.p1.4.m4.2.2.cmml">3.5</mn><mo stretchy="false" id="S3.SS1.p1.4.m4.2.3.3.2.3" xref="S3.SS1.p1.4.m4.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.2b"><apply id="S3.SS1.p1.4.m4.2.3.cmml" xref="S3.SS1.p1.4.m4.2.3"><times id="S3.SS1.p1.4.m4.2.3.1.cmml" xref="S3.SS1.p1.4.m4.2.3.1"></times><ci id="S3.SS1.p1.4.m4.2.3.2.cmml" xref="S3.SS1.p1.4.m4.2.3.2">ğ’°</ci><interval closure="open" id="S3.SS1.p1.4.m4.2.3.3.1.cmml" xref="S3.SS1.p1.4.m4.2.3.3.2"><cn type="float" id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">0.5</cn><cn type="float" id="S3.SS1.p1.4.m4.2.2.cmml" xref="S3.SS1.p1.4.m4.2.2">3.5</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.2c">\mathcal{U}(0.5,3.5)</annotation></semantics></math>, as also suggested in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Specifically, random white noise is sampled, and then its magnitude signal after applying the Fast Fourier Transform (FFT) is weighted by <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="w_{m}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><msub id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">w</mi><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">ğ‘¤</ci><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">w_{m}</annotation></semantics></math>. By applying the inverse FFT to the weighted signal, we can easily compute the synthetic image. We repeat this for RGB color channels and finally produce synthetic images. Originally, the image with <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="a=1" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mrow id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">a</mi><mo id="S3.SS1.p1.6.m6.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.cmml">=</mo><mn id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><eq id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1"></eq><ci id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2">ğ‘</ci><cn type="integer" id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">a=1</annotation></semantics></math> was named a pink noise. We call this method of generating images with <math id="S3.SS1.p1.7.m7.2" class="ltx_Math" alttext="a\sim\mathcal{U}(0.5,3.5)" display="inline"><semantics id="S3.SS1.p1.7.m7.2a"><mrow id="S3.SS1.p1.7.m7.2.3" xref="S3.SS1.p1.7.m7.2.3.cmml"><mi id="S3.SS1.p1.7.m7.2.3.2" xref="S3.SS1.p1.7.m7.2.3.2.cmml">a</mi><mo id="S3.SS1.p1.7.m7.2.3.1" xref="S3.SS1.p1.7.m7.2.3.1.cmml">âˆ¼</mo><mrow id="S3.SS1.p1.7.m7.2.3.3" xref="S3.SS1.p1.7.m7.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.7.m7.2.3.3.2" xref="S3.SS1.p1.7.m7.2.3.3.2.cmml">ğ’°</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.7.m7.2.3.3.1" xref="S3.SS1.p1.7.m7.2.3.3.1.cmml">â€‹</mo><mrow id="S3.SS1.p1.7.m7.2.3.3.3.2" xref="S3.SS1.p1.7.m7.2.3.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p1.7.m7.2.3.3.3.2.1" xref="S3.SS1.p1.7.m7.2.3.3.3.1.cmml">(</mo><mn id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">0.5</mn><mo id="S3.SS1.p1.7.m7.2.3.3.3.2.2" xref="S3.SS1.p1.7.m7.2.3.3.3.1.cmml">,</mo><mn id="S3.SS1.p1.7.m7.2.2" xref="S3.SS1.p1.7.m7.2.2.cmml">3.5</mn><mo stretchy="false" id="S3.SS1.p1.7.m7.2.3.3.3.2.3" xref="S3.SS1.p1.7.m7.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.2b"><apply id="S3.SS1.p1.7.m7.2.3.cmml" xref="S3.SS1.p1.7.m7.2.3"><csymbol cd="latexml" id="S3.SS1.p1.7.m7.2.3.1.cmml" xref="S3.SS1.p1.7.m7.2.3.1">similar-to</csymbol><ci id="S3.SS1.p1.7.m7.2.3.2.cmml" xref="S3.SS1.p1.7.m7.2.3.2">ğ‘</ci><apply id="S3.SS1.p1.7.m7.2.3.3.cmml" xref="S3.SS1.p1.7.m7.2.3.3"><times id="S3.SS1.p1.7.m7.2.3.3.1.cmml" xref="S3.SS1.p1.7.m7.2.3.3.1"></times><ci id="S3.SS1.p1.7.m7.2.3.3.2.cmml" xref="S3.SS1.p1.7.m7.2.3.3.2">ğ’°</ci><interval closure="open" id="S3.SS1.p1.7.m7.2.3.3.3.1.cmml" xref="S3.SS1.p1.7.m7.2.3.3.3.2"><cn type="float" id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">0.5</cn><cn type="float" id="S3.SS1.p1.7.m7.2.2.cmml" xref="S3.SS1.p1.7.m7.2.2">3.5</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.2c">a\sim\mathcal{U}(0.5,3.5)</annotation></semantics></math> as <span id="S3.SS1.p1.7.1" class="ltx_text ltx_font_typewriter">PinkNoise</span>. Since we only utilize the generic properties of natural images, no inductive bias toward any specific dataset influences <span id="S3.SS1.p1.7.2" class="ltx_text ltx_font_typewriter">PinkNoise</span>. As shown in <a href="#S1.F1" title="In 1 Introduction â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>(a), <span id="S3.SS1.p1.7.3" class="ltx_text ltx_font_typewriter">PinkNoise</span> produces interesting patterns with vertical, horizontal orientation, or color blobs.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2204.04950/assets/images/absraction.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="293" height="72" alt="Refer to caption">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.4.2" class="ltx_text" style="font-size:90%;">Potentials of primitive shapes for representing things. We only use a line, ellipse, and rectangle to express a cat and a temple. These examples motivate us to develop <span id="S3.F2.4.2.1" class="ltx_text ltx_font_typewriter">Primitives</span>, which generates the data by a simple composition of the shapes.</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Shape primitives inspired by natural images</h3>

<div id="S3.SS2.p1" class="ltx_para">
<blockquote id="S3.SS2.p1.1" class="ltx_quote ltx_epigraph " style="width:233.52pt; margin-left:auto;;">
<div id="S3.SS2.p1.1.1" class="ltx_block ltx_epigraph_text" style="text-align:left; ;">
<p id="S3.SS2.p1.1.1.1" class="ltx_p"><span id="S3.SS2.p1.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">â€œEverything in nature is formed upon the sphere, the cone, and the cylinder. One must learn to paint these simple figures, and then one can do all that he may wish.â€</span></p>
</div>
<div id="S3.SS2.p1.1.2" class="ltx_block ltx_epigraph_source" style="border-top:solid 0.4pt; text-align:right; ;">
<p id="S3.SS2.p1.1.2.1" class="ltx_p"><span id="S3.SS2.p1.1.2.1.1" class="ltx_text" style="font-size:90%;">Paul CÃ©zanne</span></p>
</div>
</blockquote>
<p id="S3.SS2.p1.2" class="ltx_p">Considering the importance of phase in images (e.g., determining the unique appearance of the imageÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>), <span id="S3.SS2.p1.2.1" class="ltx_text ltx_font_typewriter">PinkNoise</span> alone is insufficient to represent the rich characteristics of natural images; <span id="S3.SS2.p1.2.2" class="ltx_text ltx_font_typewriter">PinkNoise</span> is random noise on a phase spectrum. To have a meaningful signal even in its phase, we can consider 1) modeling the phase of natural images independently or 2) developing the different generation strategies to model the magnitude and phase simultaneously. Unlike the magnitude spectrum, we seldom find regularity in the phase of images; thus, it is difficult to derive the generic property of the phase spectrum. Besides, separately modeling the phase and magnitude may not produce meaningful images, preserving the proper structuresÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. For this reason, we focus on finding structural regularity in natural images because it can affect both magnitude and phase. Specifically, we are inspired by the observation that natural images can be represented by the composition of the elementary shapesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. The common practice in artistic drawings also utilizes elementary shapes as the basis for representing things (inspired by Paul CÃ©zanne).</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><a href="#S3.F2" title="In 3.1 Learning the power spectrum of natural images â€£ 3 Towards an effective data synthesizer â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> demonstrates the abstraction examples of various images using elementary shapes, such as ellipses, lines, and rectangles. We find the potential of abstraction via elementary shapes to encode the structural information of natural images and to remove the bias to a specific dataset. We then devise the data synthesizer to produce images consisting of various elementary shapes. The outputs of this synthesis procedure are akin to those of the dead leaves modelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. The dead leaves model is an early generative model, which closely mimics natural images by conducting tessellation, where their sizes and positions are determined by sampling from the Poisson process. Unlike the dead leaves model, we do not fill all the regions and use different distributions for sampling because the resultant images are quite sensitive to the hyperparameter of the Poisson process. For position, we use the uniform distribution. To prevent the large shapes in the later stage from completely overwriting those in the early stage, we gradually decrease the maximum shape size over multiple stages; drawing the small objects toward the end. In addition, it is conversely proportional to the number of currently injected shapes. We name this generation strategy <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_typewriter">Primitives</span>, and <a href="#S1.F1" title="In 1 Introduction â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>(b) visualizes the representative examples. By distributing the shapes in the image space, we observe that <span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_typewriter">Primitives</span> produces images that have a similar magnitude to those of natural images (See <a href="#S3.T1" title="In 3.2 Shape primitives inspired by natural images â€£ 3 Towards an effective data synthesizer â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> and the supplementary 10 for the supporting experiments).</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.4.2" class="ltx_text" style="font-size:90%;">SSIM between the magnitude spectrum of the frequency domain of the synthetic and target dataset. The higher score means the more similar pair. We observed that the tendency is the same with L1 or L2 distance.</span></figcaption>
<div id="S3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:214.6pt;height:25.9pt;vertical-align:-0.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-283.4pt,33.7pt) scale(0.274690947886094,0.274690947886094) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_nopad ltx_align_left ltx_border_t"><svg version="1.1" height="21.61" width="687.5" overflow="visible"><g transform="translate(0,21.61) scale(1,-1)"><path d="M 0,21.61 687.5,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,9.46) scale(1, -1)"><foreignObject width="40.01" height="9.46" overflow="visible">
<span id="S3.T1.1.1.1.1.pic1.1.1" class="ltx_inline-block">
<span id="S3.T1.1.1.1.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S3.T1.1.1.1.1.pic1.1.1.1.1" class="ltx_p">Source</span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(647.87,9.46)"><g transform="translate(0,12.15) scale(1, -1)"><foreignObject width="39.63" height="12.15" overflow="visible">
<span id="S3.T1.1.1.1.1.pic1.2.1" class="ltx_inline-block">
<span id="S3.T1.1.1.1.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S3.T1.1.1.1.1.pic1.2.1.1.1" class="ltx_p">Target</span>
</span>
</span></foreignObject></g></g></g></svg></td>
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Obama</td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">Grumpy cat</td>
<td id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">Bridge</td>
<td id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">Panda</td>
<td id="S3.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FFHQ</td>
<td id="S3.T1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t">Mean</td>
</tr>
<tr id="S3.T1.1.1.2" class="ltx_tr">
<td id="S3.T1.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.1.1.2.1.1" class="ltx_text ltx_font_typewriter">PinkNoise</span></td>
<td id="S3.T1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">0.8368</td>
<td id="S3.T1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">0.8148</td>
<td id="S3.T1.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">0.7676</td>
<td id="S3.T1.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">0.8328</td>
<td id="S3.T1.1.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.8553</td>
<td id="S3.T1.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t">0.8215</td>
</tr>
<tr id="S3.T1.1.1.3" class="ltx_tr">
<td id="S3.T1.1.1.3.1" class="ltx_td ltx_align_left"><span id="S3.T1.1.1.3.1.1" class="ltx_text ltx_font_typewriter">Primitives</span></td>
<td id="S3.T1.1.1.3.2" class="ltx_td ltx_align_center">0.9309</td>
<td id="S3.T1.1.1.3.3" class="ltx_td ltx_align_center">0.9366</td>
<td id="S3.T1.1.1.3.4" class="ltx_td ltx_align_center">0.9198</td>
<td id="S3.T1.1.1.3.5" class="ltx_td ltx_align_center">0.9200</td>
<td id="S3.T1.1.1.3.6" class="ltx_td ltx_align_center ltx_border_r">0.9635</td>
<td id="S3.T1.1.1.3.7" class="ltx_td ltx_align_center">0.9342</td>
</tr>
<tr id="S3.T1.1.1.4" class="ltx_tr">
<td id="S3.T1.1.1.4.1" class="ltx_td ltx_align_left"><span id="S3.T1.1.1.4.1.1" class="ltx_text ltx_font_typewriter">Primitives-S</span></td>
<td id="S3.T1.1.1.4.2" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.4.2.1" class="ltx_text ltx_framed ltx_framed_underline">0.9421</span></td>
<td id="S3.T1.1.1.4.3" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.4.3.1" class="ltx_text ltx_framed ltx_framed_underline">0.9463</span></td>
<td id="S3.T1.1.1.4.4" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.4.4.1" class="ltx_text ltx_font_bold">0.9308</span></td>
<td id="S3.T1.1.1.4.5" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.4.5.1" class="ltx_text ltx_framed ltx_framed_underline">0.9334</span></td>
<td id="S3.T1.1.1.4.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.1.1.4.6.1" class="ltx_text ltx_framed ltx_framed_underline">0.9756</span></td>
<td id="S3.T1.1.1.4.7" class="ltx_td ltx_align_center"><span id="S3.T1.1.1.4.7.1" class="ltx_text ltx_framed ltx_framed_underline">0.9456</span></td>
</tr>
<tr id="S3.T1.1.1.5" class="ltx_tr">
<td id="S3.T1.1.1.5.1" class="ltx_td ltx_align_left ltx_border_b"><span id="S3.T1.1.1.5.1.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span></td>
<td id="S3.T1.1.1.5.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T1.1.1.5.2.1" class="ltx_text ltx_font_bold">0.9432</span></td>
<td id="S3.T1.1.1.5.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T1.1.1.5.3.1" class="ltx_text ltx_font_bold">0.9476</span></td>
<td id="S3.T1.1.1.5.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T1.1.1.5.4.1" class="ltx_text ltx_framed ltx_framed_underline">0.9307</span></td>
<td id="S3.T1.1.1.5.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T1.1.1.5.5.1" class="ltx_text ltx_font_bold">0.9352</span></td>
<td id="S3.T1.1.1.5.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T1.1.1.5.6.1" class="ltx_text ltx_font_bold">0.9767</span></td>
<td id="S3.T1.1.1.5.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T1.1.1.5.7.1" class="ltx_text ltx_font_bold">0.9467</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Combining saliency as prior</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In addition to the natural images, we investigate the benchmark datasets and find that they commonly have saliency, target objects of interest to determine the class. These salient objects are usually located nearly in the middle of the image. For example, the animal face on the cat and panda dataset can be the saliency. To reflect the nature of benchmark datasets, we insert a large shape after applying <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_typewriter">Primitives</span> and name it as <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_typewriter">Primitives-S</span> (<span id="S3.SS3.p1.1.3" class="ltx_text ltx_font_typewriter">Primitives</span> with <span id="S3.SS3.p1.1.4" class="ltx_text ltx_font_typewriter">S</span>aliency).</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">By utilizing the three design factors, we develop four variants of our data synthesizer. They are
1) <span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_typewriter">PinkNoise</span> adopting the nature of magnitude spectrum of natural images only as shown in <a href="#S1.F1" title="In 1 Introduction â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>(a), 2) <span id="S3.SS3.p2.1.2" class="ltx_text ltx_font_typewriter">Primitives</span> generating various elementary (monotone) shapes randomly as illustrated in <a href="#S1.F1" title="In 1 Introduction â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>(b), and 3) <span id="S3.SS3.p2.1.3" class="ltx_text ltx_font_typewriter">Primitives-S</span> adding a salient object into <span id="S3.SS3.p2.1.4" class="ltx_text ltx_font_typewriter">Primitives</span> in <a href="#S1.F1" title="In 1 Introduction â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>(c).</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2204.04950/assets/images/Primitives2Obama_3x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="147" height="147" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2204.04950/assets/images/PrimitivesPS2Obama_3x3.png" id="S3.F3.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="147" height="147" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F3.2" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S3.F3.2.1" class="ltx_text ltx_inline-block" style="width:106.2pt;">(a)</span>
<span id="S3.F3.2.2" class="ltx_text ltx_inline-block" style="width:106.2pt;">(b)</span></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.6.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.7.2" class="ltx_text" style="font-size:90%;">Comparison between (a) <span id="S3.F3.7.2.1" class="ltx_text ltx_font_typewriter">Primitives</span> and (b) <span id="S3.F3.7.2.2" class="ltx_text ltx_font_typewriter">Primitives-PS</span> on Obama dataset. The model pretrained with <span id="S3.F3.7.2.3" class="ltx_text ltx_font_typewriter">Primitives</span> generates multiple faces in a single image.</span></figcaption>
</figure>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Finally, we apply a <span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_typewriter">PinkNoise</span> pattern onto the salient object and the background of <span id="S3.SS3.p3.1.2" class="ltx_text ltx_font_typewriter">Primitives-S</span>, which is called (4) <span id="S3.SS3.p3.1.3" class="ltx_text ltx_font_typewriter">Primitives-PS</span> (<span id="S3.SS3.p3.1.4" class="ltx_text ltx_font_typewriter">Primitives</span> with <span id="S3.SS3.p3.1.5" class="ltx_text ltx_font_typewriter">P</span>atterned <span id="S3.SS3.p3.1.6" class="ltx_text ltx_font_typewriter">S</span>aliency) as shown in <a href="#S1.F1" title="In 1 Introduction â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>(d). Since the size of the salient object is considerable, having a salient monotone object may induce an unwanted texture bias. Focusing on the visual effects, inserting the monotone object can be similar to the regional dropoutÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> in the weakly-supervised object localization (WSOL) task. When training a network with the regional dropout, previous WSOL methods suggest filling the dropped region with mean statistics or with other regions from the same image to prevent distribution bias. Motivated by the practice in WSOL, we apply <span id="S3.SS3.p3.1.7" class="ltx_text ltx_font_typewriter">PinkNoise</span> to the salient object.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">The effectiveness of the proposed synthetic datasets is evaluated by transferring GANs in <a href="#S4" title="4 Experiments â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>. We first pretrain GANs using the randomly generated images via our <span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span>, and then finetune the pretrained model on low-shot datasets. While finetuning, all competitors and our pretrained model utilize DiffAug (translation, cutout, and color jittering). For the pretraining results and the details, please refer to the supplementary 9.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We first demonstrate the effectiveness of four variants of our data synthesizer. Then, we choose the best strategy among the four variants and use it for pretraining GANs. Our pretrained model is compared with other pretrained models using a natural benchmark dataset in the transfer learning scenario. We also provide an ablation study on the number of particles in each synthetic image and a policy to determine the size of each particle in the supplementary 1.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Datasets.</span> For the comparison between our synthesizers, we adopt four datasets, including Obama, Grumpy cat, Panda, and Bridge of sighs (Bridge)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. To compare with transfer learning methods, we also use Wuzhen, Temple of heaven (Temple), and Medici fountain (Fountain). Each dataset has 100 images. In addition, we create a dataset, namely Buildings, by merging a subset of four datasets; Bridge of sighs, Wuzhen, Temple of heaven, and Medici fountain. Buildings is used to evaluate the performance under highly diverse conditions. For comprehensive evaluations, we also use CIFAR-10/100 datasets when training with BigGAN.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.4.2" class="ltx_text" style="font-size:90%;">The FID score of transferring to low-shot datasets from the proposed pretraining datasets. The lower is the better. Bold and underlined text indicates the best and second best performance among the pretraining datasets. It will be the same convention throughout the paper.</span></figcaption>
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:195.1pt;height:31.2pt;vertical-align:-0.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-253.0pt,40.1pt) scale(0.278297299919981,0.278297299919981) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_nopad ltx_align_left ltx_border_t"><svg version="1.1" height="21.61" width="687.5" overflow="visible"><g transform="translate(0,21.61) scale(1,-1)"><path d="M 0,21.61 687.5,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,9.46) scale(1, -1)"><foreignObject width="40.01" height="9.46" overflow="visible">
<span id="S4.T2.1.1.1.1.pic1.1.1" class="ltx_inline-block">
<span id="S4.T2.1.1.1.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S4.T2.1.1.1.1.pic1.1.1.1.1" class="ltx_p">Source</span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(647.87,9.46)"><g transform="translate(0,12.15) scale(1, -1)"><foreignObject width="39.63" height="12.15" overflow="visible">
<span id="S4.T2.1.1.1.1.pic1.2.1" class="ltx_inline-block">
<span id="S4.T2.1.1.1.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S4.T2.1.1.1.1.pic1.2.1.1.1" class="ltx_p">Target</span>
</span>
</span></foreignObject></g></g></g></svg></td>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Obama</td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">Grumpy cat</td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">Bridge</td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">Panda</td>
</tr>
<tr id="S4.T2.1.1.2" class="ltx_tr">
<td id="S4.T2.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Scratch + DiffAug</td>
<td id="S4.T2.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">48.98</td>
<td id="S4.T2.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">27.51</td>
<td id="S4.T2.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">57.72</td>
<td id="S4.T2.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">15.82</td>
</tr>
<tr id="S4.T2.1.1.3" class="ltx_tr">
<td id="S4.T2.1.1.3.1" class="ltx_td ltx_align_left"><span id="S4.T2.1.1.3.1.1" class="ltx_text ltx_font_typewriter">PinkNoise</span></td>
<td id="S4.T2.1.1.3.2" class="ltx_td ltx_align_center">50.32</td>
<td id="S4.T2.1.1.3.3" class="ltx_td ltx_align_center">29.47</td>
<td id="S4.T2.1.1.3.4" class="ltx_td ltx_align_center">73.82</td>
<td id="S4.T2.1.1.3.5" class="ltx_td ltx_align_center">15.65</td>
</tr>
<tr id="S4.T2.1.1.4" class="ltx_tr">
<td id="S4.T2.1.1.4.1" class="ltx_td ltx_align_left"><span id="S4.T2.1.1.4.1.1" class="ltx_text ltx_font_typewriter">Primitives</span></td>
<td id="S4.T2.1.1.4.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.4.2.1" class="ltx_text ltx_framed ltx_framed_underline">43.20</span></td>
<td id="S4.T2.1.1.4.3" class="ltx_td ltx_align_center">27.97</td>
<td id="S4.T2.1.1.4.4" class="ltx_td ltx_align_center">59.89</td>
<td id="S4.T2.1.1.4.5" class="ltx_td ltx_align_center">12.78</td>
</tr>
<tr id="S4.T2.1.1.5" class="ltx_tr">
<td id="S4.T2.1.1.5.1" class="ltx_td ltx_align_left"><span id="S4.T2.1.1.5.1.1" class="ltx_text ltx_font_typewriter">Primitives-S</span></td>
<td id="S4.T2.1.1.5.2" class="ltx_td ltx_align_center">43.29</td>
<td id="S4.T2.1.1.5.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.5.3.1" class="ltx_text ltx_framed ltx_framed_underline">26.57</span></td>
<td id="S4.T2.1.1.5.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.5.4.1" class="ltx_text ltx_framed ltx_framed_underline">57.24</span></td>
<td id="S4.T2.1.1.5.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.5.5.1" class="ltx_text ltx_font_bold">11.95</span></td>
</tr>
<tr id="S4.T2.1.1.6" class="ltx_tr">
<td id="S4.T2.1.1.6.1" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T2.1.1.6.1.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span></td>
<td id="S4.T2.1.1.6.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.1.6.2.1" class="ltx_text ltx_font_bold">41.62</span></td>
<td id="S4.T2.1.1.6.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.1.6.3.1" class="ltx_text ltx_font_bold">26.01</span></td>
<td id="S4.T2.1.1.6.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.1.6.4.1" class="ltx_text ltx_font_bold">54.02</span></td>
<td id="S4.T2.1.1.6.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.1.6.5.1" class="ltx_text ltx_framed ltx_framed_underline">12.23</span></td>
</tr>
</table>
</span></div>
</figure>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Evaluation protocols.</span> StyleGAN2 architectureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> with DiffAugÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> is applied when evaluating all models in the low-shot generation task. The baseline is the model trained from scratch with DiffAug. The strong competitors are TransferGANÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> and FreezeDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, where both methods suggest finetuning strategies. To reproduce the competitors, we pretrain StyleGAN2 on FFHQâ€“ the face dataset and then finetune the pretrained model using TransferGAN with DiffAug and FreezeD with DiffAug, respectively. Since the baseline can outperform the competitors upon the target datasets, we report the baseline performances for comparison. Besides, we stress that all competitors, baseline and <span id="S4.p3.1.2" class="ltx_text ltx_font_typewriter">Primitives-PS</span> use DiffAug. Specifically, we follow the configuration of DiffAug for <span id="S4.p3.1.3" class="ltx_text ltx_font_typewriter">Primitives-PS</span> and the baseline (from scratch with DiffAug). Otherwise, we use the configuration of TransferGAN and FreezeD as described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> for the best performance.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">We also apply our synthetic dataset to pretrain BigGANÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and repurpose the model to CIFAR-10/100 datasets for evaluating our synthesizer in the conditional generation task. Since <span id="S4.p4.1.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span> does not have labels, we randomly assigned the labels during pretraining. We developed the pretrained model independently for CIFAR-10 and 100 as they have different architectures due to different numbers of classes. For evaluating the conditional generation task, we compare three models; 1) the model naÃ¯vly trained from scratch, 2) the model trained with DiffAug only (DiffAug), and 3) our model pretrained with <span id="S4.p4.1.2" class="ltx_text ltx_font_typewriter">Primitives-PS</span> and then finetuned with DiffAug. We use 10%, 20%, and 100% samples of CIFAR for evaluation and check the effectiveness of our strategy under the data-scarce scenario. As an evaluation metric, we use FrÃ©chet inception distance (FID)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and report the FID score of the best model during training as suggested by DiffAugÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>. We also provide KMMDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> for the better quantitative evaluation, please refer to supplementary 11.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2204.04950/assets/images/differentiation_cut.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="296" height="148" alt="Refer to caption">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.4.2" class="ltx_text" style="font-size:90%;">Morphing upon the transfer learning iterations of the <span id="S4.F4.4.2.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span> pretrained model. We generate the images by using the same latent vector. The center lilac circles are gradually changed into salient regions.</span></figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2204.04950/assets/images/concat_qual_diffaug.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="168" height="168" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2204.04950/assets/images/concat_qual_transfergan.png" id="S4.F5.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="168" height="168" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2204.04950/assets/images/concat_qual_freezed.png" id="S4.F5.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="168" height="168" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2204.04950/assets/images/concat_qual_ours.png" id="S4.F5.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="168" height="168" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F5.2" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S4.F5.2.1" class="ltx_text ltx_inline-block" style="width:121.7pt;">(a) From scratch</span>
<span id="S4.F5.2.2" class="ltx_text ltx_inline-block" style="width:121.7pt;">(b) TransferGAN</span>
<span id="S4.F5.2.3" class="ltx_text ltx_inline-block" style="width:121.7pt;">(c) FreezeD</span>
<span id="S4.F5.2.4" class="ltx_text ltx_inline-block" style="width:121.7pt;">(d) <span id="S4.F5.2.4.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span></span></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.4.2" class="ltx_text" style="font-size:90%;">Qualitative evaluation on Obama, Grumpy cat, Temple, and Wuzhen. For more results, please refer to the supplementary 5.</span></figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.4.2" class="ltx_text" style="font-size:90%;">The FID score of transferred models to low-shot datasets. We use FFHQ pretrained weight for TransferGAN and FreezeD. For all models, we apply DiffAug. Bold and underlined text indicates the best and second best performance among the pretraining datasets.</span></figcaption>
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:422.3pt;height:44.4pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-236.8pt,24.6pt) scale(0.47141935205859,0.47141935205859) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_nopad ltx_align_left ltx_border_t"><svg version="1.1" height="21.61" width="687.5" overflow="visible"><g transform="translate(0,21.61) scale(1,-1)"><path d="M 0,21.61 687.5,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,9.46) scale(1, -1)"><foreignObject width="40.01" height="9.46" overflow="visible">
<span id="S4.T3.1.1.1.1.pic1.1.1" class="ltx_inline-block">
<span id="S4.T3.1.1.1.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S4.T3.1.1.1.1.pic1.1.1.1.1" class="ltx_p">Source</span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(647.87,9.46)"><g transform="translate(0,12.15) scale(1, -1)"><foreignObject width="39.63" height="12.15" overflow="visible">
<span id="S4.T3.1.1.1.1.pic1.2.1" class="ltx_inline-block">
<span id="S4.T3.1.1.1.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S4.T3.1.1.1.1.pic1.2.1.1.1" class="ltx_p">Target</span>
</span>
</span></foreignObject></g></g></g></svg></td>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Obama</td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">Grumpy cat</td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">Bridge</td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">Panda</td>
<td id="S4.T3.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t">Temple</td>
<td id="S4.T3.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t">Wuzhen</td>
<td id="S4.T3.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t">Fountain</td>
<td id="S4.T3.1.1.1.9" class="ltx_td ltx_align_center ltx_border_t">Buildings</td>
</tr>
<tr id="S4.T3.1.1.2" class="ltx_tr">
<td id="S4.T3.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Scratch + DiffAugÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>
</td>
<td id="S4.T3.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">48.98</td>
<td id="S4.T3.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.2.3.1" class="ltx_text ltx_framed ltx_framed_underline">27.51</span></td>
<td id="S4.T3.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.2.4.1" class="ltx_text ltx_framed ltx_framed_underline">57.72</span></td>
<td id="S4.T3.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">15.82</td>
<td id="S4.T3.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t">46.69</td>
<td id="S4.T3.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t">146.81</td>
<td id="S4.T3.1.1.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.2.8.1" class="ltx_text ltx_framed ltx_framed_underline">44.46</span></td>
<td id="S4.T3.1.1.2.9" class="ltx_td ltx_align_center ltx_border_t">93.71</td>
</tr>
<tr id="S4.T3.1.1.3" class="ltx_tr">
<td id="S4.T3.1.1.3.1" class="ltx_td ltx_align_left">TransferGANÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S4.T3.1.1.3.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.3.2.1" class="ltx_text ltx_framed ltx_framed_underline">36.50</span></td>
<td id="S4.T3.1.1.3.3" class="ltx_td ltx_align_center">30.60</td>
<td id="S4.T3.1.1.3.4" class="ltx_td ltx_align_center">60.29</td>
<td id="S4.T3.1.1.3.5" class="ltx_td ltx_align_center">14.53</td>
<td id="S4.T3.1.1.3.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.3.6.1" class="ltx_text ltx_framed ltx_framed_underline">40.58</span></td>
<td id="S4.T3.1.1.3.7" class="ltx_td ltx_align_center">95.83</td>
<td id="S4.T3.1.1.3.8" class="ltx_td ltx_align_center">46.61</td>
<td id="S4.T3.1.1.3.9" class="ltx_td ltx_align_center">81.63</td>
</tr>
<tr id="S4.T3.1.1.4" class="ltx_tr">
<td id="S4.T3.1.1.4.1" class="ltx_td ltx_align_left">FreezeDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</td>
<td id="S4.T3.1.1.4.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.4.2.1" class="ltx_text ltx_font_bold">35.90</span></td>
<td id="S4.T3.1.1.4.3" class="ltx_td ltx_align_center">29.41</td>
<td id="S4.T3.1.1.4.4" class="ltx_td ltx_align_center">59.47</td>
<td id="S4.T3.1.1.4.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.4.5.1" class="ltx_text ltx_framed ltx_framed_underline">13.39</span></td>
<td id="S4.T3.1.1.4.6" class="ltx_td ltx_align_center">42.09</td>
<td id="S4.T3.1.1.4.7" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.4.7.1" class="ltx_text ltx_framed ltx_framed_underline">93.54</span></td>
<td id="S4.T3.1.1.4.8" class="ltx_td ltx_align_center">45.70</td>
<td id="S4.T3.1.1.4.9" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.4.9.1" class="ltx_text ltx_framed ltx_framed_underline">80.48</span></td>
</tr>
<tr id="S4.T3.1.1.5" class="ltx_tr">
<td id="S4.T3.1.1.5.1" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T3.1.1.5.1.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span></td>
<td id="S4.T3.1.1.5.2" class="ltx_td ltx_align_center ltx_border_b">41.62</td>
<td id="S4.T3.1.1.5.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.1.5.3.1" class="ltx_text ltx_font_bold">26.01</span></td>
<td id="S4.T3.1.1.5.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.1.5.4.1" class="ltx_text ltx_font_bold">54.02</span></td>
<td id="S4.T3.1.1.5.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.1.5.5.1" class="ltx_text ltx_font_bold">12.23</span></td>
<td id="S4.T3.1.1.5.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.1.5.6.1" class="ltx_text ltx_font_bold">40.42</span></td>
<td id="S4.T3.1.1.5.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.1.5.7.1" class="ltx_text ltx_font_bold">88.14</span></td>
<td id="S4.T3.1.1.5.8" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.1.5.8.1" class="ltx_text ltx_font_bold">43.06</span></td>
<td id="S4.T3.1.1.5.9" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.1.5.9.1" class="ltx_text ltx_font_bold">78.74</span></td>
</tr>
</table>
</span></div>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Effects of different data synthesizers</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We developed four variants of data synthesizer: <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">PinkNoise</span>, <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_typewriter">Primitives</span>, <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_typewriter">Primitives-S</span>, and <span id="S4.SS1.p1.1.4" class="ltx_text ltx_font_typewriter">Primitives-PS</span>. We evaluate their effectiveness in the low-shot generation scenarioâ€“ pretraining with the synthetic dataset and then finetuning on target datasets with DiffAug.
<a href="#S4.T2" title="In 4 Experiments â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> summarizes the FID scores of four data synthesizers and the baseline under four different low-shot datasets.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">In general, <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_typewriter">PinkNoise</span> fails to improve the FID score (worse than the baseline), but converges fast (See the supplementary 2). Unlike <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_typewriter">PinkNoise</span>, <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_typewriter">Primitives</span> clearly improves the generation performance in Obama and Panda, large margins from the baseline. However, it is not effective on Grumpy cat and Bridge. Compared to <span id="S4.SS1.p2.1.4" class="ltx_text ltx_font_typewriter">Primitives</span>, <span id="S4.SS1.p2.1.5" class="ltx_text ltx_font_typewriter">Primitives-S</span> further improves the FID scores, demonstrating the effectiveness of saliency prior. Finally, <span id="S4.SS1.p2.1.6" class="ltx_text ltx_font_typewriter">Primitives-PS</span> clearly improves the low-shot generation performance on all datasets by about 15% on average over the baseline. We provide the qualitative evaluation in the supplementary 3. From these results, we observe that 1) a naÃ¯ve synthesizer (<span id="S4.SS1.p2.1.7" class="ltx_text ltx_font_typewriter">PinkNoise</span>) is even worse than simply using the low-shot dataset, and 2) the combination of our three design factors (<span id="S4.SS1.p2.1.8" class="ltx_text ltx_font_typewriter">Primitives-PS</span>) remarkably improves the baseline, supporting the effectiveness and importance of each factor.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">To analyze how closely our data synthesizers mimic the real datasets, we focus on measuring the similarity between our synthetic dataset (source) and the actual low-shot dataset (target). Instead of pixel distance, we compare the average structural similarity (SSIM) between two datasets in the frequency domain. Since the phase periodically varies in <math id="S4.SS1.p3.1.m1.2" class="ltx_Math" alttext="[-\pi,\pi]" display="inline"><semantics id="S4.SS1.p3.1.m1.2a"><mrow id="S4.SS1.p3.1.m1.2.2.1" xref="S4.SS1.p3.1.m1.2.2.2.cmml"><mo stretchy="false" id="S4.SS1.p3.1.m1.2.2.1.2" xref="S4.SS1.p3.1.m1.2.2.2.cmml">[</mo><mrow id="S4.SS1.p3.1.m1.2.2.1.1" xref="S4.SS1.p3.1.m1.2.2.1.1.cmml"><mo id="S4.SS1.p3.1.m1.2.2.1.1a" xref="S4.SS1.p3.1.m1.2.2.1.1.cmml">âˆ’</mo><mi id="S4.SS1.p3.1.m1.2.2.1.1.2" xref="S4.SS1.p3.1.m1.2.2.1.1.2.cmml">Ï€</mi></mrow><mo id="S4.SS1.p3.1.m1.2.2.1.3" xref="S4.SS1.p3.1.m1.2.2.2.cmml">,</mo><mi id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">Ï€</mi><mo stretchy="false" id="S4.SS1.p3.1.m1.2.2.1.4" xref="S4.SS1.p3.1.m1.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.2b"><interval closure="closed" id="S4.SS1.p3.1.m1.2.2.2.cmml" xref="S4.SS1.p3.1.m1.2.2.1"><apply id="S4.SS1.p3.1.m1.2.2.1.1.cmml" xref="S4.SS1.p3.1.m1.2.2.1.1"><minus id="S4.SS1.p3.1.m1.2.2.1.1.1.cmml" xref="S4.SS1.p3.1.m1.2.2.1.1"></minus><ci id="S4.SS1.p3.1.m1.2.2.1.1.2.cmml" xref="S4.SS1.p3.1.m1.2.2.1.1.2">ğœ‹</ci></apply><ci id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">ğœ‹</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.2c">[-\pi,\pi]</annotation></semantics></math>, the SSIM of the phase spectrum is not reliable for comparison. Therefore, we only report the SSIM using the magnitude spectrum in <a href="#S3.T1" title="In 3.2 Shape primitives inspired by natural images â€£ 3 Towards an effective data synthesizer â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>. We confirm that similar trends are consistently observed in L1 or L2 distance. The value of the SSIM is not an exact indicator for explaining the FID scores. Nevertheless, it helps understand the gains; the low-shot generation performance improves as our data synthesizer models the target dataset more similarly. In <a href="#S4.T2" title="In 4 Experiments â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>, <span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_typewriter">Primitives-S</span> and <span id="S4.SS1.p3.1.2" class="ltx_text ltx_font_typewriter">Primitives-PS</span> were ranked top-2, except for Obama. The two strategies in <a href="#S3.T1" title="In 3.2 Shape primitives inspired by natural images â€£ 3 Towards an effective data synthesizer â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> also show that their magnitude spectrum is the most similar to target datasets. This interesting trend supports that our design factors are effective choices to mimic the statistics of real images.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">We also visualize how our synthetic data gradually fit the target data by showing the generation results at different training stages. For that, <span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_typewriter">Primitives</span> and <span id="S4.SS1.p4.1.2" class="ltx_text ltx_font_typewriter">Primitives-PS</span> are selected to construct the pretrained model, and then they are transferred to Obama. By comparing <span id="S4.SS1.p4.1.3" class="ltx_text ltx_font_typewriter">Primitives</span> and <span id="S4.SS1.p4.1.4" class="ltx_text ltx_font_typewriter">Primitives-PS</span>, we observe the effect of the saliency prior. <a href="#S3.F3" title="In 3.3 Combining saliency as prior â€£ 3 Towards an effective data synthesizer â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">3</span></a> shows that the salient shape in <span id="S4.SS1.p4.1.5" class="ltx_text ltx_font_typewriter">Primitives-PS</span> forms the main object as the training evolves. Meanwhile, <span id="S4.SS1.p4.1.6" class="ltx_text ltx_font_typewriter">Primitives</span> includes multiple shapes, meaning all can be candidates for the main object. Consequently, the results often contain multiple faces in the middle of training (e.g., the top-left, the top-right, and the middle in <a href="#S3.F3" title="In 3.3 Combining saliency as prior â€£ 3 Towards an effective data synthesizer â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>(a)). On the other hand, <span id="S4.SS1.p4.1.7" class="ltx_text ltx_font_typewriter">Primitives-PS</span> focuses on generating a single face and eventually exhibits improved quality. We further visualize the gradual changes in outputs of <span id="S4.SS1.p4.1.8" class="ltx_text ltx_font_typewriter">Primitives-PS</span> pretrained model in <a href="#S4.F4" title="In 4 Experiments â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>. For the full animation, please refer to the supplementary material (GIF files).</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">Considering all, we confirm that <span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span> is the best data synthesizer, and thus it is chosen as our final model for comparative evaluations with competitors.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Comparisons with the state-of-the-arts</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We pretrain a model using <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span> and compare it with state-of-the-art models pretrained with natural images in a transfer learning task to low-shot datasets.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><a href="#S4.T3" title="In 4 Experiments â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">3</span></a> reports the quantitative results and <a href="#S4.F5" title="In 4 Experiments â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">5</span></a> shows the qualitative comparison. As expected, TransferGANÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> and FreezeDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> show outstanding performance on the Obama dataset because they are pretrained with FFHQ, meaning the source dataset is a superset of the target. Except for the Obama dataset, our pretrained model with <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span> outperforms all competitors. Unless the inductive bias in the source dataset is advantageous to the target (e.g., Obama), FreezeD does not consistently outperform the baseline (from scratch with DiffAug). In fact, the performances of existing methods highly vary upon target datasets. Contrarily, our pretrained model with <span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_typewriter">Primitives-PS</span> consistently outperforms the competitors in each dataset, except Obama. This implies that our pretrained model has strong transferability. Since <span id="S4.SS2.p2.1.3" class="ltx_text ltx_font_typewriter">Primitives-PS</span> does not use any inductive bias for modeling human faces, the performance drawback on Obama can be acceptable.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">We emphasize that our achievement in generation quality is impressive and meaningful in two aspects: 1) <span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span> uses no real but all synthetic images, which possesses all the attractive nature in application scenarios and 2) our results show the great potential of a single pretrained model for GAN transfer learning.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.3.2" class="ltx_text" style="font-size:90%;">The average consine similarity between the filters in the same layer. The lower value indicates the more diverse filters.</span></figcaption>
<div id="S4.T4.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:162.6pt;height:41pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.9pt,6.5pt) scale(0.758616028125183,0.758616028125183) ;">
<table id="S4.T4.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.4.1.1" class="ltx_tr">
<td id="S4.T4.4.1.1.1" class="ltx_td ltx_align_center ltx_border_t">Pretraining DB</td>
<td id="S4.T4.4.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Discriminator</td>
<td id="S4.T4.4.1.1.3" class="ltx_td ltx_align_center ltx_border_t">Generator</td>
</tr>
<tr id="S4.T4.4.1.2" class="ltx_tr">
<td id="S4.T4.4.1.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.2.1.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span></td>
<td id="S4.T4.4.1.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.2.2.1" class="ltx_text ltx_font_bold">0.00820</span></td>
<td id="S4.T4.4.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.4.1.2.3.1" class="ltx_text ltx_font_bold">0.00828</span></td>
</tr>
<tr id="S4.T4.4.1.3" class="ltx_tr">
<td id="S4.T4.4.1.3.1" class="ltx_td ltx_align_center ltx_border_b">FFHQ</td>
<td id="S4.T4.4.1.3.2" class="ltx_td ltx_align_center ltx_border_b">0.01348</td>
<td id="S4.T4.4.1.3.3" class="ltx_td ltx_align_center ltx_border_b">0.01434</td>
</tr>
</table>
</span></div>
</figure>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.4.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.5.2" class="ltx_text" style="font-size:90%;">The FID of BigGAN, with DiffAug, and with DiffAug initialized by <span id="S4.T5.5.2.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span> (<span id="S4.T5.5.2.2" class="ltx_text ltx_font_typewriter">PS</span>) pretrained model on CIFAR. â€™*â€™ indicates the best FID before augmentation leakageÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Please refer to the supplementary 8 for the details.</span></figcaption>
<div id="S4.T5.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:214.6pt;height:60.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-52.3pt,14.8pt) scale(0.672158689448983,0.672158689448983) ;">
<table id="S4.T5.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.6.1.1" class="ltx_tr">
<td id="S4.T5.6.1.1.1" class="ltx_td ltx_border_t"></td>
<td id="S4.T5.6.1.1.2" class="ltx_td ltx_align_center ltx_border_t" colspan="3">CIFAR-10</td>
<td id="S4.T5.6.1.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="3">CIFAR-100</td>
</tr>
<tr id="S4.T5.6.1.2" class="ltx_tr">
<td id="S4.T5.6.1.2.1" class="ltx_td ltx_border_t"></td>
<td id="S4.T5.6.1.2.2" class="ltx_td ltx_align_center ltx_border_t">10%</td>
<td id="S4.T5.6.1.2.3" class="ltx_td ltx_align_center ltx_border_t">20%</td>
<td id="S4.T5.6.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">100%</td>
<td id="S4.T5.6.1.2.5" class="ltx_td ltx_align_center ltx_border_t">10%</td>
<td id="S4.T5.6.1.2.6" class="ltx_td ltx_align_center ltx_border_t">20%</td>
<td id="S4.T5.6.1.2.7" class="ltx_td ltx_align_center ltx_border_t">100%</td>
</tr>
<tr id="S4.T5.6.1.3" class="ltx_tr">
<td id="S4.T5.6.1.3.1" class="ltx_td ltx_align_left ltx_border_t">BigGAN</td>
<td id="S4.T5.6.1.3.2" class="ltx_td ltx_align_center ltx_border_t">44.14</td>
<td id="S4.T5.6.1.3.3" class="ltx_td ltx_align_center ltx_border_t">20.80</td>
<td id="S4.T5.6.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.45</td>
<td id="S4.T5.6.1.3.5" class="ltx_td ltx_align_center ltx_border_t">66.21</td>
<td id="S4.T5.6.1.3.6" class="ltx_td ltx_align_center ltx_border_t">34.78</td>
<td id="S4.T5.6.1.3.7" class="ltx_td ltx_align_center ltx_border_t">13.45</td>
</tr>
<tr id="S4.T5.6.1.4" class="ltx_tr">
<td id="S4.T5.6.1.4.1" class="ltx_td ltx_align_left">+ DiffAug</td>
<td id="S4.T5.6.1.4.2" class="ltx_td ltx_align_center">29.78*</td>
<td id="S4.T5.6.1.4.3" class="ltx_td ltx_align_center">14.04</td>
<td id="S4.T5.6.1.4.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T5.6.1.4.4.1" class="ltx_text ltx_font_bold">8.55</span></td>
<td id="S4.T5.6.1.4.5" class="ltx_td ltx_align_center">41.70*</td>
<td id="S4.T5.6.1.4.6" class="ltx_td ltx_align_center">21.14</td>
<td id="S4.T5.6.1.4.7" class="ltx_td ltx_align_center">11.51</td>
</tr>
<tr id="S4.T5.6.1.5" class="ltx_tr">
<td id="S4.T5.6.1.5.1" class="ltx_td ltx_align_left ltx_border_b">+ Pretrained (<span id="S4.T5.6.1.5.1.1" class="ltx_text ltx_font_typewriter">PS</span>)</td>
<td id="S4.T5.6.1.5.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.6.1.5.2.1" class="ltx_text ltx_font_bold">21.33</span></td>
<td id="S4.T5.6.1.5.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.6.1.5.3.1" class="ltx_text ltx_font_bold">12.79</span></td>
<td id="S4.T5.6.1.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">8.79</td>
<td id="S4.T5.6.1.5.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.6.1.5.5.1" class="ltx_text ltx_font_bold">32.57</span></td>
<td id="S4.T5.6.1.5.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.6.1.5.6.1" class="ltx_text ltx_font_bold">20.58</span></td>
<td id="S4.T5.6.1.5.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T5.6.1.5.7.1" class="ltx_text ltx_font_bold">11.29</span></td>
</tr>
</table>
</span></div>
</figure>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.4" class="ltx_p"><span id="S4.SS2.p4.4.1" class="ltx_text ltx_font_bold">Diverse filters matter for transferring GANs.</span> From the superior performances of our pretrained model, we conjecture that our achievement was possible by the unbiased nature of our dataset; the pretrained model with FFHQ (FreezeD) has an inductive bias as the face dataset. A previous study analyzing the transferability of CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> also pointed out that the performance of the target dataset degrades when the filters are highly specialized to the source dataset. To analyze the transferability empirically, we measure the similarity between the filters of each layer of the pretrained model. We regard that highly diverse (less similar to each other) filters can indicate that the model is less biased towards a particular domain. That means that the highly transferable model tends to have low filter similarity on average. Specifically, given a weight matrix of each layer, its shape is <math id="S4.SS2.p4.1.m1.4" class="ltx_Math" alttext="[O,I,H,W]" display="inline"><semantics id="S4.SS2.p4.1.m1.4a"><mrow id="S4.SS2.p4.1.m1.4.5.2" xref="S4.SS2.p4.1.m1.4.5.1.cmml"><mo stretchy="false" id="S4.SS2.p4.1.m1.4.5.2.1" xref="S4.SS2.p4.1.m1.4.5.1.cmml">[</mo><mi id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml">O</mi><mo id="S4.SS2.p4.1.m1.4.5.2.2" xref="S4.SS2.p4.1.m1.4.5.1.cmml">,</mo><mi id="S4.SS2.p4.1.m1.2.2" xref="S4.SS2.p4.1.m1.2.2.cmml">I</mi><mo id="S4.SS2.p4.1.m1.4.5.2.3" xref="S4.SS2.p4.1.m1.4.5.1.cmml">,</mo><mi id="S4.SS2.p4.1.m1.3.3" xref="S4.SS2.p4.1.m1.3.3.cmml">H</mi><mo id="S4.SS2.p4.1.m1.4.5.2.4" xref="S4.SS2.p4.1.m1.4.5.1.cmml">,</mo><mi id="S4.SS2.p4.1.m1.4.4" xref="S4.SS2.p4.1.m1.4.4.cmml">W</mi><mo stretchy="false" id="S4.SS2.p4.1.m1.4.5.2.5" xref="S4.SS2.p4.1.m1.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.4b"><list id="S4.SS2.p4.1.m1.4.5.1.cmml" xref="S4.SS2.p4.1.m1.4.5.2"><ci id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">ğ‘‚</ci><ci id="S4.SS2.p4.1.m1.2.2.cmml" xref="S4.SS2.p4.1.m1.2.2">ğ¼</ci><ci id="S4.SS2.p4.1.m1.3.3.cmml" xref="S4.SS2.p4.1.m1.3.3">ğ»</ci><ci id="S4.SS2.p4.1.m1.4.4.cmml" xref="S4.SS2.p4.1.m1.4.4">ğ‘Š</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.4c">[O,I,H,W]</annotation></semantics></math>, where <math id="S4.SS2.p4.2.m2.1" class="ltx_Math" alttext="O" display="inline"><semantics id="S4.SS2.p4.2.m2.1a"><mi id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><ci id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1">ğ‘‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">O</annotation></semantics></math> filters have <math id="S4.SS2.p4.3.m3.1" class="ltx_Math" alttext="I\times H\times W" display="inline"><semantics id="S4.SS2.p4.3.m3.1a"><mrow id="S4.SS2.p4.3.m3.1.1" xref="S4.SS2.p4.3.m3.1.1.cmml"><mi id="S4.SS2.p4.3.m3.1.1.2" xref="S4.SS2.p4.3.m3.1.1.2.cmml">I</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p4.3.m3.1.1.1" xref="S4.SS2.p4.3.m3.1.1.1.cmml">Ã—</mo><mi id="S4.SS2.p4.3.m3.1.1.3" xref="S4.SS2.p4.3.m3.1.1.3.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p4.3.m3.1.1.1a" xref="S4.SS2.p4.3.m3.1.1.1.cmml">Ã—</mo><mi id="S4.SS2.p4.3.m3.1.1.4" xref="S4.SS2.p4.3.m3.1.1.4.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.3.m3.1b"><apply id="S4.SS2.p4.3.m3.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1"><times id="S4.SS2.p4.3.m3.1.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1.1"></times><ci id="S4.SS2.p4.3.m3.1.1.2.cmml" xref="S4.SS2.p4.3.m3.1.1.2">ğ¼</ci><ci id="S4.SS2.p4.3.m3.1.1.3.cmml" xref="S4.SS2.p4.3.m3.1.1.3">ğ»</ci><ci id="S4.SS2.p4.3.m3.1.1.4.cmml" xref="S4.SS2.p4.3.m3.1.1.4">ğ‘Š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.3.m3.1c">I\times H\times W</annotation></semantics></math> tensors. Then, we measure the cosine similarity among all possible permutations of <math id="S4.SS2.p4.4.m4.1" class="ltx_Math" alttext="O" display="inline"><semantics id="S4.SS2.p4.4.m4.1a"><mi id="S4.SS2.p4.4.m4.1.1" xref="S4.SS2.p4.4.m4.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.4.m4.1b"><ci id="S4.SS2.p4.4.m4.1.1.cmml" xref="S4.SS2.p4.4.m4.1.1">ğ‘‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.4.m4.1c">O</annotation></semantics></math> filters and report the mean value of the average similarity of all layers in <a href="#S4.T4" title="In 4.2 Comparisons with the state-of-the-arts â€£ 4 Experiments â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>. For all the layers, please refer to the supplementary 6.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">In summary, <span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span> shows the more diverse filter set in 21 out of 26 layers than the FFHQ pretrained model. According to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, the higher layer (close to the output) tends to specialize in the trained dataset. The same observation holds in our discriminator. The similarity in the last layer of the FFHQ pretrained model is approximately four times higher than <span id="S4.SS2.p5.1.2" class="ltx_text ltx_font_typewriter">Primitives-PS</span>. This explains that the FFHQ pretrained model specialized in human faces, thus transferring well to Obama but not to others.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2204.04950/assets/images/FID_per_it_both.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="296" height="222" alt="Refer to caption">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.5.2.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.2.1" class="ltx_text" style="font-size:90%;">FID per training iterations. The star marker (<math id="S4.F6.2.1.m1.1" class="ltx_Math" alttext="\bigstar" display="inline"><semantics id="S4.F6.2.1.m1.1b"><mi mathvariant="normal" id="S4.F6.2.1.m1.1.1" xref="S4.F6.2.1.m1.1.1.cmml">â˜…</mi><annotation-xml encoding="MathML-Content" id="S4.F6.2.1.m1.1c"><ci id="S4.F6.2.1.m1.1.1.cmml" xref="S4.F6.2.1.m1.1.1">â˜…</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.2.1.m1.1d">\bigstar</annotation></semantics></math>) indicates the point where the model reaches 95% of the best FID score of the from scratch model with DiffAug (baseline). Our <span id="S4.F6.2.1.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span> pretrained model is comparable to the competitors on Obama dataset (upper) and converges faster than the others on Bridge of sighs dataset (lower).</span></figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2204.04950/assets/images/cifar10_10per_vanilla.jpg" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="227" height="227" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2204.04950/assets/images/cifar10_10per_diffaug.jpg" id="S4.F7.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="227" height="227" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2204.04950/assets/images/cifar10_10per_ours.jpg" id="S4.F7.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="227" height="227" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F7.2" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S4.F7.2.1" class="ltx_text ltx_inline-block" style="width:164.0pt;">(a) From scratch</span>
<span id="S4.F7.2.2" class="ltx_text ltx_inline-block" style="width:164.0pt;">(b) + DiffAug</span>
<span id="S4.F7.2.3" class="ltx_text ltx_inline-block" style="width:164.0pt;">(c) + <span id="S4.F7.2.3.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span> pretraining</span></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.4.2" class="ltx_text" style="font-size:90%;">Qualitative evaluation on CIFAR-10 dataset with 10% of samples. Each row contains samples in the same class.</span></figcaption>
</figure>
<div id="S4.SS2.p6" class="ltx_para ltx_noindent">
<p id="S4.SS2.p6.1" class="ltx_p"><span id="S4.SS2.p6.1.1" class="ltx_text ltx_font_bold">Training convergence speed.</span> We investigate the convergence speed of transfer learning by examining FID upon training iterations. <a href="#S4.F6" title="In 4.2 Comparisons with the state-of-the-arts â€£ 4 Experiments â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">6</span></a> describes the evolution of the FID scores during the training. To save space, we provide two different datasets; Obama and Bridge. Results for the complete set are in the supplementary 4. For Obama, all pretrained models converge faster than the baseline (from scratch with DiffAug). Meanwhile, only our model converges faster than the baseline for Bridge. Compared to the baseline, the model pretrained with <span id="S4.SS2.p6.1.2" class="ltx_text ltx_font_typewriter">Primitives-PS</span> reaches 95% of the best baseline performance within the first 30% of iterations. Interestingly, other pretrained models cannot reach 95% of the best baseline performance earlier than the baseline. This shows that our model effectively reduces the required iterations for convergence, and the overhead for pretraining can be sufficiently deducted.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para ltx_noindent">
<p id="S4.SS2.p7.1" class="ltx_p"><span id="S4.SS2.p7.1.1" class="ltx_text ltx_font_bold">Toward a conditional generation task using CIFAR.</span> We conduct conditional generation via transfer learning on CIFAR-10 and 100 as summarized in <a href="#S4.T5" title="In 4.2 Comparisons with the state-of-the-arts â€£ 4 Experiments â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">5</span></a>. <a href="#S4.F7" title="In 4.2 Comparisons with the state-of-the-arts â€£ 4 Experiments â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">7</span></a> shows the qualitative evaluation result on CIFAR-10 with 10% of samples; our <span id="S4.SS2.p7.1.2" class="ltx_text ltx_font_typewriter">Primitives-PS</span> produces the general shape and its structural components better than the baseline and DiffAug. Compared to BigGAN trained from scratch, BigGAN trained from scratch with DiffAug significantly improves the FID score, and the gain is pronounced as the number of training samples decreases. However, we observe that DiffAug suffers from augmentation leakageÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> when the samples are scarce (i.e., the generated samples contain the cutout). Our pretrained model with <span id="S4.SS2.p7.1.3" class="ltx_text ltx_font_typewriter">Primitives-PS</span> shows remarkable performances under the data-hungry scenario, better than DiffAug.</p>
</div>
<div id="S4.SS2.p8" class="ltx_para">
<p id="S4.SS2.p8.1" class="ltx_p">However, when the samples are sufficient (100%), pretraining does not always provide gains over DiffAug. This tendency appears in various downstream tasks. Newell et. al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> reported that the self-supervised pretraining for semi-supervised classification is not advantageous when the amount of data-label pairs are sufficient. TransferGANÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> showed that the gain via transfer learning decreases when the amount of samples is sufficient. In the same vein, the advantage of our pretraining with <span id="S4.SS2.p8.1.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span> decreases as the number of samples increases.</p>
</div>
<div id="S4.SS2.p9" class="ltx_para">
<p id="S4.SS2.p9.1" class="ltx_p">For the extreme low-shot scenario, we also evaluated the model trained with 1% of the dataset. Only for this evaluation, we compare three models; 1) the model naÃ¯vly trained from scratch, 2) the model trained with DiffAug only (DiffAug), and 3) our model pretrained with <span id="S4.SS2.p9.1.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span> and then finetuned without DiffAug. The FID score of the baseline, DiffAug, and ours are 112.13, 101.91, and 78.48, respectively. Although DiffAug improved FID, we observe that DiffAug suffers from the augmentation leakage issue. Therefore, the improvement in FID and its generation results are not meaningful. In contrast, our pretrained model can significantly improve the generation performance without any issue. For more details and results for CIFAR, please refer to the supplementary 8.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and conclusion</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text ltx_font_bold">Societal impact.</span> Since we propose the synthetic dataset for pretraining, the proposed method can consume more power at the pretraining stage (generating the synthetic data and then pretraining the model). However, it converges much faster for finetuning on target datasets, and the same model can be repeatedly used for all targets. In this regard, our method is eventually the more efficient choice in terms of power consumption.
In the point of the ethical view, especially considering the bias issues (e.g., racial or gender bias) in the current benchmark datasets, using our method is much more safe, fair, economical, and practical. Besides, pretraining with our synthetic dataset guarantees the robustness of membership inference attack towards the source dataset because reconstructing our data is meaningless. Since our method is copyright-free, it helps small commercial groups to develop their machine-learning model.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Limitation.</span> Our <span id="S5.p2.1.2" class="ltx_text ltx_font_typewriter">Primitives-PS</span> is devised based on the observations from natural images. Hence, it is possible that more effective observations can further improve the data generation quality. In future work, we plan to develop a metric to quantify the transferability of the model and then derive the data generation process by optimizing the transferability. Formulating such a metric will be challenging but constructive for predicting the behavior of the pretrained model and practically useful in various applications.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">Conclusion.</span> Existing studies for GAN transfer learning utilize a model trained with natural images and thereby suffer from 1) biased pretrained model that can be harmful to the resultant performance and 2) copyright or privacy issues with both the model and dataset. To overcome these limitations, we introduce a new image synthesizer, namely <span id="S5.p3.1.2" class="ltx_text ltx_font_typewriter">Primitives-PS</span>, inspired by the three generic properties of natural images: 1) following the power spectrum of natural images, 2) abstracting the image via the composition of primitive shapes (e.g., line, circle, and rectangle), and 3) having saliency in the image. Experimental comparisons and analysis show that our strategy effectively improves both the generation quality and the convergence speed. We further investigate the diversity of learned filters and report that they are meaningful evidence for discovering the transferability of the pretrained model.</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Acknowledgements.</span><span id="S5.p4.1.2" class="ltx_text" style="font-size:80%;">
We thank Jongwuk Lee and CVML members for the valuable feedback. This research was supported by the NRF Korea funded by the MSIT (2022R1A2C3011154, 2020R1A4A1016619), the IITP grant funded by the MSIT (2020-0-01361, YONSEI UNIVERSITY), and the Korea Medical Device Development Fund grant (202011D06).</span><span id="S5.p4.1.3" class="ltx_text"></span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Kyungjune Baek, Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Hyunjung Shim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Rethinking the truly unsupervised image-to-image translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages 14154â€“14163, 2021.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Kyungjune Baek, Minhyun Lee, and Hyunjung Shim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Psynet: Self-supervised approach to object localization using point
symmetric transformation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial
Intelligence</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, volumeÂ 34, pages 10451â€“10459, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Manel Baradad, Jonas Wulff, Tongzhou Wang, Phillip Isola, and Antonio Torralba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Learning to see by looking at noise.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Andrew Brock, Jeff Donahue, and Karen Simonyan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Large scale GAN training for high fidelity natural image synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
GeoffreyÂ J Burton and IanÂ R Moorhead.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Color and spatial structure in natural scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Applied optics</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 26(1):157â€“170, 1987.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Jie Cao, Luanxuan Hou, Ming-Hsuan Yang, Ran He, and Zhenan Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Remix: Towards image-to-image translation with limited data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 15018â€“15027, 2021.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.5.1" class="ltx_text" style="font-size:90%;">
Nicholas Carlini, Chang Liu, Ãšlfar Erlingsson, Jernej Kos, and Dawn Song.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.6.1" class="ltx_text" style="font-size:90%;">The secret sharer: Evaluating and testing unintended memorization in
neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.7.5" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.4" class="ltx_text ltx_font_italic" style="font-size:90%;">28th <math id="bib.bib7.1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib7.1.1.m1.1a"><mo stretchy="false" id="bib.bib7.1.1.m1.1.1" xref="bib.bib7.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib7.1.1.m1.1b"><ci id="bib.bib7.1.1.m1.1.1.cmml" xref="bib.bib7.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib7.1.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib7.2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib7.2.2.m2.1a"><mo stretchy="false" id="bib.bib7.2.2.m2.1.1" xref="bib.bib7.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib7.2.2.m2.1b"><ci id="bib.bib7.2.2.m2.1.1.cmml" xref="bib.bib7.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib7.2.2.m2.1c">\}</annotation></semantics></math> Security Symposium (<math id="bib.bib7.3.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib7.3.3.m3.1a"><mo stretchy="false" id="bib.bib7.3.3.m3.1.1" xref="bib.bib7.3.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib7.3.3.m3.1b"><ci id="bib.bib7.3.3.m3.1.1.cmml" xref="bib.bib7.3.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib7.3.3.m3.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib7.4.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib7.4.4.m4.1a"><mo stretchy="false" id="bib.bib7.4.4.m4.1.1" xref="bib.bib7.4.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib7.4.4.m4.1b"><ci id="bib.bib7.4.4.m4.1.1.cmml" xref="bib.bib7.4.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib7.4.4.m4.1c">\}</annotation></semantics></math>
Security 19)</span><span id="bib.bib7.8.6" class="ltx_text" style="font-size:90%;">, pages 267â€“284, 2019.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Dingfan Chen, Ning Yu, Yang Zhang, and Mario Fritz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Gan-leaks: A taxonomy of membership inference attacks against
generative models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2020 ACM SIGSAC conference on computer and
communications security</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 343â€“362, 2020.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Junsuk Choe, Song Park, Kyungmin Kim, Joo HyunÂ Park, Dongseob Kim, and Hyunjung
Shim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Face generation for low-shot learning using generative adversarial
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision Workshops</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 1940â€“1948, 2017.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Stargan v2: Diverse image synthesis for multiple domains.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 8188â€“8197, 2020.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Terrance DeVries and GrahamÂ W Taylor.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Improved regularization of convolutional neural networks with cutout.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1708.04552</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
DavidÂ J Field.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Relations between the statistics of natural images and the response
properties of cortical cells.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Josa a</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 4(12):2379â€“2394, 1987.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Generative adversarial nets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages
2672â€“2680, 2014.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
RobertÂ M Haralick, StanleyÂ R Sternberg, and Xinhua Zhuang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Image analysis using mathematical morphology.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">,
(4):532â€“550, 1987.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Jamie Hayes, Luca Melis, George Danezis, and Emiliano DeÂ Cristofaro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Logan: Membership inference attacks against generative models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings on Privacy Enhancing Technologies (PoPETs)</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">,
volume 2019, pages 133â€“152. De Gruyter, 2019.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Momentum contrast for unsupervised visual representation learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 9729â€“9738, 2020.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
Hochreiter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Gans trained by a two time-scale update rule converge to a local nash
equilibrium.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 30, 2017.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Benjamin Hilprecht, Martin HÃ¤rterich, and Daniel Bernau.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Monte carlo and reconstruction membership inference attacks against
generative models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Priv. Enhancing Technol.</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 2019(4):232â€“249, 2019.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate
Saenko, Alexei Efros, and Trevor Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Cycada: Cycle-consistent adversarial domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages
1989â€“1998. PMLR, 2018.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Yunzhong Hou and Liang Zheng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Visualizing adapted knowledge in domain transfer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 13824â€“13833, 2021.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
GaryÂ B Huang and Erik Learned-Miller.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Labeled faces in the wild: Updates and new reporting procedures.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Dept. Comput. Sci., Univ. Massachusetts Amherst, Amherst, MA,
USA, Tech. Rep</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 14(003), 2014.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and
Timo Aila.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Training generative adversarial networks with limited data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In H. Larochelle, M. Ranzato, R. Hadsell, M.Â F. Balcan, and H. Lin,
editors, </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, volumeÂ 33,
pages 12104â€“12114. Curran Associates, Inc., 2020.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and
Timo Aila.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Analyzing and improving the image quality of stylegan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 8110â€“8119, 2020.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Hirokatsu Kataoka, Kazushige Okayasu, Asato Matsumoto, Eisuke Yamagata, Ryosuke
Yamada, Nakamasa Inoue, Akio Nakamura, and Yutaka Satoh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Pre-training without natural images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the Asian Conference on Computer Vision
(ACCV)</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, November 2020.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Christian Ledig, Lucas Theis, Ferenc HuszÃ¡r, Jose Caballero, Andrew
Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz,
Zehan Wang, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Photo-realistic single image super-resolution using a generative
adversarial network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 4681â€“4690, 2017.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
AnnÂ B Lee, David Mumford, and Jinggang Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Occlusion models for natural images: A statistical study of a
scale-invariant dead leaves model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 41(1):35â€“59, 2001.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Jianan Li, Tingfa Xu, Jianming Zhang, Aaron Hertzmann, and Jimei Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">LayoutGAN: Generating graphic layouts with wireframe discriminator.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Deep learning face attributes in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of International Conference on Computer Vision
(ICCV)</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, December 2015.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Ravish Mehra, Qingnan Zhou, Jeremy Long, Alla Sheffer, Amy Gooch, and NiloyÂ J
Mitra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Abstraction of man-made shapes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM SIGGRAPH Asia 2009 papers</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 1â€“10. 2009.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Which training methods for gans do actually converge?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages
3481â€“3490. PMLR, 2018.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Sangwoo Mo, Minsu Cho, and Jinwoo Shin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Freeze the discriminator: a simple baseline for fine-tuning gans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR AI for Content Creation Workshop</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Alejandro Newell and Jia Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">How useful is self-supervised pretraining for visual tasks?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, pages 7345â€“7354, 2020.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Maria-Elena Nilsback and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Automated flower classification over a large number of classes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2008 Sixth Indian Conference on Computer Vision, Graphics &amp;
Image Processing</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, pages 722â€“729. IEEE, 2008.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Atsuhiro Noguchi and Tatsuya Harada.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Image generation from small datasets via batch statistics adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 2750â€“2758, 2019.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Utkarsh Ojha, Yijun Li, Jingwan Lu, AlexeiÂ A Efros, YongÂ Jae Lee, Eli
Shechtman, and Richard Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Few-shot image generation via cross-domain correspondence.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, pages 10743â€“10752, 2021.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
AlanÂ V Oppenheim and JaeÂ S Lim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">The importance of phase in signals.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">, 69(5):529â€“541, 1981.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Semantic image synthesis with spatially-adaptive normalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages 2337â€“2346, 2019.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
StephanÂ R. Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Playing for data: Ground truth from computer games.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors,
</span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision (ECCV)</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, volume 9906 of </span><span id="bib.bib38.6.4" class="ltx_text ltx_font_italic" style="font-size:90%;">LNCS</span><span id="bib.bib38.7.5" class="ltx_text" style="font-size:90%;">, pages 102â€“118. Springer International Publishing, 2016.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Subhankar Roy, Evgeny Krivosheev, Zhun Zhong, Nicu Sebe, and Elisa Ricci.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Curriculum graph co-teaching for multi-target domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, pages 5351â€“5360, 2021.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Maximum classifier discrepancy for unsupervised domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages 3723â€“3732, 2018.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Rui Shu, Hung Bui, Hirokazu Narui, and Stefano Ermon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">A DIRT-t approach to unsupervised domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Natasha Singer and Mike Isaac.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Facebook to pay $550 million to settle facial recognition suit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">The New York Times</span><span id="bib.bib42.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
KrishnaÂ Kumar Singh and YongÂ Jae Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Hide-and-seek: Forcing a network to be meticulous for
weakly-supervised object and action localization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Computer Vision (ICCV)</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Y Tadmor and DJ Tolhurst.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Both the phase and the amplitude spectrum may determine the
appearance of natural images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Vision research</span><span id="bib.bib44.4.2" class="ltx_text" style="font-size:90%;">, 33(1):141â€“145, 1993.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and
Pieter Abbeel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Domain randomization for transferring deep neural networks from
simulation to the real world.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE/RSJ international conference on intelligent robots
and systems (IROS)</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, pages 23â€“30. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
DJ Tolhurst, Y_ Tadmor, and Tang Chao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Amplitude spectra of natural images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Ophthalmic and Physiological Optics</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">, 12(2):229â€“232, 1992.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Trung-Kien Nguyen, and
Ngai-Man Cheung.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">On data augmentation for gan training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Image Processing</span><span id="bib.bib47.4.2" class="ltx_text" style="font-size:90%;">, 30:1882â€“1897, 2021.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Yaxing Wang, Abel Gonzalez-Garcia, David Berga, Luis Herranz, FahadÂ Shahbaz
Khan, and Joost vanÂ de Weijer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Minegan: effective knowledge transfer from gans to target domains
with few images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, pages 9332â€“9341, 2020.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Yaxing Wang, Chenshen Wu, Luis Herranz, Joost vanÂ de Weijer, Abel
Gonzalez-Garcia, and Bogdan Raducanu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Transferring gans: generating images from limited data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, pages 218â€“234, 2018.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Qiantong Xu, Gao Huang, Yang Yuan, Chuan Guo, Yu Sun, Felix Wu, and Kilian
Weinberger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">An empirical study on evaluation metrics of generative adversarial
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1806.07755</span><span id="bib.bib50.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Yanchao Yang and Stefano Soatto.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Fda: Fourier domain adaptation for semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, pages 4085â€“4095, 2020.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">How transferable are features in deep neural networks?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib52.4.2" class="ltx_text" style="font-size:90%;">,
27:3320â€“3328, 2014.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Lsun: Construction of a large-scale image dataset using deep learning
with humans in the loop.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1506.03365</span><span id="bib.bib53.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Wenlong Zhang, Yihao Liu, Chao Dong, and Yu Qiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Ranksrgan: Generative adversarial networks with ranker for image
super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, pages 3096â€“3105, 2019.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo Li, and Dawn Song.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">The secret revealer: Generative model-inversion attacks against deep
neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:90%;">, pages 253â€“261, 2020.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Miaoyun Zhao, Yulai Cong, and Lawrence Carin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">On leveraging pretrained gans for generation with limited data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib56.5.3" class="ltx_text" style="font-size:90%;">, pages
11340â€“11351. PMLR, 2020.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Differentiable augmentation for data-efficient gan training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib57.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Neural Information Processing Systems
(NeurIPS)</span><span id="bib.bib57.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Yang Zou, Zhikun Zhang, Michael Backes, and Yang Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Privacy analysis of deep learning in the wild: Membership inference
attacks against transfer learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2009.04872</span><span id="bib.bib58.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S1a" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Ablation Study</h2>

<div id="S1a.p1" class="ltx_para">
<p id="S1a.p1.5" class="ltx_p">When developing <span id="S1a.p1.5.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span>, we introduce two hyperparameters; 1) the total number of shapes and 2) the policy to determine the size of each component. For determining the size, we consider three policies; <span id="S1a.p1.5.2" class="ltx_text ltx_font_bold">Fix</span>, <span id="S1a.p1.5.3" class="ltx_text ltx_font_bold">Rand</span> and <span id="S1a.p1.5.4" class="ltx_text ltx_font_bold">Decay</span>. <span id="S1a.p1.5.5" class="ltx_text ltx_font_bold">Fix</span> indicates that all particles have the same size. To examine the effect of various scale, we set this size as <math id="S1a.p1.1.m1.3" class="ltx_Math" alttext="H\cdot[\nicefrac{{1}}{{10}},\nicefrac{{1}}{{5}},\nicefrac{{1}}{{2}}]" display="inline"><semantics id="S1a.p1.1.m1.3a"><mrow id="S1a.p1.1.m1.3.4" xref="S1a.p1.1.m1.3.4.cmml"><mi id="S1a.p1.1.m1.3.4.2" xref="S1a.p1.1.m1.3.4.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S1a.p1.1.m1.3.4.1" xref="S1a.p1.1.m1.3.4.1.cmml">â‹…</mo><mrow id="S1a.p1.1.m1.3.4.3.2" xref="S1a.p1.1.m1.3.4.3.1.cmml"><mo stretchy="false" id="S1a.p1.1.m1.3.4.3.2.1" xref="S1a.p1.1.m1.3.4.3.1.cmml">[</mo><mrow id="S1a.p1.1.m1.1.1" xref="S1a.p1.1.m1.1.1.cmml"><mpadded voffset="0.3em" id="S1a.p1.1.m1.1.1.2" xref="S1a.p1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S1a.p1.1.m1.1.1.2a" xref="S1a.p1.1.m1.1.1.2.cmml">1</mn></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S1a.p1.1.m1.1.1.1" xref="S1a.p1.1.m1.1.1.1.cmml"><mo stretchy="true" symmetric="true" id="S1a.p1.1.m1.1.1.1a" xref="S1a.p1.1.m1.1.1.1.cmml">/</mo></mpadded><mn mathsize="70%" id="S1a.p1.1.m1.1.1.3" xref="S1a.p1.1.m1.1.1.3.cmml">10</mn></mrow><mo id="S1a.p1.1.m1.3.4.3.2.2" xref="S1a.p1.1.m1.3.4.3.1.cmml">,</mo><mrow id="S1a.p1.1.m1.2.2" xref="S1a.p1.1.m1.2.2.cmml"><mpadded voffset="0.3em" id="S1a.p1.1.m1.2.2.2" xref="S1a.p1.1.m1.2.2.2.cmml"><mn mathsize="70%" id="S1a.p1.1.m1.2.2.2a" xref="S1a.p1.1.m1.2.2.2.cmml">1</mn></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S1a.p1.1.m1.2.2.1" xref="S1a.p1.1.m1.2.2.1.cmml"><mo stretchy="true" symmetric="true" id="S1a.p1.1.m1.2.2.1a" xref="S1a.p1.1.m1.2.2.1.cmml">/</mo></mpadded><mn mathsize="70%" id="S1a.p1.1.m1.2.2.3" xref="S1a.p1.1.m1.2.2.3.cmml">5</mn></mrow><mo id="S1a.p1.1.m1.3.4.3.2.3" xref="S1a.p1.1.m1.3.4.3.1.cmml">,</mo><mrow id="S1a.p1.1.m1.3.3" xref="S1a.p1.1.m1.3.3.cmml"><mpadded voffset="0.3em" id="S1a.p1.1.m1.3.3.2" xref="S1a.p1.1.m1.3.3.2.cmml"><mn mathsize="70%" id="S1a.p1.1.m1.3.3.2a" xref="S1a.p1.1.m1.3.3.2.cmml">1</mn></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S1a.p1.1.m1.3.3.1" xref="S1a.p1.1.m1.3.3.1.cmml"><mo stretchy="true" symmetric="true" id="S1a.p1.1.m1.3.3.1a" xref="S1a.p1.1.m1.3.3.1.cmml">/</mo></mpadded><mn mathsize="70%" id="S1a.p1.1.m1.3.3.3" xref="S1a.p1.1.m1.3.3.3.cmml">2</mn></mrow><mo stretchy="false" id="S1a.p1.1.m1.3.4.3.2.4" xref="S1a.p1.1.m1.3.4.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1a.p1.1.m1.3b"><apply id="S1a.p1.1.m1.3.4.cmml" xref="S1a.p1.1.m1.3.4"><ci id="S1a.p1.1.m1.3.4.1.cmml" xref="S1a.p1.1.m1.3.4.1">â‹…</ci><ci id="S1a.p1.1.m1.3.4.2.cmml" xref="S1a.p1.1.m1.3.4.2">ğ»</ci><list id="S1a.p1.1.m1.3.4.3.1.cmml" xref="S1a.p1.1.m1.3.4.3.2"><apply id="S1a.p1.1.m1.1.1.cmml" xref="S1a.p1.1.m1.1.1"><divide id="S1a.p1.1.m1.1.1.1.cmml" xref="S1a.p1.1.m1.1.1.1"></divide><cn type="integer" id="S1a.p1.1.m1.1.1.2.cmml" xref="S1a.p1.1.m1.1.1.2">1</cn><cn type="integer" id="S1a.p1.1.m1.1.1.3.cmml" xref="S1a.p1.1.m1.1.1.3">10</cn></apply><apply id="S1a.p1.1.m1.2.2.cmml" xref="S1a.p1.1.m1.2.2"><divide id="S1a.p1.1.m1.2.2.1.cmml" xref="S1a.p1.1.m1.2.2.1"></divide><cn type="integer" id="S1a.p1.1.m1.2.2.2.cmml" xref="S1a.p1.1.m1.2.2.2">1</cn><cn type="integer" id="S1a.p1.1.m1.2.2.3.cmml" xref="S1a.p1.1.m1.2.2.3">5</cn></apply><apply id="S1a.p1.1.m1.3.3.cmml" xref="S1a.p1.1.m1.3.3"><divide id="S1a.p1.1.m1.3.3.1.cmml" xref="S1a.p1.1.m1.3.3.1"></divide><cn type="integer" id="S1a.p1.1.m1.3.3.2.cmml" xref="S1a.p1.1.m1.3.3.2">1</cn><cn type="integer" id="S1a.p1.1.m1.3.3.3.cmml" xref="S1a.p1.1.m1.3.3.3">2</cn></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S1a.p1.1.m1.3c">H\cdot[\nicefrac{{1}}{{10}},\nicefrac{{1}}{{5}},\nicefrac{{1}}{{2}}]</annotation></semantics></math>, where <math id="S1a.p1.2.m2.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S1a.p1.2.m2.1a"><mi id="S1a.p1.2.m2.1.1" xref="S1a.p1.2.m2.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S1a.p1.2.m2.1b"><ci id="S1a.p1.2.m2.1.1.cmml" xref="S1a.p1.2.m2.1.1">ğ»</ci></annotation-xml><annotation encoding="application/x-tex" id="S1a.p1.2.m2.1c">H</annotation></semantics></math> is the image resolution. <span id="S1a.p1.5.6" class="ltx_text ltx_font_bold">Rand</span> randomly samples the size from the uniform distribution. Both policies can induce the occlusion of the previously injected shapes by the later shape. <span id="S1a.p1.5.7" class="ltx_text ltx_font_bold">Decay</span> can bypass the occlusion issue effectively. <span id="S1a.p1.5.8" class="ltx_text ltx_font_bold">Decay</span> arbitrarily samples the size from the uniform distribution, where the maximum size is limited to (<math id="S1a.p1.3.m3.1" class="ltx_Math" alttext="H\cdot\nicefrac{{1}}{{5}}\cdot\nicefrac{{(N-n)}}{{N}}" display="inline"><semantics id="S1a.p1.3.m3.1a"><mrow id="S1a.p1.3.m3.1.2" xref="S1a.p1.3.m3.1.2.cmml"><mi id="S1a.p1.3.m3.1.2.2" xref="S1a.p1.3.m3.1.2.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S1a.p1.3.m3.1.2.1" xref="S1a.p1.3.m3.1.2.1.cmml">â‹…</mo><mrow id="S1a.p1.3.m3.1.2.3" xref="S1a.p1.3.m3.1.2.3.cmml"><mpadded voffset="0.3em" id="S1a.p1.3.m3.1.2.3.2" xref="S1a.p1.3.m3.1.2.3.2.cmml"><mn mathsize="70%" id="S1a.p1.3.m3.1.2.3.2a" xref="S1a.p1.3.m3.1.2.3.2.cmml">1</mn></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S1a.p1.3.m3.1.2.3.1" xref="S1a.p1.3.m3.1.2.3.1.cmml"><mo stretchy="true" symmetric="true" id="S1a.p1.3.m3.1.2.3.1a" xref="S1a.p1.3.m3.1.2.3.1.cmml">/</mo></mpadded><mn mathsize="70%" id="S1a.p1.3.m3.1.2.3.3" xref="S1a.p1.3.m3.1.2.3.3.cmml">5</mn></mrow><mo lspace="0.222em" rspace="0.222em" id="S1a.p1.3.m3.1.2.1a" xref="S1a.p1.3.m3.1.2.1.cmml">â‹…</mo><mrow id="S1a.p1.3.m3.1.1" xref="S1a.p1.3.m3.1.1.cmml"><mpadded voffset="0.3em" id="S1a.p1.3.m3.1.1.1.1" xref="S1a.p1.3.m3.1.1.1.1.1.cmml"><mo maxsize="70%" minsize="70%" id="S1a.p1.3.m3.1.1.1.1.2" xref="S1a.p1.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="S1a.p1.3.m3.1.1.1.1.1" xref="S1a.p1.3.m3.1.1.1.1.1.cmml"><mi mathsize="70%" id="S1a.p1.3.m3.1.1.1.1.1.2" xref="S1a.p1.3.m3.1.1.1.1.1.2.cmml">N</mi><mo mathsize="70%" id="S1a.p1.3.m3.1.1.1.1.1.1" xref="S1a.p1.3.m3.1.1.1.1.1.1.cmml">âˆ’</mo><mi mathsize="70%" id="S1a.p1.3.m3.1.1.1.1.1.3" xref="S1a.p1.3.m3.1.1.1.1.1.3.cmml">n</mi></mrow><mo maxsize="70%" minsize="70%" id="S1a.p1.3.m3.1.1.1.1.3" xref="S1a.p1.3.m3.1.1.1.1.1.cmml">)</mo></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S1a.p1.3.m3.1.1.2" xref="S1a.p1.3.m3.1.1.2.cmml"><mo stretchy="true" symmetric="true" id="S1a.p1.3.m3.1.1.2a" xref="S1a.p1.3.m3.1.1.2.cmml">/</mo></mpadded><mi mathsize="70%" id="S1a.p1.3.m3.1.1.3" xref="S1a.p1.3.m3.1.1.3.cmml">N</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1a.p1.3.m3.1b"><apply id="S1a.p1.3.m3.1.2.cmml" xref="S1a.p1.3.m3.1.2"><ci id="S1a.p1.3.m3.1.2.1.cmml" xref="S1a.p1.3.m3.1.2.1">â‹…</ci><ci id="S1a.p1.3.m3.1.2.2.cmml" xref="S1a.p1.3.m3.1.2.2">ğ»</ci><apply id="S1a.p1.3.m3.1.2.3.cmml" xref="S1a.p1.3.m3.1.2.3"><divide id="S1a.p1.3.m3.1.2.3.1.cmml" xref="S1a.p1.3.m3.1.2.3.1"></divide><cn type="integer" id="S1a.p1.3.m3.1.2.3.2.cmml" xref="S1a.p1.3.m3.1.2.3.2">1</cn><cn type="integer" id="S1a.p1.3.m3.1.2.3.3.cmml" xref="S1a.p1.3.m3.1.2.3.3">5</cn></apply><apply id="S1a.p1.3.m3.1.1.cmml" xref="S1a.p1.3.m3.1.1"><divide id="S1a.p1.3.m3.1.1.2.cmml" xref="S1a.p1.3.m3.1.1.2"></divide><apply id="S1a.p1.3.m3.1.1.1.1.1.cmml" xref="S1a.p1.3.m3.1.1.1.1"><minus id="S1a.p1.3.m3.1.1.1.1.1.1.cmml" xref="S1a.p1.3.m3.1.1.1.1.1.1"></minus><ci id="S1a.p1.3.m3.1.1.1.1.1.2.cmml" xref="S1a.p1.3.m3.1.1.1.1.1.2">ğ‘</ci><ci id="S1a.p1.3.m3.1.1.1.1.1.3.cmml" xref="S1a.p1.3.m3.1.1.1.1.1.3">ğ‘›</ci></apply><ci id="S1a.p1.3.m3.1.1.3.cmml" xref="S1a.p1.3.m3.1.1.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1a.p1.3.m3.1c">H\cdot\nicefrac{{1}}{{5}}\cdot\nicefrac{{(N-n)}}{{N}}</annotation></semantics></math>), and <math id="S1a.p1.4.m4.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S1a.p1.4.m4.1a"><mi id="S1a.p1.4.m4.1.1" xref="S1a.p1.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S1a.p1.4.m4.1b"><ci id="S1a.p1.4.m4.1.1.cmml" xref="S1a.p1.4.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S1a.p1.4.m4.1c">N</annotation></semantics></math> and <math id="S1a.p1.5.m5.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S1a.p1.5.m5.1a"><mi id="S1a.p1.5.m5.1.1" xref="S1a.p1.5.m5.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1a.p1.5.m5.1b"><ci id="S1a.p1.5.m5.1.1.cmml" xref="S1a.p1.5.m5.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S1a.p1.5.m5.1c">n</annotation></semantics></math> are the total number of shapes and the number of previously injected particles. In this way, we can ensure that the shapes inserted in the early stage are still visible in the final data. The upper-side of <a href="#S1.T1" title="In 1 Ablation Study â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> summarizes the FID score for each policy on four datasets. The differences in FID among <span id="S1a.p1.5.9" class="ltx_text ltx_font_bold">Fix</span> policies are trivial in that their ratios are not highly correlated with their ranks. Also, we observe that the shapes at the final stage overwrite the previous shapes. Then, the overall appearance with <span id="S1a.p1.5.10" class="ltx_text ltx_font_bold">Fix</span> are similar to <span id="S1a.p1.5.11" class="ltx_text ltx_font_typewriter">PinkNoise</span> with a salient object. We investigate the synthesizer that combines <span id="S1a.p1.5.12" class="ltx_text ltx_font_typewriter">PinkNoise</span> with <span id="S1a.p1.5.13" class="ltx_text ltx_font_typewriter">PS</span> by injecting a saliency and then applying <span id="S1a.p1.5.14" class="ltx_text ltx_font_typewriter">PinkNoise</span> on it. Interestingly, we observe that it shows the similar FID scores to <span id="S1a.p1.5.15" class="ltx_text ltx_font_bold">Fix</span>. For <span id="S1a.p1.5.16" class="ltx_text ltx_font_bold">Rand</span>, it improves the FID score on Obama and bridge, however, the overall performance is much worse than <span id="S1a.p1.5.17" class="ltx_text ltx_font_bold">Decay</span>. Therefore, we choose a <span id="S1a.p1.5.18" class="ltx_text ltx_font_bold">Decay</span> policy as default for choosing the size.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.5.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S1.T1.6.2" class="ltx_text" style="font-size:90%;">Ablation study on the policy to determine the size of each particle (upper) and the number of particles (lower).</span></figcaption>
<div id="S1.T1.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:212.5pt;height:185.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-27.5pt,24.0pt) scale(0.794496038481012,0.794496038481012) ;">
<table id="S1.T1.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.3.3.4" class="ltx_tr">
<td id="S1.T1.3.3.4.1" class="ltx_td ltx_align_left ltx_border_t">Policy</td>
<td id="S1.T1.3.3.4.2" class="ltx_td ltx_align_center ltx_border_t">Obama</td>
<td id="S1.T1.3.3.4.3" class="ltx_td ltx_align_center ltx_border_t">Grumpy cat</td>
<td id="S1.T1.3.3.4.4" class="ltx_td ltx_align_center ltx_border_t">Bridge</td>
<td id="S1.T1.3.3.4.5" class="ltx_td ltx_align_center ltx_border_t">Panda</td>
</tr>
<tr id="S1.T1.1.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S1.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Fix</span> (<math id="S1.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\nicefrac{{1}}{{10}}" display="inline"><semantics id="S1.T1.1.1.1.1.m1.1a"><mrow id="S1.T1.1.1.1.1.m1.1.1" xref="S1.T1.1.1.1.1.m1.1.1.cmml"><mpadded voffset="0.3em" id="S1.T1.1.1.1.1.m1.1.1.2" xref="S1.T1.1.1.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S1.T1.1.1.1.1.m1.1.1.2a" xref="S1.T1.1.1.1.1.m1.1.1.2.cmml">1</mn></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S1.T1.1.1.1.1.m1.1.1.1" xref="S1.T1.1.1.1.1.m1.1.1.1.cmml"><mo stretchy="true" symmetric="true" id="S1.T1.1.1.1.1.m1.1.1.1a" xref="S1.T1.1.1.1.1.m1.1.1.1.cmml">/</mo></mpadded><mn mathsize="70%" id="S1.T1.1.1.1.1.m1.1.1.3" xref="S1.T1.1.1.1.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.1.1.1.1.m1.1b"><apply id="S1.T1.1.1.1.1.m1.1.1.cmml" xref="S1.T1.1.1.1.1.m1.1.1"><divide id="S1.T1.1.1.1.1.m1.1.1.1.cmml" xref="S1.T1.1.1.1.1.m1.1.1.1"></divide><cn type="integer" id="S1.T1.1.1.1.1.m1.1.1.2.cmml" xref="S1.T1.1.1.1.1.m1.1.1.2">1</cn><cn type="integer" id="S1.T1.1.1.1.1.m1.1.1.3.cmml" xref="S1.T1.1.1.1.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.1.1.1.1.m1.1c">\nicefrac{{1}}{{10}}</annotation></semantics></math>)</td>
<td id="S1.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">48.30</td>
<td id="S1.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">29.74</td>
<td id="S1.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">63.00</td>
<td id="S1.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">17.69</td>
</tr>
<tr id="S1.T1.2.2.2" class="ltx_tr">
<td id="S1.T1.2.2.2.1" class="ltx_td ltx_align_left">
<span id="S1.T1.2.2.2.1.1" class="ltx_text ltx_font_bold">Fix</span> (<math id="S1.T1.2.2.2.1.m1.1" class="ltx_Math" alttext="\nicefrac{{1}}{{5}}" display="inline"><semantics id="S1.T1.2.2.2.1.m1.1a"><mrow id="S1.T1.2.2.2.1.m1.1.1" xref="S1.T1.2.2.2.1.m1.1.1.cmml"><mpadded voffset="0.3em" id="S1.T1.2.2.2.1.m1.1.1.2" xref="S1.T1.2.2.2.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S1.T1.2.2.2.1.m1.1.1.2a" xref="S1.T1.2.2.2.1.m1.1.1.2.cmml">1</mn></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S1.T1.2.2.2.1.m1.1.1.1" xref="S1.T1.2.2.2.1.m1.1.1.1.cmml"><mo stretchy="true" symmetric="true" id="S1.T1.2.2.2.1.m1.1.1.1a" xref="S1.T1.2.2.2.1.m1.1.1.1.cmml">/</mo></mpadded><mn mathsize="70%" id="S1.T1.2.2.2.1.m1.1.1.3" xref="S1.T1.2.2.2.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.2.2.2.1.m1.1b"><apply id="S1.T1.2.2.2.1.m1.1.1.cmml" xref="S1.T1.2.2.2.1.m1.1.1"><divide id="S1.T1.2.2.2.1.m1.1.1.1.cmml" xref="S1.T1.2.2.2.1.m1.1.1.1"></divide><cn type="integer" id="S1.T1.2.2.2.1.m1.1.1.2.cmml" xref="S1.T1.2.2.2.1.m1.1.1.2">1</cn><cn type="integer" id="S1.T1.2.2.2.1.m1.1.1.3.cmml" xref="S1.T1.2.2.2.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.2.2.2.1.m1.1c">\nicefrac{{1}}{{5}}</annotation></semantics></math>)</td>
<td id="S1.T1.2.2.2.2" class="ltx_td ltx_align_center">46.41</td>
<td id="S1.T1.2.2.2.3" class="ltx_td ltx_align_center">29.22</td>
<td id="S1.T1.2.2.2.4" class="ltx_td ltx_align_center">64.02</td>
<td id="S1.T1.2.2.2.5" class="ltx_td ltx_align_center">14.97</td>
</tr>
<tr id="S1.T1.3.3.3" class="ltx_tr">
<td id="S1.T1.3.3.3.1" class="ltx_td ltx_align_left">
<span id="S1.T1.3.3.3.1.1" class="ltx_text ltx_font_bold">Fix</span> (<math id="S1.T1.3.3.3.1.m1.1" class="ltx_Math" alttext="\nicefrac{{1}}{{2}}" display="inline"><semantics id="S1.T1.3.3.3.1.m1.1a"><mrow id="S1.T1.3.3.3.1.m1.1.1" xref="S1.T1.3.3.3.1.m1.1.1.cmml"><mpadded voffset="0.3em" id="S1.T1.3.3.3.1.m1.1.1.2" xref="S1.T1.3.3.3.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S1.T1.3.3.3.1.m1.1.1.2a" xref="S1.T1.3.3.3.1.m1.1.1.2.cmml">1</mn></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S1.T1.3.3.3.1.m1.1.1.1" xref="S1.T1.3.3.3.1.m1.1.1.1.cmml"><mo stretchy="true" symmetric="true" id="S1.T1.3.3.3.1.m1.1.1.1a" xref="S1.T1.3.3.3.1.m1.1.1.1.cmml">/</mo></mpadded><mn mathsize="70%" id="S1.T1.3.3.3.1.m1.1.1.3" xref="S1.T1.3.3.3.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.3.3.3.1.m1.1b"><apply id="S1.T1.3.3.3.1.m1.1.1.cmml" xref="S1.T1.3.3.3.1.m1.1.1"><divide id="S1.T1.3.3.3.1.m1.1.1.1.cmml" xref="S1.T1.3.3.3.1.m1.1.1.1"></divide><cn type="integer" id="S1.T1.3.3.3.1.m1.1.1.2.cmml" xref="S1.T1.3.3.3.1.m1.1.1.2">1</cn><cn type="integer" id="S1.T1.3.3.3.1.m1.1.1.3.cmml" xref="S1.T1.3.3.3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.3.3.3.1.m1.1c">\nicefrac{{1}}{{2}}</annotation></semantics></math>)</td>
<td id="S1.T1.3.3.3.2" class="ltx_td ltx_align_center">48.05</td>
<td id="S1.T1.3.3.3.3" class="ltx_td ltx_align_center">29.37</td>
<td id="S1.T1.3.3.3.4" class="ltx_td ltx_align_center">64.65</td>
<td id="S1.T1.3.3.3.5" class="ltx_td ltx_align_center">15.14</td>
</tr>
<tr id="S1.T1.3.3.5" class="ltx_tr">
<td id="S1.T1.3.3.5.1" class="ltx_td ltx_align_left"><span id="S1.T1.3.3.5.1.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">PinkNoise<span id="S1.T1.3.3.5.1.1.1" class="ltx_text ltx_font_serif"> + </span>PS</span></td>
<td id="S1.T1.3.3.5.2" class="ltx_td ltx_align_center">49.13</td>
<td id="S1.T1.3.3.5.3" class="ltx_td ltx_align_center">29.87</td>
<td id="S1.T1.3.3.5.4" class="ltx_td ltx_align_center">66.00</td>
<td id="S1.T1.3.3.5.5" class="ltx_td ltx_align_center">15.12</td>
</tr>
<tr id="S1.T1.3.3.6" class="ltx_tr">
<td id="S1.T1.3.3.6.1" class="ltx_td ltx_align_left"><span id="S1.T1.3.3.6.1.1" class="ltx_text ltx_font_bold">Rand</span></td>
<td id="S1.T1.3.3.6.2" class="ltx_td ltx_align_center">44.85</td>
<td id="S1.T1.3.3.6.3" class="ltx_td ltx_align_center">29.84</td>
<td id="S1.T1.3.3.6.4" class="ltx_td ltx_align_center">60.45</td>
<td id="S1.T1.3.3.6.5" class="ltx_td ltx_align_center">14.67</td>
</tr>
<tr id="S1.T1.3.3.7" class="ltx_tr">
<td id="S1.T1.3.3.7.1" class="ltx_td ltx_align_left"><span id="S1.T1.3.3.7.1.1" class="ltx_text ltx_font_bold">Decay</span></td>
<td id="S1.T1.3.3.7.2" class="ltx_td ltx_align_center"><span id="S1.T1.3.3.7.2.1" class="ltx_text ltx_font_bold">41.62</span></td>
<td id="S1.T1.3.3.7.3" class="ltx_td ltx_align_center"><span id="S1.T1.3.3.7.3.1" class="ltx_text ltx_font_bold">26.01</span></td>
<td id="S1.T1.3.3.7.4" class="ltx_td ltx_align_center"><span id="S1.T1.3.3.7.4.1" class="ltx_text ltx_font_bold">54.02</span></td>
<td id="S1.T1.3.3.7.5" class="ltx_td ltx_align_center"><span id="S1.T1.3.3.7.5.1" class="ltx_text ltx_font_bold">12.23</span></td>
</tr>
<tr id="S1.T1.3.3.8" class="ltx_tr">
<td id="S1.T1.3.3.8.1" class="ltx_td ltx_align_left ltx_border_t"># of particles</td>
<td id="S1.T1.3.3.8.2" class="ltx_td ltx_align_center ltx_border_t">Obama</td>
<td id="S1.T1.3.3.8.3" class="ltx_td ltx_align_center ltx_border_t">Grumpy cat</td>
<td id="S1.T1.3.3.8.4" class="ltx_td ltx_align_center ltx_border_t">Bridge</td>
<td id="S1.T1.3.3.8.5" class="ltx_td ltx_align_center ltx_border_t">Panda</td>
</tr>
<tr id="S1.T1.3.3.9" class="ltx_tr">
<td id="S1.T1.3.3.9.1" class="ltx_td ltx_align_left ltx_border_t">0</td>
<td id="S1.T1.3.3.9.2" class="ltx_td ltx_align_center ltx_border_t">49.13</td>
<td id="S1.T1.3.3.9.3" class="ltx_td ltx_align_center ltx_border_t">29.87</td>
<td id="S1.T1.3.3.9.4" class="ltx_td ltx_align_center ltx_border_t">66.00</td>
<td id="S1.T1.3.3.9.5" class="ltx_td ltx_align_center ltx_border_t">15.12</td>
</tr>
<tr id="S1.T1.3.3.10" class="ltx_tr">
<td id="S1.T1.3.3.10.1" class="ltx_td ltx_align_left">10</td>
<td id="S1.T1.3.3.10.2" class="ltx_td ltx_align_center">44.10</td>
<td id="S1.T1.3.3.10.3" class="ltx_td ltx_align_center">28.00</td>
<td id="S1.T1.3.3.10.4" class="ltx_td ltx_align_center">63.26</td>
<td id="S1.T1.3.3.10.5" class="ltx_td ltx_align_center">13.35</td>
</tr>
<tr id="S1.T1.3.3.11" class="ltx_tr">
<td id="S1.T1.3.3.11.1" class="ltx_td ltx_align_left">50</td>
<td id="S1.T1.3.3.11.2" class="ltx_td ltx_align_center">42.49</td>
<td id="S1.T1.3.3.11.3" class="ltx_td ltx_align_center">28.40</td>
<td id="S1.T1.3.3.11.4" class="ltx_td ltx_align_center">59.17</td>
<td id="S1.T1.3.3.11.5" class="ltx_td ltx_align_center"><span id="S1.T1.3.3.11.5.1" class="ltx_text ltx_font_bold">11.79</span></td>
</tr>
<tr id="S1.T1.3.3.12" class="ltx_tr">
<td id="S1.T1.3.3.12.1" class="ltx_td ltx_align_left">100</td>
<td id="S1.T1.3.3.12.2" class="ltx_td ltx_align_center"><span id="S1.T1.3.3.12.2.1" class="ltx_text ltx_font_bold">41.62</span></td>
<td id="S1.T1.3.3.12.3" class="ltx_td ltx_align_center"><span id="S1.T1.3.3.12.3.1" class="ltx_text ltx_font_bold">26.01</span></td>
<td id="S1.T1.3.3.12.4" class="ltx_td ltx_align_center">54.02</td>
<td id="S1.T1.3.3.12.5" class="ltx_td ltx_align_center">12.23</td>
</tr>
<tr id="S1.T1.3.3.13" class="ltx_tr">
<td id="S1.T1.3.3.13.1" class="ltx_td ltx_align_left ltx_border_b">500</td>
<td id="S1.T1.3.3.13.2" class="ltx_td ltx_align_center ltx_border_b">42.45</td>
<td id="S1.T1.3.3.13.3" class="ltx_td ltx_align_center ltx_border_b">27.92</td>
<td id="S1.T1.3.3.13.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S1.T1.3.3.13.4.1" class="ltx_text ltx_font_bold">52.27</span></td>
<td id="S1.T1.3.3.13.5" class="ltx_td ltx_align_center ltx_border_b">12.12</td>
</tr>
</table>
</span></div>
</figure>
<div id="S1a.p2" class="ltx_para">
<p id="S1a.p2.3" class="ltx_p">Besides, the total number of shapes is important because it affects the transferability and the time complexity of the synthesizer. The lower-side of <a href="#S1.T1" title="In 1 Ablation Study â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> demonstrates the performance trends upon the total number of shapes. A zero particle case implies that only one background and one salient object, thus equivalent to <span id="S1a.p2.3.1" class="ltx_text ltx_font_typewriter">PinkNoise</span> + <span id="S1a.p2.3.2" class="ltx_text ltx_font_typewriter">PS</span>. As the number of shapes (<math id="S1a.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S1a.p2.1.m1.1a"><mi id="S1a.p2.1.m1.1.1" xref="S1a.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S1a.p2.1.m1.1b"><ci id="S1a.p2.1.m1.1.1.cmml" xref="S1a.p2.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S1a.p2.1.m1.1c">N</annotation></semantics></math>) grows upon roughly 100, the performance tends to improve. However, over <math id="S1a.p2.2.m2.1" class="ltx_Math" alttext="N=100" display="inline"><semantics id="S1a.p2.2.m2.1a"><mrow id="S1a.p2.2.m2.1.1" xref="S1a.p2.2.m2.1.1.cmml"><mi id="S1a.p2.2.m2.1.1.2" xref="S1a.p2.2.m2.1.1.2.cmml">N</mi><mo id="S1a.p2.2.m2.1.1.1" xref="S1a.p2.2.m2.1.1.1.cmml">=</mo><mn id="S1a.p2.2.m2.1.1.3" xref="S1a.p2.2.m2.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S1a.p2.2.m2.1b"><apply id="S1a.p2.2.m2.1.1.cmml" xref="S1a.p2.2.m2.1.1"><eq id="S1a.p2.2.m2.1.1.1.cmml" xref="S1a.p2.2.m2.1.1.1"></eq><ci id="S1a.p2.2.m2.1.1.2.cmml" xref="S1a.p2.2.m2.1.1.2">ğ‘</ci><cn type="integer" id="S1a.p2.2.m2.1.1.3.cmml" xref="S1a.p2.2.m2.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1a.p2.2.m2.1c">N=100</annotation></semantics></math>, we do not observe the consistent gain. From the ablation study, we decide <math id="S1a.p2.3.m3.1" class="ltx_Math" alttext="N=100" display="inline"><semantics id="S1a.p2.3.m3.1a"><mrow id="S1a.p2.3.m3.1.1" xref="S1a.p2.3.m3.1.1.cmml"><mi id="S1a.p2.3.m3.1.1.2" xref="S1a.p2.3.m3.1.1.2.cmml">N</mi><mo id="S1a.p2.3.m3.1.1.1" xref="S1a.p2.3.m3.1.1.1.cmml">=</mo><mn id="S1a.p2.3.m3.1.1.3" xref="S1a.p2.3.m3.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S1a.p2.3.m3.1b"><apply id="S1a.p2.3.m3.1.1.cmml" xref="S1a.p2.3.m3.1.1"><eq id="S1a.p2.3.m3.1.1.1.cmml" xref="S1a.p2.3.m3.1.1.1"></eq><ci id="S1a.p2.3.m3.1.1.2.cmml" xref="S1a.p2.3.m3.1.1.2">ğ‘</ci><cn type="integer" id="S1a.p2.3.m3.1.1.3.cmml" xref="S1a.p2.3.m3.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1a.p2.3.m3.1c">N=100</annotation></semantics></math> in each image to enjoy the reasonable performance gain and to reduce the time complexity.</p>
</div>
<figure id="S1.F1a" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2204.04950/assets/images/for_appendix/Convergence_synth1.png" id="S1.F1a.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="309" height="232" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2204.04950/assets/images/for_appendix/Convergence_synth2.png" id="S1.F1a.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="309" height="232" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1a.4.2.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1a.2.1" class="ltx_text" style="font-size:90%;">FID per training iterations. The star marker (<math id="S1.F1a.2.1.m1.1" class="ltx_Math" alttext="\bigstar" display="inline"><semantics id="S1.F1a.2.1.m1.1b"><mi mathvariant="normal" id="S1.F1a.2.1.m1.1.1" xref="S1.F1a.2.1.m1.1.1.cmml">â˜…</mi><annotation-xml encoding="MathML-Content" id="S1.F1a.2.1.m1.1c"><ci id="S1.F1a.2.1.m1.1.1.cmml" xref="S1.F1a.2.1.m1.1.1">â˜…</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1a.2.1.m1.1d">\bigstar</annotation></semantics></math>) indicates the point where the model reaches 95% of the best FID score of the from scratch model with DiffAug (baseline). The legend is the same for all graphs.</span></figcaption>
</figure>
</section>
<section id="S2a" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Convergence speed of synthetic datasets</h2>

<div id="S2a.p1" class="ltx_para">
<p id="S2a.p1.1" class="ltx_p"><a href="#S1.F1a" title="In 1 Ablation Study â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">1</span></a> shows the evolution of the FID scores during the training of the models pretrained with synthetic datasets. Even if <span id="S2a.p1.1.1" class="ltx_text ltx_font_typewriter">PinkNoise</span> does not improve the generation performance, it can boost the convergence speed. In general, the pretrained models reach 95% of the best FID score of the from scratch model with DiffAug within first 30% iterations. The faster convergence speed informs us the positive potential of the pretraining.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="S3a" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Qualitative comparison among our data synthesizers</h2>

<div id="S3a.p1" class="ltx_para">
<p id="S3a.p1.1" class="ltx_p">In addition to the quantitative comparison of our data synthesizers, we also qualitatively compare our four variants of the data synthesizer used for quantitative evaluation. From the first to the last row, Bridge of sighs, Obama, Grumpy cat, and Panda. <span id="S3a.p1.1.1" class="ltx_text ltx_font_typewriter">PinkNoise</span> generates the images with unstructured samples (e.g. Obama and Grumpy cat) and the outputs of <span id="S3a.p1.1.2" class="ltx_text ltx_font_typewriter">Primitives</span> on Panda have lower fidelity (e.g. the last three samples). Compared to <span id="S3a.p1.1.3" class="ltx_text ltx_font_typewriter">PinkNoise</span> and <span id="S3a.p1.1.4" class="ltx_text ltx_font_typewriter">Primitives</span>, <span id="S3a.p1.1.5" class="ltx_text ltx_font_typewriter">Primitives-S</span> and <span id="S3a.p1.1.6" class="ltx_text ltx_font_typewriter">Primitives-PS</span> provide plausible samples. Between the last two synthetic datasets, <span id="S3a.p1.1.7" class="ltx_text ltx_font_typewriter">Primitives-S</span> sometimes drops the important factor, for example, the eyes of the cat (6-th column). While <span id="S3a.p1.1.8" class="ltx_text ltx_font_typewriter">Primitives-PS</span> generates more diverse and plausible samples than the other synthetic datasets.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S3.F2a" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2204.04950/assets/images/for_appendix/tmp_concat_qual_PinkNoise.png" id="S3.F2a.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="679" height="340" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F2a.1" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S3.F2a.1.1" class="ltx_text ltx_inline-block" style="width:491.9pt;">(a) <span id="S3.F2a.1.1.1" class="ltx_text ltx_font_typewriter">PinkNoise</span></span> 
<img src="/html/2204.04950/assets/images/for_appendix/tmp_concat_qual_Primitives.png" id="S3.F2a.1.g1" class="ltx_graphics ltx_img_landscape" width="679" height="340" alt="Refer to caption"> 
<br class="ltx_break"><span id="S3.F2a.1.2" class="ltx_text ltx_inline-block" style="width:491.9pt;">(b) <span id="S3.F2a.1.2.1" class="ltx_text ltx_font_typewriter">Primitives</span></span></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2a.5.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2a.6.2" class="ltx_text" style="font-size:90%;">Low-shot image generation results of the models transferred from <span id="S3.F2a.6.2.1" class="ltx_text ltx_font_typewriter">PinkNoise</span> and <span id="S3.F2a.6.2.2" class="ltx_text ltx_font_typewriter">Primitives</span>.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S3.F3a" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2204.04950/assets/images/for_appendix/tmp_concat_qual_PrimitivesS.png" id="S3.F3a.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="679" height="340" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F3a.1" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S3.F3a.1.1" class="ltx_text ltx_inline-block" style="width:491.9pt;">(a) <span id="S3.F3a.1.1.1" class="ltx_text ltx_font_typewriter">Primitives-S</span></span> 
<img src="/html/2204.04950/assets/images/for_appendix/tmp_concat_qual_PrimitivesPS_cut.png" id="S3.F3a.1.g1" class="ltx_graphics ltx_img_landscape" width="679" height="340" alt="Refer to caption"> 
<br class="ltx_break"><span id="S3.F3a.1.2" class="ltx_text ltx_inline-block" style="width:491.9pt;">(b) <span id="S3.F3a.1.2.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span></span></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3a.5.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3a.6.2" class="ltx_text" style="font-size:90%;">Low-shot image generation results of the models transferred from <span id="S3.F3a.6.2.1" class="ltx_text ltx_font_typewriter">Primitives-S</span> and <span id="S3.F3a.6.2.2" class="ltx_text ltx_font_typewriter">Primitives-PS</span>.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="S4a" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Convergence speed of transfer learning methods</h2>

<div id="S4a.p1" class="ltx_para">
<p id="S4a.p1.1" class="ltx_p"><a href="#S4.F4a" title="In 4 Convergence speed of transfer learning methods â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> shows the evolution of the FID scores during the training of the transfer learning methods. The model pretrained with our synthetic dataset exhibits comparable or faster convergence than the competitors that are pretrained on FFHQ. Herein, we observe the convergence speed in terms of the number of iterations to reach 95% of the best FID score of the baseline (from scratch model with DiffAug).</p>
</div>
<figure id="S4.F4a" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2204.04950/assets/images/for_appendix/Convergence_comp1.png" id="S4.F4a.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="309" height="232" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2204.04950/assets/images/for_appendix/Convergence_comp2.png" id="S4.F4a.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="309" height="232" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2204.04950/assets/images/for_appendix/Convergence_comp3.png" id="S4.F4a.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="309" height="232" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2204.04950/assets/images/for_appendix/Convergence_comp4.png" id="S4.F4a.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="309" height="232" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4a.4.2.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4a.2.1" class="ltx_text" style="font-size:90%;">The additional results of Figure 6 in the main text. FID per training iterations. The star marker (<math id="S4.F4a.2.1.m1.1" class="ltx_Math" alttext="\bigstar" display="inline"><semantics id="S4.F4a.2.1.m1.1b"><mi mathvariant="normal" id="S4.F4a.2.1.m1.1.1" xref="S4.F4a.2.1.m1.1.1.cmml">â˜…</mi><annotation-xml encoding="MathML-Content" id="S4.F4a.2.1.m1.1c"><ci id="S4.F4a.2.1.m1.1.1.cmml" xref="S4.F4a.2.1.m1.1.1">â˜…</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F4a.2.1.m1.1d">\bigstar</annotation></semantics></math>) indicates the point where the model reaches 95% of the best FID score of the from scratch model with DiffAug (baseline). The legend is the same for all graphs.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="S5a" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Qualitative comparisons with competing transfer learning methods</h2>

<div id="S5a.p1" class="ltx_para">
<p id="S5a.p1.1" class="ltx_p">In addition to the quantitative comparison, we also provide the qualitative comparisons on eight datasets that are used for quantitative evaluation in the main text. From the first to the last row, Buildings, Bridge of sighs, Obama, Medici fountain, Grumpy cat, Temple of heaven, Panda, and Wuzhen.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2204.04950/assets/images/for_appendix/tmp_concat_qual_DiffAug.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="679" height="679" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.3.2" class="ltx_text" style="font-size:90%;">The additional generated samples of Figure 5 in the main text. The images are generated with the model trained from scratch.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="S5a.p2" class="ltx_para">
<p id="S5a.p2.1" class="ltx_p">In terms of fidelity of the generated images, our <span id="S5a.p2.1.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span> outperforms the competitors. Especially, Grumpy cat images generated by the competitors often do not contain eyes or have only part of the face.</p>
<div class="ltx_pagination ltx_role_newpage"></div>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2204.04950/assets/images/for_appendix/tmp_concat_qual_TransferGAN.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="679" height="679" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.3.2" class="ltx_text" style="font-size:90%;">The additional generated samples of Figure 5 in the main text. The images are generated with the model pretrained with FFHQ and transferred by using TransferGAN.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2204.04950/assets/images/for_appendix/tmp_concat_qual_FreezeD.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="679" height="679" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S5.F7.3.2" class="ltx_text" style="font-size:90%;">The additional generated samples of Figure 5 in the main text. The images are generated with the model pretrained with FFHQ and transferred by using FreezeD.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2204.04950/assets/images/for_appendix/tmp_concat_qual_PrimitivesPS.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="679" height="679" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S5.F8.4.2" class="ltx_text" style="font-size:90%;">The additional generated samples of Figure 5 in the main text. The images are generated with the model pretrained with our <span id="S5.F8.4.2.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span>.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S5.T2.3.2" class="ltx_text" style="font-size:90%;">The additional results of Table 4 in the main text. The average consine similarity between the filters in the same layer. The lower value indicates the more diverse set of filters.</span></figcaption>
<div id="S5.T2.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:212.5pt;height:216.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-35.2pt,35.9pt) scale(0.750933569238035,0.750933569238035) ;">
<table id="S5.T2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.4.1.1" class="ltx_tr">
<td id="S5.T2.4.1.1.1" class="ltx_td ltx_border_t"></td>
<td id="S5.T2.4.1.1.2" class="ltx_td ltx_align_center ltx_border_t" colspan="2">Discriminator</td>
<td id="S5.T2.4.1.1.3" class="ltx_td ltx_align_center ltx_border_t" colspan="2">Generator</td>
</tr>
<tr id="S5.T2.4.1.2" class="ltx_tr">
<td id="S5.T2.4.1.2.1" class="ltx_td ltx_border_t"></td>
<td id="S5.T2.4.1.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.2.2.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span></td>
<td id="S5.T2.4.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FFHQ</td>
<td id="S5.T2.4.1.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.2.4.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span></td>
<td id="S5.T2.4.1.2.5" class="ltx_td ltx_align_center ltx_border_t">FFHQ</td>
</tr>
<tr id="S5.T2.4.1.3" class="ltx_tr">
<td id="S5.T2.4.1.3.1" class="ltx_td ltx_align_center ltx_border_t">conv0</td>
<td id="S5.T2.4.1.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.3.2.1" class="ltx_text ltx_font_bold">0.00660</span></td>
<td id="S5.T2.4.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.01245</td>
<td id="S5.T2.4.1.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.3.4.1" class="ltx_text ltx_font_bold">0.00315</span></td>
<td id="S5.T2.4.1.3.5" class="ltx_td ltx_align_center ltx_border_t">0.00685</td>
</tr>
<tr id="S5.T2.4.1.4" class="ltx_tr">
<td id="S5.T2.4.1.4.1" class="ltx_td ltx_align_center">conv1</td>
<td id="S5.T2.4.1.4.2" class="ltx_td ltx_align_center">0.02104</td>
<td id="S5.T2.4.1.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.4.1.4.3.1" class="ltx_text ltx_font_bold">0.00932</span></td>
<td id="S5.T2.4.1.4.4" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.4.4.1" class="ltx_text ltx_font_bold">0.00273</span></td>
<td id="S5.T2.4.1.4.5" class="ltx_td ltx_align_center">0.00843</td>
</tr>
<tr id="S5.T2.4.1.5" class="ltx_tr">
<td id="S5.T2.4.1.5.1" class="ltx_td ltx_align_center">conv2</td>
<td id="S5.T2.4.1.5.2" class="ltx_td ltx_align_center">0.01012</td>
<td id="S5.T2.4.1.5.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.4.1.5.3.1" class="ltx_text ltx_font_bold">0.00779</span></td>
<td id="S5.T2.4.1.5.4" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.5.4.1" class="ltx_text ltx_font_bold">0.00291</span></td>
<td id="S5.T2.4.1.5.5" class="ltx_td ltx_align_center">0.00956</td>
</tr>
<tr id="S5.T2.4.1.6" class="ltx_tr">
<td id="S5.T2.4.1.6.1" class="ltx_td ltx_align_center">conv3</td>
<td id="S5.T2.4.1.6.2" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.6.2.1" class="ltx_text ltx_font_bold">0.00839</span></td>
<td id="S5.T2.4.1.6.3" class="ltx_td ltx_align_center ltx_border_r">0.01216</td>
<td id="S5.T2.4.1.6.4" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.6.4.1" class="ltx_text ltx_font_bold">0.00348</span></td>
<td id="S5.T2.4.1.6.5" class="ltx_td ltx_align_center">0.01080</td>
</tr>
<tr id="S5.T2.4.1.7" class="ltx_tr">
<td id="S5.T2.4.1.7.1" class="ltx_td ltx_align_center">conv4</td>
<td id="S5.T2.4.1.7.2" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.7.2.1" class="ltx_text ltx_font_bold">0.00607</span></td>
<td id="S5.T2.4.1.7.3" class="ltx_td ltx_align_center ltx_border_r">0.00713</td>
<td id="S5.T2.4.1.7.4" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.7.4.1" class="ltx_text ltx_font_bold">0.00539</span></td>
<td id="S5.T2.4.1.7.5" class="ltx_td ltx_align_center">0.01059</td>
</tr>
<tr id="S5.T2.4.1.8" class="ltx_tr">
<td id="S5.T2.4.1.8.1" class="ltx_td ltx_align_center">conv5</td>
<td id="S5.T2.4.1.8.2" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.8.2.1" class="ltx_text ltx_font_bold">0.00596</span></td>
<td id="S5.T2.4.1.8.3" class="ltx_td ltx_align_center ltx_border_r">0.00668</td>
<td id="S5.T2.4.1.8.4" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.8.4.1" class="ltx_text ltx_font_bold">0.00329</span></td>
<td id="S5.T2.4.1.8.5" class="ltx_td ltx_align_center">0.01406</td>
</tr>
<tr id="S5.T2.4.1.9" class="ltx_tr">
<td id="S5.T2.4.1.9.1" class="ltx_td ltx_align_center">conv6</td>
<td id="S5.T2.4.1.9.2" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.9.2.1" class="ltx_text ltx_font_bold">0.00507</span></td>
<td id="S5.T2.4.1.9.3" class="ltx_td ltx_align_center ltx_border_r">0.00563</td>
<td id="S5.T2.4.1.9.4" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.9.4.1" class="ltx_text ltx_font_bold">0.00363</span></td>
<td id="S5.T2.4.1.9.5" class="ltx_td ltx_align_center">0.01199</td>
</tr>
<tr id="S5.T2.4.1.10" class="ltx_tr">
<td id="S5.T2.4.1.10.1" class="ltx_td ltx_align_center">conv7</td>
<td id="S5.T2.4.1.10.2" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.10.2.1" class="ltx_text ltx_font_bold">0.00632</span></td>
<td id="S5.T2.4.1.10.3" class="ltx_td ltx_align_center ltx_border_r">0.00714</td>
<td id="S5.T2.4.1.10.4" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.10.4.1" class="ltx_text ltx_font_bold">0.00433</span></td>
<td id="S5.T2.4.1.10.5" class="ltx_td ltx_align_center">0.01465</td>
</tr>
<tr id="S5.T2.4.1.11" class="ltx_tr">
<td id="S5.T2.4.1.11.1" class="ltx_td ltx_align_center">conv8</td>
<td id="S5.T2.4.1.11.2" class="ltx_td ltx_align_center">0.00380</td>
<td id="S5.T2.4.1.11.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.4.1.11.3.1" class="ltx_text ltx_font_bold">0.00365</span></td>
<td id="S5.T2.4.1.11.4" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.11.4.1" class="ltx_text ltx_font_bold">0.00652</span></td>
<td id="S5.T2.4.1.11.5" class="ltx_td ltx_align_center">0.01317</td>
</tr>
<tr id="S5.T2.4.1.12" class="ltx_tr">
<td id="S5.T2.4.1.12.1" class="ltx_td ltx_align_center">conv9</td>
<td id="S5.T2.4.1.12.2" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.12.2.1" class="ltx_text ltx_font_bold">0.00521</span></td>
<td id="S5.T2.4.1.12.3" class="ltx_td ltx_align_center ltx_border_r">0.00703</td>
<td id="S5.T2.4.1.12.4" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.12.4.1" class="ltx_text ltx_font_bold">0.00933</span></td>
<td id="S5.T2.4.1.12.5" class="ltx_td ltx_align_center">0.01626</td>
</tr>
<tr id="S5.T2.4.1.13" class="ltx_tr">
<td id="S5.T2.4.1.13.1" class="ltx_td ltx_align_center">conv10</td>
<td id="S5.T2.4.1.13.2" class="ltx_td ltx_align_center">0.00503</td>
<td id="S5.T2.4.1.13.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.4.1.13.3.1" class="ltx_text ltx_font_bold">0.00420</span></td>
<td id="S5.T2.4.1.13.4" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.13.4.1" class="ltx_text ltx_font_bold">0.01133</span></td>
<td id="S5.T2.4.1.13.5" class="ltx_td ltx_align_center">0.01778</td>
</tr>
<tr id="S5.T2.4.1.14" class="ltx_tr">
<td id="S5.T2.4.1.14.1" class="ltx_td ltx_align_center">conv11</td>
<td id="S5.T2.4.1.14.2" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.14.2.1" class="ltx_text ltx_font_bold">0.00462</span></td>
<td id="S5.T2.4.1.14.3" class="ltx_td ltx_align_center ltx_border_r">0.00760</td>
<td id="S5.T2.4.1.14.4" class="ltx_td ltx_align_center">0.01981</td>
<td id="S5.T2.4.1.14.5" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.14.5.1" class="ltx_text ltx_font_bold">0.01977</span></td>
</tr>
<tr id="S5.T2.4.1.15" class="ltx_tr">
<td id="S5.T2.4.1.15.1" class="ltx_td ltx_align_center">conv12</td>
<td id="S5.T2.4.1.15.2" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.15.2.1" class="ltx_text ltx_font_bold">0.01844</span></td>
<td id="S5.T2.4.1.15.3" class="ltx_td ltx_align_center ltx_border_r">0.08438</td>
<td id="S5.T2.4.1.15.4" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.15.4.1" class="ltx_text ltx_font_bold">0.03176</span></td>
<td id="S5.T2.4.1.15.5" class="ltx_td ltx_align_center">0.03250</td>
</tr>
<tr id="S5.T2.4.1.16" class="ltx_tr">
<td id="S5.T2.4.1.16.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">Mean</td>
<td id="S5.T2.4.1.16.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T2.4.1.16.2.1" class="ltx_text ltx_font_bold">0.00820</span></td>
<td id="S5.T2.4.1.16.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.01348</td>
<td id="S5.T2.4.1.16.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S5.T2.4.1.16.4.1" class="ltx_text ltx_font_bold">0.00828</span></td>
<td id="S5.T2.4.1.16.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.01434</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Similarity between filters in all layers</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We calculated the cosine similarity in each layer to measure the diversity of learned filters of pretrained models. FFHQ pretrained model exhibits lower diversity in filters. The average similarity at the last layer of FFHQ pretrained model is approximately four times higher than <span id="S6.p1.1.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span>. The similar tendency is shown in the first layer of each network â€“ the consine similarity of FFHQ pretrained model is about two times higher than <span id="S6.p1.1.2" class="ltx_text ltx_font_typewriter">Primitives-PS</span>.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S6.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T3.2.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S6.T3.3.2" class="ltx_text" style="font-size:90%;">Membership inference performance on the source dataset by attacking a transferred classifier as reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>.
</span></figcaption>
<div id="S6.T3.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:173.4pt;height:51pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-35.7pt,10.5pt) scale(0.708595500361338,0.708595500361338) ;">
<table id="S6.T3.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T3.4.1.1" class="ltx_tr">
<td id="S6.T3.4.1.1.1" class="ltx_td ltx_align_center ltx_border_t">Dataset</td>
<td id="S6.T3.4.1.1.2" class="ltx_td ltx_align_center ltx_border_t">AUC</td>
<td id="S6.T3.4.1.1.3" class="ltx_td ltx_align_center ltx_border_t">Accuracy</td>
<td id="S6.T3.4.1.1.4" class="ltx_td ltx_align_center ltx_border_t">Precision</td>
<td id="S6.T3.4.1.1.5" class="ltx_td ltx_align_center ltx_border_t">Recall</td>
</tr>
<tr id="S6.T3.4.1.2" class="ltx_tr">
<td id="S6.T3.4.1.2.1" class="ltx_td ltx_align_center ltx_border_t">CIFAR100</td>
<td id="S6.T3.4.1.2.2" class="ltx_td ltx_align_center ltx_border_t">0.522</td>
<td id="S6.T3.4.1.2.3" class="ltx_td ltx_align_center ltx_border_t">0.502</td>
<td id="S6.T3.4.1.2.4" class="ltx_td ltx_align_center ltx_border_t">0.478</td>
<td id="S6.T3.4.1.2.5" class="ltx_td ltx_align_center ltx_border_t">0.523</td>
</tr>
<tr id="S6.T3.4.1.3" class="ltx_tr">
<td id="S6.T3.4.1.3.1" class="ltx_td ltx_align_center">Flowers102</td>
<td id="S6.T3.4.1.3.2" class="ltx_td ltx_align_center">0.528</td>
<td id="S6.T3.4.1.3.3" class="ltx_td ltx_align_center">0.496</td>
<td id="S6.T3.4.1.3.4" class="ltx_td ltx_align_center">0.432</td>
<td id="S6.T3.4.1.3.5" class="ltx_td ltx_align_center">0.505</td>
</tr>
<tr id="S6.T3.4.1.4" class="ltx_tr">
<td id="S6.T3.4.1.4.1" class="ltx_td ltx_align_center ltx_border_b">PubFig83</td>
<td id="S6.T3.4.1.4.2" class="ltx_td ltx_align_center ltx_border_b">0.495</td>
<td id="S6.T3.4.1.4.3" class="ltx_td ltx_align_center ltx_border_b">0.481</td>
<td id="S6.T3.4.1.4.4" class="ltx_td ltx_align_center ltx_border_b">0.396</td>
<td id="S6.T3.4.1.4.5" class="ltx_td ltx_align_center ltx_border_b">0.524</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Copyright issue and vulnerability of pretrained model</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">When we directly finetune a pretrained model for commercial use, the trained weights of the model might be defined as software and have the CC BY-ND (creative commons license without modification) license. In this case, we can not utilize the model with post-training or should pay the license fee for the model as software. If we want to use the images for non-commercial purposes, we should acquire the credit of each image from the original author. For ImageNet-1K having 1M images, the copyright issue might not be feasible to handle. When targeting the commercial use of a dataset, the developer should negotiate with the author of each sample. Since this process requires much time and cost to complete, it is likely to be an obstacle to the practical usage of the deep learning system.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">Even if we solve the copyright issue via negotiation, the leakage of the training data is another problem. Following the recent workÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>, the source dataset for pretraining a model can be exposed by the membership inference attack even after the transfer learning. <a href="#S6.T3" title="In 6 Similarity between filters in all layers â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">3</span></a> shows the empirical evidence. The target models are first pretrained on Caltech101 and transferred to three datasets. The higher AUC, the higher accuracy of the membership inference on the source dataset. Although the accuracy is lower than the attack on the target dataset, it warns us to consider the membership inference attack towards the source dataset seriously.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S7.F9" class="ltx_figure"><img src="/html/2204.04950/assets/images/for_appendix/cifar10_diffaug_10p_leak.jpg" id="S7.F9.g1" class="ltx_graphics ltx_centering ltx_img_square" width="296" height="296" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S7.F9.3.2" class="ltx_text" style="font-size:90%;">Examples of the leakage when using DiffAug. The gray box in some images shows the leakage of cutout operation.</span></figcaption>
</figure>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Experimental results on CIFAR</h2>

<section id="S8.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1 </span>Data augmentation leakage</h3>

<div id="S8.SS1.p1" class="ltx_para">
<p id="S8.SS1.p1.1" class="ltx_p">The previous workÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> reported the ill-behavior of the data augmentation in GANs; augmentation leakage. When the leakage incurs, the unwanted data transformation is reflected in the generated results. For example, the generated images contain cutout augmentation so that some of the fakes have unwanted empty box. When we train BigGAN on CIFAR with 10% of samples using DiffAug only, we observe that augmentation leakage. Although the leakage is found, the FID score decreases; FID scores can not reflect the problem of leakage. To penalize this unwanted result, we qualitatively exclude the model with leakage when we find the best model. <a href="#S7.F9" title="In 7 Copyright issue and vulnerability of pretrained model â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">9</span></a> shows the generated images by the model trained with DiffAug (FID: 22.54). Many of the outputs have the unwanted gray box that is the result of leakage of the cutout operation, and this is why we exclude the corresponding FID score in Table 5 of the main text.</p>
</div>
<div id="S8.SS1.p2" class="ltx_para">
<p id="S8.SS1.p2.1" class="ltx_p">On the contrary, the model pretrained with <span id="S8.SS1.p2.1.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span> does not suffer from the leakage even if we use DiffAug (<a href="#S8.F10" title="In 8.1 Data augmentation leakage â€£ 8 Experimental results on CIFAR â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">10</span></a>). It shows that our pretraining dataset is also effective to prevent augmentation leakage and improves the final generation quality.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S8.F10" class="ltx_figure"><img src="/html/2204.04950/assets/images/for_appendix/cifar10_ours_10p.jpg" id="S8.F10.g1" class="ltx_graphics ltx_centering ltx_img_square" width="296" height="296" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F10.2.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S8.F10.3.2" class="ltx_text" style="font-size:90%;">Outputs of the model transferred from our model on CIFAR-10. The model does not suffer from augmentation leakage although we use DiffAug.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Pretraining results and details</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">In this section, we provide the outputs of the generator pretrained with <span id="S9.p1.1.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span>. For pretraining, we train the model during 800K images with batch size = 16, therefore, the total number of iterations is 50K. For finetuing all the models, we train the model during 400K images. The generated (fake) synthetic images are similar to the real synthetic samples as shown in Figure 1 of the main text.</p>
</div>
<figure id="S9.F11" class="ltx_figure"><img src="/html/2204.04950/assets/images/for_appendix/PrimitivesPS-fakes-cut.png" id="S9.F11.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="296" height="592" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S9.F11.3.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S9.F11.4.2" class="ltx_text" style="font-size:90%;">The outputs of the model pretrained with <span id="S9.F11.4.2.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span>. The generated outputs are similar to the synthetic samples.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="S10" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>Frequency domain analysis</h2>

<div id="S10.p1" class="ltx_para">
<p id="S10.p1.1" class="ltx_p">We visualize the average magnitude spectrum of all the samples in Bridge of sighs and compare with the average magnitude spectrum of 1000 images generated by <span id="S10.p1.1.1" class="ltx_text ltx_font_typewriter">PinkNoise</span> and 1000 images generated by <span id="S10.p1.1.2" class="ltx_text ltx_font_typewriter">Primitives</span>. The figure below demonstrates their magnitude spectrum. We observe that <span id="S10.p1.1.3" class="ltx_text ltx_font_typewriter">Primitives</span> produces images that have a similar magnitude spectrum to those of natural images.</p>
</div>
<figure id="S10.F12" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2204.04950/assets/images/bridge_mag.png" id="S10.F12.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="96" height="97" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2204.04950/assets/images/pink_noise_mag.png" id="S10.F12.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="96" height="97" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2204.04950/assets/images/primitives_mag.png" id="S10.F12.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="96" height="97" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S10.F12.2" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S10.F12.2.1" class="ltx_text ltx_inline-block" style="width:69.4pt;">(a)</span>
<span id="S10.F12.2.2" class="ltx_text ltx_inline-block" style="width:69.4pt;">(b)</span>
<span id="S10.F12.2.3" class="ltx_text ltx_inline-block" style="width:69.4pt;">(c)</span></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S10.F12.8.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="S10.F12.9.2" class="ltx_text" style="font-size:90%;">The magnitude spectrum of (a) Bridge, (b) <span id="S10.F12.9.2.1" class="ltx_text ltx_font_typewriter">PinkNoise</span>, and (c) <span id="S10.F12.9.2.2" class="ltx_text ltx_font_typewriter">Primitives</span>. We apply FFT on each image and then visualize the average magnitude of the images. When we visualize, we take a logarithmic transformation. Although <span id="S10.F12.9.2.3" class="ltx_text ltx_font_typewriter">PinkNoise</span> aims to mimic the magnitude spectrum of natural images, that of <span id="S10.F12.9.2.4" class="ltx_text ltx_font_typewriter">Primitives</span> approximates the benchmark dataset better than that of <span id="S10.F12.9.2.5" class="ltx_text ltx_font_typewriter">PinkNoise</span>.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S10.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S10.T4.3.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S10.T4.4.2" class="ltx_text" style="font-size:80%;">KMMD score for <span id="S10.T4.4.2.1" class="ltx_text ltx_font_bold">Table 3 in the main text (256)</span>.</span></figcaption>
<div id="S10.T4.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:52.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-77.4pt,18.8pt) scale(0.583278921071556,0.583278921071556) ;">
<table id="S10.T4.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S10.T4.5.1.1" class="ltx_tr">
<td id="S10.T4.5.1.1.1" class="ltx_td ltx_border_t"></td>
<td id="S10.T4.5.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Obama</td>
<td id="S10.T4.5.1.1.3" class="ltx_td ltx_align_center ltx_border_t">Cat</td>
<td id="S10.T4.5.1.1.4" class="ltx_td ltx_align_center ltx_border_t">Brid.</td>
<td id="S10.T4.5.1.1.5" class="ltx_td ltx_align_center ltx_border_t">Panda</td>
<td id="S10.T4.5.1.1.6" class="ltx_td ltx_align_center ltx_border_t">Temp.</td>
<td id="S10.T4.5.1.1.7" class="ltx_td ltx_align_center ltx_border_t">Wuzhen</td>
<td id="S10.T4.5.1.1.8" class="ltx_td ltx_align_center ltx_border_t">Fountain</td>
<td id="S10.T4.5.1.1.9" class="ltx_td ltx_align_center ltx_border_t">Build.</td>
</tr>
<tr id="S10.T4.5.1.2" class="ltx_tr">
<td id="S10.T4.5.1.2.1" class="ltx_td ltx_align_left ltx_border_t">DfAug</td>
<td id="S10.T4.5.1.2.2" class="ltx_td ltx_align_center ltx_border_t">0.23</td>
<td id="S10.T4.5.1.2.3" class="ltx_td ltx_align_center ltx_border_t">0.15</td>
<td id="S10.T4.5.1.2.4" class="ltx_td ltx_align_center ltx_border_t">0.23</td>
<td id="S10.T4.5.1.2.5" class="ltx_td ltx_align_center ltx_border_t">0.28</td>
<td id="S10.T4.5.1.2.6" class="ltx_td ltx_align_center ltx_border_t">0.18</td>
<td id="S10.T4.5.1.2.7" class="ltx_td ltx_align_center ltx_border_t">0.39</td>
<td id="S10.T4.5.1.2.8" class="ltx_td ltx_align_center ltx_border_t">0.21</td>
<td id="S10.T4.5.1.2.9" class="ltx_td ltx_align_center ltx_border_t">0.21</td>
</tr>
<tr id="S10.T4.5.1.3" class="ltx_tr">
<td id="S10.T4.5.1.3.1" class="ltx_td ltx_align_left">TGAN</td>
<td id="S10.T4.5.1.3.2" class="ltx_td ltx_align_center">0.13</td>
<td id="S10.T4.5.1.3.3" class="ltx_td ltx_align_center">0.14</td>
<td id="S10.T4.5.1.3.4" class="ltx_td ltx_align_center">0.22</td>
<td id="S10.T4.5.1.3.5" class="ltx_td ltx_align_center">0.21</td>
<td id="S10.T4.5.1.3.6" class="ltx_td ltx_align_center">0.14</td>
<td id="S10.T4.5.1.3.7" class="ltx_td ltx_align_center">0.27</td>
<td id="S10.T4.5.1.3.8" class="ltx_td ltx_align_center">0.19</td>
<td id="S10.T4.5.1.3.9" class="ltx_td ltx_align_center">0.18</td>
</tr>
<tr id="S10.T4.5.1.4" class="ltx_tr">
<td id="S10.T4.5.1.4.1" class="ltx_td ltx_align_left">FrzD</td>
<td id="S10.T4.5.1.4.2" class="ltx_td ltx_align_center"><span id="S10.T4.5.1.4.2.1" class="ltx_text ltx_font_bold">0.12</span></td>
<td id="S10.T4.5.1.4.3" class="ltx_td ltx_align_center"><span id="S10.T4.5.1.4.3.1" class="ltx_text ltx_font_bold">0.14</span></td>
<td id="S10.T4.5.1.4.4" class="ltx_td ltx_align_center">0.22</td>
<td id="S10.T4.5.1.4.5" class="ltx_td ltx_align_center"><span id="S10.T4.5.1.4.5.1" class="ltx_text ltx_font_bold">0.18</span></td>
<td id="S10.T4.5.1.4.6" class="ltx_td ltx_align_center"><span id="S10.T4.5.1.4.6.1" class="ltx_text ltx_font_bold">0.13</span></td>
<td id="S10.T4.5.1.4.7" class="ltx_td ltx_align_center">0.25</td>
<td id="S10.T4.5.1.4.8" class="ltx_td ltx_align_center">0.21</td>
<td id="S10.T4.5.1.4.9" class="ltx_td ltx_align_center"><span id="S10.T4.5.1.4.9.1" class="ltx_text ltx_font_bold">0.16</span></td>
</tr>
<tr id="S10.T4.5.1.5" class="ltx_tr">
<td id="S10.T4.5.1.5.1" class="ltx_td ltx_align_left ltx_border_b">Ours</td>
<td id="S10.T4.5.1.5.2" class="ltx_td ltx_align_center ltx_border_b">0.17</td>
<td id="S10.T4.5.1.5.3" class="ltx_td ltx_align_center ltx_border_b">0.15</td>
<td id="S10.T4.5.1.5.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S10.T4.5.1.5.4.1" class="ltx_text ltx_font_bold">0.17</span></td>
<td id="S10.T4.5.1.5.5" class="ltx_td ltx_align_center ltx_border_b">0.26</td>
<td id="S10.T4.5.1.5.6" class="ltx_td ltx_align_center ltx_border_b">0.14</td>
<td id="S10.T4.5.1.5.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S10.T4.5.1.5.7.1" class="ltx_text ltx_font_bold">0.25</span></td>
<td id="S10.T4.5.1.5.8" class="ltx_td ltx_align_center ltx_border_b"><span id="S10.T4.5.1.5.8.1" class="ltx_text ltx_font_bold">0.17</span></td>
<td id="S10.T4.5.1.5.9" class="ltx_td ltx_align_center ltx_border_b">0.18</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S11" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">11 </span>Kernel Maximum Mean Discrepancy (KMMD)</h2>

<div id="S11.p1" class="ltx_para">
<p id="S11.p1.1" class="ltx_p">Quantitative evaluation with various metrics is helpful to compare the models and understand the aspect. To this end, we also provide KMMD as suggested by Reviewer 1 in the rebuttal. We report FID only in the main text because of the following reason. In Figure 4(a) of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, KMMD considers â€œ<span id="S11.p1.1.1" class="ltx_text ltx_font_italic">scale&amp;shift</span>â€ as the best model although â€œ<span id="S11.p1.1.2" class="ltx_text ltx_font_italic">Ours</span>â€ provides more plausible results; â€œ<span id="S11.p1.1.3" class="ltx_text ltx_font_italic">scale&amp;shift</span>â€ even failed to produce eye, nose, and mouth. Contrarily, FID ranked â€œ<span id="S11.p1.1.4" class="ltx_text ltx_font_italic">Ours</span>â€ as the best, correctly reflecting the perceptual fidelity.
TableÂ <a href="#S10.T4" title="Table 4 â€£ 10 Frequency domain analysis â€£ Commonality in Natural Images Rescues GANs: Pretraining GANs with Generic and Privacy-free Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the KMMD score of each model. Although the rankings with KMMD are slightly different from those with FID, our method similarly performs or outperforms the baselines. Overall, we conclude that <span id="S11.p1.1.5" class="ltx_text ltx_font_typewriter">Primitives-PS</span> is still effective for pretraining GANs.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S11.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S11.T5.4.2.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S11.T5.2.1" class="ltx_text" style="font-size:80%;">FID score of <span id="S11.T5.2.1.1" class="ltx_text ltx_font_italic">ImageNet pretrained</span> model and <span id="S11.T5.2.1.2" class="ltx_text ltx_font_typewriter">Primitives-PS</span> pretrained model on 512<math id="S11.T5.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S11.T5.2.1.m1.1b"><mo id="S11.T5.2.1.m1.1.1" xref="S11.T5.2.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S11.T5.2.1.m1.1c"><times id="S11.T5.2.1.m1.1.1.cmml" xref="S11.T5.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S11.T5.2.1.m1.1d">\times</annotation></semantics></math>512.</span></figcaption>
<div id="S11.T5.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:52.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-77.8pt,18.8pt) scale(0.582234664804459,0.582234664804459) ;">
<table id="S11.T5.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S11.T5.5.1.1" class="ltx_tr">
<td id="S11.T5.5.1.1.1" class="ltx_td ltx_border_t"></td>
<td id="S11.T5.5.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Obama</td>
<td id="S11.T5.5.1.1.3" class="ltx_td ltx_align_center ltx_border_t">Cat</td>
<td id="S11.T5.5.1.1.4" class="ltx_td ltx_align_center ltx_border_t">Brid.</td>
<td id="S11.T5.5.1.1.5" class="ltx_td ltx_align_center ltx_border_t">Panda</td>
<td id="S11.T5.5.1.1.6" class="ltx_td ltx_align_center ltx_border_t">Temp.</td>
<td id="S11.T5.5.1.1.7" class="ltx_td ltx_align_center ltx_border_t">Wuzhen</td>
<td id="S11.T5.5.1.1.8" class="ltx_td ltx_align_center ltx_border_t">Fountain</td>
<td id="S11.T5.5.1.1.9" class="ltx_td ltx_align_center ltx_border_t">Build.</td>
</tr>
<tr id="S11.T5.5.1.2" class="ltx_tr">
<td id="S11.T5.5.1.2.1" class="ltx_td ltx_align_left ltx_border_t">DfAug</td>
<td id="S11.T5.5.1.2.2" class="ltx_td ltx_align_center ltx_border_t">59.6</td>
<td id="S11.T5.5.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S11.T5.5.1.2.3.1" class="ltx_text ltx_framed ltx_framed_underline">28.0</span></td>
<td id="S11.T5.5.1.2.4" class="ltx_td ltx_align_center ltx_border_t">147.8</td>
<td id="S11.T5.5.1.2.5" class="ltx_td ltx_align_center ltx_border_t">14.4</td>
<td id="S11.T5.5.1.2.6" class="ltx_td ltx_align_center ltx_border_t">45.0</td>
<td id="S11.T5.5.1.2.7" class="ltx_td ltx_align_center ltx_border_t">150.9</td>
<td id="S11.T5.5.1.2.8" class="ltx_td ltx_align_center ltx_border_t">214.2</td>
<td id="S11.T5.5.1.2.9" class="ltx_td ltx_align_center ltx_border_t">99.2</td>
</tr>
<tr id="S11.T5.5.1.3" class="ltx_tr">
<td id="S11.T5.5.1.3.1" class="ltx_td ltx_align_left">TGAN</td>
<td id="S11.T5.5.1.3.2" class="ltx_td ltx_align_center"><span id="S11.T5.5.1.3.2.1" class="ltx_text ltx_font_bold">37.5</span></td>
<td id="S11.T5.5.1.3.3" class="ltx_td ltx_align_center">35.2</td>
<td id="S11.T5.5.1.3.4" class="ltx_td ltx_align_center">52.0</td>
<td id="S11.T5.5.1.3.5" class="ltx_td ltx_align_center"><span id="S11.T5.5.1.3.5.1" class="ltx_text ltx_framed ltx_framed_underline">11.8</span></td>
<td id="S11.T5.5.1.3.6" class="ltx_td ltx_align_center">42.5</td>
<td id="S11.T5.5.1.3.7" class="ltx_td ltx_align_center">84.1</td>
<td id="S11.T5.5.1.3.8" class="ltx_td ltx_align_center">284.3</td>
<td id="S11.T5.5.1.3.9" class="ltx_td ltx_align_center">65.5</td>
</tr>
<tr id="S11.T5.5.1.4" class="ltx_tr">
<td id="S11.T5.5.1.4.1" class="ltx_td ltx_align_left">FrzD</td>
<td id="S11.T5.5.1.4.2" class="ltx_td ltx_align_center"><span id="S11.T5.5.1.4.2.1" class="ltx_text ltx_framed ltx_framed_underline">39.1</span></td>
<td id="S11.T5.5.1.4.3" class="ltx_td ltx_align_center">28.8</td>
<td id="S11.T5.5.1.4.4" class="ltx_td ltx_align_center"><span id="S11.T5.5.1.4.4.1" class="ltx_text ltx_font_bold">48.6</span></td>
<td id="S11.T5.5.1.4.5" class="ltx_td ltx_align_center"><span id="S11.T5.5.1.4.5.1" class="ltx_text ltx_font_bold">11.2</span></td>
<td id="S11.T5.5.1.4.6" class="ltx_td ltx_align_center"><span id="S11.T5.5.1.4.6.1" class="ltx_text ltx_font_bold">38.9</span></td>
<td id="S11.T5.5.1.4.7" class="ltx_td ltx_align_center"><span id="S11.T5.5.1.4.7.1" class="ltx_text ltx_font_bold">69.5</span></td>
<td id="S11.T5.5.1.4.8" class="ltx_td ltx_align_center"><span id="S11.T5.5.1.4.8.1" class="ltx_text ltx_font_bold">34.3</span></td>
<td id="S11.T5.5.1.4.9" class="ltx_td ltx_align_center"><span id="S11.T5.5.1.4.9.1" class="ltx_text ltx_font_bold">60.2</span></td>
</tr>
<tr id="S11.T5.5.1.5" class="ltx_tr">
<td id="S11.T5.5.1.5.1" class="ltx_td ltx_align_left ltx_border_b">Ours</td>
<td id="S11.T5.5.1.5.2" class="ltx_td ltx_align_center ltx_border_b">50.8</td>
<td id="S11.T5.5.1.5.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S11.T5.5.1.5.3.1" class="ltx_text ltx_font_bold">27.7</span></td>
<td id="S11.T5.5.1.5.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S11.T5.5.1.5.4.1" class="ltx_text ltx_framed ltx_framed_underline">51.6</span></td>
<td id="S11.T5.5.1.5.5" class="ltx_td ltx_align_center ltx_border_b">14.9</td>
<td id="S11.T5.5.1.5.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S11.T5.5.1.5.6.1" class="ltx_text ltx_framed ltx_framed_underline">41.9</span></td>
<td id="S11.T5.5.1.5.7" class="ltx_td ltx_align_center ltx_border_b"><span id="S11.T5.5.1.5.7.1" class="ltx_text ltx_framed ltx_framed_underline">81.6</span></td>
<td id="S11.T5.5.1.5.8" class="ltx_td ltx_align_center ltx_border_b"><span id="S11.T5.5.1.5.8.1" class="ltx_text ltx_framed ltx_framed_underline">42.9</span></td>
<td id="S11.T5.5.1.5.9" class="ltx_td ltx_align_center ltx_border_b"><span id="S11.T5.5.1.5.9.1" class="ltx_text ltx_framed ltx_framed_underline">80.9</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S12" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">12 </span>Scale-up to higher resolution and comparison with ImageNet</h2>

<div id="S12.p1" class="ltx_para">
<p id="S12.p1.2" class="ltx_p">To check the effectiveness of <span id="S12.p1.2.1" class="ltx_text ltx_font_typewriter">Primitives-PS</span> in the higher resolution, we pretrain StyleGAN2 with <span id="S12.p1.2.2" class="ltx_text ltx_font_typewriter">Primitives-PS</span> on 512<math id="S12.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S12.p1.1.m1.1a"><mo id="S12.p1.1.m1.1.1" xref="S12.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S12.p1.1.m1.1b"><times id="S12.p1.1.m1.1.1.cmml" xref="S12.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S12.p1.1.m1.1c">\times</annotation></semantics></math>512, and then transfer to the low-shot datasets. Moreover, we use the ImageNet pretrained model for all competitors to investigate the effect of a diverse and large-scale training dataset. The pretrained file is from the <a target="_blank" href="https://twitter.com/theshawwn/status/1244081581347598341" title="" class="ltx_ref ltx_href">link</a>. We note that this model is pretrained on the 512<math id="S12.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S12.p1.2.m2.1a"><mo id="S12.p1.2.m2.1.1" xref="S12.p1.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S12.p1.2.m2.1b"><times id="S12.p1.2.m2.1.1.cmml" xref="S12.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S12.p1.2.m2.1c">\times</annotation></semantics></math>512 ImageNet until 1.3M steps. Since the ImageNet dataset can be considered as a super-set of eight test categories, the best performance using the ImageNet pretrained model is often better than <span id="S12.p1.2.3" class="ltx_text ltx_font_typewriter">Primitives-PS</span> pretrained model. However, when the category of test set no longer overlaps with the ImageNet, we argue that only <span id="S12.p1.2.4" class="ltx_text ltx_font_typewriter">Primitives-PS</span> can provide consistent and meaningful performances, <em id="S12.p1.2.5" class="ltx_emph ltx_font_italic">e.g., medical images for diagnoses, microscopic images for gene analysis or space imaging for navigation</em>. Besides, the pretrained model with the 1M ImageNet dataset is vulnerable to the private and copyright issue. A number of images contain a person and the copyright of each image might not be free to all the users. For these practical issues related to legality, the proposed <span id="S12.p1.2.6" class="ltx_text ltx_font_typewriter">Primitives-PS</span> provides huge benefits for pretraining of GANs.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2204.04948" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2204.04950" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2204.04950">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2204.04950" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2204.04951" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 12:06:15 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
