<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>An Enhanced Batch Query Architecture in Real-time Recommendation</title>
<!--Generated on Sat Aug 31 09:03:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Key-value storage,  Recommender system,  Hash-table" lang="en" name="keywords"/>
<base href="/html/2409.00400v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#S1" title="In An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#S2" title="In An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>System Design</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#S2.SS1" title="In 2. System Design ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>NeighborHash</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#S2.SS1.SSS1" title="In 2.1. NeighborHash ‣ 2. System Design ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Hashtable structure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#S2.SS1.SSS2" title="In 2.1. NeighborHash ‣ 2. System Design ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>NVMe storage</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#S2.SS2" title="In 2. System Design ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Batch Query Subsystem</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#S2.SS2.SSS1" title="In 2.2. Batch Query Subsystem ‣ 2. System Design ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Automatic Sharding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#S2.SS2.SSS2" title="In 2.2. Batch Query Subsystem ‣ 2. System Design ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Rolling Update and Query Consistency</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#S3" title="In An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiment and Result</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#S3.SS1" title="In 3. Experiment and Result ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Evaluation of NeighborHash</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#S3.SS1.SSS1" title="In 3.1. Evaluation of NeighborHash ‣ 3. Experiment and Result ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Experiments Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#S3.SS1.SSS2" title="In 3.1. Evaluation of NeighborHash ‣ 3. Experiment and Result ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Scalar Hash-Tables Comparsion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#S3.SS2" title="In 3. Experiment and Result ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Benefits in Industrial Scenario</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#S4" title="In An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#S5" title="In An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">An Enhanced Batch Query Architecture in Real-time Recommendation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qiang Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0005-7590-3567" title="ORCID identifier">0009-0005-7590-3567</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Xi’an Jiaotong University</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Xi’an</span><span class="ltx_text ltx_affiliation_state" id="id3.3.id3">Shaanxi</span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zhangqiang@stu.xjtu.edu.cn">zhangqiang@stu.xjtu.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhipeng Teng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0000-5175-6762" title="ORCID identifier">0009-0000-5175-6762</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id5.1.id1">Bilibili</span><span class="ltx_text ltx_affiliation_city" id="id6.2.id2">Shanghai</span><span class="ltx_text ltx_affiliation_country" id="id7.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:tengzhipeng@bilibili.com">tengzhipeng@bilibili.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Disheng Wu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0008-8278-0093" title="ORCID identifier">0009-0008-8278-0093</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id8.1.id1">Bilibili</span><span class="ltx_text ltx_affiliation_city" id="id9.2.id2">Shanghai</span><span class="ltx_text ltx_affiliation_country" id="id10.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:wudisheng@bilibili.com">wudisheng@bilibili.com</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiayin Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-3862-6557" title="ORCID identifier">0000-0002-3862-6557</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id11.1.id1">Xi’an Jiaotong University</span><span class="ltx_text ltx_affiliation_city" id="id12.2.id2">Xi’an</span><span class="ltx_text ltx_affiliation_state" id="id13.3.id3">Shaanxi</span><span class="ltx_text ltx_affiliation_country" id="id14.4.id4">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:wangjiayin@mail.xjtu.edu.cn">wangjiayin@mail.xjtu.edu.cn</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id15.id1">In industrial recommendation systems on websites and apps, it is essential to recall and predict top-n results relevant to user interests from a content pool of billions within milliseconds. To cope with continuous data growth and improve real-time recommendation performance, we have designed and implemented a high-performance batch query architecture for real-time recommendation systems. Our contributions include optimizing hash structures with a cacheline-aware probing method to enhance coalesced hashing, as well as the implementation of a hybrid storage key-value service built upon it. Our experiments indicate this approach significantly surpasses conventional hash tables in batch query throughput, achieving up to 90% of the query throughput of random memory access when incorporating parallel optimization. The support for NVMe, integrating two-tier storage for hot and cold data, notably reduces resource consumption. Additionally, the system facilitates dynamic updates, automated sharding of attributes and feature embedding tables, and introduces innovative protocols for consistency in batch queries, thereby enhancing the effectiveness of real-time incremental learning updates. This architecture has been deployed and in use in the bilibili recommendation system for over a year, a video content community with hundreds of millions of users, supporting 10x increase in model computation with minimal resource growth, improving outcomes while preserving the system’s real-time performance.</p>
</div>
<div class="ltx_keywords">Key-value storage, Recommender system, Hash-table
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Proceedings of the 33rd ACM International Conference on Information and Knowledge Management; October 21–25, 2024; Boise, ID, USA</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>Proceedings of the 33rd ACM International Conference on Information and Knowledge Management (CIKM ’24), October 21–25, 2024, Boise, ID, USA</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3627673.3680034</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0436-9/24/10</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computer systems organization Real-time system architecture</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Recommender systems</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id9"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Distributed storage</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recommendation systems, the rapid computation of item relevance to users—ideally within milliseconds is crucial for optimal performance, and storage systems capable of managing vast data volumes and facilitating high-speed batch queries are one of the most vital components, as illustrated in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.00400v1#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>. Whether in the recall or ranking phases, or during offline model training <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib22" title="">2014</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib23" title="">2013</a>)</cite>, more features and faster computation generally lead to better recommendation outcomes. Redis <cite class="ltx_cite ltx_citemacro_citep">(redis, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib30" title="">2009</a>)</cite> is noted for its straightforward architecture and robust efficiency, proving effective in supporting recommendation systems. However, in larger-scale industrial settings, developing storage engines that enhance rapid batch query capabilities for recommendations remains a critical research area.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="304" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>. </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Key-Value Storage in Recommendation system</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">As deep learning becomes the mainstay in the realm of recommendation systems <cite class="ltx_cite ltx_citemacro_citep">(Covington et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib7" title="">2016</a>; Cheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib5" title="">2016</a>; Gupta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib17" title="">2020</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib41" title="">2019</a>; Davidson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib8" title="">2010</a>)</cite>, the size of these models has escalated, now reaching terabyte scale <cite class="ltx_cite ltx_citemacro_citep">(Lian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib24" title="">2022</a>)</cite>. Deep learning-based recommendation systems require a vast array of features to accurately depict complex user behaviors, attributes, and preferences. Bilibili primarily distributes video content, typically around 10 minutes in length. Therefore, compared to e-commerce recommendations, features such as key frame image embeddings, video sentiment, and the author’s attitudes and opinions are crucial for content delivery.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In industrial recommendation systems with hundreds of millions of users and billions of items, each record may comprise hundreds of simple floating-point features, potentially leading to embedding table sizes in the terabyte range. Such extensive feature storage and embedding tables cannot be housed within a single memory unit. This leads to the conclusion that optimizing the batch query service of recommendation systems necessitates addressing three key challenges.</p>
</div>
<div class="ltx_para" id="S1.p4">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">High Performance.</span> A storage engine with high-speed batch reading capabilities serves as the carrier for a vast array of feature values and embedding tables, enhancing the throughput during model training and prediction.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Scalability.</span> A distributed query services cluster maintains high throughput even as system size increases - both in terms of the number of features and the volume of items processed.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Flexibility and Consistency.</span> Recent user behaviors with session-segment are more effective in predicting future actions <cite class="ltx_cite ltx_citemacro_citep">(Tuan and Phuong, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib36" title="">2017</a>; Lu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib27" title="">2018</a>; Twardowski, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib37" title="">2016</a>; Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib34" title="">2022</a>)</cite>. rapid updates ensure that real-time recommendation systems maintain effectiveness by reflecting users’ current interests. It is essential to avoid data inconsistencies during updates to prevent losses in recommendation system performance.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this paper, we design and implement a distributed query architecture for batch query that significantly enhances recommendation system performance. Here is our contribution. First, we have made several design to improve Coalesced Hashing, resulting in a new hash table structure called NeighborHash, which minimize the number of cacheline accesses per query to achieve larger query throughput within the constraints of limited memory bandwidth. A comprehensive performance evaluation of NeighborHash reveals that, in batch query scenarios, it achieves a 170% performance improvement compared to the most optimized hash-tables, Validation experiments confirm the effectiveness of our optimizations. An open-source version of neighborhash is available at the following address <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/slow-steppers/NeighborHash" title="">https://github.com/slow-steppers/NeighborHash</a>. Second, Building on Neighborhash, we have introduced a SSD-based distributed key-value storage service that supports horizontal scaling of features and model sizes while maintaining low resource consumption. Particularly focused on the multi-version states of data during real-time learning updates, we optimized update and query protocols to ensure strong data consistency during batch queries, thereby maintaining stable recommendation performance. Finally, we deployed this architecture in the bilibili recommendation system and achieved significant benefits.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>System Design</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The design of the serving architecture draws significant inspiration from Google Mesa <cite class="ltx_cite ltx_citemacro_citep">(Gupta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib16" title="">2014</a>)</cite>. The system comprises the Batch Query Subsystem and Update Subsystem, outlined in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.00400v1#S2.F2" title="Figure 2 ‣ 2. System Design ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>. As a typical architecture in recommendation systems, the Update Subsystem manages data updates, encompassing user behavior properties and parameter updates for recommendation model training. The Query Subsystem caters to extensive data volumes and high concurrency requirements through a distributed layout with multiple shards and replicas. This setup is designed to handle a significant request volume (peaking at approximately 100k qps) and enormous feature tables that cannot be deployed on a single machine. At its foundation lies our hash table, Neighborhash, optimized for batch querying in recommendation systems and described below.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="821" id="S2.F2.g1" src="x2.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>. </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">Batch Query Architecture for Multi-Data Lookup</span></figcaption>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>NeighborHash</h3>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1. </span>Hashtable structure</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">NeighborHash, based on Coalesced Hashing, utilizes a flat array to store all buckets, with each bucket containing key, value and the index to the next spot in the chain or else the null value. To minimize the number of cachelines involved in queries, we implemented the following design:</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p2">
<p class="ltx_p" id="S2.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p2.1.1">Lodger relocation</span>. In traditional Coalesced hashing, hash table buckets are numbered 1 to M’. The first M buckets serve as the hash function’s address region, while the remaining M’–M buckets are exclusively for colliding records, known as the cellar. When the cellar fills up, subsequent colliders must occupy empty buckets in the address region, potentially leading to further collisions with later-inserted records. Hence, the PSL (probing sequence length) is sensitive to the cellar region’s size. To minimize the PSL, we eliminate the cellar region allocation. Instead, we place conflicting elements in the address region and dynamically adjust them. The method is as follows: For a record <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS1.p2.1.2">x</span> in bucket <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS1.p2.1.3">i</span>, if Hash(x.key) is <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS1.p2.1.4">i</span>, it is termed as the <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS1.p2.1.5">host</span> record; otherwise, it is termed as the <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS1.p2.1.6">lodger</span> record. When inserting a new record <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS1.p2.1.7">y</span>, with Hash(y.key) resulting in bucket <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS1.p2.1.8">j</span>, if bucket <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS1.p2.1.9">j</span> is occupied by a lodger, a vacant position is sought to relocate the lodger before storing record <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS1.p2.1.10">y</span> in bucket <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS1.p2.1.11">j</span>. If bucket <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS1.p2.1.12">j</span> is occupied by a host, a vacant position is sought to store record <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS1.p2.1.13">y</span>, and it is appended to the end of the chain.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p3">
<p class="ltx_p" id="S2.SS1.SSS1.p3.1">Lodger relocation can be seen as a dynamic cellar strategy, ensuring minimization of PSL (same as separate chaining). However, the drawback is that the insertion process becomes more complex. Considering query requests dominate the workload of recommendation systems, we believe this trade-off is worthwhile. The aforementioned process can be referenced in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.00400v1#S2.F3" title="Figure 3 ‣ 2.1.1. Hashtable structure ‣ 2.1. NeighborHash ‣ 2. System Design ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="551" id="S2.F3.g1" src="x3.png" width="746"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.2.1.1" style="font-size:90%;">Figure 3</span>. </span><span class="ltx_text" id="S2.F3.3.2" style="font-size:90%;">Insertion of NeighborHash with Lodger Relocation</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.SSS1.p4">
<p class="ltx_p" id="S2.SS1.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p4.1.1">Cacheline-aware neighbor probing</span>, NeighborHash strives to place buckets on the same chain within the same cache line to minimize memory bandwidth usage for each query. During the search for available buckets, the algorithm first examines buckets within the same cacheline. Unlike Linear Probing, which probes only in one direction, NeighborProbing conducts bidirectional probing within the cacheline. If none is found, the search expands bidirectionally to identify the nearest available bucket to the head, as illustrate in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.00400v1#S2.F4" title="Figure 4 ‣ 2.1.1. Hashtable structure ‣ 2.1. NeighborHash ‣ 2. System Design ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_tag">Figure 4</span></a> . This approach minimizes cross-cache-line probes during queries. Experimental results demonstrate that, on a random dataset of 100 million entries with a load factor of 75%, the average number of cache line accesses per query is approximately 1.12. Compared to Linear Probing, which is 1.47, Cacheline-aware Neighbor probing requires fewer cachelines. If no available bucket is within the same cacheline, relocating to nearby cachelines may mitigate TLB cache misses and lead to a smaller relative offset to the previous record in the chain, enabling offset compression.</p>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="183" id="S2.F4.g1" src="x4.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.2.1.1" style="font-size:90%;">Figure 4</span>. </span><span class="ltx_text" id="S2.F4.3.2" style="font-size:90%;">Find available node in neighbor cacheline</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.SSS1.p5">
<p class="ltx_p" id="S2.SS1.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p5.1.1">Inline chaining</span>, We utilize the high 12 bits of NeighborHash’s value field to represent relative offsets, enabling the implementation of a conflict linked list. The range of representation is -2047 to 2048. Considering the conflict allocation strategy primarily aims to find a suitable location near the root node, the 12-bit relative offset is more than sufficient. In practice, a 12-bit offset can achieve a load factor of over 80%. Consequently, NeighborHash employs a storage representation of 52 bits for the actual value, which is typically used to store pointers or offset values in recommendation system, illustrated in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.00400v1#S2.F5" title="Figure 5 ‣ 2.1.1. Hashtable structure ‣ 2.1. NeighborHash ‣ 2. System Design ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="305" id="S2.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F5.2.1.1" style="font-size:90%;">Figure 5</span>. </span><span class="ltx_text" id="S2.F5.3.2" style="font-size:90%;">Data Structure</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.SSS1.p6">
<p class="ltx_p" id="S2.SS1.SSS1.p6.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p6.1.1">Lookup Acceleration</span>.
Hash-table lookup is a predominant operation in recommender systems. Vectorization <cite class="ltx_cite ltx_citemacro_citep">(Polychroniou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib29" title="">2015</a>)</cite> and AMAC <cite class="ltx_cite ltx_citemacro_citep">(Kocberber et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib20" title="">2015</a>)</cite> have been extensively utilized for optimizing Hash-table batch lookup. We implemented inter-query vectorized query on NeighborHash using the IMV(Interleaved Multi-Vector) <cite class="ltx_cite ltx_citemacro_citep">(Fang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib13" title="">2019</a>)</cite> method. Efficient SIMD compress and expand operations can be employed for the append and fill processes. Vectorization can bring significant throughput improvements on small datasets, but on larger datasets, memory latency becomes the predominant factor. We implemented the AMAC method on NeighborHash and conducted experiments. The experiments show that on large datasets, with the help of AMAC, NeighborHash can achieve almost a doubling of throughput compared to its original performance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2. </span>NVMe storage</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">In industrial scenario with billions of users and items, there exists a vast amount of cold data. Compared to storing all data in memory, it is cost-effective to store cold data in NVMe and hot data in memory <cite class="ltx_cite ltx_citemacro_citep">(Wan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib39" title="">2021</a>)</cite>. To maintain the latency of request responses, we store index(key to value offset) in memory using the NeighborHash structure and store the value bytes in a two-tier manner. The 52-bit payload in NeighborHash includes 1 bit to indicate whether the data is stored in memory or NVMe. The payload of hot data points to the memory containing the LRU metadata, while that of cold data points to the file system offset. Eviction is completed by an asynchronous thread scanning the metadata of hot data. As illustrated in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.00400v1#S2.F6" title="Figure 6 ‣ 2.1.2. NVMe storage ‣ 2.1. NeighborHash ‣ 2. System Design ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>. Storing both hot and cold keys in memory reduces the concurrent read/write overhead on the Hash-table associated with traditional LRU methods. Additionally, during a cache miss, typically only one or a few NVMe I/O operations are involved. We believe that the additional memory overhead is more economical compared to the savings in CPU usage and the improvement in throughput. Due to space limitations, we will not provide a detailed analysis here.</p>
</div>
<figure class="ltx_figure" id="S2.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="293" id="S2.F6.g1" src="x6.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F6.2.1.1" style="font-size:90%;">Figure 6</span>. </span><span class="ltx_text" id="S2.F6.3.2" style="font-size:90%;">NeighborHash structure with NVMe support</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Batch Query Subsystem</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The Batch Query Subsystem is architected as a multi-sharded, multi-replica framework. Given the unique update cycles, query loads and geographic distribution demands of each table in the business domain, the subsystem is organized based on individual tables. Each table maps to a distinct query service, with specific shard and replica information maintained by a metadata service. This underlying infrastructure is supported by etcd <cite class="ltx_cite ltx_citemacro_citep">(etcd io, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib11" title="">2017</a>)</cite> for robustness.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1. </span>Automatic Sharding</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">Sharding data during service has two primary advantages:</p>
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">It avoids the challenges linked to overly large service instances, including lengthy start-up times and high data retrieval bandwidth, enhancing system stability. Smaller shards also facilitate quicker migration and recovery.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">Parallel queries across multiple shards reduce the batch query load on individual instances. Coupled with asynchronous processing for immediate tasks like click-through rate estimation, this approach notably decreases user request latency.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S2.SS2.SSS1.p1.2">The system automatically manages shard creation based on set configuration parameters, ensuring no shard exceeds its designated size. Should the table grow or shrink during updates, re-sharding occurs during the next update cycle, with updated metadata synchronized across the live cluster.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2. </span>Rolling Update and Query Consistency</h4>
<figure class="ltx_figure" id="S2.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="223" id="S2.F7.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F7.2.1.1" style="font-size:90%;">Figure 7</span>. </span><span class="ltx_text" id="S2.F7.3.2" style="font-size:90%;">Data Online Updating and Query Interaction</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">Real-time recommendation systems in production cannot have downtime for updates and maintenance, as they need to be constantly operational. Typically, updating data requires additional resources. A straightforward approach involves deploying a backup query service for each table to handling all traffic, switching to it once ready, necessitating double the resources. We implement a rolling update approach, updating one replica at a time and only requiring an additional 1/n of the resources. However, this introduces the issue of consistency across different versions over shards during table updating. For some model embedding tables, only values from the same training batch are comparable, which we refer to as strong version data. For sorting, it is crucial that features for the same batch of items come from the same version to ensure comparability.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.p2.1">Typically, shard and data version information stored on servers are registered with naming service and updated regularly. Clients consult this naming service for details, and request version-specific data from corresponding servers. However, in industrial-scale distributed systems with thousands of client and server instances, network delays and packet losses can prolong the process of server metadata updating in the Naming service and its retrieval by clients. This delay prevents servers from providing immediate service after they are ready, requiring explicit version consistency confirmation between clients and servers before proceeding with the next batch of rolling updates. This significantly extends the total update time, especially for data services that require frequent updates, where clients may not yet detect the previous version before the server needs to load new data, leading to request failures due to version inconsistencies. To address this issue, we utilize the Naming service only for updates to the server instance interfaces (IP:port), such as additions or deletions, while shard and version metadata are communicated directly between clients and servers through the query protocol, ensuring strong consistency in shard management on the client-side. The data update mechanism and access consistency scheme are illustrated in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.00400v1#S2.F7" title="Figure 7 ‣ 2.2.2. Rolling Update and Query Consistency ‣ 2.2. Batch Query Subsystem ‣ 2. System Design ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_tag">Figure 7</span></a> and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.00400v1#S2.F8" title="Figure 8 ‣ 2.2.2. Rolling Update and Query Consistency ‣ 2.2. Batch Query Subsystem ‣ 2. System Design ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_tag">Figure 8</span></a>, respectively. Data compression and asynchronous pipeline processing are also encapsulated within this library. Moreover, it provides direct access to embedding tables and feature storage, minimizing network bandwidth overhead compared to a proxy-based approach.</p>
</div>
<figure class="ltx_figure" id="S2.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="700" id="S2.F8.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F8.2.1.1" style="font-size:90%;">Figure 8</span>. </span><span class="ltx_text" id="S2.F8.3.2" style="font-size:90%;">Query with version control during updating</span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Experiment and Result</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we first provide a detailed evaluation and comparison of the optimization of hashtable, as this is a key factor in improving query throughput capacity. The second part directly assesses the latency optimization and resource efficiency from the new query cluster by conducting online comparisons in the recommendation system of bilibili, along with evaluating the impact of data consistency on performance.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Evaluation of NeighborHash</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Firstly, we designed comparative experiments between NeighborHash and existing scalar hashmaps. The focus was on conducting ablation experiments and analysis between NeighborHash and Coalesced Hashing. Subsequently, we conducted comparisons and analyses of different vectorization methods and implementations.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Experiments Setup</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">We conducted all experiments on a Linux Server, which is configured as outlined in README of github repo, The live recommendation system is also deployed on machines of these specifications. In the context of recommendation systems, the specific type of hash table we focus on that includes skewed data distribution, large dataset size, high load factor, very high read/write skew, and a high successful lookup ratio. Certain configurations can influence the evaluation results, as detailed below.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Hardware prefetch, In certain scenarios, for precise analysis the efficiency of hash-table, we disabled hardware prefetching.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Hash function, the hash function employed across all tests was absl::Hash¡uint64_t¿.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">Size and Load Factor, as mentioned earlier, we tested hash-tables with keys and values both being 64 bits, thus each element occupying 16 bytes of memory. We opted for datasets of sizes 256KB(16K), 2MB(128K), 16MB(1M), 256MB(16M), 2GB(128M) and 16GB(1G) for testing. We ensure that the hash-table’s load factor is set to 80%, a practical value.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.1">Data distribution significantly impacts the test results of the Hash-table. Due to space limitations and the typically high memory load in real-world scenarios, our tests and analyses were conducted on uniformly distributed datasets. We also tested skewed data, and the conclusions were consistent with those from the uniformly distributed data in terms of trends and qualitative analysis.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p4">
<p class="ltx_p" id="S3.SS1.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.p4.1.1">Metrics</span>. We measure hash-table throughput in MOPS (Million Operations Per Second) and monitor memory bandwidth consumption in BPL (Bytes Per Lookup). On Intel platforms, we utilize PCM to track bandwidth and calculate BPL by dividing bandwidth (BPS - Bytes Per Second) by the MOPS. Our evaluation also considers the LLC (Last Level Cache) cache-miss rate. We analyze probing performance using the <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p4.1.2">Average Probing Cache Lines (APCL)</span> metric, indicating the average number of cache lines accessed for each successfully found key. For exploring the upper limit of hash-table lookup operations, we created a hash-table with zero collisions, albeit without guaranteed correctness. Each lookup involved hash calculation and random reading, deemed indispensable minimal operations in hash-table lookup. This imperfect hash-table version is referred to as RA (Random Access). Subsequent experiments showed that NeighborHash achieved 90% of RA’s throughput with parallel optimization.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>Scalar Hash-Tables Comparsion</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">we evaluate the scalar hash-tables. The hash-tables evaluated include Linear Probing, Coalesced hashing, ska::bytell_hash_map <cite class="ltx_cite ltx_citemacro_citep">(Skarupke, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib35" title="">2018</a>)</cite> and Neighborhash. We also introduce the previously mentioned random access in the comparison to understand the absolute level of query performance. The results are shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.00400v1#S3.T1" title="Table 1 ‣ 3.1.2. Scalar Hash-Tables Comparsion ‣ 3.1. Evaluation of NeighborHash ‣ 3. Experiment and Result ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_tag">Table 1</span></a>. Due to the significant throughput of RA on datasets smaller than 256MB, it has been omitted from the table. As the dataset size increases, continuous cache misses lead to a sustained decline in throughput. In comparison to other implementations, NeighborHash consistently exhibits performance improvements across all scenarios. At a dataset size of 16GB, NeighborHash demonstrates over 50% higher query throughput compared to other implementations. In this test, the success query rate(SQR) was 90%, consistent with mainstream recommendation systems online. We also conducted tests under conditions of low hit rates(30%), and the results showed completely consistent trends.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T1.2.1.1.1" rowspan="2"><span class="ltx_text" id="S3.T1.2.1.1.1.1">Hashtable</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" colspan="5" id="S3.T1.2.1.1.2">Dataset size</th>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S3.T1.2.2.2.1">256KB</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S3.T1.2.2.2.2">2MB</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S3.T1.2.2.2.3">16MB</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S3.T1.2.2.2.4">256MB</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S3.T1.2.2.2.5">16GB</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.2.3.1.1">Linear probing</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.3.1.2">38</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.3.1.3">29</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.3.1.4">22</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.3.1.5">12</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.3.1.6">11</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.2.4.2.1">ska::bytell_hash_map</th>
<td class="ltx_td ltx_align_left" id="S3.T1.2.4.2.2">72</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.4.2.3">53</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.4.2.4">38</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.4.2.5">20</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.4.2.6">19</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.2.5.3.1">Coalesced hashing</th>
<td class="ltx_td ltx_align_left" id="S3.T1.2.5.3.2">92</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.5.3.3">56</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.5.3.4">48</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.5.3.5">21</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.5.3.6">19</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.2.6.4.1"><span class="ltx_text ltx_font_bold" id="S3.T1.2.6.4.1.1">Neighborhash</span></th>
<td class="ltx_td ltx_align_left" id="S3.T1.2.6.4.2"><span class="ltx_text ltx_font_bold" id="S3.T1.2.6.4.2.1">116</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.6.4.3"><span class="ltx_text ltx_font_bold" id="S3.T1.2.6.4.3.1">74</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.6.4.4"><span class="ltx_text ltx_font_bold" id="S3.T1.2.6.4.4.1">66</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.6.4.5"><span class="ltx_text ltx_font_bold" id="S3.T1.2.6.4.5.1">37</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.6.4.6"><span class="ltx_text ltx_font_bold" id="S3.T1.2.6.4.6.1">36</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T1.2.7.5.1">Random Access /</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.2.7.5.2">/</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.2.7.5.3">/</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.2.7.5.4">/</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.2.7.5.5">67</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.2.7.5.6">67</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.3.1.1" style="font-size:90%;">Table 1</span>. </span><span class="ltx_text" id="S3.T1.4.2" style="font-size:90%;">Scalar Hashtable Lookup Performance(Mops)</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1">To analyze the impact of dataset size on hash-table performance, we conducted a detailed evaluation of NeighborHash, as presented in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.00400v1#S3.T2" title="Table 2 ‣ 3.1.2. Scalar Hash-Tables Comparsion ‣ 3.1. Evaluation of NeighborHash ‣ 3. Experiment and Result ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_tag">Table 2</span></a>. For dataset sizes less than 2MB, most memory loads can be accommodated by the L1 and L2 cache. At this point, the MOPS is more influenced by the number of instructions and their cycles. Starting from a size of 16MB, the LLC miss rate begins to rise, reaching about 34% at 32MB. Simultaneously, MOPS starts to decline, while Bytes-per-lookup increases. As the LLC miss rate increases, Bytes-per-lookup exhibits a rapid growth after 32MB (L2 cache size) and converges at a dataset size of 2GB, indicating that MOPS is almost entirely dominated by memory latency.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T2.2.1.1.1">Datasets</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.2.1.1.2">LLC-LD</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.2.1.1.3">LLC-MR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.2.1.1.4">MOPS</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.2.1.1.5">BPL</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.2.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T2.2.2.1.1">256KB</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.1.2">0.004</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.1.3">25%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.1.4">116</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.2.2.1.5">0.15</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T2.2.3.2.1">2MB</th>
<td class="ltx_td ltx_align_center" id="S3.T2.2.3.2.2">54</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.3.2.3">0.01%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.3.2.4">74</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.3.2.5">0.15</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T2.2.4.3.1">16MB</th>
<td class="ltx_td ltx_align_center" id="S3.T2.2.4.3.2">82</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.4.3.3">3.58%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.4.3.4">66</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.4.3.5">2.56</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T2.2.5.4.1">32MB</th>
<td class="ltx_td ltx_align_center" id="S3.T2.2.5.4.2">59</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.5.4.3">33.98%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.5.4.4">48</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.5.4.5">27.9</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T2.2.6.5.1">256MB</th>
<td class="ltx_td ltx_align_center" id="S3.T2.2.6.5.2">46.6</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.6.5.3">90.9%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.6.5.4">37</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.6.5.5">78.7</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T2.2.7.6.1">2GB</th>
<td class="ltx_td ltx_align_center" id="S3.T2.2.7.6.2">45.5</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.7.6.3">98.6%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.7.6.4">37</td>
<td class="ltx_td ltx_align_center" id="S3.T2.2.7.6.5">81.4</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T2.2.8.7.1">16GB</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.8.7.2">44.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.8.7.3">99.2%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.8.7.4">39</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.2.8.7.5">82.1</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T2.3.1.1" style="font-size:90%;">Table 2</span>. </span><span class="ltx_text" id="S3.T2.4.2" style="font-size:90%;">NeighborHash, SQR=90%</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS2.p3">
<p class="ltx_p" id="S3.SS1.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p3.1.1">Ablation Analysis</span>.To comprehend the impact of various design components of NeighborHash on outcomes, we conducted ablation analysis on the following three key designs of NeighborHash:</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p4">
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1">Lodger relocation, employing only this strategy in Coalesced hashing, the search efficiency is equivalent to Coalesced hashing with perfect cellar, abbreviated as PerfectCellarHash.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">Cacheline-aware neighbor probing, building upon PerfectCellarHash, prioritizes searching for available buckets near the cacheline of the last node in the conflict chain and its vicinity. Relative offsets are stored in a separate offset array. This implementation is referred to as NeighborProbing.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1">Inline-chaining, based on NeighborProbing, encodes relative offsets into the high 12 bits of the value, thus completing the full implementation of NeighborHash.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p5">
<p class="ltx_p" id="S3.SS1.SSS2.p5.1">We conducted experiments and analyses on different datasets using the three aforementioned implementations. Datasets larger than 2GB exhibited similar trends. As we are particularly interested in the performance on larger datasets, the <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.00400v1#S3.T3" title="Table 3 ‣ 3.1.2. Scalar Hash-Tables Comparsion ‣ 3.1. Evaluation of NeighborHash ‣ 3. Experiment and Result ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_tag">Table 3</span></a> presents the results for the 16GB dataset.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T3.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T3.2.1.1.1">Hashmap</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T3.2.1.1.2">MOPS</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.1.1.3">MOPS-gain</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.1.1.4">APCL</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.2.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T3.2.2.1.1">CoalescedHashing</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T3.2.2.1.2">19</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.1.3">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.2.1.4">1.72</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.3.2.1">PerfectCellarHash</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.3.2.2">23</th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.3.2.3">1.21</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.3.2.4">1.48</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.4.3.1">NeighborProbing</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.4.3.2">30</th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.4.3.3">1.30</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.4.3.4">1.34</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T3.2.5.4.1">NeighborHash</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T3.2.5.4.2">39</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.5.4.3">1.30</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.5.4.4">1.14</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.3.1.1" style="font-size:90%;">Table 3</span>. </span><span class="ltx_text" id="S3.T3.4.2" style="font-size:90%;">size=16GB, SQR=90%, LF=0.8, Ablation Analysis</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS2.p6">
<p class="ltx_p" id="S3.SS1.SSS2.p6.1">Experimental results indicate that the aforementioned three key designs of NeighborHash contributed to throughput gains of 20%, 30%, and 30%, respectively, in terms of Millions of Operations Per Second (MOPS). It is noteworthy that these three designs are not entirely independent; the joint action of lodger relocation and neighbor probing enables offset compression, which, when combined with specific usage scenarios, allows for its integration into the value. Average Probing Cachelines(APCL) decreased from 1.72 to 1.14, resulting in saved memory bandwidth and achieving the goal of throughput improvement. We also evaluated the APCL of linear probing with Lodger Relocation, resulting in 1.24. It can be inferred that bidirectional probing can contribute approximately 9% to memory bandwidth efficiency compared to unidirectional probing.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p7">
<p class="ltx_p" id="S3.SS1.SSS2.p7.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p7.1.1">Integration with Optimization</span>. We implemented inter-query vectorization on NeighborHash, and the evaluation results are presented in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.00400v1#S3.F9" title="Figure 9 ‣ 3.1.2. Scalar Hash-Tables Comparsion ‣ 3.1. Evaluation of NeighborHash ‣ 3. Experiment and Result ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_tag">Figure 9</span></a> which also includes the evaluation results of AMAC combined with SIMD for NeighborHash. For queries on smaller datasets, SIMD optimizations are most effective because most of the data can be directly stored in L2-cache. As the dataset size increases, AMAC becomes the better choice. In practical system optimizations, selecting the appropriate version based on the size of the dataset can yield the best results.</p>
</div>
<figure class="ltx_figure" id="S3.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="461" id="S3.F9.g1" src="x9.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F9.2.1.1" style="font-size:90%;">Figure 9</span>. </span><span class="ltx_text" id="S3.F9.3.2" style="font-size:90%;">NB with Vec and/or AMAC,SQR=90%</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Benefits in Industrial Scenario</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We deployed the batch query architecture proposed in this paper in Bilibili recommendation system. The significant improvements in computational and storage capabilities have facilitated optimizations across the recommendation models. Here, we present two key online comparison metrics: access latency and impact of data consistency on effectiveness.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Latency</span>. We compared its effectiveness with a RocksDB-based Key-value query service, a commonly used solution in industrial recommendations that our system has previously utilized. We observed a 3 to 6 times increase in query latency compared to the baseline. In an experiment focusing on a high-traffic storage table for item features(40M items, 1KB per-item), with a peak online Key-Seek Per Second (KPS) of around 700k, we found that as the batch_size of single key queries increased, performance of query service remained stable without significant degradation, as illustrate in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.00400v1#S3.T4" title="Table 4 ‣ 3.2. Benefits in Industrial Scenario ‣ 3. Experiment and Result ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_tag">Table 4</span></a>. CPU profiling with the perf tool showed that hash lookups in Neighborhash consumed a small portion of the CPU, with about half of the CPU usage dedicated to IO operations like payload packaging. Conversely, in the RocksDB implementation with an in-memory table configured as a hashtable and 10GB of memory(same with NeighborKV), approximately 30% of CPU utilization was allocated to memory queries and retrieval. Consequently, as the batch size increased, the performance discrepancies became more noticeable.</p>
</div>
<figure class="ltx_table" id="S3.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T4.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T4.2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.2.1.1.1">Key-value</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.2.1.1.2">Batch-size</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.2.1.1.3">AVG-latency(ms)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T4.2.2.1">
<td class="ltx_td ltx_border_t" id="S3.T4.2.2.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.1.2">10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.2.1.3">1.11</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.3.2">
<td class="ltx_td ltx_align_center" id="S3.T4.2.3.2.1">KV(Rocksdb)</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.3.2.2">100</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.3.2.3">10.56</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.4.3">
<td class="ltx_td" id="S3.T4.2.4.3.1"></td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.4.3.2">500</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.4.3.3">25.81</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.5.4">
<td class="ltx_td ltx_border_t" id="S3.T4.2.5.4.1"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.5.4.2">10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.2.5.4.3">1.05</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.6.5">
<td class="ltx_td ltx_align_center" id="S3.T4.2.6.5.1">KV(Neighborhash)</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.6.5.2">100</td>
<td class="ltx_td ltx_align_center" id="S3.T4.2.6.5.3">1.78</td>
</tr>
<tr class="ltx_tr" id="S3.T4.2.7.6">
<td class="ltx_td ltx_border_bb" id="S3.T4.2.7.6.1"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.7.6.2">500</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.2.7.6.3">3.31</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T4.3.1.1" style="font-size:90%;">Table 4</span>. </span><span class="ltx_text" id="S3.T4.4.2" style="font-size:90%;">KV(Rocksdb) vs KV(Neighborhash)</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">CTR Improvements with data consistency</span>. In our A/B online experiments in the bilibili recommendation system, we compare click-through rate variances when enforcing consistent data version constraints during data shard replica rolling updates. The overall data comparison spans a full day, i.e., 24 hours, with results presented in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2409.00400v1#S3.F10" title="Figure 10 ‣ 3.2. Benefits in Industrial Scenario ‣ 3. Experiment and Result ‣ An Enhanced Batch Query Architecture in Real-time Recommendation"><span class="ltx_text ltx_ref_tag">Figure 10</span></a>. It is evident that the shorter the update interval, the more pronounced the benefits of batch query data consistency, as more inconsistencies occur with frequent updates. We analyzed the scenario where multi-shard updates of the embedding table occur without a consistency protocol, leading to approximately 3% inconsistency in results. This inconsistency implies that a single estimation might utilize multiple versions of the embedding weights. A detailed analysis of the inconsistent cases reveals that discrepancies among correlated features significantly impair the estimation results. Therefore, we conclude that the improvement in CTR can be attributed to the enhancement in consistency. While the specific effects may vary across systems and attribute tables, observations in various systems and multidimensional data showcase significant performance improvements through maintaining consistency.</p>
</div>
<figure class="ltx_figure" id="S3.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="482" id="S3.F10.g1" src="x10.png" width="745"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F10.2.1.1" style="font-size:90%;">Figure 10</span>. </span><span class="ltx_text" id="S3.F10.3.2" style="font-size:90%;">CTR Relative increase with consistency policy</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.1">Resource Saving</span>. NVMe usage depends on data categorization into hot and cold data, which varies across applications. A feasible recommendation is to employ NVMe storage for embedding tables and features that exhibit large data volumes but with very low Key-Seek Per Second (KPS) rates. The resource advantages here mainly come from the improvements in high-performance batch queries. The boost in single-instance throughput allows for fewer replicas, reducing the number of redundant resources due to rolling update capability. we has saved around 30% of machine resources in our recommendation system.
In systems with a long-tail distribution of user activity and item popularity, certain low-traffic embedding tables still require two replicas for fault tolerance. Future optimizations could focus on cluster scheduling to improve resource efficiency by sharing replicas among low-traffic tables.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Related Work</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Various evolutionary schemes have been developed based on hash strategies <cite class="ltx_cite ltx_citemacro_citep">(Pagh and Rodler, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib28" title="">2004</a>; Herlihy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib18" title="">2008</a>; Celis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib4" title="">1985</a>)</cite>. Coalesced Hashing <cite class="ltx_cite ltx_citemacro_citep">(Vitter, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib38" title="">1982</a>)</cite> is an attempt to combine linear probing and separate shaining but lacks consideration for cacheline friendliness and performs poorly in current practical environments. Neighborhash addresses these limitations and demonstrates significant improvements. Notable examples like Absl’s flat_hash_map <cite class="ltx_cite ltx_citemacro_citep">(Google, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib15" title="">2017</a>)</cite> and Facebook’s F14 <cite class="ltx_cite ltx_citemacro_citep">(Bronson and Shi, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib3" title="">2019</a>)</cite> have excelled in optimizing query efficiency through SIMD instruction utilization. Experimental evaluations show that NeighborHash achieves lower Average Probing Cache Lines and higher query throughput compared to linear probing. Monolith <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib26" title="">2022</a>)</cite> effectively filters low-frequency and outdated feature IDs in recommendation systems, reducing conflicts and enhancing model performance by utilizing Cuckoo hashing, while  <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib40" title="">2020</a>)</cite> give a hybrid hashing method to combine frequency hashing and double hashing techniques for model size reduction.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Due to variations in scenarios and applications, there is no one-size-fits-all design for key-value storage. The flexibility of key-value storage leads to ongoing research in optimization tailored to different contexts. Implementations like LevelDB, RocksDB, FlashStore, and SILT <cite class="ltx_cite ltx_citemacro_citep">(Google, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib14" title="">2011</a>; Facebook, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib12" title="">2021</a>; Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib10" title="">2021</a>; Debnath et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib9" title="">2010</a>; Lim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib25" title="">2011</a>)</cite> utilize LSM-trees for in-memory key-value storage. SlimDB <cite class="ltx_cite ltx_citemacro_citep">(Ren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib31" title="">2017</a>)</cite> employs dynamic compaction and in-memory index optimizations to enhance throughput for semi-sorted data, while F2 <cite class="ltx_cite ltx_citemacro_citep">(Kanellis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib19" title="">2023</a>)</cite> separates hot and cold data domains to boost overall performance under large data skew. In the realm of recommendation systems, feature store systems like Uber’s Michelangelo Palette, Google Feast and Amazon SageMaker Feature Store <cite class="ltx_cite ltx_citemacro_citep">(Chothani, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib6" title="">2017</a>; Sell, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib32" title="">2019</a>; AWS, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib2" title="">2019</a>)</cite> offer solutions for AI applications. RecShard <cite class="ltx_cite ltx_citemacro_citep">(Sethi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib33" title="">2022</a>)</cite> shards embedding tables based on training data distribution and model characteristics to improve model training throughput. EVTable <cite class="ltx_cite ltx_citemacro_citep">(Kurniawan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.00400v1#bib.bib21" title="">2023</a>)</cite> implemented a three-layer embedding lookup table to enhance recommendation effectiveness and optimize resources. These optimizations differ slightly from the approach in this paper, which enhances online inference throughput significantly within its feature store architecture by employing shard constraints and ensuring access consistency, suggesting that integrating the designs could yield greater benefits.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This paper presents an enhanced batch query architecture tailored for industrial-grade recommendation systems, providing high-performance throughput for batch queries of user and item features, model parameters and embedding tables.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We introduce NeighborHash, a hash-table optimized for batch point queries. By strategically placing conflicting nodes in neighboring positions, it reduces search probe times and cache misses, significantly enhancing batch query performance. Comparative analysis and ablation experiments demonstrate that Neighborhash outperforms existing hashtable structures significantly in recommendation scenarios. Building upon Neighborhash, we develop query cluster, which efficiently saves resources through NVMe storage compatibility and rolling updates. It ensures the real-time update of model parameters crucial for recommendation effectiveness while maintaining access consistency during updates. We deployed this system in the bilibili recommendation system and achieved a win-win in both resources and performance.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AWS (2019)</span>
<span class="ltx_bibblock">
AWS. 2019.

</span>
<span class="ltx_bibblock">Create, store, and share features with Amazon SageMaker Feature Store.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store.html" title="">https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bronson and Shi (2019)</span>
<span class="ltx_bibblock">
Nathan Bronson and Xiao Shi. 2019.

</span>
<span class="ltx_bibblock">Open-sourcing F14 for faster, more memory-efficient hash tables.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://engineering.fb.com/2019/04/25/developer-tools/f14/" title="">https://engineering.fb.com/2019/04/25/developer-tools/f14/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Celis et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (1985)</span>
<span class="ltx_bibblock">
Pedro Celis, Per-Ake Larson, and J. Ian Munro. 1985.

</span>
<span class="ltx_bibblock">Robin hood hashing. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">26th annual symposium on foundations of computer science (sfcs 1985)</em>. IEEE, 281–288.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al<span class="ltx_text" id="bib.bib5.3.1">.</span> 2016.

</span>
<span class="ltx_bibblock">Wide &amp; deep learning for recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.4.1">Proceedings of the 1st workshop on deep learning for recommender systems</em>. 7–10.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chothani (2017)</span>
<span class="ltx_bibblock">
Paarth Chothani. 2017.

</span>
<span class="ltx_bibblock">Palette Meta Store Journey.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.uber.com/en-KR/blog/palette-meta-store-journey/" title="">https://www.uber.com/en-KR/blog/palette-meta-store-journey/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Covington et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Paul Covington, Jay Adams, and Emre Sargin. 2016.

</span>
<span class="ltx_bibblock">Deep neural networks for youtube recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Proceedings of the 10th ACM conference on recommender systems</em>. 191–198.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Davidson et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, et al<span class="ltx_text" id="bib.bib8.3.1">.</span> 2010.

</span>
<span class="ltx_bibblock">The YouTube video recommendation system. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.4.1">Proceedings of the fourth ACM conference on Recommender systems</em>. 293–296.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Debnath et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
Biplob Debnath, Sudipta Sengupta, and Jin Li. 2010.

</span>
<span class="ltx_bibblock">FlashStore: High throughput persistent key-value store.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Proceedings of the VLDB Endowment</em> 3, 1-2 (2010), 1414–1425.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Siying Dong, Andrew Kryczka, Yanqin Jin, and Michael Stumm. 2021.

</span>
<span class="ltx_bibblock">Rocksdb: Evolution of development priorities in a key-value store serving large-scale applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">ACM Transactions on Storage (TOS)</em> 17, 4 (2021), 1–32.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">etcd io (2017)</span>
<span class="ltx_bibblock">
etcd io. 2017.

</span>
<span class="ltx_bibblock">etcd.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://etcd.io" title="">http://etcd.io</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Facebook (2021)</span>
<span class="ltx_bibblock">
Facebook. 2021.

</span>
<span class="ltx_bibblock">RocksDB.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebook/rocksdb" title="">https://github.com/facebook/rocksdb</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Zhuhe Fang, Beilei Zheng, and Chuliang Weng. 2019.

</span>
<span class="ltx_bibblock">Interleaved multi-vectorizing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">Proceedings of the VLDB Endowment</em> 13, 3 (2019), 226–238.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google (2011)</span>
<span class="ltx_bibblock">
Google. 2011.

</span>
<span class="ltx_bibblock">LevelDB.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/google/leveldb" title="">https://github.com/google/leveldb</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google (2017)</span>
<span class="ltx_bibblock">
Google. 2017.

</span>
<span class="ltx_bibblock">Abseil.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://abseil.io" title="">http://abseil.io</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Ashish Gupta, Fan Yang, Jason Govig, Adam Kirsch, Kelvin Chan, Kevin Lai, Shuo Wu, Sandeep Dhoot, Abhilash Kumar, and Ankur Agiwal. 2014.

</span>
<span class="ltx_bibblock">Mesa: Geo-replicated, near real-time, scalable data warehousing.

</span>
<span class="ltx_bibblock">(2014).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://research.google/pubs/pub42851/" title="">https://research.google/pubs/pub42851/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Udit Gupta, Carole-Jean Wu, Xiaodong Wang, Maxim Naumov, Brandon Reagen, David Brooks, Bradford Cottel, Kim Hazelwood, Mark Hempstead, Bill Jia, et al<span class="ltx_text" id="bib.bib17.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">The architectural implications of facebook’s dnn-based personalized recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.4.1">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</em>. IEEE, 488–501.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Herlihy et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2008)</span>
<span class="ltx_bibblock">
Maurice Herlihy, Nir Shavit, and Moran Tzafrir. 2008.

</span>
<span class="ltx_bibblock">Hopscotch hashing. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Distributed Computing: 22nd International Symposium, DISC 2008, Arcachon, France, September 22-24, 2008. Proceedings 22</em>. Springer, 350–364.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kanellis et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Konstantinos Kanellis, Badrish Chandramouli, and Shivaram Venkataraman. 2023.

</span>
<span class="ltx_bibblock">F2: Designing a Key-Value Store for Large Skewed Workloads.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">arXiv preprint arXiv:2305.01516</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocberber et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Onur Kocberber, Babak Falsafi, and Boris Grot. 2015.

</span>
<span class="ltx_bibblock">Asynchronous memory access chaining.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Proceedings of the VLDB Endowment</em> 9, 4 (2015), 252–263.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kurniawan et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Daniar H Kurniawan, Ruipu Wang, Kahfi S Zulkifli, Fandi A Wiranata, John Bent, Ymir Vigfusson, and Haryadi S Gunawi. 2023.

</span>
<span class="ltx_bibblock">EVStore: Storage and Caching Capabilities for Scaling Embedding Tables in Deep Recommendation Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2</em>. 281–294.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. 2014.

</span>
<span class="ltx_bibblock">Scaling distributed machine learning with the parameter server. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">11th USENIX Symposium on operating systems design and implementation (OSDI 14)</em>. 583–598.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Mu Li, Li Zhou, Zichao Yang, Aaron Li, Fei Xia, David G Andersen, and Alexander Smola. 2013.

</span>
<span class="ltx_bibblock">Parameter server for distributed machine learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Big learning NIPS workshop</em>, Vol. 6. Lake Tahoe, CA.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lian et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Xiangru Lian, Binhang Yuan, Xuefeng Zhu, Yulong Wang, Yongjun He, Honghuan Wu, Lei Sun, Haodong Lyu, Chengjun Liu, Xing Dong, et al<span class="ltx_text" id="bib.bib24.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Persia: An open, hybrid system scaling deep learning-based recommenders up to 100 trillion parameters. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.4.1">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>. 3288–3298.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lim et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
Hyeontaek Lim, Bin Fan, David G. Andersen, and Michael Kaminsky. 2011.

</span>
<span class="ltx_bibblock">SILT: A memory-efficient, high-performance key-value store. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Proceedings of the Twenty-Third ACM Symposium on Operating Systems Principles</em>. 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhuoran Liu, Leqi Zou, Xuan Zou, Caihua Wang, Biao Zhang, Da Tang, Bolin Zhu, Yijie Zhu, Peng Wu, Ke Wang, et al<span class="ltx_text" id="bib.bib26.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Monolith: real time recommendation system with collisionless embedding table.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.4.1">arXiv preprint arXiv:2209.07663</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Yichao Lu, Ruihai Dong, and Barry Smyth. 2018.

</span>
<span class="ltx_bibblock">Why I like it: multi-task learning for recommendation and explanation. In <em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">Proceedings of the 12th ACM Conference on Recommender Systems</em>. 4–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pagh and Rodler (2004)</span>
<span class="ltx_bibblock">
Rasmus Pagh and Flemming Friche Rodler. 2004.

</span>
<span class="ltx_bibblock">Cuckoo hashing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Journal of Algorithms</em> 51, 2 (2004), 122–144.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Polychroniou et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Orestis Polychroniou, Arun Raghavan, and Kenneth A. Ross. 2015.

</span>
<span class="ltx_bibblock">Rethinking SIMD vectorization for in-memory databases. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data</em>. 1493–1508.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">redis (2009)</span>
<span class="ltx_bibblock">
redis. 2009.

</span>
<span class="ltx_bibblock">Redis.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://redis.io" title="">https://redis.io</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Kai Ren, Qing Zheng, Joy Arulraj, and Garth Gibson. 2017.

</span>
<span class="ltx_bibblock">SlimDB: A space-efficient key-value storage engine for semi-sorted data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Proceedings of the VLDB Endowment</em> 10, 13 (2017), 2037–2048.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sell (2019)</span>
<span class="ltx_bibblock">
Tim Sell. 2019.

</span>
<span class="ltx_bibblock">Introducing Feast: an open source feature store for machine learning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cloud.google.com/blog/products/ai-machine-learning/introducing-feast-an-open-source-feature-store-for-machine-learning" title="">https://cloud.google.com/blog/products/ai-machine-learning/introducing-feast-an-open-source-feature-store-for-machine-learning</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sethi et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Geet Sethi, Bilge Acun, Niket Agarwal, Christos Kozyrakis, Caroline Trippel, and Carole-Jean Wu. 2022.

</span>
<span class="ltx_bibblock">RecShard: statistical feature-based memory optimization for industry-scale neural recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</em>. 344–358.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Qijie Shen, Hong Wen, Jing Zhang, and Qi Rao. 2022.

</span>
<span class="ltx_bibblock">Hierarchically Fusing Long and Short-Term User Interests for Click-Through Rate Prediction in Product Search. In <em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">Proceedings of the 31st ACM International Conference on Information and Knowledge Management</em> <em class="ltx_emph ltx_font_italic" id="bib.bib34.4.2">(CIKM ’22)</em>. ACM.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Skarupke (2018)</span>
<span class="ltx_bibblock">
Malte Skarupke. 2018.

</span>
<span class="ltx_bibblock">A new fast hash table in response to Google’s new fast hash table.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://probablydance.com/2018/05/28/a-new-fast-hash-table-in-response-to-googles-new-fast-hash-table/" title="">https://probablydance.com/2018/05/28/a-new-fast-hash-table-in-response-to-googles-new-fast-hash-table/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tuan and Phuong (2017)</span>
<span class="ltx_bibblock">
Trinh Xuan Tuan and Tu Minh Phuong. 2017.

</span>
<span class="ltx_bibblock">3D convolutional networks for session-based recommendation with content features. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the eleventh ACM conference on recommender systems</em>. 138–146.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Twardowski (2016)</span>
<span class="ltx_bibblock">
Bartłomiej Twardowski. 2016.

</span>
<span class="ltx_bibblock">Modelling contextual information in session-aware recommender systems with neural networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the 10th ACM Conference on Recommender Systems</em>. 273–276.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vitter (1982)</span>
<span class="ltx_bibblock">
Jeffrey Scott Vitter. 1982.

</span>
<span class="ltx_bibblock">Implementations for coalesced hashing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Commun. ACM</em> 25, 12 (1982), 911–926.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wan et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Hu Wan, Xuan Sun, Yufei Cui, Chia-Lin Yang, Tei-Wei Kuo, and Chun Jason Xue. 2021.

</span>
<span class="ltx_bibblock">FlashEmbedding: storing embedding tables in SSD for large-scale recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">Proceedings of the 12th ACM SIGOPS Asia-Pacific Workshop on Systems</em>. 9–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Caojin Zhang, Yicun Liu, Yuanpu Xie, Sofia Ira Ktena, Alykhan Tejani, Akshay Gupta, Pranay Kumar Myana, Deepak Dilipkumar, Suvadip Paul, Ikuhiro Ihara, et al<span class="ltx_text" id="bib.bib40.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Model size reduction using frequency based double hashing for recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib40.4.1">Proceedings of the 14th ACM Conference on Recommender Systems</em>. 521–526.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Zhe Zhao, Lichan Hong, Li Wei, Jilin Chen, Aniruddh Nath, Shawn Andrews, Aditee Kumthekar, Maheswaran Sathiamoorthy, Xinyang Yi, and Ed Chi. 2019.

</span>
<span class="ltx_bibblock">Recommending what video to watch next: a multitask ranking system. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">Proceedings of the 13th ACM Conference on Recommender Systems</em>. 43–51.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Aug 31 09:03:58 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
