<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.12778] Defending against Data Poisoning Attacks in Federated Learning via User Elimination</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Defending against Data Poisoning Attacks in Federated Learning via User Elimination">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Defending against Data Poisoning Attacks in Federated Learning via User Elimination">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.12778">

<!--Generated on Sun May  5 17:50:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on  %Affiliations␣are␣output␣in␣the␣\date{}␣command .-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Defending against Data Poisoning Attacks in Federated Learning via User Elimination</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nick Galanis<sup id="id1.1.id1" class="ltx_sup">1</sup>
</span></span>
</div>
<div class="ltx_dates">(<sup id="id2.id1" class="ltx_sup"><span id="id2.id1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">1</span></sup><span id="id3.id2" class="ltx_text" style="font-size:80%;">School of Engineering, University College London
<br class="ltx_break"></span>)</div>

<section id="S1" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Machine Learning is emerging in becoming a field at the forefront of advancing how we interact with systems and data. It is characterized by the ability of said machines to make data-oriented decisions, while facilitating its users to fulfill decision-making processes, helping them optimize tasks, and enabling a new era of automation.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Due to the rise of available data, researchers and developers are now supplying ML models with large amounts of data, as it is required by a state-of-the art model in order to function and learn correctly. Data typically flow freely and are widely available when it comes to public and everyday tasks, like images, speech, stocks, etc., with people being able to access and use them freely and without any type of license in order to train their own ML models. However, this is not the case for information that is held or produced by a person. Data like facial and private images, health records and location information are and should remain private, as there are not always good intentions by people accessing and analyzing the data.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The solution to this problem was first proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">16</a>]</cite>, and answers to the name Federated Learning (FL). The goal of FL is to be an efficient and scalable solution to find and collaborate with distributed resources in order to train a Machine Learning model. This is an approach that by its definition allows users to communicate with a central entity and contribute to the learning process while keeping their private data local. This training paradigm, as we will see moving forward, offers potential solutions to the issues haunting traditional machine learning models, such as privacy concerns and high communication costs, while also enabling access to a broader and more diverse range of data sources.</p>
</div>
<section id="S1.SS0.SSS1" class="ltx_subsubsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.0.1 </span>Problem introduction.</h4>

<div id="S1.SS0.SSS1.p1" class="ltx_para">
<p id="S1.SS0.SSS1.p1.1" class="ltx_p">Nevertheless, there is no panacea in any type of task in computer science, and same goes with Machine Learning and Federated Learning specifically. Letting users actively train a global model seems like a great idea, if of course all of them are totally honest and do not try to act maliciously, or even with curiosity. However, this is not always the case as will be discussed moving forward, as plenty of users may want to either harm or take advantage of the product while participating in the learning process.</p>
</div>
<div id="S1.SS0.SSS1.p2" class="ltx_para">
<p id="S1.SS0.SSS1.p2.1" class="ltx_p">This kind of attacks, namely Data Poisoning Attacks are a broad type of attacks that may contain users trying to alter the labels of their training set, or even the data itself, either with a goal of harming the model, thus they could act in a randomized manner, or in a targeted way in order to manipulate it. During this scenario, we thus take as a given fact that a certain percentage of the users that contribute to the training of the model have malicious intent, thus wanting to poison our system to cause it to wrongly classify instances in the testing phase.</p>
</div>
</section>
<section id="S1.SS0.SSS2" class="ltx_subsubsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.0.2 </span>Motivation and contributions.</h4>

<div id="S1.SS0.SSS2.p1" class="ltx_para">
<p id="S1.SS0.SSS2.p1.1" class="ltx_p">The essential thrust of our research is to contribute in the confrontation of the above-mentioned challenges. In this regard, we are going to examine privacy concerns created by the uncontrolled user participation in FL and present an attack that, as we are going to showcase, threatens the viability of such models. Specifically, we are going to focus on users that try to alter the dataset and thus poison the model, thus launching so called Data Poisoning Attacks. We are going to launch experiments and determine the correct metrics that must be utilized in order to detect such attacks.</p>
</div>
<div id="S1.SS0.SSS2.p2" class="ltx_para">
<p id="S1.SS0.SSS2.p2.1" class="ltx_p">However we are not going to limit our contribution to solely detecting a Data Poisoning Attack in Federated Learning, as we are also going to defend against it and minimize its impact to the final product. Although the field we are describing is rather new and has not yet grown enough, there have been some proposals to detect and prevent Poisoning Attacks. We are going to examine them and then make a valuable contribution to the developing arsenal of defenses proposed by the community, by developing and testing a novel idea which utilizes metadata reported from the users combined with modern Data Privacy techniques, in order for their identity to remain secret. As demonstrated in the paper, the proposed defense mechanisms showcased very positive results for complex image classification tasks, both in model performance and in malicious users’ detection.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Preliminaries and Relevant Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">As we lay the groundwork for our investigation into Poisoning Attacks in Federated Learning, the following section outlines the fundamental concepts and challenges at play. It will provide a comprehensive background on Federated Learning, introduce the critical issue of Poisoning Attacks within this framework, discuss the role of Differential Privacy as a defensive countermeasure, and examine relevant research that has previously tried to tackle the problem.</p>
</div>
<section id="S2.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Federated Learning</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Federated Learning is a Machine Learning paradigm where multiple users collaborate to train a model, while each individual’s data never leaves their device. The term was first introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">16</a>]</cite> by the Google Research Team, and offered a solution in the problem of decentralized learning, by forcing end-users (e.g. holders of mobile devices) to locally train an instance of the model, update the gradients that were sent to them by a centralized entity and then return their new weights back to it. This approach was conceived due to the significant volume of data on such devices, as well as their substantial computing power. Of course, this computing power is not enough to train a large and scalable ML model, it is however sufficient to train a small dataset with the private data that each device has.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The training process of an FL model is well described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">15</a>]</cite>. It begins with a global model being initialized on a central server, with either random or pre-defined weights. This model is communicated by the central server to a subset of the participating devices in the network. Then, each client trains the model that they received locally on their own data, typically over a number of epochs. The training phase is usually similar to traditional machine learning: computing the prediction, comparing it with the true value to compute the loss, and then updating the model parameters using a method like gradient descent to minimize the loss.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Once local training is complete, each client sends their update, that could include gradients, alterations in weights, or other forms of model parameters, back to the central server. The server then aggregates these updates from all the clients, a process that could be done with a number of methods, with the most simple being FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">16</a>]</cite>, which consists of an unweighted average of each user’s contribution. Finally, the global model is then updated given this aggregated information. Similarly with traditional ML training, the above process is repeated for several rounds until the model’s performance reaches a satisfactory level or does not improve significantly. The global model obtained at the end of this process is the final Federated Learning model, which is then subject to evaluation and testing.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">This seems like the optimal solution: personal data never leave users’ devices, the server does not need to train locally thus requires less computing power, as that is also distributed to the end users. Most importantly, users are able to use a model that has been trained in a wide variety of data, and not only their own, something that clearly will positively affect the ability of the model to correctly predict new behaviours.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">Federated Learning not only promises the above, but also provides privacy and security both to its end users and their raw data. The avoidance of data flow between server and users is a major step in that direction, but as we will see moving forward, that is not the only one taken. As excellently pointed out in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">15</a>]</cite>, Federated Learning brings the code to the data, instead of bringing the data to the code, something that helps in tackling the problem that legislation like GDPR is trying to prevent.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Differential Privacy</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In this piece of research, we are going to examine, comment and try to tackle problems regarding the protection of user-owned data. In order to do so, it is only fair that we start by providing an introduction to Data Privacy and the most relevant solution to the problem, Differential Privacy.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">One of the fundamental challenges for Privacy Enhancing Techniques has always been the protection of sensitive data. In the era of big-data and personal data collection it is of the utmost importance for companies to ensure their users that their data cannot be directly linked back to them. Moreover, we already have been given an idea of the importance of data privacy in machine learning and in data flow in general.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Driven by those principles, many approaches have been proposed to the community in an attempt to preserve Data Privacy. The solution was in the making for several years with approaches focusing on the insertion of random noise, most of them from the statistics and databases community, with the most influential being <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">8</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">9</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">1</a>]</cite>. The final and most complete solution came from Dwork in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">10</a>]</cite>, where the principles of a new way of anonymizing data, named Differential Privacy are communicated.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">Differential Privacy, as noted from Dwork in her original work, is rather a definition than a strict algorithm. The abstract idea behind Differential Privacy (DP), is that the output of a Differentially Private mechanism, should by independent of whether an individual is present in the domain N. The ”ability” of the adversary to recognize the existence of a column in the dataset, is regulated by the privacy parameter <math id="S2.SS2.p4.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S2.SS2.p4.1.m1.1a"><mi id="S2.SS2.p4.1.m1.1.1" xref="S2.SS2.p4.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.1.m1.1b"><ci id="S2.SS2.p4.1.m1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.1.m1.1c">\epsilon</annotation></semantics></math>. Differential Privacy is defined as following:</p>
</div>
<div id="Thmtheorem1" class="ltx_theorem ltx_theorem_theorem">
<h6 class="ltx_title ltx_runin ltx_title_theorem"><span class="ltx_tag ltx_tag_theorem"><span id="Thmtheorem1.1.1.1" class="ltx_text ltx_font_bold">THEOREM 1</span></span></h6>
<div id="Thmtheorem1.p1" class="ltx_para">
<p id="Thmtheorem1.p1.1" class="ltx_p"><span id="Thmtheorem1.p1.1.1" class="ltx_text ltx_font_italic">Differential Privacy, given in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">10</a>]</cite></span></p>
</div>
<div id="Thmtheorem1.p2" class="ltx_para">
<p id="Thmtheorem1.p2.5" class="ltx_p"><span id="Thmtheorem1.p2.5.5" class="ltx_text ltx_font_italic">A randomized algorithm <math id="Thmtheorem1.p2.1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="Thmtheorem1.p2.1.1.m1.1a"><mi id="Thmtheorem1.p2.1.1.m1.1.1" xref="Thmtheorem1.p2.1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="Thmtheorem1.p2.1.1.m1.1b"><ci id="Thmtheorem1.p2.1.1.m1.1.1.cmml" xref="Thmtheorem1.p2.1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmtheorem1.p2.1.1.m1.1c">M</annotation></semantics></math> is <math id="Thmtheorem1.p2.2.2.m2.2" class="ltx_Math" alttext="(\epsilon,\delta)" display="inline"><semantics id="Thmtheorem1.p2.2.2.m2.2a"><mrow id="Thmtheorem1.p2.2.2.m2.2.3.2" xref="Thmtheorem1.p2.2.2.m2.2.3.1.cmml"><mo stretchy="false" id="Thmtheorem1.p2.2.2.m2.2.3.2.1" xref="Thmtheorem1.p2.2.2.m2.2.3.1.cmml">(</mo><mi id="Thmtheorem1.p2.2.2.m2.1.1" xref="Thmtheorem1.p2.2.2.m2.1.1.cmml">ϵ</mi><mo id="Thmtheorem1.p2.2.2.m2.2.3.2.2" xref="Thmtheorem1.p2.2.2.m2.2.3.1.cmml">,</mo><mi id="Thmtheorem1.p2.2.2.m2.2.2" xref="Thmtheorem1.p2.2.2.m2.2.2.cmml">δ</mi><mo stretchy="false" id="Thmtheorem1.p2.2.2.m2.2.3.2.3" xref="Thmtheorem1.p2.2.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="Thmtheorem1.p2.2.2.m2.2b"><interval closure="open" id="Thmtheorem1.p2.2.2.m2.2.3.1.cmml" xref="Thmtheorem1.p2.2.2.m2.2.3.2"><ci id="Thmtheorem1.p2.2.2.m2.1.1.cmml" xref="Thmtheorem1.p2.2.2.m2.1.1">italic-ϵ</ci><ci id="Thmtheorem1.p2.2.2.m2.2.2.cmml" xref="Thmtheorem1.p2.2.2.m2.2.2">𝛿</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="Thmtheorem1.p2.2.2.m2.2c">(\epsilon,\delta)</annotation></semantics></math>-differentially private, if for all <math id="Thmtheorem1.p2.3.3.m3.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="Thmtheorem1.p2.3.3.m3.1a"><msub id="Thmtheorem1.p2.3.3.m3.1.1" xref="Thmtheorem1.p2.3.3.m3.1.1.cmml"><mi id="Thmtheorem1.p2.3.3.m3.1.1.2" xref="Thmtheorem1.p2.3.3.m3.1.1.2.cmml">D</mi><mn id="Thmtheorem1.p2.3.3.m3.1.1.3" xref="Thmtheorem1.p2.3.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="Thmtheorem1.p2.3.3.m3.1b"><apply id="Thmtheorem1.p2.3.3.m3.1.1.cmml" xref="Thmtheorem1.p2.3.3.m3.1.1"><csymbol cd="ambiguous" id="Thmtheorem1.p2.3.3.m3.1.1.1.cmml" xref="Thmtheorem1.p2.3.3.m3.1.1">subscript</csymbol><ci id="Thmtheorem1.p2.3.3.m3.1.1.2.cmml" xref="Thmtheorem1.p2.3.3.m3.1.1.2">𝐷</ci><cn type="integer" id="Thmtheorem1.p2.3.3.m3.1.1.3.cmml" xref="Thmtheorem1.p2.3.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmtheorem1.p2.3.3.m3.1c">D_{1}</annotation></semantics></math> and <math id="Thmtheorem1.p2.4.4.m4.1" class="ltx_Math" alttext="D_{2}" display="inline"><semantics id="Thmtheorem1.p2.4.4.m4.1a"><msub id="Thmtheorem1.p2.4.4.m4.1.1" xref="Thmtheorem1.p2.4.4.m4.1.1.cmml"><mi id="Thmtheorem1.p2.4.4.m4.1.1.2" xref="Thmtheorem1.p2.4.4.m4.1.1.2.cmml">D</mi><mn id="Thmtheorem1.p2.4.4.m4.1.1.3" xref="Thmtheorem1.p2.4.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="Thmtheorem1.p2.4.4.m4.1b"><apply id="Thmtheorem1.p2.4.4.m4.1.1.cmml" xref="Thmtheorem1.p2.4.4.m4.1.1"><csymbol cd="ambiguous" id="Thmtheorem1.p2.4.4.m4.1.1.1.cmml" xref="Thmtheorem1.p2.4.4.m4.1.1">subscript</csymbol><ci id="Thmtheorem1.p2.4.4.m4.1.1.2.cmml" xref="Thmtheorem1.p2.4.4.m4.1.1.2">𝐷</ci><cn type="integer" id="Thmtheorem1.p2.4.4.m4.1.1.3.cmml" xref="Thmtheorem1.p2.4.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmtheorem1.p2.4.4.m4.1c">D_{2}</annotation></semantics></math>, that differ on at most a single element, and <math id="Thmtheorem1.p2.5.5.m5.1" class="ltx_Math" alttext="S\subseteq Range(M)" display="inline"><semantics id="Thmtheorem1.p2.5.5.m5.1a"><mrow id="Thmtheorem1.p2.5.5.m5.1.2" xref="Thmtheorem1.p2.5.5.m5.1.2.cmml"><mi id="Thmtheorem1.p2.5.5.m5.1.2.2" xref="Thmtheorem1.p2.5.5.m5.1.2.2.cmml">S</mi><mo id="Thmtheorem1.p2.5.5.m5.1.2.1" xref="Thmtheorem1.p2.5.5.m5.1.2.1.cmml">⊆</mo><mrow id="Thmtheorem1.p2.5.5.m5.1.2.3" xref="Thmtheorem1.p2.5.5.m5.1.2.3.cmml"><mi id="Thmtheorem1.p2.5.5.m5.1.2.3.2" xref="Thmtheorem1.p2.5.5.m5.1.2.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="Thmtheorem1.p2.5.5.m5.1.2.3.1" xref="Thmtheorem1.p2.5.5.m5.1.2.3.1.cmml">​</mo><mi id="Thmtheorem1.p2.5.5.m5.1.2.3.3" xref="Thmtheorem1.p2.5.5.m5.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="Thmtheorem1.p2.5.5.m5.1.2.3.1a" xref="Thmtheorem1.p2.5.5.m5.1.2.3.1.cmml">​</mo><mi id="Thmtheorem1.p2.5.5.m5.1.2.3.4" xref="Thmtheorem1.p2.5.5.m5.1.2.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="Thmtheorem1.p2.5.5.m5.1.2.3.1b" xref="Thmtheorem1.p2.5.5.m5.1.2.3.1.cmml">​</mo><mi id="Thmtheorem1.p2.5.5.m5.1.2.3.5" xref="Thmtheorem1.p2.5.5.m5.1.2.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="Thmtheorem1.p2.5.5.m5.1.2.3.1c" xref="Thmtheorem1.p2.5.5.m5.1.2.3.1.cmml">​</mo><mi id="Thmtheorem1.p2.5.5.m5.1.2.3.6" xref="Thmtheorem1.p2.5.5.m5.1.2.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="Thmtheorem1.p2.5.5.m5.1.2.3.1d" xref="Thmtheorem1.p2.5.5.m5.1.2.3.1.cmml">​</mo><mrow id="Thmtheorem1.p2.5.5.m5.1.2.3.7.2" xref="Thmtheorem1.p2.5.5.m5.1.2.3.cmml"><mo stretchy="false" id="Thmtheorem1.p2.5.5.m5.1.2.3.7.2.1" xref="Thmtheorem1.p2.5.5.m5.1.2.3.cmml">(</mo><mi id="Thmtheorem1.p2.5.5.m5.1.1" xref="Thmtheorem1.p2.5.5.m5.1.1.cmml">M</mi><mo stretchy="false" id="Thmtheorem1.p2.5.5.m5.1.2.3.7.2.2" xref="Thmtheorem1.p2.5.5.m5.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Thmtheorem1.p2.5.5.m5.1b"><apply id="Thmtheorem1.p2.5.5.m5.1.2.cmml" xref="Thmtheorem1.p2.5.5.m5.1.2"><subset id="Thmtheorem1.p2.5.5.m5.1.2.1.cmml" xref="Thmtheorem1.p2.5.5.m5.1.2.1"></subset><ci id="Thmtheorem1.p2.5.5.m5.1.2.2.cmml" xref="Thmtheorem1.p2.5.5.m5.1.2.2">𝑆</ci><apply id="Thmtheorem1.p2.5.5.m5.1.2.3.cmml" xref="Thmtheorem1.p2.5.5.m5.1.2.3"><times id="Thmtheorem1.p2.5.5.m5.1.2.3.1.cmml" xref="Thmtheorem1.p2.5.5.m5.1.2.3.1"></times><ci id="Thmtheorem1.p2.5.5.m5.1.2.3.2.cmml" xref="Thmtheorem1.p2.5.5.m5.1.2.3.2">𝑅</ci><ci id="Thmtheorem1.p2.5.5.m5.1.2.3.3.cmml" xref="Thmtheorem1.p2.5.5.m5.1.2.3.3">𝑎</ci><ci id="Thmtheorem1.p2.5.5.m5.1.2.3.4.cmml" xref="Thmtheorem1.p2.5.5.m5.1.2.3.4">𝑛</ci><ci id="Thmtheorem1.p2.5.5.m5.1.2.3.5.cmml" xref="Thmtheorem1.p2.5.5.m5.1.2.3.5">𝑔</ci><ci id="Thmtheorem1.p2.5.5.m5.1.2.3.6.cmml" xref="Thmtheorem1.p2.5.5.m5.1.2.3.6">𝑒</ci><ci id="Thmtheorem1.p2.5.5.m5.1.1.cmml" xref="Thmtheorem1.p2.5.5.m5.1.1">𝑀</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmtheorem1.p2.5.5.m5.1c">S\subseteq Range(M)</annotation></semantics></math>, stands that:</span></p>
<table id="S2.Ex1" class="ltx_equation ltx_centering ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_centering ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex1.m1.2" class="ltx_Math" alttext="Pr[M(D_{1})\in S]\leq e^{\epsilon}\cdot Pr[M(D_{2})\in S]+\delta" display="block"><semantics id="S2.Ex1.m1.2a"><mrow id="S2.Ex1.m1.2.2" xref="S2.Ex1.m1.2.2.cmml"><mrow id="S2.Ex1.m1.1.1.1" xref="S2.Ex1.m1.1.1.1.cmml"><mi id="S2.Ex1.m1.1.1.1.3" xref="S2.Ex1.m1.1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.1.1.1.2" xref="S2.Ex1.m1.1.1.1.2.cmml">​</mo><mi id="S2.Ex1.m1.1.1.1.4" xref="S2.Ex1.m1.1.1.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.1.1.1.2a" xref="S2.Ex1.m1.1.1.1.2.cmml">​</mo><mrow id="S2.Ex1.m1.1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.Ex1.m1.1.1.1.1.1.2" xref="S2.Ex1.m1.1.1.1.1.2.1.cmml">[</mo><mrow id="S2.Ex1.m1.1.1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.1.1.cmml"><mrow id="S2.Ex1.m1.1.1.1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.1.1.1.cmml"><mi id="S2.Ex1.m1.1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.1.1.1.1.1.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.1.1.1.1.1.1.1.2" xref="S2.Ex1.m1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.2" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">D</mi><mn id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo stretchy="false" id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.Ex1.m1.1.1.1.1.1.1.2" xref="S2.Ex1.m1.1.1.1.1.1.1.2.cmml">∈</mo><mi id="S2.Ex1.m1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.1.1.1.1.1.1.3.cmml">S</mi></mrow><mo stretchy="false" id="S2.Ex1.m1.1.1.1.1.1.3" xref="S2.Ex1.m1.1.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S2.Ex1.m1.2.2.3" xref="S2.Ex1.m1.2.2.3.cmml">≤</mo><mrow id="S2.Ex1.m1.2.2.2" xref="S2.Ex1.m1.2.2.2.cmml"><mrow id="S2.Ex1.m1.2.2.2.1" xref="S2.Ex1.m1.2.2.2.1.cmml"><mrow id="S2.Ex1.m1.2.2.2.1.3" xref="S2.Ex1.m1.2.2.2.1.3.cmml"><msup id="S2.Ex1.m1.2.2.2.1.3.2" xref="S2.Ex1.m1.2.2.2.1.3.2.cmml"><mi id="S2.Ex1.m1.2.2.2.1.3.2.2" xref="S2.Ex1.m1.2.2.2.1.3.2.2.cmml">e</mi><mi id="S2.Ex1.m1.2.2.2.1.3.2.3" xref="S2.Ex1.m1.2.2.2.1.3.2.3.cmml">ϵ</mi></msup><mo lspace="0.222em" rspace="0.222em" id="S2.Ex1.m1.2.2.2.1.3.1" xref="S2.Ex1.m1.2.2.2.1.3.1.cmml">⋅</mo><mi id="S2.Ex1.m1.2.2.2.1.3.3" xref="S2.Ex1.m1.2.2.2.1.3.3.cmml">P</mi></mrow><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.2.2.2.1.2" xref="S2.Ex1.m1.2.2.2.1.2.cmml">​</mo><mi id="S2.Ex1.m1.2.2.2.1.4" xref="S2.Ex1.m1.2.2.2.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.2.2.2.1.2a" xref="S2.Ex1.m1.2.2.2.1.2.cmml">​</mo><mrow id="S2.Ex1.m1.2.2.2.1.1.1" xref="S2.Ex1.m1.2.2.2.1.1.2.cmml"><mo stretchy="false" id="S2.Ex1.m1.2.2.2.1.1.1.2" xref="S2.Ex1.m1.2.2.2.1.1.2.1.cmml">[</mo><mrow id="S2.Ex1.m1.2.2.2.1.1.1.1" xref="S2.Ex1.m1.2.2.2.1.1.1.1.cmml"><mrow id="S2.Ex1.m1.2.2.2.1.1.1.1.1" xref="S2.Ex1.m1.2.2.2.1.1.1.1.1.cmml"><mi id="S2.Ex1.m1.2.2.2.1.1.1.1.1.3" xref="S2.Ex1.m1.2.2.2.1.1.1.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.2.2.2.1.1.1.1.1.2" xref="S2.Ex1.m1.2.2.2.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.Ex1.m1.2.2.2.1.1.1.1.1.1.1" xref="S2.Ex1.m1.2.2.2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.Ex1.m1.2.2.2.1.1.1.1.1.1.1.2" xref="S2.Ex1.m1.2.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.Ex1.m1.2.2.2.1.1.1.1.1.1.1.1" xref="S2.Ex1.m1.2.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S2.Ex1.m1.2.2.2.1.1.1.1.1.1.1.1.2" xref="S2.Ex1.m1.2.2.2.1.1.1.1.1.1.1.1.2.cmml">D</mi><mn id="S2.Ex1.m1.2.2.2.1.1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.2.2.2.1.1.1.1.1.1.1.1.3.cmml">2</mn></msub><mo stretchy="false" id="S2.Ex1.m1.2.2.2.1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.2.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.Ex1.m1.2.2.2.1.1.1.1.2" xref="S2.Ex1.m1.2.2.2.1.1.1.1.2.cmml">∈</mo><mi id="S2.Ex1.m1.2.2.2.1.1.1.1.3" xref="S2.Ex1.m1.2.2.2.1.1.1.1.3.cmml">S</mi></mrow><mo stretchy="false" id="S2.Ex1.m1.2.2.2.1.1.1.3" xref="S2.Ex1.m1.2.2.2.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S2.Ex1.m1.2.2.2.2" xref="S2.Ex1.m1.2.2.2.2.cmml">+</mo><mi id="S2.Ex1.m1.2.2.2.3" xref="S2.Ex1.m1.2.2.2.3.cmml">δ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.2b"><apply id="S2.Ex1.m1.2.2.cmml" xref="S2.Ex1.m1.2.2"><leq id="S2.Ex1.m1.2.2.3.cmml" xref="S2.Ex1.m1.2.2.3"></leq><apply id="S2.Ex1.m1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1"><times id="S2.Ex1.m1.1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.1.2"></times><ci id="S2.Ex1.m1.1.1.1.3.cmml" xref="S2.Ex1.m1.1.1.1.3">𝑃</ci><ci id="S2.Ex1.m1.1.1.1.4.cmml" xref="S2.Ex1.m1.1.1.1.4">𝑟</ci><apply id="S2.Ex1.m1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S2.Ex1.m1.1.1.1.1.2.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S2.Ex1.m1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1"><in id="S2.Ex1.m1.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.2"></in><apply id="S2.Ex1.m1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1"><times id="S2.Ex1.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.2"></times><ci id="S2.Ex1.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.3">𝑀</ci><apply id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2">𝐷</ci><cn type="integer" id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3">1</cn></apply></apply><ci id="S2.Ex1.m1.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.3">𝑆</ci></apply></apply></apply><apply id="S2.Ex1.m1.2.2.2.cmml" xref="S2.Ex1.m1.2.2.2"><plus id="S2.Ex1.m1.2.2.2.2.cmml" xref="S2.Ex1.m1.2.2.2.2"></plus><apply id="S2.Ex1.m1.2.2.2.1.cmml" xref="S2.Ex1.m1.2.2.2.1"><times id="S2.Ex1.m1.2.2.2.1.2.cmml" xref="S2.Ex1.m1.2.2.2.1.2"></times><apply id="S2.Ex1.m1.2.2.2.1.3.cmml" xref="S2.Ex1.m1.2.2.2.1.3"><ci id="S2.Ex1.m1.2.2.2.1.3.1.cmml" xref="S2.Ex1.m1.2.2.2.1.3.1">⋅</ci><apply id="S2.Ex1.m1.2.2.2.1.3.2.cmml" xref="S2.Ex1.m1.2.2.2.1.3.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.2.2.2.1.3.2.1.cmml" xref="S2.Ex1.m1.2.2.2.1.3.2">superscript</csymbol><ci id="S2.Ex1.m1.2.2.2.1.3.2.2.cmml" xref="S2.Ex1.m1.2.2.2.1.3.2.2">𝑒</ci><ci id="S2.Ex1.m1.2.2.2.1.3.2.3.cmml" xref="S2.Ex1.m1.2.2.2.1.3.2.3">italic-ϵ</ci></apply><ci id="S2.Ex1.m1.2.2.2.1.3.3.cmml" xref="S2.Ex1.m1.2.2.2.1.3.3">𝑃</ci></apply><ci id="S2.Ex1.m1.2.2.2.1.4.cmml" xref="S2.Ex1.m1.2.2.2.1.4">𝑟</ci><apply id="S2.Ex1.m1.2.2.2.1.1.2.cmml" xref="S2.Ex1.m1.2.2.2.1.1.1"><csymbol cd="latexml" id="S2.Ex1.m1.2.2.2.1.1.2.1.cmml" xref="S2.Ex1.m1.2.2.2.1.1.1.2">delimited-[]</csymbol><apply id="S2.Ex1.m1.2.2.2.1.1.1.1.cmml" xref="S2.Ex1.m1.2.2.2.1.1.1.1"><in id="S2.Ex1.m1.2.2.2.1.1.1.1.2.cmml" xref="S2.Ex1.m1.2.2.2.1.1.1.1.2"></in><apply id="S2.Ex1.m1.2.2.2.1.1.1.1.1.cmml" xref="S2.Ex1.m1.2.2.2.1.1.1.1.1"><times id="S2.Ex1.m1.2.2.2.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.2.2.2.1.1.1.1.1.2"></times><ci id="S2.Ex1.m1.2.2.2.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.2.2.2.1.1.1.1.1.3">𝑀</ci><apply id="S2.Ex1.m1.2.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.2.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.2.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.2.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.Ex1.m1.2.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.2.2.2.1.1.1.1.1.1.1.1.2">𝐷</ci><cn type="integer" id="S2.Ex1.m1.2.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.2.2.2.1.1.1.1.1.1.1.1.3">2</cn></apply></apply><ci id="S2.Ex1.m1.2.2.2.1.1.1.1.3.cmml" xref="S2.Ex1.m1.2.2.2.1.1.1.1.3">𝑆</ci></apply></apply></apply><ci id="S2.Ex1.m1.2.2.2.3.cmml" xref="S2.Ex1.m1.2.2.2.3">𝛿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.2c">Pr[M(D_{1})\in S]\leq e^{\epsilon}\cdot Pr[M(D_{2})\in S]+\delta</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.6" class="ltx_p">The parameter <math id="S2.SS2.p5.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S2.SS2.p5.1.m1.1a"><mi id="S2.SS2.p5.1.m1.1.1" xref="S2.SS2.p5.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.1.m1.1b"><ci id="S2.SS2.p5.1.m1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.1.m1.1c">\epsilon</annotation></semantics></math> can be a regulator to the trade-off between privacy and usability that we mentioned, as lower values of <math id="S2.SS2.p5.2.m2.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S2.SS2.p5.2.m2.1a"><mi id="S2.SS2.p5.2.m2.1.1" xref="S2.SS2.p5.2.m2.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.2.m2.1b"><ci id="S2.SS2.p5.2.m2.1.1.cmml" xref="S2.SS2.p5.2.m2.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.2.m2.1c">\epsilon</annotation></semantics></math> mean stronger privacy guarantees and higher values indicate a more usable dataset. The parameter <math id="S2.SS2.p5.3.m3.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S2.SS2.p5.3.m3.1a"><mi id="S2.SS2.p5.3.m3.1.1" xref="S2.SS2.p5.3.m3.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.3.m3.1b"><ci id="S2.SS2.p5.3.m3.1.1.cmml" xref="S2.SS2.p5.3.m3.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.3.m3.1c">\delta</annotation></semantics></math> accounts for a small number that is present to even the result when the upper bound does not hold. If <math id="S2.SS2.p5.4.m4.1" class="ltx_Math" alttext="\delta=0" display="inline"><semantics id="S2.SS2.p5.4.m4.1a"><mrow id="S2.SS2.p5.4.m4.1.1" xref="S2.SS2.p5.4.m4.1.1.cmml"><mi id="S2.SS2.p5.4.m4.1.1.2" xref="S2.SS2.p5.4.m4.1.1.2.cmml">δ</mi><mo id="S2.SS2.p5.4.m4.1.1.1" xref="S2.SS2.p5.4.m4.1.1.1.cmml">=</mo><mn id="S2.SS2.p5.4.m4.1.1.3" xref="S2.SS2.p5.4.m4.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.4.m4.1b"><apply id="S2.SS2.p5.4.m4.1.1.cmml" xref="S2.SS2.p5.4.m4.1.1"><eq id="S2.SS2.p5.4.m4.1.1.1.cmml" xref="S2.SS2.p5.4.m4.1.1.1"></eq><ci id="S2.SS2.p5.4.m4.1.1.2.cmml" xref="S2.SS2.p5.4.m4.1.1.2">𝛿</ci><cn type="integer" id="S2.SS2.p5.4.m4.1.1.3.cmml" xref="S2.SS2.p5.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.4.m4.1c">\delta=0</annotation></semantics></math>, we say that <math id="S2.SS2.p5.5.m5.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.SS2.p5.5.m5.1a"><mi id="S2.SS2.p5.5.m5.1.1" xref="S2.SS2.p5.5.m5.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.5.m5.1b"><ci id="S2.SS2.p5.5.m5.1.1.cmml" xref="S2.SS2.p5.5.m5.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.5.m5.1c">M</annotation></semantics></math> is <math id="S2.SS2.p5.6.m6.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S2.SS2.p5.6.m6.1a"><mi id="S2.SS2.p5.6.m6.1.1" xref="S2.SS2.p5.6.m6.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.6.m6.1b"><ci id="S2.SS2.p5.6.m6.1.1.cmml" xref="S2.SS2.p5.6.m6.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.6.m6.1c">\epsilon</annotation></semantics></math>-differentially private.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p">The most common way that DP is introduced in a dataset or in a learning process, is via introducing random noise to the data. This noise is then “cleared out” via sophisticated aggregation methods which we are going to examine moving forward.</p>
</div>
<div id="S2.SS2.p7" class="ltx_para">
<p id="S2.SS2.p7.1" class="ltx_p">There are multiple variations of Differential Privacy due to the potential for interpretation. The two main types are Central D.P. and Local D.P. (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">6</a>]</cite>), which differ primarily in terms of who is responsible for the data. In the Central model, the data curator collects non-private data and applies a D.P. algorithm, requiring a trusted curator. Conversely, in the Local model, the data curator may be untrusted because users apply a specific protocol to perturb their own data.</p>
</div>
<div id="S2.SS2.p8" class="ltx_para">
<p id="S2.SS2.p8.1" class="ltx_p">In this paper, we are going to focus more on Local DP, because of its alignment with the decentralized tasks that we are dealing with.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Poisoning Attacks</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Federated Learning algorithms are robust for distributed learning, given the hypothesis that participating users are truthful and honest. However, in the sector of Security and in the spectre of this piece of research, this will not be the case, as we will focus on users with malicious intentions.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Specifically, the threat model for our attacks introduces users whose goal is to harm our models, in a targeted and predefined way as follows: they aim in misleading the model and try to cause it to misclassify a specific class as another <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">4</a>, <a href="#bib.bibx2" title="" class="ltx_ref">2</a>, <a href="#bib.bibx11" title="" class="ltx_ref">11</a>, <a href="#bib.bibx23" title="" class="ltx_ref">23</a>]</cite>. A valid example could be the case of image recognition, where attackers try to misclassify a specific type of image. If this is implemented in autonomous driving, some users could try to confuse the model by presenting images of bicycles as trunks, thus causing the car systems to malfunction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">We can see by the above example that this type of attack is a very serious one, and no one can guarantee that it will not happen, or that it can be controlled in a distributed scenario. This happens because each individual’s data is kept local and private, something that allows them to actively lie about their labels, without the centralized authority knowing that fact <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">2</a>]</cite>. Intuition could lead us to believe that a significant portion of the users have to collude to have a noticeable impact on the model, but as we will show in later sections, even a small percentage of malicious users can have an impact on the behaviour of the model.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">In the traditional Machine Learning setting it would be easier to detect such attacks as the central entity can access all the data that is used for training, thus catch such anomalies before the training phase. This has been attempted and succeeded in a sufficient level as shown in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">13</a>]</cite>. However, Federated Learning is meant to preserve the users’ privacy, thus the central authority must not have any kind of access to the dataset that each user utilizes to locally train the model.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p">Thus, the question that arises from this setting is: "How can we defend against an attacker that tries to inject faulty data into our model, if we never look at the data?". This is the problem that we will attempt to mitigate throughout this paper.</p>
</div>
<div id="S2.SS3.p6" class="ltx_para">
<p id="S2.SS3.p6.1" class="ltx_p">In our attempts, we are actually going to add to our arsenal a core setting of FL: data privacy techniques. We are thus going to investigate how we can utilize data privacy in our favor, in order to protect honest users and at the same time detect malicious ones whilst training our Federated Learning Models.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Defending against Data Poisoning Attacks</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Before diving into our own research, it is only right that we examine relevant research that is being conducted in the scope of the subset of interest of this paper. The majority of relevant defenses has been applied in the Centralized learning scenario, as the Federated one is rather new, and, as we will see moving forward, more difficult to defend against. The community has explored various methods to tackle the issue, which can broadly be categorized based on whether they involve the elimination of potentially malicious participants.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">Researchers developing algorithms for the first category focus on altering one or more of the layers of the model in order to “inject” the defense directly into the model and disallow attackers from poisoning the model. This can happen with several Privacy Enhancing Techniques, such as Differential Privacy, showcased in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">17</a>, <a href="#bib.bibx22" title="" class="ltx_ref">22</a>, <a href="#bib.bibx21" title="" class="ltx_ref">21</a>]</cite>, Homomorphic Encryption <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">5</a>]</cite>, Secure Multiparty Computation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p">As shown in the above-mentioned papers, the majority of algorithms succeed in detecting smaller percentages of malicious clients, but most of the times struggle to generalize when more attackers are present. This defense method is less invasive, but could introduce more computational overhead or harm the performance of the primary task, which is the model training.</p>
</div>
<div id="S2.SS4.p4" class="ltx_para">
<p id="S2.SS4.p4.1" class="ltx_p">The second category is the one that we are going to focus on and includes algorithms that try to detect anomalies in the training phase and eliminate users that create them <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">24</a>, <a href="#bib.bibx20" title="" class="ltx_ref">20</a>, <a href="#bib.bibx7" title="" class="ltx_ref">7</a>, <a href="#bib.bibx3" title="" class="ltx_ref">3</a>]</cite>. This type of defenses requires an extra step in the training process, namely an "anomaly detection algorithm", which is what each solution in that area tries to create. In theory, the more sophisticated the algorithm, the better. However, as seen in relevant literature, an extremely specific algorithm can create the equivalent of overfitting, thus not generalizing well in different and more diverse tasks.</p>
</div>
<div id="S2.SS4.p5" class="ltx_para">
<p id="S2.SS4.p5.1" class="ltx_p">All the above are tested against centralized ML models, while we were unable to find sufficient work in this subset of defenses when it comes to FL systems. A reason for that could be the young age of Federated Learning and relevant attacks in those models. Nevertheless, the main cause of absence of such defenses is the promise of FL for no extra data leakage that can link the user with their data, something that is a problem for implementing all the above papers in that scenario. In this paper, we aim in changing that, by introducing such a defense for FL, while at the same time respecting users’ privacy, by combining both of the above-mentioned techniques: adding an extra layer in our model and implementing an anomaly detection algorithm.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Poisoning Attacks against Federated Learning</h2>

<section id="S3.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Metrics used</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Throughout this paper we are going to use metrics that will allow us to better comprehend the security and the utility offered by the models that we are going to examine and poison with our experiments. Those metrics are defined above.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Sparse Categorical Accuracy</span>: Used to assess the accuracy of a model’s predictions by comparing the predicted class labels to the ground truth labels</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">CrossEntropy Loss</span>: measures the difference between the predicted probability distribution and the true probability distribution of the classes. In the context of our models, this metric will help us quantify how well the predicted probabilities match the actual class labels</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Source Class Recall</span>: The number of correct positive predictions that were made out of all positive predictions that could have been made by the model. When a dishonest user changes the labels of the data, the metric will drop, as fewer (to none) correct positive predictions will be made for the specific class the attacker is trying to poison.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Experiments results</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In this section, we delve into the experiments of Data Poisoning Attacks against Federated Learning. Our exploration is guided by the above-mentioned metrics, which will provide a comprehensive evaluation of the attack impact and, later on, the effectiveness of our defense strategies.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">We employ two widely recognized datasets, MNIST and CIFAR-10, as the basis for our experiments, leveraging their diverse and complex data. Our analyses and model implementations are conducted using the PyTorch library. The model architectures and hyper-parameter choices are available in the code accompanying this paper, as presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">We are going to train a convolutional neural network (CNN) model on the MNIST dataset for 30 epochs, and for the CIFAR dataset for 60 epochs, parameters that we are also going to use when we apply our defense mechanism. We are going to focus on the impact that the increasing presence of malicious users has in the behaviour and robustness of the model.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Impact in standard metrics.</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">The first metric we used is the Sparse Categorical Accuracy of the model. An initial observation is that the honest model’s accuracy follows the normal curve that we are used to seeing in central learning scenarios for both MNIST and CIFAR datasets. In general, the shape of the curves indicates well-trained models, with an appropriate learning rate, sound architecture, correct optimizers and good preprocessing of the data.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">When it comes to the implementation of the attack, a very interesting observation is that the overall accuracy of the models remain at satisfactory levels throughout our experiments. Even with a higher percentage of malicious users, the models still converge to better-than-average validation accuracy values, as even during the experiments with 50% of the users being malicious, the metric does not differ more than 10% from the honest models’, something that could be achieved with a model solely with honest users, because of minor issues in architecture or other differences in the ML pipeline.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2404.12778/assets/plots/acc_attack_mnist.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="419" height="314" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2404.12778/assets/plots/acc_attack_cifar.png" id="S3.F1.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="419" height="314" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Sparse Categorical Accuracy over the different percentages of malicious users present for MNIST (top) and CIFAR (bottom) datasets.</figcaption>
</figure>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.1" class="ltx_p">Therefore, it can be inferred that in a real-world scenario where a single model is trained, detecting a poisoned model solely using its testing accuracy becomes exceedingly challenging. To illustrate this, <em id="S3.SS2.SSS1.p3.1.1" class="ltx_emph ltx_font_italic">Figure 2</em> presents a side-by-side comparison of the accuracy results from two contrasting scenarios in our experiments: a fully honest model (displayed in the top graph) and a model with 50% malicious client participation (shown in the bottom graph). The comparison reveals that observing differences between these two cases is difficult through visual inspection of the accuracy metric alone, as both curves exhibit similar shapes and converge to comparable accuracy levels after a designated number of epochs.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2404.12778/assets/plots/compare_acccs_1.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="360" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Comparison of the accuracy curve for an honestly and a maliciously trained model</figcaption>
</figure>
<div id="S3.SS2.SSS1.p4" class="ltx_para">
<p id="S3.SS2.SSS1.p4.1" class="ltx_p">The next metric that we are going to comment on is the Crossentropy Loss that we gathered by evaluating our models with the test data. As we can see in <em id="S3.SS2.SSS1.p4.1.1" class="ltx_emph ltx_font_italic">Figure 3</em> below, again, every model follows the same curve, and with extremely small deviations from the honest model, even with 50% of the users being adversarial. However, as we will see later on, the ability to distinguish malicious participants (during the training process) due to their slightly higher produced loss, can be an interesting observation.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2404.12778/assets/plots/loss_attack_mnist.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="419" height="318" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2404.12778/assets/plots/loss_attack_cifar.png" id="S3.F3.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="419" height="319" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Crossentropy Loss over the different percentages of malicious users present for MNIST (top) and CIFAR (bottom) datasets.</figcaption>
</figure>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Impact in Source Class Recall.</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">The final metric that we gathered while training and evaluating our models was the Recall of the source class, i.e., the class that we attack with a goal to misclassify. It is clear from the graph shown in <em id="S3.SS2.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">Figure 4</em> that this metric represents our attack’s impact accurately.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2404.12778/assets/plots/recall_attack_mnist.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="479" height="364" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2404.12778/assets/plots/recall_attack_cifar.png" id="S3.F4.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="479" height="365" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Source Class Recall over the different percentages of malicious users present for MNIST (top) and CIFAR (bottom) datasets.</figcaption>
</figure>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p">In small percentages of the users being malicious (up to 20%), we can see that the recall curve is similar to the honest model’s one, especially for the MNIST dataset, which is considerably an easier learning task. However, when malicious users become more than 20% of the total clients, the metric struggles to surpass 0.4 for MNIST and 0.2 for CIFAR, which is an abnormal behaviour for regular training, something that successfully allows us to observe the poisoning attack. For extreme cases (i.e., half of the users being malicious) the metric struggles to get higher than 0.1 for both datasets, something that indicates the total misclassification of the source class, which was the objective of the attack in the first place.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.1" class="ltx_p">It is important to highlight that this metric derives from the evaluation of the test set, comprising solely honest labels, for each client. Thus, we are confident to make the observation that if the aggregating authority has access to an honest testing set for the dataset that the model is being trained on, then they could successfully discover a targeted poisoning attack, by computing a single class recall for every class of the task in question.</p>
</div>
<div id="S3.SS2.SSS2.p4" class="ltx_para">
<p id="S3.SS2.SSS2.p4.1" class="ltx_p">Given our above experiments, we can draw the conclusion that it is difficult for one to discover a Targeted Poisoning Attack in a federated scenario. The aggregator only has access to the weights each client returns and thus with the standard federated algorithms available, he must update the central model’s state by averaging all the weights that he receives.</p>
</div>
<div id="S3.SS2.SSS2.p5" class="ltx_para">
<p id="S3.SS2.SSS2.p5.1" class="ltx_p">Moreover, the aggregator is not able to detect the attack by utilizing the common metrics that are returned after evaluating the model, as the accuracy and the accumulative loss are not helpful in that direction. A metric that seems to help in that cause is the Recall of the source class of the attack, which produces considerably lower numbers when a high percentage of malicious clients are present.</p>
</div>
<div id="S3.SS2.SSS2.p6" class="ltx_para">
<p id="S3.SS2.SSS2.p6.1" class="ltx_p">Even when an attack is detected, pinpointing the specific users responsible for the poisoning remains infeasible due to the limited identifying data returned by each client. Of course, this is being done due to privacy concerns, as one of the key points of federated is for the clients participating to not be identifiable. It should be noted that, with the current algorithms available, even if the aggregator could identify the malicious clients, it would only be after their weights have been integrated into the global model. Consequently, this allows only for detection of the attack post-facto, rather than its prevention.</p>
</div>
<div id="S3.SS2.SSS2.p7" class="ltx_para">
<p id="S3.SS2.SSS2.p7.1" class="ltx_p">The objective of this paper is to address this challenge by developing methods to detect and defend against such attacks proactively, thereby preventing their impact on the global model. In the following section, we will present and discuss the defense mechanisms we have devised and implemented to achieve this goal.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Novel Algorithm for Defending against Poisoning Attacks</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In previous chapters we showcased the severity and impact that a targeted poisoning attack can have on a Federated Learning model. In this one, we are going to find a way to tackle it, with the end-goal of eliminating the users that try to poison our model.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Our approach to defending against these attacks adopts an innovative perspective, deviating from methodologies observed in the literature. During the literature review we observed the pattern of defense mechanisms that have been adopted by researchers in the field, which does not include the user reporting anything else than the gradients back to the aggregator. This is done due to privacy concerns of the user being identified by any other metadata that they may report.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">However, we opted to make the users return their training loss for their local training round. We assume that the training loss for users that act maliciously will behave differently than the honest ones, thus by aggregating this information we will be able to detect them and eliminate them from contributing to the training phase of the model. In later sections we will confirm that allegation by observing the behaviour of the model when this piece of information is utilized. To the best of our knowledge, as of February 2024, there is no published work utilizing loss metrics to distinguish between malicious and honest clients.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">It is clear that if a user reports the exact value of their training loss, this could prove catastrophic, as somebody could extract useful information regarding the instances that the user used for training, something that breaks the promise of the privacy offered to the users. In order to avoid that, we are going to utilize the foundations and the logic behind Local Differential Privacy, by injecting a random amount of noise every time a user reports their loss.</p>
</div>
<section id="S4.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Threat Model</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To formally describe the algorithm and the logistics of our defense solution, we must first describe the threat model under which we are operating.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">As we have already established, the attack scenario occurs in a Federated Learning context, where users have the responsibility of training a local model which is then communicated to a central authority in charge of aggregating an upgrading the global model with the gradients given by the users. Thus, the users are totally independent and decentralized, something that leads to the server not having any information about their training, other than the values reported by them. In our case, this information includes the weights shaped by the training, and the CrossEntropy Loss of the local model as a result of training the user’s dataset.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">We assume that the user reports a correct value for both above-mentioned elements, as this is crucial for our defense algorithm to function correctly. This can be easily ensured in a real world scenario, by the correct development of the framework, or by introducing cryptographic primitives that help in that context, such as Zero Knowledge Proofs or Commitment Schemes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">19</a>]</cite>, which of course add computational overhead, but at the same time ensure that a malicious user will not succeed in reporting a false value. This paper does not delve into the industrial implementation of the solution; therefore, the focus is not on the practical aspects of ensuring the reliability of user-reported data.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">We also assume that the server is not actively malicious and not colliding with malicious users, as an arbitrary acting aggregator could ignore the algorithm of the defense and only include malicious individuals in the global training step.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">When it comes to percentages of the participating users being actively malicious, there is no limit, as we are going to examine numbers ranging from 0% up to high percentages. However, as we have already seen, there is no point in raising the percentage higher than 40%, as it makes no difference to the already harmed model. Hence, we are going to assume the maximum percentage of malicious users participating in an FL training process as 40%, and point out that for higher numbers than those, the defense algorithm will work but will have worse results.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p">Finally, the definition of a "malicious user" expands as a device participating in the training procedure that is totally controlled by an adversary, who can view, alter labels of already existing instances, as well as insert new instances with new, false labels. This could be accomplished either by physical or remote access of the attacker to the victim’s device.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Hyper-parameters used</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To ensure transparency and provide clear insights into the methodology of our experiments, below we present <em id="S4.SS2.p1.1.1" class="ltx_emph ltx_font_italic">Table 1</em>, detailing the hyperparameters we employed. This matrix is designed to explain the choices made in tuning the model for both launching and defending against Poisoning Attacks in FL, for both of the datasets we are going to train our models against: MNIST and CIFAR-10.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Parameter</span></td>
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">MNIST</span></td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">CIFAR-10</span></td>
</tr>
<tr id="S4.T1.1.2.2" class="ltx_tr">
<td id="S4.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.1.2.2.1.1" class="ltx_text" style="font-size:80%;">Global training epochs</span></td>
<td id="S4.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.2.2.2.1" class="ltx_text" style="font-size:80%;">30</span></td>
<td id="S4.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.2.2.3.1" class="ltx_text" style="font-size:80%;">60</span></td>
</tr>
<tr id="S4.T1.1.3.3" class="ltx_tr">
<td id="S4.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.1.3.3.1.1" class="ltx_text" style="font-size:80%;">Number of training clients</span></td>
<td id="S4.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.3.3.2.1" class="ltx_text" style="font-size:80%;">50</span></td>
<td id="S4.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.3.3.3.1" class="ltx_text" style="font-size:80%;">100</span></td>
</tr>
<tr id="S4.T1.1.4.4" class="ltx_tr">
<td id="S4.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.1.4.4.1.1" class="ltx_text" style="font-size:80%;">Number of total clients</span></td>
<td id="S4.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.4.4.2.1" class="ltx_text" style="font-size:80%;">500</span></td>
<td id="S4.T1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.4.4.3.1" class="ltx_text" style="font-size:80%;">500</span></td>
</tr>
<tr id="S4.T1.1.5.5" class="ltx_tr">
<td id="S4.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.1.5.5.1.1" class="ltx_text" style="font-size:80%;">Client learning rate</span></td>
<td id="S4.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.5.5.2.1" class="ltx_text" style="font-size:80%;">0.01</span></td>
<td id="S4.T1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.5.5.3.1" class="ltx_text" style="font-size:80%;">0.001</span></td>
</tr>
<tr id="S4.T1.1.6.6" class="ltx_tr">
<td id="S4.T1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.1.6.6.1.1" class="ltx_text" style="font-size:80%;">Base Federated algorithm</span></td>
<td id="S4.T1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.6.6.2.1" class="ltx_text" style="font-size:80%;">FedAvg</span></td>
<td id="S4.T1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.6.6.3.1" class="ltx_text" style="font-size:80%;">FedAvg</span></td>
</tr>
<tr id="S4.T1.1.7.7" class="ltx_tr">
<td id="S4.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.1.7.7.1.1" class="ltx_text" style="font-size:80%;">Client training epochs</span></td>
<td id="S4.T1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.1.7.7.2.1" class="ltx_text" style="font-size:80%;">10</span></td>
<td id="S4.T1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.1.7.7.3.1" class="ltx_text" style="font-size:80%;">20</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Hyperparameters used in training and defending the Federated Learning model against Poisoning Attacks.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">When it comes to client selection, this is done randomly, based on the Gaussian distribution, which results in both clients that are selected to train in each round, and attackers selected to be random. However, during our experiments, the attackers are a fixed set of users, that does not change through the epochs, in order to better mimic the behaviour of real users. Each client holds a random, equally distributed subset of the training set.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">We should also note that due to the randomness introduced by the user selection and the Differential Privacy algorithms, the experiments were run multiple times (10 for each dataset), in order to cancel out any noise or extreme values that could be introduced by that uncertainty in the generated noise.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Novel Federated Learning Algorithm</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">From the introduction given, it is clear that some alterations to the FL training algorithm must be made for our defense idea to be implemented. In this section we are going to state in detail the way those alterations will result in a new FL algorithm.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Local Training Step.</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">The Local Training step involves individual users training a distributed model with their data, following specific protocols to ensure data privacy and model integrity. In this process, being carried out by all the users randomly selected to participate in a round of global training of the Federated Learning Model, the following steps are being carried out:</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">The client receives the local model from the centralized entity in charge of coordinating the FL procedure.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">The user relies on the hyperparameters decided and trains the local model with their data.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">During this process, the training loss is monitored, reflecting the updates made to the local model’s gradients.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">After completing the training process, the user locally adds to the training loss gathered a quantity of random noise generated by an already known distribution, with predefined bounds, that follows the foundations of Local Differential Privacy.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p">Finally, the user reports back to the server the gradients forming the updated version of the local model, as well as the loss value after the insertion of random noise, and nothing else that will help the centralized authority in recognizing or gathering extra information about the user.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Global Training Step.</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">In the Global Training step, the central server aggregates inputs from various users, applies a decision-making algorithm to identify and exclude potentially malicious contributions, and updates the global model accordingly. In our novel version of the Global Training Step, the following process is carried out:</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p">A random portion of the total users participating in the training procedure is selected for training in the specific round.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p">The global model from the last epoch of global training (or the initialized one if we are training for the first time) is sent to the selected users, where the above-mentioned local training algorithm is enforced.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p">The server receives as a tuple the updates from each one of the users participating in the specific round. The tuple includes the weights reported back and the training loss reported by the user.</p>
</div>
</li>
<li id="S4.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i4.p1" class="ltx_para">
<p id="S4.I2.i4.p1.1" class="ltx_p">The server gathers the losses in a data structure (as simple as a list), while keeping track of the correlation of each loss with the weights reported.</p>
</div>
</li>
<li id="S4.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i5.p1" class="ltx_para">
<p id="S4.I2.i5.p1.1" class="ltx_p">A specific elimination algorithm is utilized in order for the server to decide on the users that are going to be banned from the update process.</p>
</div>
</li>
<li id="S4.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i6.p1" class="ltx_para">
<p id="S4.I2.i6.p1.1" class="ltx_p">The clients whose loss do not meet the criteria set by that algorithm are eliminated from the global update of the model, and their identifiers are given to the aggregator in order to be excluded.</p>
</div>
</li>
<li id="S4.I2.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i7.p1" class="ltx_para">
<p id="S4.I2.i7.p1.1" class="ltx_p">The aggregator given the (predicted by the algorithm) honest users aggregates the global model by utilizing a previously decided algorithm, in the same manner as regular federated learning</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS3.SSS2.p3" class="ltx_para">
<p id="S4.SS3.SSS2.p3.1" class="ltx_p">In the course of this research, multiple algorithms were evaluated to identify the most effective method for eliminating potentially malicious users from the Federated Learning process. After extensive experimentation and analysis, one of the following algorithms emerged as significantly more successful than others. In this section we will focus on analyzing all the algorithms, as well as presenting the most successful one’s results and extracting conclusions based on them. The experimental results of other attempts are available in <em id="S4.SS3.SSS2.p3.1.1" class="ltx_emph ltx_font_italic">Appendix A</em>.</p>
</div>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Defense Algorithms</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">This section delves into various defense algorithms that will be evaluated for their effectiveness in defending against Data Poisoning Attacks in the Federated scenario.</p>
</div>
<section id="S4.SS4.SSS1" class="ltx_subsubsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Threshold-based eliminating.</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">The first, and most simple function consists of eliminating a certain percentage of the users. The clients are sorted based on the reported losses, and the last n% of them is being eliminated from the global training process. This approach is based on the premise that malicious users are likely to induce higher training losses, thus falling into the lower-performing segment of participants.</p>
</div>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Distance-based eliminating.</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">The next function is independent from a fixed percentage, and its goal is to detect the turning point in the sorted list of losses where the clients become malicious. This would function ideally if all the honest users reported significantly less loss than malicious ones, which, even based on our past experiments, cannot be guaranteed to occur consistently. However, this will be a point of observation made clear by using the percentage of correctly spotted attackers later, during our experiments.</p>
</div>
</section>
<section id="S4.SS4.SSS3" class="ltx_subsubsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3 </span>Statistical-based eliminating: Z-Score.</h4>

<div id="S4.SS4.SSS3.p1" class="ltx_para">
<p id="S4.SS4.SSS3.p1.2" class="ltx_p">The next function that we will consider is based on statistical observations, as it takes into account the distribution of the losses from each client. The Z-Score <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">25</a>]</cite>, has its roots in the theory of probability and statistics. It provides a measure of how far a given data point deviates from the mean, in terms of standard deviations. Mathematically, the Z-Score <math id="S4.SS4.SSS3.p1.1.m1.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S4.SS4.SSS3.p1.1.m1.1a"><mi id="S4.SS4.SSS3.p1.1.m1.1.1" xref="S4.SS4.SSS3.p1.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.p1.1.m1.1b"><ci id="S4.SS4.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS3.p1.1.m1.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.p1.1.m1.1c">z</annotation></semantics></math> for a data point <math id="S4.SS4.SSS3.p1.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS4.SSS3.p1.2.m2.1a"><mi id="S4.SS4.SSS3.p1.2.m2.1.1" xref="S4.SS4.SSS3.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.p1.2.m2.1b"><ci id="S4.SS4.SSS3.p1.2.m2.1.1.cmml" xref="S4.SS4.SSS3.p1.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.p1.2.m2.1c">x</annotation></semantics></math> is computed as:</p>
</div>
<div id="S4.SS4.SSS3.p2" class="ltx_para">
<table id="S7.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.Ex2.m1.1" class="ltx_Math" alttext="\displaystyle z=\frac{x-\mu}{\sigma}" display="inline"><semantics id="S4.Ex2.m1.1a"><mrow id="S4.Ex2.m1.1.1" xref="S4.Ex2.m1.1.1.cmml"><mi id="S4.Ex2.m1.1.1.2" xref="S4.Ex2.m1.1.1.2.cmml">z</mi><mo id="S4.Ex2.m1.1.1.1" xref="S4.Ex2.m1.1.1.1.cmml">=</mo><mstyle displaystyle="true" id="S4.Ex2.m1.1.1.3" xref="S4.Ex2.m1.1.1.3.cmml"><mfrac id="S4.Ex2.m1.1.1.3a" xref="S4.Ex2.m1.1.1.3.cmml"><mrow id="S4.Ex2.m1.1.1.3.2" xref="S4.Ex2.m1.1.1.3.2.cmml"><mi id="S4.Ex2.m1.1.1.3.2.2" xref="S4.Ex2.m1.1.1.3.2.2.cmml">x</mi><mo id="S4.Ex2.m1.1.1.3.2.1" xref="S4.Ex2.m1.1.1.3.2.1.cmml">−</mo><mi id="S4.Ex2.m1.1.1.3.2.3" xref="S4.Ex2.m1.1.1.3.2.3.cmml">μ</mi></mrow><mi id="S4.Ex2.m1.1.1.3.3" xref="S4.Ex2.m1.1.1.3.3.cmml">σ</mi></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex2.m1.1b"><apply id="S4.Ex2.m1.1.1.cmml" xref="S4.Ex2.m1.1.1"><eq id="S4.Ex2.m1.1.1.1.cmml" xref="S4.Ex2.m1.1.1.1"></eq><ci id="S4.Ex2.m1.1.1.2.cmml" xref="S4.Ex2.m1.1.1.2">𝑧</ci><apply id="S4.Ex2.m1.1.1.3.cmml" xref="S4.Ex2.m1.1.1.3"><divide id="S4.Ex2.m1.1.1.3.1.cmml" xref="S4.Ex2.m1.1.1.3"></divide><apply id="S4.Ex2.m1.1.1.3.2.cmml" xref="S4.Ex2.m1.1.1.3.2"><minus id="S4.Ex2.m1.1.1.3.2.1.cmml" xref="S4.Ex2.m1.1.1.3.2.1"></minus><ci id="S4.Ex2.m1.1.1.3.2.2.cmml" xref="S4.Ex2.m1.1.1.3.2.2">𝑥</ci><ci id="S4.Ex2.m1.1.1.3.2.3.cmml" xref="S4.Ex2.m1.1.1.3.2.3">𝜇</ci></apply><ci id="S4.Ex2.m1.1.1.3.3.cmml" xref="S4.Ex2.m1.1.1.3.3">𝜎</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex2.m1.1c">\displaystyle z=\frac{x-\mu}{\sigma}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS4.SSS3.p3" class="ltx_para">
<p id="S4.SS4.SSS3.p3.2" class="ltx_p">where <math id="S4.SS4.SSS3.p3.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S4.SS4.SSS3.p3.1.m1.1a"><mi id="S4.SS4.SSS3.p3.1.m1.1.1" xref="S4.SS4.SSS3.p3.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.p3.1.m1.1b"><ci id="S4.SS4.SSS3.p3.1.m1.1.1.cmml" xref="S4.SS4.SSS3.p3.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.p3.1.m1.1c">\mu</annotation></semantics></math> is the mean of the data and <math id="S4.SS4.SSS3.p3.2.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.SS4.SSS3.p3.2.m2.1a"><mi id="S4.SS4.SSS3.p3.2.m2.1.1" xref="S4.SS4.SSS3.p3.2.m2.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.p3.2.m2.1b"><ci id="S4.SS4.SSS3.p3.2.m2.1.1.cmml" xref="S4.SS4.SSS3.p3.2.m2.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.p3.2.m2.1c">\sigma</annotation></semantics></math> is the standard deviation.</p>
</div>
<div id="S4.SS4.SSS3.p4" class="ltx_para">
<p id="S4.SS4.SSS3.p4.1" class="ltx_p">The underlying assumption of the Z-Score method is the Central Limit Theorem, as stated in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">25</a>]</cite>, which posits that the sum of a large number of independent and identically distributed variables will be approximately normally distributed, regardless of the original distribution of the variables. Thus, in scenarios where the majority of the data (in our case, the reported training losses) follows a normal distribution, data points that significantly deviate from the mean become statistically notable.</p>
</div>
<div id="S4.SS4.SSS3.p5" class="ltx_para">
<p id="S4.SS4.SSS3.p5.1" class="ltx_p">For our purposes, if the absolute Z-Score of a client’s training loss exceeds a predefined threshold, which we are going to set to 1 for 68% confidence, the client is flagged as an outlier. This criterion is based on the empirical rule which states that for a normal distribution, about 68% of the data falls within one standard deviations from the mean.</p>
</div>
<div id="S4.SS4.SSS3.p6" class="ltx_para">
<p id="S4.SS4.SSS3.p6.1" class="ltx_p">In applying the Z-Score method to the context of our research, we hypothesize that training losses deviating significantly from the mean are indicative of malicious behavior.</p>
</div>
</section>
<section id="S4.SS4.SSS4" class="ltx_subsubsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.4 </span>Clustering-based eliminating: K-Means.</h4>

<div id="S4.SS4.SSS4.p1" class="ltx_para">
<p id="S4.SS4.SSS4.p1.1" class="ltx_p">K-means clustering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite> is a type of unsupervised ML algorithm that partitions a dataset into <math id="S4.SS4.SSS4.p1.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S4.SS4.SSS4.p1.1.m1.1a"><mi id="S4.SS4.SSS4.p1.1.m1.1.1" xref="S4.SS4.SSS4.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS4.p1.1.m1.1b"><ci id="S4.SS4.SSS4.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS4.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS4.p1.1.m1.1c">K</annotation></semantics></math> distinct, non-overlapping clusters. The goal of the algorithm is to minimize the variance within each cluster and maximize the variance between the clusters. To achieve that, it defines clusters such that the total intra-cluster variation, or the sum of squared distances (based on the Euclidean Distance) from the mean of the cluster, is minimized. Despite its simplicity, the K-means algorithm can be very effective and robust, especially when the structure of the data is well-defined and can be roughly easily distinctive.</p>
</div>
<div id="S4.SS4.SSS4.p2" class="ltx_para">
<p id="S4.SS4.SSS4.p2.1" class="ltx_p">In the context of our solution, we aim to leverage the K-means technique to distinguish between honest and malicious clients based on their training losses, by defining two clusters in which the two types of clients will fall, with an end-goal of categorizing them correctly.</p>
</div>
<div id="S4.SS4.SSS4.p3" class="ltx_para">
<p id="S4.SS4.SSS4.p3.1" class="ltx_p">Given the scope of this paper, a detailed exploration of the specific algorithms and mechanics of K-means clustering is beyond our purview. Readers who are interested may refer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">14</a>]</cite> for an in-depth analysis.</p>
</div>
</section>
</section>
<section id="S4.SS5" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Insertion of Local Differential Privacy</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">We already mentioned that giving a centralized entity direct access to metadata produced by users while training is breaking the promise of Federated Learning regarding the privacy of the participants. Given the exact value of the training loss, an adversary could make discoveries regarding the data distribution and the data points that each user holds and thus deanonymize the user. However, a small alteration of this reported loss value could solve the problem, as the user will no longer be identifiable by it.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">As we have seen in previous chapters, a simple yet robust tool to protect individual data is Local Differential Privacy (LDP), a solution that can be adopted by each user in order to anonymize their participation. Specifically, LDP pertains to introducing randomness at the individual data level before any aggregation or computation by a central authority takes place. In this setting, each individual’s data is perturbed in a way that provides a certain privacy guarantee, represented by an epsilon value. The smaller the <math id="S4.SS5.p2.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S4.SS5.p2.1.m1.1a"><mi id="S4.SS5.p2.1.m1.1.1" xref="S4.SS5.p2.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.1.m1.1b"><ci id="S4.SS5.p2.1.m1.1.1.cmml" xref="S4.SS5.p2.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.1.m1.1c">\epsilon</annotation></semantics></math>, the stronger the privacy guarantee, as we have already seen in our introduction.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">In our case, we opt to use the Laplace mechanism for our task, in order to add noise from the Laplace distribution to the data. The scale of the injected noise is determined by the desired <math id="S4.SS5.p3.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S4.SS5.p3.1.m1.1a"><mi id="S4.SS5.p3.1.m1.1.1" xref="S4.SS5.p3.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p3.1.m1.1b"><ci id="S4.SS5.p3.1.m1.1.1.cmml" xref="S4.SS5.p3.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p3.1.m1.1c">\epsilon</annotation></semantics></math> value and the sensitivity of the function.</p>
</div>
<section id="S4.SS5.SSS1" class="ltx_subsubsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.1 </span>Defining the Sensitivity.</h4>

<div id="S4.SS5.SSS1.p1" class="ltx_para">
<p id="S4.SS5.SSS1.p1.1" class="ltx_p">The sensitivity, <math id="S4.SS5.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\Delta f" display="inline"><semantics id="S4.SS5.SSS1.p1.1.m1.1a"><mrow id="S4.SS5.SSS1.p1.1.m1.1.1" xref="S4.SS5.SSS1.p1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S4.SS5.SSS1.p1.1.m1.1.1.2" xref="S4.SS5.SSS1.p1.1.m1.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.SS5.SSS1.p1.1.m1.1.1.1" xref="S4.SS5.SSS1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS5.SSS1.p1.1.m1.1.1.3" xref="S4.SS5.SSS1.p1.1.m1.1.1.3.cmml">f</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS1.p1.1.m1.1b"><apply id="S4.SS5.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS5.SSS1.p1.1.m1.1.1"><times id="S4.SS5.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS5.SSS1.p1.1.m1.1.1.1"></times><ci id="S4.SS5.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS5.SSS1.p1.1.m1.1.1.2">Δ</ci><ci id="S4.SS5.SSS1.p1.1.m1.1.1.3.cmml" xref="S4.SS5.SSS1.p1.1.m1.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS1.p1.1.m1.1c">\Delta f</annotation></semantics></math>, is the maximum amount the output can change by altering a single item in the dataset. Given our data, which are float values representing losses and not distinctive values, the sensitivity should be redefined based on the context.</p>
</div>
<div id="S4.SS5.SSS1.p2" class="ltx_para">
<p id="S4.SS5.SSS1.p2.1" class="ltx_p">In our distributions, the data represents measurements which we are trying to ensure that their perturbation will not drastically change their interpretation by the central entity, and thus the sensitivity is deemed as the smallest change that we want the server to be able to distinguish. Given our previous knowledge from conducting experiments, along with the standard ML pipeline and the reported losses for the datasets we will utilize, we can safely say that even a change of 0.001 can alter the data in a severe way. Therefore, we have determined the sensitivity to be <math id="S4.SS5.SSS1.p2.1.m1.1" class="ltx_Math" alttext="\Delta f=0.0001" display="inline"><semantics id="S4.SS5.SSS1.p2.1.m1.1a"><mrow id="S4.SS5.SSS1.p2.1.m1.1.1" xref="S4.SS5.SSS1.p2.1.m1.1.1.cmml"><mrow id="S4.SS5.SSS1.p2.1.m1.1.1.2" xref="S4.SS5.SSS1.p2.1.m1.1.1.2.cmml"><mi mathvariant="normal" id="S4.SS5.SSS1.p2.1.m1.1.1.2.2" xref="S4.SS5.SSS1.p2.1.m1.1.1.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.SS5.SSS1.p2.1.m1.1.1.2.1" xref="S4.SS5.SSS1.p2.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.SS5.SSS1.p2.1.m1.1.1.2.3" xref="S4.SS5.SSS1.p2.1.m1.1.1.2.3.cmml">f</mi></mrow><mo id="S4.SS5.SSS1.p2.1.m1.1.1.1" xref="S4.SS5.SSS1.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS5.SSS1.p2.1.m1.1.1.3" xref="S4.SS5.SSS1.p2.1.m1.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS1.p2.1.m1.1b"><apply id="S4.SS5.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS5.SSS1.p2.1.m1.1.1"><eq id="S4.SS5.SSS1.p2.1.m1.1.1.1.cmml" xref="S4.SS5.SSS1.p2.1.m1.1.1.1"></eq><apply id="S4.SS5.SSS1.p2.1.m1.1.1.2.cmml" xref="S4.SS5.SSS1.p2.1.m1.1.1.2"><times id="S4.SS5.SSS1.p2.1.m1.1.1.2.1.cmml" xref="S4.SS5.SSS1.p2.1.m1.1.1.2.1"></times><ci id="S4.SS5.SSS1.p2.1.m1.1.1.2.2.cmml" xref="S4.SS5.SSS1.p2.1.m1.1.1.2.2">Δ</ci><ci id="S4.SS5.SSS1.p2.1.m1.1.1.2.3.cmml" xref="S4.SS5.SSS1.p2.1.m1.1.1.2.3">𝑓</ci></apply><cn type="float" id="S4.SS5.SSS1.p2.1.m1.1.1.3.cmml" xref="S4.SS5.SSS1.p2.1.m1.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS1.p2.1.m1.1c">\Delta f=0.0001</annotation></semantics></math> based on our data and experimental insights.</p>
</div>
</section>
<section id="S4.SS5.SSS2" class="ltx_subsubsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.2 </span>Defining the epsilon value.</h4>

<div id="S4.SS5.SSS2.p1" class="ltx_para">
<p id="S4.SS5.SSS2.p1.1" class="ltx_p">The final parameter we must decide on is the epsilon value, which is responsible for balancing the utility and privacy offered to our users. The largest the epsilon value, the lower the privacy guarantees, but at the same time, the higher the utility offered. A fair balance that is commonly used is to set <math id="S4.SS5.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\epsilon=1" display="inline"><semantics id="S4.SS5.SSS2.p1.1.m1.1a"><mrow id="S4.SS5.SSS2.p1.1.m1.1.1" xref="S4.SS5.SSS2.p1.1.m1.1.1.cmml"><mi id="S4.SS5.SSS2.p1.1.m1.1.1.2" xref="S4.SS5.SSS2.p1.1.m1.1.1.2.cmml">ϵ</mi><mo id="S4.SS5.SSS2.p1.1.m1.1.1.1" xref="S4.SS5.SSS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS5.SSS2.p1.1.m1.1.1.3" xref="S4.SS5.SSS2.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS2.p1.1.m1.1b"><apply id="S4.SS5.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS5.SSS2.p1.1.m1.1.1"><eq id="S4.SS5.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS5.SSS2.p1.1.m1.1.1.1"></eq><ci id="S4.SS5.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS5.SSS2.p1.1.m1.1.1.2">italic-ϵ</ci><cn type="integer" id="S4.SS5.SSS2.p1.1.m1.1.1.3.cmml" xref="S4.SS5.SSS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS2.p1.1.m1.1c">\epsilon=1</annotation></semantics></math>. However, we must note that this should not be absolute, and each one creating an LDP algorithm for a similar task could alter it depending on their needs.</p>
</div>
</section>
<section id="S4.SS5.SSS3" class="ltx_subsubsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.3 </span>Defining the LDP algorithm.</h4>

<div id="S4.SS5.SSS3.p1" class="ltx_para">
<p id="S4.SS5.SSS3.p1.3" class="ltx_p">Thus, given a function <math id="S4.SS5.SSS3.p1.1.m1.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S4.SS5.SSS3.p1.1.m1.1a"><mi id="S4.SS5.SSS3.p1.1.m1.1.1" xref="S4.SS5.SSS3.p1.1.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS3.p1.1.m1.1b"><ci id="S4.SS5.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS5.SSS3.p1.1.m1.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS3.p1.1.m1.1c">f</annotation></semantics></math> with sensitivity <math id="S4.SS5.SSS3.p1.2.m2.1" class="ltx_Math" alttext="\Delta f" display="inline"><semantics id="S4.SS5.SSS3.p1.2.m2.1a"><mrow id="S4.SS5.SSS3.p1.2.m2.1.1" xref="S4.SS5.SSS3.p1.2.m2.1.1.cmml"><mi mathvariant="normal" id="S4.SS5.SSS3.p1.2.m2.1.1.2" xref="S4.SS5.SSS3.p1.2.m2.1.1.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.SS5.SSS3.p1.2.m2.1.1.1" xref="S4.SS5.SSS3.p1.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS5.SSS3.p1.2.m2.1.1.3" xref="S4.SS5.SSS3.p1.2.m2.1.1.3.cmml">f</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS3.p1.2.m2.1b"><apply id="S4.SS5.SSS3.p1.2.m2.1.1.cmml" xref="S4.SS5.SSS3.p1.2.m2.1.1"><times id="S4.SS5.SSS3.p1.2.m2.1.1.1.cmml" xref="S4.SS5.SSS3.p1.2.m2.1.1.1"></times><ci id="S4.SS5.SSS3.p1.2.m2.1.1.2.cmml" xref="S4.SS5.SSS3.p1.2.m2.1.1.2">Δ</ci><ci id="S4.SS5.SSS3.p1.2.m2.1.1.3.cmml" xref="S4.SS5.SSS3.p1.2.m2.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS3.p1.2.m2.1c">\Delta f</annotation></semantics></math>, the scale <math id="S4.SS5.SSS3.p1.3.m3.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S4.SS5.SSS3.p1.3.m3.1a"><mi id="S4.SS5.SSS3.p1.3.m3.1.1" xref="S4.SS5.SSS3.p1.3.m3.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS3.p1.3.m3.1b"><ci id="S4.SS5.SSS3.p1.3.m3.1.1.cmml" xref="S4.SS5.SSS3.p1.3.m3.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS3.p1.3.m3.1c">b</annotation></semantics></math> for the Laplace distribution is defined as:</p>
</div>
<div id="S4.SS5.SSS3.p2" class="ltx_para">
<table id="S7.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.Ex3.m1.1" class="ltx_Math" alttext="\displaystyle b=\frac{\Delta f}{\epsilon}" display="inline"><semantics id="S4.Ex3.m1.1a"><mrow id="S4.Ex3.m1.1.1" xref="S4.Ex3.m1.1.1.cmml"><mi id="S4.Ex3.m1.1.1.2" xref="S4.Ex3.m1.1.1.2.cmml">b</mi><mo id="S4.Ex3.m1.1.1.1" xref="S4.Ex3.m1.1.1.1.cmml">=</mo><mstyle displaystyle="true" id="S4.Ex3.m1.1.1.3" xref="S4.Ex3.m1.1.1.3.cmml"><mfrac id="S4.Ex3.m1.1.1.3a" xref="S4.Ex3.m1.1.1.3.cmml"><mrow id="S4.Ex3.m1.1.1.3.2" xref="S4.Ex3.m1.1.1.3.2.cmml"><mi mathvariant="normal" id="S4.Ex3.m1.1.1.3.2.2" xref="S4.Ex3.m1.1.1.3.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.Ex3.m1.1.1.3.2.1" xref="S4.Ex3.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.Ex3.m1.1.1.3.2.3" xref="S4.Ex3.m1.1.1.3.2.3.cmml">f</mi></mrow><mi id="S4.Ex3.m1.1.1.3.3" xref="S4.Ex3.m1.1.1.3.3.cmml">ϵ</mi></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex3.m1.1b"><apply id="S4.Ex3.m1.1.1.cmml" xref="S4.Ex3.m1.1.1"><eq id="S4.Ex3.m1.1.1.1.cmml" xref="S4.Ex3.m1.1.1.1"></eq><ci id="S4.Ex3.m1.1.1.2.cmml" xref="S4.Ex3.m1.1.1.2">𝑏</ci><apply id="S4.Ex3.m1.1.1.3.cmml" xref="S4.Ex3.m1.1.1.3"><divide id="S4.Ex3.m1.1.1.3.1.cmml" xref="S4.Ex3.m1.1.1.3"></divide><apply id="S4.Ex3.m1.1.1.3.2.cmml" xref="S4.Ex3.m1.1.1.3.2"><times id="S4.Ex3.m1.1.1.3.2.1.cmml" xref="S4.Ex3.m1.1.1.3.2.1"></times><ci id="S4.Ex3.m1.1.1.3.2.2.cmml" xref="S4.Ex3.m1.1.1.3.2.2">Δ</ci><ci id="S4.Ex3.m1.1.1.3.2.3.cmml" xref="S4.Ex3.m1.1.1.3.2.3">𝑓</ci></apply><ci id="S4.Ex3.m1.1.1.3.3.cmml" xref="S4.Ex3.m1.1.1.3.3">italic-ϵ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex3.m1.1c">\displaystyle b=\frac{\Delta f}{\epsilon}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we present the results from experiments conducted while applying our defense solution during the training of a Federated Learning model, in scenarios where different percentages of malicious users are present and perform a targeted Data Poisoning Attack.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">In our exploration, the four above-mentioned different defense algorithms were tested for their efficacy against targeted Poisoning Attacks in a Federated Learning context. While all four algorithms provided valuable insights, this section will focus primarily on presenting the results of the most effective algorithm, namely the approach utilizing K-Means. Detailed results and analyses of the other three algorithms have been included in the appendix for reference. This approach allows us to highlight the most impactful findings while making comprehensive data available for further review.</p>
</div>
<section id="S5.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Metrics used</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Before conducting the experiments, we must define, as we did in previous chapters, the metrics that are going to be utilized to extract information about the attack and the effect of the defense. Those metrics are:</p>
</div>
<section id="S5.SS1.SSS1" class="ltx_subsubsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Already-used metrics.</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.p1.1" class="ltx_p">The classic indicators that we have already seen while launching the attacks, namely the <em id="S5.SS1.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">Sparse Categorical Accuracy</em>, the <em id="S5.SS1.SSS1.p1.1.2" class="ltx_emph ltx_font_italic">CrossEntropy Loss</em> and the <em id="S5.SS1.SSS1.p1.1.3" class="ltx_emph ltx_font_italic">Recall of the source class</em> of the attack. Given our findings in previous sections, which indicated that the first two were not so useful in detecting poisoning attacks, we are mainly going to focus on the last one and compare the Recall of the source class when the defense is present and absent.</p>
</div>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Attacker-centric metrics.</h4>

<div id="S5.SS1.SSS2.p1" class="ltx_para">
<p id="S5.SS1.SSS2.p1.1" class="ltx_p">In our solution we strive to strike an optimal balance between security and utility during the model training process. This leads us to rating our solution based on metrics that are focused on the attackers’ detection, namely the <em id="S5.SS1.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">Accuracy</em> and the <em id="S5.SS1.SSS2.p1.1.2" class="ltx_emph ltx_font_italic">F1 score</em> as applied to to malicious users’ prediction by the clustering algorithm. As a result, we are going to present the mean value that those two metrics will have during training for each different experiment.</p>
</div>
<div id="S5.SS1.SSS2.p2" class="ltx_para">
<p id="S5.SS1.SSS2.p2.1" class="ltx_p">Those measurements are essential for our work, as their low values would indicate a compromise either in the security or in the utility aspect of our experiments. For example, if the algorithm eliminated 90% of the users, this could be beneficial for the global model, as no attackers would be present at all, but at the same time it would severely harm its utility, as honest users would be eliminated from the process. Thus, those two indicators will help us confirm that our proposal does not harm the model’s training phase, while also preserving the integrity of the users used in this step.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Results on model performance</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The first plots that we observe in <em id="S5.SS2.p1.1.1" class="ltx_emph ltx_font_italic">Figures 5 and 6</em>, demonstrate the metrics that we have used throughout this paper in order to compare the success of poisoning.</p>
</div>
<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2404.12778/assets/plots/accs_defense_mnist_kmeans.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="479" height="359" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2404.12778/assets/plots/acc_defense_cifar_kmeans.png" id="S5.F5.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="419" height="314" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Accuracy over the different percentages of malicious users present for MNIST (top) and CIFAR (bottom) datasets.</figcaption>
</figure>
<figure id="S5.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2404.12778/assets/plots/rec_defense_mnist_kmeans.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="538" height="404" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2404.12778/assets/plots/recall_defense_cifar_kmeans.png" id="S5.F6.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="479" height="365" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Source Class Recall over the different percentages of malicious users present for MNIST (top) and CIFAR (bottom) datasets.</figcaption>
</figure>
<figure id="S5.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2404.12778/assets/plots/comparison_defense_mnist_kmeans.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="509" height="398" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2404.12778/assets/plots/comparison_defense_kmeans_cifar.png" id="S5.F7.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="509" height="369" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Comparing Source Class Recall with and without the defense mechanism for MNIST (top) and CIFAR (bottom) datasets.</figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Remarkably, even with 40% of users being malicious, the implementation of our elimination-oriented defense results in negligible deviation from the performance of the original, honest model. A small difference was always present in the accuracy function, but the most encouraging behaviour is observed in the Source Class Recall metric, where all five models behave similarly, from the beginning of training up until the last epoch. This is a strong indication that the K-means algorithm is able to detect the vast majority of malicious users starting even from the <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="1^{st}" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><msup id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><mn id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml">1</mn><mrow id="S5.SS2.p2.1.m1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.3.cmml"><mi id="S5.SS2.p2.1.m1.1.1.3.2" xref="S5.SS2.p2.1.m1.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.1.m1.1.1.3.1" xref="S5.SS2.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S5.SS2.p2.1.m1.1.1.3.3" xref="S5.SS2.p2.1.m1.1.1.3.3.cmml">t</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">superscript</csymbol><cn type="integer" id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2">1</cn><apply id="S5.SS2.p2.1.m1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3"><times id="S5.SS2.p2.1.m1.1.1.3.1.cmml" xref="S5.SS2.p2.1.m1.1.1.3.1"></times><ci id="S5.SS2.p2.1.m1.1.1.3.2.cmml" xref="S5.SS2.p2.1.m1.1.1.3.2">𝑠</ci><ci id="S5.SS2.p2.1.m1.1.1.3.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">1^{st}</annotation></semantics></math> epoch. Of course, in the extreme case of 40% of the clients being malicious, despite the similar shape of the Recall curves, we observe a slight decrease in the values of the metric, especially in the CIFAR dataset, which is a rather more complex dataset when it comes to learning.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">However, as seen in <em id="S5.SS2.p3.1.1" class="ltx_emph ltx_font_italic">Figure 7</em> below, which presents the values of the Source Class Recall when our defense is present and absent, we can clearly state that even for 40% of the clients being malicious there is a significant improvement in the metric, as when there is no defense applied, it struggles to surpass very low standards for both of our datasets, which is definitely not the case for when we enforce our defense mechanism.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">These results indicate that the models we trained under our defense algorithm perform similarly to their honestly trained counterparts for each dataset. This leads us to believe that during the training phase, our defense mechanism effectively identified and eliminated users exhibiting anomalously high differentially private training loss. Consequently, this maintained the model in a sufficiently honest state, allowing it to yield the above-shown metrics. From the perspective of model utility, this is a highly favorable outcome, as it demonstrates our algorithm’s ability to safely train a Federated Learning model in environments with potential attackers, ensuring both efficiency and robustness in the model’s test performance.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Results on attacker detection</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">However, our exploration does not end here. The next crucial aspect of our research involves investigating the attacker-specific metrics. Our objective is to confirm that our algorithm not only accurately identifies malicious participants but also maintains a satisfactory F1 score for that task. This focus ensures that while malicious users are reliably eliminated, the contribution of honest users is preserved, optimizing the utility of their data in the training process. The above metrics are drawn as the mean value throughout the range of epochs of training the model, and thus clustering the users as malicious or honest.</p>
</div>
<figure id="S5.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2404.12778/assets/plots/attackers_accuracy.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="419" height="324" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2404.12778/assets/plots/attackers_f1.png" id="S5.F8.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="419" height="324" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Accuracy (top) and F1 Score (bottom) for the task of classifying malicious users as such during training.</figcaption>
</figure>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">As we can see in <em id="S5.SS3.p2.1.1" class="ltx_emph ltx_font_italic">Figure 8</em>, the detection algorithm does a good job in detecting the presence of malicious clients both in MNIST and in CIFAR datasets.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">The top graph presents the attacker detection accuracy for two datasets, against the percentage of malicious users present. For the MNIST dataset, the detection accuracy remains relatively high and consistent, only showing a marginal decrease as the proportion of malicious users increases to 40%. For the CIFAR dataset we observe a more pronounced decline as the percentage of malicious users grows more than 30%, suggesting that the complexity of the dataset may insert additional challenges in accurately identifying malicious users’ presence.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p">The bottom graph showcases the F1 score of the task of detecting malicious users, again in comparison with the percentage of the actual percentage of attackers in our model. This metric further underscores the performance of our defense strategy, due to its identity, namely balancing the precision and recall of our detection algorithm. As illustrated in the graph, the MNIST dataset maintains a robust F1 score across all levels of attacker presence, with only a slight decrease even as the percentage of malicious users reaches 40%. At the same time, the CIFAR dataset’s F1 score reveals a larger decline for the maximum value of malicious users percentage in our experiments, indicating a reduction in the balance between precision and recall.</p>
</div>
<div id="S5.SS3.p5" class="ltx_para">
<p id="S5.SS3.p5.1" class="ltx_p">The common ground for both datasets is that they behave satisfyingly and similarly for malicious users presence up to 30%, while declining after that, which is anticipated, as we have previously observed during the model performance experiments. However, when almost half of the users are malicious, the prediction continues to produce a satisfying accuracy for each of the datasets (over 55%), something that confirms the success of our algorithm to detect such users even under extreme circumstances in difficult learning scenarios, such as the CIFAR dataset.</p>
</div>
<div id="S5.SS3.p6" class="ltx_para">
<p id="S5.SS3.p6.1" class="ltx_p">Finally, an important observation while considering and comparing both experiments, is the following: despite the lower accuracy and F1 score produced for the task of attacker detection in the case of 40% malicious user participation in the difficult task of predicting the CIFAR dataset, the model behaviour when the defense is present (our primary task), is still of high quality. We can observe that from the fact that the Source Class Recall produced is almost equal to the level of the experiment with 10% malicious user presence.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion and Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Our research was focused on exploring the emerging field of Federated Learning and a possible set of attacks against it, namely Data Poisoning Attacks. We began by launching several such attacks against FL models and observing their behaviour and their impact in the model through detailed experiments, while defining key metrics that would help us tackle them. We found out that without harming the model’s accuracy, they can actually alter the predictions, especially if the volume of malicious users is over 20%.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Upon discovering the subtle yet significant impact of malicious users, our research pivoted towards deploying a defense mechanism that preserves the foundations of FL: accuracy and privacy. We proceeded in observing that although the overall loss reported is not altered, the users with malicious intentions tend to report a higher loss value than the honest ones.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">The breakthrough of this paper is that we took advantage of this reported loss by each user in order to predict the malicious ones and eliminate them from the training procedure. We did that while being able to respect the participants’ privacy, one of the core promises of Federated Learning.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">We found an elegant and robust way to do so, by introducing an extra layer of Local Differential Privacy based on the LaPlace distribution before the users reported their loss back to the server. We then combined the idea of predicting and eliminating malicious users with the privacy layer and came up with a clustering-based algorithm in order to classify users as attackers. We presented experiments that showcased how well we succeeded in this task, in two well-know datasets in the field of Machine Learning, namely MNIST and CIFAR. We conducted several experiments to prove the success of our solution, both regarding model behaviour, and correct user elimination. By doing so, we are positive that this method is a valid and robust defense against label Data Poisoning FL models.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">Since we introduced a new way for defending against Poisoning Attacks, there is definitely space for future work that could be conducted. The goal of this scientific work was not to provide an exhaustive way of solutions for classifying users as malicious or honest, but rather to introduce the innovative above-mentioned method of eliminating users based on their loss. We did actually provide a successful way of carrying out the classification task with the K-Means algorithm, but there are many directions that could be further explored.</p>
</div>
<div id="S6.p6" class="ltx_para">
<p id="S6.p6.1" class="ltx_p">Finally, given that our defense is not specific or tight to any model dataset or learning scenario, this mechanism could be used to defend against different type of attacks in alternate architectures, with the prerequisite that the attacks increase the reported loss during training.</p>
</div>
<section id="S6.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Acknowledgments</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">This work would not have been possible if not for some people offering their insights and knowledge of the field. I would like to thank Professor Emiliano De Cristofaro of University of California, Riverside, my MSc Thesis supervisor at UCL, for introducing me to the world of Federated Learning, and for pushing me to my limits when it comes to innovation and ideas for this extremely interesting subject. Moreover, I would like to thank Professor Lorenzo Cavallaro of UCL, who closely monitored my research and provided me with ideas and guidance throughout my experiments and the publication of this paper. I would also like to thank all the people in the Federated Learning and in the Privacy community, with whom I have had great discussions over the past two years. Last but not least, I would like to thank my current company, Canonical, for allowing me to pursue my research interests, and supporting me throughout this journey, while trying to make the world more open-source and more secure.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">Rakesh Agrawal and Ramakrishnan Srikant
</span>
<span class="ltx_bibblock">“Privacy-preserving data mining”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2000 ACM SIGMOD international conference on Management of data</em>, SIGMOD ’00
</span>
<span class="ltx_bibblock">New York, NY, USA: Association for Computing Machinery, 2000, pp. 439–450
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1145/342009.335438" title="" class="ltx_ref ltx_href">10.1145/342009.335438</a>
</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">Eugene Bagdasaryan et al.
</span>
<span class="ltx_bibblock">“How To Backdoor Federated Learning” arXiv:1807.00459 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2019
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.48550/arXiv.1807.00459" title="" class="ltx_ref ltx_href">10.48550/arXiv.1807.00459</a>
</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">Marco Barreno, Blaine Nelson, Anthony D. Joseph and J.. Tygar
</span>
<span class="ltx_bibblock">“The security of machine learning”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx3.1.1" class="ltx_emph ltx_font_italic">Machine Learning</em> <span id="bib.bibx3.2.2" class="ltx_text ltx_font_bold">81.2</span>, 2010, pp. 121–148
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1007/s10994-010-5188-5" title="" class="ltx_ref ltx_href">10.1007/s10994-010-5188-5</a>
</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">Xinyun Chen et al.
</span>
<span class="ltx_bibblock">“Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning” arXiv:1712.05526 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2017
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.48550/arXiv.1712.05526" title="" class="ltx_ref ltx_href">10.48550/arXiv.1712.05526</a>
</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">Yao Chen et al.
</span>
<span class="ltx_bibblock">“Federated Learning Attacks and Defenses: A Survey” arXiv:2211.14952 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2022
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.48550/arXiv.2211.14952" title="" class="ltx_ref ltx_href">10.48550/arXiv.2211.14952</a>
</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">Graham Cormode et al.
</span>
<span class="ltx_bibblock">“Privacy at Scale: Local Differential Privacy in Practice”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 International Conference on Management of Data</em>, SIGMOD ’18
</span>
<span class="ltx_bibblock">New York, NY, USA: Association for Computing Machinery, 2018, pp. 1655–1658
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1145/3183713.3197390" title="" class="ltx_ref ltx_href">10.1145/3183713.3197390</a>
</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">Gabriela F. Cretu et al.
</span>
<span class="ltx_bibblock">“Casting out Demons: Sanitizing Training Data for Anomaly Sensors” ISSN: 2375-1207
</span>
<span class="ltx_bibblock">In <em id="bib.bibx7.1.1" class="ltx_emph ltx_font_italic">2008 IEEE Symposium on Security and Privacy (sp 2008)</em>, 2008, pp. 81–95
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1109/SP.2008.11" title="" class="ltx_ref ltx_href">10.1109/SP.2008.11</a>
</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">Irit Dinur and Kobbi Nissim
</span>
<span class="ltx_bibblock">“Revealing information while preserving privacy”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the twenty-second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems</em>, PODS ’03
</span>
<span class="ltx_bibblock">New York, NY, USA: Association for Computing Machinery, 2003, pp. 202–210
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1145/773153.773173" title="" class="ltx_ref ltx_href">10.1145/773153.773173</a>
</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">Cynthia Dwork and Kobbi Nissim
</span>
<span class="ltx_bibblock">“Privacy-Preserving Datamining on Vertically Partitioned Databases”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx9.1.1" class="ltx_emph ltx_font_italic">Advances in Cryptology – CRYPTO 2004</em>, Lecture Notes in Computer Science
</span>
<span class="ltx_bibblock">Berlin, Heidelberg: Springer, 2004, pp. 528–544
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1007/978-3-540-28628-8_32" title="" class="ltx_ref ltx_href">10.1007/978-3-540-28628-8_32</a>
</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">Cynthia Dwork and Aaron Roth
</span>
<span class="ltx_bibblock">“The Algorithmic Foundations of Differential Privacy”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx10.1.1" class="ltx_emph ltx_font_italic">Foundations and Trends® in Theoretical Computer Science</em> <span id="bib.bibx10.2.2" class="ltx_text ltx_font_bold">9.3–4</span>, 2014, pp. 211–407
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1561/0400000042" title="" class="ltx_ref ltx_href">10.1561/0400000042</a>
</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">Minghong Fang, Xiaoyu Cao, Jinyuan Jia and Neil Zhenqiang Gong
</span>
<span class="ltx_bibblock">“Local Model Poisoning Attacks to Byzantine-Robust Federated Learning” arXiv:1911.11815 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2021
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.48550/arXiv.1911.11815" title="" class="ltx_ref ltx_href">10.48550/arXiv.1911.11815</a>
</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">Nick Galanis
</span>
<span class="ltx_bibblock">“Defending agaist Poisoning Attacks in FL (code repository)”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx12.1.1" class="ltx_emph ltx_font_italic">GitHub</em>
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://github.com/nikosgalanis/data-poisoning-defense-fl" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/nikosgalanis/data-poisoning-defense-fl</a>
</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">Micah Goldblum et al.
</span>
<span class="ltx_bibblock">“Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses” Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence
</span>
<span class="ltx_bibblock">In <em id="bib.bibx13.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</em> <span id="bib.bibx13.2.2" class="ltx_text ltx_font_bold">45.2</span>, 2023, pp. 1563–1580
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1109/TPAMI.2022.3162397" title="" class="ltx_ref ltx_href">10.1109/TPAMI.2022.3162397</a>
</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">J.. Hartigan and M.. Wong
</span>
<span class="ltx_bibblock">“Algorithm AS 136: A K-Means Clustering Algorithm”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx14.1.1" class="ltx_emph ltx_font_italic">Applied Statistics</em> <span id="bib.bibx14.2.2" class="ltx_text ltx_font_bold">28.1</span>, 1979, pp. 100
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.2307/2346830" title="" class="ltx_ref ltx_href">10.2307/2346830</a>
</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">Ji Liu et al.
</span>
<span class="ltx_bibblock">“From distributed machine learning to federated learning: a survey”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx15.1.1" class="ltx_emph ltx_font_italic">Knowledge and Information Systems</em> <span id="bib.bibx15.2.2" class="ltx_text ltx_font_bold">64.4</span>, 2022, pp. 885–917
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1007/s10115-022-01664-x" title="" class="ltx_ref ltx_href">10.1007/s10115-022-01664-x</a>
</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">H. McMahan et al.
</span>
<span class="ltx_bibblock">“Communication-Efficient Learning of Deep Networks from Decentralized Data” arXiv:1602.05629 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2023
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.48550/arXiv.1602.05629" title="" class="ltx_ref ltx_href">10.48550/arXiv.1602.05629</a>
</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">Mohammad Naseri, Jamie Hayes and Emiliano De Cristofaro
</span>
<span class="ltx_bibblock">“Local and Central Differential Privacy for Robustness and Privacy in Federated Learning” arXiv:2009.03561 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2022
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2009.03561" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2009.03561</a>
</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">Naman Patel, Prashanth Krishnamurthy, Siddharth Garg and Farshad Khorrami
</span>
<span class="ltx_bibblock">“Bait and Switch: Online Training Data Poisoning of Autonomous Driving Systems” arXiv:2011.04065 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2020
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2011.04065" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2011.04065</a>
</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">C.. Schnorr
</span>
<span class="ltx_bibblock">“Efficient Identification and Signatures for Smart Cards”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx19.1.1" class="ltx_emph ltx_font_italic">Advances in Cryptology — CRYPTO’ 89 Proceedings</em>, Lecture Notes in Computer Science
</span>
<span class="ltx_bibblock">New York, NY: Springer, 1990, pp. 239–252
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1007/0-387-34805-0_22" title="" class="ltx_ref ltx_href">10.1007/0-387-34805-0_22</a>
</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">Octavian Suciu et al.
</span>
<span class="ltx_bibblock">“When Does Machine Learning {FAIL}? Generalized Transferability for Evasion and Poisoning Attacks”, 2018, pp. 1299–1316
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://www.usenix.org/conference/usenixsecurity18/presentation/suciu" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.usenix.org/conference/usenixsecurity18/presentation/suciu</a>
</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh and H. McMahan
</span>
<span class="ltx_bibblock">“Can You Really Backdoor Federated Learning?” arXiv:1911.07963 [cs, stat]
</span>
<span class="ltx_bibblock">arXiv, 2019
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.48550/arXiv.1911.07963" title="" class="ltx_ref ltx_href">10.48550/arXiv.1911.07963</a>
</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">Chandra Thapa, M… Chamikara, Seyit Camtepe and Lichao Sun
</span>
<span class="ltx_bibblock">“SplitFed: When Federated Learning Meets Split Learning” arXiv:2004.12088 [cs]
</span>
<span class="ltx_bibblock">arXiv, 2022
</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/2004.12088" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2004.12088</a>
</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy and Ling Liu
</span>
<span class="ltx_bibblock">“Data Poisoning Attacks Against Federated Learning Systems” arXiv:2007.08432 [cs, stat]
</span>
<span class="ltx_bibblock">arXiv, 2020
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.48550/arXiv.2007.08432" title="" class="ltx_ref ltx_href">10.48550/arXiv.2007.08432</a>
</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">Brandon Tran, Jerry Li and Aleksander Madry
</span>
<span class="ltx_bibblock">“Spectral Signatures in Backdoor Attacks” arXiv:1811.00636 [cs, stat]
</span>
<span class="ltx_bibblock">arXiv, 2018
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.48550/arXiv.1811.00636" title="" class="ltx_ref ltx_href">10.48550/arXiv.1811.00636</a>
</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">Robert S. Witte and John S. Witte
</span>
<span class="ltx_bibblock">“Statistics” Google-Books-ID: KcxjDwAAQBAJ
</span>
<span class="ltx_bibblock">John Wiley &amp; Sons, 2017
</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">Chen Zhang et al.
</span>
<span class="ltx_bibblock">“A survey on federated learning”
</span>
<span class="ltx_bibblock">In <em id="bib.bibx26.1.1" class="ltx_emph ltx_font_italic">Knowledge-Based Systems</em> <span id="bib.bibx26.2.2" class="ltx_text ltx_font_bold">216</span>, 2021, pp. 106775
</span>
<span class="ltx_bibblock">DOI: <a target="_blank" href="https://dx.doi.org/10.1016/j.knosys.2021.106775" title="" class="ltx_ref ltx_href">10.1016/j.knosys.2021.106775</a>
</span>
</li>
</ul>
</section>
<section id="S7" class="ltx_section ltx_indent_first">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Appendix</h2>

<section id="S7.SS1" class="ltx_subsection ltx_indent_first">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Experiment results for other defense algorithms</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">In this appendix we will present the relevant experiments for the rest of the defense algorithms, that did not produce the expected results when it comes to either accuracy in attacker detection, or to quantitative metrics for the performance of the model. Due to the poor performance of some of those algorithms, we will only present their results when trained on the MNIST dataset.</p>
</div>
<section id="S7.SS1.SSS1" class="ltx_subsubsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.1 </span>Fixed Percentage Algorithm Results.</h4>

<div id="S7.SS1.SSS1.p1" class="ltx_para">
<p id="S7.SS1.SSS1.p1.1" class="ltx_p">We observe that he Source Class Recall metric performs very similarly to the honest model for even 40% of the users being malicious. Those high numbers of the Source Class Recall are a clear indication that our defense mechanism performs as expected and eliminates malicious users from training. This was a given for small percentages of the users being malicious, but the fact that it performs the same way and produces similar results for high percentage of poisoning proves that when a fixed threshold is applied, the vast majority of malicious users are detected.</p>
</div>
<figure id="S7.F9" class="ltx_figure"><img src="/html/2404.12778/assets/appendix_plots/rec_many_fix_percentage.png" id="S7.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="314" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Source Class Recall for different percentages of the datasets being poisoned with the Fixed Percentage Algorithm present</figcaption>
</figure>
<figure id="S7.F10" class="ltx_figure"><img src="/html/2404.12778/assets/appendix_plots/attackers_fixed_percentage.png" id="S7.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="314" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Attackers detection ratio by the Fixed Percentage Algorithm</figcaption>
</figure>
<div id="S7.SS1.SSS1.p2" class="ltx_para">
<p id="S7.SS1.SSS1.p2.1" class="ltx_p">Additionally, when it comes to attacker detection, we can observe <em id="S7.SS1.SSS1.p2.1.1" class="ltx_emph ltx_font_italic">Figure 12</em> that indeed the defense algorithm introduces a great increase in this metric in comparison with the original poisoning attack, for every one of our experiments with increasing malicious users’ percentages.</p>
</div>
<div id="S7.SS1.SSS1.p3" class="ltx_para">
<p id="S7.SS1.SSS1.p3.1" class="ltx_p">To conclude, this method has some advantages but does not lack disadvantages. Without prior knowledge of the threshold of users that must be eliminated, that is almost impossible in a real-world scenario, the algorithm does not balance well in the scale of security and utility of the model. We saw that for low percentages of poisoning, when the threshold is high, it does not manage to produce good enough accuracy for the global FL model. Moreover it is safe to assume that for high percentages of malicious users, a low threshold would fail to detect all of them and would therefore not solve the problem that we are trying to defend against.</p>
</div>
</section>
<section id="S7.SS1.SSS2" class="ltx_subsubsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.2 </span>Largest Difference Algorithm Results.</h4>

<div id="S7.SS1.SSS2.p1" class="ltx_para">
<p id="S7.SS1.SSS2.p1.1" class="ltx_p">In the relevant graphs, we can observe the failure of the algorithm with regards to the Source Class Recall metric, which reports low results for 40% of the users being attackers. It indeed provides us with better curves for less attackers, something that is a good indicator for the performance of our algorithm for a reasonable number of attackers.</p>
</div>
<figure id="S7.F11" class="ltx_figure"><img src="/html/2404.12778/assets/appendix_plots/rec_many_largest_diff.png" id="S7.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="314" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Source Class Recall for different percentages of the datasets being poisoned with the Largest Difference Algorithm present</figcaption>
</figure>
<figure id="S7.F12" class="ltx_figure"><img src="/html/2404.12778/assets/appendix_plots/attackers_largest_diff.png" id="S7.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="314" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Attackers detection ratio by the Largest Difference Algorithm</figcaption>
</figure>
<div id="S7.SS1.SSS2.p2" class="ltx_para">
<p id="S7.SS1.SSS2.p2.1" class="ltx_p">The attackers are well enough spotted, as we can see in Figure 12, but decline as the percentage of them gets higher, something that checks out with the results that we presented earlier.</p>
</div>
<div id="S7.SS1.SSS2.p3" class="ltx_para">
<p id="S7.SS1.SSS2.p3.1" class="ltx_p">Thus, the Largest Difference algorithm seems to function as expected for low percentages of malicious clients, while failing to detect the majority of the users when this percentage rises to 40%. When it comes to implementing in real-world application, the algorithm has the advantage of not requiring any prior knowledge, as the only arguments given are the losses and the total number of training clients, that is already known to the model. Moreover, in most cases it is extreme to assume that almost half of the clients selected each round for training will be malicious, hence this algorithm could prove useful for cases with less attackers, given its simplicity and its speed, as it as a linear complexity and does not introduce an extra computational overhead.</p>
</div>
</section>
<section id="S7.SS1.SSS3" class="ltx_subsubsection ltx_indent_first">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.3 </span>Z-Score Algorithm Results.</h4>

<div id="S7.SS1.SSS3.p1" class="ltx_para">
<p id="S7.SS1.SSS3.p1.1" class="ltx_p">When taking a look at the Source Class Recall metric for this algorithm, we can confirm the good response to our training when the defense technique based on eliminating the users using this statistical measure, as the Recall after 30 rounds of training converges to high standards, above 0.8, even for 30% of the users being malicious. For higher percentages, the results are not what we would like, however they were expected as we mentioned in the above theoretical foundations of the algorithm.</p>
</div>
<figure id="S7.F13" class="ltx_figure"><img src="/html/2404.12778/assets/appendix_plots/rec_many_zscore.png" id="S7.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="314" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Source Class Recall for different percentages of the datasets being poisoned with the the Z-Score Algorithm present</figcaption>
</figure>
<figure id="S7.F14" class="ltx_figure"><img src="/html/2404.12778/assets/appendix_plots/attackers_zscore.png" id="S7.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="314" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Attackers detection ratio by the Z-Score Algorithm</figcaption>
</figure>
<div id="S7.SS1.SSS3.p2" class="ltx_para">
<p id="S7.SS1.SSS3.p2.1" class="ltx_p">The above <em id="S7.SS1.SSS3.p2.1.1" class="ltx_emph ltx_font_italic">Figure 14</em> also confirms the correct classification of attackers, as we can see that their detection rate is high for the 3 first experiments, while it fails to keep up those numbers for higher percentages where it falls below 0.4.</p>
</div>
<div id="S7.SS1.SSS3.p3" class="ltx_para">
<p id="S7.SS1.SSS3.p3.1" class="ltx_p">To conclude, this method is deemed as extremely useful given the prior knowledge that there is a reasonable amount of poisoning in the model. In most real-world applications, it is safe to assume that 30% or less of the users will be malicious, which makes this algorithm excellent to use, as it does not pose an extra computational overhead and can successfully classify attackers as such.</p>
</div>
<div id="S7.SS1.SSS3.p4" class="ltx_para">
<p id="S7.SS1.SSS3.p4.1" class="ltx_p">All the above algorithms presented, despite having their own scenarios where they perform well, they do not produce similarly good results for every case, something that is possible with the K-means clustering algorithm. Because of this, and due to the similar computational complexity and overhead, we conclude in promoting the K-means clustering algorithm for the task of classifying malicious users as such.</p>
</div>
</section>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.12777" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.12778" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.12778">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.12778" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.12779" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 17:50:51 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
