<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2102.00655] Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning</title><meta property="og:description" content="Data heterogeneity has been identified as one of the key features in federated learning but often overlooked in the lens of robustness to adversarial attacks. This paper focuses on characterizing and understanding its â€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2102.00655">

<!--Generated on Fri Mar  1 14:57:05 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Curse or Redemption? How Data Heterogeneity Affects 
<br class="ltx_break">the Robustness of Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Syed Zawad, <sup id="id1.1.id1" class="ltx_sup">1</sup>
Ahsan Ali, <sup id="id2.2.id2" class="ltx_sup">1</sup>
Pin-Yu Chen, <sup id="id3.3.id3" class="ltx_sup">2</sup>
Ali Anwar, <sup id="id4.4.id4" class="ltx_sup">2</sup>
Yi Zhou, <sup id="id5.5.id5" class="ltx_sup">2</sup>
Nathalie Baracaldo, <sup id="id6.6.id6" class="ltx_sup">2</sup> 
<br class="ltx_break">Yuan Tian, <sup id="id7.7.id7" class="ltx_sup">3</sup>
Feng Yan <sup id="id8.8.id8" class="ltx_sup">1</sup> 
<br class="ltx_break">
</span></span>
</div>

<h1 class="ltx_title ltx_title_document">Curse or Redemption? How Data Heterogeneity Affects 
<br class="ltx_break">the Robustness of Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Syed Zawad, <sup id="id1.1.id1" class="ltx_sup">1</sup>
Ahsan Ali, <sup id="id2.2.id2" class="ltx_sup">1</sup>
Pin-Yu Chen, <sup id="id3.3.id3" class="ltx_sup">2</sup>
Ali Anwar, <sup id="id4.4.id4" class="ltx_sup">2</sup>
Yi Zhou, <sup id="id5.5.id5" class="ltx_sup">2</sup>
Nathalie Baracaldo, <sup id="id6.6.id6" class="ltx_sup">2</sup> 
<br class="ltx_break">Yuan Tian, <sup id="id7.7.id7" class="ltx_sup">3</sup>
Feng Yan <sup id="id8.8.id8" class="ltx_sup">1</sup> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">Data heterogeneity has been identified as one of the key features in federated learning but often overlooked in the lens of robustness to adversarial attacks. This paper focuses on characterizing and understanding its impact on backdooring attacks in federated learning through comprehensive experiments using synthetic and the LEAF benchmarks. The initial impression driven by our experimental results suggests that data heterogeneity is the dominant factor in the effectiveness of attacks and it may be a redemption for defending against backdooring as it makes the attack less efficient, more challenging to design effective attack strategies, and the attack result also becomes less predictable. However, with further investigations, we found data heterogeneity is more of a curse than a redemption as the attack effectiveness can be significantly boosted by simply adjusting the client-side backdooring timing. More importantly,data heterogeneity may result in overfitting at the local training of benign clients, which can be utilized by attackers to disguise themselves and fool skewed-feature based defenses. In addition, effective attack strategies can be made by adjusting attack data distribution. Finally, we discuss the potential directions of defending the curses brought by data heterogeneity. The results and lessons learned from our extensive experiments and analysis offer new insights for designing robust federated learning methods and systems.</p>
</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Introduction</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Federated Learning (FL) is widely successful in training machine learning (ML) models collaboratively across clients without sharing private dataÂ <cite class="ltx_cite ltx_citemacro_citep">(McMahan etÂ al. <a href="#bib.bib17" title="" class="ltx_ref">2016</a>; Zhao etÂ al. <a href="#bib.bib26" title="" class="ltx_ref">2018</a>; Bonawitz etÂ al. <a href="#bib.bib4" title="" class="ltx_ref">2019</a>)</cite>.
In FL, models are trained locally at clients to preserve data privacy and the trained model weights are sent to a central server for aggregation to update the global model. During the aggregation, privacy mechanisms such as differential privacyÂ <cite class="ltx_cite ltx_citemacro_citep">(Abadi etÂ al. <a href="#bib.bib1" title="" class="ltx_ref">2016</a>)</cite> and secure aggregationÂ <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz etÂ al. <a href="#bib.bib5" title="" class="ltx_ref">2017</a>)</cite> are often employed to strengthen the privacy.
There are two types of poisoning attacks: performance degradation attacks where the goal of the adversary is to reduce the accuracy/F1 scores of the model (such as Byzantine attacks) and backdoor attacks aiming at creating targeted misclassifications without affecting the overall performance on the main tasksÂ <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al. <a href="#bib.bib9" title="" class="ltx_ref">2017</a>; Xie etÂ al. <a href="#bib.bib24" title="" class="ltx_ref">2019</a>; Bagdasaryan etÂ al. <a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>.
Defending against such attacks usually requires complete control of the training process or monitoring the training dataÂ <cite class="ltx_cite ltx_citemacro_citep">(Steinhardt, Koh, and Liang <a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite>, which is challenging in FL due to the privacy requirements.
In this paper, we choose the popular and sophisticated backdoor attacks as an example for our study.
Although some work exists to defend against backdoor attacks, including activation clusteringÂ <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al. <a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite> and k-means clusteringÂ <cite class="ltx_cite ltx_citemacro_citep">(Shen, Tople, and Saxena <a href="#bib.bib19" title="" class="ltx_ref">2016</a>)</cite>, these approaches require access to the training data making them inapplicable for FL settings.
Some attack strategies tailored for FL have also been studied including sybil attacksÂ <cite class="ltx_cite ltx_citemacro_citep">(Fung, Yoon, and Beschastnikh <a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite>, model replacementÂ <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan etÂ al. <a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>, GANs based attacksÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al. <a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite>, and distributed attacksÂ <cite class="ltx_cite ltx_citemacro_citep">(Xie etÂ al. <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite>.
However, a comprehensive study on the effectiveness of backdoor attacks under a variety of data distribution among parties remains at unexplored.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">The training data in FL is generated by clients and thus heterogeneous inherentlyÂ <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz etÂ al. <a href="#bib.bib4" title="" class="ltx_ref">2019</a>; Chai etÂ al. <a href="#bib.bib7" title="" class="ltx_ref">2020</a>; Zhao etÂ al. <a href="#bib.bib26" title="" class="ltx_ref">2018</a>; Sattler etÂ al. <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>.
As the training is conducted locally at each client, the data cannot be balanced nor monitored like in conventional data-centralized or distributed ML.
Such uncontrollable and severe data heterogeneity is one of the key challenges of FL as it is rarely seen in conventional ML.
Despite its uniqueness and importance, data heterogeneity has been largely overlooked through the lens of robustness to backdoor attacks. Existing FL backdoor attacks either assume IID training data distribution among clients or only conduct a simplified study on non-IID dataÂ <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan etÂ al. <a href="#bib.bib2" title="" class="ltx_ref">2018</a>; Bhagoji etÂ al. <a href="#bib.bib3" title="" class="ltx_ref">2019</a>; Xie etÂ al. <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite>. None of them provides a comprehensive study nor understanding on how data heterogeneity impacts the backdoor attacks and defenses.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">In this paper, we focus on quantifying and understanding the implications brought by data heterogeneity in FL backdoor attacks through extensive empirical experiments and comprehensive analysis.</p>
</div>
<div id="Sx1.p4" class="ltx_para">
<p id="Sx1.p4.1" class="ltx_p">We define <span id="Sx1.p4.1.1" class="ltx_text ltx_font_italic">Heterogeneity Index</span> to quantify the extent of heterogeneity in training data.
From our initial investigation driven by both synthetic and the practical LEAF benchmarkÂ <cite class="ltx_cite ltx_citemacro_citep">(Caldas etÂ al. <a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>, we surprisingly found that data heterogeneity seems to be a redemption for defending against backdoor attacks.
<span id="Sx1.p4.1.2" class="ltx_text ltx_font_bold">Redemption 1</span>: the <span id="Sx1.p4.1.3" class="ltx_text ltx_font_italic">attack effectiveness</span> (usually measured as Attack Success Rate or ASR) reduces sharply when the heterogeneity of training data increases.
<span id="Sx1.p4.1.4" class="ltx_text ltx_font_bold">Redemption 2</span>: we found the malicious data distribution is an overlooked important factor when defining an attack strategy given the training data is heterogeneous. A poor selection of malicious data distribution can result in poor attack effectiveness.
<span id="Sx1.p4.1.5" class="ltx_text ltx_font_bold">Redemption 3</span>: we further discovered that malicious data distribution plays as a dominant factor in the effectiveness of backdooring. E.g., contrary to the common belief in existing works that higher <span id="Sx1.p4.1.6" class="ltx_text ltx_font_italic">attack scale</span> (defined as the number of compromised clients) and <span id="Sx1.p4.1.7" class="ltx_text ltx_font_italic">local attack budget</span> (defined as the quantity of backdoored data per client) always lead to higher attack effectiveness, our study demonstrates that this is not always the case as malicious data distribution often outperforms the impact of attack scale/budget.
This discovery indicates that data heterogeneity makes the design of effective attack strategies more challenging as the attack effectiveness is less correlated to the straightforward attack scale/budget but rather the less intuitive malicious data distribution.</p>
</div>
<div id="Sx1.p5" class="ltx_para">
<p id="Sx1.p5.1" class="ltx_p">Further investigations, however, reveal that data heterogeneity actually brings curses for the robustness of FL. <span id="Sx1.p5.1.1" class="ltx_text ltx_font_bold">Curse 1</span>: data heterogeneity makes the client-side training very sensitive to the backdoor attack timing. With a proper attack timing, e.g., at the last local batch, the effectiveness of attack can be significantly boosted with only a fraction of attack budget.
<span id="Sx1.p5.1.2" class="ltx_text ltx_font_bold">Curse 2</span>: whatâ€™s worse is that data heterogeneity makes the most promising skewed-feature based defense strategies such as cosine similarity fall short. Such defending method detects compromised clients by realizing their features are more overfitted than the benign clients. However, with data heterogeneity, benign clients may also have overfitted features that look similar to those of compromised clients. This allows the backdoor attackers to disguise themselves and fool the skewed-feature checking.
<span id="Sx1.p5.1.3" class="ltx_text ltx_font_bold">Curse 3</span>: more effective attack strategies can be derived by making the backdoor clientsâ€™ data distribution close to the overall data distribution with the help of distribution distance measures such as the Chi-Square statistics.
To defend these curses brought by data heterogeneity, we discuss how existing defense mechanisms fit here and the potential directions on data-heterogeneity aware defending strategies.</p>
</div>
<div id="Sx1.p6" class="ltx_para">
<p id="Sx1.p6.1" class="ltx_p">In summary, our empirical experimental studies show that data heterogeneity appears to be a redemption for the robustness of FL as it makes the attack less effective and more challenging to design good attack strategies.
However, our further investigations reveal that data heterogeneity also brings several curses for FL backdooring as it is harder to detect and the attack effectiveness can be significantly boosted by adjusting the local attack timing and malicious data distribution. The defending strategies we propose help alleviate these curses.
The results and lessons learned from our thorough experiments and comprehensive analysis offer new insights for designing robust FL methods and systems.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Related Works</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p"><span id="Sx2.p1.1.1" class="ltx_text ltx_font_bold">Data Heterogeneity in Federated Learning.</span>
While data heterogeneity is not new in the ML, the extent of data heterogeneity is much more prevalent in FL compared to data centralized learningÂ <cite class="ltx_cite ltx_citemacro_citep">(Chai etÂ al. <a href="#bib.bib7" title="" class="ltx_ref">2020</a>; Li etÂ al. <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>.Â <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al. <a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite> theoretically demonstrates the bounds on convergence due to heterogeneity, whileÂ <cite class="ltx_cite ltx_citemacro_citep">(Sattler etÂ al. <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> providing empirical results on how changing heterogeneity affects model performance. Â <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al. <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite> discusses the challenges of heterogeneity for FL andÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al. <a href="#bib.bib26" title="" class="ltx_ref">2018</a>)</cite> demonstrates how the clientsâ€™ local model weights diverge due to data heterogeneity.</p>
</div>
<div id="Sx2.p2" class="ltx_para">
<p id="Sx2.p2.1" class="ltx_p"><span id="Sx2.p2.1.1" class="ltx_text ltx_font_bold">Backdoor Attack.</span>
Backdoor attacks for deep learning models are presented inÂ <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al. <a href="#bib.bib9" title="" class="ltx_ref">2017</a>)</cite>, where an adversary can insert a pattern in a few training samples from a source class and relabel them to a target class, causing a targeted missclassification.
One of the earlier papersÂ <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan etÂ al. <a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite> proposes the <span id="Sx2.p2.1.2" class="ltx_text ltx_font_italic">model replacement</span> technique, whereby they eventually replace the global model with a backdoored model stealthily.Â <cite class="ltx_cite ltx_citemacro_citep">(Bhagoji etÂ al. <a href="#bib.bib3" title="" class="ltx_ref">2019</a>)</cite> demonstrates that boosting model weights can help attackers and shows that FL is highly susceptible to backdoor attacks.Â <cite class="ltx_cite ltx_citemacro_citep">(Fung, Yoon, and Beschastnikh <a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite> introduces sybil attacks in the context of FL using label-flipping and backdooring.Â <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al. <a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite> uses GANs to attack the global model, whileÂ <cite class="ltx_cite ltx_citemacro_citep">(Xie etÂ al. <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite> takes a different approach by focusing on decentralized, colluding attackers, and creating efficient trigger patterns. Our paper takes a different angle by focusing on analyzing the impact of data heterogeneity on attack effectiveness. This subject is rarely studied even though data heterogeneity is a critical aspect of FL.</p>
</div>
<div id="Sx2.p3" class="ltx_para">
<p id="Sx2.p3.1" class="ltx_p"><span id="Sx2.p3.1.1" class="ltx_text ltx_font_bold">Backdoor Defense.</span>
There have been various proposals to defend DNN from susceptible adversarial attacks such as filtering techniquesÂ <cite class="ltx_cite ltx_citemacro_citep">(Steinhardt, Koh, and Liang <a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite> and fine-pruningÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu, Dolan-Gavitt, and Garg <a href="#bib.bib15" title="" class="ltx_ref">2018</a>)</cite>, but are mainly focused on traditional data-centralized ML methods. Clustering techniques specifically for FL are proposed inÂ <cite class="ltx_cite ltx_citemacro_citep">(Tran, Li, and Madry <a href="#bib.bib22" title="" class="ltx_ref">2018</a>; Chen etÂ al. <a href="#bib.bib8" title="" class="ltx_ref">2018</a>; Shen, Tople, and Saxena <a href="#bib.bib19" title="" class="ltx_ref">2016</a>)</cite> and in <cite class="ltx_cite ltx_citemacro_citep">(Fung, Yoon, and Beschastnikh <a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite>, FoolsGold is proposed to defend against sybil attacks by using cosine similarities.
<cite class="ltx_cite ltx_citemacro_citep">(Ma, Zhu, and Hsu <a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite> proposes defending with differential privacy without compromising user confidentiality.
The authors ofÂ <cite class="ltx_cite ltx_citemacro_citep">(Sun etÂ al. <a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite> extend this by demonstrating weak differential privacy and norm-clipping mitigate attacks, but do not provide any strong defense mechanisms.
None of these defenses explore defending effectiveness under various extent of data heterogeneity.</p>
</div>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Experiment Setups for FL Backdooring</h2>

<figure id="Sx3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Training Setup.</figcaption>
<div id="Sx3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:429.1pt;height:81.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-37.9pt,7.2pt) scale(0.85,0.85) ;">
<table id="Sx3.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx3.T1.1.1.1.1" class="ltx_tr">
<th id="Sx3.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="Sx3.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="Sx3.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx3.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="Sx3.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx3.T1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Train/Test split</span></th>
<th id="Sx3.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="Sx3.T1.1.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx3.T1.1.1.1.1.4.1.1" class="ltx_tr">
<td id="Sx3.T1.1.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T1.1.1.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold">Clients</span></td>
</tr>
<tr id="Sx3.T1.1.1.1.1.4.1.2" class="ltx_tr">
<td id="Sx3.T1.1.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T1.1.1.1.1.4.1.2.1.1" class="ltx_text ltx_font_bold">Total/Per Round</span></td>
</tr>
</table>
</th>
<th id="Sx3.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="Sx3.T1.1.1.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx3.T1.1.1.1.1.5.1.1" class="ltx_tr">
<td id="Sx3.T1.1.1.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T1.1.1.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold">Learning Rate</span></td>
</tr>
<tr id="Sx3.T1.1.1.1.1.5.1.2" class="ltx_tr">
<td id="Sx3.T1.1.1.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T1.1.1.1.1.5.1.2.1.1" class="ltx_text ltx_font_bold">/Batch Size</span></td>
</tr>
</table>
</th>
<th id="Sx3.T1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="Sx3.T1.1.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="Sx3.T1.1.1.1.1.6.1.1" class="ltx_tr">
<td id="Sx3.T1.1.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T1.1.1.1.1.6.1.1.1.1" class="ltx_text ltx_font_bold">Local Epochs/</span></td>
</tr>
<tr id="Sx3.T1.1.1.1.1.6.1.2" class="ltx_tr">
<td id="Sx3.T1.1.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="Sx3.T1.1.1.1.1.6.1.2.1.1" class="ltx_text ltx_font_bold">Total Rounds</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx3.T1.1.1.2.1" class="ltx_tr">
<td id="Sx3.T1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FEMNIST</td>
<td id="Sx3.T1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2 conv 2 dense</td>
<td id="Sx3.T1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49,644/4,964</td>
<td id="Sx3.T1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">179/17</td>
<td id="Sx3.T1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.004/10</td>
<td id="Sx3.T1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1/2000</td>
</tr>
<tr id="Sx3.T1.1.1.3.2" class="ltx_tr">
<td id="Sx3.T1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Sent140</td>
<td id="Sx3.T1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">100 cell lstm 2 dense</td>
<td id="Sx3.T1.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6,553/655</td>
<td id="Sx3.T1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50/10</td>
<td id="Sx3.T1.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0003/4</td>
<td id="Sx3.T1.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1/10</td>
</tr>
<tr id="Sx3.T1.1.1.4.3" class="ltx_tr">
<td id="Sx3.T1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">CIFAR10</td>
<td id="Sx3.T1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">4 conv 2 dense</td>
<td id="Sx3.T1.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">50,000/10,000</td>
<td id="Sx3.T1.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">200/20</td>
<td id="Sx3.T1.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.0005/32</td>
<td id="Sx3.T1.1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1/500</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="Sx3.F1" class="ltx_figure"><img src="/html/2102.00655/assets/x1.png" id="Sx3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="531" height="136" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An overview of the FL backdooring procedure.</figcaption>
</figure>
<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p"><span id="Sx3.p1.1.1" class="ltx_text ltx_font_bold">Federated Learning Setup.</span> We use LEAFÂ <cite class="ltx_cite ltx_citemacro_citep">(Caldas etÂ al. <a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>, an open-source practical FL benchmark, for our experiments.
Most existing works simulate data heterogeneity by partitioning a dataset among clients using probability distributions, but LEAFÂ <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>LEAF: https://github.com/TalwalkarLab/leaf</span></span></span>
provides more realistically distributed datasets. In this paper, we use the <span id="Sx3.p1.1.2" class="ltx_text ltx_font_bold">FEMNIST</span> dataset provided by LEAF as an example for CNN model, which is a handwritten character classification task for 62 classes.
We use <span id="Sx3.p1.1.3" class="ltx_text ltx_font_bold">Sent140</span> from LEAF as an example for LSTM model, a sentiment classification task for 2 classes (positive/negative) on tweets.
As the total dataset contains millions of data points, LEAFÂ <cite class="ltx_cite ltx_citemacro_citep">(Caldas etÂ al. <a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite> suggests sampling the dataset and provides a reference implementation. We also use <span id="Sx3.p1.1.4" class="ltx_text ltx_font_bold">CIFAR10</span> (partitioned across 200 clients) for reference as it is commonly used in FL literature.
More details of the dataset, model, training settings, and learning hyperparameter parameters are summarized in TableÂ <a href="#Sx3.T1" title="Table 1 â€£ Experiment Setups for FL Backdooring â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="Sx3.p2" class="ltx_para">
<p id="Sx3.p2.4" class="ltx_p"><span id="Sx3.p2.4.1" class="ltx_text ltx_font_bold">Control and Quantify Heterogeneity.</span>
FEMNIST, Sent140, and CIFAR10 have their default data distributions.
To explore the impact of different heterogeneity on FL backdooring, we control the heterogeneity by varying the number of maximum classes per client followingÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al. <a href="#bib.bib26" title="" class="ltx_ref">2018</a>; Fung, Yoon, and Beschastnikh <a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite>.
Less number of classes per client results in less evenly distributed data and thus is more heterogeneous
To better quantify heterogeneity, we define <span id="Sx3.p2.4.2" class="ltx_text ltx_font_italic">Heterogeneity Index</span> (HI) as a normalized heterogeneity measure:</p>
<table id="Sx3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="Sx3.E1.m1.2" class="ltx_Math" alttext="HI(c)=1-\tfrac{1}{C_{max}-1}*(c-1)," display="block"><semantics id="Sx3.E1.m1.2a"><mrow id="Sx3.E1.m1.2.2.1" xref="Sx3.E1.m1.2.2.1.1.cmml"><mrow id="Sx3.E1.m1.2.2.1.1" xref="Sx3.E1.m1.2.2.1.1.cmml"><mrow id="Sx3.E1.m1.2.2.1.1.3" xref="Sx3.E1.m1.2.2.1.1.3.cmml"><mi id="Sx3.E1.m1.2.2.1.1.3.2" xref="Sx3.E1.m1.2.2.1.1.3.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.2.2.1.1.3.1" xref="Sx3.E1.m1.2.2.1.1.3.1.cmml">â€‹</mo><mi id="Sx3.E1.m1.2.2.1.1.3.3" xref="Sx3.E1.m1.2.2.1.1.3.3.cmml">I</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.2.2.1.1.3.1a" xref="Sx3.E1.m1.2.2.1.1.3.1.cmml">â€‹</mo><mrow id="Sx3.E1.m1.2.2.1.1.3.4.2" xref="Sx3.E1.m1.2.2.1.1.3.cmml"><mo stretchy="false" id="Sx3.E1.m1.2.2.1.1.3.4.2.1" xref="Sx3.E1.m1.2.2.1.1.3.cmml">(</mo><mi id="Sx3.E1.m1.1.1" xref="Sx3.E1.m1.1.1.cmml">c</mi><mo stretchy="false" id="Sx3.E1.m1.2.2.1.1.3.4.2.2" xref="Sx3.E1.m1.2.2.1.1.3.cmml">)</mo></mrow></mrow><mo id="Sx3.E1.m1.2.2.1.1.2" xref="Sx3.E1.m1.2.2.1.1.2.cmml">=</mo><mrow id="Sx3.E1.m1.2.2.1.1.1" xref="Sx3.E1.m1.2.2.1.1.1.cmml"><mn id="Sx3.E1.m1.2.2.1.1.1.3" xref="Sx3.E1.m1.2.2.1.1.1.3.cmml">1</mn><mo id="Sx3.E1.m1.2.2.1.1.1.2" xref="Sx3.E1.m1.2.2.1.1.1.2.cmml">âˆ’</mo><mrow id="Sx3.E1.m1.2.2.1.1.1.1" xref="Sx3.E1.m1.2.2.1.1.1.1.cmml"><mstyle displaystyle="false" id="Sx3.E1.m1.2.2.1.1.1.1.3" xref="Sx3.E1.m1.2.2.1.1.1.1.3.cmml"><mfrac id="Sx3.E1.m1.2.2.1.1.1.1.3a" xref="Sx3.E1.m1.2.2.1.1.1.1.3.cmml"><mn id="Sx3.E1.m1.2.2.1.1.1.1.3.2" xref="Sx3.E1.m1.2.2.1.1.1.1.3.2.cmml">1</mn><mrow id="Sx3.E1.m1.2.2.1.1.1.1.3.3" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.cmml"><msub id="Sx3.E1.m1.2.2.1.1.1.1.3.3.2" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.cmml"><mi id="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.2" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.2.cmml">C</mi><mrow id="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3.cmml"><mi id="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3.2" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3.1" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3.1.cmml">â€‹</mo><mi id="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3.3" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3.1a" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3.1.cmml">â€‹</mo><mi id="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3.4" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3.4.cmml">x</mi></mrow></msub><mo id="Sx3.E1.m1.2.2.1.1.1.1.3.3.1" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.1.cmml">âˆ’</mo><mn id="Sx3.E1.m1.2.2.1.1.1.1.3.3.3" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.3.cmml">1</mn></mrow></mfrac></mstyle><mo lspace="0.222em" rspace="0.222em" id="Sx3.E1.m1.2.2.1.1.1.1.2" xref="Sx3.E1.m1.2.2.1.1.1.1.2.cmml">âˆ—</mo><mrow id="Sx3.E1.m1.2.2.1.1.1.1.1.1" xref="Sx3.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx3.E1.m1.2.2.1.1.1.1.1.1.2" xref="Sx3.E1.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="Sx3.E1.m1.2.2.1.1.1.1.1.1.1" xref="Sx3.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="Sx3.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="Sx3.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml">c</mi><mo id="Sx3.E1.m1.2.2.1.1.1.1.1.1.1.1" xref="Sx3.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mn id="Sx3.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="Sx3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="Sx3.E1.m1.2.2.1.1.1.1.1.1.3" xref="Sx3.E1.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="Sx3.E1.m1.2.2.1.2" xref="Sx3.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx3.E1.m1.2b"><apply id="Sx3.E1.m1.2.2.1.1.cmml" xref="Sx3.E1.m1.2.2.1"><eq id="Sx3.E1.m1.2.2.1.1.2.cmml" xref="Sx3.E1.m1.2.2.1.1.2"></eq><apply id="Sx3.E1.m1.2.2.1.1.3.cmml" xref="Sx3.E1.m1.2.2.1.1.3"><times id="Sx3.E1.m1.2.2.1.1.3.1.cmml" xref="Sx3.E1.m1.2.2.1.1.3.1"></times><ci id="Sx3.E1.m1.2.2.1.1.3.2.cmml" xref="Sx3.E1.m1.2.2.1.1.3.2">ğ»</ci><ci id="Sx3.E1.m1.2.2.1.1.3.3.cmml" xref="Sx3.E1.m1.2.2.1.1.3.3">ğ¼</ci><ci id="Sx3.E1.m1.1.1.cmml" xref="Sx3.E1.m1.1.1">ğ‘</ci></apply><apply id="Sx3.E1.m1.2.2.1.1.1.cmml" xref="Sx3.E1.m1.2.2.1.1.1"><minus id="Sx3.E1.m1.2.2.1.1.1.2.cmml" xref="Sx3.E1.m1.2.2.1.1.1.2"></minus><cn type="integer" id="Sx3.E1.m1.2.2.1.1.1.3.cmml" xref="Sx3.E1.m1.2.2.1.1.1.3">1</cn><apply id="Sx3.E1.m1.2.2.1.1.1.1.cmml" xref="Sx3.E1.m1.2.2.1.1.1.1"><times id="Sx3.E1.m1.2.2.1.1.1.1.2.cmml" xref="Sx3.E1.m1.2.2.1.1.1.1.2"></times><apply id="Sx3.E1.m1.2.2.1.1.1.1.3.cmml" xref="Sx3.E1.m1.2.2.1.1.1.1.3"><divide id="Sx3.E1.m1.2.2.1.1.1.1.3.1.cmml" xref="Sx3.E1.m1.2.2.1.1.1.1.3"></divide><cn type="integer" id="Sx3.E1.m1.2.2.1.1.1.1.3.2.cmml" xref="Sx3.E1.m1.2.2.1.1.1.1.3.2">1</cn><apply id="Sx3.E1.m1.2.2.1.1.1.1.3.3.cmml" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3"><minus id="Sx3.E1.m1.2.2.1.1.1.1.3.3.1.cmml" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.1"></minus><apply id="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.cmml" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.1.cmml" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.2">subscript</csymbol><ci id="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.2.cmml" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.2">ğ¶</ci><apply id="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3.cmml" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3"><times id="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3.1.cmml" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3.1"></times><ci id="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3.2.cmml" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3.2">ğ‘š</ci><ci id="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3.3.cmml" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3.3">ğ‘</ci><ci id="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3.4.cmml" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.2.3.4">ğ‘¥</ci></apply></apply><cn type="integer" id="Sx3.E1.m1.2.2.1.1.1.1.3.3.3.cmml" xref="Sx3.E1.m1.2.2.1.1.1.1.3.3.3">1</cn></apply></apply><apply id="Sx3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.2.2.1.1.1.1.1.1"><minus id="Sx3.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="Sx3.E1.m1.2.2.1.1.1.1.1.1.1.1"></minus><ci id="Sx3.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="Sx3.E1.m1.2.2.1.1.1.1.1.1.1.2">ğ‘</ci><cn type="integer" id="Sx3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="Sx3.E1.m1.2.2.1.1.1.1.1.1.1.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.E1.m1.2c">HI(c)=1-\tfrac{1}{C_{max}-1}*(c-1),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="Sx3.p2.3" class="ltx_p">where <math id="Sx3.p2.1.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="Sx3.p2.1.m1.1a"><mi id="Sx3.p2.1.m1.1.1" xref="Sx3.p2.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="Sx3.p2.1.m1.1b"><ci id="Sx3.p2.1.m1.1.1.cmml" xref="Sx3.p2.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.1.m1.1c">c</annotation></semantics></math> adjusts the maximum number of classes per client (i.e. the parameter controlling heterogeneity), and <math id="Sx3.p2.2.m2.1" class="ltx_Math" alttext="C_{max}" display="inline"><semantics id="Sx3.p2.2.m2.1a"><msub id="Sx3.p2.2.m2.1.1" xref="Sx3.p2.2.m2.1.1.cmml"><mi id="Sx3.p2.2.m2.1.1.2" xref="Sx3.p2.2.m2.1.1.2.cmml">C</mi><mrow id="Sx3.p2.2.m2.1.1.3" xref="Sx3.p2.2.m2.1.1.3.cmml"><mi id="Sx3.p2.2.m2.1.1.3.2" xref="Sx3.p2.2.m2.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="Sx3.p2.2.m2.1.1.3.1" xref="Sx3.p2.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="Sx3.p2.2.m2.1.1.3.3" xref="Sx3.p2.2.m2.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="Sx3.p2.2.m2.1.1.3.1a" xref="Sx3.p2.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="Sx3.p2.2.m2.1.1.3.4" xref="Sx3.p2.2.m2.1.1.3.4.cmml">x</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx3.p2.2.m2.1b"><apply id="Sx3.p2.2.m2.1.1.cmml" xref="Sx3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="Sx3.p2.2.m2.1.1.1.cmml" xref="Sx3.p2.2.m2.1.1">subscript</csymbol><ci id="Sx3.p2.2.m2.1.1.2.cmml" xref="Sx3.p2.2.m2.1.1.2">ğ¶</ci><apply id="Sx3.p2.2.m2.1.1.3.cmml" xref="Sx3.p2.2.m2.1.1.3"><times id="Sx3.p2.2.m2.1.1.3.1.cmml" xref="Sx3.p2.2.m2.1.1.3.1"></times><ci id="Sx3.p2.2.m2.1.1.3.2.cmml" xref="Sx3.p2.2.m2.1.1.3.2">ğ‘š</ci><ci id="Sx3.p2.2.m2.1.1.3.3.cmml" xref="Sx3.p2.2.m2.1.1.3.3">ğ‘</ci><ci id="Sx3.p2.2.m2.1.1.3.4.cmml" xref="Sx3.p2.2.m2.1.1.3.4">ğ‘¥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.2.m2.1c">C_{max}</annotation></semantics></math> is the total number of classes in the dataset.
The scaling performed here is to normalize the value between 0 and 1, with 1 being the highest data heterogeneity, vice versa. We also perform our experiments with Gaussian and Dirichlet distributions (see Appendix) and the results are consistent with <math id="Sx3.p2.3.m3.1" class="ltx_Math" alttext="HI" display="inline"><semantics id="Sx3.p2.3.m3.1a"><mrow id="Sx3.p2.3.m3.1.1" xref="Sx3.p2.3.m3.1.1.cmml"><mi id="Sx3.p2.3.m3.1.1.2" xref="Sx3.p2.3.m3.1.1.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="Sx3.p2.3.m3.1.1.1" xref="Sx3.p2.3.m3.1.1.1.cmml">â€‹</mo><mi id="Sx3.p2.3.m3.1.1.3" xref="Sx3.p2.3.m3.1.1.3.cmml">I</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx3.p2.3.m3.1b"><apply id="Sx3.p2.3.m3.1.1.cmml" xref="Sx3.p2.3.m3.1.1"><times id="Sx3.p2.3.m3.1.1.1.cmml" xref="Sx3.p2.3.m3.1.1.1"></times><ci id="Sx3.p2.3.m3.1.1.2.cmml" xref="Sx3.p2.3.m3.1.1.2">ğ»</ci><ci id="Sx3.p2.3.m3.1.1.3.cmml" xref="Sx3.p2.3.m3.1.1.3">ğ¼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p2.3.m3.1c">HI</annotation></semantics></math>.</p>
</div>
<div id="Sx3.p3" class="ltx_para">
<p id="Sx3.p3.1" class="ltx_p"><span id="Sx3.p3.1.1" class="ltx_text ltx_font_bold">Threat Model.</span>
We use the same threat model in literatureÂ <cite class="ltx_cite ltx_citemacro_citep">(Xie etÂ al. <a href="#bib.bib24" title="" class="ltx_ref">2019</a>; Sun etÂ al. <a href="#bib.bib21" title="" class="ltx_ref">2019</a>; Chen etÂ al. <a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite>.
Specifically, an adversary (impersonated by a malicious client) can manipulate its model updates sent to the aggregator as well as its local training process in every aspect such as the training data, learning hyperparameters, model weights, and any local privacy mechanisms.
The attacker has the capacity to compromise multiple parties and multiple attackers can collude towards the same goal. The aggregation algorithm, as well as the local training mechanisms of benign clients are trusted. Our threat model assumes that only the attacker clients have malicious intent, i.e., the benign clients train their models as expected, without manipulating the data or the training procedure.
<span id="Sx3.p3.1.2" class="ltx_text ltx_font_bold">Objective and Method of Backdooring Attacks.</span>
We focus on backdoor attacks, where the objective of the attacker is to inject a trigger to cause a targeted misclassification without compromising the model accuracy or disrupting convergenceÂ <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan etÂ al. <a href="#bib.bib2" title="" class="ltx_ref">2018</a>; Xie etÂ al. <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite>.
In classification applications, backdoor attacks are achieved by adding one or more extra patterns to benign images for vision tasks and appending a trigger string for NLP tasks so that the classifier deliberately misclassifies the backdoored samples as a (different) target class.
We adopt the decentralized attack method proposed inÂ <cite class="ltx_cite ltx_citemacro_citep">(Xie etÂ al. <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite> (for details, see Appendix). We randomly select a configured number of clients as malicious clients,
where data points are backdoored by injecting a trigger pattern.
Fig.Â <a href="#Sx3.F1" title="Figure 1 â€£ Experiment Setups for FL Backdooring â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides an overview of the attack process.
We keep the learning hyperparameters the same for both malicious and benign clients.
For testing successful backdoor injection, we apply the trigger on 50% of the test dataset and evaluate the global model on it.
If the classification result is the same as the label of the target class, we report a successful attack. And the portion of successful attacks is defined as Attack Success Rate (ASR).
It is worth noting that we do not consider data points that are originally from the target class when calculating ASR.</p>
</div>
<div id="Sx3.p4" class="ltx_para">
<p id="Sx3.p4.1" class="ltx_p"><span id="Sx3.p4.1.1" class="ltx_text ltx_font_bold">Relation to Model Poisoning.</span>
When the scaling factor is large, backdooring is effectively doing model replacement (aka model poisoning), see analysis provided in literatureÂ <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan etÂ al. <a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>. We show the scaling factor analysis in Appendix.</p>
</div>
</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Data Heterogeneity Seems to Be a Redemption</h2>

<figure id="Sx4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx4.F2.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/redemption1/femnist.png" id="Sx4.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="374" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx4.F2.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/redemption1/cifar10.png" id="Sx4.F2.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="374" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx4.F2.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/redemption1/sent140.png" id="Sx4.F2.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="374" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Attack Success Rate (ASR) vs. Heterogeneity Index (HI).</figcaption>
</figure>
<figure id="Sx4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx4.F3.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/redemption2/femnist.png" id="Sx4.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="374" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx4.F3.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/redemption2/cifar10.png" id="Sx4.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="374" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx4.F3.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/redemption2/sent140.png" id="Sx4.F3.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="374" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Attack Success Rate (ASR) vs. malicious data distribution (each bar represents a unique malicious data distribution). </figcaption>
</figure>
<figure id="Sx4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx4.F4.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/redemption3/femnist.png" id="Sx4.F4.1.g1" class="ltx_graphics ltx_img_square" width="598" height="499" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx4.F4.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/redemption3/cifar10.png" id="Sx4.F4.2.g1" class="ltx_graphics ltx_img_square" width="598" height="499" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx4.F4.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/redemption3/sent140.png" id="Sx4.F4.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="427" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Attack Success Rate (ASR) scalability in terms of attack scale and total attack budget.</figcaption>
</figure>
<section id="Sx4.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Redemption 1: Data Heterogeneity Reduces Attack Effectiveness of Backdooring</h3>

<div id="Sx4.SSx1.p1" class="ltx_para">
<p id="Sx4.SSx1.p1.1" class="ltx_p">Our initial study suggests data heterogeneity seems to be a redemption for defending backdoor attacks in FL as it reduces the attack effectiveness and also challenges the design of good attack strategies. To understand how data heterogeneity affects backdoor attacks in FL, we first conduct a set of experiments by simply
varying <span id="Sx4.SSx1.p1.1.1" class="ltx_text ltx_font_italic">Heterogeneity Index</span> from 0 to 1 to observe how the extent of data heterogeneity affects the effectiveness of attacks measured as ASR.
We fix all other configurable parameters across experiments, i.e., 50% malicious clients per round and 50% of data points per batch is backdoored at each client (we evaluate other ratios of malicious clients and malicious data points in later sections), and the rest of configurations are the same as explained in the <span id="Sx4.SSx1.p1.1.2" class="ltx_text ltx_font_bold">Experiment Setup</span> section.
We run the experiment for each <span id="Sx4.SSx1.p1.1.3" class="ltx_text ltx_font_italic">Heterogeneity Index</span> 10 times with different malicious data distribution and report ASR as a box-and-whisker plot shown in Fig.Â <a href="#Sx4.F2" title="Figure 2 â€£ Data Heterogeneity Seems to Be a Redemption â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>
The results clearly suggest that the overall attack effectiveness reduces when higher heterogeneity exists in the training data as the medium ASR decreases when <span id="Sx4.SSx1.p1.1.4" class="ltx_text ltx_font_italic">Heterogeneity Index</span> increases.
Another interesting observation is that the box and whisker become much wider as <span id="Sx4.SSx1.p1.1.5" class="ltx_text ltx_font_italic">Heterogeneity Index</span> becomes higher, which indicates that the attack effectiveness also becomes less stable when higher heterogeneity presents in training data.</p>
</div>
<div id="Sx4.SSx1.p2" class="ltx_para">
<p id="Sx4.SSx1.p2.1" class="ltx_p">Backdoor attacks essentially make the model learn the trigger features.
In FL, each client performs its own local training and the local model learns towards reaching the optima of the feature space of that clientâ€™s local data.
When the training data is more heterogeneous across clients, some features at a client may be more pronounced due to the more skewed local data, i.e., results in overfitting.
Such more augmented features may suppress backdoor features (e.g., in the extreme case, the backdoor features may become noise compared to the augmented features), and thus make the attack less effective.</p>
</div>
</section>
<section id="Sx4.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Redemption 2: An Overlooked Key Factor: Malicious Data Distribution</h3>

<figure id="Sx4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Sx4.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/curse1/femnist_batch_attack_timing.png" id="Sx4.F5.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="499" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>ASR vs. attack timing.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Sx4.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/curse1/acc_vs_rounds_femnist_2.png" id="Sx4.F5.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="225" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Global (upper case) and local (lower case) attack timing.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparison of different attack timing on FEMNIST.</figcaption>
</figure>
<figure id="Sx4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx4.F6.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/curse1/femnist_asr_vs_iidness_def_opt.png" id="Sx4.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="374" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx4.F6.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/curse1/cifar10_asr_vs_iidness_def_opt.png" id="Sx4.F6.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="374" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx4.F6.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/curse1/sent140_asr_vs_iidness_def_opt.png" id="Sx4.F6.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="374" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Comparison between <span id="Sx4.F6.6.1" class="ltx_text ltx_font_italic">evenly</span> vs. <span id="Sx4.F6.7.2" class="ltx_text ltx_font_italic">last</span> batch attack timing under various Heterogeneity Index.</figcaption>
</figure>
<div id="Sx4.SSx2.p1" class="ltx_para">
<p id="Sx4.SSx2.p1.1" class="ltx_p">In Fig.Â <a href="#Sx4.F2" title="Figure 2 â€£ Data Heterogeneity Seems to Be a Redemption â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, even though the trend that data heterogeneity reduces attack effectiveness is clear, from the box-and-whisker plot, we can see that some malicious data distribution is more effective than others.
This indicates that the malicious data distribution can be an important factor in attack effectiveness. Given this has not been studied in the literature, we perform empirical experiments to verify this.
In this set of experiments, we follow the similar setup as in <span id="Sx4.SSx2.p1.1.1" class="ltx_text ltx_font_bold">Redemption 1</span>, except that we fix the <span id="Sx4.SSx2.p1.1.2" class="ltx_text ltx_font_italic">Heterogeneity Index</span>. Specifically, we use the original training data distribution from LEAF, i.e., <span id="Sx4.SSx2.p1.1.3" class="ltx_text ltx_font_italic">Heterogeneity Index</span> is 0.2 and 0.0 for FEMNIST and Sent140, respectively.
For CIFAR10, we choose a distribution with <span id="Sx4.SSx2.p1.1.4" class="ltx_text ltx_font_italic">Heterogeneity Index</span> equal to 0.5.
We report the average ASR for 20 rounds of attack across 25 different malicious data distributions in Fig.Â <a href="#Sx4.F3" title="Figure 3 â€£ Data Heterogeneity Seems to Be a Redemption â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, where each bar represents a unique malicious data distribution. Note that the data distribution of benign clients remains the same.
The results indeed demonstrate that the attack effectiveness depends on malicious data distribution as the ASR changes significantly when different malicious data distribution is used.
Such behavior can be explained as the effectiveness of learning backdoor trigger depends on the difference in feature space between training data distribution and malicious data distribution, which we provide further analysis in the <span id="Sx4.SSx2.p1.1.5" class="ltx_text ltx_font_bold">Curse 3</span> section.
This brings a redemption for the robustness of FL as an improper selection of malicious data distribution may result in poor attack effectiveness.</p>
</div>
</section>
<section id="Sx4.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Redemption 3: Effective Attack Strategies are More Challenging to Make</h3>

<div id="Sx4.SSx3.p1" class="ltx_para">
<p id="Sx4.SSx3.p1.1" class="ltx_p">Since malicious data distribution is an important factor in FL backdoor attacks, the natural question is how would it compare to other factors such as the number of attackers and the total number of poisoned datapoints.
To understand this, we conduct experiments by varying the configuration tuple (attack scale, total attack budget, malicious data distribution) and organize the results into a heat map in Fig.Â <a href="#Sx4.F4" title="Figure 4 â€£ Data Heterogeneity Seems to Be a Redemption â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. To make a fair comparison, when we increase the number of attackers, we keep the total number of poisoned datapoints (attack budget) the same and spread evenly across devices. All other parameters are the same as defined in the experimental setup.</p>
</div>
<div id="Sx4.SSx3.p2" class="ltx_para">
<p id="Sx4.SSx3.p2.1" class="ltx_p">The results are quite surprising as there is no clear pattern in the heat maps of all three benchmarks, which is in contrary to the conclusion made by almost all existing workÂ <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan etÂ al. <a href="#bib.bib2" title="" class="ltx_ref">2018</a>; Fung, Yoon, and Beschastnikh <a href="#bib.bib10" title="" class="ltx_ref">2018</a>; Sun etÂ al. <a href="#bib.bib21" title="" class="ltx_ref">2019</a>; Xie etÂ al. <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite> that higher <span id="Sx4.SSx3.p2.1.1" class="ltx_text ltx_font_italic">attack scale</span> and <span id="Sx4.SSx3.p2.1.2" class="ltx_text ltx_font_italic">total attack budget</span> always lead to more effective attacks.
These counter-intuitive results suggest that the overlooked malicious data distribution is actually a dominant factor in FL backdoor attacks.
Different from homogeneous training data case, where malicious data distribution can be simply configured as IID (the total distribution is a public secret) to maximize the attack effectiveness, malicious data distribution is more difficult to find a reference when training data is heterogeneous.
Unlike the <span id="Sx4.SSx3.p2.1.3" class="ltx_text ltx_font_italic">attack scale</span> and the <span id="Sx4.SSx3.p2.1.4" class="ltx_text ltx_font_italic">total attack budget</span>, malicious data distribution is not straightforward to configure, which makes designing effective attack strategies more challenging and the attack effectiveness is thus less predictable.
Because of this, data heterogeneity brings another redemption for the robustness of FL. To demonstrate the observed behaviour is not unique to our chosen attack mechanism, we further evaluated the backdoor attacks proposed in <cite class="ltx_cite ltx_citemacro_citep">(Sun etÂ al. <a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al. <a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite> and the results (see Appendix) are consistent with Fig.Â <a href="#Sx4.F4" title="Figure 4 â€£ Data Heterogeneity Seems to Be a Redemption â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
</section>
<section id="Sx5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Data Heterogeneity Brings Unseen Curses</h2>

<div id="Sx5.p1" class="ltx_para">
<p id="Sx5.p1.1" class="ltx_p">Despite of the redemption brought by data heterogeneity, our further investigations reveal that data heterogeneity can result in several curses for FL backdooring as the attack effectiveness can be significantly boosted by applying proper local attack timing and malicious data distribution, and the backdooring can camouflage itself much easier compared to the homogeneous data case.</p>
</div>
<section id="Sx5.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Curse 1: Local Attack Timing: a New Vulnerability</h3>

<figure id="Sx5.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx5.F7.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/curse2/cifar10.png" id="Sx5.F7.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx5.F7.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/curse2/femnist.png" id="Sx5.F7.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx5.F7.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/curse2/sent140.png" id="Sx5.F7.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Cosine Similarity Comparison between benign and malicious clients under different Heterogeneity Index.</figcaption>
</figure>
<figure id="Sx5.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx5.F8.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/curse3/femnist_chisq.png" id="Sx5.F8.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx5.F8.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/curse3/cifar10_chisq.png" id="Sx5.F8.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx5.F8.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/curse3/sent140_chisq.png" id="Sx5.F8.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>ASR trend with ChiSq Distance</figcaption>
</figure>
<div id="Sx5.SSx1.p1" class="ltx_para">
<p id="Sx5.SSx1.p1.1" class="ltx_p">One important observation is that the local attack timing at each client is important for attack effectiveness, especially with data heterogeneity.
To demonstrate this, we compare four different local attack timing strategies: 1) <span id="Sx5.SSx1.p1.1.1" class="ltx_text ltx_font_italic">evenly</span> distribute the local attack budget across 10 batches (i.e., the default attack strategy in almost all literature); 2) only attack the <span id="Sx5.SSx1.p1.1.2" class="ltx_text ltx_font_italic">first 5</span> batches; 3) attack the <span id="Sx5.SSx1.p1.1.3" class="ltx_text ltx_font_italic">middle 5</span> batches; 4) attack the <span id="Sx5.SSx1.p1.1.4" class="ltx_text ltx_font_italic">last 5</span> batches.
To make a fair comparison, all the four cases have the same local attack budget, i.e., backdoor 10% data per batch in <span id="Sx5.SSx1.p1.1.5" class="ltx_text ltx_font_italic">evenly</span> strategy while backdoor 20% data per batch for the other three timing strategies.
We use default data heterogeneity of LEAF (i.e., HI=0.2) and all other configures are the same as <span id="Sx5.SSx1.p1.1.6" class="ltx_text ltx_font_bold">Redemption 1</span>. The ASR comparison results are presented in FigÂ <a href="#Sx4.F5.sf1" title="In Figure 5 â€£ Redemption 2: An Overlooked Key Factor: Malicious Data Distribution â€£ Data Heterogeneity Seems to Be a Redemption â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(a)</span></a> and we can see the difference is quite large between different strategies with <span id="Sx5.SSx1.p1.1.7" class="ltx_text ltx_font_italic">last 5</span> being the highest.
Similar to the reason that data heterogeneity results in less effective attack due to overfitting, here later attack helps backdoor features to be easily overfitted while earlier attack may let the backdoor features easier to be forgottenÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu, Dolan-Gavitt, and Garg <a href="#bib.bib15" title="" class="ltx_ref">2018</a>)</cite>.
To understand the behaviors of considering both local and global attack timing, we combine different global attack timing strategies with different local attack timing strategies (<span id="Sx5.SSx1.p1.1.8" class="ltx_text ltx_font_italic">evenly</span>, <span id="Sx5.SSx1.p1.1.9" class="ltx_text ltx_font_italic">last</span>).
Note that <span id="Sx5.SSx1.p1.1.10" class="ltx_text ltx_font_italic">last</span> is attacking only the last batch as we found it performs similar as <span id="Sx5.SSx1.p1.1.11" class="ltx_text ltx_font_italic">last 5</span> but with 80% less attack budget
but with the same <span id="Sx5.SSx1.p1.1.12" class="ltx_text ltx_font_italic">attack scale</span>.
The comparison results are shown in Fig.Â <a href="#Sx4.F5.sf2" title="In Figure 5 â€£ Redemption 2: An Overlooked Key Factor: Malicious Data Distribution â€£ Data Heterogeneity Seems to Be a Redemption â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(b)</span></a>, where we can see the local attack timing defines the ASR while global attack timing has little impact.
Another important observation is that in <span id="Sx5.SSx1.p1.1.13" class="ltx_text ltx_font_italic">LATTER(last)</span>, the total attack budget is only 0.2% of the total training data, one order of magnitude lower than literatureÂ <cite class="ltx_cite ltx_citemacro_citep">(Sun etÂ al. <a href="#bib.bib21" title="" class="ltx_ref">2019</a>; Bagdasaryan etÂ al. <a href="#bib.bib2" title="" class="ltx_ref">2018</a>; Xie etÂ al. <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite>. Such extremely low budget but highly effective attack makes the local attack timing under data heterogeneity a new vulnerability. We further investigate how data heterogeneity impacts the effects of local attack timing. We perform the same experiments by varying HI and present the results in FigÂ <a href="#Sx4.F6" title="Figure 6 â€£ Redemption 2: An Overlooked Key Factor: Malicious Data Distribution â€£ Data Heterogeneity Seems to Be a Redemption â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="Sx5.SSx1.p2" class="ltx_para">
<p id="Sx5.SSx1.p2.1" class="ltx_p">In the <span id="Sx5.SSx1.p2.1.1" class="ltx_text ltx_font_italic">evenly</span> strategy, as expected, higher heterogeneity results in less attack effectiveness as discussed in <span id="Sx5.SSx1.p2.1.2" class="ltx_text ltx_font_bold">Redemption 1</span>. For <span id="Sx5.SSx1.p2.1.3" class="ltx_text ltx_font_italic">last</span> strategy, it is overall more robust under different heterogeneity and the improvement over <span id="Sx5.SSx1.p2.1.4" class="ltx_text ltx_font_italic">evenly</span> increases with data heterogeneity. Therefore, the local attack timing can be manipulated by attackers to increase attack effectiveness, especially in high data heterogeneity case.</p>
</div>
</section>
<section id="Sx5.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Curse 2: Failure of Skewed-Feature Based Defense</h3>

<div id="Sx5.SSx2.p1" class="ltx_para">
<p id="Sx5.SSx2.p1.1" class="ltx_p">One of the most effective ways to detect FL backdoor attacks is through differentiation between benign features and malicious features (skewed-feature based defense) as they have quite different footprints.
For instance, cosine similarity can be used to detect anomalous weightsÂ <cite class="ltx_cite ltx_citemacro_citep">(Fung, Yoon, and Beschastnikh <a href="#bib.bib10" title="" class="ltx_ref">2018</a>; Bagdasaryan etÂ al. <a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>. However, data heterogeneity may increase the weight divergences among the benign clientsÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al. <a href="#bib.bib26" title="" class="ltx_ref">2018</a>)</cite> thus may make it less distinguishable from malicious clients.
To illustrate this, we use cosine similarity as an example.
Specifically, we compute the cosine similarity of the last dense layer weights of each client against the last dense layer weights of the previous roundâ€™s global model under different data heterogeneity.
We use the <span id="Sx5.SSx2.p1.1.1" class="ltx_text ltx_font_italic">last</span> attack timing strategy and the same experiment setup as in <span id="Sx5.SSx2.p1.1.2" class="ltx_text ltx_font_bold">Redemption 1</span>.
We use box-and-whisker plot to show the distribution of the cosine similarity values of benign clients and malicious clients respectively in Fig.Â <a href="#Sx5.F7" title="Figure 7 â€£ Curse 1: Local Attack Timing: a New Vulnerability â€£ Data Heterogeneity Brings Unseen Curses â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
From the results, it is clear that higher data heterogeneity (i.e., higher HI) causes more weights dissimilarity in benign clients (i.e., lower cosine similarity).
Such high data weights dissimilarity in benign data may be even higher than the dissimilarity of backdoored data, which allows malicious data stealth themselves under the radar of skewed-feature based defense.</p>
</div>
</section>
<section id="Sx5.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Curse 3: Malicious Data Distribution as Leverage</h3>

<div id="Sx5.SSx3.p1" class="ltx_para">
<p id="Sx5.SSx3.p1.1" class="ltx_p">In our experiments from FigureÂ <a href="#Sx4.F2" title="Figure 2 â€£ Data Heterogeneity Seems to Be a Redemption â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we discovered that malicious data distribution is a dominant factor for the attack
effectiveness and it is more difficult to control compared to attack scale and budget.
With further investigation, we found a simple yet efficient way to generate malicious data distributions that are more effective in attack.
Specifically, we find the distribution distance between malicious data distribution and overall training data distribution is strongly correlated with the attack effectiveness.
We tested a number of divergence metrics such as KL divergence, Jensen-Shannon divergence, Wasserstein distance and B-Distance, and all of them can serve as a good metric here. We use the simple Chi-squared distance (ChiSq or <math id="Sx5.SSx3.p1.1.m1.1" class="ltx_Math" alttext="\chi^{2}" display="inline"><semantics id="Sx5.SSx3.p1.1.m1.1a"><msup id="Sx5.SSx3.p1.1.m1.1.1" xref="Sx5.SSx3.p1.1.m1.1.1.cmml"><mi id="Sx5.SSx3.p1.1.m1.1.1.2" xref="Sx5.SSx3.p1.1.m1.1.1.2.cmml">Ï‡</mi><mn id="Sx5.SSx3.p1.1.m1.1.1.3" xref="Sx5.SSx3.p1.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="Sx5.SSx3.p1.1.m1.1b"><apply id="Sx5.SSx3.p1.1.m1.1.1.cmml" xref="Sx5.SSx3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx5.SSx3.p1.1.m1.1.1.1.cmml" xref="Sx5.SSx3.p1.1.m1.1.1">superscript</csymbol><ci id="Sx5.SSx3.p1.1.m1.1.1.2.cmml" xref="Sx5.SSx3.p1.1.m1.1.1.2">ğœ’</ci><cn type="integer" id="Sx5.SSx3.p1.1.m1.1.1.3.cmml" xref="Sx5.SSx3.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx3.p1.1.m1.1c">\chi^{2}</annotation></semantics></math>) as an example for illustration, which is defined as</p>
<table id="Sx5.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="Sx5.E2.m1.2" class="ltx_Math" alttext="\chi^{2}=\textstyle{\sum}_{i=1}^{c}\frac{(O_{i}-E_{i})^{2}}{E_{i}}," display="block"><semantics id="Sx5.E2.m1.2a"><mrow id="Sx5.E2.m1.2.2.1" xref="Sx5.E2.m1.2.2.1.1.cmml"><mrow id="Sx5.E2.m1.2.2.1.1" xref="Sx5.E2.m1.2.2.1.1.cmml"><msup id="Sx5.E2.m1.2.2.1.1.2" xref="Sx5.E2.m1.2.2.1.1.2.cmml"><mi id="Sx5.E2.m1.2.2.1.1.2.2" xref="Sx5.E2.m1.2.2.1.1.2.2.cmml">Ï‡</mi><mn id="Sx5.E2.m1.2.2.1.1.2.3" xref="Sx5.E2.m1.2.2.1.1.2.3.cmml">2</mn></msup><mo id="Sx5.E2.m1.2.2.1.1.1" xref="Sx5.E2.m1.2.2.1.1.1.cmml">=</mo><mrow id="Sx5.E2.m1.2.2.1.1.3" xref="Sx5.E2.m1.2.2.1.1.3.cmml"><mstyle displaystyle="false" id="Sx5.E2.m1.2.2.1.1.3.1" xref="Sx5.E2.m1.2.2.1.1.3.1.cmml"><msubsup id="Sx5.E2.m1.2.2.1.1.3.1a" xref="Sx5.E2.m1.2.2.1.1.3.1.cmml"><mo id="Sx5.E2.m1.2.2.1.1.3.1.2.2" xref="Sx5.E2.m1.2.2.1.1.3.1.2.2.cmml">âˆ‘</mo><mrow id="Sx5.E2.m1.2.2.1.1.3.1.2.3" xref="Sx5.E2.m1.2.2.1.1.3.1.2.3.cmml"><mi id="Sx5.E2.m1.2.2.1.1.3.1.2.3.2" xref="Sx5.E2.m1.2.2.1.1.3.1.2.3.2.cmml">i</mi><mo id="Sx5.E2.m1.2.2.1.1.3.1.2.3.1" xref="Sx5.E2.m1.2.2.1.1.3.1.2.3.1.cmml">=</mo><mn id="Sx5.E2.m1.2.2.1.1.3.1.2.3.3" xref="Sx5.E2.m1.2.2.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="Sx5.E2.m1.2.2.1.1.3.1.3" xref="Sx5.E2.m1.2.2.1.1.3.1.3.cmml">c</mi></msubsup></mstyle><mstyle displaystyle="false" id="Sx5.E2.m1.1.1" xref="Sx5.E2.m1.1.1.cmml"><mfrac id="Sx5.E2.m1.1.1a" xref="Sx5.E2.m1.1.1.cmml"><msup id="Sx5.E2.m1.1.1.1" xref="Sx5.E2.m1.1.1.1.cmml"><mrow id="Sx5.E2.m1.1.1.1.1.1" xref="Sx5.E2.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="Sx5.E2.m1.1.1.1.1.1.2" xref="Sx5.E2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="Sx5.E2.m1.1.1.1.1.1.1" xref="Sx5.E2.m1.1.1.1.1.1.1.cmml"><msub id="Sx5.E2.m1.1.1.1.1.1.1.2" xref="Sx5.E2.m1.1.1.1.1.1.1.2.cmml"><mi id="Sx5.E2.m1.1.1.1.1.1.1.2.2" xref="Sx5.E2.m1.1.1.1.1.1.1.2.2.cmml">O</mi><mi id="Sx5.E2.m1.1.1.1.1.1.1.2.3" xref="Sx5.E2.m1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="Sx5.E2.m1.1.1.1.1.1.1.1" xref="Sx5.E2.m1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="Sx5.E2.m1.1.1.1.1.1.1.3" xref="Sx5.E2.m1.1.1.1.1.1.1.3.cmml"><mi id="Sx5.E2.m1.1.1.1.1.1.1.3.2" xref="Sx5.E2.m1.1.1.1.1.1.1.3.2.cmml">E</mi><mi id="Sx5.E2.m1.1.1.1.1.1.1.3.3" xref="Sx5.E2.m1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="Sx5.E2.m1.1.1.1.1.1.3" xref="Sx5.E2.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="Sx5.E2.m1.1.1.1.3" xref="Sx5.E2.m1.1.1.1.3.cmml">2</mn></msup><msub id="Sx5.E2.m1.1.1.3" xref="Sx5.E2.m1.1.1.3.cmml"><mi id="Sx5.E2.m1.1.1.3.2" xref="Sx5.E2.m1.1.1.3.2.cmml">E</mi><mi id="Sx5.E2.m1.1.1.3.3" xref="Sx5.E2.m1.1.1.3.3.cmml">i</mi></msub></mfrac></mstyle></mrow></mrow><mo id="Sx5.E2.m1.2.2.1.2" xref="Sx5.E2.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="Sx5.E2.m1.2b"><apply id="Sx5.E2.m1.2.2.1.1.cmml" xref="Sx5.E2.m1.2.2.1"><eq id="Sx5.E2.m1.2.2.1.1.1.cmml" xref="Sx5.E2.m1.2.2.1.1.1"></eq><apply id="Sx5.E2.m1.2.2.1.1.2.cmml" xref="Sx5.E2.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="Sx5.E2.m1.2.2.1.1.2.1.cmml" xref="Sx5.E2.m1.2.2.1.1.2">superscript</csymbol><ci id="Sx5.E2.m1.2.2.1.1.2.2.cmml" xref="Sx5.E2.m1.2.2.1.1.2.2">ğœ’</ci><cn type="integer" id="Sx5.E2.m1.2.2.1.1.2.3.cmml" xref="Sx5.E2.m1.2.2.1.1.2.3">2</cn></apply><apply id="Sx5.E2.m1.2.2.1.1.3.cmml" xref="Sx5.E2.m1.2.2.1.1.3"><apply id="Sx5.E2.m1.2.2.1.1.3.1.cmml" xref="Sx5.E2.m1.2.2.1.1.3.1"><csymbol cd="ambiguous" id="Sx5.E2.m1.2.2.1.1.3.1.1.cmml" xref="Sx5.E2.m1.2.2.1.1.3.1">superscript</csymbol><apply id="Sx5.E2.m1.2.2.1.1.3.1.2.cmml" xref="Sx5.E2.m1.2.2.1.1.3.1"><csymbol cd="ambiguous" id="Sx5.E2.m1.2.2.1.1.3.1.2.1.cmml" xref="Sx5.E2.m1.2.2.1.1.3.1">subscript</csymbol><sum id="Sx5.E2.m1.2.2.1.1.3.1.2.2.cmml" xref="Sx5.E2.m1.2.2.1.1.3.1.2.2"></sum><apply id="Sx5.E2.m1.2.2.1.1.3.1.2.3.cmml" xref="Sx5.E2.m1.2.2.1.1.3.1.2.3"><eq id="Sx5.E2.m1.2.2.1.1.3.1.2.3.1.cmml" xref="Sx5.E2.m1.2.2.1.1.3.1.2.3.1"></eq><ci id="Sx5.E2.m1.2.2.1.1.3.1.2.3.2.cmml" xref="Sx5.E2.m1.2.2.1.1.3.1.2.3.2">ğ‘–</ci><cn type="integer" id="Sx5.E2.m1.2.2.1.1.3.1.2.3.3.cmml" xref="Sx5.E2.m1.2.2.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="Sx5.E2.m1.2.2.1.1.3.1.3.cmml" xref="Sx5.E2.m1.2.2.1.1.3.1.3">ğ‘</ci></apply><apply id="Sx5.E2.m1.1.1.cmml" xref="Sx5.E2.m1.1.1"><divide id="Sx5.E2.m1.1.1.2.cmml" xref="Sx5.E2.m1.1.1"></divide><apply id="Sx5.E2.m1.1.1.1.cmml" xref="Sx5.E2.m1.1.1.1"><csymbol cd="ambiguous" id="Sx5.E2.m1.1.1.1.2.cmml" xref="Sx5.E2.m1.1.1.1">superscript</csymbol><apply id="Sx5.E2.m1.1.1.1.1.1.1.cmml" xref="Sx5.E2.m1.1.1.1.1.1"><minus id="Sx5.E2.m1.1.1.1.1.1.1.1.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.1"></minus><apply id="Sx5.E2.m1.1.1.1.1.1.1.2.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="Sx5.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="Sx5.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.2.2">ğ‘‚</ci><ci id="Sx5.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="Sx5.E2.m1.1.1.1.1.1.1.3.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="Sx5.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="Sx5.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.3.2">ğ¸</ci><ci id="Sx5.E2.m1.1.1.1.1.1.1.3.3.cmml" xref="Sx5.E2.m1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply><cn type="integer" id="Sx5.E2.m1.1.1.1.3.cmml" xref="Sx5.E2.m1.1.1.1.3">2</cn></apply><apply id="Sx5.E2.m1.1.1.3.cmml" xref="Sx5.E2.m1.1.1.3"><csymbol cd="ambiguous" id="Sx5.E2.m1.1.1.3.1.cmml" xref="Sx5.E2.m1.1.1.3">subscript</csymbol><ci id="Sx5.E2.m1.1.1.3.2.cmml" xref="Sx5.E2.m1.1.1.3.2">ğ¸</ci><ci id="Sx5.E2.m1.1.1.3.3.cmml" xref="Sx5.E2.m1.1.1.3.3">ğ‘–</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.E2.m1.2c">\chi^{2}=\textstyle{\sum}_{i=1}^{c}\frac{(O_{i}-E_{i})^{2}}{E_{i}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<figure id="Sx5.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Sx5.F9.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/curse3/femnist_budget.png" id="Sx5.F9.1.g1" class="ltx_graphics ltx_img_square" width="598" height="665" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Sx5.F9.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/curse3/femnist_scaling.png" id="Sx5.F9.2.g1" class="ltx_graphics ltx_img_square" width="598" height="499" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>ASR comparison between different total attack budget, attack scale, and ChiSq distance.</figcaption>
</figure>
<div id="Sx5.SSx3.p2" class="ltx_para">
<p id="Sx5.SSx3.p2.5" class="ltx_p">where <math id="Sx5.SSx3.p2.1.m1.1" class="ltx_Math" alttext="E_{i}" display="inline"><semantics id="Sx5.SSx3.p2.1.m1.1a"><msub id="Sx5.SSx3.p2.1.m1.1.1" xref="Sx5.SSx3.p2.1.m1.1.1.cmml"><mi id="Sx5.SSx3.p2.1.m1.1.1.2" xref="Sx5.SSx3.p2.1.m1.1.1.2.cmml">E</mi><mi id="Sx5.SSx3.p2.1.m1.1.1.3" xref="Sx5.SSx3.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Sx5.SSx3.p2.1.m1.1b"><apply id="Sx5.SSx3.p2.1.m1.1.1.cmml" xref="Sx5.SSx3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="Sx5.SSx3.p2.1.m1.1.1.1.cmml" xref="Sx5.SSx3.p2.1.m1.1.1">subscript</csymbol><ci id="Sx5.SSx3.p2.1.m1.1.1.2.cmml" xref="Sx5.SSx3.p2.1.m1.1.1.2">ğ¸</ci><ci id="Sx5.SSx3.p2.1.m1.1.1.3.cmml" xref="Sx5.SSx3.p2.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx3.p2.1.m1.1c">E_{i}</annotation></semantics></math> is the frequency of class <math id="Sx5.SSx3.p2.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="Sx5.SSx3.p2.2.m2.1a"><mi id="Sx5.SSx3.p2.2.m2.1.1" xref="Sx5.SSx3.p2.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx3.p2.2.m2.1b"><ci id="Sx5.SSx3.p2.2.m2.1.1.cmml" xref="Sx5.SSx3.p2.2.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx3.p2.2.m2.1c">i</annotation></semantics></math> in the training dataset and <math id="Sx5.SSx3.p2.3.m3.1" class="ltx_Math" alttext="O_{i}" display="inline"><semantics id="Sx5.SSx3.p2.3.m3.1a"><msub id="Sx5.SSx3.p2.3.m3.1.1" xref="Sx5.SSx3.p2.3.m3.1.1.cmml"><mi id="Sx5.SSx3.p2.3.m3.1.1.2" xref="Sx5.SSx3.p2.3.m3.1.1.2.cmml">O</mi><mi id="Sx5.SSx3.p2.3.m3.1.1.3" xref="Sx5.SSx3.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="Sx5.SSx3.p2.3.m3.1b"><apply id="Sx5.SSx3.p2.3.m3.1.1.cmml" xref="Sx5.SSx3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="Sx5.SSx3.p2.3.m3.1.1.1.cmml" xref="Sx5.SSx3.p2.3.m3.1.1">subscript</csymbol><ci id="Sx5.SSx3.p2.3.m3.1.1.2.cmml" xref="Sx5.SSx3.p2.3.m3.1.1.2">ğ‘‚</ci><ci id="Sx5.SSx3.p2.3.m3.1.1.3.cmml" xref="Sx5.SSx3.p2.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx3.p2.3.m3.1c">O_{i}</annotation></semantics></math> is frequency of class <math id="Sx5.SSx3.p2.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="Sx5.SSx3.p2.4.m4.1a"><mi id="Sx5.SSx3.p2.4.m4.1.1" xref="Sx5.SSx3.p2.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx3.p2.4.m4.1b"><ci id="Sx5.SSx3.p2.4.m4.1.1.cmml" xref="Sx5.SSx3.p2.4.m4.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx3.p2.4.m4.1c">i</annotation></semantics></math> in the malicious dataset.
The smaller the <math id="Sx5.SSx3.p2.5.m5.1" class="ltx_Math" alttext="\chi^{2}" display="inline"><semantics id="Sx5.SSx3.p2.5.m5.1a"><msup id="Sx5.SSx3.p2.5.m5.1.1" xref="Sx5.SSx3.p2.5.m5.1.1.cmml"><mi id="Sx5.SSx3.p2.5.m5.1.1.2" xref="Sx5.SSx3.p2.5.m5.1.1.2.cmml">Ï‡</mi><mn id="Sx5.SSx3.p2.5.m5.1.1.3" xref="Sx5.SSx3.p2.5.m5.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="Sx5.SSx3.p2.5.m5.1b"><apply id="Sx5.SSx3.p2.5.m5.1.1.cmml" xref="Sx5.SSx3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="Sx5.SSx3.p2.5.m5.1.1.1.cmml" xref="Sx5.SSx3.p2.5.m5.1.1">superscript</csymbol><ci id="Sx5.SSx3.p2.5.m5.1.1.2.cmml" xref="Sx5.SSx3.p2.5.m5.1.1.2">ğœ’</ci><cn type="integer" id="Sx5.SSx3.p2.5.m5.1.1.3.cmml" xref="Sx5.SSx3.p2.5.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx3.p2.5.m5.1c">\chi^{2}</annotation></semantics></math> value, the more similar the two distributions are.
Intuitively, when drawing a sample from the malicious dataset, it quantifies how close the drawn sample is compared to the training dataset.
To demonstrate the correlation, we do a scatter plot between ASR and ChiSq and perform a linear regression using the scatter points, see FigureÂ <a href="#Sx5.F8" title="Figure 8 â€£ Curse 1: Local Attack Timing: a New Vulnerability â€£ Data Heterogeneity Brings Unseen Curses â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.
The experiments follow the same setup as in <span id="Sx5.SSx3.p2.5.1" class="ltx_text ltx_font_bold">Redemption 2</span>.
The regression curve demonstrates a good correlation between ASR and ChiSq and the points are more clustered when ChiSq distance is smaller.
To verify this, we perform experiments by varying the configuration tuples (total attack budget, ChiSq) and (attack scale, ChiSq) respectively and organize the results into heat maps, see Fig.Â <a href="#Sx5.F9" title="Figure 9 â€£ Curse 3: Malicious Data Distribution as Leverage â€£ Data Heterogeneity Brings Unseen Curses â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. The results show that overall lower ChiSq attack achieves better ASR and can even outperform attacks with higher budget but also higher ChiSq.
Although these results are â€œexpectedâ€, it is contrary to the findings in Fig.Â <a href="#Sx4.F4" title="Figure 4 â€£ Data Heterogeneity Seems to Be a Redemption â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, which indicates that the existing works on robustness of FL have not been fully evaluated on stronger attacks.</p>
</div>
</section>
</section>
<section id="Sx6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Defending the Curses Brought by Data Heterogeneity</h2>

<div id="Sx6.p1" class="ltx_para">
<p id="Sx6.p1.1" class="ltx_p">In this section, we discuss the challenges and potential directions of defending the curses brought by data heterogeneity in FL backdoor attacks.</p>
</div>
<figure id="Sx6.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Attack Success Rate comparison between without and with the proposed active defense.</figcaption>
<div id="Sx6.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:212.8pt;height:64.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-11.8pt,3.6pt) scale(0.9,0.9) ;">
<table id="Sx6.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx6.T2.1.1.1.1" class="ltx_tr">
<th id="Sx6.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="Sx6.T2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="Sx6.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx6.T2.1.1.1.1.2.1" class="ltx_text ltx_font_bold">ASR w/o Defense</span></th>
<th id="Sx6.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx6.T2.1.1.1.1.3.1" class="ltx_text ltx_font_bold">ASR w/ Defense</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx6.T2.1.1.2.1" class="ltx_tr">
<td id="Sx6.T2.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">CIFAR10</td>
<td id="Sx6.T2.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.76</td>
<td id="Sx6.T2.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.26</td>
</tr>
<tr id="Sx6.T2.1.1.3.2" class="ltx_tr">
<td id="Sx6.T2.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">FEMNIST</td>
<td id="Sx6.T2.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.96</td>
<td id="Sx6.T2.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.21</td>
</tr>
<tr id="Sx6.T2.1.1.4.3" class="ltx_tr">
<td id="Sx6.T2.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Sent140</td>
<td id="Sx6.T2.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1.0</td>
<td id="Sx6.T2.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.36</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="Sx6.p2" class="ltx_para">
<p id="Sx6.p2.1" class="ltx_p"><span id="Sx6.p2.1.1" class="ltx_text ltx_font_bold">Defending Curse 1: Cut the Short Path of Overfitting.</span> Backdooring the last batch of a malicious client results in overfitting of the local model on triggered data samples. Accumulating the overfitted model weights of malicious clients to the global model may lead to high ASR.
To defend against such a strategy, evading the overfitted weight updates during the aggregation process is critical.
There is a rich line of work for addressing this problem in traditional MLÂ <cite class="ltx_cite ltx_citemacro_citep">(Shen, Tople, and Saxena <a href="#bib.bib19" title="" class="ltx_ref">2016</a>; Wang etÂ al. <a href="#bib.bib23" title="" class="ltx_ref">2019</a>; Liu, Dolan-Gavitt, and Garg <a href="#bib.bib15" title="" class="ltx_ref">2018</a>)</cite>, but all of them require knowledge from the training data, which is infeasible in FL due to privacy requirement.
Therefore, we propose an active defense mechanism in which the aggregator assumes all clients are malicious.
The aggregator maintains a global (but small) IID dataset to train the updated weights of all the participating clients before aggregation.
The overfitting due to backdoor triggers is thus minimized and the model becomes more generalizable.
This mechanism is inspired by a previous paperÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al. <a href="#bib.bib26" title="" class="ltx_ref">2018</a>)</cite>, where the goal is to increase task accuracy while we focus on mitigating attack effectiveness.
The evaluation results are presented in TableÂ <a href="#Sx6.T2" title="Table 2 â€£ Defending the Curses Brought by Data Heterogeneity â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, where we use an IID dataset with a size equal to 10% of the total dataset on the aggregator.
The results show ASR is significantly reduced after applying this defense.
The limitation of this method is that if secure aggregation is used, it may be difficult to train individual client on the IID dataset.</p>
</div>
<div id="Sx6.p3" class="ltx_para">
<p id="Sx6.p3.1" class="ltx_p"><span id="Sx6.p3.1.1" class="ltx_text ltx_font_bold">Defending Curse 2: An Overfitting Mitigating Mechanism for Client Selection.</span>
Given skewed-feature based defense is difficult to distinguish whether the overfitting is from data heterogeneity or malicious attack, we suggest diversifying the selection of clients so that even if the local model is overfitted by backdoor triggers, the overfitted local model weights have less chance to be accumulated to the global model.
We implemented a scheduling policy as proof of concept to avoid selecting the same client in nearby rounds (e.g., a client needs to wait at least 20 rounds to be selected again) so that the malicious clients are spreading out further away, which allows FL to forget backdoors easier over time.
The results show that with the help of this defend policy, ASR decreases across every heterogeneity level and none of them achieves over 23% ASR.
We also plan to investigate more complex detection methods such as using activation clusteringÂ <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al. <a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite>, spectral signaturesÂ <cite class="ltx_cite ltx_citemacro_citep">(Tran, Li, and Madry <a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite>, and gradient shapingÂ <cite class="ltx_cite ltx_citemacro_citep">(Hong etÂ al. <a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite> in our future work and potentially combine them with the client selection mechanism.
<span id="Sx6.p3.1.2" class="ltx_text ltx_font_bold">Defending Curse 3: Protect the Training Data Distribution.</span>
As observed in <span id="Sx6.p3.1.3" class="ltx_text ltx_font_bold">Curse 3</span>, attackers can design an efficient attack by generating a similar malicious data distribution as the global data.
Existing works that change or augment training data still preserve its distribution and thus difficult to be employed hereÂ <cite class="ltx_cite ltx_citemacro_citep">(Shen, Tople, and Saxena <a href="#bib.bib19" title="" class="ltx_ref">2016</a>; Liu, Dolan-Gavitt, and Garg <a href="#bib.bib15" title="" class="ltx_ref">2018</a>; Tran, Li, and Madry <a href="#bib.bib22" title="" class="ltx_ref">2018</a>; Wang etÂ al. <a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite>.
To defend such attack strategies, we need to avoid revealing the global data distribution. We also set up a simple experiment where we simulate faking the actual global data distribution, and the malicious clients end up building their attack based on a distribution that has a high Chi-Squared value (e.g., about 0.8 in our experiments) compared to the real global distribution. With this defending strategy, the ASRs are much lower â€“ on average 0.46 (reduced from on average 0.8).
When this is not possible, we can try to mislead the attackers to believe a wrong global data distribution. We can also try to disrupt the global data distribution, such as having extra data reserved at the aggregator (similar to the proposal in <span id="Sx6.p3.1.4" class="ltx_text ltx_font_italic">Defending Curse 1</span>), or through GAN like data anonymizationÂ <cite class="ltx_cite ltx_citemacro_citep">(HukkelÃ¥s, Mester, and Lindseth <a href="#bib.bib12" title="" class="ltx_ref">2019</a>)</cite>, which can be used to design a more robust aggregation method.</p>
</div>
</section>
<section id="Sx7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Conclusion</h2>

<div id="Sx7.p1" class="ltx_para">
<p id="Sx7.p1.1" class="ltx_p">In this paper, we perform extensive empirical experiments to quantify and understand the impact brought by data heterogeneity in backdoor attacks of federated learning. We identified several redemptions and curses, and proposed some potential remedy strategies. The results show that depending on the extent of data heterogeneity the impacts of backdooring can vary significantly. The lessons learned here offer new insights for designing defenses for Federated Learning.</p>
</div>
</section>
<section id="Sx8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx8.p1" class="ltx_para">
<p id="Sx8.p1.1" class="ltx_p">This work is supported in part by the following grants: National Science Foundation CCF-1756013 and IIS-1838024 (with resources from AWS as part of the NSF BIGDATA program). We thank the anonymous reviewers for their insightful comments and suggestions.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadi etÂ al. (2016)</span>
<span class="ltx_bibblock">
Abadi, M.; Chu, A.; Goodfellow, I.; McMahan, H.Â B.; Mironov, I.; Talwar, K.;
and Zhang, L. 2016.

</span>
<span class="ltx_bibblock">Deep learning with differential privacy.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 ACM SIGSAC Conference on Computer
and Communications Security</em>, 308â€“318.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bagdasaryan etÂ al. (2018)</span>
<span class="ltx_bibblock">
Bagdasaryan, E.; Veit, A.; Hua, Y.; Estrin, D.; and Shmatikov, V. 2018.

</span>
<span class="ltx_bibblock">How to backdoor federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.00459</em> .

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhagoji etÂ al. (2019)</span>
<span class="ltx_bibblock">
Bhagoji, A.Â N.; Chakraborty, S.; Mittal, P.; and Calo, S. 2019.

</span>
<span class="ltx_bibblock">Analyzing Federated Learning through an Adversarial Lens.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, 634â€“643.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz etÂ al. (2019)</span>
<span class="ltx_bibblock">
Bonawitz, K.; Eichner, H.; Grieskamp, W.; Huba, D.; Ingerman, A.; Ivanov, V.;
Kiddon, C.; Konecny, J.; Mazzocchi, S.; McMahan, H.Â B.; etÂ al. 2019.

</span>
<span class="ltx_bibblock">Towards Federated Learning at Scale: System Design.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.01046</em> .

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz etÂ al. (2017)</span>
<span class="ltx_bibblock">
Bonawitz, K.; Ivanov, V.; Kreuter, B.; Marcedone, A.; McMahan, H.Â B.; Patel,
S.; Ramage, D.; Segal, A.; and Seth, K. 2017.

</span>
<span class="ltx_bibblock">Practical secure aggregation for privacy-preserving machine learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security</em>, 1175â€“1191.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldas etÂ al. (2018)</span>
<span class="ltx_bibblock">
Caldas, S.; Wu, P.; Li, T.; KoneÄná»³, J.; McMahan, H.Â B.; Smith, V.;
and Talwalkar, A. 2018.

</span>
<span class="ltx_bibblock">Leaf: A benchmark for federated settings.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.01097</em> .

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chai etÂ al. (2020)</span>
<span class="ltx_bibblock">
Chai, Z.; Ali, A.; Zawad, S.; Truex, S.; Anwar, A.; Baracaldo, N.; Zhou, Y.;
Ludwig, H.; Yan, F.; and Cheng, Y. 2020.

</span>
<span class="ltx_bibblock">TiFL: A Tier-based Federated Learning System.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th International Symposium on
High-Performance Parallel and Distributed Computing</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2018)</span>
<span class="ltx_bibblock">
Chen, B.; Carvalho, W.; Baracaldo, N.; Ludwig, H.; Edwards, B.; Lee, T.;
Molloy, I.; and Srivastava, B. 2018.

</span>
<span class="ltx_bibblock">Detecting backdoor attacks on deep neural networks by activation
clustering.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.03728</em> .

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2017)</span>
<span class="ltx_bibblock">
Chen, X.; Liu, C.; Li, B.; Lu, K.; and Song, D. 2017.

</span>
<span class="ltx_bibblock">Targeted backdoor attacks on deep learning systems using data
poisoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1712.05526</em> .

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fung, Yoon, and Beschastnikh (2018)</span>
<span class="ltx_bibblock">
Fung, C.; Yoon, C.Â J.; and Beschastnikh, I. 2018.

</span>
<span class="ltx_bibblock">Mitigating sybils in federated learning poisoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1808.04866</em> .

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hong etÂ al. (2020)</span>
<span class="ltx_bibblock">
Hong, S.; Chandrasekaran, V.; Kaya, Y.; DumitraÅŸ, T.; and Papernot, N.
2020.

</span>
<span class="ltx_bibblock">On the Effectiveness of Mitigating Data Poisoning Attacks with
Gradient Shaping.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.11497</em> .

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">HukkelÃ¥s, Mester, and Lindseth (2019)</span>
<span class="ltx_bibblock">
HukkelÃ¥s, H.; Mester, R.; and Lindseth, F. 2019.

</span>
<span class="ltx_bibblock">DeepPrivacy: A Generative Adversarial Network for Face Anonymization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">International Symposium on Visual Computing</em>, 565â€“578.
Springer.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2020)</span>
<span class="ltx_bibblock">
Li, T.; Sahu, A.Â K.; Talwalkar, A.; and Smith, V. 2020.

</span>
<span class="ltx_bibblock">Federated Learning: Challenges, Methods, and Future Directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em> 37(3): 50â€“60.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2019)</span>
<span class="ltx_bibblock">
Li, X.; Huang, K.; Yang, W.; Wang, S.; and Zhang, Z. 2019.

</span>
<span class="ltx_bibblock">On the convergence of fedavg on non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.02189</em> .

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu, Dolan-Gavitt, and Garg (2018)</span>
<span class="ltx_bibblock">
Liu, K.; Dolan-Gavitt, B.; and Garg, S. 2018.

</span>
<span class="ltx_bibblock">Fine-pruning: Defending against backdooring attacks on deep neural
networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">International Symposium on Research in Attacks, Intrusions,
and Defenses</em>, 273â€“294. Springer.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma, Zhu, and Hsu (2019)</span>
<span class="ltx_bibblock">
Ma, Y.; Zhu, X.; and Hsu, J. 2019.

</span>
<span class="ltx_bibblock">Data poisoning against differentially-private learners: Attacks and
defenses.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1903.09860</em> .

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan etÂ al. (2016)</span>
<span class="ltx_bibblock">
McMahan, H.Â B.; Moore, E.; Ramage, D.; Hampson, S.; etÂ al. 2016.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1602.05629</em> .

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sattler etÂ al. (2019)</span>
<span class="ltx_bibblock">
Sattler, F.; Wiedemann, S.; MÃ¼ller, K.-R.; and Samek, W. 2019.

</span>
<span class="ltx_bibblock">Robust and communication-efficient federated learning from non-iid
data.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on neural networks and learning systems</em> .

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen, Tople, and Saxena (2016)</span>
<span class="ltx_bibblock">
Shen, S.; Tople, S.; and Saxena, P. 2016.

</span>
<span class="ltx_bibblock">Auror: Defending against poisoning attacks in collaborative deep
learning systems.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 32nd Annual Conference on Computer
Security Applications</em>, 508â€“519.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Steinhardt, Koh, and Liang (2017)</span>
<span class="ltx_bibblock">
Steinhardt, J.; Koh, P. W.Â W.; and Liang, P.Â S. 2017.

</span>
<span class="ltx_bibblock">Certified defenses for data poisoning attacks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
3517â€“3529.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al. (2019)</span>
<span class="ltx_bibblock">
Sun, Z.; Kairouz, P.; Suresh, A.Â T.; and McMahan, H.Â B. 2019.

</span>
<span class="ltx_bibblock">Can You Really Backdoor Federated Learning?

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.07963</em> .

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tran, Li, and Madry (2018)</span>
<span class="ltx_bibblock">
Tran, B.; Li, J.; and Madry, A. 2018.

</span>
<span class="ltx_bibblock">Spectral signatures in backdoor attacks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
8000â€“8010.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2019)</span>
<span class="ltx_bibblock">
Wang, B.; Yao, Y.; Shan, S.; Li, H.; Viswanath, B.; Zheng, H.; and Zhao, B.Â Y.
2019.

</span>
<span class="ltx_bibblock">Neural cleanse: Identifying and mitigating backdoor attacks in neural
networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">2019 IEEE Symposium on Security and Privacy (SP)</em>, 707â€“723.
IEEE.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie etÂ al. (2019)</span>
<span class="ltx_bibblock">
Xie, C.; Huang, K.; Chen, P.-Y.; and Li, B. 2019.

</span>
<span class="ltx_bibblock">DBA: Distributed Backdoor Attacks against Federated Learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2019)</span>
<span class="ltx_bibblock">
Zhang, J.; Chen, J.; Wu, D.; Chen, B.; and Yu, S. 2019.

</span>
<span class="ltx_bibblock">Poisoning Attack in Federated Learning using Generative Adversarial
Nets.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">2019 18th IEEE International Conference On Trust, Security
And Privacy In Computing And Communications/13th IEEE International
Conference On Big Data Science And Engineering (TrustCom/BigDataSE)</em>,
374â€“380. IEEE.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2018)</span>
<span class="ltx_bibblock">
Zhao, Y.; Li, M.; Lai, L.; Suda, N.; Civin, D.; and Chandra, V. 2018.

</span>
<span class="ltx_bibblock">Federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.00582</em> .

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Sx9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Appendix</h2>

<figure id="Sx9.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx9.F10.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/appendix/femnist.png" id="Sx9.F10.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="374" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx9.F10.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/appendix/cifar10.png" id="Sx9.F10.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="374" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx9.F10.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/appendix/sent140.png" id="Sx9.F10.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="374" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Attack Success Rate (ASR) vs. Data Heterogeneity simulated by Gaussian Sampling (lower variance represents higher heterogeneity).</figcaption>
</figure>
<figure id="Sx9.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx9.F11.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/appendix/femnist_cos.png" id="Sx9.F11.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx9.F11.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/appendix/cifar10_cos.png" id="Sx9.F11.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx9.F11.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/appendix/sent140_cos.png" id="Sx9.F11.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Cosine Similarities comparison between benign and malicious clients under different Data Heterogeneity simulated by Gaussian Sampling (lower variance represents higher heterogeneity).</figcaption>
</figure>
<section id="Sx9.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Data Heterogeneity by Gaussian Sampling</h3>

<div id="Sx9.SSx1.p1" class="ltx_para">
<p id="Sx9.SSx1.p1.1" class="ltx_p">Almost all existing works in federated learning simulate data heterogeneity
by limiting the number of classes available in each clientÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al. <a href="#bib.bib26" title="" class="ltx_ref">2018</a>; Chai etÂ al. <a href="#bib.bib7" title="" class="ltx_ref">2020</a>; Bonawitz etÂ al. <a href="#bib.bib5" title="" class="ltx_ref">2017</a>; Li etÂ al. <a href="#bib.bib14" title="" class="ltx_ref">2019</a>; Sattler etÂ al. <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>.
We followed existing works when conducting experiments in the main draft.
To evaluate whether our findings are robust to different data heterogeneity, here we provide another way to simulate the data heterogeneity by using Gaussian samplingÂ <cite class="ltx_cite ltx_citemacro_citep">(Bhagoji etÂ al. <a href="#bib.bib3" title="" class="ltx_ref">2019</a>)</cite>. We employ Gaussian Sampling to sample data from the total dataset for creating dataset for each client. The heterogeneity of data can be controlled by tuning the variance of the Gaussian distribution used for sampling (in Gaussian Sampling, a higher variance represents a wider distribution of data sampling), which correlates to the diversity of the features in the sampled datasets that determines the data heterogeneity. In other words, a higher variance represents the case that we select a more diverse set of data points from the total dataset.
We generate the same Attack Success Rate and Cosine Similarities plots as in the main draft (i.e., Figure 2 and Figure 7 in the main draft) and shown in FigureÂ <a href="#Sx9.F10" title="Figure 10 â€£ Appendix â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> and FigureÂ <a href="#Sx9.F11" title="Figure 11 â€£ Appendix â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>.
We can see the observations obtained in the main draft are consistent with the results present here, which verifies that our findings hold under different ways of simulating data heterogeneity.</p>
</div>
</section>
<section id="Sx9.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Weight Scaling Factor Analysis</h3>

<figure id="Sx9.F12" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx9.F12.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/appendix/femnist_scaled_weights.png" id="Sx9.F12.1.g1" class="ltx_graphics ltx_img_square" width="598" height="486" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx9.F12.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/appendix/cosine_scaled_weights.png" id="Sx9.F12.2.g1" class="ltx_graphics ltx_img_square" width="598" height="486" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="Sx9.F12.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/appendix/sent140_scaled_weights.png" id="Sx9.F12.3.g1" class="ltx_graphics ltx_img_square" width="598" height="486" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Attack success rate (ASR) and accuracy (ACC) comparison under different weight scaling factor (1, 5, 10).</figcaption>
</figure>
<div id="Sx9.SSx2.p1" class="ltx_para">
<p id="Sx9.SSx2.p1.1" class="ltx_p"><span id="Sx9.SSx2.p1.1.1" class="ltx_text ltx_font_italic">Model replacement attacks</span> (aka model poisoning attacks) attempt to replace the benign model with a malicious model, which is what backdooring on local devices aims to achieve. The malicious clients train backdoor into their local models and then send the weights to the server in an attempt to make the aggregation algorithm replace the global model with the backdoored model.
If the malicious weights during weights aggregation are pronounced enough, the malicious weights can overwhelm the aggregation process to cause model replacement attacks.
As pointed out inÂ <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan etÂ al. <a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>, data poisoning attacks in federated learning are in fact subsumed by model replacement attacks.
To demonstrate this, we run experiments by scaling up the weights of the models by a factor of 5 and 10 respectively and plot the corresponding attack success rate (ASR) and accuracy (ACC) in FigureÂ <a href="#Sx9.F12" title="Figure 12 â€£ Weight Scaling Factor Analysis â€£ Appendix â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>.
The results show that with scaled up weights, the attack success rate is only slightly better but the model accuracy is decreased. This suggests the findings of backdooring attack in this paper can be generalized to model replacement attack.
However, scaling up the weights in practice is difficult to achieve due to the privacy protection mechanism such asÂ <cite class="ltx_cite ltx_citemacro_citep">(Abadi etÂ al. <a href="#bib.bib1" title="" class="ltx_ref">2016</a>; Bonawitz etÂ al. <a href="#bib.bib5" title="" class="ltx_ref">2017</a>)</cite>.
In addition, scaled up weights can be detected as outliers compared to weights of benign clientsÂ <cite class="ltx_cite ltx_citemacro_citep">(Sun etÂ al. <a href="#bib.bib21" title="" class="ltx_ref">2019</a>; Bagdasaryan etÂ al. <a href="#bib.bib2" title="" class="ltx_ref">2018</a>; Xie etÂ al. <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite>.
Therefore, in the main draft, we focus on non-scaled weights case (i.e., weight scaling factor is 1).</p>
</div>
</section>
<section id="Sx9.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Different Attack Strategies</h3>

<figure id="Sx9.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="Sx9.F13.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/appendix/femnist_hmap_1.png" id="Sx9.F13.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="673" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Attack 1</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="Sx9.F13.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/appendix/femnist_hmap_2.png" id="Sx9.F13.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="673" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Attack 2</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="Sx9.F13.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/appendix/cifar10_hmap_1.png" id="Sx9.F13.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="673" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Attack 1</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="Sx9.F13.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2102.00655/assets/images/appendix/cifar10_hmap_2.png" id="Sx9.F13.sf4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="673" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>Attack 2</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Attack Success Rate (ASR) scalability in terms of attack scale and total attack budget when using Attack 1 and Attack 2.</figcaption>
</figure>
<div id="Sx9.SSx3.p1" class="ltx_para">
<p id="Sx9.SSx3.p1.1" class="ltx_p">We use the attack strategy proposed inÂ <cite class="ltx_cite ltx_citemacro_citep">(Xie etÂ al. <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite> for our analysis in the main draft as it claims more efficient attack than other existing works. To ensure our findings are robust to different attacks, we also conduct the experiments using the attack strategies proposed inÂ <cite class="ltx_cite ltx_citemacro_citep">(Sun etÂ al. <a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite> andÂ <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al. <a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite> (labeled Attack 1 and Attack 2 respectively).
Specifically, we run the same experiments as shown in Figure 4 of the main draft to evaluate the Attack Success Rate (ASR) scalability in terms of attack scale and total attack budget, see FigureÂ <a href="#Sx9.F13" title="Figure 13 â€£ Different Attack Strategies â€£ Appendix â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>. It is worth noting that the results of Sent140 is not included here because both the strategies focus on image-based applications.
We get the same counter-intuitive results when using the attack strategy proposed inÂ <cite class="ltx_cite ltx_citemacro_citep">(Xie etÂ al. <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite>: there is no clear pattern in the heat maps, which is in contrary to the conclusion made by almost all existing workÂ <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan etÂ al. <a href="#bib.bib2" title="" class="ltx_ref">2018</a>; Fung, Yoon, and Beschastnikh <a href="#bib.bib10" title="" class="ltx_ref">2018</a>; Sun etÂ al. <a href="#bib.bib21" title="" class="ltx_ref">2019</a>; Xie etÂ al. <a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite> that higher <span id="Sx9.SSx3.p1.1.1" class="ltx_text ltx_font_italic">attack scale</span> and <span id="Sx9.SSx3.p1.1.2" class="ltx_text ltx_font_italic">total attack budget</span> always leads to more effective attacks.</p>
</div>
</section>
<section id="Sx9.SSx4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Evaluation on Defense Strategies</h3>

<div id="Sx9.SSx4.p1" class="ltx_para">
<p id="Sx9.SSx4.p1.1" class="ltx_p">For defense, we proposed several strategies in the main draft by taking the data heterogeneity into account, which is overlooked by existing defense methods.
For strategies <span id="Sx9.SSx4.p1.1.1" class="ltx_text ltx_font_bold">Defending Curse 2</span> proposed in the main draft, one of the defense method is to diversify the selection of clients so that even if the local model is overfitted by backdoor triggers, the overfitted local model weights have less chance to be accumulated to the global model. To verify the effectiveness of such strategy, we implement a uniform random selection policy with a <span id="Sx9.SSx4.p1.1.2" class="ltx_text ltx_font_italic">selection separation factor</span> defined as the minimum number of rounds that a client can be selected.
We present the results in TableÂ <a href="#Sx9.T3" title="Table 3 â€£ Evaluation on Defense Strategies â€£ Appendix â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> where we show the Attack Success Rates under different <span id="Sx9.SSx4.p1.1.3" class="ltx_text ltx_font_italic">selection separation factor</span> values.
We can see when the factor is increasing, the Attack Success Rate drops significantly.
Therefore, we consider spacing out client selection is a promising defense strategy for defending Curse 2.</p>
</div>
<figure id="Sx9.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Defending Curse 2: spreading out client selection over rounds. The value reported in the table is the Attack Success Rate.</figcaption>
<table id="Sx9.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx9.T3.1.1.1" class="ltx_tr">
<th id="Sx9.T3.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r"></th>
<th id="Sx9.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3"><span id="Sx9.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Selection Separation Factor</span></th>
</tr>
<tr id="Sx9.T3.1.2.2" class="ltx_tr">
<th id="Sx9.T3.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_border_r"></th>
<th id="Sx9.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx9.T3.1.2.2.2.1" class="ltx_text ltx_font_bold">10</span></th>
<th id="Sx9.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx9.T3.1.2.2.3.1" class="ltx_text ltx_font_bold">20</span></th>
<th id="Sx9.T3.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx9.T3.1.2.2.4.1" class="ltx_text ltx_font_bold">50</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx9.T3.1.3.1" class="ltx_tr">
<td id="Sx9.T3.1.3.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="Sx9.T3.1.3.1.1.1" class="ltx_text ltx_font_bold">FEMNIST</span></td>
<td id="Sx9.T3.1.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Â Â  0.87</td>
<td id="Sx9.T3.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Â Â  0.16</td>
<td id="Sx9.T3.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.14</td>
</tr>
<tr id="Sx9.T3.1.4.2" class="ltx_tr">
<td id="Sx9.T3.1.4.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="Sx9.T3.1.4.2.1.1" class="ltx_text ltx_font_bold">Cifar10</span></td>
<td id="Sx9.T3.1.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.83</td>
<td id="Sx9.T3.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.23</td>
<td id="Sx9.T3.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.11</td>
</tr>
<tr id="Sx9.T3.1.5.3" class="ltx_tr">
<td id="Sx9.T3.1.5.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="Sx9.T3.1.5.3.1.1" class="ltx_text ltx_font_bold">Sent140</span></td>
<td id="Sx9.T3.1.5.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1.0</td>
<td id="Sx9.T3.1.5.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.2</td>
<td id="Sx9.T3.1.5.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.2</td>
</tr>
</tbody>
</table>
</figure>
<div id="Sx9.SSx4.p2" class="ltx_para">
<p id="Sx9.SSx4.p2.1" class="ltx_p">For strategies <span id="Sx9.SSx4.p2.1.1" class="ltx_text ltx_font_bold">Defending Curse 3</span> proposed in the main draft, one approach is to mislead the attackers to believe a false global data distribution.
To verify this idea, we generate false global data distributions based on the Chi-squared (ChiSq) distance of the true global data distribution and on purposely disclosure this false distribution information to the attackers.
In Curse 3, attackers can leverage the (true) global data distribution to generate highly effective attacks.
However, when they use the false global data distribution to generate attacks, the attack success rate is significantly dropped and the larger the ChiSq distance between false and true global data distribution, the larger drop in attack success rate, see TableÂ <a href="#Sx9.T4" title="Table 4 â€£ Evaluation on Defense Strategies â€£ Appendix â€£ Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
Therefore, if we can mislead attackers to believe a false global data distribution, we can defense well backdooring attacks.</p>
</div>
<figure id="Sx9.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Defending Curse 3: mislead attackers to believe a false global data distribution. The value reported in the table is the Attack Success Rate. The ChiSq distance is computed between false and true global data distribution.</figcaption>
<table id="Sx9.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx9.T4.1.1.1" class="ltx_tr">
<th id="Sx9.T4.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r"></th>
<th id="Sx9.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="4"><span id="Sx9.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">ChiSq Distance</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx9.T4.1.2.1" class="ltx_tr">
<td id="Sx9.T4.1.2.1.1" class="ltx_td ltx_border_r"></td>
<td id="Sx9.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx9.T4.1.2.1.2.1" class="ltx_text ltx_font_bold">0.0</span></td>
<td id="Sx9.T4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx9.T4.1.2.1.3.1" class="ltx_text ltx_font_bold">0.5</span></td>
<td id="Sx9.T4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx9.T4.1.2.1.4.1" class="ltx_text ltx_font_bold">0.7</span></td>
<td id="Sx9.T4.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx9.T4.1.2.1.5.1" class="ltx_text ltx_font_bold">0.9</span></td>
</tr>
<tr id="Sx9.T4.1.3.2" class="ltx_tr">
<td id="Sx9.T4.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="Sx9.T4.1.3.2.1.1" class="ltx_text ltx_font_bold">FEMNIST</span></td>
<td id="Sx9.T4.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.91</td>
<td id="Sx9.T4.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.45</td>
<td id="Sx9.T4.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.33</td>
<td id="Sx9.T4.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.24</td>
</tr>
<tr id="Sx9.T4.1.4.3" class="ltx_tr">
<td id="Sx9.T4.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="Sx9.T4.1.4.3.1.1" class="ltx_text ltx_font_bold">Cifar10</span></td>
<td id="Sx9.T4.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.84</td>
<td id="Sx9.T4.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.33</td>
<td id="Sx9.T4.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.23</td>
<td id="Sx9.T4.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.18</td>
</tr>
<tr id="Sx9.T4.1.5.4" class="ltx_tr">
<td id="Sx9.T4.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="Sx9.T4.1.5.4.1.1" class="ltx_text ltx_font_bold">Sent140</span></td>
<td id="Sx9.T4.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1.0</td>
<td id="Sx9.T4.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.2</td>
<td id="Sx9.T4.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.1</td>
<td id="Sx9.T4.1.5.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.1</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2102.00654" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2102.00655" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2102.00655">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2102.00655" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2102.00656" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 14:57:05 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
