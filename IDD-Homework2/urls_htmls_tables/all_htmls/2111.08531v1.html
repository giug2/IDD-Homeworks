<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2111.08531] Language bias in Visual Question Answering: A Survey and Taxonomy</title><meta property="og:description" content="Visual question answering (VQA) is a challenging task, which has attracted more and more attention in the field of computer vision and natural language processing. However, the current visual question answering has the…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Language bias in Visual Question Answering: A Survey and Taxonomy">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Language bias in Visual Question Answering: A Survey and Taxonomy">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2111.08531">

<!--Generated on Wed Mar  6 20:10:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Visual Question Answering,  Natural Language Processing,  Computer Vision,  Language Bias,  Deep Learning
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Language bias in Visual Question Answering: A Survey and Taxonomy</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Desen Yuan
</span><span class="ltx_author_notes">D. Yuan were with the School
of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu. This is the first version of this survey. There are some grammatical and detailed problems. Due to the author’s time, the survey will be updated in recent months.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Visual question answering (VQA) is a challenging task, which has attracted more and more attention in the field of computer vision and natural language processing. However, the current visual question answering has the problem of language bias, which reduces the robustness of the model and has an adverse impact on the practical application of visual question answering. In this paper, we conduct a comprehensive review and analysis of this field for the first time, and classify the existing methods according to three categories, including enhancing visual information, weakening language priors, data enhancement and training strategies. At the same time, the relevant representative methods are introduced, summarized and analyzed in turn. The causes of language bias are revealed and classified. Secondly, this paper introduces the data sets mainly used for testing, and reports the experimental results of various existing methods. Finally, we discuss the possible future research directions in this field.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Visual Question Answering, Natural Language Processing, Computer Vision, Language Bias, Deep Learning

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual question answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> is a cross task combining computer vision and natural language processing. It is a hot task in the application and research of deep learning. Computer vision task studies how to let the machine process and understand the content in the image. Its main research contents include object detection<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, action recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, image classification<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, image segmentation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, image dehazing<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> and so on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. Natural language processing (NLP) is a process that allows machines to analyze and understand human language, including articles, sentences and emotions. Its main research tasks include machine translation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, named entity recognition<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, emotion recognition<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> and so on. Visual question answering belongs to the cross field of these two research fields, namely multimodal machine learning. Visual question answering is a downstream task of multimodal machine learning.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In the application and research of multimodal machine learning, visual question answering is becoming more and more important. In the past decades, significant research progress has been made in computer vision and natural language processing. The explosive growth of visual and text data that can be obtained and processed makes these two fields develop rapidly. At present, the widely used and proven effective methods in computer vision and natural language processing include convolution neural network, cyclic neural network, long-term and short-term memory network and transfmer. These methods have good applications in visual problem tasks.
</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In the most common form of visual question and answer (VQA), data includes a picture and a question, which requires the machine to give the corresponding answer to its question. The main difference from other computer vision tasks is that the questions to be answered by the model change in real time, while in other computer vision tasks, such as target detection and target segmentation, they are given in advance. Only the image changes, while the image and text of VQA task change in real time. VQA task is more in line with the real form of artificial intelligence, which can help the model to understand vision and language more deeply. The answer can be searched in a given text set or external knowledge base. Compared with the pure text answer (QA) task, VQA task adds image data containing more information. However, due to the noisy information of image data and the lack of information structure and syntax rules that can be simply described, it is very different from text data.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Therefore, VQA can be regarded as a real and perfect application of artificial intelligence technology, which requires multi-domain and multi-modal knowledge. However, due to the bottleneck of task evaluation, the current VQA task still has great limitations. The generated answer is usually relatively simple, mostly a few words or simple sentences, rather than longer sentences. VQA task is still a challenging and open research problem.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In the field of visual question answering, a hot research problem in recent years is how to solve the problem of language bias. For example, for many visual question answering models, the answer can be inferred directly only through language data. Bias in VQA generally refers to   Language bias, which makes the model rely on the surface correlation between the question and the answer when answering the question, and ignores the image information. A classic example is that for the question ”what color is the banana in the figure?”, although the banana in the figure is immature ”green”, the model still tends to predict ”yellow”.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Language bias causes great difficulties in the application of VQA. Language bias can be decomposed into language shortcut bias generated by the model or language data bias caused by problem data distribution. Due to the existence of language bias, the model will have poor robustness in application, and the interpretability of the model is very low. Similar to language bias, this bias often exists widely in multi-modal data, which seriously affects the practical application of multimodal machine learning. At the same time, it may lead to discrimination. For example, deploying relevant VQA models in public places such as social platforms may produce some columns of discrimination and misleading due to the bias, affecting the fair use of technology.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">In order to solve the problem of language bias, a large number of papers on how to solve language bias have appeared in the past few years. The purpose of this survey is to give a comprehensive overview of this field, including models, data sets, etc., and put forward the possible future direction of this field. As far as we know, this paper is the first survey on language bias in VQA.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">In the first part of this survey (Chapter 2), according to the current methods to solve language bias, this paper divides the existing methods into three categories: enhancing visual information, weakening language a priori, data enhancement and training strategies. At the same time, the basic network architecture of existing methods is also introduced. In the third chapter, this paper introduces the relevant information of the current mainstream data sets and the relevant evaluation schemes, and provides the experimental results of the existing methods. In Chapter 4, we synthesize the current methods and the core problems of language bias, and give the possible development direction of this field in the future. In Chapter 5, we review the content of this paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Methods for debiased VQA</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Backbones</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.4" class="ltx_p">In order to solve the multi class classification problem in the field of VQA, researchers have proposedd many basic models to be applied to VQA. The general form of VQA is: Given a dataset <math id="S2.SS1.p1.1.m1.3" class="ltx_Math" alttext="\mathcal{D}=\left\{I_{i},Q_{i},a_{i}\right\}^{N}" display="inline"><semantics id="S2.SS1.p1.1.m1.3a"><mrow id="S2.SS1.p1.1.m1.3.3" xref="S2.SS1.p1.1.m1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.1.m1.3.3.5" xref="S2.SS1.p1.1.m1.3.3.5.cmml">𝒟</mi><mo id="S2.SS1.p1.1.m1.3.3.4" xref="S2.SS1.p1.1.m1.3.3.4.cmml">=</mo><msup id="S2.SS1.p1.1.m1.3.3.3" xref="S2.SS1.p1.1.m1.3.3.3.cmml"><mrow id="S2.SS1.p1.1.m1.3.3.3.3.3" xref="S2.SS1.p1.1.m1.3.3.3.3.4.cmml"><mo id="S2.SS1.p1.1.m1.3.3.3.3.3.4" xref="S2.SS1.p1.1.m1.3.3.3.3.4.cmml">{</mo><msub id="S2.SS1.p1.1.m1.1.1.1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1.1.1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.1.1.1.1.2.cmml">I</mi><mi id="S2.SS1.p1.1.m1.1.1.1.1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.SS1.p1.1.m1.3.3.3.3.3.5" xref="S2.SS1.p1.1.m1.3.3.3.3.4.cmml">,</mo><msub id="S2.SS1.p1.1.m1.2.2.2.2.2.2" xref="S2.SS1.p1.1.m1.2.2.2.2.2.2.cmml"><mi id="S2.SS1.p1.1.m1.2.2.2.2.2.2.2" xref="S2.SS1.p1.1.m1.2.2.2.2.2.2.2.cmml">Q</mi><mi id="S2.SS1.p1.1.m1.2.2.2.2.2.2.3" xref="S2.SS1.p1.1.m1.2.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S2.SS1.p1.1.m1.3.3.3.3.3.6" xref="S2.SS1.p1.1.m1.3.3.3.3.4.cmml">,</mo><msub id="S2.SS1.p1.1.m1.3.3.3.3.3.3" xref="S2.SS1.p1.1.m1.3.3.3.3.3.3.cmml"><mi id="S2.SS1.p1.1.m1.3.3.3.3.3.3.2" xref="S2.SS1.p1.1.m1.3.3.3.3.3.3.2.cmml">a</mi><mi id="S2.SS1.p1.1.m1.3.3.3.3.3.3.3" xref="S2.SS1.p1.1.m1.3.3.3.3.3.3.3.cmml">i</mi></msub><mo id="S2.SS1.p1.1.m1.3.3.3.3.3.7" xref="S2.SS1.p1.1.m1.3.3.3.3.4.cmml">}</mo></mrow><mi id="S2.SS1.p1.1.m1.3.3.3.5" xref="S2.SS1.p1.1.m1.3.3.3.5.cmml">N</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.3b"><apply id="S2.SS1.p1.1.m1.3.3.cmml" xref="S2.SS1.p1.1.m1.3.3"><eq id="S2.SS1.p1.1.m1.3.3.4.cmml" xref="S2.SS1.p1.1.m1.3.3.4"></eq><ci id="S2.SS1.p1.1.m1.3.3.5.cmml" xref="S2.SS1.p1.1.m1.3.3.5">𝒟</ci><apply id="S2.SS1.p1.1.m1.3.3.3.cmml" xref="S2.SS1.p1.1.m1.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.3.3.3.4.cmml" xref="S2.SS1.p1.1.m1.3.3.3">superscript</csymbol><set id="S2.SS1.p1.1.m1.3.3.3.3.4.cmml" xref="S2.SS1.p1.1.m1.3.3.3.3.3"><apply id="S2.SS1.p1.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p1.1.m1.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.1.1.2">𝐼</ci><ci id="S2.SS1.p1.1.m1.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S2.SS1.p1.1.m1.2.2.2.2.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.2.2.2.2.2.2.1.cmml" xref="S2.SS1.p1.1.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S2.SS1.p1.1.m1.2.2.2.2.2.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2.2.2.2.2.2">𝑄</ci><ci id="S2.SS1.p1.1.m1.2.2.2.2.2.2.3.cmml" xref="S2.SS1.p1.1.m1.2.2.2.2.2.2.3">𝑖</ci></apply><apply id="S2.SS1.p1.1.m1.3.3.3.3.3.3.cmml" xref="S2.SS1.p1.1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.3.3.3.3.3.3.1.cmml" xref="S2.SS1.p1.1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S2.SS1.p1.1.m1.3.3.3.3.3.3.2.cmml" xref="S2.SS1.p1.1.m1.3.3.3.3.3.3.2">𝑎</ci><ci id="S2.SS1.p1.1.m1.3.3.3.3.3.3.3.cmml" xref="S2.SS1.p1.1.m1.3.3.3.3.3.3.3">𝑖</ci></apply></set><ci id="S2.SS1.p1.1.m1.3.3.3.5.cmml" xref="S2.SS1.p1.1.m1.3.3.3.5">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.3c">\mathcal{D}=\left\{I_{i},Q_{i},a_{i}\right\}^{N}</annotation></semantics></math> containing N triplets of images <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="I_{i}\in\mathcal{I}" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mrow id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><msub id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml"><mi id="S2.SS1.p1.2.m2.1.1.2.2" xref="S2.SS1.p1.2.m2.1.1.2.2.cmml">I</mi><mi id="S2.SS1.p1.2.m2.1.1.2.3" xref="S2.SS1.p1.2.m2.1.1.2.3.cmml">i</mi></msub><mo id="S2.SS1.p1.2.m2.1.1.1" xref="S2.SS1.p1.2.m2.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml">ℐ</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><in id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1.1"></in><apply id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.2.1.cmml" xref="S2.SS1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.2.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2.2">𝐼</ci><ci id="S2.SS1.p1.2.m2.1.1.2.3.cmml" xref="S2.SS1.p1.2.m2.1.1.2.3">𝑖</ci></apply><ci id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3">ℐ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">I_{i}\in\mathcal{I}</annotation></semantics></math> , questions <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="Q_{i}\in\mathcal{Q}" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mrow id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml"><msub id="S2.SS1.p1.3.m3.1.1.2" xref="S2.SS1.p1.3.m3.1.1.2.cmml"><mi id="S2.SS1.p1.3.m3.1.1.2.2" xref="S2.SS1.p1.3.m3.1.1.2.2.cmml">Q</mi><mi id="S2.SS1.p1.3.m3.1.1.2.3" xref="S2.SS1.p1.3.m3.1.1.2.3.cmml">i</mi></msub><mo id="S2.SS1.p1.3.m3.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.3.m3.1.1.3" xref="S2.SS1.p1.3.m3.1.1.3.cmml">𝒬</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"><in id="S2.SS1.p1.3.m3.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1.1"></in><apply id="S2.SS1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.3.m3.1.1.2.1.cmml" xref="S2.SS1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.3.m3.1.1.2.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2.2">𝑄</ci><ci id="S2.SS1.p1.3.m3.1.1.2.3.cmml" xref="S2.SS1.p1.3.m3.1.1.2.3">𝑖</ci></apply><ci id="S2.SS1.p1.3.m3.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3">𝒬</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">Q_{i}\in\mathcal{Q}</annotation></semantics></math> and answers <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="a_{i}\in\mathcal{A}" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mrow id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml"><msub id="S2.SS1.p1.4.m4.1.1.2" xref="S2.SS1.p1.4.m4.1.1.2.cmml"><mi id="S2.SS1.p1.4.m4.1.1.2.2" xref="S2.SS1.p1.4.m4.1.1.2.2.cmml">a</mi><mi id="S2.SS1.p1.4.m4.1.1.2.3" xref="S2.SS1.p1.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="S2.SS1.p1.4.m4.1.1.1" xref="S2.SS1.p1.4.m4.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.4.m4.1.1.3" xref="S2.SS1.p1.4.m4.1.1.3.cmml">𝒜</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><apply id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1"><in id="S2.SS1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1.1"></in><apply id="S2.SS1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.1.2.1.cmml" xref="S2.SS1.p1.4.m4.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.4.m4.1.1.2.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2.2">𝑎</ci><ci id="S2.SS1.p1.4.m4.1.1.2.3.cmml" xref="S2.SS1.p1.4.m4.1.1.2.3">𝑖</ci></apply><ci id="S2.SS1.p1.4.m4.1.1.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3">𝒜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">a_{i}\in\mathcal{A}</annotation></semantics></math>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.2" class="ltx_p">The aim of the VQA task is to learn a mapping function <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="f_{vqa}" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><msub id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">f</mi><mrow id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml"><mi id="S2.SS1.p2.1.m1.1.1.3.2" xref="S2.SS1.p2.1.m1.1.1.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.1.m1.1.1.3.1" xref="S2.SS1.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p2.1.m1.1.1.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.cmml">q</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.1.m1.1.1.3.1a" xref="S2.SS1.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.p2.1.m1.1.1.3.4" xref="S2.SS1.p2.1.m1.1.1.3.4.cmml">a</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">𝑓</ci><apply id="S2.SS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3"><times id="S2.SS1.p2.1.m1.1.1.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3.1"></times><ci id="S2.SS1.p2.1.m1.1.1.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.2">𝑣</ci><ci id="S2.SS1.p2.1.m1.1.1.3.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3">𝑞</ci><ci id="S2.SS1.p2.1.m1.1.1.3.4.cmml" xref="S2.SS1.p2.1.m1.1.1.3.4">𝑎</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">f_{vqa}</annotation></semantics></math>:<math id="S2.SS1.p2.2.m2.3" class="ltx_Math" alttext="I\times Q\to[0,1]^{\left|\mathcal{A}\right|}" display="inline"><semantics id="S2.SS1.p2.2.m2.3a"><mrow id="S2.SS1.p2.2.m2.3.4" xref="S2.SS1.p2.2.m2.3.4.cmml"><mrow id="S2.SS1.p2.2.m2.3.4.2" xref="S2.SS1.p2.2.m2.3.4.2.cmml"><mi id="S2.SS1.p2.2.m2.3.4.2.2" xref="S2.SS1.p2.2.m2.3.4.2.2.cmml">I</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p2.2.m2.3.4.2.1" xref="S2.SS1.p2.2.m2.3.4.2.1.cmml">×</mo><mi id="S2.SS1.p2.2.m2.3.4.2.3" xref="S2.SS1.p2.2.m2.3.4.2.3.cmml">Q</mi></mrow><mo stretchy="false" id="S2.SS1.p2.2.m2.3.4.1" xref="S2.SS1.p2.2.m2.3.4.1.cmml">→</mo><msup id="S2.SS1.p2.2.m2.3.4.3" xref="S2.SS1.p2.2.m2.3.4.3.cmml"><mrow id="S2.SS1.p2.2.m2.3.4.3.2.2" xref="S2.SS1.p2.2.m2.3.4.3.2.1.cmml"><mo stretchy="false" id="S2.SS1.p2.2.m2.3.4.3.2.2.1" xref="S2.SS1.p2.2.m2.3.4.3.2.1.cmml">[</mo><mn id="S2.SS1.p2.2.m2.2.2" xref="S2.SS1.p2.2.m2.2.2.cmml">0</mn><mo id="S2.SS1.p2.2.m2.3.4.3.2.2.2" xref="S2.SS1.p2.2.m2.3.4.3.2.1.cmml">,</mo><mn id="S2.SS1.p2.2.m2.3.3" xref="S2.SS1.p2.2.m2.3.3.cmml">1</mn><mo stretchy="false" id="S2.SS1.p2.2.m2.3.4.3.2.2.3" xref="S2.SS1.p2.2.m2.3.4.3.2.1.cmml">]</mo></mrow><mrow id="S2.SS1.p2.2.m2.1.1.1.3" xref="S2.SS1.p2.2.m2.1.1.1.2.cmml"><mo id="S2.SS1.p2.2.m2.1.1.1.3.1" xref="S2.SS1.p2.2.m2.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p2.2.m2.1.1.1.1" xref="S2.SS1.p2.2.m2.1.1.1.1.cmml">𝒜</mi><mo id="S2.SS1.p2.2.m2.1.1.1.3.2" xref="S2.SS1.p2.2.m2.1.1.1.2.1.cmml">|</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.3b"><apply id="S2.SS1.p2.2.m2.3.4.cmml" xref="S2.SS1.p2.2.m2.3.4"><ci id="S2.SS1.p2.2.m2.3.4.1.cmml" xref="S2.SS1.p2.2.m2.3.4.1">→</ci><apply id="S2.SS1.p2.2.m2.3.4.2.cmml" xref="S2.SS1.p2.2.m2.3.4.2"><times id="S2.SS1.p2.2.m2.3.4.2.1.cmml" xref="S2.SS1.p2.2.m2.3.4.2.1"></times><ci id="S2.SS1.p2.2.m2.3.4.2.2.cmml" xref="S2.SS1.p2.2.m2.3.4.2.2">𝐼</ci><ci id="S2.SS1.p2.2.m2.3.4.2.3.cmml" xref="S2.SS1.p2.2.m2.3.4.2.3">𝑄</ci></apply><apply id="S2.SS1.p2.2.m2.3.4.3.cmml" xref="S2.SS1.p2.2.m2.3.4.3"><csymbol cd="ambiguous" id="S2.SS1.p2.2.m2.3.4.3.1.cmml" xref="S2.SS1.p2.2.m2.3.4.3">superscript</csymbol><interval closure="closed" id="S2.SS1.p2.2.m2.3.4.3.2.1.cmml" xref="S2.SS1.p2.2.m2.3.4.3.2.2"><cn type="integer" id="S2.SS1.p2.2.m2.2.2.cmml" xref="S2.SS1.p2.2.m2.2.2">0</cn><cn type="integer" id="S2.SS1.p2.2.m2.3.3.cmml" xref="S2.SS1.p2.2.m2.3.3">1</cn></interval><apply id="S2.SS1.p2.2.m2.1.1.1.2.cmml" xref="S2.SS1.p2.2.m2.1.1.1.3"><abs id="S2.SS1.p2.2.m2.1.1.1.2.1.cmml" xref="S2.SS1.p2.2.m2.1.1.1.3.1"></abs><ci id="S2.SS1.p2.2.m2.1.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1.1.1">𝒜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.3c">I\times Q\to[0,1]^{\left|\mathcal{A}\right|}</annotation></semantics></math>, which generates the answer distributions for any given image-question pairs.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.1" class="ltx_p"><em id="S2.SS1.p3.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">GVQA:</em> GVQA divides the task of VQA into two steps, Look: find the object / image block required to answer the question and identify the visual concept in the block; Find the space for reasonable answers from the questions, and return the correct visual concepts from a group of visual concepts by considering which concepts are reasonable. The novelty of GVQA is that answering ”yes” or ”no”is an independent task. GVQA includes 1) visual concept classifier (VCC): it is responsible for locating the image blocks required to answer questions and generating a set of visual concepts related to the location blocks. 2) Answer clustering predictor (ACP) 3) concept extractor (CE) 4) the outputs of VCC and ACP are fed to answer predictor (AP) to generate answers.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para ltx_noindent">
<p id="S2.SS1.p4.1" class="ltx_p"><em id="S2.SS1.p4.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">SAN:</em> Yang et al. proposed the SAN model. The model takes the problem as a query condition and looks for the area related to the problem in the figure. The whole model is divided into three modules: 1) image model: using VGGNet to extract image features, the selected features are the features of the last layer - the pool layer, which well maintains the spatial information of the original image. 2) Question model: extract text features using LSTM or CNN. 3) Stacked attention networks: realize the attention of image area through multiple iterations. Firstly, a feature attention distribution is generated according to image features and text features, and the weight of each region of the image is obtained according to this distribution. This process is iterated for many times, and finally the problem related region is noticed.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para ltx_noindent">
<p id="S2.SS1.p5.1" class="ltx_p"><em id="S2.SS1.p5.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">UpDn:</em> Anderson et al. Proposed a top-down and bottom-up attention model method, which was applied to the related problems of visual scene understanding and visual question answering system. The bottom-up attention model (using fast r-cnn) is used to extract the region of interest in the image and obtain the object features; The top-down attention model is used to learn the weight corresponding to the feature (using LSTM), so as to realize the in-depth understanding of the visual image. UpDn model won the first place in VQA 2017 challenge and is widely used to solve the problem of language bias.</p>
</div>
<div id="S2.SS1.p6" class="ltx_para ltx_noindent">
<p id="S2.SS1.p6.1" class="ltx_p"><em id="S2.SS1.p6.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">NSM:</em> Manning et al. have introduced the Neural State Machine, a graph-based network that simulates the operation of an automaton, and demonstrated its versatility, robustness and high generalization skills on the tasks of real-world visual reasoning and compositional question answering. By incorporating the concept of a state machine into neural networks, they introduce a strong structural prior that enhances compositinality both in terms of the representation, by having a structured graph to serve as the world model, as well as in terms of the computation, by performing sequential reasoning over such graphs.</p>
</div>
<div id="S2.SS1.p7" class="ltx_para ltx_noindent">
<p id="S2.SS1.p7.1" class="ltx_p"><em id="S2.SS1.p7.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">MUREL and S-MRL:</em> Cadène et al. introduced MuRel, a multimodal relational network for Visual Question Answering task. This system was based on rich representations of visual image regions that are progressively merged with the question representation. They included region relations with pairwise combinations in the fusion, and the whole system can be leveraged to define visualization schemes helping to interpret the decision process of MuRel. They validated the approach on three challenging datasets: VQA 2.0, VQA-CP v2 and TDIUC. They clearly demonstrating the gain of the vectorial representation to model the attention. S-MRL stands for Simplified-MUREL. The architecture was proposed in RUBi by Cadene et al.</p>
</div>
<div id="S2.SS1.p8" class="ltx_para ltx_noindent">
<p id="S2.SS1.p8.1" class="ltx_p"><em id="S2.SS1.p8.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">CAM:</em> Peng et al. proposed a novel VQA model, termed Cascaded-Answering Model (CAM), to take advantage of the semantics of the predicted answers that are neglected by previous models. CAM extended the conventional one-stage VQA model to a two-stage model using two cascaded answering modules, Candidate Answer Generation (CAG) module and Final Answer Prediction (FAP) module.,CAG generated the candidate answers, and FAP predicted the final answer by combining the information of question, image and candidate answers.</p>
</div>
<div id="S2.SS1.p9" class="ltx_para ltx_noindent">
<p id="S2.SS1.p9.1" class="ltx_p"><em id="S2.SS1.p9.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">LXMERT:</em> Tan et al. proposed a cross modal framework LXMERT, which uses a two-way transformer structure and is constructed based on transformer encoder and a new cross modal encoder. Three encoders are used to endow the model with cross modal capability through five pre-training methods. The encoders are object relationship encoder, language encoder and cross modal encoder. The model has two kinds of input information, one is the image-based object feature information, and the other is the text-based question embedding information.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Strengthening visual information</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Selvaraju et al. proposed the human importance perception network adjustment (HINT) , a general framework for aligning network sensitivity with spatial input regions that humans perceive as relevant to a task. The authors demonstrate the effectiveness of this approach in improving the visual basis of visual and language tasks such as VQA and image captioning. The research shows that the good grounding improves the generalization ability of the model and the credibility of the model.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Li et al proposed a new visual question-and-answer framework called relational graph attention network (REGAT) to model multi-type object relationships using question adaptive attention mechanism. Regat utilizes two types of visual object relations: explicit and implicit, in order to perceive region representation through graph-based attention learning, the new results are obtained on both VQA 2.0 and VQA-cp V2 datasets. Comprehensive experiments on two VQA data sets show that the model can be injected into the most advanced VQA architecture in a plug-and-play way.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Shrestha et al. advocate the use of synthetic data sets to create a basic fact-based data set for 100 percent of instances, allowing communities to assess whether their methods can focus on relevant information, another option is to use tasks that have a clear testing basis, for example, in visual query detection, the agent must be around the output box in any area of the scenario that matches the natural language query (Acharya et al. , 2019a) . Here, Shrestha et al. show that existing vision-based VQA bias mitigation methods do not work as expected.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">Teney et al. suggested taking a random subset of 8K instances from VQA-CP training data to evaluate domain performance and compare it to some existing work. VGQE was proposed by KV et al. . It is a new type of problem encoder, which uses pattern and generates visual problem representation. This problem indicates sufficient discrimination based on visual counterparts and helps the model to reduce language bias from training sets. The VQA-CP V2 data set, which is sensitive to deviation, has been extensively tested by KV et al, and a new and up-to-date technology has been realized. Unlike existing deviation reduction techniques, VGQE does not sacrifice the performance of the original model on the standard VQA V2 benchmark.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">Hirota et al. explored the use of text representations of images, rather than the deep visual features of VQA. In order to increase the size and diversity of training data, the authors explore the data expansion methods of description and problem. Through experiments, including ablation experiments, the author verifies the competitiveness of the pure language model and the model based on depth vision features. Most of the data enhancement techniques improve performance; in particular, problem-based back translation is a strong driver.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p">Si et al proposed a progressive framework of selection and reordering (Sar) based on visual implication, in which the authors first select candidate answers to reduce the prediction space, and then reorder the candidate answers through visual inclusion tasks, the task verifies that the image semantically contains a composite statement of the question and each candidate answer. The framework can take full advantage of the interaction of images, questions, and candidate answers. It is a general framework that can be combined with existing VQA models to further enhance their capabilities. Si et al. demonstrated the advantages of the framework on the VQA-CP V2 dataset through a number of experiments and analyses.</p>
</div>
<div id="S2.SS2.p7" class="ltx_para">
<p id="S2.SS2.p7.1" class="ltx_p">Zhang et al. propose a new VQA Kan that introduces richer visual information and compensates for common sense from an external knowledge base. For different types of problems, Zhang et al. make the model adaptively balance the importance of visual information and external knowledge. The self-adaptive scoring attention module can automatically select the appropriate information source according to the type of problem.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Weakening language priors</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Ramakrishnan et al. propose a new antagonistic regularization scheme to reduce the memory bias of data sets in VQA, which is based on the differences in the reliability of the model between the problem-only branches and the processed images. Experiments on VQA-CP datasets show that this technique allows existing VQA models to significantly improve performance in changing priorities. This approach can be implemented as a simple plug-in module on top of the existing VQA model, with end-to-end training from scratch.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Grand et al. examined several advantages and limitations of adversarial regularization, a recently introduced technique for reducing language bias in the VQA model. Although the authors find that AdvReg improves the performance of the outfield example in VQA-CP, one concern is that the pendulum swings too far: there are quantitative and qualitative indications that the model is over-normalized. The performance of the ADVREG model is influenced by the VQA-CP and the examples in the domain of the original VQA dataset. While AdvReg can improve performance for binary problems, it can degrade performance for other problem types. The author observes that ADVREG model makes use of significant image features and ignores important linguistic clues in the problem.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Cadene et al. proposed RUBi to reduce the unimodal bias learned by visual QA models. Rubi is a simple learning strategy designed to be model agnostic. It is based on a problem-only branch that captures unwanted statistics from the problem pattern. This branch influences the underlying VQA model to prevent learning single peak bias from problems. The authors show that the accuracy of VQA-CP V2 is significantly improved by + 5.94 percentage points compared with the latest results of VQA-CP V2, a data set used to explain problem deviations. Rubi is valid for different types of common VQA models.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">Clark et al. propose a method that uses human knowledge about which methods can not be well generalized to improve the robustness of the model to domain transitions. This approach uses a pre-trained naive model to train the robust model in a set and a separate robust model for testing. A large number of experiments show that the method works well on two antagonistic datasets and two ever-changing prior datasets, including 12-point gain on VQA-CP.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p">KERVADEC and others have shown that properly structuring the semantic space of the output class can overcome some of the shortcomings of the widely used classification strategy in VQA, and their aim is to continue to pave the way for the long-term goals of the community, implementing a direct structured prediction of text output will bring VQA closer to the more traditional models of NLP.</p>
</div>
<div id="S2.SS3.p6" class="ltx_para">
<p id="S2.SS3.p6.1" class="ltx_p">Classical regularifiers applied to multi-modal datasets cause the model to ignore one or more modes. This is suboptimal because the authors expect all patterns to contribute to classification. To address this concern, Gat et al. studied regularization through functional entropy. It encourages a more uniform use of existing models.</p>
</div>
<div id="S2.SS3.p7" class="ltx_para">
<p id="S2.SS3.p7.1" class="ltx_p">To address the language bias issue, Niu and others argue that biased data sets should not be subject to time consuming depolarization. If the prediction made by the training model is biased, the reasons for the bias can be explored. When the model has only one input, I will only consider the bias data to get the predicted results. If the total effect (TE) is subtracted from the natural direct effect (NDE) , the language bias in the model prediction can be eliminated. In this framework, the authors use counterfactual reasoning to eliminate the effects of bias.</p>
</div>
<div id="S2.SS3.p8" class="ltx_para">
<p id="S2.SS3.p8.1" class="ltx_p">Han et al analyzed several methods of robust VQA through experiments, and proposed a new framework to reduce the language deviation in VQA. The author proves that language bias in VQA can be decomposed into distribution bias and shortcut bias, and proposes a greedy gradient integration strategy to eliminate these two biases. The experimental results show the rationality of the deviation decomposition and the effectiveness of GGE. The authors believe that the ideas behind GGE are valuable and have the potential to become a general method for solving data set bias problems. The authors extend GGE to solve other bias problems, provide more rigorous analysis to ensure convergence of the model, and learn to automatically detect different types of bias features without prior knowledge.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS4.5.1.1" class="ltx_text">II-D</span> </span><span id="S2.SS4.6.2" class="ltx_text ltx_font_italic">Data argumentation and Training strategies</span>
</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Teney et al proposed a new VQA method, which trains the interface between the model and the external data source and uses it to support its reply process. The model introduces new features, especially the use of non-VQA data to support the reply process. This provides many opportunities for future research to access “Black box”data sources such as web search and dynamic databases. This opens the door to systems that can reason about vision and language beyond the limited areas covered by any given training set.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">Chen et al proposed a model unknown counterfactual synthesis (CSS) training scheme to improve the visual interpretation ability and problem sensitivity of the model. CSS generates counterfactual training samples by masking key objects or words. CSS continues to improve the performance of different VQA models. Chen et al. verified the effectiveness of CSS by extensive contrast and ablation experiments.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p">In order to make full use of the supervisory information of synthetic counterfactual samples in robust VQA, Liang and others introduced a self-supervised contrast learning mechanism to learn the relationship between fact samples and counterfactual samples. Experimental results show that this method improves the reasoning ability and robustness of VQA model.</p>
</div>
<div id="S2.SS4.p4" class="ltx_para">
<p id="S2.SS4.p4.1" class="ltx_p">At present, there are few researches on the internal causes of the language apriori problem in VQA. Guo et al. propose to fill this gap by explaining it from the perspective of class imbalance. Guo et al put forward two hypotheses and verified their feasibility. Guo et al have developed a simple and effective method of loss rescaling, which assigns different weights to each answer according to the given type of question. A large number of experiments on three public data sets verify the effectiveness of the proposed method. Guo and others hope to develop other approaches typically used to overcome class balancing problems, namely data rebalancing or transfer learning, to ease language prioritization.</p>
</div>
<div id="S2.SS4.p5" class="ltx_para">
<p id="S2.SS4.p5.1" class="ltx_p">Gokhale et al. proposed a method to train VQA model by using input mutation, which aims at realizing distributed generalization. Parallel work in the field of image classification shows that well-designed input perturbations or operations are beneficial to generalization and lead to performance improvements. Zhu et al proposed a new self-supervised learning framework to overcome language priors in VQA. Zhu and others argue that this work could be a meaningful step towards a realistic VQA and language bias, and that such self-monitoring could be extended to other tasks affected by inherent data bias. Experimental results show that the method achieves a balance between answering questions and overcoming linguistic prior knowledge.</p>
</div>
<div id="S2.SS4.p6" class="ltx_para">
<p id="S2.SS4.p6.1" class="ltx_p">Teney et al. ensure that data scaling uses exactly the same problem in each small batch, so improvements are strictly caused by architectural differences in approach. Under the condition of low training data, the improvement of this method is maximum. They show advantages under a variety of conditions, including distributed test data, low data training.</p>
</div>
<div id="S2.SS4.p7" class="ltx_para">
<p id="S2.SS4.p7.1" class="ltx_p">Guo et al proposed to solve the language apriori problem in VQA from the perspective of answer feature space learning, which has not been explored in previous studies. To achieve this goal, they designed an adaptive marginal cosine loss to distinguish the answer by correctly describing the answer feature space. For a given question, the frequent and sparse answers to the corresponding question types span wider and narrower angular spaces, respectively.</p>
</div>
<div id="S2.SS4.p8" class="ltx_para">
<p id="S2.SS4.p8.1" class="ltx_p">Yang et al. proposed a learning strategy called CCB to deal with language bias in visual question-and-answer (VQA) tasks. Based on the content and context branches, CCB guides the model by combining decisive content information and necessary context information. They constructed an additional loss function by separating the effects of the language bias on the model and jointly optimizing the two branches and the final prediction.</p>
</div>
<div id="S2.SS4.p9" class="ltx_para">
<p id="S2.SS4.p9.1" class="ltx_p">Jiang et al. proposed a graph-based generation modeling scheme X-GGM to improve the OOD generalization ability of baseline VQA model while maintaining ID performance. The X-GGM scenario randomly executes R-GGM or N-GGM to generate a new relational matrix or a new node representation. The gradient derivative and disturbance distribution are used to solve the problem of unstable distribution of adversarial data.</p>
</div>
<div id="S2.SS4.p10" class="ltx_para">
<p id="S2.SS4.p10.1" class="ltx_p">Yuan et al. proposed a counterfactual thinking module to solve the problem of language bias, and helped the model improve the counterfactual thinking ability through multi-level construction of comparative learning loss. It alleviates the problem of language bias from the perspective of causal inference and data enhancement.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Datasets and evaluation</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In the field of VQA, researchers have proposedd many datasets to evaluate the performance of VQA. These data sets generally contain a triple consisting of an image, a question and the correct answer. Sometimes, these datasets provide additional annotation information. Such as image title, image area and multiple candidate answers.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Datasets</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">At present, researchers usually use VQA CP V1 and V2 data sets to evaluate the performance of the proposedd model. And perform auxiliary verification on the data set VQA V1 or V2. Most of the existing research results are tested on VQA CP V2, and a few will also be tested on vqacp v1. The training set of vqacpv2 dataset contains 121k pictures, 438k questions and 4.4m answers, and its test set contains 98K pictures, 220K questions and 2.2m answers. The dataset is divided by VQA V2 dataset. In order to measure the problem of language bias, the question and answer distribution of the training set and the test set of the data set are quite different, that is, for the same type of questions, the training set and the test set have inconsistent answer distribution. Therefore, the data set is very suitable to measure the generalization ability and robustness of the model.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><em id="S3.SS1.p2.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">VQA v1:</em>
VQA v1 is the first version of the VQA dataset. The training set contains 248349 image problem pairs, the verification set contains 121512 image problem pairs, and the test set contains 244302 image problem pairs. In the test of the model, the annotation of the test set is not available except for the remote server used for evaluation. All images of VQA V1 are from the mscoco dataset. For each sample pair in the data set, there is a question, an image and ten ground-truth answers.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><em id="S3.SS1.p3.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">VQA v2:</em>
Vqa-2.0 is the second version of VQA data set. The training set contains 443757 image problem pairs, the verification set contains 214354 image problem pairs, and the test set contains 447793 image problem pairs. Its dataset is twice as large as its first version. The questions and answers in the data set are also artificially annotated. In addition, for each question in the dataset, there are two similar images, but the answers are different, which makes the sample distribution of the dataset more balanced. Vqa-2.0 datasets are larger and more balanced than vqa-1.0.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p"><em id="S3.SS1.p4.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">VQA-CP v1:</em>
VQA-CP v1 dataset is obtained after sample re division from VQA V1 data set. The training set of vqacp V1 data set contains 118K pictures, 245k questions and 2.5m answers, and its test set contains 87k pictures, 125k questions and 1.3m answers.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para ltx_noindent">
<p id="S3.SS1.p5.1" class="ltx_p"><em id="S3.SS1.p5.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">VQA-CP v2:</em>
VQA-CP v2 dataset is obtained after sample redivision from VQA v2 dataset. The training set of vqacp V2 data set contains 121k pictures, 438k questions and 4.4m answers, and its test set contains 98K pictures, 220K questions and 2.2m answers. This data set is the most used data set for testing methods by researchers.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span><span id="S3.T1.10.1" class="ltx_text ltx_font_bold">Comparison on VQA-CP v2 test set and VQA v2 val set</span>. We show the method based on the UpDn model and its corresponding results. “Base.” indicates the VQA base model.</figcaption>
<div id="S3.T1.8" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:623.9pt;height:821.2pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-23.5pt,30.9pt) scale(0.93,0.93) ;">
<table id="S3.T1.8.8" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.8.8.9.1" class="ltx_tr">
<td id="S3.T1.8.8.9.1.1" class="ltx_td ltx_align_left ltx_border_tt ltx_border_t">Test set</td>
<td id="S3.T1.8.8.9.1.2" class="ltx_td ltx_border_tt ltx_border_t"></td>
<td id="S3.T1.8.8.9.1.3" class="ltx_td ltx_border_tt ltx_border_t"></td>
<td id="S3.T1.8.8.9.1.4" class="ltx_td ltx_border_tt ltx_border_t"></td>
<td id="S3.T1.8.8.9.1.5" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" colspan="4">VQA-CP v2 test</td>
<td id="S3.T1.8.8.9.1.6" class="ltx_td ltx_border_tt ltx_border_t"></td>
<td id="S3.T1.8.8.9.1.7" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t" colspan="4">VQA v2 val</td>
</tr>
<tr id="S3.T1.8.8.10.2" class="ltx_tr">
<td id="S3.T1.8.8.10.2.1" class="ltx_td ltx_align_left">Methods</td>
<td id="S3.T1.8.8.10.2.2" class="ltx_td"></td>
<td id="S3.T1.8.8.10.2.3" class="ltx_td ltx_align_center">Base.</td>
<td id="S3.T1.8.8.10.2.4" class="ltx_td"></td>
<td id="S3.T1.8.8.10.2.5" class="ltx_td ltx_align_center ltx_border_t">All</td>
<td id="S3.T1.8.8.10.2.6" class="ltx_td ltx_align_center ltx_border_t">Y/N</td>
<td id="S3.T1.8.8.10.2.7" class="ltx_td ltx_align_center ltx_border_t">Num.</td>
<td id="S3.T1.8.8.10.2.8" class="ltx_td ltx_align_center ltx_border_t">Other</td>
<td id="S3.T1.8.8.10.2.9" class="ltx_td"></td>
<td id="S3.T1.8.8.10.2.10" class="ltx_td ltx_align_center ltx_border_t">All</td>
<td id="S3.T1.8.8.10.2.11" class="ltx_td ltx_align_center ltx_border_t">Y/N</td>
<td id="S3.T1.8.8.10.2.12" class="ltx_td ltx_align_center ltx_border_t">Num.</td>
<td id="S3.T1.8.8.10.2.13" class="ltx_td ltx_align_center ltx_border_t">Other</td>
</tr>
<tr id="S3.T1.8.8.11.3" class="ltx_tr">
<td id="S3.T1.8.8.11.3.1" class="ltx_td ltx_align_left ltx_border_t">GVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>
</td>
<td id="S3.T1.8.8.11.3.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.8.8.11.3.3" class="ltx_td ltx_align_center ltx_border_t">–</td>
<td id="S3.T1.8.8.11.3.4" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.8.8.11.3.5" class="ltx_td ltx_align_center ltx_border_t">31.30</td>
<td id="S3.T1.8.8.11.3.6" class="ltx_td ltx_align_center ltx_border_t">57.99</td>
<td id="S3.T1.8.8.11.3.7" class="ltx_td ltx_align_center ltx_border_t">13.68</td>
<td id="S3.T1.8.8.11.3.8" class="ltx_td ltx_align_center ltx_border_t">22.14</td>
<td id="S3.T1.8.8.11.3.9" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.8.8.11.3.10" class="ltx_td ltx_align_center ltx_border_t">48.24</td>
<td id="S3.T1.8.8.11.3.11" class="ltx_td ltx_align_center ltx_border_t">72.03</td>
<td id="S3.T1.8.8.11.3.12" class="ltx_td ltx_align_center ltx_border_t">31.17</td>
<td id="S3.T1.8.8.11.3.13" class="ltx_td ltx_align_center ltx_border_t">34.65</td>
</tr>
<tr id="S3.T1.8.8.12.4" class="ltx_tr">
<td id="S3.T1.8.8.12.4.1" class="ltx_td ltx_align_left">SAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>
</td>
<td id="S3.T1.8.8.12.4.2" class="ltx_td"></td>
<td id="S3.T1.8.8.12.4.3" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.12.4.4" class="ltx_td"></td>
<td id="S3.T1.8.8.12.4.5" class="ltx_td ltx_align_center">24.96</td>
<td id="S3.T1.8.8.12.4.6" class="ltx_td ltx_align_center">38.35</td>
<td id="S3.T1.8.8.12.4.7" class="ltx_td ltx_align_center">11.14</td>
<td id="S3.T1.8.8.12.4.8" class="ltx_td ltx_align_center">21.74</td>
<td id="S3.T1.8.8.12.4.9" class="ltx_td"></td>
<td id="S3.T1.8.8.12.4.10" class="ltx_td ltx_align_center">52.41</td>
<td id="S3.T1.8.8.12.4.11" class="ltx_td ltx_align_center">70.06</td>
<td id="S3.T1.8.8.12.4.12" class="ltx_td ltx_align_center">39.28</td>
<td id="S3.T1.8.8.12.4.13" class="ltx_td ltx_align_center">47.84</td>
</tr>
<tr id="S3.T1.8.8.13.5" class="ltx_tr">
<td id="S3.T1.8.8.13.5.1" class="ltx_td ltx_align_left">UpDn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>
</td>
<td id="S3.T1.8.8.13.5.2" class="ltx_td"></td>
<td id="S3.T1.8.8.13.5.3" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.13.5.4" class="ltx_td"></td>
<td id="S3.T1.8.8.13.5.5" class="ltx_td ltx_align_center">39.74</td>
<td id="S3.T1.8.8.13.5.6" class="ltx_td ltx_align_center">42.27</td>
<td id="S3.T1.8.8.13.5.7" class="ltx_td ltx_align_center">11.93</td>
<td id="S3.T1.8.8.13.5.8" class="ltx_td ltx_align_center">46.05</td>
<td id="S3.T1.8.8.13.5.9" class="ltx_td"></td>
<td id="S3.T1.8.8.13.5.10" class="ltx_td ltx_align_center">63.48</td>
<td id="S3.T1.8.8.13.5.11" class="ltx_td ltx_align_center">81.18</td>
<td id="S3.T1.8.8.13.5.12" class="ltx_td ltx_align_center">42.14</td>
<td id="S3.T1.8.8.13.5.13" class="ltx_td ltx_align_center">55.66</td>
</tr>
<tr id="S3.T1.8.8.14.6" class="ltx_tr">
<td id="S3.T1.8.8.14.6.1" class="ltx_td ltx_align_left">S-MRL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</td>
<td id="S3.T1.8.8.14.6.2" class="ltx_td"></td>
<td id="S3.T1.8.8.14.6.3" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.14.6.4" class="ltx_td"></td>
<td id="S3.T1.8.8.14.6.5" class="ltx_td ltx_align_center">38.46</td>
<td id="S3.T1.8.8.14.6.6" class="ltx_td ltx_align_center">42.85</td>
<td id="S3.T1.8.8.14.6.7" class="ltx_td ltx_align_center">12.81</td>
<td id="S3.T1.8.8.14.6.8" class="ltx_td ltx_align_center">43.20</td>
<td id="S3.T1.8.8.14.6.9" class="ltx_td"></td>
<td id="S3.T1.8.8.14.6.10" class="ltx_td ltx_align_center">63.10</td>
<td id="S3.T1.8.8.14.6.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.14.6.12" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.14.6.13" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.8.8.15.7" class="ltx_tr">
<td id="S3.T1.8.8.15.7.1" class="ltx_td ltx_align_left">NSM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>
</td>
<td id="S3.T1.8.8.15.7.2" class="ltx_td"></td>
<td id="S3.T1.8.8.15.7.3" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.15.7.4" class="ltx_td"></td>
<td id="S3.T1.8.8.15.7.5" class="ltx_td ltx_align_center">45.80</td>
<td id="S3.T1.8.8.15.7.6" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.15.7.7" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.15.7.8" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.15.7.9" class="ltx_td"></td>
<td id="S3.T1.8.8.15.7.10" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.15.7.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.15.7.12" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.15.7.13" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.8.8.16.8" class="ltx_tr">
<td id="S3.T1.8.8.16.8.1" class="ltx_td ltx_align_left">MUREL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>
</td>
<td id="S3.T1.8.8.16.8.2" class="ltx_td"></td>
<td id="S3.T1.8.8.16.8.3" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.16.8.4" class="ltx_td"></td>
<td id="S3.T1.8.8.16.8.5" class="ltx_td ltx_align_center">39.54</td>
<td id="S3.T1.8.8.16.8.6" class="ltx_td ltx_align_center">42.85</td>
<td id="S3.T1.8.8.16.8.7" class="ltx_td ltx_align_center">13.17</td>
<td id="S3.T1.8.8.16.8.8" class="ltx_td ltx_align_center">45.04</td>
<td id="S3.T1.8.8.16.8.9" class="ltx_td"></td>
<td id="S3.T1.8.8.16.8.10" class="ltx_td ltx_align_center">65.14</td>
<td id="S3.T1.8.8.16.8.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.16.8.12" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.16.8.13" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.8.8.17.9" class="ltx_tr">
<td id="S3.T1.8.8.17.9.1" class="ltx_td ltx_align_left">CAM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>
</td>
<td id="S3.T1.8.8.17.9.2" class="ltx_td"></td>
<td id="S3.T1.8.8.17.9.3" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.17.9.4" class="ltx_td"></td>
<td id="S3.T1.8.8.17.9.5" class="ltx_td ltx_align_center">39.75</td>
<td id="S3.T1.8.8.17.9.6" class="ltx_td ltx_align_center">43.29</td>
<td id="S3.T1.8.8.17.9.7" class="ltx_td ltx_align_center">12.31</td>
<td id="S3.T1.8.8.17.9.8" class="ltx_td ltx_align_center">45.41</td>
<td id="S3.T1.8.8.17.9.9" class="ltx_td"></td>
<td id="S3.T1.8.8.17.9.10" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.17.9.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.17.9.12" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.17.9.13" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.8.8.18.10" class="ltx_tr">
<td id="S3.T1.8.8.18.10.1" class="ltx_td ltx_align_left">LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>
</td>
<td id="S3.T1.8.8.18.10.2" class="ltx_td"></td>
<td id="S3.T1.8.8.18.10.3" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.18.10.4" class="ltx_td"></td>
<td id="S3.T1.8.8.18.10.5" class="ltx_td ltx_align_center">41.28</td>
<td id="S3.T1.8.8.18.10.6" class="ltx_td ltx_align_center">42.01</td>
<td id="S3.T1.8.8.18.10.7" class="ltx_td ltx_align_center">14.16</td>
<td id="S3.T1.8.8.18.10.8" class="ltx_td ltx_align_center">48.34</td>
<td id="S3.T1.8.8.18.10.9" class="ltx_td"></td>
<td id="S3.T1.8.8.18.10.10" class="ltx_td ltx_align_center">65.31</td>
<td id="S3.T1.8.8.18.10.11" class="ltx_td ltx_align_center">83.30</td>
<td id="S3.T1.8.8.18.10.12" class="ltx_td ltx_align_center">46.15</td>
<td id="S3.T1.8.8.18.10.13" class="ltx_td ltx_align_center">56.91</td>
</tr>
<tr id="S3.T1.8.8.19.11" class="ltx_tr">
<td id="S3.T1.8.8.19.11.1" class="ltx_td ltx_align_left ltx_border_t" colspan="13"><span id="S3.T1.8.8.19.11.1.1" class="ltx_text ltx_font_italic">methods based on strengthening visual information:</span></td>
</tr>
<tr id="S3.T1.8.8.20.12" class="ltx_tr">
<td id="S3.T1.8.8.20.12.1" class="ltx_td ltx_align_left ltx_border_t">AttAlign <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>
</td>
<td id="S3.T1.8.8.20.12.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.8.8.20.12.3" class="ltx_td ltx_align_center ltx_border_t">UpDn</td>
<td id="S3.T1.8.8.20.12.4" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.8.8.20.12.5" class="ltx_td ltx_align_center ltx_border_t">39.37</td>
<td id="S3.T1.8.8.20.12.6" class="ltx_td ltx_align_center ltx_border_t">43.02</td>
<td id="S3.T1.8.8.20.12.7" class="ltx_td ltx_align_center ltx_border_t">11.89</td>
<td id="S3.T1.8.8.20.12.8" class="ltx_td ltx_align_center ltx_border_t">45.00</td>
<td id="S3.T1.8.8.20.12.9" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.8.8.20.12.10" class="ltx_td ltx_align_center ltx_border_t">63.24</td>
<td id="S3.T1.8.8.20.12.11" class="ltx_td ltx_align_center ltx_border_t">80.99</td>
<td id="S3.T1.8.8.20.12.12" class="ltx_td ltx_align_center ltx_border_t">42.55</td>
<td id="S3.T1.8.8.20.12.13" class="ltx_td ltx_align_center ltx_border_t">55.22</td>
</tr>
<tr id="S3.T1.8.8.21.13" class="ltx_tr">
<td id="S3.T1.8.8.21.13.1" class="ltx_td ltx_align_left">HINT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>
</td>
<td id="S3.T1.8.8.21.13.2" class="ltx_td"></td>
<td id="S3.T1.8.8.21.13.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.21.13.4" class="ltx_td"></td>
<td id="S3.T1.8.8.21.13.5" class="ltx_td ltx_align_center">46.73</td>
<td id="S3.T1.8.8.21.13.6" class="ltx_td ltx_align_center">67.27</td>
<td id="S3.T1.8.8.21.13.7" class="ltx_td ltx_align_center">10.61</td>
<td id="S3.T1.8.8.21.13.8" class="ltx_td ltx_align_center">45.88</td>
<td id="S3.T1.8.8.21.13.9" class="ltx_td"></td>
<td id="S3.T1.8.8.21.13.10" class="ltx_td ltx_align_center">63.38</td>
<td id="S3.T1.8.8.21.13.11" class="ltx_td ltx_align_center">81.18</td>
<td id="S3.T1.8.8.21.13.12" class="ltx_td ltx_align_center">42.99</td>
<td id="S3.T1.8.8.21.13.13" class="ltx_td ltx_align_center">55.56</td>
</tr>
<tr id="S3.T1.8.8.22.14" class="ltx_tr">
<td id="S3.T1.8.8.22.14.1" class="ltx_td ltx_align_left">SCR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>
</td>
<td id="S3.T1.8.8.22.14.2" class="ltx_td"></td>
<td id="S3.T1.8.8.22.14.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.22.14.4" class="ltx_td"></td>
<td id="S3.T1.8.8.22.14.5" class="ltx_td ltx_align_center">49.45</td>
<td id="S3.T1.8.8.22.14.6" class="ltx_td ltx_align_center">72.36</td>
<td id="S3.T1.8.8.22.14.7" class="ltx_td ltx_align_center">10.93</td>
<td id="S3.T1.8.8.22.14.8" class="ltx_td ltx_align_center">48.02</td>
<td id="S3.T1.8.8.22.14.9" class="ltx_td"></td>
<td id="S3.T1.8.8.22.14.10" class="ltx_td ltx_align_center">62.2</td>
<td id="S3.T1.8.8.22.14.11" class="ltx_td ltx_align_center">78.8</td>
<td id="S3.T1.8.8.22.14.12" class="ltx_td ltx_align_center">41.6</td>
<td id="S3.T1.8.8.22.14.13" class="ltx_td ltx_align_center">54.5</td>
</tr>
<tr id="S3.T1.8.8.23.15" class="ltx_tr">
<td id="S3.T1.8.8.23.15.1" class="ltx_td ltx_align_left">ReGAT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
<td id="S3.T1.8.8.23.15.2" class="ltx_td"></td>
<td id="S3.T1.8.8.23.15.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.23.15.4" class="ltx_td"></td>
<td id="S3.T1.8.8.23.15.5" class="ltx_td ltx_align_center">40.42</td>
<td id="S3.T1.8.8.23.15.6" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.23.15.7" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.23.15.8" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.23.15.9" class="ltx_td"></td>
<td id="S3.T1.8.8.23.15.10" class="ltx_td ltx_align_center">67.18</td>
<td id="S3.T1.8.8.23.15.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.23.15.12" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.23.15.13" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.8.8.24.16" class="ltx_tr">
<td id="S3.T1.8.8.24.16.1" class="ltx_td ltx_align_left">ESR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>
</td>
<td id="S3.T1.8.8.24.16.2" class="ltx_td"></td>
<td id="S3.T1.8.8.24.16.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.24.16.4" class="ltx_td"></td>
<td id="S3.T1.8.8.24.16.5" class="ltx_td ltx_align_center">48.9</td>
<td id="S3.T1.8.8.24.16.6" class="ltx_td ltx_align_center">69.8</td>
<td id="S3.T1.8.8.24.16.7" class="ltx_td ltx_align_center">11.3</td>
<td id="S3.T1.8.8.24.16.8" class="ltx_td ltx_align_center">47.8</td>
<td id="S3.T1.8.8.24.16.9" class="ltx_td"></td>
<td id="S3.T1.8.8.24.16.10" class="ltx_td ltx_align_center">62.6</td>
<td id="S3.T1.8.8.24.16.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.24.16.12" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.24.16.13" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.8.8.25.17" class="ltx_tr">
<td id="S3.T1.8.8.25.17.1" class="ltx_td ltx_align_left">VGQE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>
</td>
<td id="S3.T1.8.8.25.17.2" class="ltx_td"></td>
<td id="S3.T1.8.8.25.17.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.25.17.4" class="ltx_td"></td>
<td id="S3.T1.8.8.25.17.5" class="ltx_td ltx_align_center">48.75</td>
<td id="S3.T1.8.8.25.17.6" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.25.17.7" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.25.17.8" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.25.17.9" class="ltx_td"></td>
<td id="S3.T1.8.8.25.17.10" class="ltx_td ltx_align_center">64.04</td>
<td id="S3.T1.8.8.25.17.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.25.17.12" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.25.17.13" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.8.8.26.18" class="ltx_tr">
<td id="S3.T1.8.8.26.18.1" class="ltx_td ltx_align_left">Picture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>
</td>
<td id="S3.T1.8.8.26.18.2" class="ltx_td"></td>
<td id="S3.T1.8.8.26.18.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.26.18.4" class="ltx_td"></td>
<td id="S3.T1.8.8.26.18.5" class="ltx_td ltx_align_center">43.64</td>
<td id="S3.T1.8.8.26.18.6" class="ltx_td ltx_align_center">45.13</td>
<td id="S3.T1.8.8.26.18.7" class="ltx_td ltx_align_center">20.06</td>
<td id="S3.T1.8.8.26.18.8" class="ltx_td ltx_align_center">49.33</td>
<td id="S3.T1.8.8.26.18.9" class="ltx_td"></td>
<td id="S3.T1.8.8.26.18.10" class="ltx_td ltx_align_center">69.74</td>
<td id="S3.T1.8.8.26.18.11" class="ltx_td ltx_align_center">87.91</td>
<td id="S3.T1.8.8.26.18.12" class="ltx_td ltx_align_center">56.47</td>
<td id="S3.T1.8.8.26.18.13" class="ltx_td ltx_align_center">59.43</td>
</tr>
<tr id="S3.T1.8.8.27.19" class="ltx_tr">
<td id="S3.T1.8.8.27.19.1" class="ltx_td ltx_align_left">SAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>
</td>
<td id="S3.T1.8.8.27.19.2" class="ltx_td"></td>
<td id="S3.T1.8.8.27.19.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.27.19.4" class="ltx_td"></td>
<td id="S3.T1.8.8.27.19.5" class="ltx_td ltx_align_center">61.71</td>
<td id="S3.T1.8.8.27.19.6" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.27.19.7" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.27.19.8" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.27.19.9" class="ltx_td"></td>
<td id="S3.T1.8.8.27.19.10" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.27.19.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.27.19.12" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.27.19.13" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.8.8.28.20" class="ltx_tr">
<td id="S3.T1.8.8.28.20.1" class="ltx_td ltx_align_left">KAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>
</td>
<td id="S3.T1.8.8.28.20.2" class="ltx_td"></td>
<td id="S3.T1.8.8.28.20.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.28.20.4" class="ltx_td"></td>
<td id="S3.T1.8.8.28.20.5" class="ltx_td ltx_align_center">42.60</td>
<td id="S3.T1.8.8.28.20.6" class="ltx_td ltx_align_center">42.12</td>
<td id="S3.T1.8.8.28.20.7" class="ltx_td ltx_align_center">15.52</td>
<td id="S3.T1.8.8.28.20.8" class="ltx_td ltx_align_center">50.28</td>
<td id="S3.T1.8.8.28.20.9" class="ltx_td"></td>
<td id="S3.T1.8.8.28.20.10" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.28.20.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.28.20.12" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.28.20.13" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.8.8.29.21" class="ltx_tr">
<td id="S3.T1.8.8.29.21.1" class="ltx_td ltx_align_left ltx_border_t" colspan="13"><span id="S3.T1.8.8.29.21.1.1" class="ltx_text ltx_font_italic">methods based on weakening language priors:</span></td>
</tr>
<tr id="S3.T1.8.8.30.22" class="ltx_tr">
<td id="S3.T1.8.8.30.22.1" class="ltx_td ltx_align_left ltx_border_t">AdvReg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>
</td>
<td id="S3.T1.8.8.30.22.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.8.8.30.22.3" class="ltx_td ltx_align_center ltx_border_t">UpDn</td>
<td id="S3.T1.8.8.30.22.4" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.8.8.30.22.5" class="ltx_td ltx_align_center ltx_border_t">41.17</td>
<td id="S3.T1.8.8.30.22.6" class="ltx_td ltx_align_center ltx_border_t">65.49</td>
<td id="S3.T1.8.8.30.22.7" class="ltx_td ltx_align_center ltx_border_t">15.48</td>
<td id="S3.T1.8.8.30.22.8" class="ltx_td ltx_align_center ltx_border_t">35.48</td>
<td id="S3.T1.8.8.30.22.9" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.8.8.30.22.10" class="ltx_td ltx_align_center ltx_border_t">62.75</td>
<td id="S3.T1.8.8.30.22.11" class="ltx_td ltx_align_center ltx_border_t">79.84</td>
<td id="S3.T1.8.8.30.22.12" class="ltx_td ltx_align_center ltx_border_t">42.35</td>
<td id="S3.T1.8.8.30.22.13" class="ltx_td ltx_align_center ltx_border_t">55.16</td>
</tr>
<tr id="S3.T1.8.8.31.23" class="ltx_tr">
<td id="S3.T1.8.8.31.23.1" class="ltx_td ltx_align_left">GRL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>
</td>
<td id="S3.T1.8.8.31.23.2" class="ltx_td"></td>
<td id="S3.T1.8.8.31.23.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.31.23.4" class="ltx_td"></td>
<td id="S3.T1.8.8.31.23.5" class="ltx_td ltx_align_center">42.33</td>
<td id="S3.T1.8.8.31.23.6" class="ltx_td ltx_align_center">59.74</td>
<td id="S3.T1.8.8.31.23.7" class="ltx_td ltx_align_center">14.78</td>
<td id="S3.T1.8.8.31.23.8" class="ltx_td ltx_align_center">40.76</td>
<td id="S3.T1.8.8.31.23.9" class="ltx_td"></td>
<td id="S3.T1.8.8.31.23.10" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.31.23.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.31.23.12" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.31.23.13" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.8.8.32.24" class="ltx_tr">
<td id="S3.T1.8.8.32.24.1" class="ltx_td ltx_align_left">RUBi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</td>
<td id="S3.T1.8.8.32.24.2" class="ltx_td"></td>
<td id="S3.T1.8.8.32.24.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.32.24.4" class="ltx_td"></td>
<td id="S3.T1.8.8.32.24.5" class="ltx_td ltx_align_center">44.23</td>
<td id="S3.T1.8.8.32.24.6" class="ltx_td ltx_align_center">67.05</td>
<td id="S3.T1.8.8.32.24.7" class="ltx_td ltx_align_center">17.48</td>
<td id="S3.T1.8.8.32.24.8" class="ltx_td ltx_align_center">39.61</td>
<td id="S3.T1.8.8.32.24.9" class="ltx_td"></td>
<td id="S3.T1.8.8.32.24.10" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.32.24.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.32.24.12" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.32.24.13" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.8.8.33.25" class="ltx_tr">
<td id="S3.T1.8.8.33.25.1" class="ltx_td ltx_align_left">LM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>
</td>
<td id="S3.T1.8.8.33.25.2" class="ltx_td"></td>
<td id="S3.T1.8.8.33.25.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.33.25.4" class="ltx_td"></td>
<td id="S3.T1.8.8.33.25.5" class="ltx_td ltx_align_center">48.78</td>
<td id="S3.T1.8.8.33.25.6" class="ltx_td ltx_align_center">72.78</td>
<td id="S3.T1.8.8.33.25.7" class="ltx_td ltx_align_center">14.61</td>
<td id="S3.T1.8.8.33.25.8" class="ltx_td ltx_align_center">45.58</td>
<td id="S3.T1.8.8.33.25.9" class="ltx_td"></td>
<td id="S3.T1.8.8.33.25.10" class="ltx_td ltx_align_center">63.26</td>
<td id="S3.T1.8.8.33.25.11" class="ltx_td ltx_align_center">81.16</td>
<td id="S3.T1.8.8.33.25.12" class="ltx_td ltx_align_center">42.22</td>
<td id="S3.T1.8.8.33.25.13" class="ltx_td ltx_align_center">55.22</td>
</tr>
<tr id="S3.T1.8.8.34.26" class="ltx_tr">
<td id="S3.T1.8.8.34.26.1" class="ltx_td ltx_align_left">LM+H <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>
</td>
<td id="S3.T1.8.8.34.26.2" class="ltx_td"></td>
<td id="S3.T1.8.8.34.26.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.34.26.4" class="ltx_td"></td>
<td id="S3.T1.8.8.34.26.5" class="ltx_td ltx_align_center">52.01</td>
<td id="S3.T1.8.8.34.26.6" class="ltx_td ltx_align_center">72.58</td>
<td id="S3.T1.8.8.34.26.7" class="ltx_td ltx_align_center">31.12</td>
<td id="S3.T1.8.8.34.26.8" class="ltx_td ltx_align_center">46.97</td>
<td id="S3.T1.8.8.34.26.9" class="ltx_td"></td>
<td id="S3.T1.8.8.34.26.10" class="ltx_td ltx_align_center">56.35</td>
<td id="S3.T1.8.8.34.26.11" class="ltx_td ltx_align_center">65.06</td>
<td id="S3.T1.8.8.34.26.12" class="ltx_td ltx_align_center">37.63</td>
<td id="S3.T1.8.8.34.26.13" class="ltx_td ltx_align_center">54.69</td>
</tr>
<tr id="S3.T1.8.8.35.27" class="ltx_tr">
<td id="S3.T1.8.8.35.27.1" class="ltx_td ltx_align_left">Semantic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>
</td>
<td id="S3.T1.8.8.35.27.2" class="ltx_td"></td>
<td id="S3.T1.8.8.35.27.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.35.27.4" class="ltx_td"></td>
<td id="S3.T1.8.8.35.27.5" class="ltx_td ltx_align_center">47.5</td>
<td id="S3.T1.8.8.35.27.6" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.35.27.7" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.35.27.8" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.35.27.9" class="ltx_td"></td>
<td id="S3.T1.8.8.35.27.10" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.35.27.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.35.27.12" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.35.27.13" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.8.8.36.28" class="ltx_tr">
<td id="S3.T1.8.8.36.28.1" class="ltx_td ltx_align_left">RMFE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>
</td>
<td id="S3.T1.8.8.36.28.2" class="ltx_td"></td>
<td id="S3.T1.8.8.36.28.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.36.28.4" class="ltx_td"></td>
<td id="S3.T1.8.8.36.28.5" class="ltx_td ltx_align_center">54.55</td>
<td id="S3.T1.8.8.36.28.6" class="ltx_td ltx_align_center">74.03</td>
<td id="S3.T1.8.8.36.28.7" class="ltx_td ltx_align_center">49.16</td>
<td id="S3.T1.8.8.36.28.8" class="ltx_td ltx_align_center">45.82</td>
<td id="S3.T1.8.8.36.28.9" class="ltx_td"></td>
<td id="S3.T1.8.8.36.28.10" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.36.28.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.36.28.12" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.36.28.13" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.8.8.8" class="ltx_tr">
<td id="S3.T1.8.8.8.9" class="ltx_td ltx_align_left">CF-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>
</td>
<td id="S3.T1.8.8.8.10" class="ltx_td"></td>
<td id="S3.T1.8.8.8.11" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.8.12" class="ltx_td"></td>
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center">53.55<sup id="S3.T1.1.1.1.1.1" class="ltx_sup"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text ltx_font_italic">±0.10</span></sup>
</td>
<td id="S3.T1.2.2.2.2" class="ltx_td ltx_align_center">91.15<sup id="S3.T1.2.2.2.2.1" class="ltx_sup"><span id="S3.T1.2.2.2.2.1.1" class="ltx_text ltx_font_italic">±0.06</span></sup>
</td>
<td id="S3.T1.3.3.3.3" class="ltx_td ltx_align_center">13.03<sup id="S3.T1.3.3.3.3.1" class="ltx_sup"><span id="S3.T1.3.3.3.3.1.1" class="ltx_text ltx_font_italic">±0.21</span></sup>
</td>
<td id="S3.T1.4.4.4.4" class="ltx_td ltx_align_center">44.97<sup id="S3.T1.4.4.4.4.1" class="ltx_sup"><span id="S3.T1.4.4.4.4.1.1" class="ltx_text ltx_font_italic">±0.20</span></sup>
</td>
<td id="S3.T1.8.8.8.13" class="ltx_td"></td>
<td id="S3.T1.5.5.5.5" class="ltx_td ltx_align_center">63.54<sup id="S3.T1.5.5.5.5.1" class="ltx_sup"><span id="S3.T1.5.5.5.5.1.1" class="ltx_text ltx_font_italic">±0.09</span></sup>
</td>
<td id="S3.T1.6.6.6.6" class="ltx_td ltx_align_center">82.51<sup id="S3.T1.6.6.6.6.1" class="ltx_sup"><span id="S3.T1.6.6.6.6.1.1" class="ltx_text ltx_font_italic">±0.12</span></sup>
</td>
<td id="S3.T1.7.7.7.7" class="ltx_td ltx_align_center">43.96<sup id="S3.T1.7.7.7.7.1" class="ltx_sup"><span id="S3.T1.7.7.7.7.1.1" class="ltx_text ltx_font_italic">±0.17</span></sup>
</td>
<td id="S3.T1.8.8.8.8" class="ltx_td ltx_align_center">54.30<sup id="S3.T1.8.8.8.8.1" class="ltx_sup"><span id="S3.T1.8.8.8.8.1.1" class="ltx_text ltx_font_italic">±0.09</span></sup>
</td>
</tr>
<tr id="S3.T1.8.8.37.29" class="ltx_tr">
<td id="S3.T1.8.8.37.29.1" class="ltx_td ltx_align_left">GGE-DQ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>
</td>
<td id="S3.T1.8.8.37.29.2" class="ltx_td"></td>
<td id="S3.T1.8.8.37.29.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.37.29.4" class="ltx_td"></td>
<td id="S3.T1.8.8.37.29.5" class="ltx_td ltx_align_center">57.32</td>
<td id="S3.T1.8.8.37.29.6" class="ltx_td ltx_align_center">87.04</td>
<td id="S3.T1.8.8.37.29.7" class="ltx_td ltx_align_center">27.75</td>
<td id="S3.T1.8.8.37.29.8" class="ltx_td ltx_align_center">49.59</td>
<td id="S3.T1.8.8.37.29.9" class="ltx_td"></td>
<td id="S3.T1.8.8.37.29.10" class="ltx_td ltx_align_center">59.11</td>
<td id="S3.T1.8.8.37.29.11" class="ltx_td ltx_align_center">73.27</td>
<td id="S3.T1.8.8.37.29.12" class="ltx_td ltx_align_center">39.99</td>
<td id="S3.T1.8.8.37.29.13" class="ltx_td ltx_align_center">54.39</td>
</tr>
<tr id="S3.T1.8.8.38.30" class="ltx_tr">
<td id="S3.T1.8.8.38.30.1" class="ltx_td ltx_align_left">LPF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>
</td>
<td id="S3.T1.8.8.38.30.2" class="ltx_td"></td>
<td id="S3.T1.8.8.38.30.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.38.30.4" class="ltx_td"></td>
<td id="S3.T1.8.8.38.30.5" class="ltx_td ltx_align_center">55.34</td>
<td id="S3.T1.8.8.38.30.6" class="ltx_td ltx_align_center">88.61</td>
<td id="S3.T1.8.8.38.30.7" class="ltx_td ltx_align_center">23.78</td>
<td id="S3.T1.8.8.38.30.8" class="ltx_td ltx_align_center">46.57</td>
<td id="S3.T1.8.8.38.30.9" class="ltx_td"></td>
<td id="S3.T1.8.8.38.30.10" class="ltx_td ltx_align_center">55.01</td>
<td id="S3.T1.8.8.38.30.11" class="ltx_td ltx_align_center">64.87</td>
<td id="S3.T1.8.8.38.30.12" class="ltx_td ltx_align_center">37.45</td>
<td id="S3.T1.8.8.38.30.13" class="ltx_td ltx_align_center">52.08</td>
</tr>
<tr id="S3.T1.8.8.39.31" class="ltx_tr">
<td id="S3.T1.8.8.39.31.1" class="ltx_td ltx_align_left ltx_border_t" colspan="13"><span id="S3.T1.8.8.39.31.1.1" class="ltx_text ltx_font_italic">methods based on data argumentation and training strategies:</span></td>
</tr>
<tr id="S3.T1.8.8.40.32" class="ltx_tr">
<td id="S3.T1.8.8.40.32.1" class="ltx_td ltx_align_left ltx_border_t">ActSeek <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>
</td>
<td id="S3.T1.8.8.40.32.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.8.8.40.32.3" class="ltx_td ltx_align_center ltx_border_t">UpDn</td>
<td id="S3.T1.8.8.40.32.4" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.8.8.40.32.5" class="ltx_td ltx_align_center ltx_border_t">46.00</td>
<td id="S3.T1.8.8.40.32.6" class="ltx_td ltx_align_center ltx_border_t">58.24</td>
<td id="S3.T1.8.8.40.32.7" class="ltx_td ltx_align_center ltx_border_t">29.49</td>
<td id="S3.T1.8.8.40.32.8" class="ltx_td ltx_align_center ltx_border_t">44.33</td>
<td id="S3.T1.8.8.40.32.9" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.8.8.40.32.10" class="ltx_td ltx_align_center ltx_border_t">–</td>
<td id="S3.T1.8.8.40.32.11" class="ltx_td ltx_align_center ltx_border_t">–</td>
<td id="S3.T1.8.8.40.32.12" class="ltx_td ltx_align_center ltx_border_t">–</td>
<td id="S3.T1.8.8.40.32.13" class="ltx_td ltx_align_center ltx_border_t">–</td>
</tr>
<tr id="S3.T1.8.8.41.33" class="ltx_tr">
<td id="S3.T1.8.8.41.33.1" class="ltx_td ltx_align_left">A1C-WS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>
</td>
<td id="S3.T1.8.8.41.33.2" class="ltx_td"></td>
<td id="S3.T1.8.8.41.33.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.41.33.4" class="ltx_td"></td>
<td id="S3.T1.8.8.41.33.5" class="ltx_td ltx_align_center">39.6</td>
<td id="S3.T1.8.8.41.33.6" class="ltx_td ltx_align_center">42.7</td>
<td id="S3.T1.8.8.41.33.7" class="ltx_td ltx_align_center">12.9</td>
<td id="S3.T1.8.8.41.33.8" class="ltx_td ltx_align_center">45.3</td>
<td id="S3.T1.8.8.41.33.9" class="ltx_td"></td>
<td id="S3.T1.8.8.41.33.10" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.41.33.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.41.33.12" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.41.33.13" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.8.8.42.34" class="ltx_tr">
<td id="S3.T1.8.8.42.34.1" class="ltx_td ltx_align_left">CSS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>
</td>
<td id="S3.T1.8.8.42.34.2" class="ltx_td"></td>
<td id="S3.T1.8.8.42.34.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.42.34.4" class="ltx_td"></td>
<td id="S3.T1.8.8.42.34.5" class="ltx_td ltx_align_center">58.95</td>
<td id="S3.T1.8.8.42.34.6" class="ltx_td ltx_align_center">84.37</td>
<td id="S3.T1.8.8.42.34.7" class="ltx_td ltx_align_center">49.42</td>
<td id="S3.T1.8.8.42.34.8" class="ltx_td ltx_align_center">48.21</td>
<td id="S3.T1.8.8.42.34.9" class="ltx_td"></td>
<td id="S3.T1.8.8.42.34.10" class="ltx_td ltx_align_center">59.91</td>
<td id="S3.T1.8.8.42.34.11" class="ltx_td ltx_align_center">73.25</td>
<td id="S3.T1.8.8.42.34.12" class="ltx_td ltx_align_center">39.77</td>
<td id="S3.T1.8.8.42.34.13" class="ltx_td ltx_align_center">55.11</td>
</tr>
<tr id="S3.T1.8.8.43.35" class="ltx_tr">
<td id="S3.T1.8.8.43.35.1" class="ltx_td ltx_align_left">CL-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>
</td>
<td id="S3.T1.8.8.43.35.2" class="ltx_td"></td>
<td id="S3.T1.8.8.43.35.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.43.35.4" class="ltx_td"></td>
<td id="S3.T1.8.8.43.35.5" class="ltx_td ltx_align_center">59.18</td>
<td id="S3.T1.8.8.43.35.6" class="ltx_td ltx_align_center">86.99</td>
<td id="S3.T1.8.8.43.35.7" class="ltx_td ltx_align_center">49.89</td>
<td id="S3.T1.8.8.43.35.8" class="ltx_td ltx_align_center">47.16</td>
<td id="S3.T1.8.8.43.35.9" class="ltx_td"></td>
<td id="S3.T1.8.8.43.35.10" class="ltx_td ltx_align_center">57.29</td>
<td id="S3.T1.8.8.43.35.11" class="ltx_td ltx_align_center">67.27</td>
<td id="S3.T1.8.8.43.35.12" class="ltx_td ltx_align_center">38.40</td>
<td id="S3.T1.8.8.43.35.13" class="ltx_td ltx_align_center">54.71</td>
</tr>
<tr id="S3.T1.8.8.44.36" class="ltx_tr">
<td id="S3.T1.8.8.44.36.1" class="ltx_td ltx_align_left">GradSup <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>
</td>
<td id="S3.T1.8.8.44.36.2" class="ltx_td"></td>
<td id="S3.T1.8.8.44.36.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.44.36.4" class="ltx_td"></td>
<td id="S3.T1.8.8.44.36.5" class="ltx_td ltx_align_center">46.8</td>
<td id="S3.T1.8.8.44.36.6" class="ltx_td ltx_align_center">64.5</td>
<td id="S3.T1.8.8.44.36.7" class="ltx_td ltx_align_center">15.3</td>
<td id="S3.T1.8.8.44.36.8" class="ltx_td ltx_align_center">45.9</td>
<td id="S3.T1.8.8.44.36.9" class="ltx_td"></td>
<td id="S3.T1.8.8.44.36.10" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.44.36.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.44.36.12" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.44.36.13" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.8.8.45.37" class="ltx_tr">
<td id="S3.T1.8.8.45.37.1" class="ltx_td ltx_align_left">Loss-Rescaling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>
</td>
<td id="S3.T1.8.8.45.37.2" class="ltx_td"></td>
<td id="S3.T1.8.8.45.37.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.45.37.4" class="ltx_td"></td>
<td id="S3.T1.8.8.45.37.5" class="ltx_td ltx_align_center">53.26</td>
<td id="S3.T1.8.8.45.37.6" class="ltx_td ltx_align_center">72.82</td>
<td id="S3.T1.8.8.45.37.7" class="ltx_td ltx_align_center">48.00</td>
<td id="S3.T1.8.8.45.37.8" class="ltx_td ltx_align_center">44.46</td>
<td id="S3.T1.8.8.45.37.9" class="ltx_td"></td>
<td id="S3.T1.8.8.45.37.10" class="ltx_td ltx_align_center">56.81</td>
<td id="S3.T1.8.8.45.37.11" class="ltx_td ltx_align_center">68.21</td>
<td id="S3.T1.8.8.45.37.12" class="ltx_td ltx_align_center">36.37</td>
<td id="S3.T1.8.8.45.37.13" class="ltx_td ltx_align_center">52.29</td>
</tr>
<tr id="S3.T1.8.8.46.38" class="ltx_tr">
<td id="S3.T1.8.8.46.38.1" class="ltx_td ltx_align_left">Mutant <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>
</td>
<td id="S3.T1.8.8.46.38.2" class="ltx_td"></td>
<td id="S3.T1.8.8.46.38.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.46.38.4" class="ltx_td"></td>
<td id="S3.T1.8.8.46.38.5" class="ltx_td ltx_align_center">61.72</td>
<td id="S3.T1.8.8.46.38.6" class="ltx_td ltx_align_center">88.90</td>
<td id="S3.T1.8.8.46.38.7" class="ltx_td ltx_align_center">49.68</td>
<td id="S3.T1.8.8.46.38.8" class="ltx_td ltx_align_center">50.78</td>
<td id="S3.T1.8.8.46.38.9" class="ltx_td"></td>
<td id="S3.T1.8.8.46.38.10" class="ltx_td ltx_align_center">62.56</td>
<td id="S3.T1.8.8.46.38.11" class="ltx_td ltx_align_center">82.07</td>
<td id="S3.T1.8.8.46.38.12" class="ltx_td ltx_align_center">42.52</td>
<td id="S3.T1.8.8.46.38.13" class="ltx_td ltx_align_center">53.28</td>
</tr>
<tr id="S3.T1.8.8.47.39" class="ltx_tr">
<td id="S3.T1.8.8.47.39.1" class="ltx_td ltx_align_left">RandImg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>
</td>
<td id="S3.T1.8.8.47.39.2" class="ltx_td"></td>
<td id="S3.T1.8.8.47.39.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.47.39.4" class="ltx_td"></td>
<td id="S3.T1.8.8.47.39.5" class="ltx_td ltx_align_center">55.37</td>
<td id="S3.T1.8.8.47.39.6" class="ltx_td ltx_align_center">83.89</td>
<td id="S3.T1.8.8.47.39.7" class="ltx_td ltx_align_center">41.60</td>
<td id="S3.T1.8.8.47.39.8" class="ltx_td ltx_align_center">44.20</td>
<td id="S3.T1.8.8.47.39.9" class="ltx_td"></td>
<td id="S3.T1.8.8.47.39.10" class="ltx_td ltx_align_center">57.24</td>
<td id="S3.T1.8.8.47.39.11" class="ltx_td ltx_align_center">76.53</td>
<td id="S3.T1.8.8.47.39.12" class="ltx_td ltx_align_center">33.87</td>
<td id="S3.T1.8.8.47.39.13" class="ltx_td ltx_align_center">48.57</td>
</tr>
<tr id="S3.T1.8.8.48.40" class="ltx_tr">
<td id="S3.T1.8.8.48.40.1" class="ltx_td ltx_align_left">SSL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>
</td>
<td id="S3.T1.8.8.48.40.2" class="ltx_td"></td>
<td id="S3.T1.8.8.48.40.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.48.40.4" class="ltx_td"></td>
<td id="S3.T1.8.8.48.40.5" class="ltx_td ltx_align_center">57.59</td>
<td id="S3.T1.8.8.48.40.6" class="ltx_td ltx_align_center">86.53</td>
<td id="S3.T1.8.8.48.40.7" class="ltx_td ltx_align_center">29.87</td>
<td id="S3.T1.8.8.48.40.8" class="ltx_td ltx_align_center">50.03</td>
<td id="S3.T1.8.8.48.40.9" class="ltx_td"></td>
<td id="S3.T1.8.8.48.40.10" class="ltx_td ltx_align_center">63.73</td>
<td id="S3.T1.8.8.48.40.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.48.40.12" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.48.40.13" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.8.8.49.41" class="ltx_tr">
<td id="S3.T1.8.8.49.41.1" class="ltx_td ltx_align_left">Unshuffling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>
</td>
<td id="S3.T1.8.8.49.41.2" class="ltx_td"></td>
<td id="S3.T1.8.8.49.41.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.49.41.4" class="ltx_td"></td>
<td id="S3.T1.8.8.49.41.5" class="ltx_td ltx_align_center">42.39</td>
<td id="S3.T1.8.8.49.41.6" class="ltx_td ltx_align_center">47.72</td>
<td id="S3.T1.8.8.49.41.7" class="ltx_td ltx_align_center">14.43</td>
<td id="S3.T1.8.8.49.41.8" class="ltx_td ltx_align_center">47.24</td>
<td id="S3.T1.8.8.49.41.9" class="ltx_td"></td>
<td id="S3.T1.8.8.49.41.10" class="ltx_td ltx_align_center">61.08</td>
<td id="S3.T1.8.8.49.41.11" class="ltx_td ltx_align_center">78.32</td>
<td id="S3.T1.8.8.49.41.12" class="ltx_td ltx_align_center">42.16</td>
<td id="S3.T1.8.8.49.41.13" class="ltx_td ltx_align_center">52.81</td>
</tr>
<tr id="S3.T1.8.8.50.42" class="ltx_tr">
<td id="S3.T1.8.8.50.42.1" class="ltx_td ltx_align_left">LP-Focal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>
</td>
<td id="S3.T1.8.8.50.42.2" class="ltx_td"></td>
<td id="S3.T1.8.8.50.42.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.50.42.4" class="ltx_td"></td>
<td id="S3.T1.8.8.50.42.5" class="ltx_td ltx_align_center">58.45</td>
<td id="S3.T1.8.8.50.42.6" class="ltx_td ltx_align_center">88.34</td>
<td id="S3.T1.8.8.50.42.7" class="ltx_td ltx_align_center">34.67</td>
<td id="S3.T1.8.8.50.42.8" class="ltx_td ltx_align_center">49.32</td>
<td id="S3.T1.8.8.50.42.9" class="ltx_td"></td>
<td id="S3.T1.8.8.50.42.10" class="ltx_td ltx_align_center">62.45</td>
<td id="S3.T1.8.8.50.42.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.50.42.12" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.50.42.13" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.8.8.51.43" class="ltx_tr">
<td id="S3.T1.8.8.51.43.1" class="ltx_td ltx_align_left">ADA-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>
</td>
<td id="S3.T1.8.8.51.43.2" class="ltx_td"></td>
<td id="S3.T1.8.8.51.43.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.51.43.4" class="ltx_td"></td>
<td id="S3.T1.8.8.51.43.5" class="ltx_td ltx_align_center">54.67</td>
<td id="S3.T1.8.8.51.43.6" class="ltx_td ltx_align_center">72.47</td>
<td id="S3.T1.8.8.51.43.7" class="ltx_td ltx_align_center">53.81</td>
<td id="S3.T1.8.8.51.43.8" class="ltx_td ltx_align_center">45.58</td>
<td id="S3.T1.8.8.51.43.9" class="ltx_td"></td>
<td id="S3.T1.8.8.51.43.10" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.51.43.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.51.43.12" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.51.43.13" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.8.8.52.44" class="ltx_tr">
<td id="S3.T1.8.8.52.44.1" class="ltx_td ltx_align_left">CCB-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>
</td>
<td id="S3.T1.8.8.52.44.2" class="ltx_td"></td>
<td id="S3.T1.8.8.52.44.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.52.44.4" class="ltx_td"></td>
<td id="S3.T1.8.8.52.44.5" class="ltx_td ltx_align_center">59.12</td>
<td id="S3.T1.8.8.52.44.6" class="ltx_td ltx_align_center">89.12</td>
<td id="S3.T1.8.8.52.44.7" class="ltx_td ltx_align_center">51.04</td>
<td id="S3.T1.8.8.52.44.8" class="ltx_td ltx_align_center">45.62</td>
<td id="S3.T1.8.8.52.44.9" class="ltx_td"></td>
<td id="S3.T1.8.8.52.44.10" class="ltx_td ltx_align_center">59.17</td>
<td id="S3.T1.8.8.52.44.11" class="ltx_td ltx_align_center">77.28</td>
<td id="S3.T1.8.8.52.44.12" class="ltx_td ltx_align_center">33.71</td>
<td id="S3.T1.8.8.52.44.13" class="ltx_td ltx_align_center">52.14</td>
</tr>
<tr id="S3.T1.8.8.53.45" class="ltx_tr">
<td id="S3.T1.8.8.53.45.1" class="ltx_td ltx_align_left">SBS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>
</td>
<td id="S3.T1.8.8.53.45.2" class="ltx_td"></td>
<td id="S3.T1.8.8.53.45.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.53.45.4" class="ltx_td"></td>
<td id="S3.T1.8.8.53.45.5" class="ltx_td ltx_align_center">59.57</td>
<td id="S3.T1.8.8.53.45.6" class="ltx_td ltx_align_center">87.44</td>
<td id="S3.T1.8.8.53.45.7" class="ltx_td ltx_align_center">52.96</td>
<td id="S3.T1.8.8.53.45.8" class="ltx_td ltx_align_center">46.79</td>
<td id="S3.T1.8.8.53.45.9" class="ltx_td"></td>
<td id="S3.T1.8.8.53.45.10" class="ltx_td ltx_align_center">61.97</td>
<td id="S3.T1.8.8.53.45.11" class="ltx_td ltx_align_center">78.80</td>
<td id="S3.T1.8.8.53.45.12" class="ltx_td ltx_align_center">42.17</td>
<td id="S3.T1.8.8.53.45.13" class="ltx_td ltx_align_center">54.41</td>
</tr>
<tr id="S3.T1.8.8.54.46" class="ltx_tr">
<td id="S3.T1.8.8.54.46.1" class="ltx_td ltx_align_left">WeaQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite>
</td>
<td id="S3.T1.8.8.54.46.2" class="ltx_td"></td>
<td id="S3.T1.8.8.54.46.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.54.46.4" class="ltx_td"></td>
<td id="S3.T1.8.8.54.46.5" class="ltx_td ltx_align_center">41.2</td>
<td id="S3.T1.8.8.54.46.6" class="ltx_td ltx_align_center">68.5</td>
<td id="S3.T1.8.8.54.46.7" class="ltx_td ltx_align_center">29.8</td>
<td id="S3.T1.8.8.54.46.8" class="ltx_td ltx_align_center">30.0</td>
<td id="S3.T1.8.8.54.46.9" class="ltx_td"></td>
<td id="S3.T1.8.8.54.46.10" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.54.46.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.54.46.12" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.54.46.13" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.8.8.55.47" class="ltx_tr">
<td id="S3.T1.8.8.55.47.1" class="ltx_td ltx_align_left">X-GGM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>
</td>
<td id="S3.T1.8.8.55.47.2" class="ltx_td"></td>
<td id="S3.T1.8.8.55.47.3" class="ltx_td ltx_align_center">UpDn</td>
<td id="S3.T1.8.8.55.47.4" class="ltx_td"></td>
<td id="S3.T1.8.8.55.47.5" class="ltx_td ltx_align_center">45.71</td>
<td id="S3.T1.8.8.55.47.6" class="ltx_td ltx_align_center">43.48</td>
<td id="S3.T1.8.8.55.47.7" class="ltx_td ltx_align_center">27.65</td>
<td id="S3.T1.8.8.55.47.8" class="ltx_td ltx_align_center">52.34</td>
<td id="S3.T1.8.8.55.47.9" class="ltx_td"></td>
<td id="S3.T1.8.8.55.47.10" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.55.47.11" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.55.47.12" class="ltx_td ltx_align_center">–</td>
<td id="S3.T1.8.8.55.47.13" class="ltx_td ltx_align_center">–</td>
</tr>
<tr id="S3.T1.8.8.56.48" class="ltx_tr">
<td id="S3.T1.8.8.56.48.1" class="ltx_td ltx_align_left ltx_border_bb">CFT-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>
</td>
<td id="S3.T1.8.8.56.48.2" class="ltx_td ltx_border_bb"></td>
<td id="S3.T1.8.8.56.48.3" class="ltx_td ltx_align_center ltx_border_bb">UpDn</td>
<td id="S3.T1.8.8.56.48.4" class="ltx_td ltx_border_bb"></td>
<td id="S3.T1.8.8.56.48.5" class="ltx_td ltx_align_center ltx_border_bb">59.37</td>
<td id="S3.T1.8.8.56.48.6" class="ltx_td ltx_align_center ltx_border_bb">87.95</td>
<td id="S3.T1.8.8.56.48.7" class="ltx_td ltx_align_center ltx_border_bb">52.42</td>
<td id="S3.T1.8.8.56.48.8" class="ltx_td ltx_align_center ltx_border_bb">46.30</td>
<td id="S3.T1.8.8.56.48.9" class="ltx_td ltx_border_bb"></td>
<td id="S3.T1.8.8.56.48.10" class="ltx_td ltx_align_center ltx_border_bb">59.82</td>
<td id="S3.T1.8.8.56.48.11" class="ltx_td ltx_align_center ltx_border_bb">74.91</td>
<td id="S3.T1.8.8.56.48.12" class="ltx_td ltx_align_center ltx_border_bb">38.64</td>
<td id="S3.T1.8.8.56.48.13" class="ltx_td ltx_align_center ltx_border_bb">53.97</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Evaluation Measures</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">How to evaluate the correctness and rationality of sentences generated by computer is a basic and difficult task. When evaluating the correctness of sentences, we need to consider the correctness of syntax and whether the semantic information expressed by sentences is correct. In order to simplify this problem, most data sets of VQA are evaluated by limiting the generated answer sentences to a single word or phrase, generally 1 to 3 words in length. Through simplification, the semantic expression of sentences is more specific, and the difficulty of sentence correctness evaluation is greatly reduced.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Malinowski et al. creatively proposedd two indicators for VQA tasks. The first method uses string to match the difference between the generated sentence and ground truth. It can be considered correct only when the two sentences are completely consistent. The second method uses Wu Palmer similarity to evaluate the generated sentences. This method evaluates the similarity between the common subsequences of the two sentences in the syntax classification tree. When the similarity between two lists exceeds a fixed threshold, the generated sentence (answer) is considered to be correct.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">VQA-real dataset collected 10 different ground truth answers for each question in order to solve the problem of fuzzy evaluation criteria in VQA evaluation. When evaluating this data set, the generated sentences (answers) need to be compared with the answers of 10 human answers. The formula is as follows:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="accuracy=\min\left(\frac{\#\text{ humans provided that answer }}{3},1\right)" display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.4" xref="S3.E1.m1.3.4.cmml"><mrow id="S3.E1.m1.3.4.2" xref="S3.E1.m1.3.4.2.cmml"><mi id="S3.E1.m1.3.4.2.2" xref="S3.E1.m1.3.4.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.4.2.1" xref="S3.E1.m1.3.4.2.1.cmml">​</mo><mi id="S3.E1.m1.3.4.2.3" xref="S3.E1.m1.3.4.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.4.2.1a" xref="S3.E1.m1.3.4.2.1.cmml">​</mo><mi id="S3.E1.m1.3.4.2.4" xref="S3.E1.m1.3.4.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.4.2.1b" xref="S3.E1.m1.3.4.2.1.cmml">​</mo><mi id="S3.E1.m1.3.4.2.5" xref="S3.E1.m1.3.4.2.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.4.2.1c" xref="S3.E1.m1.3.4.2.1.cmml">​</mo><mi id="S3.E1.m1.3.4.2.6" xref="S3.E1.m1.3.4.2.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.4.2.1d" xref="S3.E1.m1.3.4.2.1.cmml">​</mo><mi id="S3.E1.m1.3.4.2.7" xref="S3.E1.m1.3.4.2.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.4.2.1e" xref="S3.E1.m1.3.4.2.1.cmml">​</mo><mi id="S3.E1.m1.3.4.2.8" xref="S3.E1.m1.3.4.2.8.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.4.2.1f" xref="S3.E1.m1.3.4.2.1.cmml">​</mo><mi id="S3.E1.m1.3.4.2.9" xref="S3.E1.m1.3.4.2.9.cmml">y</mi></mrow><mo id="S3.E1.m1.3.4.1" xref="S3.E1.m1.3.4.1.cmml">=</mo><mrow id="S3.E1.m1.3.4.3.2" xref="S3.E1.m1.3.4.3.1.cmml"><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">min</mi><mo id="S3.E1.m1.3.4.3.2a" xref="S3.E1.m1.3.4.3.1.cmml">⁡</mo><mrow id="S3.E1.m1.3.4.3.2.1" xref="S3.E1.m1.3.4.3.1.cmml"><mo id="S3.E1.m1.3.4.3.2.1.1" xref="S3.E1.m1.3.4.3.1.cmml">(</mo><mfrac id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mi mathvariant="normal" id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">#</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.1" xref="S3.E1.m1.2.2.2.1.cmml">​</mo><mtext id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3a.cmml"> humans provided that answer </mtext></mrow><mn id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.3.cmml">3</mn></mfrac><mo id="S3.E1.m1.3.4.3.2.1.2" xref="S3.E1.m1.3.4.3.1.cmml">,</mo><mn id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">1</mn><mo id="S3.E1.m1.3.4.3.2.1.3" xref="S3.E1.m1.3.4.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.4.cmml" xref="S3.E1.m1.3.4"><eq id="S3.E1.m1.3.4.1.cmml" xref="S3.E1.m1.3.4.1"></eq><apply id="S3.E1.m1.3.4.2.cmml" xref="S3.E1.m1.3.4.2"><times id="S3.E1.m1.3.4.2.1.cmml" xref="S3.E1.m1.3.4.2.1"></times><ci id="S3.E1.m1.3.4.2.2.cmml" xref="S3.E1.m1.3.4.2.2">𝑎</ci><ci id="S3.E1.m1.3.4.2.3.cmml" xref="S3.E1.m1.3.4.2.3">𝑐</ci><ci id="S3.E1.m1.3.4.2.4.cmml" xref="S3.E1.m1.3.4.2.4">𝑐</ci><ci id="S3.E1.m1.3.4.2.5.cmml" xref="S3.E1.m1.3.4.2.5">𝑢</ci><ci id="S3.E1.m1.3.4.2.6.cmml" xref="S3.E1.m1.3.4.2.6">𝑟</ci><ci id="S3.E1.m1.3.4.2.7.cmml" xref="S3.E1.m1.3.4.2.7">𝑎</ci><ci id="S3.E1.m1.3.4.2.8.cmml" xref="S3.E1.m1.3.4.2.8">𝑐</ci><ci id="S3.E1.m1.3.4.2.9.cmml" xref="S3.E1.m1.3.4.2.9">𝑦</ci></apply><apply id="S3.E1.m1.3.4.3.1.cmml" xref="S3.E1.m1.3.4.3.2"><min id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"></min><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><divide id="S3.E1.m1.2.2.1.cmml" xref="S3.E1.m1.2.2"></divide><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><times id="S3.E1.m1.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.1"></times><ci id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">#</ci><ci id="S3.E1.m1.2.2.2.3a.cmml" xref="S3.E1.m1.2.2.2.3"><mtext id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"> humans provided that answer </mtext></ci></apply><cn type="integer" id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.3">3</cn></apply><cn type="integer" id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">accuracy=\min\left(\frac{\#\text{ humans provided that answer }}{3},1\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Results of existing methods</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">At present, most of the existing methods to solve the problem of language bias in VQA are evaluated on VQA-CP v2 and verified on VQA v2. This paper investigates the papers published in top conferences and relevant journals in recent four years, and counts the existing experimental results in table 1. These results are derived from their original papers.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Discussion and future directions</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">VQA is a relatively new task compared with other visual tasks. It has produced great development in just a few years, but it is far from mature compared with other tasks, such as image recognition, face recognition and other tasks that can be almost completely applied. Compared with VQA’s initial vision and completely free visual QA in a completely open world, the current VQA task adds many constraints and restrictions to simplify this task. At the same time, due to the existence of language bias, there are great difficulties and limitations in the application of visual question answering. Therefore, this paper summarizes the progress of deep learning and existing methods, and gives possible future research directions.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p"><em id="S4.p2.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">External knowledge:</em> VQA task is a complete artificial intelligence task, which contains image information and language information to realize intelligent question and answer. However, the current VQA task is generally to find the correct answer from the preset answer. One of the sources of language bias is due to the uneven distribution of data and question answers. Therefore, for a reasonable VQA system, it should search for the correct answers more widely from the external database, rather than using the predetermined question types and answer types. This has great limitations on the application of VQA. Using more complete external data sets or combining knowledge maps to answer questions may be one of the effective means to reduce language bias, which can help the model obtain a more evenly distributed question answer database. At present, there are some data sets related to external databases<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>, <a href="#bib.bib100" title="" class="ltx_ref">100</a>, <a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite> in other VQA tasks, but the existing methods generally improve performance through unlimited cumulative data set size. How to find more efficient external knowledge assistance methods is one of the research directions to help solve language bias.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p"><em id="S4.p3.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">Long-tail learning:</em> Because the distribution of question answers in the data set is uneven, there is a long tail distribution problem. However, the labels of questions and answers are uniformly divided into counting, whether or not. The answers are simply classified and contain less bias information. The category is a banana color problem. Due to the lack of more fine-grained banana color labels, it is difficult to simply transform the language bias problem into a long tail<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>, <a href="#bib.bib103" title="" class="ltx_ref">103</a>, <a href="#bib.bib104" title="" class="ltx_ref">104</a>, <a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite> distribution problem. This is also reasonable in real life. Due to the limitations of manpower and resources, it is difficult to classify data sets very fine-grained, which inevitably leads to the imbalance of more fine-grained labels within the classification, and also the problem of language bias. Therefore, how to resolve the language bias problem in VQA into a long tail distribution problem, simplify the problem and realize the in-depth mining of language and visual information to extract fine-grained labels may be the most effective research direction.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p"><em id="S4.p4.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">Causality:</em> Some of the existing methods to slow down language priors and data enhancement can be classified as causal inference methods<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>, <a href="#bib.bib107" title="" class="ltx_ref">107</a>, <a href="#bib.bib108" title="" class="ltx_ref">108</a>, <a href="#bib.bib109" title="" class="ltx_ref">109</a>, <a href="#bib.bib110" title="" class="ltx_ref">110</a>, <a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite>. The problem of language bias in VQA can be analyzed from the perspective of causality, including the construction of causality diagram <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>, counterfactual data enhancement <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> and counterfactual thinking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>. In the case of balanced distribution of data sets, the causality method can help the model better analyze and use the core information of images and problems, and correlate the two. Counterfactual thinking is a method of modeling human thinking process. Through targeted data enhancement and constructing counterfactual examples to deduce the model from multiple levels, we can analyze the source of language bias from the perspective of causality and slow it down to a certain extent. In most cases, there is no bias due to different data. Therefore, how to let the machine learn the profound causal relationship between data and learn the way of human thinking is an effective way to solve the language bias.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">As the first survey in this field, this paper makes a comprehensive review of language bias in Visual Question Answering (VQA). This paper combs the existing methods in detail, which is divided into three parts: enhancing visual information, reducing language information, data enhancement and training processing. We introduce relevant methods and re-examine the advantages and disadvantages of existing methods. At the same time, the mainstream data sets and the experimental results of various methods on the data sets are also introduced. In addition, we have not pointed out some possible hopes for the future development and research in this field, and revealed that language bias is a common phenomenon in practical application and deep learning.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick, and
D. Parikh, “Vqa: Visual question answering,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Agrawal, J. Lu, S. Antol, M. Mitchell, C. L. Zitnick, D. Parikh, and
D. Batra, “Vqa: Visual question answering,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">International Journal of
Computer Vision</em>, vol. 123, no. 1, pp. 4–31, 2017.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: towards real-time
object detection with region proposal networks,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on
pattern analysis and machine intelligence</em>, vol. 39, no. 6, pp. 1137–1149,
2016.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in
<em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>,
2017, pp. 2961–2969.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
“Feature pyramid networks for object detection,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the IEEE conference on computer vision and pattern recognition</em>, 2017, pp.
2117–2125.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for
dense object detection,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international
conference on computer vision</em>, 2017, pp. 2980–2988.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J. Ding, L. Xu, J. Guo, and S. Dai, “Human detection in dense scene of
classrooms,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">2020 IEEE International Conference on Image Processing
(ICIP)</em>.   IEEE, 2020, pp. 618–622.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
H. Qiu, H. Li, Q. Wu, F. Meng, L. Xu, K. N. Ngan, and H. Shi, “Hierarchical
context features embedding for object detection,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Multimedia</em>, vol. 22, no. 12, pp. 3039–3050, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
H. Qiu, H. Li, Q. Wu, and H. Shi, “Offset bin classification network for
accurate object detection,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition</em>, 2020, pp. 13 188–13 197.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
L. Wang, C. Shang, H. Qiu, T. Zhao, B. Qiu, and H. Li, “Multi-stage tag
guidance network in video caption,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM
International Conference on Multimedia</em>, 2020, pp. 4610–4614.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
W. Li, Z. Wang, X. Wu, J. Zhang, Q. Peng, and H. Li, “Codan: Counting-driven
attention network for vehicle detection in congested scenes,” in
<em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM International Conference on Multimedia</em>,
2020, pp. 73–82.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
W. Li, H. Li, Q. Wu, X. Chen, and K. N. Ngan, “Simultaneously detecting and
counting dense vehicles from drone images,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Industrial Electronics</em>, vol. 66, no. 12, pp. 9651–9662, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
W. Li, H. Li, Q. Wu, F. Meng, L. Xu, and K. N. Ngan, “Headnet: An end-to-end
adaptive relational network for head detection,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Circuits and Systems for Video Technology</em>, vol. 30, no. 2, pp. 482–494,
2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
H. Qiu, H. Li, Q. Wu, F. Meng, K. N. Ngan, and H. Shi, “A2rmnet: Adaptively
aspect ratio multi-scale network for object detection in remote sensing
images,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Remote Sensing</em>, vol. 11, no. 13, p. 1594, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
X. Chen, H. Li, Q. Wu, F. Meng, and H. Qiu, “Bal-r2cnn: High quality recurrent
object detection with balance optimization,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Multimedia</em>, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
X. Chen, H. Li, Q. Wu, K. N. Ngan, and L. Xu, “High-quality r-cnn object
detection using multi-path detection calibration network,” <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Circuits and Systems for Video Technology</em>, vol. 31, no. 2,
pp. 715–727, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
W. Li, Q. Wu, L. Xu, and C. Shang, “Incremental learning of single-stage
detectors with mining memory neurons,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">2018 IEEE 4th International
Conference on Computer and Communications (ICCC)</em>.   IEEE, 2018, pp. 1981–1985.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J. Carreira and A. Zisserman, “Quo vadis, action recognition? a new model and
the kinetics dataset,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Computer Vision &amp; Pattern Recognition</em>,
2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M. Paluri, “A closer
look at spatiotemporal convolutions for action recognition,” in
<em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition</em>, 2018, pp. 6450–6459.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning spatio
temporal features with 3d convolutional networks,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the IEEE international conference on computer vision</em>, 2015, pp. 4489–4497.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A
large-scale hierarchical image database,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">2009 IEEE conference on
computer vision and pattern recognition</em>.   Ieee, 2009, pp. 248–255.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision
and pattern recognition</em>, 2016, pp. 770–778.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Y. Yang, F. Meng, H. Li, Q. Wu, X. Xu, and S. Chen, “A new local
transformation module for few-shot segmentation,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">International
Conference on Multimedia Modeling</em>.   Springer, 2020, pp. 76–87.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J. Guo, L. Xu, J. Ding, B. He, S. Dai, and F. Liu, “A deep supervised edge
optimization algorithm for salt body segmentation,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">IEEE Geoscience
and Remote Sensing Letters</em>, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
H. Shi, H. Li, Q. Wu, and K. N. Ngan, “Query reconstruction network for
referring expression image segmentation,” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Multimedia</em>, vol. 23, pp. 995–1007, 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
L. Yang, F. Meng, H. Li, Q. Wu, and Q. Cheng, “Learning with noisy class
labels for instance segmentation,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer
Vision</em>.   Springer, 2020, pp. 38–53.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
L. Yang, H. Li, Q. Wu, F. Meng, and K. N. Ngan, “Mono is enough: Instance
segmentation from single annotated sample,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">2020 IEEE International
Conference on Visual Communications and Image Processing (VCIP)</em>.   IEEE, 2020, pp. 120–123.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
F. Meng, L. Guo, Q. Wu, and H. Li, “A new deep segmentation quality assessment
network for refining bounding box based segmentation,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>,
vol. 7, pp. 59 514–59 523, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Y. Yang, F. Meng, H. Li, K. N. Ngan, and Q. Wu, “A new few-shot segmentation
network based on class representation,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">2019 IEEE Visual
Communications and Image Processing (VCIP)</em>.   IEEE, 2019, pp. 1–4.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
X. Xu, F. Meng, H. Li, Q. Wu, Y. Yang, and S. Chen, “Bounding box based
annotation generation for semantic segmentation by boundary detection,” in
<em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">2019 International Symposium on Intelligent Signal Processing and
Communication Systems (ISPACS)</em>.   IEEE,
2019, pp. 1–2.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
C. Shang, Q. Wu, F. Meng, and L. Xu, “Instance segmentation by learning deep
feature in embedding space,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on
Image Processing (ICIP)</em>.   IEEE, 2019,
pp. 2444–2448.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
K. Luo, F. Meng, Q. Wu, and H. Li, “Weakly supervised semantic segmentation by
multiple group cosegmentation,” in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">2018 IEEE Visual Communications and
Image Processing (VCIP)</em>.   IEEE, 2018,
pp. 1–4.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
H. Shi, H. Li, F. Meng, and Q. Wu, “Key-word-aware network for referring
expression image segmentation,” in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European
Conference on Computer Vision (ECCV)</em>, 2018, pp. 38–54.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
H. Wei, Q. Wu, H. Li, K. N. Ngan, H. Li, and F. Meng, “Single image dehazing
via artificial multiple shots and multidimensional context,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">2020
IEEE International Conference on Image Processing (ICIP)</em>.   IEEE, 2020, pp. 1023–1027.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
H. Li, Q. Wu, K. N. Ngan, H. Li, and F. Meng, “Region adaptive two-shot
network for single image dehazing,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">2020 IEEE International
Conference on Multimedia and Expo (ICME)</em>.   IEEE, 2020, pp. 1–6.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Q. Wu, L. Wang, K. N. Ngan, H. Li, F. Meng, and L. Xu, “Subjective and
objective de-raining quality assessment towards authentic rain image,”
<em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Circuits and Systems for Video Technology</em>,
vol. 30, no. 11, pp. 3883–3897, 2020.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
H. Li, Q. Wu, H. Wei, K. N. Ngan, H. Li, F. Meng, and L. Xu, “Haze-robust
image understanding via context-aware deep feature refinement,” in
<em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">2020 IEEE 22nd International Workshop on Multimedia Signal Processing
(MMSP)</em>.   IEEE, 2020, pp. 1–6.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Q. Wu, L. Chen, K. N. Ngan, H. Li, F. Meng, and L. Xu, “A unified single image
de-raining model via region adaptive coupled network,” in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">2020 IEEE
International Conference on Visual Communications and Image Processing
(VCIP)</em>.   IEEE, 2020, pp. 1–4.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
H. Luo, H. Luo, Q. Wu, K. N. Ngan, H. Li, F. Meng, and L. Xu, “Single image
deraining via multi-scale gated feature enhancement network,” in
<em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Digital TV and Wireless Multimedia Communication: 17th International
Forum, IFTC 2020, Shanghai, China, December 2, 2020: Revised Selected
Papers</em>, vol. 1390.   Springer Nature,
2021, p. 73.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Q. Wu, L. Wang, K. N. Ngan, H. Li, and F. Meng, “Beyond synthetic data: A
blind deraining quality assessment metric towards authentic rain image,” in
<em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on Image Processing (ICIP)</em>.   IEEE, 2019, pp. 2364–2368.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
H. Wei, Q. Wu, H. Li, K. N. Ngan, H. Li, F. Meng, and L. Xu, “Non-homogeneous
haze removal via artificial scene prior and bidimensional graph reasoning,”
<em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.01888</em>, 2021.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
H. Li, Q. Wu, K. N. Ngan, H. Li, and F. Meng, “Single image dehazing via
region adaptive two-shot network,” <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">IEEE MultiMedia</em>, 2021.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
T. Song, H. Li, F. Meng, Q. Wu, and J. Cai, “Letrist: Locally encoded
transform feature histogram for rotation-invariant texture classification,”
<em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on circuits and systems for video technology</em>,
vol. 28, no. 7, pp. 1565–1579, 2017.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
K. Ma, Z. Duanmu, Q. Wu, Z. Wang, H. Yong, H. Li, and L. Zhang, “Waterloo
exploration database: New challenges for image quality assessment models,”
<em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Image Processing</em>, vol. 26, no. 2, pp. 1004–1016,
2016.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
T. Song, H. Li, F. Meng, Q. Wu, B. Luo, B. Zeng, and M. Gabbouj, “Noise-robust
texture description using local contrast patterns via global measures,”
<em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Letters</em>, vol. 21, no. 1, pp. 93–96, 2013.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
K. Ma, Q. Wu, Z. Wang, Z. Duanmu, H. Yong, H. Li, and L. Zhang, “Group mad
competition-a new methodology to compare objective image quality models,” in
<em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition</em>, 2016, pp. 1664–1673.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Q. Wu, H. Li, Z. Wang, F. Meng, B. Luo, W. Li, and K. N. Ngan, “Blind image
quality assessment based on rank-order regularized regression,” <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Multimedia</em>, vol. 19, no. 11, pp. 2490–2504, 2017.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
F. Meng, H. Li, K. N. Ngan, L. Zeng, and Q. Wu, “Feature adaptive
co-segmentation by complexity awareness,” <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Image
Processing</em>, vol. 22, no. 12, pp. 4809–4824, 2013.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Q. Wu, H. Li, F. Meng, K. N. Ngan, and S. Zhu, “No reference image quality
assessment metric via multi-domain structural information and piecewise
regression,” <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Journal of Visual Communication and Image
Representation</em>, vol. 32, pp. 205–216, 2015.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Q. Wu, H. Li, K. N. Ngan, and K. Ma, “Blind image quality assessment using
local consistency aware retriever and uncertainty aware evaluator,”
<em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Circuits and Systems for Video Technology</em>,
vol. 28, no. 9, pp. 2078–2089, 2017.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
learning to align and translate,” <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1409.0473</em>,
2014.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for automatic
evaluation of machine translation,” in <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th annual
meeting of the Association for Computational Linguistics</em>, 2002, pp.
311–318.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
J. R. Curran and S. Clark, “Language independent ner using a maximum entropy
tagger,” in <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Proceedings of the seventh conference on Natural language
learning at HLT-NAACL 2003</em>, 2003, pp. 164–167.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer, “Neural
architectures for named entity recognition,” <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1603.01360</em>, 2016.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
J. Huang, C. Xiang, S. Yuan, D. Yuan, and X. Huang, “Character-aware
convolutional recurrent networks with self-attention for emotion detection on
twitter,” in <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">2019 International Joint Conference on Neural Networks
(IJCNN)</em>.   IEEE, 2019, pp. 1–8.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
R. Cowie, E. Douglas-Cowie, N. Tsapatsoulis, G. Votsis, S. Kollias, W. Fellenz,
and J. G. Taylor, “Emotion recognition in human-computer interaction,”
<em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">IEEE Signal processing magazine</em>, vol. 18, no. 1, pp. 32–80, 2001.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
D. Ghosal, N. Majumder, S. Poria, N. Chhaya, and A. Gelbukh, “Dialoguegcn: A
graph convolutional neural network for emotion recognition in conversation,”
in <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP)</em>, 2019, pp. 154–164.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
A. Agrawal, D. Batra, D. Parikh, and A. Kembhavi, “Don’t just assume; look and
answer: Overcoming priors for visual question answering,” in
<em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition</em>, 2018, pp. 4971–4980.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Z. Yang, X. He, J. Gao, L. Deng, and A. Smola, “Stacked attention networks for
image question answering,” in <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on
computer vision and pattern recognition</em>, 2016, pp. 21–29.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang,
“Bottom-up and top-down attention for image captioning and visual question
answering,” in <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition</em>, 2018, pp. 6077–6086.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
R. Cadene, C. Dancette, M. Cord, D. Parikh <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Rubi: Reducing
unimodal biases for visual question answering,” <em id="bib.bib61.2.2" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems</em>, vol. 32, pp. 841–852, 2019.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
D. Hudson and C. D. Manning, “Learning by abstraction: The neural state
machine,” <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 32,
pp. 5903–5916, 2019.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
R. Cadene, H. Ben-younes, M. Cord, and N. Thome, “Murel: Multimodal relational
reasoning for visual question answering,” in <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">2019 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR)</em>.   IEEE Computer Society, 2019, pp. 1989–1998.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
L. Peng, Y. Yang, X. Zhang, Y. Ji, H. Lu, and H. T. Shen, “Answer again:
Improving vqa with cascaded-answering model,” <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Knowledge and Data Engineering</em>, 2020.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
H. Tan and M. Bansal, “Lxmert: Learning cross-modality encoder representations
from transformers,” in <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint
Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, 2019, pp.
5100–5111.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
R. R. Selvaraju, S. Lee, Y. Shen, H. Jin, S. Ghosh, L. Heck, D. Batra, and
D. Parikh, “Taking a hint: Leveraging explanations to make vision and
language models more grounded,” in <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
International Conference on Computer Vision</em>, 2019, pp. 2591–2600.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
J. Wu and R. Mooney, “Self-critical reasoning for robust visual question
answering,” <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
vol. 32, pp. 8604–8614, 2019.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
L. Li, Z. Gan, Y. Cheng, and J. Liu, “Relation-aware graph attention network
for visual question answering,” in <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
International Conference on Computer Vision</em>, 2019, pp. 10 313–10 322.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
R. Shrestha, K. Kafle, and C. Kanan, “A negative case analysis of visual
grounding methods for vqa,” in <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics</em>, 2020, pp. 8172–8181.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
G. KV and A. Mittal, “Reducing language biases in visual question answering
with visually-grounded question encoder,” in <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV
2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part XIII 16</em>.   Springer,
2020, pp. 18–34.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Y. Hirota, N. Garcia, M. Otani, C. Chu, Y. Nakashima, I. Taniguchi, and
T. Onoye, “A picture may be worth a hundred words for visual question
answering,” <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.13445</em>, 2021.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Q. Si, Z. Lin, M. y. Zheng, P. Fu, and W. Wang, “Check it again:progressive
visual question answering via visual entailment,” in <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the 59th Annual Meeting of the Association for Computational Linguistics and
the 11th International Joint Conference on Natural Language Processing
(Volume 1: Long Papers)</em>.   Online:
Association for Computational Linguistics, Aug. 2021, pp. 4101–4110.
[Online]. Available: <a target="_blank" href="https://aclanthology.org/2021.acl-long.317" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.acl-long.317</a>

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
L. Zhang, S. Liu, D. Liu, P. Zeng, X. Li, J. Song, and L. Gao, “Rich visual
knowledge-based augmentation network for visual question answering,”
<em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>, 2020.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
S. Ramakrishnan, A. Agrawal, and S. Lee, “Overcoming language priors in visual
question answering with adversarial regularization,” <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">Advances in
Neural Information Processing Systems</em>, vol. 31, 2018.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
G. Grand and Y. Belinkov, “Adversarial regularization for visual question
answering: Strengths, shortcomings, and side effects,” <em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">NAACL HLT
2019</em>, p. 1, 2019.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
C. Clark, M. Yatskar, and L. Zettlemoyer, “Don’t take the easy way out:
Ensemble based methods for avoiding known dataset biases,” in <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>,
2019.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
C. Kervadec, G. Antipov, M. Baccouche, and C. Wolf, “Estimating semantic
structure for the vqa answer space,” <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.05726</em>,
2020.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
I. Gat, I. Schwartz, A. G. Schwing, and T. Hazan, “Removing bias in
multi-modal classifiers: Regularization by maximizing functional entropies,”
in <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 33, 2020,
pp. 3197–3208.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Y. Niu, K. Tang, H. Zhang, Z. Lu, X.-S. Hua, and J.-R. Wen, “Counterfactual
vqa: A cause-effect look at language bias,” in <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021, pp.
12 700–12 710.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
X. Han, S. Wang, C. Su, Q. Huang, and Q. Tian, “Greedy gradient
ensemble for robust visual question answering,” in <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2021.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Z. Liang, H. Hu, and J. Zhu, “Lpf: A language-prior feedback objective
function for de-biased visual question answering,” in <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the 44th International Conference on Research and Development in Information
Retrieval (SIGIR)</em>, 2021.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
D. Teney and A. van den Hengel, “Actively seeking and learning from live
data,” in <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">2019 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2019, pp. 1940–1949.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Y. Zhou, R. Ji, X. Sun, J. Su, D. Meng, Y. Gao, and C. Shen, “Plenty is
plague: Fine-grained learning for visual question answering,” <em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">IEEE
transactions on pattern analysis and machine intelligence</em>, 2019.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
L. Chen, X. Yan, J. Xiao, H. Zhang, S. Pu, and Y. Zhuang, “Counterfactual
samples synthesizing for robust visual question answering,” in
<em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, 2020, pp. 10 800–10 809.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Z. Liang, W. Jiang, H. Hu, and J. Zhu, “Learning to contrast the
counterfactual samples for robust visual question answering,” in
<em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP)</em>, 2020, pp. 3285–3292.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
D. Teney, E. Abbasnedjad, and A. van den Hengel, “Learning what makes a
difference from counterfactual examples and gradient supervision,” in
<em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK,
August 23–28, 2020, Proceedings, Part X 16</em>.   Springer, 2020, pp. 580–599.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Y. Guo, L. Nie, Z. Cheng, and Q. Tian, “Loss-rescaling vqa: Revisiting
language prior problem from a class-imbalance view,” <em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2010.16010</em>, 2020.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
T. Gokhale, P. Banerjee, C. Baral, and Y. Yang, “Mutant: A training paradigm
for out-of-distribution generalization in visual question answering,” in
<em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP)</em>, 2020, pp. 878–892.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
D. Teney, E. Abbasnejad, K. Kafle, R. Shrestha, C. Kanan, and A. van den
Hengel, “On the value of out-of-distribution testing: An example of
goodhart's law,” in <em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information
Processing Systems</em>, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin, Eds., vol. 33.   Curran
Associates, Inc., 2020, pp. 407–417. [Online]. Available:
<a target="_blank" href="https://proceedings.neurips.cc/paper/2020/file/045117b0e0a11a242b9765e79cbf113f-Paper.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2020/file/045117b0e0a11a242b9765e79cbf113f-Paper.pdf</a>

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
X. Zhu, Z. Mao, C. Liu, P. Zhang, B. Wang, and Y. Zhang, “Overcoming language
priors with self-supervised learning for visual question answering,” in
<em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Twenty-Ninth International Joint Conference on
Artificial Intelligence, IJCAI-20</em>, C. Bessiere, Ed.   International Joint Conferences on Artificial Intelligence
Organization, 2020, pp. 1083–1089.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
D. Teney, E. Abbasnejad, and A. van den Hengel, “Unshuffling data for improved
generalization in visual question answering,” in <em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE/CVF International Conference on Computer Vision</em>, 2021, pp. 1417–1427.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
M. Lao, Y. Guo, Y. Liu, and M. S. Lew, “A language prior based focal loss for
visual question answering,” in <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">2021 IEEE International Conference on
Multimedia and Expo (ICME)</em>.   IEEE,
2021, pp. 1–6.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Y. Guo, L. Nie, Z. Cheng, F. Ji, J. Zhang, and A. Del Bimbo, “Adavqa:
Overcoming language priors with adapted margin cosine loss,” in
<em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Thirtieth International Joint Conference on
Artificial Intelligence, IJCAI-21</em>, Z.-H. Zhou, Ed.   International Joint Conferences on Artificial Intelligence
Organization, 2021, pp. 708–714.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
C. Yang, S. Feng, D. Li, H. Shen, G. Wang, and B. Jiang, “Learning content and
context with language bias for visual question answering,” in <em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">2021
IEEE International Conference on Multimedia and Expo (ICME)</em>.   IEEE, 2021, pp. 1–6.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
N. Ouyang, Q. Huang, P. Li, C. Yi, B. Liu, H.-f. Leung, and Q. Li,
“Suppressing biased samples for robust vqa,” <em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Multimedia</em>, 2021.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
P. Banerjee, T. Gokhale, Y. Yang, and C. Baral, “Weaqa: Weak supervision via
captions for visual question answering,” in <em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">Findings of the
Association for Computational Linguistics: ACL-IJCNLP 2021</em>, 2021, pp.
3420–3435.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
J. Jiang, Z. Liu, Y. Liu, Z. Nan, and N. Zheng, “X-ggm: Graph generative
modeling for out-of-distribution generalization in visual question
answering,” in <em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th ACM International Conference on
Multimedia</em>, 2021, pp. 199–208.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
D. Yuan, X. Liu, Q. Wu, H. Li, F. Meng, K. N. Ngan, and L. Xu, “Empower
counterfactual thinking via contrastive learning for robust visual question
answering,” in <em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.
(ICASSP)</em>.   IEEE, 2022, p. under
review.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
Q. Cao, B. Li, X. Liang, K. Wang, and L. Lin, “Knowledge-routed visual
question reasoning: Challenges for deep representation embedding,”
<em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</em>, 2021.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi, “Ok-vqa: A visual
question answering benchmark requiring external knowledge,” in
<em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, 2019, pp. 3195–3204.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
M. Ziaeefard and F. Lécué, “Towards knowledge-augmented visual
question answering,” in <em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th International
Conference on Computational Linguistics</em>, 2020, pp. 1863–1873.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
A. K. Menon, S. Jayasumana, A. S. Rawat, H. Jain, A. Veit, and S. Kumar,
“Long-tail learning via logit adjustment,” in <em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">International
Conference on Learning Representations</em>, 2020.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
W. Wang, M. Wang, S. Wang, G. Long, L. Yao, G. Qi, and Y. Chen, “One-shot
learning for long-tail visual relation detection,” in <em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the AAAI Conference on Artificial Intelligence</em>, vol. 34, no. 07, 2020, pp.
12 225–12 232.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
W. Ouyang, X. Wang, C. Zhang, and X. Yang, “Factors in finetuning deep model
for object detection with long-tail distribution,” in <em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the IEEE conference on computer vision and pattern recognition</em>, 2016, pp.
864–873.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
Y. Zhou, Q. Hu, and Y. Wang, “Deep super-class learning for long-tail
distributed image classification,” <em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">Pattern Recognition</em>, vol. 80, pp.
118–128, 2018.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
M. A. Hernán and J. M. Robins, “Causal inference,” 2010.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
P. W. Holland, “Statistics and causal inference,” <em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">Journal of the
American statistical Association</em>, vol. 81, no. 396, pp. 945–960, 1986.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
J. Pearl, “Causal inference in statistics: An overview,” <em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">Statistics
surveys</em>, vol. 3, pp. 96–146, 2009.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
——, “Causal inference,” <em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">Causality: Objectives and Assessment</em>, pp.
39–58, 2010.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
S. L. Morgan and C. Winship, <em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">Counterfactuals and causal inference</em>.   Cambridge University Press, 2015.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
J. Pearl, M. Glymour, and N. P. Jewell, <em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">Causal inference in statistics: A
primer</em>.   John Wiley &amp; Sons, 2016.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2111.08530" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2111.08531" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2111.08531">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2111.08531" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2111.08532" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  6 20:10:55 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
