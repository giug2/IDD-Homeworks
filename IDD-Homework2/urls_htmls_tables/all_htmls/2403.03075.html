<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Detecting concrete visual tokens for Multimodal Machine Translation</title>
<!--Generated on Tue Mar  5 15:57:57 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.03075v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S1" title="1 Introduction ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S2" title="2 Related Works ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S2.SS1" title="2.1 Masking for Visual Grounding ‣ 2 Related Works ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Masking for Visual Grounding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S2.SS2" title="2.2 Token Selection for Visual Grounding ‣ 2 Related Works ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Token Selection for Visual Grounding</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3" title="3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS1" title="3.1 Detection of Concrete Tokens ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Detection of Concrete Tokens</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS1.SSS1" title="3.1.1 Detection with NLTK ‣ 3.1 Detection of Concrete Tokens ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Detection with NLTK</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS1.SSS2" title="3.1.2 Detection with MDETR ‣ 3.1 Detection of Concrete Tokens ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Detection with MDETR</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS1.SSS3" title="3.1.3 Detection with Joint Visual Grounding ‣ 3.1 Detection of Concrete Tokens ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.3 </span>Detection with Joint Visual Grounding</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS2" title="3.2 Synthetic Dataset Collation ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Synthetic Dataset Collation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS2.SSS1" title="3.2.1 Token Selection Techniques ‣ 3.2 Synthetic Dataset Collation ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Token Selection Techniques</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS3" title="3.3 GRAM Model ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>GRAM Model</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S4" title="4 Results and Discussion ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S4.SS1" title="4.1 Experimental Framework ‣ 4 Results and Discussion ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Framework</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S4.SS2" title="4.2 Results ‣ 4 Results and Discussion ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S4.SS3" title="4.3 Detection Results ‣ 4 Results and Discussion ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Detection Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S4.SS4" title="4.4 Selection Results ‣ 4 Results and Discussion ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Selection Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S4.SS5" title="4.5 Future Work ‣ 4 Results and Discussion ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S5" title="5 Conclusion ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewbox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
<li>failed: nccmath</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
</div><div class="section" id="target-section"><div id="license-tr">License: CC Zero</div><div id="watermark-tr">arXiv:2403.03075v1 [cs.CL] 05 Mar 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Detecting concrete visual tokens for Multimodal Machine Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Braeden Bowen<math alttext="{}^{\text{1}}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mi id="id1.1.m1.1.1a" xref="id1.1.m1.1.1.cmml"></mi><mtext id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1a.cmml">1</mtext></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><ci id="id1.1.m1.1.1.1a.cmml" xref="id1.1.m1.1.1.1"><mtext id="id1.1.m1.1.1.1.cmml" mathsize="70%" xref="id1.1.m1.1.1.1">1</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">{}^{\text{1}}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Vipin Vijayan<math alttext="{}^{\text{1}}" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><msup id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mi id="id2.2.m2.1.1a" xref="id2.2.m2.1.1.cmml"></mi><mtext id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1a.cmml">1</mtext></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><ci id="id2.2.m2.1.1.1a.cmml" xref="id2.2.m2.1.1.1"><mtext id="id2.2.m2.1.1.1.cmml" mathsize="70%" xref="id2.2.m2.1.1.1">1</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">{}^{\text{1}}</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Scott Grigsby<math alttext="{}^{\text{1}}" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><msup id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml"><mi id="id3.3.m3.1.1a" xref="id3.3.m3.1.1.cmml"></mi><mtext id="id3.3.m3.1.1.1" xref="id3.3.m3.1.1.1a.cmml">1</mtext></msup><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><apply id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1"><ci id="id3.3.m3.1.1.1a.cmml" xref="id3.3.m3.1.1.1"><mtext id="id3.3.m3.1.1.1.cmml" mathsize="70%" xref="id3.3.m3.1.1.1">1</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">{}^{\text{1}}</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Timothy Anderson<math alttext="{}^{\text{2}}" class="ltx_Math" display="inline" id="id4.4.m4.1"><semantics id="id4.4.m4.1a"><msup id="id4.4.m4.1.1" xref="id4.4.m4.1.1.cmml"><mi id="id4.4.m4.1.1a" xref="id4.4.m4.1.1.cmml"></mi><mtext id="id4.4.m4.1.1.1" xref="id4.4.m4.1.1.1a.cmml">2</mtext></msup><annotation-xml encoding="MathML-Content" id="id4.4.m4.1b"><apply id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1"><ci id="id4.4.m4.1.1.1a.cmml" xref="id4.4.m4.1.1.1"><mtext id="id4.4.m4.1.1.1.cmml" mathsize="70%" xref="id4.4.m4.1.1.1">2</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.1c">{}^{\text{2}}</annotation><annotation encoding="application/x-llamapun" id="id4.4.m4.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>,
Jeremy Gwinnup<math alttext="{}^{\text{2}}" class="ltx_Math" display="inline" id="id5.5.m5.1"><semantics id="id5.5.m5.1a"><msup id="id5.5.m5.1.1" xref="id5.5.m5.1.1.cmml"><mi id="id5.5.m5.1.1a" xref="id5.5.m5.1.1.cmml"></mi><mtext id="id5.5.m5.1.1.1" xref="id5.5.m5.1.1.1a.cmml">2</mtext></msup><annotation-xml encoding="MathML-Content" id="id5.5.m5.1b"><apply id="id5.5.m5.1.1.cmml" xref="id5.5.m5.1.1"><ci id="id5.5.m5.1.1.1a.cmml" xref="id5.5.m5.1.1.1"><mtext id="id5.5.m5.1.1.1.cmml" mathsize="70%" xref="id5.5.m5.1.1.1">2</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id5.5.m5.1c">{}^{\text{2}}</annotation><annotation encoding="application/x-llamapun" id="id5.5.m5.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break"/><math alttext="{}^{\text{1}}" class="ltx_Math" display="inline" id="id6.6.m6.1"><semantics id="id6.6.m6.1a"><msup id="id6.6.m6.1.1" xref="id6.6.m6.1.1.cmml"><mi id="id6.6.m6.1.1a" xref="id6.6.m6.1.1.cmml"></mi><mtext id="id6.6.m6.1.1.1" xref="id6.6.m6.1.1.1a.cmml">1</mtext></msup><annotation-xml encoding="MathML-Content" id="id6.6.m6.1b"><apply id="id6.6.m6.1.1.cmml" xref="id6.6.m6.1.1"><ci id="id6.6.m6.1.1.1a.cmml" xref="id6.6.m6.1.1.1"><mtext id="id6.6.m6.1.1.1.cmml" mathsize="70%" xref="id6.6.m6.1.1.1">1</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id6.6.m6.1c">{}^{\text{1}}</annotation><annotation encoding="application/x-llamapun" id="id6.6.m6.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>PAR Government Systems Corporation, <math alttext="{}^{\text{2}}" class="ltx_Math" display="inline" id="id7.7.m7.1"><semantics id="id7.7.m7.1a"><msup id="id7.7.m7.1.1" xref="id7.7.m7.1.1.cmml"><mi id="id7.7.m7.1.1a" xref="id7.7.m7.1.1.cmml"></mi><mtext id="id7.7.m7.1.1.1" xref="id7.7.m7.1.1.1a.cmml">2</mtext></msup><annotation-xml encoding="MathML-Content" id="id7.7.m7.1b"><apply id="id7.7.m7.1.1.cmml" xref="id7.7.m7.1.1"><ci id="id7.7.m7.1.1.1a.cmml" xref="id7.7.m7.1.1.1"><mtext id="id7.7.m7.1.1.1.cmml" mathsize="70%" xref="id7.7.m7.1.1.1">2</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id7.7.m7.1c">{}^{\text{2}}</annotation><annotation encoding="application/x-llamapun" id="id7.7.m7.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>Air Force Research Laboratory
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id10.8.id1">{braeden_bowen, vipin_vijayan, scott_grigsby}@partech.com</span>,
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id11.9.id2">{timothy.anderson.20, jeremy.gwinnup.1}@us.af.mil</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id9.2">The challenge of visual grounding and masking in multimodal machine translation (MMT) systems has encouraged varying approaches to the detection and selection of visually-grounded text tokens for masking. We introduce new methods for detection of visually and contextually relevant (concrete) tokens from source sentences, including detection with natural language processing (NLP), detection with object detection, and a joint detection-verification technique. We also introduce new methods for selection of detected tokens, including shortest <math alttext="n" class="ltx_Math" display="inline" id="id8.1.m1.1"><semantics id="id8.1.m1.1a"><mi id="id8.1.m1.1.1" xref="id8.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="id8.1.m1.1b"><ci id="id8.1.m1.1.1.cmml" xref="id8.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="id8.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="id8.1.m1.1d">italic_n</annotation></semantics></math> tokens, longest <math alttext="n" class="ltx_Math" display="inline" id="id9.2.m2.1"><semantics id="id9.2.m2.1a"><mi id="id9.2.m2.1.1" xref="id9.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="id9.2.m2.1b"><ci id="id9.2.m2.1.1.cmml" xref="id9.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="id9.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="id9.2.m2.1d">italic_n</annotation></semantics></math> tokens, and all detected concrete tokens. We utilize the GRAM MMT architecture to train models against synthetically collated multimodal datasets of source images with masked sentences, showing performance improvements and improved usage of visual context during translation tasks over the baseline model.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p ltx_align_center ltx_align_bottom" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1">Detecting concrete visual tokens for Multimodal Machine Translation</span></p>
</div>
<div class="ltx_para" id="p2">
<br class="ltx_break"/>
<p class="ltx_p" id="p2.7"><span class="ltx_text" id="p2.7.7" style="width:433.6pt;"><span class="ltx_text" id="p2.7.7.7" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p2.7.7.7.7">
<span class="ltx_tbody">
<span class="ltx_tr" id="p2.5.5.5.5.5">
<span class="ltx_td ltx_align_center" id="p2.5.5.5.5.5.5"><span class="ltx_text ltx_font_bold" id="p2.5.5.5.5.5.5.5">Braeden Bowen<math alttext="{}^{\text{1}}" class="ltx_Math" display="inline" id="p2.1.1.1.1.1.1.1.m1.1"><semantics id="p2.1.1.1.1.1.1.1.m1.1a"><msup id="p2.1.1.1.1.1.1.1.m1.1.1" xref="p2.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="p2.1.1.1.1.1.1.1.m1.1.1a" xref="p2.1.1.1.1.1.1.1.m1.1.1.cmml"></mi><mtext id="p2.1.1.1.1.1.1.1.m1.1.1.1" xref="p2.1.1.1.1.1.1.1.m1.1.1.1a.cmml">𝟏</mtext></msup><annotation-xml encoding="MathML-Content" id="p2.1.1.1.1.1.1.1.m1.1b"><apply id="p2.1.1.1.1.1.1.1.m1.1.1.cmml" xref="p2.1.1.1.1.1.1.1.m1.1.1"><ci id="p2.1.1.1.1.1.1.1.m1.1.1.1a.cmml" xref="p2.1.1.1.1.1.1.1.m1.1.1.1"><mtext id="p2.1.1.1.1.1.1.1.m1.1.1.1.cmml" mathsize="70%" xref="p2.1.1.1.1.1.1.1.m1.1.1.1">𝟏</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.1.1.1.1.1.1.1.m1.1c">{}^{\text{1}}</annotation><annotation encoding="application/x-llamapun" id="p2.1.1.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Vipin Vijayan<math alttext="{}^{\text{1}}" class="ltx_Math" display="inline" id="p2.2.2.2.2.2.2.2.m2.1"><semantics id="p2.2.2.2.2.2.2.2.m2.1a"><msup id="p2.2.2.2.2.2.2.2.m2.1.1" xref="p2.2.2.2.2.2.2.2.m2.1.1.cmml"><mi id="p2.2.2.2.2.2.2.2.m2.1.1a" xref="p2.2.2.2.2.2.2.2.m2.1.1.cmml"></mi><mtext id="p2.2.2.2.2.2.2.2.m2.1.1.1" xref="p2.2.2.2.2.2.2.2.m2.1.1.1a.cmml">𝟏</mtext></msup><annotation-xml encoding="MathML-Content" id="p2.2.2.2.2.2.2.2.m2.1b"><apply id="p2.2.2.2.2.2.2.2.m2.1.1.cmml" xref="p2.2.2.2.2.2.2.2.m2.1.1"><ci id="p2.2.2.2.2.2.2.2.m2.1.1.1a.cmml" xref="p2.2.2.2.2.2.2.2.m2.1.1.1"><mtext id="p2.2.2.2.2.2.2.2.m2.1.1.1.cmml" mathsize="70%" xref="p2.2.2.2.2.2.2.2.m2.1.1.1">𝟏</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.2.2.2.2.2.2.2.m2.1c">{}^{\text{1}}</annotation><annotation encoding="application/x-llamapun" id="p2.2.2.2.2.2.2.2.m2.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Scott Grigsby<math alttext="{}^{\text{1}}" class="ltx_Math" display="inline" id="p2.3.3.3.3.3.3.3.m3.1"><semantics id="p2.3.3.3.3.3.3.3.m3.1a"><msup id="p2.3.3.3.3.3.3.3.m3.1.1" xref="p2.3.3.3.3.3.3.3.m3.1.1.cmml"><mi id="p2.3.3.3.3.3.3.3.m3.1.1a" xref="p2.3.3.3.3.3.3.3.m3.1.1.cmml"></mi><mtext id="p2.3.3.3.3.3.3.3.m3.1.1.1" xref="p2.3.3.3.3.3.3.3.m3.1.1.1a.cmml">𝟏</mtext></msup><annotation-xml encoding="MathML-Content" id="p2.3.3.3.3.3.3.3.m3.1b"><apply id="p2.3.3.3.3.3.3.3.m3.1.1.cmml" xref="p2.3.3.3.3.3.3.3.m3.1.1"><ci id="p2.3.3.3.3.3.3.3.m3.1.1.1a.cmml" xref="p2.3.3.3.3.3.3.3.m3.1.1.1"><mtext id="p2.3.3.3.3.3.3.3.m3.1.1.1.cmml" mathsize="70%" xref="p2.3.3.3.3.3.3.3.m3.1.1.1">𝟏</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.3.3.3.3.3.3.3.m3.1c">{}^{\text{1}}</annotation><annotation encoding="application/x-llamapun" id="p2.3.3.3.3.3.3.3.m3.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Timothy Anderson<math alttext="{}^{\text{2}}" class="ltx_Math" display="inline" id="p2.4.4.4.4.4.4.4.m4.1"><semantics id="p2.4.4.4.4.4.4.4.m4.1a"><msup id="p2.4.4.4.4.4.4.4.m4.1.1" xref="p2.4.4.4.4.4.4.4.m4.1.1.cmml"><mi id="p2.4.4.4.4.4.4.4.m4.1.1a" xref="p2.4.4.4.4.4.4.4.m4.1.1.cmml"></mi><mtext id="p2.4.4.4.4.4.4.4.m4.1.1.1" xref="p2.4.4.4.4.4.4.4.m4.1.1.1a.cmml">𝟐</mtext></msup><annotation-xml encoding="MathML-Content" id="p2.4.4.4.4.4.4.4.m4.1b"><apply id="p2.4.4.4.4.4.4.4.m4.1.1.cmml" xref="p2.4.4.4.4.4.4.4.m4.1.1"><ci id="p2.4.4.4.4.4.4.4.m4.1.1.1a.cmml" xref="p2.4.4.4.4.4.4.4.m4.1.1.1"><mtext id="p2.4.4.4.4.4.4.4.m4.1.1.1.cmml" mathsize="70%" xref="p2.4.4.4.4.4.4.4.m4.1.1.1">𝟐</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.4.4.4.4.4.4.4.m4.1c">{}^{\text{2}}</annotation><annotation encoding="application/x-llamapun" id="p2.4.4.4.4.4.4.4.m4.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>,
Jeremy Gwinnup<math alttext="{}^{\text{2}}" class="ltx_Math" display="inline" id="p2.5.5.5.5.5.5.5.m5.1"><semantics id="p2.5.5.5.5.5.5.5.m5.1a"><msup id="p2.5.5.5.5.5.5.5.m5.1.1" xref="p2.5.5.5.5.5.5.5.m5.1.1.cmml"><mi id="p2.5.5.5.5.5.5.5.m5.1.1a" xref="p2.5.5.5.5.5.5.5.m5.1.1.cmml"></mi><mtext id="p2.5.5.5.5.5.5.5.m5.1.1.1" xref="p2.5.5.5.5.5.5.5.m5.1.1.1a.cmml">𝟐</mtext></msup><annotation-xml encoding="MathML-Content" id="p2.5.5.5.5.5.5.5.m5.1b"><apply id="p2.5.5.5.5.5.5.5.m5.1.1.cmml" xref="p2.5.5.5.5.5.5.5.m5.1.1"><ci id="p2.5.5.5.5.5.5.5.m5.1.1.1a.cmml" xref="p2.5.5.5.5.5.5.5.m5.1.1.1"><mtext id="p2.5.5.5.5.5.5.5.m5.1.1.1.cmml" mathsize="70%" xref="p2.5.5.5.5.5.5.5.m5.1.1.1">𝟐</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.5.5.5.5.5.5.5.m5.1c">{}^{\text{2}}</annotation><annotation encoding="application/x-llamapun" id="p2.5.5.5.5.5.5.5.m5.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></span></span>
<span class="ltx_tr" id="p2.7.7.7.7.7">
<span class="ltx_td ltx_align_center" id="p2.7.7.7.7.7.2"><math alttext="{}^{\text{1}}" class="ltx_Math" display="inline" id="p2.6.6.6.6.6.1.m1.1"><semantics id="p2.6.6.6.6.6.1.m1.1a"><msup id="p2.6.6.6.6.6.1.m1.1.1" xref="p2.6.6.6.6.6.1.m1.1.1.cmml"><mi id="p2.6.6.6.6.6.1.m1.1.1a" xref="p2.6.6.6.6.6.1.m1.1.1.cmml"></mi><mtext id="p2.6.6.6.6.6.1.m1.1.1.1" xref="p2.6.6.6.6.6.1.m1.1.1.1a.cmml">1</mtext></msup><annotation-xml encoding="MathML-Content" id="p2.6.6.6.6.6.1.m1.1b"><apply id="p2.6.6.6.6.6.1.m1.1.1.cmml" xref="p2.6.6.6.6.6.1.m1.1.1"><ci id="p2.6.6.6.6.6.1.m1.1.1.1a.cmml" xref="p2.6.6.6.6.6.1.m1.1.1.1"><mtext id="p2.6.6.6.6.6.1.m1.1.1.1.cmml" mathsize="70%" xref="p2.6.6.6.6.6.1.m1.1.1.1">1</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.6.6.6.6.6.1.m1.1c">{}^{\text{1}}</annotation><annotation encoding="application/x-llamapun" id="p2.6.6.6.6.6.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>PAR Government Systems Corporation, <math alttext="{}^{\text{2}}" class="ltx_Math" display="inline" id="p2.7.7.7.7.7.2.m2.1"><semantics id="p2.7.7.7.7.7.2.m2.1a"><msup id="p2.7.7.7.7.7.2.m2.1.1" xref="p2.7.7.7.7.7.2.m2.1.1.cmml"><mi id="p2.7.7.7.7.7.2.m2.1.1a" xref="p2.7.7.7.7.7.2.m2.1.1.cmml"></mi><mtext id="p2.7.7.7.7.7.2.m2.1.1.1" xref="p2.7.7.7.7.7.2.m2.1.1.1a.cmml">2</mtext></msup><annotation-xml encoding="MathML-Content" id="p2.7.7.7.7.7.2.m2.1b"><apply id="p2.7.7.7.7.7.2.m2.1.1.cmml" xref="p2.7.7.7.7.7.2.m2.1.1"><ci id="p2.7.7.7.7.7.2.m2.1.1.1a.cmml" xref="p2.7.7.7.7.7.2.m2.1.1.1"><mtext id="p2.7.7.7.7.7.2.m2.1.1.1.cmml" mathsize="70%" xref="p2.7.7.7.7.7.2.m2.1.1.1">2</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="p2.7.7.7.7.7.2.m2.1c">{}^{\text{2}}</annotation><annotation encoding="application/x-llamapun" id="p2.7.7.7.7.7.2.m2.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>Air Force Research Laboratory</span></span>
<span class="ltx_tr" id="p2.7.7.7.7.8.1">
<span class="ltx_td ltx_align_center" id="p2.7.7.7.7.8.1.1"><span class="ltx_text ltx_font_typewriter" id="p2.7.7.7.7.8.1.1.1">{braeden_bowen, vipin_vijayan, scott_grigsby}@partech.com</span>,</span></span>
<span class="ltx_tr" id="p2.7.7.7.7.9.2">
<span class="ltx_td ltx_align_center" id="p2.7.7.7.7.9.2.1"><span class="ltx_text ltx_font_typewriter" id="p2.7.7.7.7.9.2.1.1">{timothy.anderson.20, jeremy.gwinnup.1}@us.af.mil</span></span></span>
</span>
</span></span> </span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The challenge of multimodal machine translation (MMT) is to design a system that automatically translates text from one language to another while utilizing other modalities (e.g., image, video, audio) as inputs to assist in translation <cite class="ltx_cite ltx_citemacro_cite">Caglayan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib1" title="">2016</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Prior work has shown that translation ambiguities and missing textual information can be supplied by contextually-relevant images, aiding in multilingual translation <cite class="ltx_cite ltx_citemacro_cite">Lala and Specia (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib11" title="">2018</a>); Caglayan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib2" title="">2019</a>); Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib23" title="">2021</a>)</cite>. For example, the noun “bank” is ambiguous and contextually dependent in English (“financial institution” or “river edge”) but unambiguous in French (“<span class="ltx_text ltx_font_italic" id="S1.p2.1.1">banque</span>” or “<span class="ltx_text ltx_font_italic" id="S1.p2.1.2">rive</span>”) <cite class="ltx_cite ltx_citemacro_citep">(Futeral et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib7" title="">2023</a>)</cite>. The hypothesis for MMT research is that these translation ambiguities can be resolved with the inclusion of image context.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In practice, not every sentence has semantic ambiguities, missing information, or relevant visual context; it is therefore beneficial to ensure that ambiguous text is visually and contextually relevant to an associated image <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib25" title="">2018</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To enforce reliance on image context for translation tasks, some MMT models mask tokens from text inputs <cite class="ltx_cite ltx_citemacro_citep">(Caglayan et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib2" title="">2019</a>; Sato et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib16" title="">2023</a>)</cite>. While most early masking iterations randomly selected tokens for masking, more recent efforts have sought to mask tokens based on contextual relevance to a given image <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib18" title="">2020</a>)</cite>, increasing the usefulness of the image in resolving ambiguity. Still, those methods tend to ignore deterministic selection of relevant tokens, opting to randomly select from a pool of viable tokens.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">While these approaches have displayed performance improvements over text-only and random masking models, these methods generally do not take into account the relevance of a masked token. Therefore, we hypothesize that more intentional selection and masking of <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">concrete</span> (i.e., visually and contextually relevant) text tokens, will improve visual grounding and increase model usage of multimodal context.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In order to select visually and contextually relevant tokens, we explore a combination of natural language processing (NLP) techniques and object detection models and examine deterministic methods for selection of tokens from the available detections.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Using these techniques, we collate multimodal datasets based on the Multi30k dataset <cite class="ltx_cite ltx_citemacro_citep">(Elliott et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib5" title="">2016</a>)</cite>; the resulting datasets are triplets of source sentences with masked concrete tokens, unmasked target sentences, and associated images.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.2">When masking concrete text tokens from source sentences, we find improvements in both usage of visual information in translation and in performance on evaluation challenges, including CoMMuTE scores of up to <math alttext="0.67" class="ltx_Math" display="inline" id="S1.p8.1.m1.1"><semantics id="S1.p8.1.m1.1a"><mn id="S1.p8.1.m1.1.1" xref="S1.p8.1.m1.1.1.cmml">0.67</mn><annotation-xml encoding="MathML-Content" id="S1.p8.1.m1.1b"><cn id="S1.p8.1.m1.1.1.cmml" type="float" xref="S1.p8.1.m1.1.1">0.67</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.1.m1.1c">0.67</annotation><annotation encoding="application/x-llamapun" id="S1.p8.1.m1.1d">0.67</annotation></semantics></math> and BLEU scores of up to <math alttext="46.2" class="ltx_Math" display="inline" id="S1.p8.2.m2.1"><semantics id="S1.p8.2.m2.1a"><mn id="S1.p8.2.m2.1.1" xref="S1.p8.2.m2.1.1.cmml">46.2</mn><annotation-xml encoding="MathML-Content" id="S1.p8.2.m2.1b"><cn id="S1.p8.2.m2.1.1.cmml" type="float" xref="S1.p8.2.m2.1.1">46.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p8.2.m2.1c">46.2</annotation><annotation encoding="application/x-llamapun" id="S1.p8.2.m2.1d">46.2</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="301" id="S1.F1.g1" src="x1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Multi30k source pairs (image, <span class="ltx_text ltx_font_bold" id="S1.F1.12.1">SRC</span>) with results from each detection technique (<span class="ltx_text ltx_font_bold" id="S1.F1.13.2">DT</span>) and an example masked source text (<span class="ltx_text ltx_font_bold" id="S1.F1.14.3">MSK</span>). <span class="ltx_text ltx_font_bold" id="S1.F1.15.4">DT1</span> represents the <span class="ltx_text ltx_font_italic" id="S1.F1.16.5">NLTK</span> technique; <span class="ltx_text ltx_font_bold" id="S1.F1.17.6">DT2</span> represents the <span class="ltx_text ltx_font_italic" id="S1.F1.18.7">MDETR Detection</span> technique; <span class="ltx_text ltx_font_bold" id="S1.F1.19.8">DT3</span> represents the <span class="ltx_text ltx_font_italic" id="S1.F1.20.9">Joint Detection</span> technique. The masked sentence <span class="ltx_text ltx_font_bold" id="S1.F1.21.10">MSK</span> represents a possible masked sentence based on the bold token in the <span class="ltx_text ltx_font_bold" id="S1.F1.22.11">DT3</span> detections.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Masking for Visual Grounding</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">In a text-only modality, <cite class="ltx_cite ltx_citemacro_citet">Devlin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib4" title="">2019</a>)</cite> randomly masked text tokens during pre-training of a bidirectional transformer encoder-decoder and found performance improvements against other text-only models.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Zhou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib25" title="">2018</a>)</cite> utilized jointly-encoded unmasked text and image embeddings to visually ground entire source sentences to images. Using a visual-text attention mechanism on the embeddings, they extracted words that shared semantic context with the images.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1"><cite class="ltx_cite ltx_citemacro_citet">Ive et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib8" title="">2019</a>)</cite> combined these approaches, randomly <span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.1">and</span> manually masking ambiguous and gender-neutral words from source texts to force their MMT model to utilize visual information on evaluation tasks. This work showed that the model was able to use image context to recover from missing, inaccurate, or ambiguous textual context.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1"><cite class="ltx_cite ltx_citemacro_citet">Caglayan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib2" title="">2019</a>)</cite> used image descriptions from the Flicker30k-Entities dataset <cite class="ltx_cite ltx_citemacro_cite">Plummer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib14" title="">2015</a>)</cite> to dynamically mask <span class="ltx_text ltx_font_italic" id="S2.SS1.p4.1.1">visually depictable entities</span> and color descriptors from source sentences, but noted a degradation in performance on the Multi30k test sets <cite class="ltx_cite ltx_citemacro_cite">Elliott et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib5" title="">2016</a>)</cite>. In contrast, <cite class="ltx_cite ltx_citemacro_citet">Wang and Xiong (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib22" title="">2021</a>)</cite> found that masking <span class="ltx_text ltx_font_italic" id="S2.SS1.p4.1.2">irrelevant</span> objects improved performance on MMT evaluation tasks, suggesting that state-of-the-art MMT models are ineffectively utilizing visual information.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">A meta-analysis by <cite class="ltx_cite ltx_citemacro_citet">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib23" title="">2021</a>)</cite> found that many reported improvements in MMT performance are the result of regularization effects, not model interpolation of multimodal features; similarly, <cite class="ltx_cite ltx_citemacro_citet">Zhuang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib26" title="">2023</a>)</cite> found that while visual grounding can improve performance in word learning, these improvements are only marginal. However, they also found that training sets with less textual information and fewer direct co-occurrences of visual words more effectively utilize visual information, suggesting that the relationship between text and image context is still viable.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Token Selection for Visual Grounding</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">In practice, many sentences have more than one visually grounded token; in these cases, available tokens must be dynamically selected for masking. The standard method is to randomly select viable tokens <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib4" title="">2019</a>)</cite>; however, recent work in masked language modeling (MLM) has shown that informed selection of masked tokens may improve performance <cite class="ltx_cite ltx_citemacro_citep">(Sato et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib16" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Other work has given consideration to the length of source segments in text masking <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib24" title="">2023</a>)</cite> and to the number of tokens selected <cite class="ltx_cite ltx_citemacro_citep">(Joshi et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib9" title="">2020</a>)</cite>, but little work has been done to select tokens deterministically (e.g., by token length).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We perform improved visual grounding by detecting concrete tokens in source sentences. We explore three detection techniques to identify concrete text tokens (Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS1" title="3.1 Detection of Concrete Tokens ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3.1</span></a>) and four selection techniques to appropriately select the identified concrete text tokens (Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS2.SSS1" title="3.2.1 Token Selection Techniques ‣ 3.2 Synthetic Dataset Collation ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>). We then collate permutations of synthetic MMT datasets by masking the selected concrete tokens from source sentences and aligning each sentence with its original dataset image pair. We then train an MMT model (Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS3" title="3.3 GRAM Model ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3.3</span></a>) on these datasets, expanding on work by <cite class="ltx_cite ltx_citemacro_citet">Vijayan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib21" title="">2024</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Caglayan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib2" title="">2019</a>)</cite>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Detection of Concrete Tokens</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">As <cite class="ltx_cite ltx_citemacro_citet">Caglayan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib2" title="">2019</a>)</cite> found, masking visually relevant objects from a source text can force the model to utilize image context to fill in the artificially-created gap in lexical/semantic understanding. We hypothesize that for a given text-image pair, the masking of text tokens that are directly relevant to the image (i.e., “concrete” tokens), will improve visual grounding, increasing model correlation of image inputs during downstream translation tasks.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">We present three techniques for detection of concrete tokens: NLP with NLTK (Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS1.SSS1" title="3.1.1 Detection with NLTK ‣ 3.1 Detection of Concrete Tokens ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3.1.1</span></a>), object detection with MDETR (Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS1.SSS2" title="3.1.2 Detection with MDETR ‣ 3.1 Detection of Concrete Tokens ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3.1.2</span></a>), and joint NLTK/MDETR detection and grounding (Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS1.SSS3" title="3.1.3 Detection with Joint Visual Grounding ‣ 3.1 Detection of Concrete Tokens ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3.1.3</span></a>). While techniques one and two respectively use text and image context, method three uses contextual information from both modalities to make decisions about which text tokens are concrete.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Detection with NLTK</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">The first concrete token detection approach is to parse sentences for nouns and noun phrases that are likely to represent visual context. By masking tokens that are critical to comprehension and translation of the text, we can encourage the model to learn with visual context.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1">The Natural Language Toolkit (NLTK) <cite class="ltx_cite ltx_citemacro_cite">Loper and Bird (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib12" title="">2002</a>)</cite> includes the WordNet corpus <cite class="ltx_cite ltx_citemacro_cite">Fellbaum and Miller (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib6" title="">1998</a>)</cite>, an English-language lexical database that provides structured relationships between cognitive synonyms (“<span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p2.1.1">synsets</span>”) for nouns, verbs, adjectives, and adverbs. Specifically, WordNet defines a directed acyclic graph (DAG) for each of these parts of speech (POS), containing synonyms, troponyms, antonyms, and meronyms (Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.F2" title="Figure 2 ‣ 3.1.1 Detection with NLTK ‣ 3.1 Detection of Concrete Tokens ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>). Critically, these relational graphs establish affiliations between synsets and hypernyms: that is, English words, their definitions, and their related parent categories.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="414" id="S3.F2.g1" src="x2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An example hypernym graph. The original token, <span class="ltx_text ltx_font_typewriter" id="S3.F2.2.1">sedan</span>, its three synset entries (labeled in blue), and its associated concrete hypernyms (labeled in red).</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.1">Starting with specific synonyms and troponyms (e.g., “<span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS1.p3.1.1">sedan</span>”, “<span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS1.p3.1.2">hatchback</span>”, “<span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS1.p3.1.3">SUV</span>”) and traversing the DAG upwards, WordNet collapses definitions and synsets into their associated hypernym classes (e.g., “<span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS1.p3.1.4">car</span>”, “<span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS1.p3.1.5">vehicle</span>”) until it reaches a root hypernym (e.g., “<span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS1.p3.1.6">physical_entity</span>”, “<span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS1.p3.1.7">entity</span>”). Using recursive graph traversal, we can select any node in the DAG and parse its hypernyms upward until we reach either a root hypernym or a parent hypernym on which we can base an estimate of the root hypernym (e.g., “<span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS1.p3.1.8">object</span>” generally maps to “<span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS1.p3.1.9">physical_entity</span>”).</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p4">
<p class="ltx_p" id="S3.SS1.SSS1.p4.1">Given that there exists only a small cluster of root and high-level parent hypernyms for nouns in WordNet, we can classify the hypernym DAG of any noun or noun phrase as “<span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.p4.1.1">concrete</span>” or “<span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.p4.1.2">abstract</span>” based on these high-level hypernyms (Table <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#A0.T3" title="Table 3 ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p5">
<p class="ltx_p" id="S3.SS1.SSS1.p5.1">While this method provides a simple concrete/abstract classifier for text tokens, it introduces additional complications. Although most DAG nodes have multiple child hyponyms (e.g., “<span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS1.p5.1.1">car</span>” may have “<span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS1.p5.1.2">sedan</span>” and “<span class="ltx_text ltx_font_typewriter" id="S3.SS1.SSS1.p5.1.3">hatchback</span>”), some have multiple cognitive synonyms, as English words often have multiple equally likely definitions. For a given node, each of its “definitions” will appear as an entry into its synset; for example, the English noun “link” has nine values in its WordNet synset, ranging from “<span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p5.1.4">URL</span>” to “<span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p5.1.5">channel for communication</span>” to “<span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p5.1.6">element of a chain</span>.” These varied definitions may branch to different root hypernyms, impacting the classification based on which definition is chosen (Table <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#A0.T3" title="Table 3 ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p6">
<p class="ltx_p" id="S3.SS1.SSS1.p6.1">To compensate, we consider each entry in a word’s synset and extract a ratio of concrete/abstract definitions, which more comprehensively projects a token’s likelihood of being concrete. We perform recursive graph traversal for each entry and retain the percent of concrete entries as a “concreteness score.” To then classify the original word as abstract or concrete, we establish a threshold of <math alttext="33\%" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p6.1.m1.1"><semantics id="S3.SS1.SSS1.p6.1.m1.1a"><mrow id="S3.SS1.SSS1.p6.1.m1.1.1" xref="S3.SS1.SSS1.p6.1.m1.1.1.cmml"><mn id="S3.SS1.SSS1.p6.1.m1.1.1.2" xref="S3.SS1.SSS1.p6.1.m1.1.1.2.cmml">33</mn><mo id="S3.SS1.SSS1.p6.1.m1.1.1.1" xref="S3.SS1.SSS1.p6.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p6.1.m1.1b"><apply id="S3.SS1.SSS1.p6.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p6.1.m1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS1.p6.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p6.1.m1.1.1.1">percent</csymbol><cn id="S3.SS1.SSS1.p6.1.m1.1.1.2.cmml" type="integer" xref="S3.SS1.SSS1.p6.1.m1.1.1.2">33</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p6.1.m1.1c">33\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p6.1.m1.1d">33 %</annotation></semantics></math> likelihood and only accept words above that concreteness score.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Detection with MDETR</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">While the NLTK approach can quickly and efficiently select concrete tokens from a sentence, it incorrectly assumes that every concrete token in the sentence is relevant to its associated image. Contextually linking an irrelevant concrete token to a given image could negatively impact model performance, especially if the token has high commonality in a dataset. As a second approach to concrete token detection, we utilize an object detection model to select concrete tokens. Rather than relying solely on the text processing for detection, we inspect the image itself for object classes relevant to the text.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.4">For this approach, we use MDETR <cite class="ltx_cite ltx_citemacro_cite">Kamath et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib10" title="">2021</a>)</cite>, an end-to-end object detection model. Rather than relying exclusively on pre-defined object classes, MDETR uses NLP techniques alongside a pre-trained detection model <cite class="ltx_cite ltx_citemacro_cite">Carion et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib3" title="">2020</a>)</cite> to perform object detection and image classification based on the input tokens. Given a text-image pair (Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.F3" title="Figure 3 ‣ 3.1.2 Detection with MDETR ‣ 3.1 Detection of Concrete Tokens ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>), the model assigns each text token an object classification and assigns it confidence score and bounding box within the image. To maximize the number of detectable tokens, we pass an entire Multi30k sentence into the MDETR model and filter out detections with low confidence scores, retaining only the tokens with a high confidence of correlation to the image. While <cite class="ltx_cite ltx_citemacro_citet">Kamath et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib10" title="">2021</a>)</cite> filter all outputs with confidence less than <math alttext="0.7" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p2.1.m1.1"><semantics id="S3.SS1.SSS2.p2.1.m1.1a"><mn id="S3.SS1.SSS2.p2.1.m1.1.1" xref="S3.SS1.SSS2.p2.1.m1.1.1.cmml">0.7</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.1.m1.1b"><cn id="S3.SS1.SSS2.p2.1.m1.1.1.cmml" type="float" xref="S3.SS1.SSS2.p2.1.m1.1.1">0.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.1.m1.1c">0.7</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p2.1.m1.1d">0.7</annotation></semantics></math>, we filter at <math alttext="0.85" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p2.2.m2.1"><semantics id="S3.SS1.SSS2.p2.2.m2.1a"><mn id="S3.SS1.SSS2.p2.2.m2.1.1" xref="S3.SS1.SSS2.p2.2.m2.1.1.cmml">0.85</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.2.m2.1b"><cn id="S3.SS1.SSS2.p2.2.m2.1.1.cmml" type="float" xref="S3.SS1.SSS2.p2.2.m2.1.1">0.85</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.2.m2.1c">0.85</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p2.2.m2.1d">0.85</annotation></semantics></math>; after analyzing performance at threshold increments between <math alttext="0.5" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p2.3.m3.1"><semantics id="S3.SS1.SSS2.p2.3.m3.1a"><mn id="S3.SS1.SSS2.p2.3.m3.1.1" xref="S3.SS1.SSS2.p2.3.m3.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.3.m3.1b"><cn id="S3.SS1.SSS2.p2.3.m3.1.1.cmml" type="float" xref="S3.SS1.SSS2.p2.3.m3.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.3.m3.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p2.3.m3.1d">0.5</annotation></semantics></math> and <math alttext="0.95" class="ltx_Math" display="inline" id="S3.SS1.SSS2.p2.4.m4.1"><semantics id="S3.SS1.SSS2.p2.4.m4.1a"><mn id="S3.SS1.SSS2.p2.4.m4.1.1" xref="S3.SS1.SSS2.p2.4.m4.1.1.cmml">0.95</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.4.m4.1b"><cn id="S3.SS1.SSS2.p2.4.m4.1.1.cmml" type="float" xref="S3.SS1.SSS2.p2.4.m4.1.1">0.95</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.4.m4.1c">0.95</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS2.p2.4.m4.1d">0.95</annotation></semantics></math>, we found that this threshold ensured the most balanced confidence in image objects.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="978" id="S3.F3.g1" src="x3.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Multi30k source pair (image, <span class="ltx_text ltx_font_bold" id="S3.F3.5.1">SRC</span>) with results from the MDETR (<span class="ltx_text ltx_font_bold" id="S3.F3.6.2">DT2</span>, top image) and Joint (<span class="ltx_text ltx_font_bold" id="S3.F3.7.3">DT3</span>, bottom image) detection techniques. MDETR query strings, bounding boxes, and confidence scores are shown. In this example, supplying the entire source sentence as text input to the MDETR object detection model incorrectly identifies the peppers being cooked, while querying only the word “<span class="ltx_text ltx_font_italic" id="S3.F3.8.4">pepper</span>” more closely identifies the region containing the query.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Detection with Joint Visual Grounding</h4>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.1">While the MDETR technique is less likely than the NLTK technique to improperly select text tokens as visually-grounded, the pre-trained MDETR model will always attempt to match text tokens with a bounding box in the image, often resulting in outputs with high confidence but incorrect alignment. In practice, providing extended textual context (i.e., entire captions or sentences) further exacerbates this problem (Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.F3" title="Figure 3 ‣ 3.1.2 Detection with MDETR ‣ 3.1 Detection of Concrete Tokens ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p2">
<p class="ltx_p" id="S3.SS1.SSS3.p2.1">Therefore, we are left with two techniques with contrasting weaknesses: NLTK ignores image context, and MDETR misinterprets textual context. To mitigate these issues, we present a conjoined detection technique that “verifies” the presence of NLTK-detected concrete tokens within an image using MDETR, ensuring that concrete tokens are visually grounded in the image.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p3">
<p class="ltx_p" id="S3.SS1.SSS3.p3.1">Like the MDETR technique, the joint technique parses text-image pairs (unlike the NLTK technique, which is image-agnostic). The source sentence is first processed by the NLTK technique, which returns the noun and noun phrase tokens that met or surpassed the concrete threshold. Each of those tokens is paired with a copy of the source image and passed into the MDETR technique, which performs object detection and filters out all tokens whose resulting confidence is below the confidence threshold. This simultaneously reduces the probability of incorrect alignment by the object detection model and ensures that text tokens are visually grounded, resulting in a set of linguistically concrete and visually-grounded text tokens with high probability of relevance to the source image. Masking these explicitly-relevant tokens will force model reliance on image context.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="630" id="S3.F4.g1" src="x4.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>GRAM model architecture from <cite class="ltx_cite ltx_citemacro_citet">Vijayan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib21" title="">2024</a>)</cite>.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Synthetic Dataset Collation</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Because most current work in MMT focuses on the Multi30k dataset <cite class="ltx_cite ltx_citemacro_citep">(Elliott et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib5" title="">2016</a>)</cite>, an image-caption dataset consisting of 30,014 images with English sentences and corresponding multilingual translations, we collate synthetic datasets of masked sentence-image pairs from Multi30k.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">We use each detection technique (Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS1" title="3.1 Detection of Concrete Tokens ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3.1</span></a>) to detect concrete tokens and align them to their original dataset image. From these masked sentence-image pairs, we collate a series of MMT datasets in which a maximum of two concrete tokens are masked from each sentence and associated with the relevant image from the original dataset, resulting in training and validation sets that are at most twice as large as the original Multi30k sets.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Token Selection Techniques</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.2">During the dataset collation process, a single sentence may have more than <math alttext="n" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.1.m1.1"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mi id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><ci id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.1.m1.1d">italic_n</annotation></semantics></math>=<math alttext="2" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.2.m2.1"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><mn id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><cn id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS2.SSS1.p1.2.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">2</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.2.m2.1d">2</annotation></semantics></math> available concrete tokens; in this case, additional consideration must be given to which tokens are selected for inclusion in the dataset. The standard method has generally been to randomly select from the available tokens <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib4" title="">2019</a>)</cite>, but recent work in masked language modeling (MLM) has shown that more informed selection of masked tokens may actually improve performance <cite class="ltx_cite ltx_citemacro_citep">(Sato et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib16" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.4">To examine this, we implement two deterministic token selection techniques, selecting the <math alttext="n" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.1.m1.1"><semantics id="S3.SS2.SSS1.p2.1.m1.1a"><mi id="S3.SS2.SSS1.p2.1.m1.1.1" xref="S3.SS2.SSS1.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.1.m1.1b"><ci id="S3.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p2.1.m1.1d">italic_n</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p2.4.1">longest</span> and <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p2.4.2">shortest</span> tokens (by number of characters) respectively for each sentence. We compare these techniques to a <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p2.4.3">random</span> selection of <math alttext="n" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.2.m2.1"><semantics id="S3.SS2.SSS1.p2.2.m2.1a"><mi id="S3.SS2.SSS1.p2.2.m2.1.1" xref="S3.SS2.SSS1.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.2.m2.1b"><ci id="S3.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p2.2.m2.1d">italic_n</annotation></semantics></math> tokens and an <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p2.4.4">unrestricted</span> selection which ignores the <math alttext="n" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.3.m3.1"><semantics id="S3.SS2.SSS1.p2.3.m3.1a"><mi id="S3.SS2.SSS1.p2.3.m3.1.1" xref="S3.SS2.SSS1.p2.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.3.m3.1b"><ci id="S3.SS2.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p2.3.m3.1d">italic_n</annotation></semantics></math>=<math alttext="2" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.4.m4.1"><semantics id="S3.SS2.SSS1.p2.4.m4.1a"><mn id="S3.SS2.SSS1.p2.4.m4.1.1" xref="S3.SS2.SSS1.p2.4.m4.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.4.m4.1b"><cn id="S3.SS2.SSS1.p2.4.m4.1.1.cmml" type="integer" xref="S3.SS2.SSS1.p2.4.m4.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.4.m4.1c">2</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p2.4.m4.1d">2</annotation></semantics></math> normalization and accepts all available concrete tokens.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>GRAM Model</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">As the basis for our multimodal translation architecture, we utilize the GRAM architecture <cite class="ltx_cite ltx_citemacro_cite">Vijayan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib21" title="">2024</a>)</cite>. GRAM modifies the FAIR WMT19 <cite class="ltx_cite ltx_citemacro_cite">Ng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib13" title="">2019</a>)</cite> text-only model, an encoder/decoder-based transformer architecture <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib20" title="">2017</a>)</cite>, by adding additional multimodal components (Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.F4" title="Figure 4 ‣ 3.1.3 Detection with Joint Visual Grounding ‣ 3.1 Detection of Concrete Tokens ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>) to create an MMT model.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">To process text input, GRAM uses the same byte-pair encoding (BPE) and vocabulary dictionary as the FAIR WMT19 model <cite class="ltx_cite ltx_citemacro_cite">Ng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib13" title="">2019</a>)</cite>. Masked sentences are BPE-encoded and fed as standard text inputs to the MMT model. We mask by replacing each token with an <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p2.1.1">&lt;unk&gt;</span> token, as that token is the closest to a mask token available in the FAIR WMT19 model <cite class="ltx_cite ltx_citemacro_cite">Ng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib13" title="">2019</a>)</cite>. Our method expands on prior work by <cite class="ltx_cite ltx_citemacro_citet">Tang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib19" title="">2022</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib23" title="">2021</a>)</cite> while increasing the requirements for a token to be visually grounded to an image.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">To process image input, the GRAM model uses CLIP, a pre-trained text-only translation model alongside a pre-trained vision encoder, a perceiver resampler, and vision-text cross-attention layers <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib15" title="">2021</a>)</cite>. While the original GRAM paper utilizes the <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.1.1">ViT-L/14@336px</span> CLIP model, we noted better results within our evaluation framework when using the <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.1.2">RN50x4</span> CLIP model; we present those results below (Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S4.SS2" title="4.2 Results ‣ 4 Results and Discussion ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">4.2</span></a>). This vision encoder converts input images into image embeddings, enabling the perceiver resampler to convert those embeddings into a fixed number of vision tokens. Vision tokens and corresponding text embeddings are interleaved into vision-text cross-attention layers within the transformer encoder, creating mappings from both the text and the image embeddings onto a sequence of joint representations. Finally, the transformer decoder ingests this sequence and outputs probabilities for the next output text token in the target sequence.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">The number of parameters in the original text-only Transformer is 269,746,176; the number of parameters in the RN50x4 CLIP vision encoder is 101,520,396, for a total of 371,266,572 parameters in our GRAM model. Additionally, our GRAM perceiver resampler contains 87,137,080 parameters.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S3.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1">Detection</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1">Selection</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S3.T1.1.1.1.3">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.1">Score</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.2">
<th class="ltx_td ltx_th ltx_th_row" colspan="2" id="S3.T1.1.2.2.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.2">CoMMuTE</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S3.T1.1.2.2.3">Multi30k BLEU4 (en-de)</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.3">
<th class="ltx_td ltx_th ltx_th_row" colspan="3" id="S3.T1.1.3.3.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.3.2">2016</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.3.3">2017</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.3.4">COCO</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="2" id="S3.T1.1.4.4.1"><cite class="ltx_cite ltx_citemacro_citet">Futeral et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib7" title="">2023</a>)</cite></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.4.4.2"><span class="ltx_text ltx_font_italic" id="S3.T1.1.4.4.2.1">0.59</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.4.4.3"><span class="ltx_text ltx_font_italic" id="S3.T1.1.4.4.3.1">43.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.4.4.4"><span class="ltx_text ltx_font_italic" id="S3.T1.1.4.4.4.1">38.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.4.4.5"><span class="ltx_text ltx_font_italic" id="S3.T1.1.4.4.5.1">35.7</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" colspan="2" id="S3.T1.1.5.5.1"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">anon-gram-2024</span></cite></th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.2"><span class="ltx_text ltx_font_italic" id="S3.T1.1.5.5.2.1">0.61</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.3"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.T1.1.5.5.3.1">46.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.4"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.T1.1.5.5.4.1">43.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.5"><span class="ltx_text ltx_font_italic" id="S3.T1.1.5.5.5.1">39.1</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.6.6.1">Unmasked</th>
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.6.6.2"></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.6.6.3">0.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.6.6.4">45.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.6.6.5">42.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.6.6.6">38.2</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.7.7.1">NLTK</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.7.7.2">Unrestricted</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.7.7.3">0.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.7.7.4">45.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.7.7.5">41.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.7.7.6"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S3.T1.1.7.7.6.1">39.2</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.8.8.1">NLTK</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.8.8.2">Restricted-Longest</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.8.3">0.62</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.8.4">46.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.8.5"><span class="ltx_text ltx_framed_underline" id="S3.T1.1.8.8.5.1">42.5</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.8.6">37.8</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.9.9.1">NLTK</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.9.9.2">Restricted-Shortest</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.9.3">0.63</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.9.4">46.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.9.5">42.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.9.6">37.9</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.10.10.1">NLTK</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.10.10.2">Restricted-Random</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.10.3"><span class="ltx_text ltx_font_bold ltx_framed_underline" id="S3.T1.1.10.10.3.1">0.67</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.10.4"><span class="ltx_text ltx_framed_underline" id="S3.T1.1.10.10.4.1">46.2</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.10.5">41.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.10.6">37.8</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.11.11.1">MDETR</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.11.11.2">Unrestricted</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.11.11.3">0.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.11.11.4"><span class="ltx_text ltx_framed_underline" id="S3.T1.1.11.11.4.1">46.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.11.11.5"><span class="ltx_text ltx_framed_underline" id="S3.T1.1.11.11.5.1">42.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.11.11.6"><span class="ltx_text ltx_framed_underline" id="S3.T1.1.11.11.6.1">38.4</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.12.12.1">MDETR</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.12.12.2">Restricted-Longest</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.12.3">0.63</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.12.4">45.7</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.12.5">41.7</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.12.6">38.0</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.13.13.1">MDETR</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.13.13.2">Restricted-Shortest</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.13.3">0.63</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.13.4">45.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.13.5">41.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.13.6">36.9</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.14.14.1">MDETR</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.14.14.2">Restricted-Random</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.14.14.3">0.63</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.14.14.4">45.6</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.14.14.5">42.2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.14.14.6">37.6</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.15.15.1">Joint</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.15.15.2">Unrestricted</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.15.15.3">0.52</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.15.15.4">45.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.15.15.5">42.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.15.15.6"><span class="ltx_text ltx_framed_underline" id="S3.T1.1.15.15.6.1">38.9</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.16.16.1">Joint</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.16.16.2">Restricted-Longest</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.16.16.3"><span class="ltx_text ltx_framed_underline" id="S3.T1.1.16.16.3.1">0.63</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.16.16.4"><span class="ltx_text ltx_framed_underline" id="S3.T1.1.16.16.4.1">45.8</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.16.16.5"><span class="ltx_text ltx_framed_underline" id="S3.T1.1.16.16.5.1">42.6</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.16.16.6">38.8</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.17.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.17.17.1">Joint</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.17.17.2">Restricted-Shortest</th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.17.17.3">0.61</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.17.17.4">45.4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.17.17.5">42.0</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.17.17.6">37.9</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.18.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T1.1.18.18.1">Joint</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T1.1.18.18.2">Restricted-Random</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.18.18.3">0.61</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.18.18.4">45.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.18.18.5">42.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.18.18.6">37.5</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Selected performance results of our model against the CoMMuTE and Multi30k test sets. The best result by column is indicated in <span class="ltx_text ltx_font_bold" id="S3.T1.4.1">bold</span>; the best result for each detection technique is <span class="ltx_text ltx_framed_underline" id="S3.T1.5.2">underlined</span>. Results as reported by GRAM <cite class="ltx_cite ltx_citemacro_cite">Vijayan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib21" title="">2024</a>)</cite> and VGAMT <cite class="ltx_cite ltx_citemacro_cite">Futeral et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib7" title="">2023</a>)</cite> are included for reference.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results and Discussion</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Framework</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We train the GRAM models on unique permutations of synthetically collated datasets representing each combination of detection (<span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">NLTK</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.2">MDETR</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.3">Joint</span>) (Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS1" title="3.1 Detection of Concrete Tokens ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3.1</span></a>) and selection (<span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.4">unrestricted</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.5">restricted-long</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.6">restricted-short</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.7">restricted-random</span>) (Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS2.SSS1" title="3.2.1 Token Selection Techniques ‣ 3.2 Synthetic Dataset Collation ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>) techniques. We compare the resulting trained versions to the GRAM model trained on a unmasked dataset of original sentences.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Most current work in MMT focuses on the Multi30k dataset; because of its prevalence in other MMT works, we utilize the Multi30k dataset for collation of our training datasets. We then evaluate the GRAM models on the Multi30k 2016, 2017, and COCO test sets using BLEU4 scores.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">We also evaluate the GRAM model with an additional metric, Contrastive Multilingual Multimodal Translation Evaluation (CoMMuTE). <cite class="ltx_cite ltx_citemacro_citet">Futeral et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib7" title="">2023</a>)</cite> proposed the CoMMuTE dataset to evaluate both performance on translation tasks and usage of visual information by MMT models. In the ensemble CoMMuTE evaluation, the model is given two images, a lexically or semantically ambiguous English sentence, and a target language translation that resolves the ambiguity according to one of the two images. The task involves determining which of the two images the sentence pairs best match. The evaluation is made using the perplexity of the model output, and the resulting CoMMuTE score is calculated using the model’s determination of accuracy across 100 text-image pairs.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1">Detection</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.1">Concrete %</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.3.1">Unique Detections</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.1.1.1">NLTK</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.2">99.51%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.3">5,393</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.3.2.1.1">MDETR</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.2">99.92%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.3">6,674</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.4.3.1.1">Joint</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.4.3.2">99.49%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.4.3.3">4,761</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Unique concrete token detections and percent of Multi30k sentences with detected tokens by detection technique.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.2">We review the performance of the model variants trained using the synthetic Multi30k datasets (Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS2" title="3.2 Synthetic Dataset Collation ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3.2</span></a>) on the above evaluation metrics. We train <math alttext="13" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">13</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><cn id="S4.SS2.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS2.p1.1.m1.1.1">13</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">13</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">13</annotation></semantics></math> variants, consisting of one unmasked baseline and <math alttext="12" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mn id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><cn id="S4.SS2.p1.2.m2.1.1.cmml" type="integer" xref="S4.SS2.p1.2.m2.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">12</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">12</annotation></semantics></math> models representing each combination of detection (Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS1" title="3.1 Detection of Concrete Tokens ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3.1</span></a>) and selection (Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS2.SSS1" title="3.2.1 Token Selection Techniques ‣ 3.2 Synthetic Dataset Collation ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>) techniques.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Detection Results</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We introduced three distinct methods for detection of concrete text tokens: the NLTK technique (Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS1.SSS1" title="3.1.1 Detection with NLTK ‣ 3.1 Detection of Concrete Tokens ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3.1.1</span></a>), which parses nouns and noun phrases from sentences, the MDETR technique (Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS1.SSS2" title="3.1.2 Detection with MDETR ‣ 3.1 Detection of Concrete Tokens ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3.1.2</span></a>), which inputs sentences as queries to an object detection model, and the Joint technique (Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS1.SSS3" title="3.1.3 Detection with Joint Visual Grounding ‣ 3.1 Detection of Concrete Tokens ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3.1.3</span></a>). Each technique generates the same output structure: multimodal datasets of sentences masked concrete tokens and matching images. We hypothesize that masking concrete tokens with these techniques will improve performance on evaluation metrics. We further hypothesize that the Joint technique will be more selective with its detections than its component NLTK and MDETR techniques, and will thus utilize image context more efficiently and critically.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.5">We found that all three techniques consistently extracted relevant tokens from the text: each technique extracted concrete tokens from over <math alttext="99\%" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mn id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">99</mn><mo id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1">percent</csymbol><cn id="S4.SS3.p2.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.p2.1.m1.1.1.2">99</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">99\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">99 %</annotation></semantics></math> of Multi30k sentences (Table <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S4.T2" title="Table 2 ‣ 4.1 Experimental Framework ‣ 4 Results and Discussion ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>). The MDETR detection technique was the most successful, extracting <math alttext="23.8\%" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.1"><semantics id="S4.SS3.p2.2.m2.1a"><mrow id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mn id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">23.8</mn><mo id="S4.SS3.p2.2.m2.1.1.1" xref="S4.SS3.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1.1">percent</csymbol><cn id="S4.SS3.p2.2.m2.1.1.2.cmml" type="float" xref="S4.SS3.p2.2.m2.1.1.2">23.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">23.8\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m2.1d">23.8 %</annotation></semantics></math> and <math alttext="40.2\%" class="ltx_Math" display="inline" id="S4.SS3.p2.3.m3.1"><semantics id="S4.SS3.p2.3.m3.1a"><mrow id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml"><mn id="S4.SS3.p2.3.m3.1.1.2" xref="S4.SS3.p2.3.m3.1.1.2.cmml">40.2</mn><mo id="S4.SS3.p2.3.m3.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><apply id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1.1">percent</csymbol><cn id="S4.SS3.p2.3.m3.1.1.2.cmml" type="float" xref="S4.SS3.p2.3.m3.1.1.2">40.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">40.2\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.3.m3.1d">40.2 %</annotation></semantics></math> more unique concrete tokens than the NLTK and Joint techniques, respectively. This resulted in the MDETR technique masking the highest concentration of original Multi30k sentences (<math alttext="114" class="ltx_Math" display="inline" id="S4.SS3.p2.4.m4.1"><semantics id="S4.SS3.p2.4.m4.1a"><mn id="S4.SS3.p2.4.m4.1.1" xref="S4.SS3.p2.4.m4.1.1.cmml">114</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><cn id="S4.SS3.p2.4.m4.1.1.cmml" type="integer" xref="S4.SS3.p2.4.m4.1.1">114</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">114</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.4.m4.1d">114</annotation></semantics></math> and <math alttext="120" class="ltx_Math" display="inline" id="S4.SS3.p2.5.m5.1"><semantics id="S4.SS3.p2.5.m5.1a"><mn id="S4.SS3.p2.5.m5.1.1" xref="S4.SS3.p2.5.m5.1.1.cmml">120</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.5.m5.1b"><cn id="S4.SS3.p2.5.m5.1.1.cmml" type="integer" xref="S4.SS3.p2.5.m5.1.1">120</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.5.m5.1c">120</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.5.m5.1d">120</annotation></semantics></math> sentences more than NLTK and Joint, respectively).</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.2">Increased rates of detection did not correlate with better performance, though. All tested models outperformed the unmasked (baseline) dataset in CoMMuTE and BLEU scores, but in contrast to our hypothesis the NLTK technique outperformed both the MDETR and Joint techniques both in CoMMuTE and BLEU score (Table <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.T1" title="Table 1 ‣ 3.3 GRAM Model ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.T1" title="Table 1 ‣ 3.3 GRAM Model ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>). The Joint technique, which we hypothesized would improve on its component techniques, consistently underperformed against the others. This is especially true in the <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p3.2.1">Joint Unrestricted</span> model, which only improved its CoMMuTE score by <math alttext="0.02" class="ltx_Math" display="inline" id="S4.SS3.p3.1.m1.1"><semantics id="S4.SS3.p3.1.m1.1a"><mn id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml">0.02</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><cn id="S4.SS3.p3.1.m1.1.1.cmml" type="float" xref="S4.SS3.p3.1.m1.1.1">0.02</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">0.02</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.1.m1.1d">0.02</annotation></semantics></math> and its BLEU score <math alttext="0.5" class="ltx_Math" display="inline" id="S4.SS3.p3.2.m2.1"><semantics id="S4.SS3.p3.2.m2.1a"><mn id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><cn id="S4.SS3.p3.2.m2.1.1.cmml" type="float" xref="S4.SS3.p3.2.m2.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.2.m2.1d">0.5</annotation></semantics></math> over the baseline. We suggest that the Joint technique was actually hindered by its strict selection process, leading to a much smaller pool of objects to mask from. Conversely, the MDETR technique tended to over-select longer, rarely-used, or irrelevant tokens (Figure <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.F3" title="Figure 3 ‣ 3.1.2 Detection with MDETR ‣ 3.1 Detection of Concrete Tokens ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>), contributing to the larger masking percentages but the lower overall performance. The success of the NLTK technique over the others was its “middle ground” approach, classifying concrete tokens more liberally than the Joint technique but more consistently than the MDETR technique.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.5"><math alttext="23\%" class="ltx_Math" display="inline" id="S4.SS3.p4.1.m1.1"><semantics id="S4.SS3.p4.1.m1.1a"><mrow id="S4.SS3.p4.1.m1.1.1" xref="S4.SS3.p4.1.m1.1.1.cmml"><mn id="S4.SS3.p4.1.m1.1.1.2" xref="S4.SS3.p4.1.m1.1.1.2.cmml">23</mn><mo id="S4.SS3.p4.1.m1.1.1.1" xref="S4.SS3.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.1.m1.1b"><apply id="S4.SS3.p4.1.m1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.p4.1.m1.1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1.1">percent</csymbol><cn id="S4.SS3.p4.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.p4.1.m1.1.1.2">23</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.1.m1.1c">23\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p4.1.m1.1d">23 %</annotation></semantics></math> of tested models underperformed the original GRAM model <cite class="ltx_cite ltx_citemacro_cite">Vijayan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib21" title="">2024</a>)</cite> on CoMMuTE metrics, <math alttext="15.4\%" class="ltx_Math" display="inline" id="S4.SS3.p4.2.m2.1"><semantics id="S4.SS3.p4.2.m2.1a"><mrow id="S4.SS3.p4.2.m2.1.1" xref="S4.SS3.p4.2.m2.1.1.cmml"><mn id="S4.SS3.p4.2.m2.1.1.2" xref="S4.SS3.p4.2.m2.1.1.2.cmml">15.4</mn><mo id="S4.SS3.p4.2.m2.1.1.1" xref="S4.SS3.p4.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.2.m2.1b"><apply id="S4.SS3.p4.2.m2.1.1.cmml" xref="S4.SS3.p4.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.p4.2.m2.1.1.1.cmml" xref="S4.SS3.p4.2.m2.1.1.1">percent</csymbol><cn id="S4.SS3.p4.2.m2.1.1.2.cmml" type="float" xref="S4.SS3.p4.2.m2.1.1.2">15.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.2.m2.1c">15.4\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p4.2.m2.1d">15.4 %</annotation></semantics></math> performed identically, and the remaining <math alttext="53.8\%" class="ltx_Math" display="inline" id="S4.SS3.p4.3.m3.1"><semantics id="S4.SS3.p4.3.m3.1a"><mrow id="S4.SS3.p4.3.m3.1.1" xref="S4.SS3.p4.3.m3.1.1.cmml"><mn id="S4.SS3.p4.3.m3.1.1.2" xref="S4.SS3.p4.3.m3.1.1.2.cmml">53.8</mn><mo id="S4.SS3.p4.3.m3.1.1.1" xref="S4.SS3.p4.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.3.m3.1b"><apply id="S4.SS3.p4.3.m3.1.1.cmml" xref="S4.SS3.p4.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.p4.3.m3.1.1.1.cmml" xref="S4.SS3.p4.3.m3.1.1.1">percent</csymbol><cn id="S4.SS3.p4.3.m3.1.1.2.cmml" type="float" xref="S4.SS3.p4.3.m3.1.1.2">53.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.3.m3.1c">53.8\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p4.3.m3.1d">53.8 %</annotation></semantics></math> outperformed. All tested models underperformed the original GRAM model in Multi30k 2016/2017 BLEU metrics. One model (<span class="ltx_text ltx_font_typewriter" id="S4.SS3.p4.5.1">NLTK Unrestricted</span>) outperformed the original GRAM model in the Multi30k COCO metric, but the improvement is well within a margin for normalization effects. We suggest that the performance disparity between models in these Multi30k BLEU metrics is due to dataset size: the original GRAM model was pre-trained trained on the Conceptual Captions dataset <cite class="ltx_cite ltx_citemacro_cite">Sharma et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib17" title="">2018</a>)</cite> of <math alttext="2,878,999" class="ltx_Math" display="inline" id="S4.SS3.p4.4.m4.3"><semantics id="S4.SS3.p4.4.m4.3a"><mrow id="S4.SS3.p4.4.m4.3.4.2" xref="S4.SS3.p4.4.m4.3.4.1.cmml"><mn id="S4.SS3.p4.4.m4.1.1" xref="S4.SS3.p4.4.m4.1.1.cmml">2</mn><mo id="S4.SS3.p4.4.m4.3.4.2.1" xref="S4.SS3.p4.4.m4.3.4.1.cmml">,</mo><mn id="S4.SS3.p4.4.m4.2.2" xref="S4.SS3.p4.4.m4.2.2.cmml">878</mn><mo id="S4.SS3.p4.4.m4.3.4.2.2" xref="S4.SS3.p4.4.m4.3.4.1.cmml">,</mo><mn id="S4.SS3.p4.4.m4.3.3" xref="S4.SS3.p4.4.m4.3.3.cmml">999</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.4.m4.3b"><list id="S4.SS3.p4.4.m4.3.4.1.cmml" xref="S4.SS3.p4.4.m4.3.4.2"><cn id="S4.SS3.p4.4.m4.1.1.cmml" type="integer" xref="S4.SS3.p4.4.m4.1.1">2</cn><cn id="S4.SS3.p4.4.m4.2.2.cmml" type="integer" xref="S4.SS3.p4.4.m4.2.2">878</cn><cn id="S4.SS3.p4.4.m4.3.3.cmml" type="integer" xref="S4.SS3.p4.4.m4.3.3">999</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.4.m4.3c">2,878,999</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p4.4.m4.3d">2 , 878 , 999</annotation></semantics></math> text-image pairs, resulting in synthetic datasets nearly 100 times larger than those used here. Despite this, the majority of models outperformed GRAM in CoMMuTE metrics, achieving scores of up to <math alttext="0.67" class="ltx_Math" display="inline" id="S4.SS3.p4.5.m5.1"><semantics id="S4.SS3.p4.5.m5.1a"><mn id="S4.SS3.p4.5.m5.1.1" xref="S4.SS3.p4.5.m5.1.1.cmml">0.67</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.5.m5.1b"><cn id="S4.SS3.p4.5.m5.1.1.cmml" type="float" xref="S4.SS3.p4.5.m5.1.1">0.67</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.5.m5.1c">0.67</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p4.5.m5.1d">0.67</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1">In general, we also note an inverse relationship between CoMMuTE and BLEU performance: that is, when CoMMuTE scores increase, BLEU scores tend to decrease. For example, the <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p5.1.1">MDETR Unrestricted</span> model notched the highest average BLEU score across all three Multi30k metrics, but had the second-lowest CoMMuTE score.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Selection Results</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Critical to the synthetic dataset collation system is the process of selecting concrete tokens for masking. Prior efforts have generally selected tokens at random <cite class="ltx_cite ltx_citemacro_cite">Ive et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib8" title="">2019</a>)</cite>; we introduced three additional techniques (Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS2.SSS1" title="3.2.1 Token Selection Techniques ‣ 3.2 Synthetic Dataset Collation ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>), longest-token selection, shortest-token selection, and unrestricted selection, and test each against a baseline of randomly-selected concrete tokens. We hypothesize that the presented token selection techniques will outperform the baseline of random selection; specifically, we hypothesize that longest-token and unrestricted selection will encourage additional usage of visual context and thus improve CoMMuTE score, and that shortest-token selection will minimize the number of token predictions required by the model (Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS3" title="3.3 GRAM Model ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3.3</span></a>) and thus improve BLEU score.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">We found that while all tested selection techniques (Section <a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#S3.SS2.SSS1" title="3.2.1 Token Selection Techniques ‣ 3.2 Synthetic Dataset Collation ‣ 3 Approach ‣ Detecting concrete visual tokens for Multimodal Machine Translation"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>) outperformed the unmasked baseline, comparative performance between techniques are less conclusive. When paired with the NLTK detection technique, the random selection technique outperformed the others in CoMMuTE and Multi30k 2016 BLEU scores. When paired with the MDETR metric, none of the restricted selection techniques had any impact on CoMMuTE score. When paired with the Joint detection technique, the longest-token selection technique improved CoMMuTE and Multi30k 2016/2017 BLEU scores.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">Contrary to our hypothesis, the deterministic token selection techniques did not consistently outperform the random selection technique. The most consistent results were with the unrestricted selection technique, which significantly degraded CoMMuTE performance but tended to improve BLEU performance (especially in the COCO BLEU metric, where it outperformed all other tested models). Shortest-token selection also tended to follow these patterns of performance degradation, but not as substantially: its NLTK and Joint detection variants performed identically on the Multi30k 2017 and COCO BLEU metrics and performed near the bottom of results for the CoMMuTE and 2016 BLEU metrics across all three detection techniques.</p>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1">Each of these findings runs counter to our hypotheses in this area, suggesting that token selection at this scale has less impact on model performance than we expected; in fact, random or pseudo-random token selection of the identified concrete tokens may actually improve performance over deterministic methods.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Future Work</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">Given the high percentage of visually-grounded tokens in the Multi30k training set, future work should consider the techniques against both larger MMT datasets and MMT datasets with lower concentrations of visually-grounded tokens (e.g., Conceptual Captions). Similarly, future work should consider synthetically collated datasets that combine elements of multiple multimodal datasets (e.g., images from Conceptual Captions, sentences from Multi30k), including synthetic datasets created from text-only datasets.</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1">Additionally, future work should compare baseline scores for tokens selected completely at random to more accurately gauge the efficacy of object token selection.</p>
</div>
<div class="ltx_para" id="S4.SS5.p3">
<p class="ltx_p" id="S4.SS5.p3.1">Finally, future work should consider a more deterministic way to classify the concreteness of a token with NLP, including selection of definitions based on contextual awareness.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The continued challenge of visual grounding and masking in MMT systems has encouraged varying approaches to the detection and selection of visually-grounded text tokens for masking <cite class="ltx_cite ltx_citemacro_cite">Caglayan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib2" title="">2019</a>); Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib23" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We introduced three new techniques for detection of concrete tokens from source sentences: detection with natural language processing (NLP), detection with object detection, and joint NLP/object detection. We also introduced deterministic methods for the selection of detected tokens, including longest and shortest <math alttext="n" class="ltx_Math" display="inline" id="S5.p2.1.m1.1"><semantics id="S5.p2.1.m1.1a"><mi id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><ci id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S5.p2.1.m1.1d">italic_n</annotation></semantics></math> tokens.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Finally, we utilized the GRAM MMT architecture <cite class="ltx_cite ltx_citemacro_cite">Vijayan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib21" title="">2024</a>)</cite> to train models against synthetically collated datasets of masked sentences and associated images. We showed performance improvement over the baseline models and elevated usage of visual context during translation tasks.</p>
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">0</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">0</sup><span class="ltx_note_type">footnotetext: </span>Opinions, interpretations, conclusions, and recommendations are those of the authors and are not necessarily endorsed by the United States Government. Cleared for public release on 12 Feb 2024. Originator reference number RH-24-125351. Case number AFRL-2024-0803.</span></span></span>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caglayan et al. (2016)</span>
<span class="ltx_bibblock">
Ozan Caglayan, Walid Aransa, Yaxing Wang, Marc Masana, Mercedes
García-Martínez, Fethi Bougares, Loïc Barrault, and Joost van de Weijer.
2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/w16-2358" title="">Does multimodality help
human and machine for translation and image captioning?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the First Conference on Machine Translation:
Volume 2, Shared Task Papers</em>. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caglayan et al. (2019)</span>
<span class="ltx_bibblock">
Ozan Caglayan, Pranava Madhyastha, Lucia Specia, and Loïc Barrault. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1422" title="">Probing the need for
visual context in multimodal machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 4159–4170,
Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carion et al. (2020)</span>
<span class="ltx_bibblock">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2005.12872" title="">End-to-end object detection
with transformers</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-1423" title="">BERT: Pre-training of
deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186,
Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elliott et al. (2016)</span>
<span class="ltx_bibblock">
Desmond Elliott, Stella Frank, Khalil Sima’an, and Lucia Specia. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1605.00459" title="">Multi30k: Multilingual
english-german image descriptions</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fellbaum and Miller (1998)</span>
<span class="ltx_bibblock">
Christiane Fellbaum and George A. Miller. 1998.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://mitpress.mit.edu/9780262561167/" title=""><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1.1">WordNet: An
Electronic Lexical Database</em></a>.

</span>
<span class="ltx_bibblock">The MIT Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Futeral et al. (2023)</span>
<span class="ltx_bibblock">
Matthieu Futeral, Cordelia Schmid, Ivan Laptev, Benoît Sagot, and Rachel
Bawden. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.295" title="">Tackling
ambiguity with images: Improved multimodal machine translation and
contrastive evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 5394–5413,
Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ive et al. (2019)</span>
<span class="ltx_bibblock">
J. Ive, P. Madhyastha, and Lucia Specia. 2019.

</span>
<span class="ltx_bibblock">Distilling Translations with Visual Awareness.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">ArXiv</em>, abs/1906.07701.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al. (2020)</span>
<span class="ltx_bibblock">
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and
Omer Levy. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00300" title="">SpanBERT: Improving
Pre-training by Representing and Predicting Spans</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Transactions of the Association for Computational Linguistics</em>,
8:64–77.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamath et al. (2021)</span>
<span class="ltx_bibblock">
Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and
Nicolas Carion. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2104.12763" title="">Mdetr – modulated detection
for end-to-end multi-modal understanding</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lala and Specia (2018)</span>
<span class="ltx_bibblock">
Chiraag Lala and Lucia Specia. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/L18-1602" title="">Multimodal lexical
translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the Eleventh International Conference on
Language Resources and Evaluation (LREC 2018)</em>, Miyazaki, Japan. European
Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loper and Bird (2002)</span>
<span class="ltx_bibblock">
Edward Loper and Steven Bird. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/cs/0205028" title="">NLTK: The natural
language toolkit</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the ACL Workshop on Effective Tools and
Methodologies for Teaching Natural Language Processing and Computational
Linguistics</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ng et al. (2019)</span>
<span class="ltx_bibblock">
Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov.
2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.1907.06616" title="">Facebook FAIR’s
WMT19 News Translation Task Submission</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plummer et al. (2015)</span>
<span class="ltx_bibblock">
Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia
Hockenmaier, and Svetlana Lazebnik. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICCV.2015.303" title="">Flickr30k entities:
Collecting region-to-phrase correspondences for richer image-to-sentence
models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">2015 IEEE International Conference on Computer Vision
(ICCV)</em>, pages 2641–2649.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
Gretchen Krueger, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock">Learning Transferable Visual Models From Natural
Language Supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 38th International Conference on
Machine Learning</em>, volume 139 of <em class="ltx_emph ltx_font_italic" id="bib.bib15.2.2">Proceedings of Machine
Learning Research</em>, pages 8748–8763. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sato et al. (2023)</span>
<span class="ltx_bibblock">
Julia Sato, Helena Caseli, and Lucia Specia. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-srw.35" title="">Choosing what to
mask: More informed masking for multimodal machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 4: Student Research Workshop)</em>, pages
244–253, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al. (2018)</span>
<span class="ltx_bibblock">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P18-1238" title="">Conceptual captions: A
cleaned, hypernymed, image alt-text dataset for automatic image captioning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 2556–2565,
Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Bansal (2020)</span>
<span class="ltx_bibblock">
Hao Tan and Mohit Bansal. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2010.06775" title="">Vokenization: Improving
language understanding with contextualized, visual-grounded supervision</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2022)</span>
<span class="ltx_bibblock">
ZhenHao Tang, XiaoBing Zhang, Zi Long, and XiangHua Fu. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wat-1.11" title="">Multimodal neural
machine translation with search engine based image retrieval</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 9th Workshop on Asian Translation</em>, pages
89–98, Gyeongju, Republic of Korea. International Conference on
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1706.03762" title="">Attention is all you need</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vijayan et al. (2024)</span>
<span class="ltx_bibblock">
Vipin Vijayan, Braeden Bowen, Jeremy Gwinnup, Scott Grigsby, and Timothy
Anderson. 2024.

</span>
<span class="ltx_bibblock">Adding multimodal capabilities to a text-only translation model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">ArXiV</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Xiong (2021)</span>
<span class="ltx_bibblock">
Dexin Wang and Deyi Xiong. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1609/aaai.v35i4.16376" title="">Efficient
Object-Level Visual Context Modeling for Multimodal Machine
Translation: Masking Irrelevant Objects Helps Grounding</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>,
35(4):2720–2728.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2021)</span>
<span class="ltx_bibblock">
Zhiyong Wu, Lingpeng Kong, Wei Bi, Xiang Li, and Ben Kao. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2105.14462" title="">Good for misconceived
reasons: An empirical revisiting on the need for visual context in multimodal
machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. (2023)</span>
<span class="ltx_bibblock">
Yisheng Xiao, Ruiyang Xu, Lijun Wu, Juntao Li, Tao Qin, Yan-Tie Liu, and Min
Zhang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2303.07457" title="">Amom: Adaptive masking over
masking for conditional masked language model</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2018)</span>
<span class="ltx_bibblock">
Mingyang Zhou, Runxiang Cheng, Yong Jae Lee, and Zhou Yu. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1808.08266" title="">A visual attention grounding
neural model for multimodal machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuang et al. (2023)</span>
<span class="ltx_bibblock">
Chengxu Zhuang, Evelina Fedorenko, and Jacob Andreas. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.13257" title="">Visual grounding helps learn
word meanings in low-data regimes</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="p3">
<p class="ltx_p" id="p3.1"><span class="ltx_text ltx_font_bold" id="p3.1.1">Appendix</span></p>
</div>
<figure class="ltx_table" id="A0.T3"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="334" id="A0.T3.g1" src="x5.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Labeled WordNet <cite class="ltx_cite ltx_citemacro_cite">Fellbaum and Miller (<a class="ltx_ref" href="https://arxiv.org/html/2403.03075v1#bib.bib6" title="">1998</a>)</cite> hypernyms. A token is classified as concrete or abstract if any of the above hypernyms are in its DAG.</figcaption>
</figure>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Mar  5 15:57:57 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
