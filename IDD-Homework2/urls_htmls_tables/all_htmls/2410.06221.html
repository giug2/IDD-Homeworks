<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings</title>
<!--Generated on Tue Oct  8 17:26:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Microphone identification,  Audio recording,  Source Attribution,  Audio Forensics,  Multi-class Classification
" lang="en" name="keywords"/>
<base href="/html/2410.06221v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S1" title="In POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S2" title="In POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Background</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S2.SS1" title="In II Background ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Datasets for smartphone model identification from audio recordings</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S2.SS2" title="In II Background ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Methods for smartphone model identification from audio recordings</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S3" title="In POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Dataset Acquisition</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S3.SS1" title="In III Dataset Acquisition ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Recording Setup</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S3.SS2" title="In III Dataset Acquisition ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Recorded tracks</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S3.SS3" title="In III Dataset Acquisition ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Post-processing operations</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S3.SS4" title="In III Dataset Acquisition ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Released data</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S4" title="In POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Dataset Analysis</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S5" title="In POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Smartphone Model Identification from Audio Recordings</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S5.SS1" title="In V Smartphone Model Identification from Audio Recordings ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Clean Speech experiments</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S5.SS2" title="In V Smartphone Model Identification from Audio Recordings ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Different Domains’ experiments</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S6" title="In POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Davide Salvi,
Daniele Ugo Leonzio,
Antonio Giganti,
Claudio Eutizi, 
<br class="ltx_break"/>Sara Mandelli,
Paolo Bestagini,
Stefano Tubaro
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id1.1.id1">Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Politecnico di Milano, Milan, Italy</span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">When dealing with multimedia data, source attribution is a key challenge from a forensic perspective. This task aims to determine how a given content was captured, providing valuable insights for various applications, including legal proceedings and integrity investigations. The source attribution problem has been addressed in different domains, from identifying the camera model used to capture specific photographs to detecting the synthetic speech generator or microphone model used to create or record given audio tracks.
Recent advancements in this area rely heavily on machine learning and data-driven techniques, which often outperform traditional signal processing-based methods.
However, a drawback of these systems is their need for large volumes of training data, which must reflect the latest technological trends to produce accurate and reliable predictions.
This presents a significant challenge, as the rapid pace of technological progress makes it difficult to maintain datasets that are up-to-date with real-world conditions.
For instance, in the task of smartphone model identification from audio recordings, the available datasets are often outdated or acquired inconsistently, making it difficult to develop solutions that are valid beyond a research environment.
In this paper we present POLIPHONE, a dataset for smartphone model identification from audio recordings. It includes data from 20 recent smartphones recorded in a controlled environment to ensure reproducibility and scalability for future research.
The released tracks contain audio data from various domains (i.e., speech, music, environmental sounds), making the corpus versatile and applicable to a wide range of use cases.
We also present numerous experiments to benchmark the proposed dataset using a state-of-the-art classifier for smartphone model identification from audio recordings.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Microphone identification, Audio recording, Source Attribution, Audio Forensics, Multi-class Classification

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In the last few years, the creation and sharing of multimedia content have become highly accessible to the general public. This happened thanks to rapid technological advancements and the increasing availability of processing techniques and computational power.
As a result, the amount of media uploaded on the web and social platforms has been increasing daily, leading to the rise of new opportunities and techniques based on the use of the data itself.
At the same time, the creation of synthetic material and the alteration of existing media have become more accessible, with new technologies capable of generating fake data with impressive levels of realism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib1" title="">1</a>]</cite>.
This aspect is controversial, as misusing these powerful technologies could lead to unpleasant situations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib2" title="">2</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Given the importance of multimedia content in today’s society, it is becoming of paramount importance to determine the integrity of media under analysis and mitigate potential issues.
This is crucial not only to ensure people’s cyber-security but also to use the media content as evidence in legal proceedings.
The scientific community is continuously working in this direction and proposing innovative solutions to address diverse multimedia forensic problems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib4" title="">4</a>]</cite>.
Among the considered tasks, one of the most crucial is source identification.
This consists of, given a multimedia signal, determining how it was generated or acquired.
The same problem has been addressed in several domains, ranging from images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib7" title="">7</a>]</cite>, videos <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib9" title="">9</a>]</cite> to audio signals <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib11" title="">11</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In the case of audio signals, numerous approaches have been proposed to address the challenge of identifying the recording microphone from a given track over the last few years.
The most recent solutions focus on using multi-class classifiers based on data-driven systems where each class represents a distinct microphone model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib13" title="">13</a>]</cite>.
While these methods have achieved excellent results, they are heavily reliant on the quality and scope of the training data.
For a model to perform accurately and produce reliable predictions, it must be trained on large, diverse datasets.
Additionally, to ensure real-world applicability, the training data needs to incorporate cutting-edge technologies, as the rapid evolution of audio recording devices makes outdated data incomplete.
However, obtaining up-to-date and comprehensive datasets is a significant challenge, as many of the available datasets are either outdated, inconsistently collected or lack proper documentation, making them difficult to reproduce or apply in practical scenarios.
This limitation impacts the state of research in this field, as the performance and accuracy of the developed classifiers depend significantly on the quality of the data used to train and test them.
Hence, the need for new data for identifying smartphone models from audio recordings is becoming critical.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper we present and release POLIPHONE, a novel dataset for microphone model identification from audio recordings in the case of smartphones.
It contains recordings captured in a controlled environment by recent smartphones, bringing several contributions to advancing the state-of-the-art.
First of all, all the considered models are recent, as they have been released from <math alttext="2018" class="ltx_Math" display="inline" id="S1.p4.1.m1.1"><semantics id="S1.p4.1.m1.1a"><mn id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">2018</mn><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><cn id="S1.p4.1.m1.1.1.cmml" type="integer" xref="S1.p4.1.m1.1.1">2018</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">2018</annotation><annotation encoding="application/x-llamapun" id="S1.p4.1.m1.1d">2018</annotation></semantics></math> onwards, making the dataset functional also for current real-world applications.
Secondly, the recording setup is entirely controlled, making the dataset reproducible and expandable with the addition of new microphone models in the future, even by other research groups. This is fundamental as the lack of a standard in the acquisition of recordings is one of the main reasons for the dearth of data in this field.
Finally, the recorded data comes from different audio domains, including clean and noisy speech, music and environmental sounds. This makes the data usable in different scenarios and for various applications.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.2">The released dataset contains a total of more than <math alttext="50" class="ltx_Math" display="inline" id="S1.p5.1.m1.1"><semantics id="S1.p5.1.m1.1a"><mn id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><cn id="S1.p5.1.m1.1.1.cmml" type="integer" xref="S1.p5.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">50</annotation><annotation encoding="application/x-llamapun" id="S1.p5.1.m1.1d">50</annotation></semantics></math> hours of audio recordings acquired with <math alttext="20" class="ltx_Math" display="inline" id="S1.p5.2.m2.1"><semantics id="S1.p5.2.m2.1a"><mn id="S1.p5.2.m2.1.1" xref="S1.p5.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S1.p5.2.m2.1b"><cn id="S1.p5.2.m2.1.1.cmml" type="integer" xref="S1.p5.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.2.m2.1c">20</annotation><annotation encoding="application/x-llamapun" id="S1.p5.2.m2.1d">20</annotation></semantics></math> different smartphones. The <span class="ltx_glossaryref" title="">Impulse Responses (IRs)</span> of each model is also provided, increasing the possible applications of the corpus.
Finally, we conduct a series of experiments to benchmark the proposed dataset and provide some information on the challenges it raises.
We do so by considering a state-of-the-art classifier presented in the literature, based on a <span class="ltx_glossaryref" title="">Convolutional Neural Network (CNN)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib14" title="">14</a>]</cite>.
Results confirm the need for new data to address the smartphone model identification task from audio recordings, as it could be an exciting topic for further research.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The rest of paper is structured as follows.
Section <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S2" title="II Background ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">II</span></a> describes the background in the considered task, both regarding available data and presented methods. Here, we also provide a review of the most important datasets released in the field for research purposes, highlighting the motivations that led to the development of POLIPHONE.
Section <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S3" title="III Dataset Acquisition ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">III</span></a> provides all the information about the presented dataset, illustrating the recorded corpus, the considered smartphones and the used recording setup.
Section <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S4" title="IV Dataset Analysis ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">IV</span></a> presents the analysis performed on the released data, investigating the differences between the acquired recordings.
Section <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S5" title="V Smartphone Model Identification from Audio Recordings ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">V</span></a> illustrates the experiments conducted to benchmark the released dataset, together with the achieved results.
Finally, Section <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S6" title="VI Conclusion ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">VI</span></a> concludes the paper, states its main contributions, and briefly discusses possible future work.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Background</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">This section provides the reader with background details regarding the problem of smartphone model identification from audio recordings.
First, we introduce the existing datasets in the state-of-the-art for the task at hand and outline their strengths and limitations.
This emphasizes the need for a novel set like the one we release in this paper.
Then, we provide an overview of the techniques presented in the literature to address the identification problem.
The information reported here can serve as a review of this research topic, which is often overlooked despite its importance in multimedia forensics.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Datasets proposed in the state-of-the-art for the microphone model identification task applied to smartphones.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T1.1" style="width:433.6pt;height:213.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-245.9pt,121.0pt) scale(0.468603426006628,0.468603426006628) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.2">Dataset Name</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.3">No. Brands</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.4">No. Models</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.5">Audio Type</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.6">No. Speakers</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.7">No. Env.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.8">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.1.1.1.8.1">
<tr class="ltx_tr" id="S2.T1.1.1.1.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.1.8.1.1.1">Recorded</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.1.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.1.8.1.2.1">Corpus</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.1">Samp. Rate [<math alttext="\mathrm{kHz}" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.m1.1a"><mi class="ltx_unit" id="S2.T1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.m1.1.1.cmml">kHz</mi><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1">kilohertz</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.m1.1d">roman_kHz</annotation></semantics></math>]</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.1.2.1.1">Buchholz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib15" title="">15</a>]</cite>, 2009</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.2.1.2">6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.2.1.3">7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.2.1.4">Music, Noise, Pure Tone</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.2.1.5">N.A.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.2.1.6">12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.2.1.7">N.A.</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S2.T1.1.1.2.1.8">44.1</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.3.2">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.3.2.1">Garcia-Romero et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib16" title="">16</a>]</cite>, 2010</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.3.2.2">N.A.</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.3.2.3">8</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.3.2.4">Speech</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.3.2.5">61</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.3.2.6">1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.3.2.7">TIMIT</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.3.2.8">8</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.4.3">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.4.3.1">Hanilci et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib17" title="">17</a>]</cite>, 2012</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.3.2">6</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.3.3">14</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.3.4">Speech</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.3.5">25</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.3.6">1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.3.7">TIMIT, various</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.4.3.8">8</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.5.4">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.5.4.1">Ikram et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib18" title="">18</a>]</cite>, 2012</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.4.2">4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.4.3">8</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.4.4">Noise</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.4.5">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.4.6">1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.4.7">-</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.5.4.8">N.A.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.6.5">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.6.5.1">Cuccovillo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib19" title="">19</a>]</cite>, 2013</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.5.2">4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.5.3">8</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.5.4">Noise, Speech, Music</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.5.5">N.A.</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.5.6">N.A.</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.5.7">N.A.</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.6.5.8">8</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.7.6">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.7.6.1">Pandey et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib20" title="">20</a>]</cite>, 2014</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.7.6.2">5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.7.6.3">26</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.7.6.4">Speech</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.7.6.5">N.A.</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.7.6.6">N.A.</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.7.6.7">N.A.</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.7.6.8">N.A.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.8.7">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.8.7.1">Eskidere <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib21" title="">21</a>]</cite>, 2014</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.8.7.2">12</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.8.7.3">16</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.8.7.4">Speech</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.8.7.5">41</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.8.7.6">1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.8.7.7">TIMIT</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.8.7.8">16</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.9.8">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.9.8.1">Aggarwal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib22" title="">22</a>]</cite>, 2014</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.9.8.2">5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.9.8.3">26</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.9.8.4">Speech</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.9.8.5">N.A.</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.9.8.6">1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.9.8.7">N.A.</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.9.8.8">N.A.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.10.9">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.10.9.1">Kotropoulos et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib23" title="">23</a>]</cite>, 2014</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.10.9.2">7</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.10.9.3">21</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.10.9.4">Speech</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.10.9.5">24</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.10.9.6">1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.10.9.7">TIMIT</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.10.9.8">16</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.11.10">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.11.10.1">Zeng J. et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib24" title="">24</a>]</cite>, 2015</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.11.10.2">13</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.11.10.3">21</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.11.10.4">Noise</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.11.10.5">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.11.10.6">1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.11.10.7">N.A.</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.11.10.8">N.A.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.12.11">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.12.11.1">Zou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib25" title="">25</a>]</cite>, 2017</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.12.11.2">6</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.12.11.3">15</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.12.11.4">Speech</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.12.11.5">24</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.12.11.6">1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.12.11.7">TIMIT</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.12.11.8">8</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.13.12">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.13.12.1">Luo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib26" title="">26</a>]</cite>, 2018</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.13.12.2">N.A.</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.13.12.3">31+141</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.13.12.4">Speech</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.13.12.5">4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.13.12.6">4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.13.12.7">N.A.</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.13.12.8">44.1</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.14.13">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.14.13.1">Qin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib27" title="">27</a>]</cite>, 2018</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.14.13.2">7</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.14.13.3">24</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.14.13.4">Speech</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.14.13.5">172</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.14.13.6">1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.14.13.7">TIMIT, CKC-SD</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.14.13.8">N.A.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.15.14">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.15.14.1">Verma et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib28" title="">28</a>]</cite>, 2019</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.15.14.2">8</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.15.14.3">19</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.15.14.4">Speech</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.15.14.5">3</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.15.14.6">1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.15.14.7">N.A.</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.15.14.8">N.A.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.16.15">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.16.15.1">Baldini et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib14" title="">14</a>]</cite>, 2019</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.16.15.2">4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.16.15.3">32</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.16.15.4">Pure Tones, Noise</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.16.15.5">-</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.16.15.6">N.A.</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.16.15.7">N.A.</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.16.15.8">44.1</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.17.16">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.17.16.1">Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib29" title="">29</a>]</cite>, 2019</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.17.16.2">7</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.17.16.3">16</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.17.16.4">Speech</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.17.16.5">172</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.17.16.6">1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.17.16.7">TIMIT, various</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.17.16.8">N.A.</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.18.17">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.18.17.1">Lin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib30" title="">30</a>]</cite>, 2020</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.18.17.2">5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.18.17.3">20</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.18.17.4">Speech, Various</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.18.17.5">N.A.</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.18.17.6">N.A.</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.18.17.7">N.A.</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.18.17.8">16</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.19.18">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.19.18.1">Zeng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib31" title="">31</a>]</cite>, 2020</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.19.18.2">8</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.19.18.3">45</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.19.18.4">Speech</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.19.18.5">N.A.</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.19.18.6">1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.19.18.7">TIMIT</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.19.18.8">32</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.20.19">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.20.19.1">Shen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib32" title="">32</a>]</cite>, 2021</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.20.19.2">N.A.</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.20.19.3">19</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.20.19.4">Speech</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.20.19.5">32</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.20.19.6">N.A.</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.20.19.7">TIMIT</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.20.19.8">16</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.21.20">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.21.20.1">Hashim et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib33" title="">33</a>]</cite>, 2021</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.21.20.2">4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.21.20.3">12</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.21.20.4">Speech</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.21.20.5">7</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.21.20.6">N.A.</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.21.20.7">N.A.</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.21.20.8">44.1</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.22.21">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.22.21.1">Singhal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib34" title="">34</a>]</cite>, 2021</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.22.21.2">5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.22.21.3">5</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.22.21.4">Whispered Speech, Speech, Pure Tones</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.22.21.5">88</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.22.21.6">1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.22.21.7">MOCHA-TIMIT</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.22.21.8">16</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.23.22">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.23.22.1">Berdich et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib35" title="">35</a>]</cite>, 2022</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.23.22.2">7</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.23.22.3">32</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.23.22.4">Speech, Environmental Sounds</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.23.22.5">24</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.23.22.6">4</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.23.22.7">MOBIPHONE</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.23.22.8">48</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.24.23">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T1.1.1.24.23.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.24.23.1.1">POLIPHONE, 2024</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.1.24.23.2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.24.23.2.1">10</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.1.24.23.3"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.24.23.3.1">20</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.1.24.23.4"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.24.23.4.1">Speech, Music, Noise</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.1.24.23.5"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.24.23.5.1">30</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.1.24.23.6"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.24.23.6.1">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.1.24.23.7"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.24.23.7.1">VCTK, OrchSet, ESC-10</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S2.T1.1.1.24.23.8"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.24.23.8.1">44.1</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Datasets for smartphone model identification from audio recordings</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">When developing classification algorithms for recording smartphone model identification, one of the main requirements is to dispose of a large amount of data comprising audio recordings from several models.
Having data acquired from various sources will allow the classifier to learn how to differentiate between them.
For this reason, in recent years, the forensics community has released numerous datasets tailored for this purpose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib38" title="">38</a>]</cite>.
These have been developed considering different scenarios and application contexts and have been crucial in pushing research in the field.
Among these, a great effort has been invested in creating datasets containing tracks recorded by smartphones <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib35" title="">35</a>]</cite>. These models are the most prevalent at the consumer level and generate the vast majority of multimedia data that is shared online. Intending to make datasets that are practical for real-world applications, the inclusion of smartphones is a logical step.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">While the existing datasets have been crucial in driving research in recording smartphone model identification, a comparison between them reveals several inconsistencies in data collection methods, which can hinder the performance of developed systems. The acquisition conditions vary considerably across recording setups and environments, resulting in a loose connection among most state-of-the-art corpora.
This makes it difficult to compare the performance of the developed classifiers, which affects the reliability of the results, making them less applicable to real-world scenarios.
Additionally, as this research field is closely related to consumer devices, there is a need for continuous renewal of the data released to ensure that developed systems can also be usable in the real world.
For these reasons, the need for more data for the task of smartphone microphone model identification remains an ongoing challenge within the audio forensics community.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">In this section, we collect the primary datasets presented in the literature for the task of smartphone model identification from audio recordings and highlight their strengths and weaknesses.
We restrict our analysis to datasets that contain audio tracks recorded from smartphones.
<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S2.T1" title="In II Background ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">I</span></a> lists them in chronological order, from the earliest to the most recent.
For further details regarding each corpus, we direct readers to the respective papers.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">The majority of released datasets include only recorded speech tracks, while only a few consider audio data from different domains (e.g., music, noise, etc.) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib35" title="">35</a>]</cite>.
This is presumably due to the fact that speech is the most acquired type of signal when dealing with smartphones, compared to others.
However, in a real-world scenario, recordings from other domains like environmental sounds, noise, and music may also be encountered, leading to the inclusion of other audio types in some sets.
Additionally, there are cases where pure tone signals are recorded to gather more information about the response of the microphones at specific frequencies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib34" title="">34</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.2">Another critical aspect of the released data concerns the sampling rate of acquisitions. As it is shown in <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S2.T1" title="In II Background ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">I</span></a>, most of the data were acquired at a relatively low frequency compared to the current recording standards employed by the newest models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib25" title="">25</a>]</cite>.
Although most of the speech content resides within the <math alttext="0" class="ltx_Math" display="inline" id="S2.SS1.p5.1.m1.1"><semantics id="S2.SS1.p5.1.m1.1a"><mn id="S2.SS1.p5.1.m1.1.1" xref="S2.SS1.p5.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.1.m1.1b"><cn id="S2.SS1.p5.1.m1.1.1.cmml" type="integer" xref="S2.SS1.p5.1.m1.1.1">0</cn></annotation-xml></semantics></math>-<math alttext="8\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S2.SS1.p5.2.m2.3"><semantics id="S2.SS1.p5.2.m2.3a"><mrow id="S2.SS1.p5.2.m2.3.3" xref="S2.SS1.p5.2.m2.3.3.cmml"><mn id="S2.SS1.p5.2.m2.1.1.1.1.1.1" xref="S2.SS1.p5.2.m2.1.1.1.1.1.1.cmml">8</mn><mtext id="S2.SS1.p5.2.m2.2.2.2.2.2.2" xref="S2.SS1.p5.2.m2.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S2.SS1.p5.2.m2.3.3.3.3.3.3" xref="S2.SS1.p5.2.m2.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.2.m2.3b"><apply id="S2.SS1.p5.2.m2.3.3.cmml" xref="S2.SS1.p5.2.m2.3.3"><csymbol cd="latexml" id="S2.SS1.p5.2.m2.2.2.2.2.2.2.cmml" xref="S2.SS1.p5.2.m2.2.2.2.2.2.2">times</csymbol><cn id="S2.SS1.p5.2.m2.1.1.1.1.1.1.cmml" type="integer" xref="S2.SS1.p5.2.m2.1.1.1.1.1.1">8</cn><csymbol cd="latexml" id="S2.SS1.p5.2.m2.3.3.3.3.3.3.cmml" xref="S2.SS1.p5.2.m2.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.2.m2.3c">8\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p5.2.m2.3d">start_ARG 8 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math> band,
using a low sampling rate precludes studies regarding the impact of higher frequency bands on model identification problems.
This marks the need for data that aligns with the latest technological advancements, enabling the development of classification algorithms that can be more reliable in today’s scenario.</p>
</div>
<div class="ltx_para" id="S2.SS1.p6">
<p class="ltx_p" id="S2.SS1.p6.1">Additionally, the majority of datasets presented in <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S2.T1" title="In II Background ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">I</span></a> lack an explanation regarding the considered acquisition setup, both on the used equipment and the environmental parameters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib33" title="">33</a>]</cite>.
Many of them do not include technical specifications regarding recorded corpus, number of speakers, sampling rate, and bit depth.
This is a critical point regarding the reproducibility of the experiments, preventing a reliable comparison between different datasets and classification methods.
In some cases, highly specific scenarios are considered when performing the recordings. For instance, the authors of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib35" title="">35</a>]</cite> consider car sounds recorded inside and outside the vehicles, making their experiments challenging to compare with those performed on other corpora.
Finally, most datasets reported here are not publicly available to the scientific community.
The only unrestricted sets are MOBIPHONE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib23" title="">23</a>]</cite> and CCNU_Mobile <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib31" title="">31</a>]</cite>, which, however, include smartphones that are now outdated and not functional in the current state of the art.
This aspect further limits the research on the model identification task, making the proposed methods challenging to compare with each other.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Methods for smartphone model identification from audio recordings</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">In recent years, several methods have been proposed to address the recording smartphone model identification problem, leveraging different approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib40" title="">40</a>]</cite>.
Most of these are based on the analysis of acoustic features directly extracted from raw acquisition at the frame level.
This pipeline is usually implemented in two steps, where a feature extraction phase is followed by a supervised classification step.
Different acoustic features can be considered by these approaches, ranging from cepstral-based, spectral-based, and <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.1">ad-hoc</em> features.
Among cepstral-based features, <span class="ltx_glossaryref" title="">Mel Frequency Cepstral Coefficients (MFCCs)</span> are the most adopted ones <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib41" title="">41</a>]</cite>, either directly used as input of the classifier or further processed before subsequent steps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib43" title="">43</a>]</cite>.
At the same time, spectral-based features are directly computed from the frequency representation of the signal and have proved their effectiveness in the model identification task in several studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib27" title="">27</a>]</cite>.
For instance, the authors of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib35" title="">35</a>]</cite> explore the smartphone model identification task by employing the power spectrum of the recorded signals and feeding it to several supervised machine learning algorithms. Finally, problem-specific features have also been proposed.
These are not based on standard acoustic aspects but are designed to highlight some characteristics of the audio content that are helpful for the task at hand.
The authors of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib46" title="">46</a>]</cite> propose features that exploit the information related to encoding, while others leverage different entropy measures to increase their robustness <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib39" title="">39</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">After the feature extraction phase, the classification step can be implemented using any supervised learning technique.
For instance, the authors of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib49" title="">49</a>]</cite> use a <span class="ltx_glossaryref" title="">Support Vector Machine (SVM)</span> to classify non-linear features, while those of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib50" title="">50</a>]</cite> leverage <span class="ltx_glossaryref" title="">Gaussian Mixture Models (GMMs)</span>.
The first systems proposed for this task were based on classical machine learning approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib48" title="">48</a>]</cite>.
However, most recent methods have applied <span class="ltx_glossaryref" title="">Deep Learning (DL)</span> to the recording smartphone model identification tasks, developing powerful representations able to extract abstract and complex features from the input data.
In this context, <span class="ltx_glossaryref" title="">CNNs</span> have been employed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib52" title="">52</a>]</cite> on spectral-based features, while the authors of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib30" title="">30</a>]</cite> add an attention mechanism on a time-frequency representation of the recording, to emphasize the most informative frequency bands.
Deep autoencoder networks have been used to extract intrinsic signatures of the smartphones in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib43" title="">43</a>]</cite>, performing unsupervised clustering to aggregate recordings from the same microphone model. ResNet architecture has been used in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib32" title="">32</a>]</cite>, applying it to <span class="ltx_glossaryref" title="">MFCCs</span> extracted from speech-free segments of the recording. The authors of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib40" title="">40</a>]</cite> use a Bidirectional-LSTM architecture on cepstrum-based features sequentially extracted from the recording, considering the spatial and temporal information simultaneously.
Finally, a <span class="ltx_glossaryref" title="">Neural Network (NN)</span> has been also applied in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib11" title="">11</a>]</cite> to mitigate the effect of a noise-injection counter-forensic attack, comparing the benefits of this approach on three state-of-the-art cepstrum and spectral-based features.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Dataset Acquisition</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we provide all the information about POLIPHONE, the dataset we introduce and release in this paper.
First, we illustrate the structure of the recorded corpus.
Then, we present the smartphones and the recording setup we considered.
Finally, we describe the post-processing operations applied to the acquired tracks and detail the released data.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Recording Setup</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">One of the main aspects we considered when creating the POLIPHONE dataset was to ensure consistent acquisition conditions for all the smartphone models.
We did so for two main reasons. First, we wanted the differences between the tracks of the various smartphones to be solely due to their recording pipeline and not to external factors. This makes the audio data more trustworthy from a forensics point of view.
Second, fully controlling the acquisition conditions will allow us, in the future, to perform new recordings and expand the released dataset by integrating new models.
This is a crucial aspect in the model identification field as new smartphones are presented more and more frequently, and having a dataset that can be enlarged over time enables train classifiers on more comprehensive and updated data, leading to performances that are in line with the real-world technologies.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.5">To achieve this goal, we organized the recording session in an anechoic environment, compliant with the ISO3745 standard <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib54" title="">54</a>]</cite>, playing back the tracks from a Genelec 8020C loudspeaker amplified by a Focusrite Scarlett 2i2 soundcard.
We ensured that the signals were stored in their raw form, capturing mono audio directly from the internal microphone of the models (the one at the bottom edge of the smartphone) without including any software post-processing or compression.
This approach was adopted to prevent the presence of additional software operations that could bias the recorded tracks.
The recordings were performed using the <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.5.1">Auphonic</span> app, which is available for both Android and iOS devices.
All the data were acquired with a sampling frequency <math alttext="f_{\text{s}}=" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><msub id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2.2" xref="S3.SS1.p2.1.m1.1.1.2.2.cmml">f</mi><mtext id="S3.SS1.p2.1.m1.1.1.2.3" xref="S3.SS1.p2.1.m1.1.1.2.3a.cmml">s</mtext></msub><mo id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">=</mo><mi id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><eq id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"></eq><apply id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.2.1.cmml" xref="S3.SS1.p2.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2.2">𝑓</ci><ci id="S3.SS1.p2.1.m1.1.1.2.3a.cmml" xref="S3.SS1.p2.1.m1.1.1.2.3"><mtext id="S3.SS1.p2.1.m1.1.1.2.3.cmml" mathsize="70%" xref="S3.SS1.p2.1.m1.1.1.2.3">s</mtext></ci></apply><csymbol cd="latexml" id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">f_{\text{s}}=</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_f start_POSTSUBSCRIPT s end_POSTSUBSCRIPT =</annotation></semantics></math> <math alttext="44.1\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.3"><semantics id="S3.SS1.p2.2.m2.3a"><mrow id="S3.SS1.p2.2.m2.3.3" xref="S3.SS1.p2.2.m2.3.3.cmml"><mn id="S3.SS1.p2.2.m2.1.1.1.1.1.1" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1.cmml">44.1</mn><mtext id="S3.SS1.p2.2.m2.2.2.2.2.2.2" xref="S3.SS1.p2.2.m2.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S3.SS1.p2.2.m2.3.3.3.3.3.3" xref="S3.SS1.p2.2.m2.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.3b"><apply id="S3.SS1.p2.2.m2.3.3.cmml" xref="S3.SS1.p2.2.m2.3.3"><csymbol cd="latexml" id="S3.SS1.p2.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.2.m2.2.2.2.2.2.2">times</csymbol><cn id="S3.SS1.p2.2.m2.1.1.1.1.1.1.cmml" type="float" xref="S3.SS1.p2.2.m2.1.1.1.1.1.1">44.1</cn><csymbol cd="latexml" id="S3.SS1.p2.2.m2.3.3.3.3.3.3.cmml" xref="S3.SS1.p2.2.m2.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.3c">44.1\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.3d">start_ARG 44.1 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math> and were saved in WAV <span class="ltx_glossaryref" title="">Pulse Code Modulation (PCM)</span> encoded format at <math alttext="16\text{\,}\mathrm{bit}" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.3"><semantics id="S3.SS1.p2.3.m3.3a"><mrow id="S3.SS1.p2.3.m3.3.3" xref="S3.SS1.p2.3.m3.3.3.cmml"><mn id="S3.SS1.p2.3.m3.1.1.1.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.cmml">16</mn><mtext id="S3.SS1.p2.3.m3.2.2.2.2.2.2" xref="S3.SS1.p2.3.m3.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S3.SS1.p2.3.m3.3.3.3.3.3.3" xref="S3.SS1.p2.3.m3.3.3.3.3.3.3.cmml">bit</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.3b"><apply id="S3.SS1.p2.3.m3.3.3.cmml" xref="S3.SS1.p2.3.m3.3.3"><csymbol cd="latexml" id="S3.SS1.p2.3.m3.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.3.m3.2.2.2.2.2.2">times</csymbol><cn id="S3.SS1.p2.3.m3.1.1.1.1.1.1.cmml" type="integer" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1">16</cn><csymbol cd="latexml" id="S3.SS1.p2.3.m3.3.3.3.3.3.3.cmml" xref="S3.SS1.p2.3.m3.3.3.3.3.3.3">bit</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.3c">16\text{\,}\mathrm{bit}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.3d">start_ARG 16 end_ARG start_ARG times end_ARG start_ARG roman_bit end_ARG</annotation></semantics></math>.
We acquired the audio tracks considering <math alttext="20" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><mn id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><cn id="S3.SS1.p2.4.m4.1.1.cmml" type="integer" xref="S3.SS1.p2.4.m4.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">20</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">20</annotation></semantics></math> different smartphones, all released from late <math alttext="2018" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><mn id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">2018</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><cn id="S3.SS1.p2.5.m5.1.1.cmml" type="integer" xref="S3.SS1.p2.5.m5.1.1">2018</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">2018</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.1d">2018</annotation></semantics></math> onwards.
<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S3.T2" title="In III-A Recording Setup ‣ III Dataset Acquisition ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">II</span></a> shows the complete list of the considered smartphone models.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.2">We structured the recording sessions in two batches, each involving <math alttext="10" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><mn id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><cn id="S3.SS1.p3.1.m1.1.1.cmml" type="integer" xref="S3.SS1.p3.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">10</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">10</annotation></semantics></math> models, to guarantee that the acquisition conditions were the same for all smartphones and to eliminate differences due their positioning relative to the loudspeaker.
During the sessions, the phones were positioned approximately <math alttext="3\text{\,}\mathrm{m}" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.3"><semantics id="S3.SS1.p3.2.m2.3a"><mrow id="S3.SS1.p3.2.m2.3.3" xref="S3.SS1.p3.2.m2.3.3.cmml"><mn id="S3.SS1.p3.2.m2.1.1.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.cmml">3</mn><mtext id="S3.SS1.p3.2.m2.2.2.2.2.2.2" xref="S3.SS1.p3.2.m2.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S3.SS1.p3.2.m2.3.3.3.3.3.3" mathvariant="normal" xref="S3.SS1.p3.2.m2.3.3.3.3.3.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.3b"><apply id="S3.SS1.p3.2.m2.3.3.cmml" xref="S3.SS1.p3.2.m2.3.3"><csymbol cd="latexml" id="S3.SS1.p3.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS1.p3.2.m2.2.2.2.2.2.2">times</csymbol><cn id="S3.SS1.p3.2.m2.1.1.1.1.1.1.cmml" type="integer" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1">3</cn><csymbol cd="latexml" id="S3.SS1.p3.2.m2.3.3.3.3.3.3.cmml" xref="S3.SS1.p3.2.m2.3.3.3.3.3.3">meter</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.3c">3\text{\,}\mathrm{m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.3d">start_ARG 3 end_ARG start_ARG times end_ARG start_ARG roman_m end_ARG</annotation></semantics></math> away from the loudspeaker on a suspended structure to mitigate unwanted vibrations and ensure far-field recording conditions. <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S3.F1" title="In III-A Recording Setup ‣ III Dataset Acquisition ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a> shows a picture of the considered recording setup.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>List of the recorded smartphone models.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.1">No.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.2">Model Name</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.3">Brand</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.4">Year</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.1.1">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.1.2">OnePlus Nord 2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.1.3">OnePlus</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.1.4">2021</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.2">
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.2.1">2</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.2.2">Huawei P30</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.2.3">Huawei</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.2.4">2019</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.3">
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.3.1">3</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.3.2">Huawei Nova 9</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.3.3">Huawei</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.4.3.4">2021</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5.4">
<td class="ltx_td ltx_align_center" id="S3.T2.1.5.4.1">4</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.5.4.2">Huawei Nova 9 SE</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.5.4.3">Huawei</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.5.4.4">2021</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.6.5">
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.5.1">5</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.5.2">Motorola Edge 20</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.5.3">Motorola</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.6.5.4">2022</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.7.6">
<td class="ltx_td ltx_align_center" id="S3.T2.1.7.6.1">6</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.7.6.2">Motorola Edge 30</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.7.6.3">Motorola</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.7.6.4">2021</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.8.7">
<td class="ltx_td ltx_align_center" id="S3.T2.1.8.7.1">7</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.8.7.2">Motorola Moto G9 Power</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.8.7.3">Motorola</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.8.7.4">2020</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.9.8">
<td class="ltx_td ltx_align_center" id="S3.T2.1.9.8.1">8</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.9.8.2">Realme GT</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.9.8.3">Realme</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.9.8.4">2021</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.10.9">
<td class="ltx_td ltx_align_center" id="S3.T2.1.10.9.1">9</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.10.9.2">ROG Phone 3</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.10.9.3">Asus</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.10.9.4">2021</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.11.10">
<td class="ltx_td ltx_align_center" id="S3.T2.1.11.10.1">10</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.11.10.2">ROG Phone 5</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.11.10.3">Asus</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.11.10.4">2020</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.12.11">
<td class="ltx_td ltx_align_center" id="S3.T2.1.12.11.1">11</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.12.11.2">RedMagic 6</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.12.11.3">RedMagic</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.12.11.4">2021</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.13.12">
<td class="ltx_td ltx_align_center" id="S3.T2.1.13.12.1">12</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.13.12.2">Xiaomi 12</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.13.12.3">Xiaomi</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.13.12.4">2021</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.14.13">
<td class="ltx_td ltx_align_center" id="S3.T2.1.14.13.1">13</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.14.13.2">Xiaomi 12X</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.14.13.3">Xiaomi</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.14.13.4">2021</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.15.14">
<td class="ltx_td ltx_align_center" id="S3.T2.1.15.14.1">14</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.15.14.2">Xiaomi 12 Pro</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.15.14.3">Xiaomi</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.15.14.4">2021</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.16.15">
<td class="ltx_td ltx_align_center" id="S3.T2.1.16.15.1">15</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.16.15.2">POCO M4 Pro</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.16.15.3">POCO</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.16.15.4">2021</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.17.16">
<td class="ltx_td ltx_align_center" id="S3.T2.1.17.16.1">16</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.17.16.2">POCO X4 Pro</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.17.16.3">POCO</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.17.16.4">2022</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.18.17">
<td class="ltx_td ltx_align_center" id="S3.T2.1.18.17.1">17</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.18.17.2">Redmi Note 11</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.18.17.3">Redmi</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.18.17.4">2022</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.19.18">
<td class="ltx_td ltx_align_center" id="S3.T2.1.19.18.1">18</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.19.18.2">iPhone XS Max</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.19.18.3">Apple</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.19.18.4">2018</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.20.19">
<td class="ltx_td ltx_align_center" id="S3.T2.1.20.19.1">19</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.20.19.2">iPhone 12 mini</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.20.19.3">Apple</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.20.19.4">2020</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.21.20">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.21.20.1">20</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.21.20.2">iPhone 13</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.21.20.3">Apple</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.21.20.4">2021</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="376" id="S3.F1.g1" src="extracted/5911313/figures/recording_setup.jpeg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Recording setup used during the acquisition of the dataset tracks in the anechoic environment.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Recorded tracks</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The purpose of the proposed dataset is to provide a valuable corpus for the recording microphone model identification task in the case of smartphones.
As discussed in <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S2" title="II Background ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">II</span></a>, multiple studies have been carried out in this field, and most focus on signals containing speech.
Although speech signals are commonly acquired using models such as smartphones, testing the developed algorithms solely on such data is somehow reductive as it does not sample the generalization capabilities of the algorithms on different audio domains.
Also, mobile phone microphones are often optimized to work well on voice frequencies, as these are the most common ones they could encounter.
Analyzing the behavior of the microphones on full-spectrum signals could yield valuable insights for a more comprehensive analysis.
To make the released data as complete as possible, we recorded the audio tracks considering several scenarios, increasing the number of possible studies that can be performed.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">The categories of audio signals that we recorded are: (i) speech, (ii) music, and (iii) environmental sounds, resulting in almost <math alttext="150" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mn id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">150</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><cn id="S3.SS2.p2.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p2.1.m1.1.1">150</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">150</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">150</annotation></semantics></math> minutes of audio data for each smartphone.
Additionally, we recorded a chirp signal, which can be used to extract the <span class="ltx_glossaryref" title="">IR</span> of the single microphones and perform more in-depth studies.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.3">Regarding speech, we considered the audio tracks from the VCTK corpus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib55" title="">55</a>]</cite> and we recorded data from <math alttext="30" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><mn id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><cn id="S3.SS2.p3.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p3.1.m1.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">30</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">30</annotation></semantics></math> different speakers, with each speaker contributing for approximately <math alttext="1" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><mn id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><cn id="S3.SS2.p3.2.m2.1.1.cmml" type="integer" xref="S3.SS2.p3.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">1</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">1</annotation></semantics></math> minute of recordings.
We acquired the first <math alttext="13" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.1"><semantics id="S3.SS2.p3.3.m3.1a"><mn id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">13</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><cn id="S3.SS2.p3.3.m3.1.1.cmml" type="integer" xref="S3.SS2.p3.3.m3.1.1">13</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">13</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.1d">13</annotation></semantics></math> tracks of the corpus for each speaker, ensuring uniformity in the spoken sentences across all of them.
<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S3.T3" title="In III-B Recorded tracks ‣ III Dataset Acquisition ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">III</span></a> shows a list of considered speakers.
Our selection aimed to encompass a scenario as broad as possible, i.e., balancing the number of male and female speakers and considering several different English accents.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.4">In the POLIPHONE dataset, we release a single track for each speaker containing all the <math alttext="13" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><mn id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">13</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><cn id="S3.SS2.p4.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p4.1.m1.1.1">13</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">13</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">13</annotation></semantics></math> utterances mentioned above.
At the beginning of each released track, we recorded <math alttext="5\text{\,}\mathrm{s}" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m2.3"><semantics id="S3.SS2.p4.2.m2.3a"><mrow id="S3.SS2.p4.2.m2.3.3" xref="S3.SS2.p4.2.m2.3.3.cmml"><mn id="S3.SS2.p4.2.m2.1.1.1.1.1.1" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1.cmml">5</mn><mtext id="S3.SS2.p4.2.m2.2.2.2.2.2.2" xref="S3.SS2.p4.2.m2.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S3.SS2.p4.2.m2.3.3.3.3.3.3" mathvariant="normal" xref="S3.SS2.p4.2.m2.3.3.3.3.3.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.3b"><apply id="S3.SS2.p4.2.m2.3.3.cmml" xref="S3.SS2.p4.2.m2.3.3"><csymbol cd="latexml" id="S3.SS2.p4.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS2.p4.2.m2.2.2.2.2.2.2">times</csymbol><cn id="S3.SS2.p4.2.m2.1.1.1.1.1.1.cmml" type="integer" xref="S3.SS2.p4.2.m2.1.1.1.1.1.1">5</cn><csymbol cd="latexml" id="S3.SS2.p4.2.m2.3.3.3.3.3.3.cmml" xref="S3.SS2.p4.2.m2.3.3.3.3.3.3">second</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.3c">5\text{\,}\mathrm{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.2.m2.3d">start_ARG 5 end_ARG start_ARG times end_ARG start_ARG roman_s end_ARG</annotation></semantics></math> of silence, which can be helpful in computing the microphone noise floor, as requested in some state-of-the-art studies.
The total length of the recorded speech signals is approx. <math alttext="40" class="ltx_Math" display="inline" id="S3.SS2.p4.3.m3.1"><semantics id="S3.SS2.p4.3.m3.1a"><mn id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><cn id="S3.SS2.p4.3.m3.1.1.cmml" type="integer" xref="S3.SS2.p4.3.m3.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">40</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.3.m3.1d">40</annotation></semantics></math> minutes.
Additionally, we include a <span class="ltx_text ltx_font_italic" id="S3.SS2.p4.4.1">txt</span> file containing the text transcriptions of the considered utterances.
We recorded the speech signals in both clean and noisy conditions, injecting noise into the audio tracks before their acquisition.
This is paramount as this kind of noise is difficult to recreate through post-processing operations. However, it is a compelling use case to analyze, as it may influence some of the factors leveraged by the detectors to generate their predictions.
In particular, we recorded the same data of the clean speech case by injecting <span class="ltx_glossaryref" title="">Additive White Gaussian Noise (AWGN)</span> with a <span class="ltx_glossaryref" title="">Signal-to-noise ratio (SNR)</span> of <math alttext="30\text{\,}\mathrm{dB}" class="ltx_Math" display="inline" id="S3.SS2.p4.4.m4.3"><semantics id="S3.SS2.p4.4.m4.3a"><mrow id="S3.SS2.p4.4.m4.3.3" xref="S3.SS2.p4.4.m4.3.3.cmml"><mn id="S3.SS2.p4.4.m4.1.1.1.1.1.1" xref="S3.SS2.p4.4.m4.1.1.1.1.1.1.cmml">30</mn><mtext id="S3.SS2.p4.4.m4.2.2.2.2.2.2" xref="S3.SS2.p4.4.m4.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S3.SS2.p4.4.m4.3.3.3.3.3.3" xref="S3.SS2.p4.4.m4.3.3.3.3.3.3.cmml">dB</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.3b"><apply id="S3.SS2.p4.4.m4.3.3.cmml" xref="S3.SS2.p4.4.m4.3.3"><csymbol cd="latexml" id="S3.SS2.p4.4.m4.2.2.2.2.2.2.cmml" xref="S3.SS2.p4.4.m4.2.2.2.2.2.2">times</csymbol><cn id="S3.SS2.p4.4.m4.1.1.1.1.1.1.cmml" type="integer" xref="S3.SS2.p4.4.m4.1.1.1.1.1.1">30</cn><csymbol cd="latexml" id="S3.SS2.p4.4.m4.3.3.3.3.3.3.cmml" xref="S3.SS2.p4.4.m4.3.3.3.3.3.3">decibel</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.3c">30\text{\,}\mathrm{dB}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.4.m4.3d">start_ARG 30 end_ARG start_ARG times end_ARG start_ARG roman_dB end_ARG</annotation></semantics></math>.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>List of the considered speakers from the VCTK corpus.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_minipage ltx_align_middle ltx_transformed_outer" id="S3.T3.1" style="width:212.5pt;height:598.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(112.5pt,-155.3pt) scale(2.07818705535149,2.07818705535149) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T3.1.1.1.1.1">No.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.2">VCTK ID</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.3">Gender</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.4">Accent</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T3.1.1.2.1.1">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.2.1.2">P258</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.2.1.3">M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.2.1.4">English</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.1.1.3.2.1">2</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.3.2.2">P256</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.3.2.3">M</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.3.2.4">English</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.1.1.4.3.1">3</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.4.3.2">P232</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.4.3.3">M</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.4.3.4">English</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.1.1.5.4.1">4</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.5.4.2">P243</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.5.4.3">M</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.5.4.4">English</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.1.1.6.5.1">5</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.6.5.2">P254</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.6.5.3">M</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.6.5.4">English</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.1.1.7.6.1">6</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.7.6.2">P311</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.7.6.3">M</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.7.6.4">American</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.1.1.8.7.1">7</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.8.7.2">P334</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.8.7.3">M</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.8.7.4">American</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.9.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.1.1.9.8.1">8</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.9.8.2">P345</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.9.8.3">M</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.9.8.4">American</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.10.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.1.1.10.9.1">9</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.10.9.2">P360</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.10.9.3">M</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.10.9.4">American</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.11.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.1.1.11.10.1">10</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.11.10.2">P316</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.11.10.3">M</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.11.10.4">Canadian</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.12.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.1.1.12.11.1">11</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.12.11.2">P347</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.12.11.3">M</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.12.11.4">South African</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.13.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.1.1.13.12.1">12</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.13.12.2">P304</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.13.12.3">M</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.13.12.4">Northern Irish</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.14.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.1.1.14.13.1">13</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.14.13.2">P245</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.14.13.3">M</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.14.13.4">Irish</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.15.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.1.1.15.14.1">14</th>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.15.14.2">P237</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.15.14.3">M</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.1.15.14.4">Scottish</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.16.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T3.1.1.16.15.1">15</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.16.15.2">P251</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.16.15.3">M</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.16.15.4">Indian</td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" id="S3.T3.2" style="width:212.5pt;">
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T3.2.1" style="width:433.6pt;height:598.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(112.5pt,-155.3pt) scale(2.07818705535149,2.07818705535149) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T3.2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.2.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T3.2.1.1.1.1.1">No.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.1.1.1.1.2">VCTK ID</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.1.1.1.1.3">Gender</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.2.1.1.1.1.4">Accent</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.2.1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T3.2.1.1.2.1.1">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.1.1.2.1.2">P225</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.1.1.2.1.3">F</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.2.1.1.2.1.4">English</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.1.1.3.2.1">2</th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.3.2.2">P228</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.3.2.3">F</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.3.2.4">English</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.1.1.4.3.1">3</th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.4.3.2">P229</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.4.3.3">F</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.4.3.4">English</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.1.1.5.4.1">4</th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.5.4.2">P239</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.5.4.3">F</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.5.4.4">English</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.1.1.6.5.1">5</th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.6.5.2">P240</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.6.5.3">F</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.6.5.4">English</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.1.1.7.6.1">6</th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.7.6.2">P294</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.7.6.3">F</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.7.6.4">American</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.1.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.1.1.8.7.1">7</th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.8.7.2">P297</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.8.7.3">F</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.8.7.4">American</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.1.9.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.1.1.9.8.1">8</th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.9.8.2">P299</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.9.8.3">F</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.9.8.4">American</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.1.10.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.1.1.10.9.1">9</th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.10.9.2">P300</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.10.9.3">F</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.10.9.4">American</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.1.11.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.1.1.11.10.1">10</th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.11.10.2">P303</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.11.10.3">F</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.11.10.4">Canadian</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.1.12.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.1.1.12.11.1">11</th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.12.11.2">P314</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.12.11.3">F</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.12.11.4">South African</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.1.13.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.1.1.13.12.1">12</th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.13.12.2">P238</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.13.12.3">F</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.13.12.4">Northern Irish</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.1.14.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.1.1.14.13.1">13</th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.14.13.2">P266</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.14.13.3">F</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.14.13.4">Irish</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.1.15.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T3.2.1.1.15.14.1">14</th>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.15.14.2">P262</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.15.14.3">F</td>
<td class="ltx_td ltx_align_center" id="S3.T3.2.1.1.15.14.4">Scottish</td>
</tr>
<tr class="ltx_tr" id="S3.T3.2.1.1.16.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T3.2.1.1.16.15.1">15</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.1.1.16.15.2">P248</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.1.1.16.15.3">F</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.2.1.1.16.15.4">Indian</td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
</div>
</figure>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.2">As for the music tracks, we recorded the entire ORCHSET dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib56" title="">56</a>]</cite>, which includes <math alttext="64" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1.1"><semantics id="S3.SS2.p5.1.m1.1a"><mn id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><cn id="S3.SS2.p5.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p5.1.m1.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">64</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.1.m1.1d">64</annotation></semantics></math> audio excerpts of symphonic music, for a total of approximately <math alttext="23" class="ltx_Math" display="inline" id="S3.SS2.p5.2.m2.1"><semantics id="S3.SS2.p5.2.m2.1a"><mn id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml">23</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><cn id="S3.SS2.p5.2.m2.1.1.cmml" type="integer" xref="S3.SS2.p5.2.m2.1.1">23</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">23</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.2.m2.1d">23</annotation></semantics></math> minutes.</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.5">Finally, for environmental sounds, we utilized the ESC dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib57" title="">57</a>]</cite>, which is a collection of <math alttext="5" class="ltx_Math" display="inline" id="S3.SS2.p6.1.m1.1"><semantics id="S3.SS2.p6.1.m1.1a"><mn id="S3.SS2.p6.1.m1.1.1" xref="S3.SS2.p6.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.1b"><cn id="S3.SS2.p6.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p6.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.1c">5</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.1.m1.1d">5</annotation></semantics></math>-second-long tracks that include sound events.
In particular, we recorded the complete ESC-10 subset, which comprises <math alttext="400" class="ltx_Math" display="inline" id="S3.SS2.p6.2.m2.1"><semantics id="S3.SS2.p6.2.m2.1a"><mn id="S3.SS2.p6.2.m2.1.1" xref="S3.SS2.p6.2.m2.1.1.cmml">400</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.2.m2.1b"><cn id="S3.SS2.p6.2.m2.1.1.cmml" type="integer" xref="S3.SS2.p6.2.m2.1.1">400</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.2.m2.1c">400</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.2.m2.1d">400</annotation></semantics></math> labeled environmental recordings (<math alttext="10" class="ltx_Math" display="inline" id="S3.SS2.p6.3.m3.1"><semantics id="S3.SS2.p6.3.m3.1a"><mn id="S3.SS2.p6.3.m3.1.1" xref="S3.SS2.p6.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.3.m3.1b"><cn id="S3.SS2.p6.3.m3.1.1.cmml" type="integer" xref="S3.SS2.p6.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.3.m3.1c">10</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.3.m3.1d">10</annotation></semantics></math> classes, <math alttext="40" class="ltx_Math" display="inline" id="S3.SS2.p6.4.m4.1"><semantics id="S3.SS2.p6.4.m4.1a"><mn id="S3.SS2.p6.4.m4.1.1" xref="S3.SS2.p6.4.m4.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.4.m4.1b"><cn id="S3.SS2.p6.4.m4.1.1.cmml" type="integer" xref="S3.SS2.p6.4.m4.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.4.m4.1c">40</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.4.m4.1d">40</annotation></semantics></math> clips per class), for a total of approximately <math alttext="35" class="ltx_Math" display="inline" id="S3.SS2.p6.5.m5.1"><semantics id="S3.SS2.p6.5.m5.1a"><mn id="S3.SS2.p6.5.m5.1.1" xref="S3.SS2.p6.5.m5.1.1.cmml">35</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.5.m5.1b"><cn id="S3.SS2.p6.5.m5.1.1.cmml" type="integer" xref="S3.SS2.p6.5.m5.1.1">35</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.5.m5.1c">35</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.5.m5.1d">35</annotation></semantics></math> minutes of recorded signals.</p>
</div>
<div class="ltx_para" id="S3.SS2.p7">
<p class="ltx_p" id="S3.SS2.p7.1">For further details about these datasets, we refer the reader to the respective papers.
We release the recordings of both music and environmental tracks, adopting the same organization as the original sets to facilitate their use.
Since the frequency bands covered by these tracks are remarkably different from those of speech, their exploration in the recording model identification scenario could be fascinating.</p>
</div>
<div class="ltx_para" id="S3.SS2.p8">
<p class="ltx_p" id="S3.SS2.p8.5">As mentioned above, we also recorded a chirp signal with each smartphone.
We considered a sine sweep lasting <math alttext="5\text{\,}\mathrm{s}" class="ltx_Math" display="inline" id="S3.SS2.p8.1.m1.3"><semantics id="S3.SS2.p8.1.m1.3a"><mrow id="S3.SS2.p8.1.m1.3.3" xref="S3.SS2.p8.1.m1.3.3.cmml"><mn id="S3.SS2.p8.1.m1.1.1.1.1.1.1" xref="S3.SS2.p8.1.m1.1.1.1.1.1.1.cmml">5</mn><mtext id="S3.SS2.p8.1.m1.2.2.2.2.2.2" xref="S3.SS2.p8.1.m1.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S3.SS2.p8.1.m1.3.3.3.3.3.3" mathvariant="normal" xref="S3.SS2.p8.1.m1.3.3.3.3.3.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.1.m1.3b"><apply id="S3.SS2.p8.1.m1.3.3.cmml" xref="S3.SS2.p8.1.m1.3.3"><csymbol cd="latexml" id="S3.SS2.p8.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS2.p8.1.m1.2.2.2.2.2.2">times</csymbol><cn id="S3.SS2.p8.1.m1.1.1.1.1.1.1.cmml" type="integer" xref="S3.SS2.p8.1.m1.1.1.1.1.1.1">5</cn><csymbol cd="latexml" id="S3.SS2.p8.1.m1.3.3.3.3.3.3.cmml" xref="S3.SS2.p8.1.m1.3.3.3.3.3.3">second</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.1.m1.3c">5\text{\,}\mathrm{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.1.m1.3d">start_ARG 5 end_ARG start_ARG times end_ARG start_ARG roman_s end_ARG</annotation></semantics></math>, ranging from <math alttext="20" class="ltx_Math" display="inline" id="S3.SS2.p8.2.m2.1"><semantics id="S3.SS2.p8.2.m2.1a"><mn id="S3.SS2.p8.2.m2.1.1" xref="S3.SS2.p8.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.2.m2.1b"><cn id="S3.SS2.p8.2.m2.1.1.cmml" type="integer" xref="S3.SS2.p8.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.2.m2.1c">20</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.2.m2.1d">20</annotation></semantics></math> to <math alttext="20\,000\text{\,}\mathrm{Hz}" class="ltx_Math" display="inline" id="S3.SS2.p8.3.m3.3"><semantics id="S3.SS2.p8.3.m3.3a"><mrow id="S3.SS2.p8.3.m3.3.3" xref="S3.SS2.p8.3.m3.3.3.cmml"><mn id="S3.SS2.p8.3.m3.1.1.1.1.1.1" xref="S3.SS2.p8.3.m3.1.1.1.1.1.1.cmml">20 000</mn><mtext id="S3.SS2.p8.3.m3.2.2.2.2.2.2" xref="S3.SS2.p8.3.m3.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S3.SS2.p8.3.m3.3.3.3.3.3.3" xref="S3.SS2.p8.3.m3.3.3.3.3.3.3.cmml">Hz</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.3.m3.3b"><apply id="S3.SS2.p8.3.m3.3.3.cmml" xref="S3.SS2.p8.3.m3.3.3"><csymbol cd="latexml" id="S3.SS2.p8.3.m3.2.2.2.2.2.2.cmml" xref="S3.SS2.p8.3.m3.2.2.2.2.2.2">times</csymbol><cn id="S3.SS2.p8.3.m3.1.1.1.1.1.1.cmml" type="integer" xref="S3.SS2.p8.3.m3.1.1.1.1.1.1">20000</cn><csymbol cd="latexml" id="S3.SS2.p8.3.m3.3.3.3.3.3.3.cmml" xref="S3.SS2.p8.3.m3.3.3.3.3.3.3">hertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.3.m3.3c">20\,000\text{\,}\mathrm{Hz}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.3.m3.3d">start_ARG 20 000 end_ARG start_ARG times end_ARG start_ARG roman_Hz end_ARG</annotation></semantics></math> with a sampling frequency <math alttext="f_{\text{s}}=" class="ltx_Math" display="inline" id="S3.SS2.p8.4.m4.1"><semantics id="S3.SS2.p8.4.m4.1a"><mrow id="S3.SS2.p8.4.m4.1.1" xref="S3.SS2.p8.4.m4.1.1.cmml"><msub id="S3.SS2.p8.4.m4.1.1.2" xref="S3.SS2.p8.4.m4.1.1.2.cmml"><mi id="S3.SS2.p8.4.m4.1.1.2.2" xref="S3.SS2.p8.4.m4.1.1.2.2.cmml">f</mi><mtext id="S3.SS2.p8.4.m4.1.1.2.3" xref="S3.SS2.p8.4.m4.1.1.2.3a.cmml">s</mtext></msub><mo id="S3.SS2.p8.4.m4.1.1.1" xref="S3.SS2.p8.4.m4.1.1.1.cmml">=</mo><mi id="S3.SS2.p8.4.m4.1.1.3" xref="S3.SS2.p8.4.m4.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.4.m4.1b"><apply id="S3.SS2.p8.4.m4.1.1.cmml" xref="S3.SS2.p8.4.m4.1.1"><eq id="S3.SS2.p8.4.m4.1.1.1.cmml" xref="S3.SS2.p8.4.m4.1.1.1"></eq><apply id="S3.SS2.p8.4.m4.1.1.2.cmml" xref="S3.SS2.p8.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p8.4.m4.1.1.2.1.cmml" xref="S3.SS2.p8.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS2.p8.4.m4.1.1.2.2.cmml" xref="S3.SS2.p8.4.m4.1.1.2.2">𝑓</ci><ci id="S3.SS2.p8.4.m4.1.1.2.3a.cmml" xref="S3.SS2.p8.4.m4.1.1.2.3"><mtext id="S3.SS2.p8.4.m4.1.1.2.3.cmml" mathsize="70%" xref="S3.SS2.p8.4.m4.1.1.2.3">s</mtext></ci></apply><csymbol cd="latexml" id="S3.SS2.p8.4.m4.1.1.3.cmml" xref="S3.SS2.p8.4.m4.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.4.m4.1c">f_{\text{s}}=</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.4.m4.1d">italic_f start_POSTSUBSCRIPT s end_POSTSUBSCRIPT =</annotation></semantics></math> <math alttext="44.1\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S3.SS2.p8.5.m5.3"><semantics id="S3.SS2.p8.5.m5.3a"><mrow id="S3.SS2.p8.5.m5.3.3" xref="S3.SS2.p8.5.m5.3.3.cmml"><mn id="S3.SS2.p8.5.m5.1.1.1.1.1.1" xref="S3.SS2.p8.5.m5.1.1.1.1.1.1.cmml">44.1</mn><mtext id="S3.SS2.p8.5.m5.2.2.2.2.2.2" xref="S3.SS2.p8.5.m5.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S3.SS2.p8.5.m5.3.3.3.3.3.3" xref="S3.SS2.p8.5.m5.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.5.m5.3b"><apply id="S3.SS2.p8.5.m5.3.3.cmml" xref="S3.SS2.p8.5.m5.3.3"><csymbol cd="latexml" id="S3.SS2.p8.5.m5.2.2.2.2.2.2.cmml" xref="S3.SS2.p8.5.m5.2.2.2.2.2.2">times</csymbol><cn id="S3.SS2.p8.5.m5.1.1.1.1.1.1.cmml" type="float" xref="S3.SS2.p8.5.m5.1.1.1.1.1.1">44.1</cn><csymbol cd="latexml" id="S3.SS2.p8.5.m5.3.3.3.3.3.3.cmml" xref="S3.SS2.p8.5.m5.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.5.m5.3c">44.1\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.5.m5.3d">start_ARG 44.1 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math>.
We can use these recordings to extract the <span class="ltx_glossaryref" title="">IR</span> of each model’s microphone.
The <span class="ltx_glossaryref" title="">IRs</span> not only provide information about the microphones themselves but also allow to perform several experiments.
For instance, we can convolve a clean audio signal with the <span class="ltx_glossaryref" title="">IR</span> of a specific smartphone, making the track sound as if it was recorded by that specific smartphone.</p>
</div>
<div class="ltx_para" id="S3.SS2.p9">
<p class="ltx_p" id="S3.SS2.p9.1">To compute the <span class="ltx_glossaryref" title="">IR</span> of a microphone, we need to convolve its recorded sweep with the inverse of the original sweep <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib58" title="">58</a>]</cite>. This operation is performed in the frequency domain, where it corresponds to a simple multiplication.
After the convolution, we consider only the real part of the result and assume only the first half of its samples, which correspond to the linear <span class="ltx_glossaryref" title="">IR</span> of the microphone.
Finally, the <span class="ltx_glossaryref" title="">IR</span> is ready to use, allowing us to convolve it with a given audio track to produce the same signal, but as if it had been captured by the specified microphone.
The computation of the <span class="ltx_glossaryref" title="">IRs</span> is possible thanks to the acquisition setup we considered.
In fact, given the controlled and anechoic conditions in which the tracks were recorded, we can consider the contribution of the recording environment as null and the impulse response as due solely to the model’s microphone.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Post-processing operations</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Although we considered the same acquisition setup across all the smartphones, the recorded tracks exhibit distinct characteristics among models.
In particular, both dynamic range and response at distinct frequencies are prone to high variations across different smartphones.
The cause of this phenomenon likely lies in the diverse hardware configurations and acquisition pipelines employed by individual smartphones, leading to easily distinguishable recorded signals (cf. <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S4" title="IV Dataset Analysis ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">IV</span></a>).
In order to make the recording smartphone model identification task more challenging within the POLIPHONE dataset, we aim to mitigate variations between tracks from different devices by conducting post-processing on the recorded data.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.5">The post-processing pipeline we propose involves three steps.
First, we standardize the signals to have zero mean and unit standard deviation, ensuring that the signals are on a comparable scale.
Then, we multiply each signal by a <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.5.1">tanh</span> function, aiming to reduce biases in the dynamic range of the signals <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib59" title="">59</a>]</cite>.
This operation is particularly beneficial when dealing with data exhibiting extreme values or widely varying magnitudes.
Finally, we resample the data from <math alttext="f_{\text{s}}=" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><msub id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2.2" xref="S3.SS3.p2.1.m1.1.1.2.2.cmml">f</mi><mtext id="S3.SS3.p2.1.m1.1.1.2.3" xref="S3.SS3.p2.1.m1.1.1.2.3a.cmml">s</mtext></msub><mo id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">=</mo><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><eq id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></eq><apply id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.2.1.cmml" xref="S3.SS3.p2.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2.2">𝑓</ci><ci id="S3.SS3.p2.1.m1.1.1.2.3a.cmml" xref="S3.SS3.p2.1.m1.1.1.2.3"><mtext id="S3.SS3.p2.1.m1.1.1.2.3.cmml" mathsize="70%" xref="S3.SS3.p2.1.m1.1.1.2.3">s</mtext></ci></apply><csymbol cd="latexml" id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">f_{\text{s}}=</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">italic_f start_POSTSUBSCRIPT s end_POSTSUBSCRIPT =</annotation></semantics></math> <math alttext="44.1\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.3"><semantics id="S3.SS3.p2.2.m2.3a"><mrow id="S3.SS3.p2.2.m2.3.3" xref="S3.SS3.p2.2.m2.3.3.cmml"><mn id="S3.SS3.p2.2.m2.1.1.1.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.1.1.1.cmml">44.1</mn><mtext id="S3.SS3.p2.2.m2.2.2.2.2.2.2" xref="S3.SS3.p2.2.m2.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S3.SS3.p2.2.m2.3.3.3.3.3.3" xref="S3.SS3.p2.2.m2.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.3b"><apply id="S3.SS3.p2.2.m2.3.3.cmml" xref="S3.SS3.p2.2.m2.3.3"><csymbol cd="latexml" id="S3.SS3.p2.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS3.p2.2.m2.2.2.2.2.2.2">times</csymbol><cn id="S3.SS3.p2.2.m2.1.1.1.1.1.1.cmml" type="float" xref="S3.SS3.p2.2.m2.1.1.1.1.1.1">44.1</cn><csymbol cd="latexml" id="S3.SS3.p2.2.m2.3.3.3.3.3.3.cmml" xref="S3.SS3.p2.2.m2.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.3c">44.1\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.3d">start_ARG 44.1 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math> to <math alttext="f_{\text{s}}=" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.1"><semantics id="S3.SS3.p2.3.m3.1a"><mrow id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><msub id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml"><mi id="S3.SS3.p2.3.m3.1.1.2.2" xref="S3.SS3.p2.3.m3.1.1.2.2.cmml">f</mi><mtext id="S3.SS3.p2.3.m3.1.1.2.3" xref="S3.SS3.p2.3.m3.1.1.2.3a.cmml">s</mtext></msub><mo id="S3.SS3.p2.3.m3.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.cmml">=</mo><mi id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><eq id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1"></eq><apply id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.2.1.cmml" xref="S3.SS3.p2.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS3.p2.3.m3.1.1.2.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2.2">𝑓</ci><ci id="S3.SS3.p2.3.m3.1.1.2.3a.cmml" xref="S3.SS3.p2.3.m3.1.1.2.3"><mtext id="S3.SS3.p2.3.m3.1.1.2.3.cmml" mathsize="70%" xref="S3.SS3.p2.3.m3.1.1.2.3">s</mtext></ci></apply><csymbol cd="latexml" id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">f_{\text{s}}=</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m3.1d">italic_f start_POSTSUBSCRIPT s end_POSTSUBSCRIPT =</annotation></semantics></math> <math alttext="16\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S3.SS3.p2.4.m4.3"><semantics id="S3.SS3.p2.4.m4.3a"><mrow id="S3.SS3.p2.4.m4.3.3" xref="S3.SS3.p2.4.m4.3.3.cmml"><mn id="S3.SS3.p2.4.m4.1.1.1.1.1.1" xref="S3.SS3.p2.4.m4.1.1.1.1.1.1.cmml">16</mn><mtext id="S3.SS3.p2.4.m4.2.2.2.2.2.2" xref="S3.SS3.p2.4.m4.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S3.SS3.p2.4.m4.3.3.3.3.3.3" xref="S3.SS3.p2.4.m4.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.3b"><apply id="S3.SS3.p2.4.m4.3.3.cmml" xref="S3.SS3.p2.4.m4.3.3"><csymbol cd="latexml" id="S3.SS3.p2.4.m4.2.2.2.2.2.2.cmml" xref="S3.SS3.p2.4.m4.2.2.2.2.2.2">times</csymbol><cn id="S3.SS3.p2.4.m4.1.1.1.1.1.1.cmml" type="integer" xref="S3.SS3.p2.4.m4.1.1.1.1.1.1">16</cn><csymbol cd="latexml" id="S3.SS3.p2.4.m4.3.3.3.3.3.3.cmml" xref="S3.SS3.p2.4.m4.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.3c">16\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.4.m4.3d">start_ARG 16 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math>.
While acknowledging the non-linear nature of the <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.5.2">tanh</span> transformation, we believe that its use is necessary in our scenario. Specific models show recorded signals with notably different dynamic ranges compared to other devices, leading to highly distinguishable patterns.
By applying a <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.5.3">tanh</span> transform, signal values are compressed towards the boundaries of the range <math alttext="[-1,1]" class="ltx_Math" display="inline" id="S3.SS3.p2.5.m5.2"><semantics id="S3.SS3.p2.5.m5.2a"><mrow id="S3.SS3.p2.5.m5.2.2.1" xref="S3.SS3.p2.5.m5.2.2.2.cmml"><mo id="S3.SS3.p2.5.m5.2.2.1.2" stretchy="false" xref="S3.SS3.p2.5.m5.2.2.2.cmml">[</mo><mrow id="S3.SS3.p2.5.m5.2.2.1.1" xref="S3.SS3.p2.5.m5.2.2.1.1.cmml"><mo id="S3.SS3.p2.5.m5.2.2.1.1a" xref="S3.SS3.p2.5.m5.2.2.1.1.cmml">−</mo><mn id="S3.SS3.p2.5.m5.2.2.1.1.2" xref="S3.SS3.p2.5.m5.2.2.1.1.2.cmml">1</mn></mrow><mo id="S3.SS3.p2.5.m5.2.2.1.3" xref="S3.SS3.p2.5.m5.2.2.2.cmml">,</mo><mn id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml">1</mn><mo id="S3.SS3.p2.5.m5.2.2.1.4" stretchy="false" xref="S3.SS3.p2.5.m5.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.2b"><interval closure="closed" id="S3.SS3.p2.5.m5.2.2.2.cmml" xref="S3.SS3.p2.5.m5.2.2.1"><apply id="S3.SS3.p2.5.m5.2.2.1.1.cmml" xref="S3.SS3.p2.5.m5.2.2.1.1"><minus id="S3.SS3.p2.5.m5.2.2.1.1.1.cmml" xref="S3.SS3.p2.5.m5.2.2.1.1"></minus><cn id="S3.SS3.p2.5.m5.2.2.1.1.2.cmml" type="integer" xref="S3.SS3.p2.5.m5.2.2.1.1.2">1</cn></apply><cn id="S3.SS3.p2.5.m5.1.1.cmml" type="integer" xref="S3.SS3.p2.5.m5.1.1">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.2c">[-1,1]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.5.m5.2d">[ - 1 , 1 ]</annotation></semantics></math>, aligning the dynamic ranges of the signals more closely. This adjustment significantly minimizes disparities between signals from different models and enhances overall uniformity in our dataset.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.2">Formally, starting from a recorded signal <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">𝐱</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">𝐱</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">bold_x</annotation></semantics></math>, we compute its post-processed version <math alttext="\mathbf{x}_{\text{proc}}" class="ltx_Math" display="inline" id="S3.SS3.p3.2.m2.1"><semantics id="S3.SS3.p3.2.m2.1a"><msub id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">𝐱</mi><mtext id="S3.SS3.p3.2.m2.1.1.3" xref="S3.SS3.p3.2.m2.1.1.3a.cmml">proc</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2">𝐱</ci><ci id="S3.SS3.p3.2.m2.1.1.3a.cmml" xref="S3.SS3.p3.2.m2.1.1.3"><mtext id="S3.SS3.p3.2.m2.1.1.3.cmml" mathsize="70%" xref="S3.SS3.p3.2.m2.1.1.3">proc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">\mathbf{x}_{\text{proc}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.2.m2.1d">bold_x start_POSTSUBSCRIPT proc end_POSTSUBSCRIPT</annotation></semantics></math> as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{x}_{\text{proc}}=\tanh\left(\frac{\mathbf{x}-\mu(\mathbf{x})}{\sigma(%
\mathbf{x})}\right)," class="ltx_Math" display="block" id="S3.E1.m1.4"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4.1" xref="S3.E1.m1.4.4.1.1.cmml"><mrow id="S3.E1.m1.4.4.1.1" xref="S3.E1.m1.4.4.1.1.cmml"><msub id="S3.E1.m1.4.4.1.1.2" xref="S3.E1.m1.4.4.1.1.2.cmml"><mi id="S3.E1.m1.4.4.1.1.2.2" xref="S3.E1.m1.4.4.1.1.2.2.cmml">𝐱</mi><mtext id="S3.E1.m1.4.4.1.1.2.3" xref="S3.E1.m1.4.4.1.1.2.3a.cmml">proc</mtext></msub><mo id="S3.E1.m1.4.4.1.1.1" xref="S3.E1.m1.4.4.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.4.4.1.1.3.2" xref="S3.E1.m1.4.4.1.1.3.1.cmml"><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">tanh</mi><mo id="S3.E1.m1.4.4.1.1.3.2a" xref="S3.E1.m1.4.4.1.1.3.1.cmml">⁡</mo><mrow id="S3.E1.m1.4.4.1.1.3.2.1" xref="S3.E1.m1.4.4.1.1.3.1.cmml"><mo id="S3.E1.m1.4.4.1.1.3.2.1.1" xref="S3.E1.m1.4.4.1.1.3.1.cmml">(</mo><mfrac id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml">𝐱</mi><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">−</mo><mrow id="S3.E1.m1.1.1.1.4" xref="S3.E1.m1.1.1.1.4.cmml"><mi id="S3.E1.m1.1.1.1.4.2" xref="S3.E1.m1.1.1.1.4.2.cmml">μ</mi><mo id="S3.E1.m1.1.1.1.4.1" xref="S3.E1.m1.1.1.1.4.1.cmml">⁢</mo><mrow id="S3.E1.m1.1.1.1.4.3.2" xref="S3.E1.m1.1.1.1.4.cmml"><mo id="S3.E1.m1.1.1.1.4.3.2.1" stretchy="false" xref="S3.E1.m1.1.1.1.4.cmml">(</mo><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">𝐱</mi><mo id="S3.E1.m1.1.1.1.4.3.2.2" stretchy="false" xref="S3.E1.m1.1.1.1.4.cmml">)</mo></mrow></mrow></mrow><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">σ</mi><mo id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">⁢</mo><mrow id="S3.E1.m1.2.2.2.4.2" xref="S3.E1.m1.2.2.2.cmml"><mo id="S3.E1.m1.2.2.2.4.2.1" stretchy="false" xref="S3.E1.m1.2.2.2.cmml">(</mo><mi id="S3.E1.m1.2.2.2.1" xref="S3.E1.m1.2.2.2.1.cmml">𝐱</mi><mo id="S3.E1.m1.2.2.2.4.2.2" stretchy="false" xref="S3.E1.m1.2.2.2.cmml">)</mo></mrow></mrow></mfrac><mo id="S3.E1.m1.4.4.1.1.3.2.1.2" xref="S3.E1.m1.4.4.1.1.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.4.4.1.2" xref="S3.E1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.1.1.cmml" xref="S3.E1.m1.4.4.1"><eq id="S3.E1.m1.4.4.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1"></eq><apply id="S3.E1.m1.4.4.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.2.1.cmml" xref="S3.E1.m1.4.4.1.1.2">subscript</csymbol><ci id="S3.E1.m1.4.4.1.1.2.2.cmml" xref="S3.E1.m1.4.4.1.1.2.2">𝐱</ci><ci id="S3.E1.m1.4.4.1.1.2.3a.cmml" xref="S3.E1.m1.4.4.1.1.2.3"><mtext id="S3.E1.m1.4.4.1.1.2.3.cmml" mathsize="70%" xref="S3.E1.m1.4.4.1.1.2.3">proc</mtext></ci></apply><apply id="S3.E1.m1.4.4.1.1.3.1.cmml" xref="S3.E1.m1.4.4.1.1.3.2"><tanh id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"></tanh><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><divide id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2"></divide><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><minus id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></minus><ci id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3">𝐱</ci><apply id="S3.E1.m1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.4"><times id="S3.E1.m1.1.1.1.4.1.cmml" xref="S3.E1.m1.1.1.1.4.1"></times><ci id="S3.E1.m1.1.1.1.4.2.cmml" xref="S3.E1.m1.1.1.1.4.2">𝜇</ci><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">𝐱</ci></apply></apply><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><times id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2"></times><ci id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3">𝜎</ci><ci id="S3.E1.m1.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.1">𝐱</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">\mathbf{x}_{\text{proc}}=\tanh\left(\frac{\mathbf{x}-\mu(\mathbf{x})}{\sigma(%
\mathbf{x})}\right),</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.4d">bold_x start_POSTSUBSCRIPT proc end_POSTSUBSCRIPT = roman_tanh ( divide start_ARG bold_x - italic_μ ( bold_x ) end_ARG start_ARG italic_σ ( bold_x ) end_ARG ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p3.4">where <math alttext="\mu(\cdot)" class="ltx_Math" display="inline" id="S3.SS3.p3.3.m1.1"><semantics id="S3.SS3.p3.3.m1.1a"><mrow id="S3.SS3.p3.3.m1.1.2" xref="S3.SS3.p3.3.m1.1.2.cmml"><mi id="S3.SS3.p3.3.m1.1.2.2" xref="S3.SS3.p3.3.m1.1.2.2.cmml">μ</mi><mo id="S3.SS3.p3.3.m1.1.2.1" xref="S3.SS3.p3.3.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.p3.3.m1.1.2.3.2" xref="S3.SS3.p3.3.m1.1.2.cmml"><mo id="S3.SS3.p3.3.m1.1.2.3.2.1" stretchy="false" xref="S3.SS3.p3.3.m1.1.2.cmml">(</mo><mo id="S3.SS3.p3.3.m1.1.1" lspace="0em" rspace="0em" xref="S3.SS3.p3.3.m1.1.1.cmml">⋅</mo><mo id="S3.SS3.p3.3.m1.1.2.3.2.2" stretchy="false" xref="S3.SS3.p3.3.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m1.1b"><apply id="S3.SS3.p3.3.m1.1.2.cmml" xref="S3.SS3.p3.3.m1.1.2"><times id="S3.SS3.p3.3.m1.1.2.1.cmml" xref="S3.SS3.p3.3.m1.1.2.1"></times><ci id="S3.SS3.p3.3.m1.1.2.2.cmml" xref="S3.SS3.p3.3.m1.1.2.2">𝜇</ci><ci id="S3.SS3.p3.3.m1.1.1.cmml" xref="S3.SS3.p3.3.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m1.1c">\mu(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.3.m1.1d">italic_μ ( ⋅ )</annotation></semantics></math> and <math alttext="\sigma(\cdot)" class="ltx_Math" display="inline" id="S3.SS3.p3.4.m2.1"><semantics id="S3.SS3.p3.4.m2.1a"><mrow id="S3.SS3.p3.4.m2.1.2" xref="S3.SS3.p3.4.m2.1.2.cmml"><mi id="S3.SS3.p3.4.m2.1.2.2" xref="S3.SS3.p3.4.m2.1.2.2.cmml">σ</mi><mo id="S3.SS3.p3.4.m2.1.2.1" xref="S3.SS3.p3.4.m2.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.p3.4.m2.1.2.3.2" xref="S3.SS3.p3.4.m2.1.2.cmml"><mo id="S3.SS3.p3.4.m2.1.2.3.2.1" stretchy="false" xref="S3.SS3.p3.4.m2.1.2.cmml">(</mo><mo id="S3.SS3.p3.4.m2.1.1" lspace="0em" rspace="0em" xref="S3.SS3.p3.4.m2.1.1.cmml">⋅</mo><mo id="S3.SS3.p3.4.m2.1.2.3.2.2" stretchy="false" xref="S3.SS3.p3.4.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m2.1b"><apply id="S3.SS3.p3.4.m2.1.2.cmml" xref="S3.SS3.p3.4.m2.1.2"><times id="S3.SS3.p3.4.m2.1.2.1.cmml" xref="S3.SS3.p3.4.m2.1.2.1"></times><ci id="S3.SS3.p3.4.m2.1.2.2.cmml" xref="S3.SS3.p3.4.m2.1.2.2">𝜎</ci><ci id="S3.SS3.p3.4.m2.1.1.cmml" xref="S3.SS3.p3.4.m2.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m2.1c">\sigma(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.4.m2.1d">italic_σ ( ⋅ )</annotation></semantics></math> are the mean and standard deviation operations, respectively.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.5.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.6.2">Released data</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">The POLIPHONE dataset is open-source and can be downloaded at this link<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/records/13903412" title="">https://zenodo.org/records/13903412</a></span></span></span>.
We release the data in three different versions to address various research and application needs. Here are the descriptions of each version:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.2"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.2.1">original_recordings</span>: This partition contains the unaltered, raw audio recordings as captured by the recording smartphone models. These files preserve the original quality and sampling frequency (<math alttext="f_{\text{s}}=" class="ltx_Math" display="inline" id="S3.I1.i1.p1.1.m1.1"><semantics id="S3.I1.i1.p1.1.m1.1a"><mrow id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml"><msub id="S3.I1.i1.p1.1.m1.1.1.2" xref="S3.I1.i1.p1.1.m1.1.1.2.cmml"><mi id="S3.I1.i1.p1.1.m1.1.1.2.2" xref="S3.I1.i1.p1.1.m1.1.1.2.2.cmml">f</mi><mtext id="S3.I1.i1.p1.1.m1.1.1.2.3" xref="S3.I1.i1.p1.1.m1.1.1.2.3a.cmml">s</mtext></msub><mo id="S3.I1.i1.p1.1.m1.1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.1.cmml">=</mo><mi id="S3.I1.i1.p1.1.m1.1.1.3" xref="S3.I1.i1.p1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><apply id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1"><eq id="S3.I1.i1.p1.1.m1.1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1.1"></eq><apply id="S3.I1.i1.p1.1.m1.1.1.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.1.1.2.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.I1.i1.p1.1.m1.1.1.2.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2.2">𝑓</ci><ci id="S3.I1.i1.p1.1.m1.1.1.2.3a.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2.3"><mtext id="S3.I1.i1.p1.1.m1.1.1.2.3.cmml" mathsize="70%" xref="S3.I1.i1.p1.1.m1.1.1.2.3">s</mtext></ci></apply><csymbol cd="latexml" id="S3.I1.i1.p1.1.m1.1.1.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">f_{\text{s}}=</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT s end_POSTSUBSCRIPT =</annotation></semantics></math> <math alttext="44.1\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.2.m2.3"><semantics id="S3.I1.i1.p1.2.m2.3a"><mrow id="S3.I1.i1.p1.2.m2.3.3" xref="S3.I1.i1.p1.2.m2.3.3.cmml"><mn id="S3.I1.i1.p1.2.m2.1.1.1.1.1.1" xref="S3.I1.i1.p1.2.m2.1.1.1.1.1.1.cmml">44.1</mn><mtext id="S3.I1.i1.p1.2.m2.2.2.2.2.2.2" xref="S3.I1.i1.p1.2.m2.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S3.I1.i1.p1.2.m2.3.3.3.3.3.3" xref="S3.I1.i1.p1.2.m2.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.2.m2.3b"><apply id="S3.I1.i1.p1.2.m2.3.3.cmml" xref="S3.I1.i1.p1.2.m2.3.3"><csymbol cd="latexml" id="S3.I1.i1.p1.2.m2.2.2.2.2.2.2.cmml" xref="S3.I1.i1.p1.2.m2.2.2.2.2.2.2">times</csymbol><cn id="S3.I1.i1.p1.2.m2.1.1.1.1.1.1.cmml" type="float" xref="S3.I1.i1.p1.2.m2.1.1.1.1.1.1">44.1</cn><csymbol cd="latexml" id="S3.I1.i1.p1.2.m2.3.3.3.3.3.3.cmml" xref="S3.I1.i1.p1.2.m2.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.2.m2.3c">44.1\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.2.m2.3d">start_ARG 44.1 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math>), making them ideal for those who require the highest fidelity for tasks such as detailed acoustic analysis or audio restoration.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.4"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.4.1">normalized_data</span>: These audio files have been processed following the pipeline described in <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S3.SS3" title="III-C Post-processing operations ‣ III Dataset Acquisition ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>. We release these tracks at both <math alttext="f_{\text{s}}=" class="ltx_Math" display="inline" id="S3.I1.i2.p1.1.m1.1"><semantics id="S3.I1.i2.p1.1.m1.1a"><mrow id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml"><msub id="S3.I1.i2.p1.1.m1.1.1.2" xref="S3.I1.i2.p1.1.m1.1.1.2.cmml"><mi id="S3.I1.i2.p1.1.m1.1.1.2.2" xref="S3.I1.i2.p1.1.m1.1.1.2.2.cmml">f</mi><mtext id="S3.I1.i2.p1.1.m1.1.1.2.3" xref="S3.I1.i2.p1.1.m1.1.1.2.3a.cmml">s</mtext></msub><mo id="S3.I1.i2.p1.1.m1.1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.1.cmml">=</mo><mi id="S3.I1.i2.p1.1.m1.1.1.3" xref="S3.I1.i2.p1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><apply id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1"><eq id="S3.I1.i2.p1.1.m1.1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1.1"></eq><apply id="S3.I1.i2.p1.1.m1.1.1.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.m1.1.1.2.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.I1.i2.p1.1.m1.1.1.2.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2.2">𝑓</ci><ci id="S3.I1.i2.p1.1.m1.1.1.2.3a.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2.3"><mtext id="S3.I1.i2.p1.1.m1.1.1.2.3.cmml" mathsize="70%" xref="S3.I1.i2.p1.1.m1.1.1.2.3">s</mtext></ci></apply><csymbol cd="latexml" id="S3.I1.i2.p1.1.m1.1.1.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">f_{\text{s}}=</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT s end_POSTSUBSCRIPT =</annotation></semantics></math> <math alttext="44.1\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.2.m2.3"><semantics id="S3.I1.i2.p1.2.m2.3a"><mrow id="S3.I1.i2.p1.2.m2.3.3" xref="S3.I1.i2.p1.2.m2.3.3.cmml"><mn id="S3.I1.i2.p1.2.m2.1.1.1.1.1.1" xref="S3.I1.i2.p1.2.m2.1.1.1.1.1.1.cmml">44.1</mn><mtext id="S3.I1.i2.p1.2.m2.2.2.2.2.2.2" xref="S3.I1.i2.p1.2.m2.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S3.I1.i2.p1.2.m2.3.3.3.3.3.3" xref="S3.I1.i2.p1.2.m2.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.3b"><apply id="S3.I1.i2.p1.2.m2.3.3.cmml" xref="S3.I1.i2.p1.2.m2.3.3"><csymbol cd="latexml" id="S3.I1.i2.p1.2.m2.2.2.2.2.2.2.cmml" xref="S3.I1.i2.p1.2.m2.2.2.2.2.2.2">times</csymbol><cn id="S3.I1.i2.p1.2.m2.1.1.1.1.1.1.cmml" type="float" xref="S3.I1.i2.p1.2.m2.1.1.1.1.1.1">44.1</cn><csymbol cd="latexml" id="S3.I1.i2.p1.2.m2.3.3.3.3.3.3.cmml" xref="S3.I1.i2.p1.2.m2.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.3c">44.1\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.2.m2.3d">start_ARG 44.1 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math> and <math alttext="f_{\text{s}}=" class="ltx_Math" display="inline" id="S3.I1.i2.p1.3.m3.1"><semantics id="S3.I1.i2.p1.3.m3.1a"><mrow id="S3.I1.i2.p1.3.m3.1.1" xref="S3.I1.i2.p1.3.m3.1.1.cmml"><msub id="S3.I1.i2.p1.3.m3.1.1.2" xref="S3.I1.i2.p1.3.m3.1.1.2.cmml"><mi id="S3.I1.i2.p1.3.m3.1.1.2.2" xref="S3.I1.i2.p1.3.m3.1.1.2.2.cmml">f</mi><mtext id="S3.I1.i2.p1.3.m3.1.1.2.3" xref="S3.I1.i2.p1.3.m3.1.1.2.3a.cmml">s</mtext></msub><mo id="S3.I1.i2.p1.3.m3.1.1.1" xref="S3.I1.i2.p1.3.m3.1.1.1.cmml">=</mo><mi id="S3.I1.i2.p1.3.m3.1.1.3" xref="S3.I1.i2.p1.3.m3.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.3.m3.1b"><apply id="S3.I1.i2.p1.3.m3.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1"><eq id="S3.I1.i2.p1.3.m3.1.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1.1"></eq><apply id="S3.I1.i2.p1.3.m3.1.1.2.cmml" xref="S3.I1.i2.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.I1.i2.p1.3.m3.1.1.2.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.I1.i2.p1.3.m3.1.1.2.2.cmml" xref="S3.I1.i2.p1.3.m3.1.1.2.2">𝑓</ci><ci id="S3.I1.i2.p1.3.m3.1.1.2.3a.cmml" xref="S3.I1.i2.p1.3.m3.1.1.2.3"><mtext id="S3.I1.i2.p1.3.m3.1.1.2.3.cmml" mathsize="70%" xref="S3.I1.i2.p1.3.m3.1.1.2.3">s</mtext></ci></apply><csymbol cd="latexml" id="S3.I1.i2.p1.3.m3.1.1.3.cmml" xref="S3.I1.i2.p1.3.m3.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.3.m3.1c">f_{\text{s}}=</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.3.m3.1d">italic_f start_POSTSUBSCRIPT s end_POSTSUBSCRIPT =</annotation></semantics></math> <math alttext="16\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.4.m4.3"><semantics id="S3.I1.i2.p1.4.m4.3a"><mrow id="S3.I1.i2.p1.4.m4.3.3" xref="S3.I1.i2.p1.4.m4.3.3.cmml"><mn id="S3.I1.i2.p1.4.m4.1.1.1.1.1.1" xref="S3.I1.i2.p1.4.m4.1.1.1.1.1.1.cmml">16</mn><mtext id="S3.I1.i2.p1.4.m4.2.2.2.2.2.2" xref="S3.I1.i2.p1.4.m4.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S3.I1.i2.p1.4.m4.3.3.3.3.3.3" xref="S3.I1.i2.p1.4.m4.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.4.m4.3b"><apply id="S3.I1.i2.p1.4.m4.3.3.cmml" xref="S3.I1.i2.p1.4.m4.3.3"><csymbol cd="latexml" id="S3.I1.i2.p1.4.m4.2.2.2.2.2.2.cmml" xref="S3.I1.i2.p1.4.m4.2.2.2.2.2.2">times</csymbol><cn id="S3.I1.i2.p1.4.m4.1.1.1.1.1.1.cmml" type="integer" xref="S3.I1.i2.p1.4.m4.1.1.1.1.1.1">16</cn><csymbol cd="latexml" id="S3.I1.i2.p1.4.m4.3.3.3.3.3.3.cmml" xref="S3.I1.i2.p1.4.m4.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.4.m4.3c">16\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.4.m4.3d">start_ARG 16 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.2"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.2.1">convolved_data</span>: This partition contains the clean audio tracks that have been convolved with the <span class="ltx_glossaryref" title="">IRs</span> of specific smartphone models to simulate their acquisition by the microphone of those devices. This version is helpful for testing the robustness of the identification algorithms under more challenging multimedia forensic conditions.
We release these tracks at <math alttext="f_{\text{s}}=" class="ltx_Math" display="inline" id="S3.I1.i3.p1.1.m1.1"><semantics id="S3.I1.i3.p1.1.m1.1a"><mrow id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml"><msub id="S3.I1.i3.p1.1.m1.1.1.2" xref="S3.I1.i3.p1.1.m1.1.1.2.cmml"><mi id="S3.I1.i3.p1.1.m1.1.1.2.2" xref="S3.I1.i3.p1.1.m1.1.1.2.2.cmml">f</mi><mtext id="S3.I1.i3.p1.1.m1.1.1.2.3" xref="S3.I1.i3.p1.1.m1.1.1.2.3a.cmml">s</mtext></msub><mo id="S3.I1.i3.p1.1.m1.1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.1.cmml">=</mo><mi id="S3.I1.i3.p1.1.m1.1.1.3" xref="S3.I1.i3.p1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><apply id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1"><eq id="S3.I1.i3.p1.1.m1.1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1.1"></eq><apply id="S3.I1.i3.p1.1.m1.1.1.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.I1.i3.p1.1.m1.1.1.2.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.I1.i3.p1.1.m1.1.1.2.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2.2">𝑓</ci><ci id="S3.I1.i3.p1.1.m1.1.1.2.3a.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2.3"><mtext id="S3.I1.i3.p1.1.m1.1.1.2.3.cmml" mathsize="70%" xref="S3.I1.i3.p1.1.m1.1.1.2.3">s</mtext></ci></apply><csymbol cd="latexml" id="S3.I1.i3.p1.1.m1.1.1.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">f_{\text{s}}=</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT s end_POSTSUBSCRIPT =</annotation></semantics></math> <math alttext="16\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S3.I1.i3.p1.2.m2.3"><semantics id="S3.I1.i3.p1.2.m2.3a"><mrow id="S3.I1.i3.p1.2.m2.3.3" xref="S3.I1.i3.p1.2.m2.3.3.cmml"><mn id="S3.I1.i3.p1.2.m2.1.1.1.1.1.1" xref="S3.I1.i3.p1.2.m2.1.1.1.1.1.1.cmml">16</mn><mtext id="S3.I1.i3.p1.2.m2.2.2.2.2.2.2" xref="S3.I1.i3.p1.2.m2.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S3.I1.i3.p1.2.m2.3.3.3.3.3.3" xref="S3.I1.i3.p1.2.m2.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.2.m2.3b"><apply id="S3.I1.i3.p1.2.m2.3.3.cmml" xref="S3.I1.i3.p1.2.m2.3.3"><csymbol cd="latexml" id="S3.I1.i3.p1.2.m2.2.2.2.2.2.2.cmml" xref="S3.I1.i3.p1.2.m2.2.2.2.2.2.2">times</csymbol><cn id="S3.I1.i3.p1.2.m2.1.1.1.1.1.1.cmml" type="integer" xref="S3.I1.i3.p1.2.m2.1.1.1.1.1.1">16</cn><csymbol cd="latexml" id="S3.I1.i3.p1.2.m2.3.3.3.3.3.3.cmml" xref="S3.I1.i3.p1.2.m2.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.2.m2.3c">16\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.2.m2.3d">start_ARG 16 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math>.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">In addition to these audio data, we release the chirp signals recorded from the smartphones and the corresponding <span class="ltx_glossaryref" title="">IRs</span> we computed.
We also provide the code used to compute the <span class="ltx_glossaryref" title="">IRs</span> from the recorded tracks in both Matlab and Python<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>
Code will be released upon acceptance.
</span></span></span>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Dataset Analysis</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.2">In this section, we analyze the recorded tracks, highlighting the dissimilarities between signals acquired by smartphones included in this dataset.
As detailed in <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S3.SS1" title="III-A Recording Setup ‣ III Dataset Acquisition ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>, since the recording setup considered is consistent across all the tracks in the dataset, we can effectively compare them with each other and look for dissimilarities between different models.
All the differences identified between the tracks are attributable to distinct acquisition pipelines implemented by the models or the variations in their hardware.
In this case, we consider the original recordings acquired in the anechoic chamber at <math alttext="f_{\text{s}}=" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mrow id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><msub id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml"><mi id="S4.p1.1.m1.1.1.2.2" xref="S4.p1.1.m1.1.1.2.2.cmml">f</mi><mtext id="S4.p1.1.m1.1.1.2.3" xref="S4.p1.1.m1.1.1.2.3a.cmml">s</mtext></msub><mo id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1.cmml">=</mo><mi id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><eq id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1"></eq><apply id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.p1.1.m1.1.1.2.1.cmml" xref="S4.p1.1.m1.1.1.2">subscript</csymbol><ci id="S4.p1.1.m1.1.1.2.2.cmml" xref="S4.p1.1.m1.1.1.2.2">𝑓</ci><ci id="S4.p1.1.m1.1.1.2.3a.cmml" xref="S4.p1.1.m1.1.1.2.3"><mtext id="S4.p1.1.m1.1.1.2.3.cmml" mathsize="70%" xref="S4.p1.1.m1.1.1.2.3">s</mtext></ci></apply><csymbol cd="latexml" id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">f_{\text{s}}=</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT s end_POSTSUBSCRIPT =</annotation></semantics></math> <math alttext="44.1\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S4.p1.2.m2.3"><semantics id="S4.p1.2.m2.3a"><mrow id="S4.p1.2.m2.3.3" xref="S4.p1.2.m2.3.3.cmml"><mn id="S4.p1.2.m2.1.1.1.1.1.1" xref="S4.p1.2.m2.1.1.1.1.1.1.cmml">44.1</mn><mtext id="S4.p1.2.m2.2.2.2.2.2.2" xref="S4.p1.2.m2.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S4.p1.2.m2.3.3.3.3.3.3" xref="S4.p1.2.m2.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.3b"><apply id="S4.p1.2.m2.3.3.cmml" xref="S4.p1.2.m2.3.3"><csymbol cd="latexml" id="S4.p1.2.m2.2.2.2.2.2.2.cmml" xref="S4.p1.2.m2.2.2.2.2.2.2">times</csymbol><cn id="S4.p1.2.m2.1.1.1.1.1.1.cmml" type="float" xref="S4.p1.2.m2.1.1.1.1.1.1">44.1</cn><csymbol cd="latexml" id="S4.p1.2.m2.3.3.3.3.3.3.cmml" xref="S4.p1.2.m2.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.3c">44.1\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.2.m2.3d">start_ARG 44.1 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math>, without applying any post-processing technique (i.e., version 1 of our dataset).</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="202" id="S4.F2.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Waveforms of the sine sweep signals recorded by all the considered smartphone models.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="204" id="S4.F3.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Spectral representations of the Impulse Responses of all the considered smartphone models.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="201" id="S4.F4.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Mel-spectrogram representations of the noisy speech signals recorded by all the considered smartphone models.</figcaption>
</figure>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">First, we focus our analysis on the recordings of sine sweep signals.
We do so since these recordings, although brief, include highly informative content regarding the microphone characteristics of the considered smartphones.
<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S4.F2" title="In IV Dataset Analysis ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> shows the waveforms of the recorded chirps, while <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S4.F3" title="In IV Dataset Analysis ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a> shows the spectra of the <span class="ltx_glossaryref" title="">IRs</span> extracted from them.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">The acquired signals are very different from one model to another, with differences in both the dynamic range and the equalization at different frequencies.
We would expect an ideal acquisition to exhibit a monotonic increase in the signal amplitude over time and an almost flat spectrum for the extracted <span class="ltx_glossaryref" title="">IR</span>, similar to those presented by models like iPhone 12 mini and Motorola G9 Power.
However, many of the devices do not present this behavior.
For instance, phones like Huawei P30 and ROG Phone 5 show an increased amplitude in the frequency bands that are typical of speech.
This behavior may be attributed to the design choices made by the manufacturers, which emphasize certain frequency bands to compensate for the limitations of the microphone hardware.
Indeed, smartphones are often provided with low-cost microphones, whose primary use is acquiring speech signals, whether it is during calls, voice messages, or video recordings. For this reason, it makes sense to emphasize certain frequency bands for better hearing results.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.5">During the analysis, we noticed a particular behavior exhibited by two Xiaomi phones, namely the Xiaomi 12 and Xiaomi 12 Pro.
Despite their capability to capture audio data at <math alttext="44.1\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S4.p4.1.m1.3"><semantics id="S4.p4.1.m1.3a"><mrow id="S4.p4.1.m1.3.3" xref="S4.p4.1.m1.3.3.cmml"><mn id="S4.p4.1.m1.1.1.1.1.1.1" xref="S4.p4.1.m1.1.1.1.1.1.1.cmml">44.1</mn><mtext id="S4.p4.1.m1.2.2.2.2.2.2" xref="S4.p4.1.m1.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S4.p4.1.m1.3.3.3.3.3.3" xref="S4.p4.1.m1.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.3b"><apply id="S4.p4.1.m1.3.3.cmml" xref="S4.p4.1.m1.3.3"><csymbol cd="latexml" id="S4.p4.1.m1.2.2.2.2.2.2.cmml" xref="S4.p4.1.m1.2.2.2.2.2.2">times</csymbol><cn id="S4.p4.1.m1.1.1.1.1.1.1.cmml" type="float" xref="S4.p4.1.m1.1.1.1.1.1.1">44.1</cn><csymbol cd="latexml" id="S4.p4.1.m1.3.3.3.3.3.3.cmml" xref="S4.p4.1.m1.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.3c">44.1\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S4.p4.1.m1.3d">start_ARG 44.1 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math>, these models clip all the content above <math alttext="8\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S4.p4.2.m2.3"><semantics id="S4.p4.2.m2.3a"><mrow id="S4.p4.2.m2.3.3" xref="S4.p4.2.m2.3.3.cmml"><mn id="S4.p4.2.m2.1.1.1.1.1.1" xref="S4.p4.2.m2.1.1.1.1.1.1.cmml">8</mn><mtext id="S4.p4.2.m2.2.2.2.2.2.2" xref="S4.p4.2.m2.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S4.p4.2.m2.3.3.3.3.3.3" xref="S4.p4.2.m2.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.3b"><apply id="S4.p4.2.m2.3.3.cmml" xref="S4.p4.2.m2.3.3"><csymbol cd="latexml" id="S4.p4.2.m2.2.2.2.2.2.2.cmml" xref="S4.p4.2.m2.2.2.2.2.2.2">times</csymbol><cn id="S4.p4.2.m2.1.1.1.1.1.1.cmml" type="integer" xref="S4.p4.2.m2.1.1.1.1.1.1">8</cn><csymbol cd="latexml" id="S4.p4.2.m2.3.3.3.3.3.3.cmml" xref="S4.p4.2.m2.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.3c">8\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S4.p4.2.m2.3d">start_ARG 8 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math>, as if acquiring at <math alttext="f_{\text{s}}=" class="ltx_Math" display="inline" id="S4.p4.3.m3.1"><semantics id="S4.p4.3.m3.1a"><mrow id="S4.p4.3.m3.1.1" xref="S4.p4.3.m3.1.1.cmml"><msub id="S4.p4.3.m3.1.1.2" xref="S4.p4.3.m3.1.1.2.cmml"><mi id="S4.p4.3.m3.1.1.2.2" xref="S4.p4.3.m3.1.1.2.2.cmml">f</mi><mtext id="S4.p4.3.m3.1.1.2.3" xref="S4.p4.3.m3.1.1.2.3a.cmml">s</mtext></msub><mo id="S4.p4.3.m3.1.1.1" xref="S4.p4.3.m3.1.1.1.cmml">=</mo><mi id="S4.p4.3.m3.1.1.3" xref="S4.p4.3.m3.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><apply id="S4.p4.3.m3.1.1.cmml" xref="S4.p4.3.m3.1.1"><eq id="S4.p4.3.m3.1.1.1.cmml" xref="S4.p4.3.m3.1.1.1"></eq><apply id="S4.p4.3.m3.1.1.2.cmml" xref="S4.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.p4.3.m3.1.1.2.1.cmml" xref="S4.p4.3.m3.1.1.2">subscript</csymbol><ci id="S4.p4.3.m3.1.1.2.2.cmml" xref="S4.p4.3.m3.1.1.2.2">𝑓</ci><ci id="S4.p4.3.m3.1.1.2.3a.cmml" xref="S4.p4.3.m3.1.1.2.3"><mtext id="S4.p4.3.m3.1.1.2.3.cmml" mathsize="70%" xref="S4.p4.3.m3.1.1.2.3">s</mtext></ci></apply><csymbol cd="latexml" id="S4.p4.3.m3.1.1.3.cmml" xref="S4.p4.3.m3.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">f_{\text{s}}=</annotation><annotation encoding="application/x-llamapun" id="S4.p4.3.m3.1d">italic_f start_POSTSUBSCRIPT s end_POSTSUBSCRIPT =</annotation></semantics></math> <math alttext="16\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S4.p4.4.m4.3"><semantics id="S4.p4.4.m4.3a"><mrow id="S4.p4.4.m4.3.3" xref="S4.p4.4.m4.3.3.cmml"><mn id="S4.p4.4.m4.1.1.1.1.1.1" xref="S4.p4.4.m4.1.1.1.1.1.1.cmml">16</mn><mtext id="S4.p4.4.m4.2.2.2.2.2.2" xref="S4.p4.4.m4.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S4.p4.4.m4.3.3.3.3.3.3" xref="S4.p4.4.m4.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.4.m4.3b"><apply id="S4.p4.4.m4.3.3.cmml" xref="S4.p4.4.m4.3.3"><csymbol cd="latexml" id="S4.p4.4.m4.2.2.2.2.2.2.cmml" xref="S4.p4.4.m4.2.2.2.2.2.2">times</csymbol><cn id="S4.p4.4.m4.1.1.1.1.1.1.cmml" type="integer" xref="S4.p4.4.m4.1.1.1.1.1.1">16</cn><csymbol cd="latexml" id="S4.p4.4.m4.3.3.3.3.3.3.cmml" xref="S4.p4.4.m4.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.4.m4.3c">16\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S4.p4.4.m4.3d">start_ARG 16 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math>.
This characteristic is even more evident by looking at <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S4.F4" title="In IV Dataset Analysis ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a>, which shows the Mel-spectrogram representations of the noisy speech signals recorded by all the considered smartphones.
In the case of these two models, a black horizontal band shows the lack of content above <math alttext="8\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S4.p4.5.m5.3"><semantics id="S4.p4.5.m5.3a"><mrow id="S4.p4.5.m5.3.3" xref="S4.p4.5.m5.3.3.cmml"><mn id="S4.p4.5.m5.1.1.1.1.1.1" xref="S4.p4.5.m5.1.1.1.1.1.1.cmml">8</mn><mtext id="S4.p4.5.m5.2.2.2.2.2.2" xref="S4.p4.5.m5.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S4.p4.5.m5.3.3.3.3.3.3" xref="S4.p4.5.m5.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.5.m5.3b"><apply id="S4.p4.5.m5.3.3.cmml" xref="S4.p4.5.m5.3.3"><csymbol cd="latexml" id="S4.p4.5.m5.2.2.2.2.2.2.cmml" xref="S4.p4.5.m5.2.2.2.2.2.2">times</csymbol><cn id="S4.p4.5.m5.1.1.1.1.1.1.cmml" type="integer" xref="S4.p4.5.m5.1.1.1.1.1.1">8</cn><csymbol cd="latexml" id="S4.p4.5.m5.3.3.3.3.3.3.cmml" xref="S4.p4.5.m5.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.5.m5.3c">8\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S4.p4.5.m5.3d">start_ARG 8 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math>. Interestingly, this particular behavior is not present in the recordings of Xiaomi 12X, which is produced by the same manufacturer.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">Another notable aspect observed in <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S4.F4" title="In IV Dataset Analysis ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a> belongs to Moto G9 Power.
As the recorded signal is injected with <span class="ltx_glossaryref" title="">AWGN</span>, we expect to observe content across the entire frequency range at each time instant.
However, it seems like this model includes a <span class="ltx_glossaryref" title="">Voice Activity Detector (VAD)</span> mechanism and puts to zero all the time windows where no speech is detected.
This behavior may prove advantageous or disadvantageous, depending on the purpose of the recording.
While it discards content of un-voiced segments, enhancing clarity in acquired speech, it also discards potentially valuable audio information.
As in the previous case, this behavior belongs only to this specific model and is not extended to other smartphones from the same manufacturer (i.e., Motorola Edge 20 and Motorola Edge 30).
We recall that we considered the same recording pipeline for all the models, meaning that these behaviors are likely due to hardware filtering or post-processing operations specific to each model.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Smartphone Model Identification from Audio Recordings</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we benchmark the released dataset on the task of microphone model identification in the case of smartphones. We do so by considering a method proposed in the literature for this specific task and testing it in various scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib14" title="">14</a>]</cite>.
The goal of these experiments is to evaluate the contribution that POLIPHONE can make to state-of-the-art.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.7">The problem we consider is a multi-class classification task in closed-set conditions, and it is formally defined as follows.
Let us consider a discrete-time input speech signal <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S5.p2.1.m1.1"><semantics id="S5.p2.1.m1.1a"><mi id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">𝐱</mi><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><ci id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1">𝐱</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S5.p2.1.m1.1d">bold_x</annotation></semantics></math>, sampled with sampling frequency <math alttext="f_{\text{s}}" class="ltx_Math" display="inline" id="S5.p2.2.m2.1"><semantics id="S5.p2.2.m2.1a"><msub id="S5.p2.2.m2.1.1" xref="S5.p2.2.m2.1.1.cmml"><mi id="S5.p2.2.m2.1.1.2" xref="S5.p2.2.m2.1.1.2.cmml">f</mi><mtext id="S5.p2.2.m2.1.1.3" xref="S5.p2.2.m2.1.1.3a.cmml">s</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.p2.2.m2.1b"><apply id="S5.p2.2.m2.1.1.cmml" xref="S5.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.p2.2.m2.1.1.1.cmml" xref="S5.p2.2.m2.1.1">subscript</csymbol><ci id="S5.p2.2.m2.1.1.2.cmml" xref="S5.p2.2.m2.1.1.2">𝑓</ci><ci id="S5.p2.2.m2.1.1.3a.cmml" xref="S5.p2.2.m2.1.1.3"><mtext id="S5.p2.2.m2.1.1.3.cmml" mathsize="70%" xref="S5.p2.2.m2.1.1.3">s</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.2.m2.1c">f_{\text{s}}</annotation><annotation encoding="application/x-llamapun" id="S5.p2.2.m2.1d">italic_f start_POSTSUBSCRIPT s end_POSTSUBSCRIPT</annotation></semantics></math> and acquired by a smartphone belonging to a set of <math alttext="I" class="ltx_Math" display="inline" id="S5.p2.3.m3.1"><semantics id="S5.p2.3.m3.1a"><mi id="S5.p2.3.m3.1.1" xref="S5.p2.3.m3.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S5.p2.3.m3.1b"><ci id="S5.p2.3.m3.1.1.cmml" xref="S5.p2.3.m3.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.3.m3.1c">I</annotation><annotation encoding="application/x-llamapun" id="S5.p2.3.m3.1d">italic_I</annotation></semantics></math> models <math alttext="y_{i}\in\{y_{0},y_{1},...,y_{I-1}\}" class="ltx_Math" display="inline" id="S5.p2.4.m4.4"><semantics id="S5.p2.4.m4.4a"><mrow id="S5.p2.4.m4.4.4" xref="S5.p2.4.m4.4.4.cmml"><msub id="S5.p2.4.m4.4.4.5" xref="S5.p2.4.m4.4.4.5.cmml"><mi id="S5.p2.4.m4.4.4.5.2" xref="S5.p2.4.m4.4.4.5.2.cmml">y</mi><mi id="S5.p2.4.m4.4.4.5.3" xref="S5.p2.4.m4.4.4.5.3.cmml">i</mi></msub><mo id="S5.p2.4.m4.4.4.4" xref="S5.p2.4.m4.4.4.4.cmml">∈</mo><mrow id="S5.p2.4.m4.4.4.3.3" xref="S5.p2.4.m4.4.4.3.4.cmml"><mo id="S5.p2.4.m4.4.4.3.3.4" stretchy="false" xref="S5.p2.4.m4.4.4.3.4.cmml">{</mo><msub id="S5.p2.4.m4.2.2.1.1.1" xref="S5.p2.4.m4.2.2.1.1.1.cmml"><mi id="S5.p2.4.m4.2.2.1.1.1.2" xref="S5.p2.4.m4.2.2.1.1.1.2.cmml">y</mi><mn id="S5.p2.4.m4.2.2.1.1.1.3" xref="S5.p2.4.m4.2.2.1.1.1.3.cmml">0</mn></msub><mo id="S5.p2.4.m4.4.4.3.3.5" xref="S5.p2.4.m4.4.4.3.4.cmml">,</mo><msub id="S5.p2.4.m4.3.3.2.2.2" xref="S5.p2.4.m4.3.3.2.2.2.cmml"><mi id="S5.p2.4.m4.3.3.2.2.2.2" xref="S5.p2.4.m4.3.3.2.2.2.2.cmml">y</mi><mn id="S5.p2.4.m4.3.3.2.2.2.3" xref="S5.p2.4.m4.3.3.2.2.2.3.cmml">1</mn></msub><mo id="S5.p2.4.m4.4.4.3.3.6" xref="S5.p2.4.m4.4.4.3.4.cmml">,</mo><mi id="S5.p2.4.m4.1.1" mathvariant="normal" xref="S5.p2.4.m4.1.1.cmml">…</mi><mo id="S5.p2.4.m4.4.4.3.3.7" xref="S5.p2.4.m4.4.4.3.4.cmml">,</mo><msub id="S5.p2.4.m4.4.4.3.3.3" xref="S5.p2.4.m4.4.4.3.3.3.cmml"><mi id="S5.p2.4.m4.4.4.3.3.3.2" xref="S5.p2.4.m4.4.4.3.3.3.2.cmml">y</mi><mrow id="S5.p2.4.m4.4.4.3.3.3.3" xref="S5.p2.4.m4.4.4.3.3.3.3.cmml"><mi id="S5.p2.4.m4.4.4.3.3.3.3.2" xref="S5.p2.4.m4.4.4.3.3.3.3.2.cmml">I</mi><mo id="S5.p2.4.m4.4.4.3.3.3.3.1" xref="S5.p2.4.m4.4.4.3.3.3.3.1.cmml">−</mo><mn id="S5.p2.4.m4.4.4.3.3.3.3.3" xref="S5.p2.4.m4.4.4.3.3.3.3.3.cmml">1</mn></mrow></msub><mo id="S5.p2.4.m4.4.4.3.3.8" stretchy="false" xref="S5.p2.4.m4.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.4.m4.4b"><apply id="S5.p2.4.m4.4.4.cmml" xref="S5.p2.4.m4.4.4"><in id="S5.p2.4.m4.4.4.4.cmml" xref="S5.p2.4.m4.4.4.4"></in><apply id="S5.p2.4.m4.4.4.5.cmml" xref="S5.p2.4.m4.4.4.5"><csymbol cd="ambiguous" id="S5.p2.4.m4.4.4.5.1.cmml" xref="S5.p2.4.m4.4.4.5">subscript</csymbol><ci id="S5.p2.4.m4.4.4.5.2.cmml" xref="S5.p2.4.m4.4.4.5.2">𝑦</ci><ci id="S5.p2.4.m4.4.4.5.3.cmml" xref="S5.p2.4.m4.4.4.5.3">𝑖</ci></apply><set id="S5.p2.4.m4.4.4.3.4.cmml" xref="S5.p2.4.m4.4.4.3.3"><apply id="S5.p2.4.m4.2.2.1.1.1.cmml" xref="S5.p2.4.m4.2.2.1.1.1"><csymbol cd="ambiguous" id="S5.p2.4.m4.2.2.1.1.1.1.cmml" xref="S5.p2.4.m4.2.2.1.1.1">subscript</csymbol><ci id="S5.p2.4.m4.2.2.1.1.1.2.cmml" xref="S5.p2.4.m4.2.2.1.1.1.2">𝑦</ci><cn id="S5.p2.4.m4.2.2.1.1.1.3.cmml" type="integer" xref="S5.p2.4.m4.2.2.1.1.1.3">0</cn></apply><apply id="S5.p2.4.m4.3.3.2.2.2.cmml" xref="S5.p2.4.m4.3.3.2.2.2"><csymbol cd="ambiguous" id="S5.p2.4.m4.3.3.2.2.2.1.cmml" xref="S5.p2.4.m4.3.3.2.2.2">subscript</csymbol><ci id="S5.p2.4.m4.3.3.2.2.2.2.cmml" xref="S5.p2.4.m4.3.3.2.2.2.2">𝑦</ci><cn id="S5.p2.4.m4.3.3.2.2.2.3.cmml" type="integer" xref="S5.p2.4.m4.3.3.2.2.2.3">1</cn></apply><ci id="S5.p2.4.m4.1.1.cmml" xref="S5.p2.4.m4.1.1">…</ci><apply id="S5.p2.4.m4.4.4.3.3.3.cmml" xref="S5.p2.4.m4.4.4.3.3.3"><csymbol cd="ambiguous" id="S5.p2.4.m4.4.4.3.3.3.1.cmml" xref="S5.p2.4.m4.4.4.3.3.3">subscript</csymbol><ci id="S5.p2.4.m4.4.4.3.3.3.2.cmml" xref="S5.p2.4.m4.4.4.3.3.3.2">𝑦</ci><apply id="S5.p2.4.m4.4.4.3.3.3.3.cmml" xref="S5.p2.4.m4.4.4.3.3.3.3"><minus id="S5.p2.4.m4.4.4.3.3.3.3.1.cmml" xref="S5.p2.4.m4.4.4.3.3.3.3.1"></minus><ci id="S5.p2.4.m4.4.4.3.3.3.3.2.cmml" xref="S5.p2.4.m4.4.4.3.3.3.3.2">𝐼</ci><cn id="S5.p2.4.m4.4.4.3.3.3.3.3.cmml" type="integer" xref="S5.p2.4.m4.4.4.3.3.3.3.3">1</cn></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.4.m4.4c">y_{i}\in\{y_{0},y_{1},...,y_{I-1}\}</annotation><annotation encoding="application/x-llamapun" id="S5.p2.4.m4.4d">italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ { italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_I - 1 end_POSTSUBSCRIPT }</annotation></semantics></math>.
The goal of this task is to develop a classifier <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S5.p2.5.m5.1"><semantics id="S5.p2.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S5.p2.5.m5.1.1" xref="S5.p2.5.m5.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S5.p2.5.m5.1b"><ci id="S5.p2.5.m5.1.1.cmml" xref="S5.p2.5.m5.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.5.m5.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S5.p2.5.m5.1d">caligraphic_C</annotation></semantics></math> able to predict which model <math alttext="y_{i}" class="ltx_Math" display="inline" id="S5.p2.6.m6.1"><semantics id="S5.p2.6.m6.1a"><msub id="S5.p2.6.m6.1.1" xref="S5.p2.6.m6.1.1.cmml"><mi id="S5.p2.6.m6.1.1.2" xref="S5.p2.6.m6.1.1.2.cmml">y</mi><mi id="S5.p2.6.m6.1.1.3" xref="S5.p2.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.p2.6.m6.1b"><apply id="S5.p2.6.m6.1.1.cmml" xref="S5.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S5.p2.6.m6.1.1.1.cmml" xref="S5.p2.6.m6.1.1">subscript</csymbol><ci id="S5.p2.6.m6.1.1.2.cmml" xref="S5.p2.6.m6.1.1.2">𝑦</ci><ci id="S5.p2.6.m6.1.1.3.cmml" xref="S5.p2.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.6.m6.1c">y_{i}</annotation><annotation encoding="application/x-llamapun" id="S5.p2.6.m6.1d">italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> has been used to record <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S5.p2.7.m7.1"><semantics id="S5.p2.7.m7.1a"><mi id="S5.p2.7.m7.1.1" xref="S5.p2.7.m7.1.1.cmml">𝐱</mi><annotation-xml encoding="MathML-Content" id="S5.p2.7.m7.1b"><ci id="S5.p2.7.m7.1.1.cmml" xref="S5.p2.7.m7.1.1">𝐱</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.7.m7.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S5.p2.7.m7.1d">bold_x</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">The considered classifier <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S5.p3.1.m1.1"><semantics id="S5.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><ci id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S5.p3.1.m1.1d">caligraphic_C</annotation></semantics></math> is a <span class="ltx_glossaryref" title="">CNN</span>-based model which takes as input the logarithmic <span class="ltx_glossaryref" title="">Short-Time Fourier Transform (STFT)</span> of the audio recordings under analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib14" title="">14</a>]</cite>.
For details regarding the model used, please refer to the original paper <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib14" title="">14</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.8">We trained the network for <math alttext="100" class="ltx_Math" display="inline" id="S5.p4.1.m1.1"><semantics id="S5.p4.1.m1.1a"><mn id="S5.p4.1.m1.1.1" xref="S5.p4.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S5.p4.1.m1.1b"><cn id="S5.p4.1.m1.1.1.cmml" type="integer" xref="S5.p4.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.1.m1.1c">100</annotation><annotation encoding="application/x-llamapun" id="S5.p4.1.m1.1d">100</annotation></semantics></math> epochs by monitoring the value of the validation loss.
We assumed <math alttext="10" class="ltx_Math" display="inline" id="S5.p4.2.m2.1"><semantics id="S5.p4.2.m2.1a"><mn id="S5.p4.2.m2.1.1" xref="S5.p4.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.p4.2.m2.1b"><cn id="S5.p4.2.m2.1.1.cmml" type="integer" xref="S5.p4.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.2.m2.1c">10</annotation><annotation encoding="application/x-llamapun" id="S5.p4.2.m2.1d">10</annotation></semantics></math> epochs as early stopping, a batch size of <math alttext="32" class="ltx_Math" display="inline" id="S5.p4.3.m3.1"><semantics id="S5.p4.3.m3.1a"><mn id="S5.p4.3.m3.1.1" xref="S5.p4.3.m3.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S5.p4.3.m3.1b"><cn id="S5.p4.3.m3.1.1.cmml" type="integer" xref="S5.p4.3.m3.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.3.m3.1c">32</annotation><annotation encoding="application/x-llamapun" id="S5.p4.3.m3.1d">32</annotation></semantics></math>, and a learning rate of <math alttext="{10}^{-4}" class="ltx_Math" display="inline" id="S5.p4.4.m4.3"><semantics id="S5.p4.4.m4.3a"><mrow id="S5.p4.4.m4.3.3.3" xref="S5.p4.4.m4.3.3.3.cmml"><mi id="S5.p4.4.m4.1.1.1.1.1.1.1" xref="S5.p4.4.m4.3.3.3.cmml"></mi><mo id="S5.p4.4.m4.2.2.2.2.2.2.2" xref="S5.p4.4.m4.3.3.3.cmml">⁢</mo><msup id="S5.p4.4.m4.3.3.3.3.3.3.3" xref="S5.p4.4.m4.3.3.3.cmml"><mn id="S5.p4.4.m4.3.3.3.3.3.3.3.2" xref="S5.p4.4.m4.3.3.3.cmml">10</mn><mrow id="S5.p4.4.m4.3.3.3.3.3.3.3.3.2" xref="S5.p4.4.m4.3.3.3.cmml"><mo id="S5.p4.4.m4.3.3.3.3.3.3.3.3.2a" xref="S5.p4.4.m4.3.3.3.cmml">−</mo><mn id="S5.p4.4.m4.3.3.3.3.3.3.3.3.2.2" xref="S5.p4.4.m4.3.3.3.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.4.m4.3b"><csymbol cd="latexml" id="S5.p4.4.m4.3.3.3.cmml" xref="S5.p4.4.m4.3.3.3">E-4</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.4.m4.3c">{10}^{-4}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.4.m4.3d">start_ARG end_ARG start_ARG ⁢ end_ARG start_ARG power start_ARG 10 end_ARG start_ARG - 4 end_ARG end_ARG</annotation></semantics></math> appropriately reduced on plateau.
We assumed Cross Entropy as loss function and RMSProp as optimizer.
The input of the network consisted of log-spectrograms derived from <math alttext="1\text{\,}\mathrm{s}" class="ltx_Math" display="inline" id="S5.p4.5.m5.3"><semantics id="S5.p4.5.m5.3a"><mrow id="S5.p4.5.m5.3.3" xref="S5.p4.5.m5.3.3.cmml"><mn id="S5.p4.5.m5.1.1.1.1.1.1" xref="S5.p4.5.m5.1.1.1.1.1.1.cmml">1</mn><mtext id="S5.p4.5.m5.2.2.2.2.2.2" xref="S5.p4.5.m5.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S5.p4.5.m5.3.3.3.3.3.3" mathvariant="normal" xref="S5.p4.5.m5.3.3.3.3.3.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.5.m5.3b"><apply id="S5.p4.5.m5.3.3.cmml" xref="S5.p4.5.m5.3.3"><csymbol cd="latexml" id="S5.p4.5.m5.2.2.2.2.2.2.cmml" xref="S5.p4.5.m5.2.2.2.2.2.2">times</csymbol><cn id="S5.p4.5.m5.1.1.1.1.1.1.cmml" type="integer" xref="S5.p4.5.m5.1.1.1.1.1.1">1</cn><csymbol cd="latexml" id="S5.p4.5.m5.3.3.3.3.3.3.cmml" xref="S5.p4.5.m5.3.3.3.3.3.3">second</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.5.m5.3c">1\text{\,}\mathrm{s}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.5.m5.3d">start_ARG 1 end_ARG start_ARG times end_ARG start_ARG roman_s end_ARG</annotation></semantics></math> audio windows, with a configuration of <math alttext="N_{\text{FFT}}=2048" class="ltx_Math" display="inline" id="S5.p4.6.m6.1"><semantics id="S5.p4.6.m6.1a"><mrow id="S5.p4.6.m6.1.1" xref="S5.p4.6.m6.1.1.cmml"><msub id="S5.p4.6.m6.1.1.2" xref="S5.p4.6.m6.1.1.2.cmml"><mi id="S5.p4.6.m6.1.1.2.2" xref="S5.p4.6.m6.1.1.2.2.cmml">N</mi><mtext id="S5.p4.6.m6.1.1.2.3" xref="S5.p4.6.m6.1.1.2.3a.cmml">FFT</mtext></msub><mo id="S5.p4.6.m6.1.1.1" xref="S5.p4.6.m6.1.1.1.cmml">=</mo><mn id="S5.p4.6.m6.1.1.3" xref="S5.p4.6.m6.1.1.3.cmml">2048</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.6.m6.1b"><apply id="S5.p4.6.m6.1.1.cmml" xref="S5.p4.6.m6.1.1"><eq id="S5.p4.6.m6.1.1.1.cmml" xref="S5.p4.6.m6.1.1.1"></eq><apply id="S5.p4.6.m6.1.1.2.cmml" xref="S5.p4.6.m6.1.1.2"><csymbol cd="ambiguous" id="S5.p4.6.m6.1.1.2.1.cmml" xref="S5.p4.6.m6.1.1.2">subscript</csymbol><ci id="S5.p4.6.m6.1.1.2.2.cmml" xref="S5.p4.6.m6.1.1.2.2">𝑁</ci><ci id="S5.p4.6.m6.1.1.2.3a.cmml" xref="S5.p4.6.m6.1.1.2.3"><mtext id="S5.p4.6.m6.1.1.2.3.cmml" mathsize="70%" xref="S5.p4.6.m6.1.1.2.3">FFT</mtext></ci></apply><cn id="S5.p4.6.m6.1.1.3.cmml" type="integer" xref="S5.p4.6.m6.1.1.3">2048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.6.m6.1c">N_{\text{FFT}}=2048</annotation><annotation encoding="application/x-llamapun" id="S5.p4.6.m6.1d">italic_N start_POSTSUBSCRIPT FFT end_POSTSUBSCRIPT = 2048</annotation></semantics></math> points and non-overlapping Hanning windows. Each track of the dataset is represented by <math alttext="15" class="ltx_Math" display="inline" id="S5.p4.7.m7.1"><semantics id="S5.p4.7.m7.1a"><mn id="S5.p4.7.m7.1.1" xref="S5.p4.7.m7.1.1.cmml">15</mn><annotation-xml encoding="MathML-Content" id="S5.p4.7.m7.1b"><cn id="S5.p4.7.m7.1.1.cmml" type="integer" xref="S5.p4.7.m7.1.1">15</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.7.m7.1c">15</annotation><annotation encoding="application/x-llamapun" id="S5.p4.7.m7.1d">15</annotation></semantics></math> windows, covering the initial <math alttext="15\text{\,}\mathrm{s}" class="ltx_Math" display="inline" id="S5.p4.8.m8.3"><semantics id="S5.p4.8.m8.3a"><mrow id="S5.p4.8.m8.3.3" xref="S5.p4.8.m8.3.3.cmml"><mn id="S5.p4.8.m8.1.1.1.1.1.1" xref="S5.p4.8.m8.1.1.1.1.1.1.cmml">15</mn><mtext id="S5.p4.8.m8.2.2.2.2.2.2" xref="S5.p4.8.m8.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S5.p4.8.m8.3.3.3.3.3.3" mathvariant="normal" xref="S5.p4.8.m8.3.3.3.3.3.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.8.m8.3b"><apply id="S5.p4.8.m8.3.3.cmml" xref="S5.p4.8.m8.3.3"><csymbol cd="latexml" id="S5.p4.8.m8.2.2.2.2.2.2.cmml" xref="S5.p4.8.m8.2.2.2.2.2.2">times</csymbol><cn id="S5.p4.8.m8.1.1.1.1.1.1.cmml" type="integer" xref="S5.p4.8.m8.1.1.1.1.1.1">15</cn><csymbol cd="latexml" id="S5.p4.8.m8.3.3.3.3.3.3.cmml" xref="S5.p4.8.m8.3.3.3.3.3.3">second</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.8.m8.3c">15\text{\,}\mathrm{s}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.8.m8.3d">start_ARG 15 end_ARG start_ARG times end_ARG start_ARG roman_s end_ARG</annotation></semantics></math>. Longer tracks are trimmed, while shorter ones are replicated to match the desired length.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Clean Speech experiments</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.6">As a first experiment, we evaluate the considered baseline on the clean speech recordings of the POLIPHONE dataset.
We do so as this represents the most common scenario for the recording smartphone model identification task, and the baseline was specifically designed for this purpose.
Specifically, we consider the <em class="ltx_emph ltx_font_italic" id="S5.SS1.p1.6.1">normalized_data</em> partition of the dataset, which enhances the uniformity across all the classes (cf. <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S3.SS3" title="III-C Post-processing operations ‣ III Dataset Acquisition ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>).
We examine two different variants of this subset: the original release, which has a sampling frequency <math alttext="f_{\text{s}}=" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><mrow id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><msub id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml"><mi id="S5.SS1.p1.1.m1.1.1.2.2" xref="S5.SS1.p1.1.m1.1.1.2.2.cmml">f</mi><mtext id="S5.SS1.p1.1.m1.1.1.2.3" xref="S5.SS1.p1.1.m1.1.1.2.3a.cmml">s</mtext></msub><mo id="S5.SS1.p1.1.m1.1.1.1" xref="S5.SS1.p1.1.m1.1.1.1.cmml">=</mo><mi id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><eq id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1"></eq><apply id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.1.1.2.1.cmml" xref="S5.SS1.p1.1.m1.1.1.2">subscript</csymbol><ci id="S5.SS1.p1.1.m1.1.1.2.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2.2">𝑓</ci><ci id="S5.SS1.p1.1.m1.1.1.2.3a.cmml" xref="S5.SS1.p1.1.m1.1.1.2.3"><mtext id="S5.SS1.p1.1.m1.1.1.2.3.cmml" mathsize="70%" xref="S5.SS1.p1.1.m1.1.1.2.3">s</mtext></ci></apply><csymbol cd="latexml" id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">f_{\text{s}}=</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT s end_POSTSUBSCRIPT =</annotation></semantics></math> <math alttext="44.1\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S5.SS1.p1.2.m2.3"><semantics id="S5.SS1.p1.2.m2.3a"><mrow id="S5.SS1.p1.2.m2.3.3" xref="S5.SS1.p1.2.m2.3.3.cmml"><mn id="S5.SS1.p1.2.m2.1.1.1.1.1.1" xref="S5.SS1.p1.2.m2.1.1.1.1.1.1.cmml">44.1</mn><mtext id="S5.SS1.p1.2.m2.2.2.2.2.2.2" xref="S5.SS1.p1.2.m2.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S5.SS1.p1.2.m2.3.3.3.3.3.3" xref="S5.SS1.p1.2.m2.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.3b"><apply id="S5.SS1.p1.2.m2.3.3.cmml" xref="S5.SS1.p1.2.m2.3.3"><csymbol cd="latexml" id="S5.SS1.p1.2.m2.2.2.2.2.2.2.cmml" xref="S5.SS1.p1.2.m2.2.2.2.2.2.2">times</csymbol><cn id="S5.SS1.p1.2.m2.1.1.1.1.1.1.cmml" type="float" xref="S5.SS1.p1.2.m2.1.1.1.1.1.1">44.1</cn><csymbol cd="latexml" id="S5.SS1.p1.2.m2.3.3.3.3.3.3.cmml" xref="S5.SS1.p1.2.m2.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.3c">44.1\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.2.m2.3d">start_ARG 44.1 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math> and a resampled version at <math alttext="f_{\text{s}}=" class="ltx_Math" display="inline" id="S5.SS1.p1.3.m3.1"><semantics id="S5.SS1.p1.3.m3.1a"><mrow id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml"><msub id="S5.SS1.p1.3.m3.1.1.2" xref="S5.SS1.p1.3.m3.1.1.2.cmml"><mi id="S5.SS1.p1.3.m3.1.1.2.2" xref="S5.SS1.p1.3.m3.1.1.2.2.cmml">f</mi><mtext id="S5.SS1.p1.3.m3.1.1.2.3" xref="S5.SS1.p1.3.m3.1.1.2.3a.cmml">s</mtext></msub><mo id="S5.SS1.p1.3.m3.1.1.1" xref="S5.SS1.p1.3.m3.1.1.1.cmml">=</mo><mi id="S5.SS1.p1.3.m3.1.1.3" xref="S5.SS1.p1.3.m3.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><apply id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1"><eq id="S5.SS1.p1.3.m3.1.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1.1"></eq><apply id="S5.SS1.p1.3.m3.1.1.2.cmml" xref="S5.SS1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p1.3.m3.1.1.2.1.cmml" xref="S5.SS1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S5.SS1.p1.3.m3.1.1.2.2.cmml" xref="S5.SS1.p1.3.m3.1.1.2.2">𝑓</ci><ci id="S5.SS1.p1.3.m3.1.1.2.3a.cmml" xref="S5.SS1.p1.3.m3.1.1.2.3"><mtext id="S5.SS1.p1.3.m3.1.1.2.3.cmml" mathsize="70%" xref="S5.SS1.p1.3.m3.1.1.2.3">s</mtext></ci></apply><csymbol cd="latexml" id="S5.SS1.p1.3.m3.1.1.3.cmml" xref="S5.SS1.p1.3.m3.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">f_{\text{s}}=</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.3.m3.1d">italic_f start_POSTSUBSCRIPT s end_POSTSUBSCRIPT =</annotation></semantics></math> <math alttext="16\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S5.SS1.p1.4.m4.3"><semantics id="S5.SS1.p1.4.m4.3a"><mrow id="S5.SS1.p1.4.m4.3.3" xref="S5.SS1.p1.4.m4.3.3.cmml"><mn id="S5.SS1.p1.4.m4.1.1.1.1.1.1" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1.cmml">16</mn><mtext id="S5.SS1.p1.4.m4.2.2.2.2.2.2" xref="S5.SS1.p1.4.m4.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S5.SS1.p1.4.m4.3.3.3.3.3.3" xref="S5.SS1.p1.4.m4.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.4.m4.3b"><apply id="S5.SS1.p1.4.m4.3.3.cmml" xref="S5.SS1.p1.4.m4.3.3"><csymbol cd="latexml" id="S5.SS1.p1.4.m4.2.2.2.2.2.2.cmml" xref="S5.SS1.p1.4.m4.2.2.2.2.2.2">times</csymbol><cn id="S5.SS1.p1.4.m4.1.1.1.1.1.1.cmml" type="integer" xref="S5.SS1.p1.4.m4.1.1.1.1.1.1">16</cn><csymbol cd="latexml" id="S5.SS1.p1.4.m4.3.3.3.3.3.3.cmml" xref="S5.SS1.p1.4.m4.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.4.m4.3c">16\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.4.m4.3d">start_ARG 16 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math>.
This experiment aims to benchmark the dataset and to verify whether the most critical information for this task resides in the <math alttext="0" class="ltx_Math" display="inline" id="S5.SS1.p1.5.m5.1"><semantics id="S5.SS1.p1.5.m5.1a"><mn id="S5.SS1.p1.5.m5.1.1" xref="S5.SS1.p1.5.m5.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.5.m5.1b"><cn id="S5.SS1.p1.5.m5.1.1.cmml" type="integer" xref="S5.SS1.p1.5.m5.1.1">0</cn></annotation-xml></semantics></math>-<math alttext="8\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S5.SS1.p1.6.m6.3"><semantics id="S5.SS1.p1.6.m6.3a"><mrow id="S5.SS1.p1.6.m6.3.3" xref="S5.SS1.p1.6.m6.3.3.cmml"><mn id="S5.SS1.p1.6.m6.1.1.1.1.1.1" xref="S5.SS1.p1.6.m6.1.1.1.1.1.1.cmml">8</mn><mtext id="S5.SS1.p1.6.m6.2.2.2.2.2.2" xref="S5.SS1.p1.6.m6.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S5.SS1.p1.6.m6.3.3.3.3.3.3" xref="S5.SS1.p1.6.m6.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.6.m6.3b"><apply id="S5.SS1.p1.6.m6.3.3.cmml" xref="S5.SS1.p1.6.m6.3.3"><csymbol cd="latexml" id="S5.SS1.p1.6.m6.2.2.2.2.2.2.cmml" xref="S5.SS1.p1.6.m6.2.2.2.2.2.2">times</csymbol><cn id="S5.SS1.p1.6.m6.1.1.1.1.1.1.cmml" type="integer" xref="S5.SS1.p1.6.m6.1.1.1.1.1.1">8</cn><csymbol cd="latexml" id="S5.SS1.p1.6.m6.3.3.3.3.3.3.cmml" xref="S5.SS1.p1.6.m6.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.6.m6.3c">8\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.6.m6.3d">start_ARG 8 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math> frequency band or in higher frequencies.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.5">In this experiment, we divide the corpus into three subsets (train, validation, and test sets) using proportions of <math alttext="60" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.1"><semantics id="S5.SS1.p2.1.m1.1a"><mn id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><cn id="S5.SS1.p2.1.m1.1.1.cmml" type="integer" xref="S5.SS1.p2.1.m1.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">60</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.1.m1.1d">60</annotation></semantics></math>%, <math alttext="20" class="ltx_Math" display="inline" id="S5.SS1.p2.2.m2.1"><semantics id="S5.SS1.p2.2.m2.1a"><mn id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><cn id="S5.SS1.p2.2.m2.1.1.cmml" type="integer" xref="S5.SS1.p2.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">20</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.2.m2.1d">20</annotation></semantics></math>%, and <math alttext="20" class="ltx_Math" display="inline" id="S5.SS1.p2.3.m3.1"><semantics id="S5.SS1.p2.3.m3.1a"><mn id="S5.SS1.p2.3.m3.1.1" xref="S5.SS1.p2.3.m3.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><cn id="S5.SS1.p2.3.m3.1.1.cmml" type="integer" xref="S5.SS1.p2.3.m3.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">20</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.3.m3.1d">20</annotation></semantics></math>%, respectively. In the case of speech data, we partition the dataset ensuring that speakers are disjointed across the three subsets.
We also train multiple iterations of the classifier, each time utilizing a distinct fraction of the training set, ranging from <math alttext="10" class="ltx_Math" display="inline" id="S5.SS1.p2.4.m4.1"><semantics id="S5.SS1.p2.4.m4.1a"><mn id="S5.SS1.p2.4.m4.1.1" xref="S5.SS1.p2.4.m4.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.4.m4.1b"><cn id="S5.SS1.p2.4.m4.1.1.cmml" type="integer" xref="S5.SS1.p2.4.m4.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.4.m4.1c">10</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.4.m4.1d">10</annotation></semantics></math>% to <math alttext="100" class="ltx_Math" display="inline" id="S5.SS1.p2.5.m5.1"><semantics id="S5.SS1.p2.5.m5.1a"><mn id="S5.SS1.p2.5.m5.1.1" xref="S5.SS1.p2.5.m5.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.5.m5.1b"><cn id="S5.SS1.p2.5.m5.1.1.cmml" type="integer" xref="S5.SS1.p2.5.m5.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.5.m5.1c">100</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.5.m5.1d">100</annotation></semantics></math>%.
This approach allows us to investigate how the classifier’s performance evolves with varying access to training data.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.5"><a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S5.F5" title="In V-A Clean Speech experiments ‣ V Smartphone Model Identification from Audio Recordings ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a> shows the results of this analysis considering balanced accuracy as a metric.
We observe a significant change in performance trends when considering different sampling frequencies.
The classifier trained and tested on <math alttext="44.1\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S5.SS1.p3.1.m1.3"><semantics id="S5.SS1.p3.1.m1.3a"><mrow id="S5.SS1.p3.1.m1.3.3" xref="S5.SS1.p3.1.m1.3.3.cmml"><mn id="S5.SS1.p3.1.m1.1.1.1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.1.1.1.cmml">44.1</mn><mtext id="S5.SS1.p3.1.m1.2.2.2.2.2.2" xref="S5.SS1.p3.1.m1.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S5.SS1.p3.1.m1.3.3.3.3.3.3" xref="S5.SS1.p3.1.m1.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.3b"><apply id="S5.SS1.p3.1.m1.3.3.cmml" xref="S5.SS1.p3.1.m1.3.3"><csymbol cd="latexml" id="S5.SS1.p3.1.m1.2.2.2.2.2.2.cmml" xref="S5.SS1.p3.1.m1.2.2.2.2.2.2">times</csymbol><cn id="S5.SS1.p3.1.m1.1.1.1.1.1.1.cmml" type="float" xref="S5.SS1.p3.1.m1.1.1.1.1.1.1">44.1</cn><csymbol cd="latexml" id="S5.SS1.p3.1.m1.3.3.3.3.3.3.cmml" xref="S5.SS1.p3.1.m1.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.3c">44.1\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.1.m1.3d">start_ARG 44.1 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math> data demonstrates nearly perfect accuracy, reaching almost 100% in all scenarios except when trained on only 10% of the training set, where performance drops by <math alttext="5" class="ltx_Math" display="inline" id="S5.SS1.p3.2.m2.1"><semantics id="S5.SS1.p3.2.m2.1a"><mn id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><cn id="S5.SS1.p3.2.m2.1.1.cmml" type="integer" xref="S5.SS1.p3.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">5</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.2.m2.1d">5</annotation></semantics></math>%. Conversely, the classifier that considers <math alttext="16\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S5.SS1.p3.3.m3.3"><semantics id="S5.SS1.p3.3.m3.3a"><mrow id="S5.SS1.p3.3.m3.3.3" xref="S5.SS1.p3.3.m3.3.3.cmml"><mn id="S5.SS1.p3.3.m3.1.1.1.1.1.1" xref="S5.SS1.p3.3.m3.1.1.1.1.1.1.cmml">16</mn><mtext id="S5.SS1.p3.3.m3.2.2.2.2.2.2" xref="S5.SS1.p3.3.m3.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S5.SS1.p3.3.m3.3.3.3.3.3.3" xref="S5.SS1.p3.3.m3.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.3b"><apply id="S5.SS1.p3.3.m3.3.3.cmml" xref="S5.SS1.p3.3.m3.3.3"><csymbol cd="latexml" id="S5.SS1.p3.3.m3.2.2.2.2.2.2.cmml" xref="S5.SS1.p3.3.m3.2.2.2.2.2.2">times</csymbol><cn id="S5.SS1.p3.3.m3.1.1.1.1.1.1.cmml" type="integer" xref="S5.SS1.p3.3.m3.1.1.1.1.1.1">16</cn><csymbol cd="latexml" id="S5.SS1.p3.3.m3.3.3.3.3.3.3.cmml" xref="S5.SS1.p3.3.m3.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.3c">16\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.3.m3.3d">start_ARG 16 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math> data achieves a maximum accuracy of <math alttext="92" class="ltx_Math" display="inline" id="S5.SS1.p3.4.m4.1"><semantics id="S5.SS1.p3.4.m4.1a"><mn id="S5.SS1.p3.4.m4.1.1" xref="S5.SS1.p3.4.m4.1.1.cmml">92</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.4.m4.1b"><cn id="S5.SS1.p3.4.m4.1.1.cmml" type="integer" xref="S5.SS1.p3.4.m4.1.1">92</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.4.m4.1c">92</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.4.m4.1d">92</annotation></semantics></math>% and experiences a substantial drop in performance as the training set size decreases. This indicates that the critical audio content for recording smartphone model identification is likely found in the frequency bands above <math alttext="8\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S5.SS1.p3.5.m5.3"><semantics id="S5.SS1.p3.5.m5.3a"><mrow id="S5.SS1.p3.5.m5.3.3" xref="S5.SS1.p3.5.m5.3.3.cmml"><mn id="S5.SS1.p3.5.m5.1.1.1.1.1.1" xref="S5.SS1.p3.5.m5.1.1.1.1.1.1.cmml">8</mn><mtext id="S5.SS1.p3.5.m5.2.2.2.2.2.2" xref="S5.SS1.p3.5.m5.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S5.SS1.p3.5.m5.3.3.3.3.3.3" xref="S5.SS1.p3.5.m5.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.5.m5.3b"><apply id="S5.SS1.p3.5.m5.3.3.cmml" xref="S5.SS1.p3.5.m5.3.3"><csymbol cd="latexml" id="S5.SS1.p3.5.m5.2.2.2.2.2.2.cmml" xref="S5.SS1.p3.5.m5.2.2.2.2.2.2">times</csymbol><cn id="S5.SS1.p3.5.m5.1.1.1.1.1.1.cmml" type="integer" xref="S5.SS1.p3.5.m5.1.1.1.1.1.1">8</cn><csymbol cd="latexml" id="S5.SS1.p3.5.m5.3.3.3.3.3.3.cmml" xref="S5.SS1.p3.5.m5.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.5.m5.3c">8\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.5.m5.3d">start_ARG 8 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math>, enabling the first classifier to perform exceptionally well even with limited training samples. In contrast, the second classifier’s task is more challenging, necessitating more training data to enhance its classification performance.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="492" id="S5.F5.g1" src="x4.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Balanced accuracy values of the considered baseline, trained on different percentages of the training set. </figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Different Domains’ experiments</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.6">We now broaden our investigations by exploring other scenarios beyond clean speech.
In this analysis, we train and test the considered classifier on audio data from diverse domains (i.e., clean speech, noisy speech, music, and environmental noises), investigating challenging cross-test situations.
As in our previous approach, the classifier is trained using a <math alttext="60" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><mn id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><cn id="S5.SS2.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS2.p1.1.m1.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">60</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">60</annotation></semantics></math>%-<math alttext="20" class="ltx_Math" display="inline" id="S5.SS2.p1.2.m2.1"><semantics id="S5.SS2.p1.2.m2.1a"><mn id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><cn id="S5.SS2.p1.2.m2.1.1.cmml" type="integer" xref="S5.SS2.p1.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">20</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.2.m2.1d">20</annotation></semantics></math>%-<math alttext="20" class="ltx_Math" display="inline" id="S5.SS2.p1.3.m3.1"><semantics id="S5.SS2.p1.3.m3.1a"><mn id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><cn id="S5.SS2.p1.3.m3.1.1.cmml" type="integer" xref="S5.SS2.p1.3.m3.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">20</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.3.m3.1d">20</annotation></semantics></math>% split for train, validation, and test sets, and we consider the first <math alttext="15\text{\,}\mathrm{s}" class="ltx_Math" display="inline" id="S5.SS2.p1.4.m4.3"><semantics id="S5.SS2.p1.4.m4.3a"><mrow id="S5.SS2.p1.4.m4.3.3" xref="S5.SS2.p1.4.m4.3.3.cmml"><mn id="S5.SS2.p1.4.m4.1.1.1.1.1.1" xref="S5.SS2.p1.4.m4.1.1.1.1.1.1.cmml">15</mn><mtext id="S5.SS2.p1.4.m4.2.2.2.2.2.2" xref="S5.SS2.p1.4.m4.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S5.SS2.p1.4.m4.3.3.3.3.3.3" mathvariant="normal" xref="S5.SS2.p1.4.m4.3.3.3.3.3.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.3b"><apply id="S5.SS2.p1.4.m4.3.3.cmml" xref="S5.SS2.p1.4.m4.3.3"><csymbol cd="latexml" id="S5.SS2.p1.4.m4.2.2.2.2.2.2.cmml" xref="S5.SS2.p1.4.m4.2.2.2.2.2.2">times</csymbol><cn id="S5.SS2.p1.4.m4.1.1.1.1.1.1.cmml" type="integer" xref="S5.SS2.p1.4.m4.1.1.1.1.1.1">15</cn><csymbol cd="latexml" id="S5.SS2.p1.4.m4.3.3.3.3.3.3.cmml" xref="S5.SS2.p1.4.m4.3.3.3.3.3.3">second</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.3c">15\text{\,}\mathrm{s}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.4.m4.3d">start_ARG 15 end_ARG start_ARG times end_ARG start_ARG roman_s end_ARG</annotation></semantics></math> of each audio track.
Based on the results of the previous experiment, we focus on the post-processed data at <math alttext="16\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S5.SS2.p1.5.m5.3"><semantics id="S5.SS2.p1.5.m5.3a"><mrow id="S5.SS2.p1.5.m5.3.3" xref="S5.SS2.p1.5.m5.3.3.cmml"><mn id="S5.SS2.p1.5.m5.1.1.1.1.1.1" xref="S5.SS2.p1.5.m5.1.1.1.1.1.1.cmml">16</mn><mtext id="S5.SS2.p1.5.m5.2.2.2.2.2.2" xref="S5.SS2.p1.5.m5.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S5.SS2.p1.5.m5.3.3.3.3.3.3" xref="S5.SS2.p1.5.m5.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.5.m5.3b"><apply id="S5.SS2.p1.5.m5.3.3.cmml" xref="S5.SS2.p1.5.m5.3.3"><csymbol cd="latexml" id="S5.SS2.p1.5.m5.2.2.2.2.2.2.cmml" xref="S5.SS2.p1.5.m5.2.2.2.2.2.2">times</csymbol><cn id="S5.SS2.p1.5.m5.1.1.1.1.1.1.cmml" type="integer" xref="S5.SS2.p1.5.m5.1.1.1.1.1.1">16</cn><csymbol cd="latexml" id="S5.SS2.p1.5.m5.3.3.3.3.3.3.cmml" xref="S5.SS2.p1.5.m5.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.5.m5.3c">16\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.5.m5.3d">start_ARG 16 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math> because the excessively high accuracy values achieved at <math alttext="44\text{\,}\mathrm{kHz}" class="ltx_Math" display="inline" id="S5.SS2.p1.6.m6.3"><semantics id="S5.SS2.p1.6.m6.3a"><mrow id="S5.SS2.p1.6.m6.3.3" xref="S5.SS2.p1.6.m6.3.3.cmml"><mn id="S5.SS2.p1.6.m6.1.1.1.1.1.1" xref="S5.SS2.p1.6.m6.1.1.1.1.1.1.cmml">44</mn><mtext id="S5.SS2.p1.6.m6.2.2.2.2.2.2" xref="S5.SS2.p1.6.m6.2.2.2.2.2.2.cmml"> </mtext><mi class="ltx_unit" id="S5.SS2.p1.6.m6.3.3.3.3.3.3" xref="S5.SS2.p1.6.m6.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.6.m6.3b"><apply id="S5.SS2.p1.6.m6.3.3.cmml" xref="S5.SS2.p1.6.m6.3.3"><csymbol cd="latexml" id="S5.SS2.p1.6.m6.2.2.2.2.2.2.cmml" xref="S5.SS2.p1.6.m6.2.2.2.2.2.2">times</csymbol><cn id="S5.SS2.p1.6.m6.1.1.1.1.1.1.cmml" type="integer" xref="S5.SS2.p1.6.m6.1.1.1.1.1.1">44</cn><csymbol cd="latexml" id="S5.SS2.p1.6.m6.3.3.3.3.3.3.cmml" xref="S5.SS2.p1.6.m6.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.6.m6.3c">44\text{\,}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.6.m6.3d">start_ARG 44 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG</annotation></semantics></math> may obscure the insights we gain from each experiment.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S5.F6" title="In V-B Different Domains’ experiments ‣ V Smartphone Model Identification from Audio Recordings ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a> shows the balanced accuracy results of this analysis, where all the training and test combinations between audio domains are explored.
We observe that the values along the diagonal of the heatmap are consistently higher than the others, with balanced accuracy values always above <math alttext="92" class="ltx_Math" display="inline" id="S5.SS2.p2.1.m1.1"><semantics id="S5.SS2.p2.1.m1.1a"><mn id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">92</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><cn id="S5.SS2.p2.1.m1.1.1.cmml" type="integer" xref="S5.SS2.p2.1.m1.1.1">92</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">92</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.1.m1.1d">92</annotation></semantics></math>%.
This is an expected result, as the diagonal corresponds to classifiers trained and tested on recordings from the same audio domain.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">The classifier trained on clean speech exhibits strong performance on noisy speech, whereas the reverse is not observed. We hypothesize that this is due to the fact that noisy speech signals contain useful information within the entire frequency spectrum, attributable to the presence of background noise.
Consequently, the classifier trained on noisy signals may overfit on some elements present in frequency bands distinct from those of speech, resulting in diminished performance when evaluated on signals lacking these components, such as clean speech.
Our hypothesis is also supported by existing literature, which showcases methods using the microphone channel response rather than the signal content to tackle the recording smartphone model identification task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#bib.bib45" title="">45</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1">Regarding the considered training configurations, the classifier trained on noisy speech shows the lowest cross-test performance overall.
Conversely, during test, the noisy speech signals are the ones that are identified best by all the classifiers. We attribute this trend to the reasons mentioned above.
In general, the best-performing classifier is the one trained on the ESC10 dataset, which contains environmental noise, with an average classification accuracy higher than <math alttext="86" class="ltx_Math" display="inline" id="S5.SS2.p4.1.m1.1"><semantics id="S5.SS2.p4.1.m1.1a"><mn id="S5.SS2.p4.1.m1.1.1" xref="S5.SS2.p4.1.m1.1.1.cmml">86</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.1.m1.1b"><cn id="S5.SS2.p4.1.m1.1.1.cmml" type="integer" xref="S5.SS2.p4.1.m1.1.1">86</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.1.m1.1c">86</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p4.1.m1.1d">86</annotation></semantics></math>%.</p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="663" id="S5.F6.g1" src="x5.png" width="663"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Balanced accuracy values of the considered classifier when trained and tested on recorded signals of different audio domains.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1">As a last experiment, we aim to assess the effectiveness of the <span class="ltx_glossaryref" title="">IRs</span> provided in the POLIPHONE dataset.
When convolved with a clean signal, the <span class="ltx_glossaryref" title="">IRs</span> can make it sound as if it were recorded by the microphone to which the response belongs.
From a forensic view-point and for the recording smartphone model identification task, this holds significant importance, as it enables the creation of a dataset comprising a potentially infinite number of recorded audio signals.
To evaluate this aspect, we repeat the previous analysis, training the model on the convolved tracks instead of the originally recorded ones.</p>
</div>
<div class="ltx_para" id="S5.SS2.p6">
<p class="ltx_p" id="S5.SS2.p6.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S5.F7" title="In V-B Different Domains’ experiments ‣ V Smartphone Model Identification from Audio Recordings ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a> shows the results of this analysis.
The measured accuracy values exhibit an average decrease of <math alttext="9" class="ltx_Math" display="inline" id="S5.SS2.p6.1.m1.1"><semantics id="S5.SS2.p6.1.m1.1a"><mn id="S5.SS2.p6.1.m1.1.1" xref="S5.SS2.p6.1.m1.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p6.1.m1.1b"><cn id="S5.SS2.p6.1.m1.1.1.cmml" type="integer" xref="S5.SS2.p6.1.m1.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p6.1.m1.1c">9</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p6.1.m1.1d">9</annotation></semantics></math>% compared to those of <a class="ltx_ref" href="https://arxiv.org/html/2410.06221v1#S5.F6" title="In V-B Different Domains’ experiments ‣ V Smartphone Model Identification from Audio Recordings ‣ POLIPHONE: A Dataset for Smartphone Model Identification from Audio Recordings"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a>.
Nonetheless, the color pattern of the two figures remains consistent: noisy speech is in general the easiest to classify (see second column), and training on environmental noise provides the best overall results (see last row).
These results validate our hypothesis: while the convolved results may not match the efficacy of the actual recordings, they enable the training of an effective classifier by considering only the <span class="ltx_glossaryref" title="">IRs</span> of the models.
This opens the door for numerous considerations and new training strategies that can be considered to increase the robustness of the proposed classifiers.</p>
</div>
<figure class="ltx_figure" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="663" id="S5.F7.g1" src="x6.png" width="663"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Balanced accuracy values of the considered classifier when trained on the convolved signals and tested on the recorded ones, in different audio domains.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.2">In this work we presented POLIPHONE, a dataset for the task of microphone model identification in the case of smartphones, that includes audio recordings acquired using <math alttext="20" class="ltx_Math" display="inline" id="S6.p1.1.m1.1"><semantics id="S6.p1.1.m1.1a"><mn id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><cn id="S6.p1.1.m1.1.1.cmml" type="integer" xref="S6.p1.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">20</annotation><annotation encoding="application/x-llamapun" id="S6.p1.1.m1.1d">20</annotation></semantics></math> different recent smartphones, for a total amount of almost <math alttext="50" class="ltx_Math" display="inline" id="S6.p1.2.m2.1"><semantics id="S6.p1.2.m2.1a"><mn id="S6.p1.2.m2.1.1" xref="S6.p1.2.m2.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S6.p1.2.m2.1b"><cn id="S6.p1.2.m2.1.1.cmml" type="integer" xref="S6.p1.2.m2.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.2.m2.1c">50</annotation><annotation encoding="application/x-llamapun" id="S6.p1.2.m2.1d">50</annotation></semantics></math> hours of audio signals.
The released dataset can be useful for pushing research in the considered task and, thanks to its variety, can be employed for various types of investigations in the forensics field at large.
This corpus addresses a gap in the current state-of-the-art, where publicly available sets for the task at hand are few and outdated.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">To summarize, we bring the following contributions:</p>
<ul class="ltx_itemize" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i1.p1.1">We released POLIPHONE, a comprehensive dataset designed for smartphone model identification from audio recordings, featuring audio tracks from recent devices.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i2.p1.1">We designed the recording setup of this dataset so that it can be expanded in the future, allowing for the inclusion of additional models and various types of audio data.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i3.p1">
<p class="ltx_p" id="S6.I1.i3.p1.1">We benchmarked the dataset using a state-of-the-art baseline, showing the effectiveness of high-frequency content in improving classification accuracy.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i4.p1">
<p class="ltx_p" id="S6.I1.i4.p1.1">We demonstrated the impact of training data volume on model performance, highlighting the need for extensive datasets to achieve optimal results.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i5.p1">
<p class="ltx_p" id="S6.I1.i5.p1.1">We explored the utility of <span class="ltx_glossaryref" title="">Impulse Responses (IRs)</span> for generating training datasets in scenarios where recordings from target models are unavailable.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">This is the first version of the dataset. We plan to release future developments, including more recent smartphones and different types of audio data that can be used for diverse analyses. Additionally, we will include multiple devices from the same acquisition model, enabling intra-model classification and more in-depth studies.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We would like to sincerely acknowledge the editorial team of HWupgrade (<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.hwupgrade.it/" title="">https://www.hwupgrade.it/</a>) for supplying us with the smartphones needed for the creation of this dataset.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government.
This work was supported by the FOSTERER project, funded by the Italian Ministry of Education, University, and Research within the PRIN 2022 program.
This work was partially supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU, partnership on “Telecommunications of the Future” (PE00000001 - program “RESTART”, PE00000014 - program “SERICS”).</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
VICE.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.2.1" style="font-size:90%;">DALL-E Is Now Generating Realistic Faces of Fake People.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.vice.com/amp/en/article/g5vbx9/da" style="font-size:90%;" title="">https://www.vice.com/amp/en/article/g5vbx9/da</a><span class="ltx_text" id="bib.bib1.3.1" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
MIT Technology Review.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">The viral AI avatar app Lensa undressed me—without my consent.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.technologyreview.com/2022/12/12/1064751/the-viral-ai-avatar-app-lensa-undressed-me-without-my-consent/" style="font-size:90%;" title="">https://www.technologyreview.com/2022/12/12/1064751/the-viral-ai-avatar-app-lensa-undressed-me-without-my-consent/</a><span class="ltx_text" id="bib.bib2.3.1" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
Adriana Berdich, Bogdan Groza, and René Mayrhofer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.2.1" style="font-size:90%;">A survey on fingerprinting technologies for smartphones based on embedded transducers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.3.1" style="font-size:90%;">IEEE Internet of Things Journal</span><span class="ltx_text" id="bib.bib3.4.2" style="font-size:90%;">, 10(16):14646–14670, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
Luisa Verdoliva.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">Media forensics and deepfakes: an overview.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.3.1" style="font-size:90%;">IEEE Journal of Selected Topics in Signal Processing</span><span class="ltx_text" id="bib.bib4.4.2" style="font-size:90%;">, 14(5):910–932, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
Sara Mandelli, Davide Cozzolino, Paolo Bestagini, Luisa Verdoliva, and Stefano Tubaro.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">Cnn-based fast source device identification.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.3.1" style="font-size:90%;">IEEE Signal Processing Letters</span><span class="ltx_text" id="bib.bib5.4.2" style="font-size:90%;">, 27:1285–1289, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
Davide Cozzolino and Luisa Verdoliva.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">Noiseprint: A cnn-based camera model fingerprint.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.3.1" style="font-size:90%;">IEEE Transactions on Information Forensics and Security</span><span class="ltx_text" id="bib.bib6.4.2" style="font-size:90%;">, 15:144–159, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
Sara Mandelli, Nicolò Bonettini, and Paolo Bestagini.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">Source camera model identification.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib7.4.2" style="font-size:90%;">Multimedia Forensics</span><span class="ltx_text" id="bib.bib7.5.3" style="font-size:90%;">, pages 133–173. Springer Singapore Singapore, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
Davide Dal Cortivo, Sara Mandelli, Paolo Bestagini, and Stefano Tubaro.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">Cnn-based multi-modal camera model identification on video sequences.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.3.1" style="font-size:90%;">Journal of Imaging</span><span class="ltx_text" id="bib.bib8.4.2" style="font-size:90%;">, 7(8):135, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
Sara Mandelli, Paolo Bestagini, Luisa Verdoliva, and Stefano Tubaro.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">Facing device attribution problem for stabilized video sequences.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.3.1" style="font-size:90%;">IEEE Transactions on Information Forensics and Security</span><span class="ltx_text" id="bib.bib9.4.2" style="font-size:90%;">, 15:14–27, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
Davide Salvi, Paolo Bestagini, and Stefano Tubaro.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">Exploring the synthetic speech attribution problem through data-driven detectors.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib10.4.2" style="font-size:90%;">IEEE International Workshop on Information Forensics and Security (WIFS)</span><span class="ltx_text" id="bib.bib10.5.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
Luca Cuccovillo, Antonio Giganti, Paolo Bestagini, Patrick Aichroth, and Stefano Tubaro.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.2.1" style="font-size:90%;">Spectral denoising for microphone classification.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib11.4.2" style="font-size:90%;">International Workshop on Multimedia AI against Disinformation (MAD)</span><span class="ltx_text" id="bib.bib11.5.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
Chunyan Zeng, Shixiong Feng, Dongliang Zhu, and Zhifeng Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">Source acquisition device identification from recorded audio based on spatiotemporal representation learning with multi-attention mechanisms.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.3.1" style="font-size:90%;">Entropy</span><span class="ltx_text" id="bib.bib12.4.2" style="font-size:90%;">, 25(4):626, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
Chunyan Zeng, Yuhao Zhao, Zhifeng Wang, Kun Li, Xiangkui Wan, and Min Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">Squeeze-and-excitation self-attention mechanism enhanced digital audio source recognition based on transfer learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.3.1" style="font-size:90%;">Circuits, Systems, and Signal Processing</span><span class="ltx_text" id="bib.bib13.4.2" style="font-size:90%;">, pages 1–33, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
Gianmarco Baldini and Irene Amerini.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">Smartphones identification through the built-in microphones with convolutional neural network.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.3.1" style="font-size:90%;">IEEE Access</span><span class="ltx_text" id="bib.bib14.4.2" style="font-size:90%;">, 7:158685–158696, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
Robert Buchholz, Christian Kraetzer, and Jana Dittmann.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">Microphone Classification Using Fourier Coefficients.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.3.1" style="font-size:90%;">In Stefan Katzenbeisser and Ahmad-Reza Sadeghi, editors, </span><span class="ltx_text ltx_font_italic" id="bib.bib15.4.2" style="font-size:90%;">Information Hiding</span><span class="ltx_text" id="bib.bib15.5.3" style="font-size:90%;">, volume 5806, pages 235–246. Springer Berlin Heidelberg, Berlin, Heidelberg, 2009.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.6.1" style="font-size:90%;">Series Title: Lecture Notes in Computer Science.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
Daniel Garcia-Romero and Carol Y. Espy-Wilson.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">Automatic acquisition device identification from speech recordings.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib16.4.2" style="font-size:90%;">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="ltx_text" id="bib.bib16.5.3" style="font-size:90%;">, 2010.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
Cemal Hanilci, Figen Ertas, Tuncay Ertas, and Ömer Eskidere.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">Recognition of brand and models of cell-phones from recorded speech signals.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.3.1" style="font-size:90%;">IEEE Transactions on Information Forensics and Security</span><span class="ltx_text" id="bib.bib17.4.2" style="font-size:90%;">, 7(2):625–634, 2012.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
Hafiz Malik Sohaib Ikram.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">Microphone identification using higher-order statistics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib18.4.2" style="font-size:90%;">Audio Engineering Society Conference: 46th International Conference: Audio Forensics</span><span class="ltx_text" id="bib.bib18.5.3" style="font-size:90%;">, Jun 2012.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
Luca Cuccovillo, Sebastian Mann, Marco Tagliasacchi, and Patrick Aichroth.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">Audio tampering detection via microphone classification.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib19.4.2" style="font-size:90%;">IEEE International Workshop on Multimedia Signal Processing (MMSP)</span><span class="ltx_text" id="bib.bib19.5.3" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
Vandana Pandey, Vicky Kumar Verma, and Nitin Khanna.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.2.1" style="font-size:90%;">Cell-phone identification from audio recordings using PSD of speech-free regions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib20.4.2" style="font-size:90%;">IEEE Students’ Conference on Electrical, Electronics and Computer Science</span><span class="ltx_text" id="bib.bib20.5.3" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
Eskidere Omer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.2.1" style="font-size:90%;">Source microphone identification from speech recordings based on a Gaussian mixture model.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.3.1" style="font-size:90%;">Turkish Journal of Electrical Engineering &amp; Computer Science</span><span class="ltx_text" id="bib.bib21.4.2" style="font-size:90%;">, 22:754–767, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
Rachit Aggarwal, Shivam Singh, Amulya Kumar Roul, and Nitin Khanna.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.2.1" style="font-size:90%;">Cellphone identification using noise estimates from recorded audio.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib22.4.2" style="font-size:90%;">International Conference on Communication and Signal Processing</span><span class="ltx_text" id="bib.bib22.5.3" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
Constantine Kotropoulos and Stamatios Samaras.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.2.1" style="font-size:90%;">Mobile phone identification using recorded speech signals.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib23.4.2" style="font-size:90%;">International Conference on Digital Signal Processing</span><span class="ltx_text" id="bib.bib23.5.3" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
Jinhua Zeng, Shaopei Shi, Xu Yang, Yan Li, Qimeng Lu, Xiulian Qiu, and Huaping Zhu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.2.1" style="font-size:90%;">Audio recorder forensic identification in 21 audio recorders.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib24.4.2" style="font-size:90%;">IEEE International Conference on Progress in Informatics and Computing (PIC)</span><span class="ltx_text" id="bib.bib24.5.3" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
Ling Zou, Qianhua He, and Junfeng Wu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.2.1" style="font-size:90%;">Source cell phone verification from speech recordings using sparse representation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.3.1" style="font-size:90%;">Digital Signal Processing</span><span class="ltx_text" id="bib.bib25.4.2" style="font-size:90%;">, 62:125–136, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
Da Luo, Paweł Korus, and Jiwu Huang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.2.1" style="font-size:90%;">Band energy difference for source attribution in audio forensics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.3.1" style="font-size:90%;">IEEE Transactions on Information Forensics and Security</span><span class="ltx_text" id="bib.bib26.4.2" style="font-size:90%;">, 13(9):2179–2189, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
Tianyun Qin, Rangding Wang, Diqun Yan, and Lang Lin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.2.1" style="font-size:90%;">Source cell-phone identification in the presence of additive noise from cqt domain.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.3.1" style="font-size:90%;">Information</span><span class="ltx_text" id="bib.bib27.4.2" style="font-size:90%;">, 9(8), 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="font-size:90%;">
Vinay Verma and Nitin Khanna.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.2.1" style="font-size:90%;">CNN-based System for Speaker Independent Cell-Phone Identification from Recorded Audio.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.3.1" style="font-size:90%;">Proceedings of the IEEE CVPR 2019</span><span class="ltx_text" id="bib.bib28.4.2" style="font-size:90%;">, page 9, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="font-size:90%;">
Xiaowen Li, Diqun Yan, Li Dong, and Rangding Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.2.1" style="font-size:90%;">Anti-forensics of audio source identification using generative adversarial network.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.3.1" style="font-size:90%;">IEEE Access</span><span class="ltx_text" id="bib.bib29.4.2" style="font-size:90%;">, 7:184332–184339, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="font-size:90%;">
Xiaodan Lin, Jianqing Zhu, and Donghua Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.2.1" style="font-size:90%;">Subband aware cnn for cell-phone recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.3.1" style="font-size:90%;">IEEE Signal Processing Letters</span><span class="ltx_text" id="bib.bib30.4.2" style="font-size:90%;">, 27:605–609, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="font-size:90%;">
Chunyan Zeng, Dongliang Zhu, Zhifeng Wang, Zhenghui Wang, Nan Zhao, and Lu He.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.2.1" style="font-size:90%;">An end-to-end deep source recording device identification system for Web media forensics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.3.1" style="font-size:90%;">International Journal of Web Information Systems</span><span class="ltx_text" id="bib.bib31.4.2" style="font-size:90%;">, 16(4):413–425, January 2020.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.5.1" style="font-size:90%;">Publisher: Emerald Publishing Limited.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.1.1" style="font-size:90%;">
Xingfa Shen, Xingkun Shao, Quanbo Ge, and Lili Liu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.2.1" style="font-size:90%;">Rars: Recognition of audio recording source based on residual neural network.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.3.1" style="font-size:90%;">IEEE/ACM Transactions on Audio, Speech, and Language Processing</span><span class="ltx_text" id="bib.bib32.4.2" style="font-size:90%;">, 29:575–584, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.1.1" style="font-size:90%;">
Nik Nur Wahidah Nik Hashim, Mugahed Al-Ezzi Ahmed Ezzi, and Mitchell D. Wilkes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.2.1" style="font-size:90%;">Mobile microphone robust acoustic feature identification using coefficient of variance.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.3.1" style="font-size:90%;">International Journal of Speech Technology</span><span class="ltx_text" id="bib.bib33.4.2" style="font-size:90%;">, 24(4):1089–1100, December 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.1.1" style="font-size:90%;">
Bhavuk Singhal, Abinay Reddy Naini, and Prasanta Kumar Ghosh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.2.1" style="font-size:90%;">wSPIRE: A Parallel Multi-Device Corpus in Neutral and Whispered Speech.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib34.4.2" style="font-size:90%;">Conference of the Oriental COCOSDA</span><span class="ltx_text" id="bib.bib34.5.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.1.1" style="font-size:90%;">
Adriana Berdich, Bogdan Groza, Efrat Levy, Asaf Shabtai, Yuval Elovici, and René Mayrhofer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.2.1" style="font-size:90%;">Fingerprinting Smartphones Based on Microphone Characteristics From Environment Affected Recordings.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.3.1" style="font-size:90%;">IEEE Access</span><span class="ltx_text" id="bib.bib35.4.2" style="font-size:90%;">, 10:122399–122413, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.1.1" style="font-size:90%;">
Sohaib Ikram and Hafiz Malik.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.2.1" style="font-size:90%;">Microphone identification using higher-order statistics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib36.4.2" style="font-size:90%;">46th International AES Conference: Audio Forensics</span><span class="ltx_text" id="bib.bib36.5.3" style="font-size:90%;">. Audio Engineering Society, 2012.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.1.1" style="font-size:90%;">
ÖMER ESKİDERE.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.2.1" style="font-size:90%;">Source microphone identification from speech recordings based on a gaussian mixture model.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.3.1" style="font-size:90%;">Turkish Journal of Electrical Engineering and Computer Sciences</span><span class="ltx_text" id="bib.bib37.4.2" style="font-size:90%;">, 22(3):754–767, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.1.1" style="font-size:90%;">
Jinhua Zeng, Shaopei Shi, Xu Yang, Yan Li, Qimeng Lu, Xiulian Qiu, and Huaping Zhu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.2.1" style="font-size:90%;">Audio recorder forensic identification in 21 audio recorders.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib38.4.2" style="font-size:90%;">IEEE International Conference on Progress in Informatics and Computing (PIC)</span><span class="ltx_text" id="bib.bib38.5.3" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.1.1" style="font-size:90%;">
Nik Nur Wahidah Nik Hashim, Mugahed Al-Ezzi Ahmed Ezzi, and Mitchell D. Wilkes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.2.1" style="font-size:90%;">Mobile microphone robust acoustic feature identification using coefficient of variance.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib39.3.1" style="font-size:90%;">Int. J. Speech Technol.</span><span class="ltx_text" id="bib.bib39.4.2" style="font-size:90%;">, 24(4):1089–1100, dec 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.1.1" style="font-size:90%;">
Chunyan Zeng, Shixiong Feng, Zhifeng Wang, Xiangkui Wan, Yunfan Chen, and Nan Zhao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.2.1" style="font-size:90%;">Spatio-temporal representation learning enhanced source cell-phone recognition from speech recordings.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.3.1" style="font-size:90%;">Journal of Information Security and Applications</span><span class="ltx_text" id="bib.bib40.4.2" style="font-size:90%;">, 80:103672, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.1.1" style="font-size:90%;">
Vinay Verma, Preet Khaturia, and Nitin Khanna.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.2.1" style="font-size:90%;">Cell-Phone Identification from Recompressed Audio Recordings.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib41.4.2" style="font-size:90%;">National Conference on Communications (NCC)</span><span class="ltx_text" id="bib.bib41.5.3" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.1.1" style="font-size:90%;">
Ling Zou, Qianhua He, and Xiaohui Feng.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.2.1" style="font-size:90%;">Cell phone verification from speech recordings using sparse representation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib42.4.2" style="font-size:90%;">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="ltx_text" id="bib.bib42.5.3" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.1.1" style="font-size:90%;">
Yanxiong Li, Xue Zhang, Xianku Li, Yuhan Zhang, Jichen Yang, and Qianhua He.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.2.1" style="font-size:90%;">Mobile phone clustering from speech recordings using deep representation and spectral clustering.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.3.1" style="font-size:90%;">IEEE Transactions on Information Forensics and Security</span><span class="ltx_text" id="bib.bib43.4.2" style="font-size:90%;">, 13(4):965–977, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.1.1" style="font-size:90%;">
Constantine L. Kotropoulos.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.2.1" style="font-size:90%;">Source phone identification using sketches of features.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.3.1" style="font-size:90%;">IET Biometrics</span><span class="ltx_text" id="bib.bib44.4.2" style="font-size:90%;">, 3(2):75–83, June 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.1.1" style="font-size:90%;">
Luca Cuccovillo and Patrick Aichroth.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.2.1" style="font-size:90%;">Open-set microphone classification via blind channel analysis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib45.4.2" style="font-size:90%;">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="ltx_text" id="bib.bib45.5.3" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.1.1" style="font-size:90%;">
Chao Jin, Rangding Wang, and Diqun Yan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.2.1" style="font-size:90%;">Source smartphone identification by exploiting encoding characteristics of recorded speech.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.3.1" style="font-size:90%;">Digit. Investig.</span><span class="ltx_text" id="bib.bib46.4.2" style="font-size:90%;">, 29(C):129–146, jun 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.1.1" style="font-size:90%;">
Gianmarco Baldini and Irene Amerini.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.2.1" style="font-size:90%;">An evaluation of entropy measures for microphone identification.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib47.3.1" style="font-size:90%;">Entropy</span><span class="ltx_text" id="bib.bib47.4.2" style="font-size:90%;">, 22, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.1.1" style="font-size:90%;">
Yuechi Jiang and Frank H. F. Leung.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.2.1" style="font-size:90%;">Source microphone recognition aided by a kernel-based projection method.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib48.3.1" style="font-size:90%;">IEEE Transactions on Information Forensics and Security</span><span class="ltx_text" id="bib.bib48.4.2" style="font-size:90%;">, 14(11):2875–2886, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.1.1" style="font-size:90%;">
Antonio Giganti, Luca Cuccovillo, Paolo Bestagini, Patrick Aichroth, and Stefano Tubaro.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.2.1" style="font-size:90%;">Speaker-independent microphone identification in noisy conditions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib49.4.2" style="font-size:90%;">European Signal Processing Conference (EUSIPCO)</span><span class="ltx_text" id="bib.bib49.5.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.1.1" style="font-size:90%;">
Cemal Hanilci and Tomi Kinnunen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.2.1" style="font-size:90%;">Source cell-phone recognition from recorded speech using non-speech segments.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib50.3.1" style="font-size:90%;">Digital Signal Processing</span><span class="ltx_text" id="bib.bib50.4.2" style="font-size:90%;">, 35:75–85, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.1.1" style="font-size:90%;">
Vinay Verma and Nitin Khanna.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.2.1" style="font-size:90%;">Speaker-independent source cell-phone identification for re-compressed and noisy audio recordings.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib51.3.1" style="font-size:90%;">Multimedia Tools Appl.</span><span class="ltx_text" id="bib.bib51.4.2" style="font-size:90%;">, 80(15):23581–23603, jun 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.1.1" style="font-size:90%;">
Abinay Reddy Naini, Bhavuk Singhal, and Prasanta Kumar Ghosh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.2.1" style="font-size:90%;">Dual Attention Pooling Network for Recording Device Classification Using Neutral and Whispered Speech.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib52.4.2" style="font-size:90%;">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span class="ltx_text" id="bib.bib52.5.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.1.1" style="font-size:90%;">
Chunyan Zeng, Dongliang Zhu, Zhifeng Wang, Minghu Wu, Wei Xiong, and Nan Zhao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.2.1" style="font-size:90%;">Spatial and temporal learning representation for end-to-end recording device identification.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib53.3.1" style="font-size:90%;">EURASIP Journal on Advances in Signal Processing</span><span class="ltx_text" id="bib.bib53.4.2" style="font-size:90%;">, 2021(1):41, July 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.1.1" style="font-size:90%;">
International Organization for Standardization.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.2.1" style="font-size:90%;">Acoustics – Determination of sound power levels of noise sources using sound pressure – Precision methods for anechoic and hemi-anechoic rooms.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.iso.org/standard/52056.html" style="font-size:90%;" title="">https://www.iso.org/standard/52056.html</a><span class="ltx_text" id="bib.bib54.3.1" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.1.1" style="font-size:90%;">
Junichi Yamagishi, Christophe Veaux, Kirsten MacDonald, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.2.1" style="font-size:90%;">CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib55.3.1" style="font-size:90%;">University of Edinburgh. The Centre for Speech Technology Research (CSTR)</span><span class="ltx_text" id="bib.bib55.4.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.1.1" style="font-size:90%;">
Juan J Bosch, Ricard Marxer, and Emilia Gómez.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.2.1" style="font-size:90%;">Evaluation and combination of pitch estimation methods for melody extraction in symphonic classical music.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib56.3.1" style="font-size:90%;">Journal of New Music Research</span><span class="ltx_text" id="bib.bib56.4.2" style="font-size:90%;">, 45(2):101–117, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.1.1" style="font-size:90%;">
Karol J Piczak.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.2.1" style="font-size:90%;">Esc: Dataset for environmental sound classification.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib57.4.2" style="font-size:90%;">ACM International Conference on Multimedia</span><span class="ltx_text" id="bib.bib57.5.3" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.1.1" style="font-size:90%;">
Swen Müller and Paulo Massarani.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.2.1" style="font-size:90%;">Transfer-function measurement with sweeps.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib58.3.1" style="font-size:90%;">Journal of the Audio Engineering Society</span><span class="ltx_text" id="bib.bib58.4.2" style="font-size:90%;">, 49(6):443–471, 2001.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.1.1" style="font-size:90%;">
Hynek Hermansky and Nelson Morgan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.2.1" style="font-size:90%;">Rasta processing of speech.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib59.3.1" style="font-size:90%;">IEEE Transactions on Speech and Audio Processing</span><span class="ltx_text" id="bib.bib59.4.2" style="font-size:90%;">, 2(4):578–589, 1994.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.1.1" style="font-size:90%;">
Zhi-Feng Wang, Gang Wei, and Qian-Hua He.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.2.1" style="font-size:90%;">Channel pattern noise based playback attack detection algorithm for speaker recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib60.4.2" style="font-size:90%;">IEEE International Conference on Machine Learning and Cybernetics (ICMLC)</span><span class="ltx_text" id="bib.bib60.5.3" style="font-size:90%;">, 2011.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct  8 17:26:47 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
