<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Enhancing Image Classification in Small and Unbalanced Datasets through Synthetic Data Augmentation</title>
<!--Generated on Tue Oct  1 11:08:10 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Synthetic Data Augmentation Variational Autoencoder Esophagogastroduodenoscopy Image Classification Image Classification" lang="en" name="keywords"/>
<base href="/html/2409.10286v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#S1" title="In Enhancing Image Classification in Small and Unbalanced Datasets through Synthetic Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#S2" title="In Enhancing Image Classification in Small and Unbalanced Datasets through Synthetic Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#S3" title="In Enhancing Image Classification in Small and Unbalanced Datasets through Synthetic Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#S4" title="In Enhancing Image Classification in Small and Unbalanced Datasets through Synthetic Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#S4.SS1" title="In 4 Experimental Setup ‣ Enhancing Image Classification in Small and Unbalanced Datasets through Synthetic Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#S4.SS2" title="In 4 Experimental Setup ‣ Enhancing Image Classification in Small and Unbalanced Datasets through Synthetic Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#S4.SS3" title="In 4 Experimental Setup ‣ Enhancing Image Classification in Small and Unbalanced Datasets through Synthetic Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Implementation Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#S5" title="In Enhancing Image Classification in Small and Unbalanced Datasets through Synthetic Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#S6" title="In Enhancing Image Classification in Small and Unbalanced Datasets through Synthetic Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusions and Future Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#S6.SS0.SSS1" title="In 6 Conclusions and Future Work ‣ Enhancing Image Classification in Small and Unbalanced Datasets through Synthetic Data Augmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.0.1 </span><span class="ltx_ERROR undefined">\discintname</span></span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Computer Vision Center, Barcelona, Spain </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Universitat Autònoma de Barcelona, Barcelona, Spain
<br class="ltx_break"/></span></span></span><span class="ltx_note ltx_role_institutetext" id="id3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Hospital Clinic, Barcelona, Spain
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id3.1"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">email: </span>{ neil.delafuente , mireia.majo , jorge.bernal } @ autonoma.cat </span></span></span>
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id3.2"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">email: </span>{ luzko, hcordova , mgfernan } @ clinic.cat</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Enhancing Image Classification in Small and Unbalanced Datasets through Synthetic Data Augmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Neil de la Fuente 
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mireia Majó
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Irina Luzko
</span><span class="ltx_author_notes">33</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Henry Córdova
</span><span class="ltx_author_notes">33</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gloria Fernández-Esparrach
</span><span class="ltx_author_notes">33</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jorge Bernal
</span><span class="ltx_author_notes">1122</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Accurate and robust medical image classification is a challenging task, especially in application domains where available annotated datasets are small and present high imbalance between target classes. Considering that data acquisition is not always feasible, especially for underrepresented classes, our approach introduces a novel synthetic augmentation strategy using class-specific Variational Autoencoders (VAEs) and latent space
interpolation to improve discrimination capabilities.</p>
<p class="ltx_p" id="id2.id2">By generating realistic, varied synthetic data that fills feature space gaps, we address issues of data scarcity and class imbalance. The method presented in this paper relies on the interpolation of latent representations within each class, thus enriching the training set and improving the model’s generalizability and diagnostic accuracy.</p>
<p class="ltx_p" id="id3.id3">The proposed strategy was tested in a small dataset of 321 images created to train and validate an automatic method for assessing the quality of cleanliness of esophagogastroduodenoscopy images.</p>
<p class="ltx_p" id="id4.id4">By combining real and synthetic data, an increase of over 18% in the accuracy of the most challenging underrepresented class was observed. The proposed strategy not only benefited the underrepresented class but also led to a general improvement in other metrics, including a 6% increase in global accuracy and precision.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Synthetic Data Augmentation Variational Autoencoder Esophagogastroduodenoscopy Image Classification Image Classification
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Gastric cancer (GC) is the <math alttext="5^{\text{th}}" class="ltx_Math" display="inline" id="S1.p1.1.m1.1"><semantics id="S1.p1.1.m1.1a"><msup id="S1.p1.1.m1.1.1" xref="S1.p1.1.m1.1.1.cmml"><mn id="S1.p1.1.m1.1.1.2" xref="S1.p1.1.m1.1.1.2.cmml">5</mn><mtext id="S1.p1.1.m1.1.1.3" xref="S1.p1.1.m1.1.1.3a.cmml">th</mtext></msup><annotation-xml encoding="MathML-Content" id="S1.p1.1.m1.1b"><apply id="S1.p1.1.m1.1.1.cmml" xref="S1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p1.1.m1.1.1.1.cmml" xref="S1.p1.1.m1.1.1">superscript</csymbol><cn id="S1.p1.1.m1.1.1.2.cmml" type="integer" xref="S1.p1.1.m1.1.1.2">5</cn><ci id="S1.p1.1.m1.1.1.3a.cmml" xref="S1.p1.1.m1.1.1.3"><mtext id="S1.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S1.p1.1.m1.1.1.3">th</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.1.m1.1c">5^{\text{th}}</annotation><annotation encoding="application/x-llamapun" id="S1.p1.1.m1.1d">5 start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT</annotation></semantics></math> most common cancer worldwide and there were more than 1 million new cases of GC reported in 2020. Esophagoduodenoscopy (EGD) is the gold standard method for the diagnosis of GC: several studies show that the detection of GC at earlier stages has a clear impact in the decrease of the mortality (hazard ratio [HR] 0.51) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib2" title="">2</a>]</cite>. Nevertheless, up to 10% of the cancers are missed during the exploration, with a clear impact on patient’s survival rate <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib3" title="">3</a>]</cite>. Poor mucosal visualization is one of the factors that can negatively affect the diagnostic accuracy of gastric cancer.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">For this reason, the degree of cleanliness and the quality of gastric mucosa visibility are of paramount importance. However, no broadly accepted cleanliness scale for the upper gastrointestinal tract (UGI) has been uniformly accepted and used in routine practice. Two scales have been recently published: POLPREP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib4" title="">4</a>]</cite> and Barcelona scale <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib5" title="">5</a>]</cite>. Both evaluate the level of cleanliness in the esophagus, stomach and duodenum. They differ on the number of levels (4 for POLPREP, 3 for Barcelona scale) and in the degree of evaluation detail: Barcelona scale further divides stomach by segments (fundus, corpus and antrum).</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">However, these scales are prone to a certain degree of subjectivity. To cope with this, and following other methods already developed to assist clinicians in similar tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib6" title="">6</a>]</cite>, there is room for AI systems that can provide an objective assessment of the degree of UGI cleanliness by an automatic classification of EGD images. The benefits of such a system are clear: if clinicians can be sure of those cases when gastroscopies are inappropriate due to insufficient cleanliness, they can make a recommendation to repeat the exploration. In the opposite case, where the UGI is clean, unnecessary repetitions can be avoided with the consequent saving of scarce economic resources.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The main technical challenge in medical image classification comes from the limited size and imbalance of available datasets. This limitation reflects the real-world shortage of annotated medical images and uneven distribution of pathological findings, making it difficult to develop robust models with traditional deep learning which usually requires large, balanced datasets. Additionally, the detailed nature of EGD images, which needs accurate identification of different levels of cleanliness, adds to the challenge. The scarcity of significant features in smaller datasets can result in model biases or underperformance. Overcoming these challenges requires innovative approaches that improve data diversity and representation, enhancing model’s ability to generalize in real-world scenarios.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The key contributions of our work include:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Use of class-Specific variational autoencoders (VAEs):</span> By generating synthetic images through latent representation interpolation within classes, we can expand the feature space and directly address class imbalance.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Focused enrichment of feature space:</span> Our technique fills gaps in the feature space with realistic synthetic images, improving training effectiveness and model sensitivity to critical subtle features for accurate classification.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Proof of the versatility of our approach across architectures:</span> We demonstrate the benefits of our methodology across two prominent image classification architectures such as EfficientNet-V2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib20" title="">20</a>]</cite> and ResNet-50.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The previously mentioned scarcity of annotated medical datasets has led to novel strategies for data augmentation in image classification.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Garay-Maestre et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib7" title="">7</a>]</cite> exploited Variational Autoencoders (VAEs) to generate synthetic samples, demonstrating how synthetic data, when combined with traditional augmentation methods, can improve the robustness and performance of machine learning models. Following this line of thought, Auzine et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib8" title="">8</a>]</cite> applied Generative Adversarial Networks (GANs) with conventional augmentation to enhance the accuracy of deep learning architectures, such as ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib17" title="">17</a>]</cite> and VGG16 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib18" title="">18</a>]</cite>, on endoscopic esophagus imagery. Further advancing the field, Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib19" title="">19</a>]</cite> introduced ’Diffusion Inversion’, a technique for creating synthetic data by manipulating the latent space of pre-trained diffusion models to achieve comprehensive data manifold coverage and improved generalization.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib9" title="">9</a>]</cite> investigated data augmentation via latent space interpolation for image classification, showing significant improvements in model performance. Oring <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib10" title="">10</a>]</cite> and Cristovao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib11" title="">11</a>]</cite> also explored the use of VAEs for generating in-between images through latent space interpolation, emphasizing the potential of this approach for enhancing data diversity. Moreno-Barea et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib12" title="">12</a>]</cite> and Elbattah et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib13" title="">13</a>]</cite> focused on improving classification accuracy using data augmentation techniques on small datasets, highlighting the effectiveness of synthetic data in addressing class imbalance. Wan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib14" title="">14</a>]</cite> specifically addressed imbalanced learning through VAE-based synthetic data generation, demonstrating its potential to improve model performance on imbalanced datasets.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">While data augmentation has been widely used in several research domains there is no work, to the best of our knowledge, that applies this technique to assess the degree of cleanliness of EGD images. Nevertheless there are works applied to similar images, such as the work of Nam et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib16" title="">16</a>]</cite>, which use InceptionResnetV2 to classify wireless capsule endoscopy images to determine the degree of mucosa visualization or the work of Zhu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib15" title="">15</a>]</cite>, which applies a compact convolutional neural network with 2 Densenet layers to label bowel preparation in colonoscopy images according to Boston Bowel Preparation Scale.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">While some of the previously mentioned studies have demonstrated the potential of generative models for data augmentation, our approach specifically targets class imbalance in EGD image classification by leveraging class-specific VAEs. By generating realistic synthetic images for each class, we aim to fill gaps in the feature space, thereby enhancing the training process and improving sensitivity to subtle features. This synthetic data augmentation methodology stands out by directly addressing the challenges posed by small and unbalanced datasets, particularly for EGD images, and notably improving performance.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.4">Variational Autoencoders (VAEs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#bib.bib21" title="">21</a>]</cite> represented a groundbreaking shift in the generation of synthetic data by providing a probabilistic approach to data encoding and decoding. A VAE is composed of an encoder, for translating input data into a latent space representation, and a decoder, for reconstructing data from this latent space. The encoder function, denoted as <math alttext="q_{\phi}(z|x)" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mrow id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><msub id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml"><mi id="S3.p1.1.m1.1.1.3.2" xref="S3.p1.1.m1.1.1.3.2.cmml">q</mi><mi id="S3.p1.1.m1.1.1.3.3" xref="S3.p1.1.m1.1.1.3.3.cmml">ϕ</mi></msub><mo id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">⁢</mo><mrow id="S3.p1.1.m1.1.1.1.1" xref="S3.p1.1.m1.1.1.1.1.1.cmml"><mo id="S3.p1.1.m1.1.1.1.1.2" stretchy="false" xref="S3.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.p1.1.m1.1.1.1.1.1" xref="S3.p1.1.m1.1.1.1.1.1.cmml"><mi id="S3.p1.1.m1.1.1.1.1.1.2" xref="S3.p1.1.m1.1.1.1.1.1.2.cmml">z</mi><mo fence="false" id="S3.p1.1.m1.1.1.1.1.1.1" xref="S3.p1.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.p1.1.m1.1.1.1.1.1.3" xref="S3.p1.1.m1.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="S3.p1.1.m1.1.1.1.1.3" stretchy="false" xref="S3.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><times id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2"></times><apply id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.3.1.cmml" xref="S3.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.p1.1.m1.1.1.3.2.cmml" xref="S3.p1.1.m1.1.1.3.2">𝑞</ci><ci id="S3.p1.1.m1.1.1.3.3.cmml" xref="S3.p1.1.m1.1.1.3.3">italic-ϕ</ci></apply><apply id="S3.p1.1.m1.1.1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1.1"><csymbol cd="latexml" id="S3.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.1.1.1.2">𝑧</ci><ci id="S3.p1.1.m1.1.1.1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.1.1.1.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">q_{\phi}(z|x)</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ( italic_z | italic_x )</annotation></semantics></math>, maps an input <math alttext="x" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">italic_x</annotation></semantics></math> to a latent space representation <math alttext="z" class="ltx_Math" display="inline" id="S3.p1.3.m3.1"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">z</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.1d">italic_z</annotation></semantics></math>, parameterized by <math alttext="\phi" class="ltx_Math" display="inline" id="S3.p1.4.m4.1"><semantics id="S3.p1.4.m4.1a"><mi id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S3.p1.4.m4.1d">italic_ϕ</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.4">This process introduces a stochastic element by generating a distribution characterized by mean <math alttext="\mu" class="ltx_Math" display="inline" id="S3.p2.1.m1.1"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.1d">italic_μ</annotation></semantics></math> and variance <math alttext="\sigma" class="ltx_Math" display="inline" id="S3.p2.2.m2.1"><semantics id="S3.p2.2.m2.1a"><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">\sigma</annotation><annotation encoding="application/x-llamapun" id="S3.p2.2.m2.1d">italic_σ</annotation></semantics></math>, rather than a fixed point for the latent variables. This distribution allows for the sampling of new data points from the latent space, using the reparameterization trick: <math alttext="z=\mu+\sigma\odot\epsilon" class="ltx_Math" display="inline" id="S3.p2.3.m3.1"><semantics id="S3.p2.3.m3.1a"><mrow id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml"><mi id="S3.p2.3.m3.1.1.2" xref="S3.p2.3.m3.1.1.2.cmml">z</mi><mo id="S3.p2.3.m3.1.1.1" xref="S3.p2.3.m3.1.1.1.cmml">=</mo><mrow id="S3.p2.3.m3.1.1.3" xref="S3.p2.3.m3.1.1.3.cmml"><mi id="S3.p2.3.m3.1.1.3.2" xref="S3.p2.3.m3.1.1.3.2.cmml">μ</mi><mo id="S3.p2.3.m3.1.1.3.1" xref="S3.p2.3.m3.1.1.3.1.cmml">+</mo><mrow id="S3.p2.3.m3.1.1.3.3" xref="S3.p2.3.m3.1.1.3.3.cmml"><mi id="S3.p2.3.m3.1.1.3.3.2" xref="S3.p2.3.m3.1.1.3.3.2.cmml">σ</mi><mo id="S3.p2.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.p2.3.m3.1.1.3.3.1.cmml">⊙</mo><mi id="S3.p2.3.m3.1.1.3.3.3" xref="S3.p2.3.m3.1.1.3.3.3.cmml">ϵ</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><apply id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1"><eq id="S3.p2.3.m3.1.1.1.cmml" xref="S3.p2.3.m3.1.1.1"></eq><ci id="S3.p2.3.m3.1.1.2.cmml" xref="S3.p2.3.m3.1.1.2">𝑧</ci><apply id="S3.p2.3.m3.1.1.3.cmml" xref="S3.p2.3.m3.1.1.3"><plus id="S3.p2.3.m3.1.1.3.1.cmml" xref="S3.p2.3.m3.1.1.3.1"></plus><ci id="S3.p2.3.m3.1.1.3.2.cmml" xref="S3.p2.3.m3.1.1.3.2">𝜇</ci><apply id="S3.p2.3.m3.1.1.3.3.cmml" xref="S3.p2.3.m3.1.1.3.3"><csymbol cd="latexml" id="S3.p2.3.m3.1.1.3.3.1.cmml" xref="S3.p2.3.m3.1.1.3.3.1">direct-product</csymbol><ci id="S3.p2.3.m3.1.1.3.3.2.cmml" xref="S3.p2.3.m3.1.1.3.3.2">𝜎</ci><ci id="S3.p2.3.m3.1.1.3.3.3.cmml" xref="S3.p2.3.m3.1.1.3.3.3">italic-ϵ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">z=\mu+\sigma\odot\epsilon</annotation><annotation encoding="application/x-llamapun" id="S3.p2.3.m3.1d">italic_z = italic_μ + italic_σ ⊙ italic_ϵ</annotation></semantics></math>, where <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.p2.4.m4.1"><semantics id="S3.p2.4.m4.1a"><mi id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><ci id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S3.p2.4.m4.1d">italic_ϵ</annotation></semantics></math> is an element-wise product with a random noise vector.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.3">The decoder, denoted as <math alttext="p_{\theta}(x|z)" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><mrow id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><msub id="S3.p3.1.m1.1.1.3" xref="S3.p3.1.m1.1.1.3.cmml"><mi id="S3.p3.1.m1.1.1.3.2" xref="S3.p3.1.m1.1.1.3.2.cmml">p</mi><mi id="S3.p3.1.m1.1.1.3.3" xref="S3.p3.1.m1.1.1.3.3.cmml">θ</mi></msub><mo id="S3.p3.1.m1.1.1.2" xref="S3.p3.1.m1.1.1.2.cmml">⁢</mo><mrow id="S3.p3.1.m1.1.1.1.1" xref="S3.p3.1.m1.1.1.1.1.1.cmml"><mo id="S3.p3.1.m1.1.1.1.1.2" stretchy="false" xref="S3.p3.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.p3.1.m1.1.1.1.1.1" xref="S3.p3.1.m1.1.1.1.1.1.cmml"><mi id="S3.p3.1.m1.1.1.1.1.1.2" xref="S3.p3.1.m1.1.1.1.1.1.2.cmml">x</mi><mo fence="false" id="S3.p3.1.m1.1.1.1.1.1.1" xref="S3.p3.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.p3.1.m1.1.1.1.1.1.3" xref="S3.p3.1.m1.1.1.1.1.1.3.cmml">z</mi></mrow><mo id="S3.p3.1.m1.1.1.1.1.3" stretchy="false" xref="S3.p3.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"><times id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.2"></times><apply id="S3.p3.1.m1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.1.3.1.cmml" xref="S3.p3.1.m1.1.1.3">subscript</csymbol><ci id="S3.p3.1.m1.1.1.3.2.cmml" xref="S3.p3.1.m1.1.1.3.2">𝑝</ci><ci id="S3.p3.1.m1.1.1.3.3.cmml" xref="S3.p3.1.m1.1.1.3.3">𝜃</ci></apply><apply id="S3.p3.1.m1.1.1.1.1.1.cmml" xref="S3.p3.1.m1.1.1.1.1"><csymbol cd="latexml" id="S3.p3.1.m1.1.1.1.1.1.1.cmml" xref="S3.p3.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.p3.1.m1.1.1.1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.1.1.1.2">𝑥</ci><ci id="S3.p3.1.m1.1.1.1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.1.1.1.3">𝑧</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">p_{\theta}(x|z)</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x | italic_z )</annotation></semantics></math> and parameterized by <math alttext="\theta" class="ltx_Math" display="inline" id="S3.p3.2.m2.1"><semantics id="S3.p3.2.m2.1a"><mi id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><ci id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.p3.2.m2.1d">italic_θ</annotation></semantics></math>, reconstructs the data from the latent space representation. The objective of training a VAE is to minimize the loss function <math alttext="\mathcal{L}(\theta,\phi;x)" class="ltx_Math" display="inline" id="S3.p3.3.m3.3"><semantics id="S3.p3.3.m3.3a"><mrow id="S3.p3.3.m3.3.4" xref="S3.p3.3.m3.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p3.3.m3.3.4.2" xref="S3.p3.3.m3.3.4.2.cmml">ℒ</mi><mo id="S3.p3.3.m3.3.4.1" xref="S3.p3.3.m3.3.4.1.cmml">⁢</mo><mrow id="S3.p3.3.m3.3.4.3.2" xref="S3.p3.3.m3.3.4.3.1.cmml"><mo id="S3.p3.3.m3.3.4.3.2.1" stretchy="false" xref="S3.p3.3.m3.3.4.3.1.cmml">(</mo><mi id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">θ</mi><mo id="S3.p3.3.m3.3.4.3.2.2" xref="S3.p3.3.m3.3.4.3.1.cmml">,</mo><mi id="S3.p3.3.m3.2.2" xref="S3.p3.3.m3.2.2.cmml">ϕ</mi><mo id="S3.p3.3.m3.3.4.3.2.3" xref="S3.p3.3.m3.3.4.3.1.cmml">;</mo><mi id="S3.p3.3.m3.3.3" xref="S3.p3.3.m3.3.3.cmml">x</mi><mo id="S3.p3.3.m3.3.4.3.2.4" stretchy="false" xref="S3.p3.3.m3.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.3b"><apply id="S3.p3.3.m3.3.4.cmml" xref="S3.p3.3.m3.3.4"><times id="S3.p3.3.m3.3.4.1.cmml" xref="S3.p3.3.m3.3.4.1"></times><ci id="S3.p3.3.m3.3.4.2.cmml" xref="S3.p3.3.m3.3.4.2">ℒ</ci><vector id="S3.p3.3.m3.3.4.3.1.cmml" xref="S3.p3.3.m3.3.4.3.2"><ci id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1">𝜃</ci><ci id="S3.p3.3.m3.2.2.cmml" xref="S3.p3.3.m3.2.2">italic-ϕ</ci><ci id="S3.p3.3.m3.3.3.cmml" xref="S3.p3.3.m3.3.3">𝑥</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.3c">\mathcal{L}(\theta,\phi;x)</annotation><annotation encoding="application/x-llamapun" id="S3.p3.3.m3.3d">caligraphic_L ( italic_θ , italic_ϕ ; italic_x )</annotation></semantics></math>, which is a combination of the negative log-likelihood of the reconstructed data and the Kullback-Leibler (KL) divergence, promoting an effective balance between data reconstruction fidelity and distribution approximation:</p>
</div>
<div class="ltx_para" id="S3.p4">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}(\theta,\phi;x)=-\mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)]+%
\text{KL}(q_{\phi}(z|x)\|p(z))" class="ltx_Math" display="block" id="S3.E1.m1.7"><semantics id="S3.E1.m1.7a"><mrow id="S3.E1.m1.7.7" xref="S3.E1.m1.7.7.cmml"><mrow id="S3.E1.m1.7.7.4" xref="S3.E1.m1.7.7.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.7.7.4.2" xref="S3.E1.m1.7.7.4.2.cmml">ℒ</mi><mo id="S3.E1.m1.7.7.4.1" xref="S3.E1.m1.7.7.4.1.cmml">⁢</mo><mrow id="S3.E1.m1.7.7.4.3.2" xref="S3.E1.m1.7.7.4.3.1.cmml"><mo id="S3.E1.m1.7.7.4.3.2.1" stretchy="false" xref="S3.E1.m1.7.7.4.3.1.cmml">(</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">θ</mi><mo id="S3.E1.m1.7.7.4.3.2.2" xref="S3.E1.m1.7.7.4.3.1.cmml">,</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">ϕ</mi><mo id="S3.E1.m1.7.7.4.3.2.3" xref="S3.E1.m1.7.7.4.3.1.cmml">;</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">x</mi><mo id="S3.E1.m1.7.7.4.3.2.4" stretchy="false" xref="S3.E1.m1.7.7.4.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.7.7.3" xref="S3.E1.m1.7.7.3.cmml">=</mo><mrow id="S3.E1.m1.7.7.2" xref="S3.E1.m1.7.7.2.cmml"><mrow id="S3.E1.m1.6.6.1.1" xref="S3.E1.m1.6.6.1.1.cmml"><mo id="S3.E1.m1.6.6.1.1a" xref="S3.E1.m1.6.6.1.1.cmml">−</mo><mrow id="S3.E1.m1.6.6.1.1.1" xref="S3.E1.m1.6.6.1.1.1.cmml"><msub id="S3.E1.m1.6.6.1.1.1.3" xref="S3.E1.m1.6.6.1.1.1.3.cmml"><mi id="S3.E1.m1.6.6.1.1.1.3.2" xref="S3.E1.m1.6.6.1.1.1.3.2.cmml">𝔼</mi><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.3.2.cmml">q</mi><mi id="S3.E1.m1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.3.3.cmml">ϕ</mi></msub><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml">z</mi><mo fence="false" id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="S3.E1.m1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></msub><mo id="S3.E1.m1.6.6.1.1.1.2" xref="S3.E1.m1.6.6.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.2.cmml"><mo id="S3.E1.m1.6.6.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.6.6.1.1.1.1.2.1.cmml">[</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.3.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3.1.cmml">log</mi><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.3a" lspace="0.167em" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3.cmml">⁡</mo><msub id="S3.E1.m1.6.6.1.1.1.1.1.1.3.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.3.2.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3.2.2.cmml">p</mi><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.3.2.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3.2.3.cmml">θ</mi></msub></mrow><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mo fence="false" id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.3.cmml">z</mi></mrow><mo id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.6.6.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.6.6.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo id="S3.E1.m1.7.7.2.3" xref="S3.E1.m1.7.7.2.3.cmml">+</mo><mrow id="S3.E1.m1.7.7.2.2" xref="S3.E1.m1.7.7.2.2.cmml"><mtext id="S3.E1.m1.7.7.2.2.3" xref="S3.E1.m1.7.7.2.2.3a.cmml">KL</mtext><mo id="S3.E1.m1.7.7.2.2.2" xref="S3.E1.m1.7.7.2.2.2.cmml">⁢</mo><mrow id="S3.E1.m1.7.7.2.2.1.1" xref="S3.E1.m1.7.7.2.2.1.1.1.cmml"><mo id="S3.E1.m1.7.7.2.2.1.1.2" stretchy="false" xref="S3.E1.m1.7.7.2.2.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.7.7.2.2.1.1.1" xref="S3.E1.m1.7.7.2.2.1.1.1.cmml"><mrow id="S3.E1.m1.7.7.2.2.1.1.1.1" xref="S3.E1.m1.7.7.2.2.1.1.1.1.cmml"><msub id="S3.E1.m1.7.7.2.2.1.1.1.1.3" xref="S3.E1.m1.7.7.2.2.1.1.1.1.3.cmml"><mi id="S3.E1.m1.7.7.2.2.1.1.1.1.3.2" xref="S3.E1.m1.7.7.2.2.1.1.1.1.3.2.cmml">q</mi><mi id="S3.E1.m1.7.7.2.2.1.1.1.1.3.3" xref="S3.E1.m1.7.7.2.2.1.1.1.1.3.3.cmml">ϕ</mi></msub><mo id="S3.E1.m1.7.7.2.2.1.1.1.1.2" xref="S3.E1.m1.7.7.2.2.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.7.7.2.2.1.1.1.1.1.1" xref="S3.E1.m1.7.7.2.2.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.7.7.2.2.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.7.7.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.7.7.2.2.1.1.1.1.1.1.1" xref="S3.E1.m1.7.7.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.7.7.2.2.1.1.1.1.1.1.1.2" xref="S3.E1.m1.7.7.2.2.1.1.1.1.1.1.1.2.cmml">z</mi><mo fence="false" id="S3.E1.m1.7.7.2.2.1.1.1.1.1.1.1.1" xref="S3.E1.m1.7.7.2.2.1.1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.E1.m1.7.7.2.2.1.1.1.1.1.1.1.3" xref="S3.E1.m1.7.7.2.2.1.1.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="S3.E1.m1.7.7.2.2.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.7.7.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.7.7.2.2.1.1.1.2" xref="S3.E1.m1.7.7.2.2.1.1.1.2.cmml">∥</mo><mrow id="S3.E1.m1.7.7.2.2.1.1.1.3" xref="S3.E1.m1.7.7.2.2.1.1.1.3.cmml"><mi id="S3.E1.m1.7.7.2.2.1.1.1.3.2" xref="S3.E1.m1.7.7.2.2.1.1.1.3.2.cmml">p</mi><mo id="S3.E1.m1.7.7.2.2.1.1.1.3.1" xref="S3.E1.m1.7.7.2.2.1.1.1.3.1.cmml">⁢</mo><mrow id="S3.E1.m1.7.7.2.2.1.1.1.3.3.2" xref="S3.E1.m1.7.7.2.2.1.1.1.3.cmml"><mo id="S3.E1.m1.7.7.2.2.1.1.1.3.3.2.1" stretchy="false" xref="S3.E1.m1.7.7.2.2.1.1.1.3.cmml">(</mo><mi id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml">z</mi><mo id="S3.E1.m1.7.7.2.2.1.1.1.3.3.2.2" stretchy="false" xref="S3.E1.m1.7.7.2.2.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.7.7.2.2.1.1.3" stretchy="false" xref="S3.E1.m1.7.7.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.7b"><apply id="S3.E1.m1.7.7.cmml" xref="S3.E1.m1.7.7"><eq id="S3.E1.m1.7.7.3.cmml" xref="S3.E1.m1.7.7.3"></eq><apply id="S3.E1.m1.7.7.4.cmml" xref="S3.E1.m1.7.7.4"><times id="S3.E1.m1.7.7.4.1.cmml" xref="S3.E1.m1.7.7.4.1"></times><ci id="S3.E1.m1.7.7.4.2.cmml" xref="S3.E1.m1.7.7.4.2">ℒ</ci><vector id="S3.E1.m1.7.7.4.3.1.cmml" xref="S3.E1.m1.7.7.4.3.2"><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝜃</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">italic-ϕ</ci><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">𝑥</ci></vector></apply><apply id="S3.E1.m1.7.7.2.cmml" xref="S3.E1.m1.7.7.2"><plus id="S3.E1.m1.7.7.2.3.cmml" xref="S3.E1.m1.7.7.2.3"></plus><apply id="S3.E1.m1.6.6.1.1.cmml" xref="S3.E1.m1.6.6.1.1"><minus id="S3.E1.m1.6.6.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1"></minus><apply id="S3.E1.m1.6.6.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1"><times id="S3.E1.m1.6.6.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.2"></times><apply id="S3.E1.m1.6.6.1.1.1.3.cmml" xref="S3.E1.m1.6.6.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.1.3.1.cmml" xref="S3.E1.m1.6.6.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.1.3.2.cmml" xref="S3.E1.m1.6.6.1.1.1.3.2">𝔼</ci><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><times id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.3.2">𝑞</ci><ci id="S3.E1.m1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.3.3">italic-ϕ</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">𝑧</ci><ci id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">𝑥</ci></apply></apply></apply><apply id="S3.E1.m1.6.6.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.6.6.1.1.1.1.2.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1"><times id="S3.E1.m1.6.6.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.2"></times><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3"><log id="S3.E1.m1.6.6.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3.1"></log><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3.2.2">𝑝</ci><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.3.2.3">𝜃</ci></apply></apply><apply id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.6.6.1.1.1.1.1.1.1.1.1.3">𝑧</ci></apply></apply></apply></apply></apply><apply id="S3.E1.m1.7.7.2.2.cmml" xref="S3.E1.m1.7.7.2.2"><times id="S3.E1.m1.7.7.2.2.2.cmml" xref="S3.E1.m1.7.7.2.2.2"></times><ci id="S3.E1.m1.7.7.2.2.3a.cmml" xref="S3.E1.m1.7.7.2.2.3"><mtext id="S3.E1.m1.7.7.2.2.3.cmml" xref="S3.E1.m1.7.7.2.2.3">KL</mtext></ci><apply id="S3.E1.m1.7.7.2.2.1.1.1.cmml" xref="S3.E1.m1.7.7.2.2.1.1"><csymbol cd="latexml" id="S3.E1.m1.7.7.2.2.1.1.1.2.cmml" xref="S3.E1.m1.7.7.2.2.1.1.1.2">conditional</csymbol><apply id="S3.E1.m1.7.7.2.2.1.1.1.1.cmml" xref="S3.E1.m1.7.7.2.2.1.1.1.1"><times id="S3.E1.m1.7.7.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.2.2.1.1.1.1.2"></times><apply id="S3.E1.m1.7.7.2.2.1.1.1.1.3.cmml" xref="S3.E1.m1.7.7.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.2.2.1.1.1.1.3.1.cmml" xref="S3.E1.m1.7.7.2.2.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.7.7.2.2.1.1.1.1.3.2.cmml" xref="S3.E1.m1.7.7.2.2.1.1.1.1.3.2">𝑞</ci><ci id="S3.E1.m1.7.7.2.2.1.1.1.1.3.3.cmml" xref="S3.E1.m1.7.7.2.2.1.1.1.1.3.3">italic-ϕ</ci></apply><apply id="S3.E1.m1.7.7.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.2.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.7.7.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.2.2.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.7.7.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.2.2.1.1.1.1.1.1.1.2">𝑧</ci><ci id="S3.E1.m1.7.7.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.7.7.2.2.1.1.1.1.1.1.1.3">𝑥</ci></apply></apply><apply id="S3.E1.m1.7.7.2.2.1.1.1.3.cmml" xref="S3.E1.m1.7.7.2.2.1.1.1.3"><times id="S3.E1.m1.7.7.2.2.1.1.1.3.1.cmml" xref="S3.E1.m1.7.7.2.2.1.1.1.3.1"></times><ci id="S3.E1.m1.7.7.2.2.1.1.1.3.2.cmml" xref="S3.E1.m1.7.7.2.2.1.1.1.3.2">𝑝</ci><ci id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5">𝑧</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.7c">\mathcal{L}(\theta,\phi;x)=-\mathbb{E}_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)]+%
\text{KL}(q_{\phi}(z|x)\|p(z))</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.7d">caligraphic_L ( italic_θ , italic_ϕ ; italic_x ) = - blackboard_E start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ( italic_z | italic_x ) end_POSTSUBSCRIPT [ roman_log italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x | italic_z ) ] + KL ( italic_q start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ( italic_z | italic_x ) ∥ italic_p ( italic_z ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">The flexibility in generating new data points through this probabilistic framework makes VAEs particularly suitable for tasks like medical image augmentation, where capturing the diversity of pathological features is crucial.</p>
</div>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.1">The capabilities of VAEs are leveraged to generate synthetic images tailored for each class within the dataset. By training class-specific autoencoders, the unique characteristics and nuances of each class are captured in the latent space.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="243" id="S3.F1.g1" src="extracted/5892627/rsz_miccai_figure1.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The encoding and decoding process in VAEs for synthetic image generation via latent vector interpolation.</figcaption>
</figure>
<div class="ltx_para" id="S3.p7">
<p class="ltx_p" id="S3.p7.3">Post-training, the VAE synthesizes new images by performing an interpolation between the latent representations of two images within the same class, with the process illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#S3.F1" title="Figure 1 ‣ 3 Methodology ‣ Enhancing Image Classification in Small and Unbalanced Datasets through Synthetic Data Augmentation"><span class="ltx_text ltx_ref_tag">1</span></a>. This interpolation is achieved by computing a weighted sum of their latent vectors <math alttext="z_{1}" class="ltx_Math" display="inline" id="S3.p7.1.m1.1"><semantics id="S3.p7.1.m1.1a"><msub id="S3.p7.1.m1.1.1" xref="S3.p7.1.m1.1.1.cmml"><mi id="S3.p7.1.m1.1.1.2" xref="S3.p7.1.m1.1.1.2.cmml">z</mi><mn id="S3.p7.1.m1.1.1.3" xref="S3.p7.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p7.1.m1.1b"><apply id="S3.p7.1.m1.1.1.cmml" xref="S3.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p7.1.m1.1.1.1.cmml" xref="S3.p7.1.m1.1.1">subscript</csymbol><ci id="S3.p7.1.m1.1.1.2.cmml" xref="S3.p7.1.m1.1.1.2">𝑧</ci><cn id="S3.p7.1.m1.1.1.3.cmml" type="integer" xref="S3.p7.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.1.m1.1c">z_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.p7.1.m1.1d">italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="z_{2}" class="ltx_Math" display="inline" id="S3.p7.2.m2.1"><semantics id="S3.p7.2.m2.1a"><msub id="S3.p7.2.m2.1.1" xref="S3.p7.2.m2.1.1.cmml"><mi id="S3.p7.2.m2.1.1.2" xref="S3.p7.2.m2.1.1.2.cmml">z</mi><mn id="S3.p7.2.m2.1.1.3" xref="S3.p7.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p7.2.m2.1b"><apply id="S3.p7.2.m2.1.1.cmml" xref="S3.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p7.2.m2.1.1.1.cmml" xref="S3.p7.2.m2.1.1">subscript</csymbol><ci id="S3.p7.2.m2.1.1.2.cmml" xref="S3.p7.2.m2.1.1.2">𝑧</ci><cn id="S3.p7.2.m2.1.1.3.cmml" type="integer" xref="S3.p7.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.2.m2.1c">z_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.p7.2.m2.1d">italic_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, yielding a new latent representation, denoted as <math alttext="z_{interp}" class="ltx_Math" display="inline" id="S3.p7.3.m3.1"><semantics id="S3.p7.3.m3.1a"><msub id="S3.p7.3.m3.1.1" xref="S3.p7.3.m3.1.1.cmml"><mi id="S3.p7.3.m3.1.1.2" xref="S3.p7.3.m3.1.1.2.cmml">z</mi><mrow id="S3.p7.3.m3.1.1.3" xref="S3.p7.3.m3.1.1.3.cmml"><mi id="S3.p7.3.m3.1.1.3.2" xref="S3.p7.3.m3.1.1.3.2.cmml">i</mi><mo id="S3.p7.3.m3.1.1.3.1" xref="S3.p7.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.p7.3.m3.1.1.3.3" xref="S3.p7.3.m3.1.1.3.3.cmml">n</mi><mo id="S3.p7.3.m3.1.1.3.1a" xref="S3.p7.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.p7.3.m3.1.1.3.4" xref="S3.p7.3.m3.1.1.3.4.cmml">t</mi><mo id="S3.p7.3.m3.1.1.3.1b" xref="S3.p7.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.p7.3.m3.1.1.3.5" xref="S3.p7.3.m3.1.1.3.5.cmml">e</mi><mo id="S3.p7.3.m3.1.1.3.1c" xref="S3.p7.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.p7.3.m3.1.1.3.6" xref="S3.p7.3.m3.1.1.3.6.cmml">r</mi><mo id="S3.p7.3.m3.1.1.3.1d" xref="S3.p7.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.p7.3.m3.1.1.3.7" xref="S3.p7.3.m3.1.1.3.7.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p7.3.m3.1b"><apply id="S3.p7.3.m3.1.1.cmml" xref="S3.p7.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p7.3.m3.1.1.1.cmml" xref="S3.p7.3.m3.1.1">subscript</csymbol><ci id="S3.p7.3.m3.1.1.2.cmml" xref="S3.p7.3.m3.1.1.2">𝑧</ci><apply id="S3.p7.3.m3.1.1.3.cmml" xref="S3.p7.3.m3.1.1.3"><times id="S3.p7.3.m3.1.1.3.1.cmml" xref="S3.p7.3.m3.1.1.3.1"></times><ci id="S3.p7.3.m3.1.1.3.2.cmml" xref="S3.p7.3.m3.1.1.3.2">𝑖</ci><ci id="S3.p7.3.m3.1.1.3.3.cmml" xref="S3.p7.3.m3.1.1.3.3">𝑛</ci><ci id="S3.p7.3.m3.1.1.3.4.cmml" xref="S3.p7.3.m3.1.1.3.4">𝑡</ci><ci id="S3.p7.3.m3.1.1.3.5.cmml" xref="S3.p7.3.m3.1.1.3.5">𝑒</ci><ci id="S3.p7.3.m3.1.1.3.6.cmml" xref="S3.p7.3.m3.1.1.3.6">𝑟</ci><ci id="S3.p7.3.m3.1.1.3.7.cmml" xref="S3.p7.3.m3.1.1.3.7">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.3.m3.1c">z_{interp}</annotation><annotation encoding="application/x-llamapun" id="S3.p7.3.m3.1d">italic_z start_POSTSUBSCRIPT italic_i italic_n italic_t italic_e italic_r italic_p end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
</div>
<div class="ltx_para" id="S3.p8">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="z_{interp}=\alpha z_{1}+(1-\alpha)z_{2}\ " class="ltx_Math" display="block" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">z</mi><mrow id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.3.3.2" xref="S3.E2.m1.1.1.3.3.2.cmml">i</mi><mo id="S3.E2.m1.1.1.3.3.1" xref="S3.E2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.3.3.3" xref="S3.E2.m1.1.1.3.3.3.cmml">n</mi><mo id="S3.E2.m1.1.1.3.3.1a" xref="S3.E2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.3.3.4" xref="S3.E2.m1.1.1.3.3.4.cmml">t</mi><mo id="S3.E2.m1.1.1.3.3.1b" xref="S3.E2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.3.3.5" xref="S3.E2.m1.1.1.3.3.5.cmml">e</mi><mo id="S3.E2.m1.1.1.3.3.1c" xref="S3.E2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.3.3.6" xref="S3.E2.m1.1.1.3.3.6.cmml">r</mi><mo id="S3.E2.m1.1.1.3.3.1d" xref="S3.E2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.E2.m1.1.1.3.3.7" xref="S3.E2.m1.1.1.3.3.7.cmml">p</mi></mrow></msub><mo id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.3.2.cmml">α</mi><mo id="S3.E2.m1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.3.1.cmml">⁢</mo><msub id="S3.E2.m1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.3.3.2.cmml">z</mi><mn id="S3.E2.m1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.3.3.3.cmml">1</mn></msub></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml">+</mo><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mn id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.cmml">α</mi></mrow><mo id="S3.E2.m1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">⁢</mo><msub id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml">z</mi><mn id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml">2</mn></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"></eq><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">𝑧</ci><apply id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3"><times id="S3.E2.m1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3.1"></times><ci id="S3.E2.m1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.2">𝑖</ci><ci id="S3.E2.m1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3">𝑛</ci><ci id="S3.E2.m1.1.1.3.3.4.cmml" xref="S3.E2.m1.1.1.3.3.4">𝑡</ci><ci id="S3.E2.m1.1.1.3.3.5.cmml" xref="S3.E2.m1.1.1.3.3.5">𝑒</ci><ci id="S3.E2.m1.1.1.3.3.6.cmml" xref="S3.E2.m1.1.1.3.3.6">𝑟</ci><ci id="S3.E2.m1.1.1.3.3.7.cmml" xref="S3.E2.m1.1.1.3.3.7">𝑝</ci></apply></apply><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><plus id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2"></plus><apply id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3"><times id="S3.E2.m1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.3.1"></times><ci id="S3.E2.m1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.3.2">𝛼</ci><apply id="S3.E2.m1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.3.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.3.3.2">𝑧</ci><cn id="S3.E2.m1.1.1.1.3.3.3.cmml" type="integer" xref="S3.E2.m1.1.1.1.3.3.3">1</cn></apply></apply><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"></times><apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><minus id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"></minus><cn id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" type="integer" xref="S3.E2.m1.1.1.1.1.1.1.1.2">1</cn><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3">𝛼</ci></apply><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2">𝑧</ci><cn id="S3.E2.m1.1.1.1.1.3.3.cmml" type="integer" xref="S3.E2.m1.1.1.1.1.3.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">z_{interp}=\alpha z_{1}+(1-\alpha)z_{2}\ </annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">italic_z start_POSTSUBSCRIPT italic_i italic_n italic_t italic_e italic_r italic_p end_POSTSUBSCRIPT = italic_α italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + ( 1 - italic_α ) italic_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.p9">
<p class="ltx_p" id="S3.p9.3">where <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.p9.1.m1.1"><semantics id="S3.p9.1.m1.1a"><mi id="S3.p9.1.m1.1.1" xref="S3.p9.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.p9.1.m1.1b"><ci id="S3.p9.1.m1.1.1.cmml" xref="S3.p9.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.p9.1.m1.1d">italic_α</annotation></semantics></math> corresponds to the interpolation weight <math alttext="[0,1]" class="ltx_Math" display="inline" id="S3.p9.2.m2.2"><semantics id="S3.p9.2.m2.2a"><mrow id="S3.p9.2.m2.2.3.2" xref="S3.p9.2.m2.2.3.1.cmml"><mo id="S3.p9.2.m2.2.3.2.1" stretchy="false" xref="S3.p9.2.m2.2.3.1.cmml">[</mo><mn id="S3.p9.2.m2.1.1" xref="S3.p9.2.m2.1.1.cmml">0</mn><mo id="S3.p9.2.m2.2.3.2.2" xref="S3.p9.2.m2.2.3.1.cmml">,</mo><mn id="S3.p9.2.m2.2.2" xref="S3.p9.2.m2.2.2.cmml">1</mn><mo id="S3.p9.2.m2.2.3.2.3" stretchy="false" xref="S3.p9.2.m2.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p9.2.m2.2b"><interval closure="closed" id="S3.p9.2.m2.2.3.1.cmml" xref="S3.p9.2.m2.2.3.2"><cn id="S3.p9.2.m2.1.1.cmml" type="integer" xref="S3.p9.2.m2.1.1">0</cn><cn id="S3.p9.2.m2.2.2.cmml" type="integer" xref="S3.p9.2.m2.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.2.m2.2c">[0,1]</annotation><annotation encoding="application/x-llamapun" id="S3.p9.2.m2.2d">[ 0 , 1 ]</annotation></semantics></math>. This parameter modulates the contribution of each original image to the synthesized image. The resulting latent vector <math alttext="z_{interp}" class="ltx_Math" display="inline" id="S3.p9.3.m3.1"><semantics id="S3.p9.3.m3.1a"><msub id="S3.p9.3.m3.1.1" xref="S3.p9.3.m3.1.1.cmml"><mi id="S3.p9.3.m3.1.1.2" xref="S3.p9.3.m3.1.1.2.cmml">z</mi><mrow id="S3.p9.3.m3.1.1.3" xref="S3.p9.3.m3.1.1.3.cmml"><mi id="S3.p9.3.m3.1.1.3.2" xref="S3.p9.3.m3.1.1.3.2.cmml">i</mi><mo id="S3.p9.3.m3.1.1.3.1" xref="S3.p9.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.p9.3.m3.1.1.3.3" xref="S3.p9.3.m3.1.1.3.3.cmml">n</mi><mo id="S3.p9.3.m3.1.1.3.1a" xref="S3.p9.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.p9.3.m3.1.1.3.4" xref="S3.p9.3.m3.1.1.3.4.cmml">t</mi><mo id="S3.p9.3.m3.1.1.3.1b" xref="S3.p9.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.p9.3.m3.1.1.3.5" xref="S3.p9.3.m3.1.1.3.5.cmml">e</mi><mo id="S3.p9.3.m3.1.1.3.1c" xref="S3.p9.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.p9.3.m3.1.1.3.6" xref="S3.p9.3.m3.1.1.3.6.cmml">r</mi><mo id="S3.p9.3.m3.1.1.3.1d" xref="S3.p9.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.p9.3.m3.1.1.3.7" xref="S3.p9.3.m3.1.1.3.7.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p9.3.m3.1b"><apply id="S3.p9.3.m3.1.1.cmml" xref="S3.p9.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p9.3.m3.1.1.1.cmml" xref="S3.p9.3.m3.1.1">subscript</csymbol><ci id="S3.p9.3.m3.1.1.2.cmml" xref="S3.p9.3.m3.1.1.2">𝑧</ci><apply id="S3.p9.3.m3.1.1.3.cmml" xref="S3.p9.3.m3.1.1.3"><times id="S3.p9.3.m3.1.1.3.1.cmml" xref="S3.p9.3.m3.1.1.3.1"></times><ci id="S3.p9.3.m3.1.1.3.2.cmml" xref="S3.p9.3.m3.1.1.3.2">𝑖</ci><ci id="S3.p9.3.m3.1.1.3.3.cmml" xref="S3.p9.3.m3.1.1.3.3">𝑛</ci><ci id="S3.p9.3.m3.1.1.3.4.cmml" xref="S3.p9.3.m3.1.1.3.4">𝑡</ci><ci id="S3.p9.3.m3.1.1.3.5.cmml" xref="S3.p9.3.m3.1.1.3.5">𝑒</ci><ci id="S3.p9.3.m3.1.1.3.6.cmml" xref="S3.p9.3.m3.1.1.3.6">𝑟</ci><ci id="S3.p9.3.m3.1.1.3.7.cmml" xref="S3.p9.3.m3.1.1.3.7">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.3.m3.1c">z_{interp}</annotation><annotation encoding="application/x-llamapun" id="S3.p9.3.m3.1d">italic_z start_POSTSUBSCRIPT italic_i italic_n italic_t italic_e italic_r italic_p end_POSTSUBSCRIPT</annotation></semantics></math> is then decoded to generate a new synthetic image. This new image merges characteristics of the parent images while maintaining the class’s defining features, effectively enriching the dataset’s diversity and quantity.</p>
</div>
<div class="ltx_para" id="S3.p10">
<p class="ltx_p" id="S3.p10.1">By integrating additional synthetic images for each class, this iterative and monitored approach not only augments the dataset but also introduces meaningful and realistic variations which enrich the feature space.</p>
</div>
<div class="ltx_para" id="S3.p11">
<p class="ltx_p" id="S3.p11.1">The incorporation of these synthetic images into the training process is designed to enhance the classifiers’ performance, particularly by improving its understanding of the underlying distribution of the underrepresented classes, thereby ensuring a more comprehensive representation of each class’s feature space and subsequently elevating model accuracy.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Our study utilized a dataset comprising 321 esophagogastroduodenoscopy (EGD) images, capturing various stomach regions including the esophagus, duodenum, antrum, body, and fundus. Images were labelled into different categories according to different degree of stomach cleanliness by seven clinicians following the definitions of the Barcelona scale.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="174" id="S4.F2.g1" src="extracted/5892627/samples.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Sample image for each of the classes, labelled according to Barcelona scale.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">The consensus among the experts determined the final class assignment: 1) class 0, which corresponds to images with presence of non aspirable solid or semisolid particles, bile or foam preventing from clear mucosa visualization; 2) class 1, which corresponds to images with small amount of semisolid particles, bile or foam and 3) class 2, which comprises images without any kind of rest, allowing a complete visualization of the mucosa. We show in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#S4.F2" title="Figure 2 ‣ 4.1 Dataset ‣ 4 Experimental Setup ‣ Enhancing Image Classification in Small and Unbalanced Datasets through Synthetic Data Augmentation"><span class="ltx_text ltx_ref_tag">2</span></a> an example of some of the images in the dataset.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">With respect to class distribution within the dataset, class 0 was represented with 65 images, class 1 with 91 and class 2 with 165. The dataset was split in a standard 80-20 fashion for training/validation and testing.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Metrics</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To assess model performance, a suite of metrics was employed, including overall accuracy, precision, recall, and F1-score, complemented by class-specific measures for a thorough analysis. The evaluation concentrated on the efficacy of several augmentation strategies, encompassing traditional methods such as rotations and mirroring, as well the proposed approach involving synthetic data generation via VAEs. Results were tabulated to compare the impact of these techniques on model performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Implementation Details</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">Synthetic Data Generation:</span> The synthetic data was generated using class-specific Variational Autoencoders (VAEs). Each VAE was trained separately for each class, capturing the unique characteristics of the respective class. The architecture of the VAE consisted of an encoder and decoder, with the encoder mapping input images to a latent space and the decoder reconstructing images from the latent space. We selected a latent space dimension of 256 based on empirical analysis, and the VAEs were trained for 1000 epochs with a learning rate of 0.0001, using the Adam optimizer. To generate synthetic images, we performed latent space interpolation between pairs of latent vectors within the same class, ensuring realistic and varied synthetic samples. We experimented with different quantities of synthetic images, ultimately finding that adding 300 extra images per class yielded the best performance, hence, results shown in section <a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#S5" title="5 Results ‣ Enhancing Image Classification in Small and Unbalanced Datasets through Synthetic Data Augmentation"><span class="ltx_text ltx_ref_tag">5</span></a> include 300 synthetic images per class.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">Classification:</span> For the classification task, we employed two well-known architectures: EfficientNet-V2 and ResNet-50. Both pretrained models were implemented and fine-tuned using PyTorch. The training process involved a batch size of 24 images and an initial learning rate set to <math alttext="5\times 10^{-4}" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mn id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">5</mn><mo id="S4.SS3.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS3.p2.1.m1.1.1.1.cmml">×</mo><msup id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml"><mn id="S4.SS3.p2.1.m1.1.1.3.2" xref="S4.SS3.p2.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.SS3.p2.1.m1.1.1.3.3" xref="S4.SS3.p2.1.m1.1.1.3.3.cmml"><mo id="S4.SS3.p2.1.m1.1.1.3.3a" xref="S4.SS3.p2.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.SS3.p2.1.m1.1.1.3.3.2" xref="S4.SS3.p2.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><times id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></times><cn id="S4.SS3.p2.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.p2.1.m1.1.1.2">5</cn><apply id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.3.1.cmml" xref="S4.SS3.p2.1.m1.1.1.3">superscript</csymbol><cn id="S4.SS3.p2.1.m1.1.1.3.2.cmml" type="integer" xref="S4.SS3.p2.1.m1.1.1.3.2">10</cn><apply id="S4.SS3.p2.1.m1.1.1.3.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3.3"><minus id="S4.SS3.p2.1.m1.1.1.3.3.1.cmml" xref="S4.SS3.p2.1.m1.1.1.3.3"></minus><cn id="S4.SS3.p2.1.m1.1.1.3.3.2.cmml" type="integer" xref="S4.SS3.p2.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">5\times 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">5 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math>. The learning rate was dynamically adjusted based on performance plateaus, with a reduction factor of 10 upon stagnation in validation loss improvement. To ensure reproducibility and consistency across experiments, a random seed was set to 42.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">Multiple configurations were explored for both EfficientNet-V2 and ResNet-50 architectures, including training on real data with and without traditional augmentations, and extending the dataset with synthetically generated images through VAE interpolation. Synthetic images were integrated into both classically augmented and non-augmented training sets to assess the combined effect of classical and synthetic augmentation techniques on the models’ ability to generalize and accurately classify EGD images.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Experimental results, presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#S5.T1" title="Table 1 ‣ 5 Results ‣ Enhancing Image Classification in Small and Unbalanced Datasets through Synthetic Data Augmentation"><span class="ltx_text ltx_ref_tag">1</span></a>, remark the efficacy of incorporating synthetic data augmentation via class-specific VAEs in enhancing classification performance. A comparative analysis across different models and augmentation strategies reveals that the addition of VAE-generated synthetic images leads to substantial improvements in overall accuracy, precision, recall, and F1-scores.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Classification performance of EfficientNet-V2 and ResNet-50 models with various data augmentation strategies.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T1.1" style="width:433.6pt;height:109pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-107.4pt,26.8pt) scale(0.66875014031466,0.66875014031466) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.2.1">Data</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.3.1">Overall Acc.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.4.1">Overall Prec.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.5.1">Overall Rec.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.6.1">Overall F1</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.7.1">Class-0 Acc.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1.8"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.8.1">Class-1 Acc.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T1.1.1.1.1.9"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.9.1">Class-2 Acc.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.2.1.1" rowspan="3"><span class="ltx_text" id="S5.T1.1.1.2.1.1.1">EfficientNet-V2</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.1.1.2.1.2">Real No Aug.</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.1.3">85.94</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.1.4">86.18</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.1.5">86.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.1.6">85.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.1.7">92.81</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.1.8">64.05</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.2.1.9">96.85</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.1.3.2.1">Real with Aug.</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.3.2.2">87.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.3.2.3">87.51</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.3.2.4">87.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.3.2.5">87.46</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.3.2.6">81.43</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.3.2.7">74.56</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.3.2.8">97.10</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.1.4.3.1">Real + Gen No Aug.</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.4.3.2">89.06</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.4.3.3">88.96</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.4.3.4">88.88</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.4.3.5">88.74</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.4.3.6"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.4.3.6.1">92.85</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.4.3.7">72.22</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.4.3.8">97.15</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.5.4">
<th class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S5.T1.1.1.5.4.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.1.5.4.2">Real + Gen with Aug.</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.5.4.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.5.4.3.1">92.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.5.4.4"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.5.4.4.1">92.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.5.4.5"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.5.4.5.1">91.03</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.5.4.6"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.5.4.6.1">91.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.5.4.7">92.23</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.5.4.8"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.5.4.8.1">82.06</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.5.4.9"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.5.4.9.1">98.73</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T1.1.1.6.5.1" rowspan="3"><span class="ltx_text" id="S5.T1.1.1.6.5.1.1">ResNet-50</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T1.1.1.6.5.2">Real No Aug.</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.6.5.3">82.81</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.6.5.4">82.84</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.6.5.5">82.81</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.6.5.6">81.41</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.6.5.7"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.6.5.7.1">92.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.6.5.8">52.18</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.1.1.6.5.9"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.6.5.9.1">96.72</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.1.7.6.1">Real with Aug.</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.7.6.2">84.38</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.7.6.3">84.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.7.6.4">84.38</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.7.6.5">84.38</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.7.6.6">88.89</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.7.6.7">68.89</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.7.6.8">94.15</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T1.1.1.8.7.1">Real + Gen No Aug.</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.8.7.2">86.02</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.8.7.3">85.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.8.7.4">85.94</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.8.7.5">85.16</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.8.7.6">92.00</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.8.7.7">66.91</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.1.1.8.7.8">96.55</td>
</tr>
<tr class="ltx_tr" id="S5.T1.1.1.9.8">
<th class="ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S5.T1.1.1.9.8.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T1.1.1.9.8.2">Real + Gen with Aug.</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T1.1.1.9.8.3"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.9.8.3.1">89.49</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T1.1.1.9.8.4"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.9.8.4.1">88.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T1.1.1.9.8.5"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.9.8.5.1">89.06</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T1.1.1.9.8.6"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.9.8.6.1">88.74</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T1.1.1.9.8.7">92.15</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T1.1.1.9.8.8"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.9.8.8.1">75.12</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T1.1.1.9.8.9">96.61</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="251" id="S5.F3.g1" src="extracted/5892627/miccaiplot_mejorada_fig3.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Comparison of Class 1 Accuracy Across Different Augmentation Techniques and Classifiers. Improvement points are with respect to the Real No Augmentation bar for each model.</figcaption>
</figure>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Notably, the most pronounced gains are observed in the accuracy metrics for the most challenging underrepresented class; class 1, where the conventional data pool is limited. The proposed strategy achieves the best performance in the majority of the experiments, being a very close second in the remaining cases.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">The impact of different augmentation techniques on Class-1 accuracy is further highlighted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#S5.F3" title="Figure 3 ‣ 5 Results ‣ Enhancing Image Classification in Small and Unbalanced Datasets through Synthetic Data Augmentation"><span class="ltx_text ltx_ref_tag">3</span></a>. The results confirm that synthetic data augmentation, particularly when combined with traditional augmentation techniques, substantially improves the robustness of classification models.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">EfficientNet-V2, when trained with both real and synthetically augmented data, shows an overall accuracy boost from 85.94% to 92.19%, and a significant increase in Class-1 accuracy from 64.05% to 82.06%. Similarly, ResNet-50’s performance escalates from 82.81% to 89.49% in overall accuracy, with Class-1 accuracy rising from 52.18% to 75.12%. These enhancements suggest that synthetic data not only supplements the training set but also instills a better understanding of the feature space associated with each class.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">Importantly, the augmented data appears to guide the model towards a more detailed comprehension of the subtle distinctions within the EGD image classes. This is critical for clinical applications where the differentiation between varying levels of cleanliness directly impacts the diagnostic process and subsequent patient care. Therefore the use of VAEs for data augmentation could suppose an advancement for medical imaging fields struggling with data constraints.</p>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10286v2#S5.F4" title="Figure 4 ‣ 5 Results ‣ Enhancing Image Classification in Small and Unbalanced Datasets through Synthetic Data Augmentation"><span class="ltx_text ltx_ref_tag">4</span></a> represents the data distribution for each class before and after synthetic augmentation. The original sparse distribution of each of the classes, as seen on the left side of each class’s panel, becomes notably denser on the right side, following the application of VAE-based latent vector interpolation. This visual enhancement of the feature space is especially significant for Class 1, the primary focus of our study, where the augmented data points fill previously underpopulated regions, indicating a more balanced representation post-augmentation.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="121" id="S5.F4.g1" src="extracted/5892627/miccai24_fig2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Expansion of feature space for each EGD image class post-augmentation. <span class="ltx_text ltx_font_italic" id="S5.F4.3.1">X</span> and <span class="ltx_text ltx_font_italic" id="S5.F4.4.2">Y</span> axes represent PCA features 1 and 2 respectively.</figcaption>
</figure>
<div class="ltx_para" id="S5.p7">
<p class="ltx_p" id="S5.p7.1">These visual findings align with the quantitative improvements in classification performance, confirming the value of VAE-based synthetic data in addressing class imbalance and enhancing model training for medical diagnostics.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions and Future Work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The research proposed in this paper demonstrates the effectiveness of synthetic data augmentation using class-specific Variational AutoEncoders (VAEs) for medical image classification, specifically targeting esophagogastroduodenoscopy (EGD) images. This is the first time such a system with these characteristics has been applied to the EGD imaging field, marking a significant impact in this domain. By interpolating latent representations within classes, a new method that significantly counters the limitations posed by small and unbalanced datasets has been developed and validated.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">This approach has improved performance, particularly for challenging underrepresented classes, by effectively filling feature space gaps and achieving a more uniform dataset distribution. The success of this approach is demonstrated across two distinct architectures, EfficientNet-V2 and ResNet-50, showing its adaptability and the broad applicability of synthetic data augmentation in improving model classification capabilities.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Furthermore, the study explored the compounded benefits of combining traditional augmentation techniques with synthetic data augmentation, revealing a notable enhancement in the models’ ability to generalize and accurately classify EGD images, discovering a synergistic effect.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">The proposed work represents a significant step forward in utilizing AI for medical diagnostics, particularly by employing a methodologically innovative approach to synthetic data generation. By focusing on class-specific latent representation interpolations, it provides a scalable solution to the persisting problem of data scarcity and imbalance in medical imaging. This innovative methodology has set a precedent in the EGD imaging field, paving the way for its potential application in other medical imaging domains.</p>
</div>
<div class="ltx_para" id="S6.p5">
<p class="ltx_p" id="S6.p5.1">Despite these promising results, several limitations should be noted. The synthetic images generated by VAEs, while effective, may not capture all the nuances of real medical images, potentially leading to some biases in the training process. Additionally, the study was conducted on a relatively small dataset, which may limit the generalizability of the findings.</p>
</div>
<div class="ltx_para" id="S6.p6">
<p class="ltx_p" id="S6.p6.1">This contribution lays the groundwork for future explorations into more sophisticated synthetic data generation methods and their application across various domains within medical image analysis. A compelling direction for this research could involve adopting latent diffusion models (LDMs), well known for their capacity to generate high-quality, realistic images, to augment the diversity and authenticity of synthetic medical images. This approach, coupled with assessing the impact of such augmentation techniques on larger and more diverse medical image datasets, could significantly advance the scalability, robustness and applicability of these methods.</p>
</div>
<div class="ltx_para" id="S6.p7">
<p class="ltx_p" id="S6.p7.1">Finally, refining the interpolation techniques for synthetic image creation to achieve more precise and clinically relevant datasets remains a critical area for development. Future efforts could also focus on understanding how synthetic data influences model interpretability and reliability in real-world clinical environments, aiming to not only elevate classification accuracy but also improve the trust and efficacy of diagnostic models in medical practice.</p>
</div>
<div class="ltx_para" id="S6.p8">
<span class="ltx_ERROR undefined" id="S6.p8.1">{credits}</span>
</div>
<section class="ltx_subsubsection" id="S6.SS0.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.0.1 </span><span class="ltx_ERROR undefined" id="S6.SS0.SSS1.1.1">\discintname</span>
</h4>
<div class="ltx_para" id="S6.SS0.SSS1.p1">
<p class="ltx_p" id="S6.SS0.SSS1.p1.1">The authors have no competing interests to declare.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was supported by the following Grant Numbers: PID2020-120311RB-I00 and RED2022-134964-T and funded by MCIN-AEI/10.13039/501100011033.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Y. Ezoe, M. Muto et al., "Magnifying narrowband imaging is more accurate than conventional white-light imaging in diagnosis of gastric mucosal cancer" in
<em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Gastroenterology</em>, 141(6), pp.2017-2025, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
P.G.D. Guillena, V.J.M. Alvarado et al., "Gastric cancer missed at esophagogastroduodenoscopy in a well-defined Spanish population" in <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1"> Digestive and Liver Disease</em>, 51(8), pp.1123-1129, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
H. Tsukuma, A. Oshima et al., "Natural history of early gastric cancer: a non-concurrent, long term, follow up study" in <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Gut</em>, 47(5), pp.618-621.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
M. Romańczyk, B. Ostrowski et al., "Scoring system assessing mucosal visibility of upper gastrointestinal tract: The POLPREP scale" in <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Journal of Gastroenterology and Hepatology</em>, 37(1), pp.164-168, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
H. Córdova, E. Barreiro-Alonso et al., "Applicability of the Barcelona scale to assess the quality of cleanliness of mucosa at esophagogastroduodenoscopy" in <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Gastroenterología y Hepatología</em>, 47, pp. 246-2522024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
M.S. Haithami et al., "Automatic Bowel Preparation Assessment Using Deep Learning", in <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">International Conference on Pattern Recognition</em>, pp. 574-588, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
U. Garay-Maestre, A. Gallego, and J. Calvo-Zaragoza, "Data Augmentation via Variational Auto-Encoders," in <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
M. Auzine et al., "Endoscopic Image Analysis Using Deep Convolutional GAN and Traditional Data Augmentation," in <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the International Conference on Electronics, Communications and Control Engineering (ICECCME)</em>, IEEE, pp. 1-6, 2022

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
X. Liu, et al., "Data Augmentation via Latent Space Interpolation for Image Classification," in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the International Conference on Pattern Recognition (ICPR)</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Oring, "Autoencoder Image Interpolation by Shaping the Latent Space," in <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
P. Cristovao, et al., "Generating In-Between Images Through Learned Latent Space Representation Using Variational Autoencoders," in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">IEEE Access</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
F.J. Moreno-Barea, et al., "Improving classification accuracy using data augmentation on small data sets," in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Expert Systems with Applications</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M. Elbattah, et al., "Variational Autoencoder for Image-Based Augmentation of Eye-Tracking Data," in <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Journal of Imaging</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Z. Wan, Y. Zhang, H. He, "Variational Autoencoder Based Synthetic Data Generation for Imbalanced Learning," in <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">IEEE Symposium on Computational Intelligence (SSCI)</em>, Honolulu, HI, USA, 2017, pp. 1-7, doi:10.1109/SSCI.2017.8285168.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Y. Zhu, X. Yiwen, et al. "A CNN-based cleanliness evaluation for bowel preparation in colonoscopy.", in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">12th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)</em>, pp. 1-5. IEEE, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J. H. Nam et al., "Development of a deep learning-based software for calculating cleansing score in small bowel capsule endoscopy", in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Scientific reports</em>, 11(1), 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
K. He et al., "Deep Residual Learning for Image Recognition," in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
K. Simonyan and A. Zisserman, "Very Deep Convolutional Networks for Large-Scale Image Recognition", arXiv:1409.1556, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Y. Zhou, H. Sahak, and J. Ba, "Using Synthetic Data for Data Augmentation to Improve Classification Accuracy," in <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the Workshop on Challenges in Deployable Machine Learning, ICML</em>, Honolulu, Hawaii, USA, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M. Tan and Q. V. Le, "EfficientNetV2: Smaller Models and Faster Training," in <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the International Conference on Machine Learning (ICML)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
D. P. Kingma and M. Welling, "Auto-Encoding Variational Bayes," in <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 2nd International Conference on Learning Representations (ICLR)</em>, Banff, AB, Canada, 2014.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct  1 11:08:10 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
