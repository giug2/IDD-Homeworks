<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation</title>
<!--Generated on Wed Sep  4 21:21:05 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.03087v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S1" title="In Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S2" title="In Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S3" title="In Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Our Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S3.SS1" title="In 3 Our Approach ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Main Contributions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S3.SS2" title="In 3 Our Approach ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Crowd Labelling Platform Deployment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S3.SS3" title="In 3 Our Approach ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Segmentation AI Integration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S3.SS4" title="In 3 Our Approach ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Generative AI Integration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S3.SS5" title="In 3 Our Approach ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Test Trials</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S3.SS6" title="In 3 Our Approach ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Merged Crowd Labels</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S3.SS7" title="In 3 Our Approach ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.7 </span>Evaluation Metrics - Comparison with Ground Truth</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4" title="In Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.SS1" title="In 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Quality – Achieving Image Crowd Labelling at a Professional Level</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.SS1.SSS1" title="In 4.1 Quality – Achieving Image Crowd Labelling at a Professional Level ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Segmentation AI Comparison for Crowd Use</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.SS1.SSS2" title="In 4.1 Quality – Achieving Image Crowd Labelling at a Professional Level ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Necessity of AI Assistance and Instruction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.SS1.SSS3" title="In 4.1 Quality – Achieving Image Crowd Labelling at a Professional Level ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Crowd Segmentation in Different Modalities</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.SS2" title="In 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Quantity – Enlarging Dataset with Synthetic Data</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.SS2.SSS1" title="In 4.2 Quantity – Enlarging Dataset with Synthetic Data ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Synthetic Images in Different Modalities</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.SS2.SSS2" title="In 4.2 Quantity – Enlarging Dataset with Synthetic Data ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Efficiency of Enlarged Dataset</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.SS3" title="In 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Enhanced Dataset – Combining Generative AI and Crowdsourcing</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S5" title="In Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S6" title="In Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Acknowledgements</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amir Syahmi*, Xiangrong L. Lu*, Yinxuan Li*, Haoxuan Yao*, Hanjun Jiang,
<br class="ltx_break"/>Ishita Acharya, Shiyi Wang, Yang Nan, Xiaodan Xing, Guang Yang†
<br class="ltx_break"/>
Department of Bioengineering and Imperial-X, Imperial College London
<br class="ltx_break"/>London, W12 7SL, United Kingdom
<br class="ltx_break"/><math alttext="\{" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" maxsize="90%" minsize="90%" xref="id1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">{</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="id2.2.1" style="font-size:90%;">amir.syahmi21, xiangrong.lu21, yinxuan.li21, haoxuan.yao21, barry.jiang20,
<br class="ltx_break"/>ishita.acharya20, s.wang22, y.nan20, x.xing, g.yang<math alttext="\}" class="ltx_Math" display="inline" id="id2.2.1.m1.1"><semantics id="id2.2.1.m1.1a"><mo id="id2.2.1.m1.1.1" stretchy="false" xref="id2.2.1.m1.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="id2.2.1.m1.1b"><ci id="id2.2.1.m1.1.1.cmml" xref="id2.2.1.m1.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.1.m1.1c">\}</annotation><annotation encoding="application/x-llamapun" id="id2.2.1.m1.1d">}</annotation></semantics></math>@imperial.ac.uk<span class="ltx_text ltx_font_serif" id="id2.2.1.1">
</span></span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1"><span class="ltx_text" id="id3.id1.1">Recent advancements in medical imaging and artificial intelligence (AI) have greatly enhanced diagnostic capabilities, but the development of effective deep learning (DL) models is still constrained by the lack of high-quality annotated datasets.
The traditional manual annotation process by medical experts is time- and resource-intensive, limiting the scalability of these datasets.
In this work, we introduce a robust and versatile framework that combines AI and crowdsourcing to improve both the quality and quantity of medical image datasets across different modalities.
Our approach utilises a user-friendly online platform that enables a diverse group of crowd annotators to label medical images efficiently.
By integrating the MedSAM segmentation AI with this platform, we accelerate the annotation process while maintaining expert-level quality through an algorithm that merges crowd-labelled images.
Additionally, we employ pix2pixGAN, a generative AI model, to expand the training dataset with synthetic images that capture realistic morphological features.
These methods are combined into a cohesive framework designed to produce an enhanced dataset, which can serve as a universal pre-processing pipeline to boost the training of any medical deep learning segmentation model.
Our results demonstrate that this framework significantly improves model performance, especially when training data is limited.</span></p>
</div>
<section class="ltx_section ltx_indent_first" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup>*Contributed equally</span></span></span><span class="ltx_note ltx_role_footnote" id="footnote1a"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup>†Correspondence Author</span></span></span><span class="ltx_note ltx_role_footnote" id="footnote1b"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup>The code for this proposed framework is publicly available on GitHub: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Am1rSy/SGC-Enhanced-Dataset" title="">https://github.com/Am1rSy/SGC-Enhanced-Dataset</a>.</span></span></span>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="237" id="S1.F1.g1" src="extracted/5830858/figure/workflow2.png" width="509"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Summary of the proposed framework: Real medical image datasets are provided to a crowdsourced labelling platform and also used to train pix2pixGAN. Annotators would label the real dataset images with assistance from MedSAM. These annotated real images are then used by pix2pixGAN to generate synthetic images, which are subsequently sent back to the labelling platform for further annotation. The labelled synthetic and real images are finally combined into an enhanced dataset. Figure illustrated by Lark</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The rapid advancements in medical imaging technologies, such as CT and MRI, have revolutionised diagnostic capabilities in healthcare by enabling non-invasive visualisation of internal anatomical structures.
Alongside this, the recent progress in artificial intelligence (AI) has led to growing interest and significant improvements in developing medical image analysis algorithms, which subsequently improve automated diagnostics.
These advancements enable healthcare professionals to make more precise diagnoses and develop more effective treatment plans<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib12" title="">12</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib36" title="">36</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib32" title="">32</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Image segmentation is one of the most commonly used analysis techniques and involves the delineation of structures and forms the building blocks of any AI-assisted image labelling algorithm.
However, a major limiting factor to the development of a robust and accurate AI-based medical image analysis model is the lack of high-volume and high-quality annotated training datasets<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib2" title="">2</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">High-quality labelled datasets are crucial as they directly influence the accuracy and reliability of AI models, ensuring these technologies can make precise diagnostic predictions<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib6" title="">6</a>]</cite>.
Generating these labelled datasets is both time-consuming and resource-intensive, making scalability a challenge<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib33" title="">33</a>]</cite>.
Furthermore, hiring medical experts to manually label a large quantity of medical images is costly, and the process is often tedious due to its repetitive nature<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib33" title="">33</a>]</cite>.
Thus, crowdsourcing has been seen as an attractive method to help improve the annotation rate for medical images.
It operates based on allowing untrained individuals to help annotate new, unlabelled datasets<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib28" title="">28</a>]</cite>.
Researchers have explored the potential of crowdsourcing to cost-effectively expand the annotated datasets for medical images<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib27" title="">27</a>]</cite>.
Studies have shown that with clear instructions and examples, non-experts can achieve performance that matches that of trained experts, particularly for certain imaging modalities<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib10" title="">10</a>]</cite>.
However, the complexity of the medical images still needs to be investigated to understand the limitations of crowd-sourcing fully.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The challenge of acquiring a substantial volume of medical images for training AI solutions is another significant bottleneck in the field.
This is due to the high cost and logistical complexity involved in producing such datasets, which require specialised equipment and trained personnel.
Additionally, privacy concerns are also limiting the acquisition due to the handling of sensitive personal information, which usually requires extra data masking procedures.
The diversity of medical imaging modalities (e.g. CT, MRI) further complicates the collection process, as acquiring comprehensive data across all these modalities is a daunting task<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib9" title="">9</a>]</cite>.
Although, the release of datasets publicly by various research groups in recent years (MMWHS<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib35" title="">35</a>]</cite>, FLARE22<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib21" title="">21</a>]</cite>, Cancer Imaging Archive<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib16" title="">16</a>]</cite>) has somewhat mitigated the issue, given the data-intensive nature of AI model training, particularly with Deep Learning (DL) approaches, the demand for extensive datasets remains unabated<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib29" title="">29</a>]</cite>.
Consequently, exploring the potential of generative AI to augment real datasets by creating synthetic but close-to-realistic images presents a promising area of research<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib26" title="">26</a>]</cite>.
This approach can help overcome the inherent limitations of insufficient variety and volume of real datasets, by generating diverse and extensive training data.
Such synthetic data can improve the training of AI models, enabling them to help achieve higher accuracy and generalisability in medical image analysis<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib1" title="">1</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this work, we present a versatile framework that enhances medical image dataset in both quality and quantity by coupling crowdsourcing and generative AI.
This method ultimately increases segmentation accuracy of DL models when available training dataset is limited.</p>
</div>
</section>
<section class="ltx_section ltx_indent_first" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Crowdsourcing in image analysis, seen in Google’s reCAPTCHA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib19" title="">19</a>]</cite> and Duolingo<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib14" title="">14</a>]</cite>, also applies to biomedical imaging.
Studies show that crowds can accurately label complex medical images, demonstrating the potential for medical image analysis.
Platforms like Amazon Mechanical Turk (MTurk)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib13" title="">13</a>]</cite> and Appen Figure Eight<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib20" title="">20</a>]</cite> streamline crowdsourcing by providing a diverse pool of participants and reducing the need for custom setups.
Alternatively, custom platforms like Label Studio<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib15" title="">15</a>]</cite>, though more time-intensive to develop, offer precise control over the crowdsourcing tasks, improving the engagement and specificity of the work.
In summary, crowdsourcing in image analysis extends its utility to biomedical fields, demonstrating significant potential in medical diagnostics.
By utilising diverse participant pools and flexible setup options, these methods elevate data accuracy and streamline the labelling process, and prove essential for advancing accurate DL approach for medical image segmentation.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Moreover, there has been a lot of research in using generative AI to improve data augmentations<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib34" title="">34</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib30" title="">30</a>]</cite>.
Specifically, Generative Adversarial Network (GAN)-based models<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib7" title="">7</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib8" title="">8</a>]</cite> are widely used for synthesising different medical images, successfully expanding the size of biomedical imaging datasets<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib3" title="">3</a>]</cite>.
Deep Convolutional GAN (DCGAN), an unconditional GAN-based model, has been used to generate high-quality liver lesion region of interests (ROIs)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib5" title="">5</a>]</cite>.
These synthetic images, combined with real images, are used to train Convolutional Neural Networks (CNNs) and greatly improve the performance of CNN classification models, thereby enhancing diagnosis.
Additionally, synthetic MRI images generated by a conditional GAN model have successfully boosted the training of segmentation networks and improved the performance of brain tumour MR image segmentation<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib25" title="">25</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="260" id="S2.F2.g1" src="extracted/5830858/figure/LabelStudioUI.png" width="509"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">Label Studio UI</span></figcaption>
</figure>
</section>
<section class="ltx_section ltx_indent_first" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Our Approach</h2>
<section class="ltx_subsection ltx_indent_first" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Main Contributions</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">By coupling AI and citizen science, we aim to improve the data gathering rate and create an extensive and labelled dataset for future medical image DL research.
This enhancement can be achieved by: deploying a flexible segmentation model to facilitate the labelling process, thereby reducing both time and tedium for crowd annotators; enlisting the aid of the public via crowdsourcing, with adequate guidance, to accelerate the annotation rate; implementing generative AI to expand the medical image datasets with synthetic images.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">By achieving the above aims, we highlight 4 key contributions of this work:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">We proposed a <em class="ltx_emph ltx_font_italic" id="S3.I1.i1.p1.1.1">versatile framework</em> to efficiently and effectively resolve the scarcity and costliness of medical image datasets for DL model training.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">We implemented a state-of-the-art <em class="ltx_emph ltx_font_italic" id="S3.I1.i2.p1.1.1">segmentation AI</em> MedSAM to simplify crowdsourced segmentation which attains labelling at an expert <em class="ltx_emph ltx_font_italic" id="S3.I1.i2.p1.1.2">quality</em>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">We incorporated a novel <em class="ltx_emph ltx_font_italic" id="S3.I1.i3.p1.1.1">generative AI</em> pix2pixGAN to expand the <em class="ltx_emph ltx_font_italic" id="S3.I1.i3.p1.1.2">quantity</em> of existing dataset in different modalities.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">We verified that, using the framework we proposed, the <em class="ltx_emph ltx_font_italic" id="S3.I1.i4.p1.1.1">accuracy</em> and <em class="ltx_emph ltx_font_italic" id="S3.I1.i4.p1.1.2">performance</em> of DL models for medical images would be significantly improved with <em class="ltx_emph ltx_font_italic" id="S3.I1.i4.p1.1.3">small</em> training dataset.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">In short, we established a <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">versatile</span> framework that can expand limited medical image dataset in <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.2">quantity</span> and also with similar label <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.3">quality</span> as the domain experts.
Such dataset will be called <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.4">enhanced dataset</span> (quality + quantity) for the rest of this work.
An overview of our framework can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Crowd Labelling Platform Deployment</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Image labelling tasks for crowds, especially with medical images, are often complex and result in a lack of accuracy and willingness to participate.
We first needed to implement an online platform for the ease of communication and labelling operation from the researchers to a wide audience with various types of device.
Label Studio was chosen as the main data labelling platform as it is open source and contains a simple, friendly user interface (UI) (See Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S2.F2" title="Figure 2 ‣ 2 Related Works ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">2</span></a>)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib15" title="">15</a>]</cite>.
An easily navigable UI is key in this study as the labelling process needs to be straightforward to account for various computer literacy in the public.
We designed the platform to allow the use of a few tools which includes labelling brush, eraser, zooming, etc.
Furthermore, we provided all users with an instructional PDF file containing ground truth label exemplars and a short video to guide them on using the platform (see Supplementary Section 3 and 4).
Label Studio was also chosen because it is easily deployed on online servers<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib15" title="">15</a>]</cite>, a feature well supported by its extensive developer community.
Our current implementation of Label Studio is hosted on the Hugging Face Spaces platform<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib4" title="">4</a>]</cite>.
This hosting platform was selected for its capability to support application deployment using Docker containers.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Segmentation AI Integration</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Segmentation AI assistance has been proven to be an effective approach to further resolve the complexity of labelling<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib11" title="">11</a>]</cite>.
However, the use of segmentation AI aiding in the labelling process needs to be versatile for a wide range of tasks regarding medical images, and intuitive for the users to operate.
Label Studio<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib15" title="">15</a>]</cite> also supports AI integration during the labelling process.
It operates by using another server as a Machine Learning (ML) backend, where the AI model is hosted.
We chose MedSAM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib22" title="">22</a>]</cite> for our AI integration (see Section <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.SS1.SSS1" title="4.1.1 Segmentation AI Comparison for Crowd Use ‣ 4.1 Quality – Achieving Image Crowd Labelling at a Professional Level ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">4.1.1</span></a>).
MedSAM is a zero-shot segmentation model based on Meta’s Segment Anything model (SAM)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib18" title="">18</a>]</cite> that has been trained on 1 million medical images.
For integration with the labelling platform, we only allow the bounding box functionality to appear when a toggle switch is activated.
A user would select the rectangular label from the available selection and draw a box on the image (see Supplementary Section 3).
Then, Label Studio will send the necessary information (bounding box coordinates and current image) to MedSAM.
MedSAM will consider the spatial relationship and contextual information inherent in medical images when processing the information for segmentation<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib22" title="">22</a>]</cite>.
Finally, it will send its predicted labels of the specified region back to Label Studio.
This would allow for faster and more accurate labelling created by the users.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Generative AI Integration</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">As crowd-labelling methods are limited by the availability of raw medical images, generative AI, particularly Generative Adversarial Networks (GANs)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib7" title="">7</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib8" title="">8</a>]</cite>, is used for data augmentation purposes.
Using the GAN model of our choice, synthetic medical images are generated using labels provided by the crowd and from the input dataset.
To achieve this, the GAN model can be extended to a conditional GAN (cGAN) model<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib24" title="">24</a>]</cite>.
A condition will be given to both the generator and the discriminator during the training process to partially control the generated images.
We used user-generated labels from crowdsourcing as input into the cGAN.
As the labels are in image format, a variant of cGAN named pix2pixGAN<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib17" title="">17</a>]</cite> was adapted for our project.
This is due to pix2pixGAN being specially designed to solve image-to-image translation problems, where the input consists of image-label pairs.
These synthetic images generated by pix2pixGAN are then integrated with the annotated image dataset to meet the needs of training future medical Image models.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Test Trials</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">The framework was tested on 3 different datasets which are MMWHS-MRI, MMWHS-CT and FLARE22<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib35" title="">35</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib21" title="">21</a>]</cite>.
We recruited 12 annotators to investigate the effectiveness of the general public in labelling medical images, and their results were further used to verify the potential of improving DL model training with crowdsourcing.
We assigned each annotator 6 tasks, containing 5 images each. The objective and criteria of each task are listed as follows:</p>
<ol class="ltx_enumerate" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1">Task 1: Labelling of the specified heart regions (MMWHS-CT) without any AI assistance or ground truth exemplars.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">Task 2: Labelling of the specified heart regions (MMWHS-CT) with AI assistance but no ground truth exemplars are provided.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1">Task 3: Labelling of the specified heart regions (MMWHS-CT) with AI assistance and ground truth exemplars.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1">Task 4: Labelling of the specified heart regions of the artificial GAN-generated dataset with AI assistance.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S3.I2.i5.p1">
<p class="ltx_p" id="S3.I2.i5.p1.1">Task 5: Labelling of the specified abdominal organs (FLARE22) with AI assistance and ground truth exemplars.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S3.I2.i6.p1">
<p class="ltx_p" id="S3.I2.i6.p1.1">Task 6: Labelling of the specified heart regions (MMWHS-MRI) with AI assistance and ground truth exemplars.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S3.SS5.p1.2">Tasks 1, 2, and 3 serve to evaluate the necessity of AI assistance and instructions with ground truth exemplars in crowdsourcing platforms.
Task 4 serves to assess the crowdsourcing label performance on GAN generated dataset.
Tasks 3, 5, and 6 serve to understand the versatility of the platform in various datasets.
(Detailed results see Supplementary Section 6)</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Merged Crowd Labels</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">To combine the ensemble of crowd labels of a single image into a merged label, a pixel-wise majority voting approach is taken<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib23" title="">23</a>]</cite>.
In this approach, the labels of each pixel are summed to create a frequency map.
Subsequently, a threshold is applied to this map to generate the merged label.
This threshold represents the minimum number of crowd annotators required to agree that a specific pixel belongs to the object of interest.
In this work, a minimum threshold of 4 was chosen based on the consideration of the number of unique crowd annotators.</p>
</div>
</section>
<section class="ltx_subsection ltx_indent_first" id="S3.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>Evaluation Metrics - Comparison with Ground Truth</h3>
<div class="ltx_para" id="S3.SS7.p1">
<p class="ltx_p" id="S3.SS7.p1.1">To evaluate the quality of the segmentations results, the Sørensen-Dice index (DSC)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib31" title="">31</a>]</cite> and the Jaccard Index (IoU)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#bib.bib31" title="">31</a>]</cite> are commonly used.
These metrics, defined in Equation <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S3.E1" title="Equation 1 ‣ 3.7 Evaluation Metrics - Comparison with Ground Truth ‣ 3 Our Approach ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S3.E2" title="Equation 2 ‣ 3.7 Evaluation Metrics - Comparison with Ground Truth ‣ 3 Our Approach ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">2</span></a> respectively, are selected due to their widespread usage and ease of comparison with other publications and methods.</p>
</div>
<div class="ltx_para ltx_minipage ltx_align_top" id="S3.SS7.p2" style="width:212.5pt;">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="D(X,Y)=\frac{2\left|X\cap Y\right|}{\left|X\right|+\left|Y\right|}" class="ltx_Math" display="block" id="S3.E1.m1.5"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.6" xref="S3.E1.m1.5.6.cmml"><mrow id="S3.E1.m1.5.6.2" xref="S3.E1.m1.5.6.2.cmml"><mi id="S3.E1.m1.5.6.2.2" xref="S3.E1.m1.5.6.2.2.cmml">D</mi><mo id="S3.E1.m1.5.6.2.1" xref="S3.E1.m1.5.6.2.1.cmml">⁢</mo><mrow id="S3.E1.m1.5.6.2.3.2" xref="S3.E1.m1.5.6.2.3.1.cmml"><mo id="S3.E1.m1.5.6.2.3.2.1" stretchy="false" xref="S3.E1.m1.5.6.2.3.1.cmml">(</mo><mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">X</mi><mo id="S3.E1.m1.5.6.2.3.2.2" xref="S3.E1.m1.5.6.2.3.1.cmml">,</mo><mi id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml">Y</mi><mo id="S3.E1.m1.5.6.2.3.2.3" stretchy="false" xref="S3.E1.m1.5.6.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.5.6.1" xref="S3.E1.m1.5.6.1.cmml">=</mo><mfrac id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mn id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml">2</mn><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.2.cmml"><mo id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.1.cmml">|</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml">X</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">∩</mo><mi id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">Y</mi></mrow><mo id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow><mrow id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.3.cmml"><mrow id="S3.E1.m1.3.3.3.4.2" xref="S3.E1.m1.3.3.3.4.1.cmml"><mo id="S3.E1.m1.3.3.3.4.2.1" xref="S3.E1.m1.3.3.3.4.1.1.cmml">|</mo><mi id="S3.E1.m1.2.2.2.1" xref="S3.E1.m1.2.2.2.1.cmml">X</mi><mo id="S3.E1.m1.3.3.3.4.2.2" xref="S3.E1.m1.3.3.3.4.1.1.cmml">|</mo></mrow><mo id="S3.E1.m1.3.3.3.3" xref="S3.E1.m1.3.3.3.3.cmml">+</mo><mrow id="S3.E1.m1.3.3.3.5.2" xref="S3.E1.m1.3.3.3.5.1.cmml"><mo id="S3.E1.m1.3.3.3.5.2.1" xref="S3.E1.m1.3.3.3.5.1.1.cmml">|</mo><mi id="S3.E1.m1.3.3.3.2" xref="S3.E1.m1.3.3.3.2.cmml">Y</mi><mo id="S3.E1.m1.3.3.3.5.2.2" xref="S3.E1.m1.3.3.3.5.1.1.cmml">|</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.6.cmml" xref="S3.E1.m1.5.6"><eq id="S3.E1.m1.5.6.1.cmml" xref="S3.E1.m1.5.6.1"></eq><apply id="S3.E1.m1.5.6.2.cmml" xref="S3.E1.m1.5.6.2"><times id="S3.E1.m1.5.6.2.1.cmml" xref="S3.E1.m1.5.6.2.1"></times><ci id="S3.E1.m1.5.6.2.2.cmml" xref="S3.E1.m1.5.6.2.2">𝐷</ci><interval closure="open" id="S3.E1.m1.5.6.2.3.1.cmml" xref="S3.E1.m1.5.6.2.3.2"><ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">𝑋</ci><ci id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5">𝑌</ci></interval></apply><apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"><divide id="S3.E1.m1.3.3.4.cmml" xref="S3.E1.m1.3.3"></divide><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><times id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></times><cn id="S3.E1.m1.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.1.1.1.3">2</cn><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1"><abs id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.2"></abs><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><intersect id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"></intersect><ci id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">𝑋</ci><ci id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">𝑌</ci></apply></apply></apply><apply id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.3"><plus id="S3.E1.m1.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3"></plus><apply id="S3.E1.m1.3.3.3.4.1.cmml" xref="S3.E1.m1.3.3.3.4.2"><abs id="S3.E1.m1.3.3.3.4.1.1.cmml" xref="S3.E1.m1.3.3.3.4.2.1"></abs><ci id="S3.E1.m1.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.1">𝑋</ci></apply><apply id="S3.E1.m1.3.3.3.5.1.cmml" xref="S3.E1.m1.3.3.3.5.2"><abs id="S3.E1.m1.3.3.3.5.1.1.cmml" xref="S3.E1.m1.3.3.3.5.2.1"></abs><ci id="S3.E1.m1.3.3.3.2.cmml" xref="S3.E1.m1.3.3.3.2">𝑌</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">D(X,Y)=\frac{2\left|X\cap Y\right|}{\left|X\right|+\left|Y\right|}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.5d">italic_D ( italic_X , italic_Y ) = divide start_ARG 2 | italic_X ∩ italic_Y | end_ARG start_ARG | italic_X | + | italic_Y | end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_minipage ltx_align_top" id="S3.SS7.p3" style="width:195.1pt;">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="J(X,Y)=\frac{\left|X\cap Y\right|}{\left|X\cup Y\right|}" class="ltx_Math" display="block" id="S3.E2.m1.4"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.5" xref="S3.E2.m1.4.5.cmml"><mrow id="S3.E2.m1.4.5.2" xref="S3.E2.m1.4.5.2.cmml"><mi id="S3.E2.m1.4.5.2.2" xref="S3.E2.m1.4.5.2.2.cmml">J</mi><mo id="S3.E2.m1.4.5.2.1" xref="S3.E2.m1.4.5.2.1.cmml">⁢</mo><mrow id="S3.E2.m1.4.5.2.3.2" xref="S3.E2.m1.4.5.2.3.1.cmml"><mo id="S3.E2.m1.4.5.2.3.2.1" stretchy="false" xref="S3.E2.m1.4.5.2.3.1.cmml">(</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">X</mi><mo id="S3.E2.m1.4.5.2.3.2.2" xref="S3.E2.m1.4.5.2.3.1.cmml">,</mo><mi id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">Y</mi><mo id="S3.E2.m1.4.5.2.3.2.3" stretchy="false" xref="S3.E2.m1.4.5.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.5.1" xref="S3.E2.m1.4.5.1.cmml">=</mo><mfrac id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.2.cmml"><mo id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.1.cmml">|</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml">X</mi><mo id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml">∩</mo><mi id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.3.cmml">Y</mi></mrow><mo id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S3.E2.m1.2.2.2.1" xref="S3.E2.m1.2.2.2.2.cmml"><mo id="S3.E2.m1.2.2.2.1.2" xref="S3.E2.m1.2.2.2.2.1.cmml">|</mo><mrow id="S3.E2.m1.2.2.2.1.1" xref="S3.E2.m1.2.2.2.1.1.cmml"><mi id="S3.E2.m1.2.2.2.1.1.2" xref="S3.E2.m1.2.2.2.1.1.2.cmml">X</mi><mo id="S3.E2.m1.2.2.2.1.1.1" xref="S3.E2.m1.2.2.2.1.1.1.cmml">∪</mo><mi id="S3.E2.m1.2.2.2.1.1.3" xref="S3.E2.m1.2.2.2.1.1.3.cmml">Y</mi></mrow><mo id="S3.E2.m1.2.2.2.1.3" xref="S3.E2.m1.2.2.2.2.1.cmml">|</mo></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.5.cmml" xref="S3.E2.m1.4.5"><eq id="S3.E2.m1.4.5.1.cmml" xref="S3.E2.m1.4.5.1"></eq><apply id="S3.E2.m1.4.5.2.cmml" xref="S3.E2.m1.4.5.2"><times id="S3.E2.m1.4.5.2.1.cmml" xref="S3.E2.m1.4.5.2.1"></times><ci id="S3.E2.m1.4.5.2.2.cmml" xref="S3.E2.m1.4.5.2.2">𝐽</ci><interval closure="open" id="S3.E2.m1.4.5.2.3.1.cmml" xref="S3.E2.m1.4.5.2.3.2"><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">𝑋</ci><ci id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">𝑌</ci></interval></apply><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><divide id="S3.E2.m1.2.2.3.cmml" xref="S3.E2.m1.2.2"></divide><apply id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1"><abs id="S3.E2.m1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2"></abs><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><intersect id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"></intersect><ci id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2">𝑋</ci><ci id="S3.E2.m1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3">𝑌</ci></apply></apply><apply id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.1"><abs id="S3.E2.m1.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.1.2"></abs><apply id="S3.E2.m1.2.2.2.1.1.cmml" xref="S3.E2.m1.2.2.2.1.1"><union id="S3.E2.m1.2.2.2.1.1.1.cmml" xref="S3.E2.m1.2.2.2.1.1.1"></union><ci id="S3.E2.m1.2.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.2.1.1.2">𝑋</ci><ci id="S3.E2.m1.2.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.2.1.1.3">𝑌</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">J(X,Y)=\frac{\left|X\cap Y\right|}{\left|X\cup Y\right|}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.4d">italic_J ( italic_X , italic_Y ) = divide start_ARG | italic_X ∩ italic_Y | end_ARG start_ARG | italic_X ∪ italic_Y | end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section class="ltx_section ltx_indent_first" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The experiment section is comprised of 3 sub-sections.
Section <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.SS1" title="4.1 Quality – Achieving Image Crowd Labelling at a Professional Level ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">4.1</span></a> contains our initial findings on the framework and evaluates the performance of crowd labellers in achieving expert-level labelling.
Section <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.SS2" title="4.2 Quantity – Enlarging Dataset with Synthetic Data ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">4.2</span></a> evaluates the performance of an enlarged dataset using synthetic images generated by pix2pixGAN.
Lastly, Section <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.SS3" title="4.3 Enhanced Dataset – Combining Generative AI and Crowdsourcing ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">4.3</span></a> evaluates the effectiveness of training DL segmentation models by combining crowd-labelled images and synthetic images into one enhanced dataset, which is the overall outcome of the framework.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="S4.F3.g1" src="extracted/5830858/figure/SegmentationComparison.png" width="449"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.3.2" style="font-size:90%;">
Comparison of the results for, from Left to Right, Ground Truth, UNet_1, UNet_19, SAM, MedSAM on MMWHS-CT slice 110
</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.2" style="width:260.5pt;height:88.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-55.8pt,18.9pt) scale(0.7,0.7) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.2.1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T1.2.1.1.1.1"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.2.1.1.1.2">UNet_1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.2.1.1.1.3">UNet_19</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.2.1.1.1.4">SAM</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.2.1.1.1.5">MedSAM</th>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.2.1.2.2.1">ROI</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.2.1.2.2.2">DSC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.2.1.2.2.3">IoU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.2.1.2.2.4">DSC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.2.1.2.2.5">IoU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.2.1.2.2.6">DSC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.2.1.2.2.7">IoU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.2.1.2.2.8">DSC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.2.1.2.2.9">IoU</th>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.3.3.1">LA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.3.3.2">0.5268</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.3.3.3">0.5232</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.3.3.4">0.4535</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.3.3.5">0.4721</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.3.3.6">0.6079</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.3.3.7">0.5149</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.3.3.8">0.7129</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.1.3.3.9">0.6306</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.4.4">
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.4.4.1">RA</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.4.4.2">0.4109</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.4.4.3">0.4970</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.4.4.4">0.2986</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.4.4.5">0.3522</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.4.4.6">0.7253</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.4.4.7">0.6102</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.4.4.8">0.7827</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.4.4.9">0.6860</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.5.5">
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.5.5.1">LV</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.5.5.2">0.7749</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.5.5.3">0.8909</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.5.5.4">0.7973</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.5.5.5">0.7675</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.5.5.6">0.7926</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.5.5.7">0.6639</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.5.5.8">0.8929</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.5.5.9">0.8070</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.6.6">
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.6.6.1">RV</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.6.6.2">0.8509</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.6.6.3">0.8943</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.6.6.4">0.6574</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.6.6.5">0.6840</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.6.6.6">0.8118</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.6.6.7">0.6840</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.6.6.8">0.8605</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.1.6.6.9">0.7555</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.2.1.7.7.1">Average</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.2.1.7.7.2">0.6409</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.2.1.7.7.3">0.7014</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.2.1.7.7.4">0.5517</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.2.1.7.7.5">0.5690</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.2.1.7.7.6">0.7344</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.2.1.7.7.7">0.6183</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.2.1.7.7.8">0.8123</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T1.2.1.7.7.9">0.7197</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.4.2" style="font-size:90%;">Mean DSC and IoU result for, from Left to Right, UNet_1, UNet_19, SAM, and MedSAM</span></figcaption>
</figure>
<section class="ltx_subsection ltx_indent_first" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Quality – Achieving Image Crowd Labelling at a Professional Level</h3>
<section class="ltx_subsubsection ltx_indent_first" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Segmentation AI Comparison for Crowd Use</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">The primary objective of incorporating an AI assistance labelling tool on our platform is to improve the efficiency and ease of the segmentation task for crowd annotators.
As a preliminary study, we investigated the most suitable segmentation AI model that is capable of assisting users in labelling tasks<span class="ltx_note ltx_role_footnote" id="footnote1c"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Abbreviation of the ROIs: LA - Left Atrium; RA - Right Atrium; LV - Left Ventricle; RV - Right Ventricle</span></span></span>.
A comparative analysis is conducted on prediction masks generated by 4 segmentation models: UNet_1, UNet_19, SAM, and MedSAM.
Both UNet_1 and UNet_19 are based on simple UNet structure, UNet_1 is trained on 10 training slices from 1 sub-dataset of MMWHS-CT, whereas UNet_19 is trained on 76 training slices from 19 sub-datasets of MMWHS-CT.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.F3" title="Figure 3 ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">3</span></a> shows example prediction masks generated by UNet_1 and UNet_19 models on MMWHS-CT slice 110.
Notably, both predicted masks are characterized by un-smooth and irregular contours with small, scattered regions due to overlapping labels.
These graphical observations are confirmed by the metrics in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.T1" title="Table 1 ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>, where UNet_1 outperforms UNet_19 with a higher overall metric score.
Both models achieve relatively high metric scores above 0.65 for ventricle labels and relatively low scores below 0.53 for atrium labels.
Results from Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.F3" title="Figure 3 ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">3</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.T1" title="Table 1 ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a> suggest that the UNet models are unsuitable for platform AI assistance due to their poor versatility across different datasets.
Each label task in the platform requires a new UNet model specifically trained for the corresponding dataset, and even ideally, the sub-datasets, despite the same modality and similar morphological structure.
This is validated by the superior performance of UNet_1 over UNet_19, which was achieved even with less training data and reduced training time.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.1">SAM and MedSAM are tested as large-scale models without specific training on any MMWHS datasets.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.F3" title="Figure 3 ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">3</span></a> also illustrates the prediction masks of the same testing slice generated by SAM and MedSAM models, characterized by smooth contours and significantly fewer overlapping regions.
These observations are corroborated by metric scores detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.T1" title="Table 1 ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>.
In contrast to the UNet models, which demonstrated higher performance of ventricle labels over atrium labels, both SAM and MedSAM exhibit consistent performance across all labels, which demonstrate their high versatility.
Specifically, SAM achieves an average DSC of 0.7344 (IoU of 0.6183), while MedSAM reaches an average DSC of 0.8123 (IoU of 0.7197).
MedSAM outperforms SAM and the UNet models in graphical representation and metric evaluations and hence, the chosen segmentation AI for crowd-labelling.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.2" style="width:347.1pt;height:126pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-74.4pt,27.0pt) scale(0.7,0.7) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T2.2.1.1.1.1"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T2.2.1.1.1.2">Task 1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T2.2.1.1.1.3">Task 2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T2.2.1.1.1.4">Task 3</th>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.2.1.2.2.1">ROI</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.2.1.2.2.2">DSC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.2.1.2.2.3">IoU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.2.1.2.2.4">DSC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.2.1.2.2.5">IoU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.2.1.2.2.6">DSC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.2.1.2.2.7">IoU</th>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.3.3.1" rowspan="2"><span class="ltx_text" id="S4.T2.2.1.3.3.1.1">LA</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.3.3.2">0.3106</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.3.3.3">0.2874</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.3.3.4">0.3113</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.3.3.5">0.2842</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.3.3.6">0.4272</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.1.3.3.7">0.4003</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.4.4">
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.4.4.1">[0.1876 0.4336]</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.4.4.2">[0.1726 0.4021]</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.4.4.3">[0.1950 0.4276]</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.4.4.4">[0.1760 0.3924]</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.4.4.5">[0.3002 0.5542]</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.4.4.6">[0.2793 0.5213]</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.5.5">
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.5.5.1" rowspan="2"><span class="ltx_text" id="S4.T2.2.1.5.5.1.1">RA</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.5.5.2">0.4132</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.5.5.3">0.3511</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.5.5.4">0.5093</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.5.5.5">0.4327</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.5.5.6">0.7020</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.5.5.7">0.6161</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.6.6">
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.6.6.1">[0.3014 0.5250]</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.6.6.2">[0.2528 0.4494]</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.6.6.3">[0.4019 0.6167]</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.6.6.4">[0.3359 0.5295]</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.6.6.5">[0.6143 0.7898]</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.6.6.6">[0.5340 0.6983]</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.7.7">
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.7.7.1" rowspan="2"><span class="ltx_text" id="S4.T2.2.1.7.7.1.1">LV</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.7.7.2">0.6661</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.7.7.3">0.6010</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.7.7.4">0.5332</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.7.7.5">0.4521</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.7.7.6">0.8722</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.7.7.7">0.7871</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.8.8">
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.8.8.1">[0.5740 0.7581]</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.8.8.2">[0.5139 0.6880]</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.8.8.3">[0.4391 0.6272]</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.8.8.4">[0.3677 0.5364]</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.8.8.5">[0.8460 0.8984]</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.8.8.6">[0.7482 0.8261]</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.9.9">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.1.9.9.1" rowspan="2"><span class="ltx_text" id="S4.T2.2.1.9.9.1.1">RV</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.9.9.2">0.6513</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.9.9.3">0.5729</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.9.9.4">0.6506</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.9.9.5">0.5924</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.9.9.6">0.8877</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.1.9.9.7">0.8112</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.10.10">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.1.10.10.1">[0.5624 0.7402]</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.1.10.10.2">[0.4926 0.6533]</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.1.10.10.3">[0.5512 0.7499]</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.1.10.10.4">[0.5001 0.6847]</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.1.10.10.5">[0.8569 0.9185]</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.1.10.10.6">[0.7793 0.8431]</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.3.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.4.2" style="font-size:90%;">Mean DSC and IoU with 95% CI for results collected from Task 1 (Hand-drawn), Task 2 (with AI assistance), and Task 3 (with AI assistance and instructions)</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.SS1.SSS1.fig1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_table ltx_figure_panel ltx_minipage ltx_align_bottom" id="S4.T3" style="width:130.1pt;">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.2" style="width:81.7pt;height:63pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-17.5pt,13.5pt) scale(0.7,0.7) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T3.2.1.1.1.1">ROI</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.1.1.1.2">DSC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.2.1.1.1.3">IoU</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.2.1.2.1.1">LA</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.2.1.2">0.3839</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.1.2.1.3">0.3627</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.1.3.2.1">RA</th>
<td class="ltx_td ltx_align_center" id="S4.T3.2.1.3.2.2">0.8504</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.1.3.2.3">0.7531</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.1.4.3.1">LV</th>
<td class="ltx_td ltx_align_center" id="S4.T3.2.1.4.3.2">0.7246</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.1.4.3.3">0.6622</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.2.1.5.4.1">RV</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.1.5.4.2">0.9159</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.2.1.5.4.3">0.8454</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.3.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S4.T3.4.2" style="font-size:90%;">Mean DSC and IoU for merged labels from Task 3 MMWHS-CT</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_table ltx_figure_panel ltx_minipage ltx_align_bottom" id="S4.T4" style="width:130.1pt;">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.2" style="width:90.7pt;height:50.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-19.4pt,10.8pt) scale(0.7,0.7) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T4.2.1.1.1.1">ROI</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.2.1.1.1.2">DSC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.2.1.1.1.3">IoU</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.2.1.2.1.1">Liver</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.2.1.2">0.9698</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.2.1.2.1.3">0.9415</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.2.1.3.2.1">Kidney</th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.3.2.2">0.7597</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.1.3.2.3">0.6536</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T4.2.1.4.3.1">Aorta</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.1.4.3.2">0.9584</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.2.1.4.3.3">0.9204</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.3.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S4.T4.4.2" style="font-size:90%;">Mean DSC and IoU for merged label from Task 5 FLARE22 Abdomen</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_table ltx_figure_panel ltx_minipage ltx_align_bottom" id="S4.T5" style="width:130.1pt;">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T5.2" style="width:126.4pt;height:63pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-27.1pt,13.5pt) scale(0.7,0.7) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T5.2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T5.2.1.1.1.1">ROI</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.2.1.1.1.2">DSC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.2.1.1.1.3">IoU</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.2.1.2.1.1">LV</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.2.1.2">0.6966</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.2.1.2.1.3">0.5850</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.2.1.3.2.1">Myocardium of LV</th>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.3.2.2">0.7197</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.3.2.3">0.6260</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.2.1.4.3.1">RV</th>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.4.3.2">0.7146</td>
<td class="ltx_td ltx_align_center" id="S4.T5.2.1.4.3.3">0.6472</td>
</tr>
<tr class="ltx_tr" id="S4.T5.2.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T5.2.1.5.4.1">Pulmonary Artery</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.2.1.5.4.2">0.7382</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.2.1.5.4.3">0.5917</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T5.3.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S4.T5.4.2" style="font-size:90%;">Mean DSC and IoU for merged label from Task 6 MMWHS-MRI</span></figcaption>
</figure>
</div>
</div>
</figure>
</section>
<section class="ltx_subsubsection ltx_indent_first" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Necessity of AI Assistance and Instruction</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">To investigate if AI assistance can improve crowd segmentation results, the comparison and analysis between the results from Task 1 and Task 2 (Hand-drawn vs. AI assistance) are as follows, using the DSC and IoU metrics.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="241" id="S4.F4.g1" src="extracted/5830858/figure/T1vT2prism.png" width="419"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.3.2" style="font-size:90%;">
Comparison of DICE and IoU between each ROI for Task 1 (hand-drawn) and Task 2 (AI assistance); illustrated by Prism GraphPad
</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1">From Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.F4" title="Figure 4 ‣ 4.1.2 Necessity of AI Assistance and Instruction ‣ 4.1 Quality – Achieving Image Crowd Labelling at a Professional Level ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">4</span></a>, it is demonstrated that the distribution of metrics scores does not vary significantly between Task 1 and Task 2.
A noticeable amount of data points clustered around 0 is observed.
Quantitatively in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.T2" title="Table 2 ‣ 4.1.1 Segmentation AI Comparison for Crowd Use ‣ 4.1 Quality – Achieving Image Crowd Labelling at a Professional Level ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">2</span></a>, it is statistically evident that, for all compartments of the heart, crowd segmentation accuracy from Task 1 and Task 2 are not significantly different (<math alttext="p&gt;0.05" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p2.1.m1.1"><semantics id="S4.SS1.SSS2.p2.1.m1.1a"><mrow id="S4.SS1.SSS2.p2.1.m1.1.1" xref="S4.SS1.SSS2.p2.1.m1.1.1.cmml"><mi id="S4.SS1.SSS2.p2.1.m1.1.1.2" xref="S4.SS1.SSS2.p2.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS1.SSS2.p2.1.m1.1.1.1" xref="S4.SS1.SSS2.p2.1.m1.1.1.1.cmml">&gt;</mo><mn id="S4.SS1.SSS2.p2.1.m1.1.1.3" xref="S4.SS1.SSS2.p2.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.1.m1.1b"><apply id="S4.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p2.1.m1.1.1"><gt id="S4.SS1.SSS2.p2.1.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p2.1.m1.1.1.1"></gt><ci id="S4.SS1.SSS2.p2.1.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p2.1.m1.1.1.2">𝑝</ci><cn id="S4.SS1.SSS2.p2.1.m1.1.1.3.cmml" type="float" xref="S4.SS1.SSS2.p2.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.1.m1.1c">p&gt;0.05</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p2.1.m1.1d">italic_p &gt; 0.05</annotation></semantics></math>).
This indicates that with only segmentation AI assistance provided, the accuracy of the crowd result would not be improved.
It is hypothesised that most of the volunteers have no prior knowledge of heart anatomy, leading to almost random annotations that do not fit with the ground truth.
Furthermore, some users also reported being confused with the orientation of the images.
It should be noted that once participants became accustomed to the AI tool, the majority of users reported an easier labelling process and reduction in labelling time by simply making quick refinements on the AI-generated regions.
This result highlights the success of making the segmentation process easier and less tedium with the deployment of the MedSAM segmentation facilitated tool.
To seek for actual improvement in accuracy, we hypothesised that an instruction, in addition to AI assistance, would provide fundamental knowledge to users that will in turn increase labelling accuracy.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p3">
<p class="ltx_p" id="S4.SS1.SSS2.p3.1">To investigate whether the crowd segmentation results would improve when crowd annotators receive AI assistance along with detailed instructions including ground truth label exemplars.
The results from Task 2 and Task 3 (AI assistance vs. AI assistance with instructions) using DSC and IoU metrics are computed as follows.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="244" id="S4.F5.g1" src="extracted/5830858/figure/T2vT3prism.png" width="419"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.3.2" style="font-size:90%;">
Comparison of DICE and IoU between each ROI for Task 2 (AI-assisted) and Task 3 (AI-assisted and instructions with ground truth exemplars); illustrated by Prism GraphPad
</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS2.p4">
<p class="ltx_p" id="S4.SS1.SSS2.p4.1">It is evident in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.T1" title="Table 1 ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a> that the crowd segmentation results from Task 3 are statistically more accurate than those from Task 2 with 95% CI (<math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p4.1.m1.1"><semantics id="S4.SS1.SSS2.p4.1.m1.1a"><mrow id="S4.SS1.SSS2.p4.1.m1.1.1" xref="S4.SS1.SSS2.p4.1.m1.1.1.cmml"><mi id="S4.SS1.SSS2.p4.1.m1.1.1.2" xref="S4.SS1.SSS2.p4.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS1.SSS2.p4.1.m1.1.1.1" xref="S4.SS1.SSS2.p4.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.SS1.SSS2.p4.1.m1.1.1.3" xref="S4.SS1.SSS2.p4.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p4.1.m1.1b"><apply id="S4.SS1.SSS2.p4.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p4.1.m1.1.1"><lt id="S4.SS1.SSS2.p4.1.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p4.1.m1.1.1.1"></lt><ci id="S4.SS1.SSS2.p4.1.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p4.1.m1.1.1.2">𝑝</ci><cn id="S4.SS1.SSS2.p4.1.m1.1.1.3.cmml" type="float" xref="S4.SS1.SSS2.p4.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p4.1.m1.1c">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p4.1.m1.1d">italic_p &lt; 0.05</annotation></semantics></math>), demonstrating that the instructions with ground truth exemplars are crucial to the accuracy of crowd labelling outcomes.
To account for the variance between annotators, we merged the crowd labels using pixel-wise majority voting approach with threshold of 4 (as discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S3.SS6" title="3.6 Merged Crowd Labels ‣ 3 Our Approach ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">3.6</span></a>).
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.T3" title="Table 3 ‣ 4.1.1 Segmentation AI Comparison for Crowd Use ‣ 4.1 Quality – Achieving Image Crowd Labelling at a Professional Level ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">3</span></a> shows the metrics after merging.
Notably, LA has a relatively low DSC of 0.3839 (IoU of 0.3627), indicating the difficulty in labelling this ROI.
Hence, it is demonstrated that crowd annotators tend to perform better with simple anatomical structures that have less variance between slices.
This observation suggests that crowdsourcing should be limited to datasets with relatively simple structures.
Nonetheless, this indicates the importance of providing clear guidance and ground truth exemplars to the crowd annotators by the researchers when setting up segmentation tasks.</p>
</div>
</section>
<section class="ltx_subsubsection ltx_indent_first" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Crowd Segmentation in Different Modalities</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">It has been demonstrated that AI assistance in combination with instructions can dramatically improve the accuracy of labelling tasks.
To further investigate the versatility of our platform, we conducted tests involving AI-assisted labelling on MRI images from the MMWHS dataset, which differ significantly from the CT images from the same dataset, and CT images of the abdomen from the FLARE22 dataset.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="378" id="S4.F6.g1" src="extracted/5830858/figure/versatility.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S4.F6.3.2" style="font-size:90%;">
Comparison between the ground truth (Bottom) and merged crowd labels from different Datasets (Top): Task 3 MM-WHS CT (Left); Task 5 FLARE22 (Middle); and Task 6 MMWHS-MRI (Right)
</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS3.p2">
<p class="ltx_p" id="S4.SS1.SSS3.p2.1">It is notable that in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.F6" title="Figure 6 ‣ 4.1.3 Crowd Segmentation in Different Modalities ‣ 4.1 Quality – Achieving Image Crowd Labelling at a Professional Level ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">6</span></a>, visually the merged crowd segmentation is close to the ground truth, with the edges of the organ identified with high definition.
Quantitatively, Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.T4" title="Table 4 ‣ 4.1.1 Segmentation AI Comparison for Crowd Use ‣ 4.1 Quality – Achieving Image Crowd Labelling at a Professional Level ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">4</span></a> shows that the labelling accuracy is very high in liver and kidney segmentation.
Specifically, the DSC is approximately 0.96 for both the liver and aorta (IoU of about 0.93) and about 0.75 for the kidney (IoU of about 0.65).
According to Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.T5" title="Table 5 ‣ 4.1.1 Segmentation AI Comparison for Crowd Use ‣ 4.1 Quality – Achieving Image Crowd Labelling at a Professional Level ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">5</span></a>, despite the complexity of MRI images, the DSC and IoU metrics are acceptable, yielding a DSC of about 0.7 (IoU of about 0.6) for all.
These results illustrate that our platform is versatile to ensure the accuracy of labelling tasks across different modalities of images.
This endorses the customizability of the crowdsourcing platform by ensuring that different datasets can all be segmented efficiently by the merged crowd labels.</p>
</div>
</section>
</section>
<section class="ltx_subsection ltx_indent_first" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Quantity – Enlarging Dataset with Synthetic Data</h3>
<section class="ltx_subsubsection ltx_indent_first" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Synthetic Images in Different Modalities</h4>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="197" id="S4.F7.g1" src="extracted/5830858/figure/GAN_Versatility.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S4.F7.3.2" style="font-size:90%;">
Exemplars of synthetic images generated by pix2pixGAN, using masks from: MMWHS-CT Slice 100 (Left); FLARE22 Slice 55 (Middle); MMWHS-MRI Slice 65 (Right)
</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">To evaluate the versatility of the pix2pixGAN model, we trained it on datasets comprising different diverse segmentation pairs from medical imaging modalities and organs.
The results demonstrated the model’s ability to generate synthetic images across different modalities, including MMWHS-CT, MMWHS-MRI, and FLARE22.
Furthermore, the model’s versatility was evidenced by its capacity to generate clearly identifiable organs and compartment tissues, such as the heart in MMWHS, and the liver and kidneys in FLARE22, as we can see in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.F7" title="Figure 7 ‣ 4.2.1 Synthetic Images in Different Modalities ‣ 4.2 Quantity – Enlarging Dataset with Synthetic Data ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">7</span></a>.
The generated synthetic images exhibited distinct edges and good contrast, particularly when multiple organs are present within a single image, with characterisable and identifiable morphology.
These findings demonstrated pix2pixGAN’s high versatility in generating synthetic images across various modalities and anatomical structures in medical imaging.
However, it is noted that the synthetic image organs are often found at wrong vertebral levels, which indicates a lack of realism.
A potential improvement is suggested where landmarks apart from ROIs could be included during synthesis to refine anatomical accuracy.</p>
</div>
</section>
<section class="ltx_subsubsection ltx_indent_first" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Efficiency of Enlarged Dataset</h4>
<figure class="ltx_figure" id="S4.SS2.SSS2.fig1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_table ltx_figure_panel ltx_minipage ltx_align_bottom" id="S4.T6" style="width:130.1pt;">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T6.2" style="width:137.4pt;height:75.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-29.4pt,16.2pt) scale(0.7,0.7) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T6.2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.2.1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T6.2.1.1.1.1"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T6.2.1.1.1.2">Control</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T6.2.1.1.1.3">Enlarged</th>
</tr>
<tr class="ltx_tr" id="S4.T6.2.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T6.2.1.2.2.1">ROI</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T6.2.1.2.2.2">DSC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T6.2.1.2.2.3">IoU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T6.2.1.2.2.4">DSC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T6.2.1.2.2.5">IoU</th>
</tr>
<tr class="ltx_tr" id="S4.T6.2.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.2.1.3.3.1">LA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.2.1.3.3.2">0.6318</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.2.1.3.3.3">0.6859</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.2.1.3.3.4">0.7767</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.2.1.3.3.5">0.7901</td>
</tr>
<tr class="ltx_tr" id="S4.T6.2.1.4.4">
<td class="ltx_td ltx_align_center" id="S4.T6.2.1.4.4.1">RA</td>
<td class="ltx_td ltx_align_center" id="S4.T6.2.1.4.4.2">0.5992</td>
<td class="ltx_td ltx_align_center" id="S4.T6.2.1.4.4.3">0.6361</td>
<td class="ltx_td ltx_align_center" id="S4.T6.2.1.4.4.4">0.6988</td>
<td class="ltx_td ltx_align_center" id="S4.T6.2.1.4.4.5">0.7000</td>
</tr>
<tr class="ltx_tr" id="S4.T6.2.1.5.5">
<td class="ltx_td ltx_align_center" id="S4.T6.2.1.5.5.1">LV</td>
<td class="ltx_td ltx_align_center" id="S4.T6.2.1.5.5.2">0.6025</td>
<td class="ltx_td ltx_align_center" id="S4.T6.2.1.5.5.3">0.5689</td>
<td class="ltx_td ltx_align_center" id="S4.T6.2.1.5.5.4">0.6821</td>
<td class="ltx_td ltx_align_center" id="S4.T6.2.1.5.5.5">0.6464</td>
</tr>
<tr class="ltx_tr" id="S4.T6.2.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.2.1.6.6.1">RV</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.2.1.6.6.2">0.7254</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.2.1.6.6.3">0.7390</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.2.1.6.6.4">0.8086</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T6.2.1.6.6.5">0.7850</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T6.3.1.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="S4.T6.4.2" style="font-size:90%;">Mean DSC and IoU for UNet trained with control and enlarged MMWHS-CT dataset</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_table ltx_figure_panel ltx_minipage ltx_align_bottom" id="S4.T7" style="width:130.1pt;">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T7.2" style="width:146.4pt;height:63pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-31.4pt,13.5pt) scale(0.7,0.7) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T7.2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T7.2.1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T7.2.1.1.1.1"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T7.2.1.1.1.2">Control</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T7.2.1.1.1.3">Enlarged</th>
</tr>
<tr class="ltx_tr" id="S4.T7.2.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T7.2.1.2.2.1">ROI</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T7.2.1.2.2.2">DSC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T7.2.1.2.2.3">IoU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T7.2.1.2.2.4">DSC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T7.2.1.2.2.5">IoU</th>
</tr>
<tr class="ltx_tr" id="S4.T7.2.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.2.1.3.3.1">Liver</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.2.1.3.3.2">0.7511</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.2.1.3.3.3">0.8695</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.2.1.3.3.4">0.8092</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.2.1.3.3.5">0.8369</td>
</tr>
<tr class="ltx_tr" id="S4.T7.2.1.4.4">
<td class="ltx_td ltx_align_center" id="S4.T7.2.1.4.4.1">Kidney</td>
<td class="ltx_td ltx_align_center" id="S4.T7.2.1.4.4.2">0.5648</td>
<td class="ltx_td ltx_align_center" id="S4.T7.2.1.4.4.3">0.7039</td>
<td class="ltx_td ltx_align_center" id="S4.T7.2.1.4.4.4">0.6030</td>
<td class="ltx_td ltx_align_center" id="S4.T7.2.1.4.4.5">0.8135</td>
</tr>
<tr class="ltx_tr" id="S4.T7.2.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.2.1.5.5.1">Aorta</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.2.1.5.5.2">0.1490</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.2.1.5.5.3">0.6580</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.2.1.5.5.4">0.3392</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T7.2.1.5.5.5">0.7802</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T7.3.1.1" style="font-size:90%;">Table 7</span>: </span><span class="ltx_text" id="S4.T7.4.2" style="font-size:90%;">Mean DSC and IoU for UNet trained with control and enlarged FLARE22 dataset</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_table ltx_figure_panel ltx_minipage ltx_align_bottom" id="S4.T8" style="width:130.1pt;">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T8.2" style="width:182.1pt;height:75.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-39.0pt,16.2pt) scale(0.7,0.7) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T8.2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T8.2.1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T8.2.1.1.1.1"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T8.2.1.1.1.2">Control</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T8.2.1.1.1.3">Enlarged</th>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T8.2.1.2.2.1">ROI</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T8.2.1.2.2.2">DSC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T8.2.1.2.2.3">IoU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T8.2.1.2.2.4">DSC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T8.2.1.2.2.5">IoU</th>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.2.1.3.3.1">LV</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.2.1.3.3.2">0.6727</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.2.1.3.3.3">0.786</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.2.1.3.3.4">0.7689</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.2.1.3.3.5">0.7703</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.4.4">
<td class="ltx_td ltx_align_center" id="S4.T8.2.1.4.4.1">Myocardium of LV</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.1.4.4.2">0.7063</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.1.4.4.3">0.7281</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.1.4.4.4">0.7271</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.1.4.4.5">0.7734</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.5.5">
<td class="ltx_td ltx_align_center" id="S4.T8.2.1.5.5.1">RV</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.1.5.5.2">0.7608</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.1.5.5.3">0.8606</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.1.5.5.4">0.8268</td>
<td class="ltx_td ltx_align_center" id="S4.T8.2.1.5.5.5">0.8591</td>
</tr>
<tr class="ltx_tr" id="S4.T8.2.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.2.1.6.6.1">Pulmonary Artery</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.2.1.6.6.2">0.4716</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.2.1.6.6.3">0.5629</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.2.1.6.6.4">0.5156</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T8.2.1.6.6.5">0.5513</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T8.3.1.1" style="font-size:90%;">Table 8</span>: </span><span class="ltx_text" id="S4.T8.4.2" style="font-size:90%;">Mean DSC and IoU for UNet trained with control and enlarged MMWHS-MRI dataset</span></figcaption>
</figure>
</div>
</div>
</figure>
<figure class="ltx_table" id="S4.T9">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T9.2" style="width:349.5pt;height:76.3pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-74.9pt,16.2pt) scale(0.7,0.7) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T9.2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T9.2.1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T9.2.1.1.1.1"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T9.2.1.1.1.2">Control</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T9.2.1.1.1.3">Enlarged</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T9.2.1.1.1.4">Enhanced</th>
</tr>
<tr class="ltx_tr" id="S4.T9.2.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T9.2.1.2.2.1">ROI</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T9.2.1.2.2.2">DSC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T9.2.1.2.2.3">IoU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T9.2.1.2.2.4">DSC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T9.2.1.2.2.5">IoU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T9.2.1.2.2.6">DSC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T9.2.1.2.2.7">IoU</th>
</tr>
<tr class="ltx_tr" id="S4.T9.2.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.2.1.3.3.1" rowspan="2"><span class="ltx_text" id="S4.T9.2.1.3.3.1.1">Liver</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.2.1.3.3.2">0.7921</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.2.1.3.3.3">0.9005</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.2.1.3.3.4">0.8546</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.2.1.3.3.5">0.9061</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.2.1.3.3.6">0.8895</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.2.1.3.3.7">0.9322</td>
</tr>
<tr class="ltx_tr" id="S4.T9.2.1.4.4">
<td class="ltx_td ltx_align_center" id="S4.T9.2.1.4.4.1">[0.7634 0.8209]</td>
<td class="ltx_td ltx_align_center" id="S4.T9.2.1.4.4.2">[0.8928 0.9081]</td>
<td class="ltx_td ltx_align_center" id="S4.T9.2.1.4.4.3">[0.8377 0.8714]</td>
<td class="ltx_td ltx_align_center" id="S4.T9.2.1.4.4.4">[0.8999 0.9123]</td>
<td class="ltx_td ltx_align_center" id="S4.T9.2.1.4.4.5">[0.8797 0.8993]</td>
<td class="ltx_td ltx_align_center" id="S4.T9.2.1.4.4.6">[0.9271 0.9372]</td>
</tr>
<tr class="ltx_tr" id="S4.T9.2.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T9.2.1.5.5.1" rowspan="2"><span class="ltx_text" id="S4.T9.2.1.5.5.1.1">Aorta</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.2.1.5.5.2">0.1467</td>
<td class="ltx_td ltx_align_center" id="S4.T9.2.1.5.5.3">0.4932</td>
<td class="ltx_td ltx_align_center" id="S4.T9.2.1.5.5.4">0.2604</td>
<td class="ltx_td ltx_align_center" id="S4.T9.2.1.5.5.5">0.7453</td>
<td class="ltx_td ltx_align_center" id="S4.T9.2.1.5.5.6">0.5045</td>
<td class="ltx_td ltx_align_center" id="S4.T9.2.1.5.5.7">0.7291</td>
</tr>
<tr class="ltx_tr" id="S4.T9.2.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T9.2.1.6.6.1">[0.1411 0.1522]</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T9.2.1.6.6.2">[0.4659 0.5205]</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T9.2.1.6.6.3">[0.2537 0.2671]</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T9.2.1.6.6.4">[0.7161 0.7745]</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T9.2.1.6.6.5">[0.4928 0.5162]</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T9.2.1.6.6.6">[0.7059 0.7523]</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T9.3.1.1" style="font-size:90%;">Table 9</span>: </span><span class="ltx_text" id="S4.T9.4.2" style="font-size:90%;">Mean DSC and IoU with 95% CI for UNet specifically trained for liver and aorta from control, enlarged, and enhanced FLARE22 dataset</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">To further evaluate the feasibility of using an enlarged dataset to improve model training in scenarios with limited data, we conducted a segmentation task comparison between the original and enlarged datasets.
For each of the MMWHS-CT, MMWHS-MRI, and FLARE22 datasets, 20 slices were randomly selected, with 10 used for training and 10 for testing.
Additionally, 10 synthetic images were generated from the ground truth labels of the training slices using pix2pixGAN.
For the control dataset, a UNet model was trained on 10 real images with their corresponding ground truth labels.
For the enlarged dataset, a UNet model was trained on 20 images, comprising of 10 real and 10 synthetic images, along with their ground truth labels.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1">Results from Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.T6" title="Table 6 ‣ 4.2.2 Efficiency of Enlarged Dataset ‣ 4.2 Quantity – Enlarging Dataset with Synthetic Data ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">6</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.T7" title="Table 7 ‣ 4.2.2 Efficiency of Enlarged Dataset ‣ 4.2 Quantity – Enlarging Dataset with Synthetic Data ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">7</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.T8" title="Table 8 ‣ 4.2.2 Efficiency of Enlarged Dataset ‣ 4.2 Quantity – Enlarging Dataset with Synthetic Data ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">8</span></a> suggest all modalities have shown notable improvements in training scores with the enlarged dataset, apart from few IoUs fluctuated at a slightly lower score.
Notably, Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.T7" title="Table 7 ‣ 4.2.2 Efficiency of Enlarged Dataset ‣ 4.2 Quantity – Enlarging Dataset with Synthetic Data ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">7</span></a> shows an average of 15.9% increase for DSC and 11.1% increase for IoU.
Aorta, as the hardest segmented ROI in FLARE22, has improved from a DSC value of 0.1490 (IoU of 0.6580) to a DSC value of 0.3392 (IoU of 0.7802).
Overall, this result validates the effectiveness of incorporating synthetic data to improve model training outcomes in data-limited scenarios.</p>
</div>
</section>
</section>
<section class="ltx_subsection ltx_indent_first" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Enhanced Dataset – Combining Generative AI and Crowdsourcing</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Finally, we combined the high quality merged crowd labels with GAN enlarged dataset as an enhanced dataset to evaluate the potential to improve model training in limited data scenarios further.
To ensure the training dataset’s quality, only 5 FLARE22 Liver and Aorta merged crowd labels are used for the enhanced dataset due to their high similarity to the ground truth, with DSC above 0.95 and IoU above 0.92 as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.T4" title="Table 4 ‣ 4.1.1 Segmentation AI Comparison for Crowd Use ‣ 4.1 Quality – Achieving Image Crowd Labelling at a Professional Level ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">4</span></a>.
Therefore, as a preliminary evaluation, we trained three segmentation UNet models for the Aorta and Liver using three versions of the FLARE22 dataset: a control dataset, an enlarged dataset, and an enhanced dataset.
The control dataset included 10 real images; the enlarged dataset consisted of 10 real images and 10 synthetic images; and the enhanced dataset comprised 10 real images, 10 synthetic images, and 5 merged crowd labels.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.3">The metrics present in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.03087v1#S4.T9" title="Table 9 ‣ 4.2.2 Efficiency of Enlarged Dataset ‣ 4.2 Quantity – Enlarging Dataset with Synthetic Data ‣ 4 Experiment ‣ Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation"><span class="ltx_text ltx_ref_tag">9</span></a> indicate a significant improvement (<math alttext="p&lt;0.001" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><lt id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></lt><ci id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">𝑝</ci><cn id="S4.SS3.p2.1.m1.1.1.3.cmml" type="float" xref="S4.SS3.p2.1.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">p&lt;0.001</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">italic_p &lt; 0.001</annotation></semantics></math> for both liver and aorta DSC using unpaired t-test) in segmentation accuracy from the control dataset to the enlarged dataset, with further enhancement (<math alttext="p&lt;0.001" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.1"><semantics id="S4.SS3.p2.2.m2.1a"><mrow id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mi id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">p</mi><mo id="S4.SS3.p2.2.m2.1.1.1" xref="S4.SS3.p2.2.m2.1.1.1.cmml">&lt;</mo><mn id="S4.SS3.p2.2.m2.1.1.3" xref="S4.SS3.p2.2.m2.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><lt id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1.1"></lt><ci id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">𝑝</ci><cn id="S4.SS3.p2.2.m2.1.1.3.cmml" type="float" xref="S4.SS3.p2.2.m2.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">p&lt;0.001</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m2.1d">italic_p &lt; 0.001</annotation></semantics></math> for both liver and aorta DSC using unpaired t-test) when utilising the enhanced dataset compared to the enlarged dataset.
Overall, the enhance dataset performance has significant improvement when compared with the control dataset (<math alttext="p&lt;0.0001" class="ltx_Math" display="inline" id="S4.SS3.p2.3.m3.1"><semantics id="S4.SS3.p2.3.m3.1a"><mrow id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml"><mi id="S4.SS3.p2.3.m3.1.1.2" xref="S4.SS3.p2.3.m3.1.1.2.cmml">p</mi><mo id="S4.SS3.p2.3.m3.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.cmml">&lt;</mo><mn id="S4.SS3.p2.3.m3.1.1.3" xref="S4.SS3.p2.3.m3.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><apply id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1"><lt id="S4.SS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1.1"></lt><ci id="S4.SS3.p2.3.m3.1.1.2.cmml" xref="S4.SS3.p2.3.m3.1.1.2">𝑝</ci><cn id="S4.SS3.p2.3.m3.1.1.3.cmml" type="float" xref="S4.SS3.p2.3.m3.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.3.m3.1d">italic_p &lt; 0.0001</annotation></semantics></math> for both liver and aorta DSC using unpaired t-test).
Notably, the enhanced dataset achieves a 12.3% increase in DSC for liver segmentation compared to the control dataset.
Furthermore, the DSC for the Aorta increase substantially, from 0.1467 to 0.5045, and IoU improve from 0.4932 to 0.7291, highlighting enhanced feature extraction for challenging segmented ROIs.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">These findings validate the potential of augmenting a limited training dataset with GAN-generated synthetic images and high-quality merged crowd labels, supporting the feasibility of our proposed framework.</p>
</div>
</section>
</section>
<section class="ltx_section ltx_indent_first" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">To conclude, it is evident that it is possible to improve the data-gathering rate to create a fully labelled dataset by using crowdsourcing.
We demonstrated that using a flexible zero-shot segmentation AI model such as MedSAM can improve the user experience and efficiency of labelling.
Including synthetic images generated by GAN models like pix2pixGAN to enlarge the dataset has been proven to improve the accuracy of the segmentation model.
A prototype platform is implemented to demonstrate the workflow and can act as a provision for a more robust platform that can effectively collect labelling data from the crowd.
Crowdsourcing can be included as a data-gathering pipeline for future researchers in training their AI models and algorithms.
Building upon the foundation of this research, we demonstrated a framework exploiting the potential of coupling AI and crowdsourcing to resolve the scarcity in the availability of medical images for model training.
Our framework is general and versatile, and can be extended by others to contribute and incorporate for specific modalities.</p>
</div>
</section>
<section class="ltx_section ltx_indent_first" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgements</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This study was supported in part by the ERC IMI (101005122), the H2020 (952172), the MRC (MC/PC/21013), the Royal Society (IEC/NSFC/211235),
the NVIDIA Academic Hardware Grant Program, the SABER project supported by Boehringer Ingelheim Ltd, NIHR Imperial Biomedical Research Centre (RDA01), Wellcome Leap Dynamic Resilience,
UKRI guarantee funding for Horizon Europe MSCA Postdoctoral Fellowships (EP/Z002206/1), and the UKRI Future Leaders Fellowship (MR/V023799/1).</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
Hazrat Ali, Md Rafiul Biswas, Farida Mohsen, Uzair Shah, Asma Alamgir, Osama Mousa, and Zubair Shah.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.2.1" style="font-size:90%;">The role of generative adversarial networks in brain mri: a scoping review.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.3.1" style="font-size:90%;">Insights into imaging</span><span class="ltx_text" id="bib.bib1.4.2" style="font-size:90%;">, 13(1):98, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
Laith Alzubaidi, Jinshuai Bai, Aiman Al-Sabaawi, Jose Santamaría, Ahmed Shihab Albahri, Bashar Sami Nayyef Al-dabbagh, Mohammed A Fadhel, Mohamed Manoufali, Jinglan Zhang, Ali H Al-Timemy, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.3.1" style="font-size:90%;">Journal of Big Data</span><span class="ltx_text" id="bib.bib2.4.2" style="font-size:90%;">, 10(1):46, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A Bharath.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.2.1" style="font-size:90%;">Generative adversarial networks: An overview.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.3.1" style="font-size:90%;">IEEE signal processing magazine</span><span class="ltx_text" id="bib.bib3.4.2" style="font-size:90%;">, 35(1):53–65, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
Huggin Face Enterprise.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">Huggin face spaces, 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.3.1" style="font-size:90%;">Available at: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/" style="font-size:90%;" title="">https://huggingface.co/</a><span class="ltx_text" id="bib.bib4.4.2" style="font-size:90%;">; Accessed on 17th January 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
Maayan Frid-Adar, Idit Diamant, Eyal Klang, Michal Amitai, Jacob Goldberger, and Hayit Greenspan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">Gan-based synthetic medical image augmentation for increased cnn performance in liver lesion classification.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.3.1" style="font-size:90%;">Neurocomputing</span><span class="ltx_text" id="bib.bib5.4.2" style="font-size:90%;">, 321:321–331, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
Fabio Galbusera and Andrea Cina.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">Image annotation and curation in radiology: an overview for machine learning practitioners.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.3.1" style="font-size:90%;">European Radiology Experimental</span><span class="ltx_text" id="bib.bib6.4.2" style="font-size:90%;">, 8(1):11, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">Generative adversarial nets.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="bib.bib7.4.2" style="font-size:90%;">, 27, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">Generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.3.1" style="font-size:90%;">Communications of the ACM</span><span class="ltx_text" id="bib.bib8.4.2" style="font-size:90%;">, 63(11):139–144, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
Hayit Greenspan, Bram Van Ginneken, and Ronald M Summers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">Guest editorial deep learning in medical imaging: Overview and future promise of an exciting new technique.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.3.1" style="font-size:90%;">IEEE transactions on medical imaging</span><span class="ltx_text" id="bib.bib9.4.2" style="font-size:90%;">, 35(5):1153–1159, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
Danna Gurari, Diane Theriault, Mehrnoosh Sameki, Brett Isenberg, Tuan A Pham, Alberto Purwada, Patricia Solski, Matthew Walker, Chentian Zhang, Joyce Y Wong, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">How to collect segmentations for biomedical images? a benchmark evaluating the performance of experts, crowdsourced non-experts, and algorithms.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib10.4.2" style="font-size:90%;">2015 IEEE winter conference on applications of computer vision</span><span class="ltx_text" id="bib.bib10.5.3" style="font-size:90%;">, pages 1169–1176. IEEE, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
Eric Heim, Tobias Roß, Alexander Seitel, Keno März, Bram Stieltjes, Matthias Eisenmann, Johannes Lebert, Jasmin Metzger, Gregor Sommer, Alexander W Sauter, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.2.1" style="font-size:90%;">Large-scale medical image annotation with crowd-powered algorithms.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.3.1" style="font-size:90%;">Journal of Medical Imaging</span><span class="ltx_text" id="bib.bib11.4.2" style="font-size:90%;">, 5(3):034002–034002, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
Tobias Heimann and Hans-Peter Meinzer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">Statistical shape models for 3d medical image segmentation: a review.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.3.1" style="font-size:90%;">Medical image analysis</span><span class="ltx_text" id="bib.bib12.4.2" style="font-size:90%;">, 13(4):543–563, 2009.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
Amazon Mechanical Turk Inc.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">Amazon mechanical turk (mturk), 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.3.1" style="font-size:90%;">Available at: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mturk.com/" style="font-size:90%;" title="">https://www.mturk.com/</a><span class="ltx_text" id="bib.bib13.4.2" style="font-size:90%;">; Accessed on 7th April 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
Google Inc.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">Duolingo fraud detection, 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.3.1" style="font-size:90%;">Available at: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.duolingo.com/" style="font-size:90%;" title="">https://www.duolingo.com/</a><span class="ltx_text" id="bib.bib14.4.2" style="font-size:90%;">; Accessed on 7th April 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
Human Signal Inc.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">Open source data labelling platform, 2023.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.3.1" style="font-size:90%;">Available at: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://labelstud.io/" style="font-size:90%;" title="">https://labelstud.io/</a><span class="ltx_text" id="bib.bib15.4.2" style="font-size:90%;">; Accessed on 3rd November 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
National Cancer Institute.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">Cip cancer imaging program, cancer imaging archive, 2015.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.3.1" style="font-size:90%;">Available at: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cancerimagingarchive.net/" style="font-size:90%;" title="">https://www.cancerimagingarchive.net/</a><span class="ltx_text" id="bib.bib16.4.2" style="font-size:90%;">; Accessed on 27th December 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">Image-to-image translation with conditional adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib17.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib17.5.3" style="font-size:90%;">, pages 1125–1134, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">Segment anything.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib18.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib18.5.3" style="font-size:90%;">, pages 4015–4026, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
Google LLC.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">Google recaptcha, 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.3.1" style="font-size:90%;">Available at: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.google.com/recaptcha/about/" style="font-size:90%;" title="">https://www.google.com/recaptcha/about/</a><span class="ltx_text" id="bib.bib19.4.2" style="font-size:90%;">; Accessed on 7th April 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
Appen ltd.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.2.1" style="font-size:90%;">Appen figure eight, 2024.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.3.1" style="font-size:90%;">Available at: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.appen.com/ai-data" style="font-size:90%;" title="">https://www.appen.com/ai-data</a><span class="ltx_text" id="bib.bib20.4.2" style="font-size:90%;">; Accessed on 7th April 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
Jun Ma.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.2.1" style="font-size:90%;">Miccai flare22 challenge dataset (50 labeled abdomen ct scans), 2022.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.3.1" style="font-size:90%;">Available at: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/records/7860267" style="font-size:90%;" title="">https://zenodo.org/records/7860267</a><span class="ltx_text" id="bib.bib21.4.2" style="font-size:90%;">; Accessed on 10th December 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.2.1" style="font-size:90%;">Segment anything in medical images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.3.1" style="font-size:90%;">Nature Communications</span><span class="ltx_text" id="bib.bib22.4.2" style="font-size:90%;">, 15(1):654, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
Lena Maier-Hein, Sven Mersmann, Daniel Kondermann, Sebastian Bodenstedt, Alexandro Sanchez, Christian Stock, Hannes Gotz Kenngott, Mathias Eisenmann, and Stefanie Speidel.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.2.1" style="font-size:90%;">Can masses of non-experts train highly accurate image classifiers? a crowdsourcing approach to instrument segmentation in laparoscopic images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib23.4.2" style="font-size:90%;">Medical Image Computing and Computer-Assisted Intervention–MICCAI 2014: 17th International Conference, Boston, MA, USA, September 14-18, 2014, Proceedings, Part II 17</span><span class="ltx_text" id="bib.bib23.5.3" style="font-size:90%;">, pages 438–445. Springer, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
Mehdi Mirza and Simon Osindero.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.2.1" style="font-size:90%;">Conditional generative adversarial nets.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.3.1" style="font-size:90%;">arXiv preprint arXiv:1411.1784</span><span class="ltx_text" id="bib.bib24.4.2" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
Tony CW Mok and Albert CS Chung.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.2.1" style="font-size:90%;">Learning data augmentation for brain tumor segmentation with coarse-to-fine generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib25.4.2" style="font-size:90%;">Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 4th International Workshop, BrainLes 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Revised Selected Papers, Part I 4</span><span class="ltx_text" id="bib.bib25.5.3" style="font-size:90%;">, pages 70–80. Springer, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
Tanmai Sree Musalamadugu and Hemachandran Kannan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.2.1" style="font-size:90%;">Generative ai for medical imaging analysis and applications.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.3.1" style="font-size:90%;">Future Medicine AI</span><span class="ltx_text" id="bib.bib26.4.2" style="font-size:90%;">, 1(0):FMAI5, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
Nataša Petrović, Gabriel Moyà-Alcover, Javier Varona, and Antoni Jaume-i Capó.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.2.1" style="font-size:90%;">Crowdsourcing human-based computation for medical image analysis: A systematic literature review.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.3.1" style="font-size:90%;">Health Informatics Journal</span><span class="ltx_text" id="bib.bib27.4.2" style="font-size:90%;">, 26(4):2446–2469, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="font-size:90%;">
Vikas C Raykar, Shipeng Yu, Linda H Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, and Linda Moy.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.2.1" style="font-size:90%;">Learning from crowds.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.3.1" style="font-size:90%;">Journal of machine learning research</span><span class="ltx_text" id="bib.bib28.4.2" style="font-size:90%;">, 11(4), 2010.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="font-size:90%;">
Iqbal H Sarker.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.2.1" style="font-size:90%;">Deep learning: a comprehensive overview on techniques, taxonomy, applications and research directions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.3.1" style="font-size:90%;">SN Computer Science</span><span class="ltx_text" id="bib.bib29.4.2" style="font-size:90%;">, 2(6):420, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="font-size:90%;">
Connor Shorten and Taghi M. Khoshgoftaar.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.2.1" style="font-size:90%;">A survey on image data augmentation for deep learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.3.1" style="font-size:90%;">Journal of Big Data</span><span class="ltx_text" id="bib.bib30.4.2" style="font-size:90%;">, 6(1):60, Jul 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="font-size:90%;">
Thorvald Sorensen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.2.1" style="font-size:90%;">A method of establishing groups of equal amplitude in plant sociology based on similarity of species content and its application to analyses of the vegetation on danish commons.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.3.1" style="font-size:90%;">Biologiske skrifter</span><span class="ltx_text" id="bib.bib31.4.2" style="font-size:90%;">, 5:1–34, 1948.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.1.1" style="font-size:90%;">
Simon LF Walsh, Stephen M Humphries, Athol U Wells, and Kevin K Brown.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.2.1" style="font-size:90%;">Imaging research in fibrotic lung disease; applying deep learning to unsolved problems.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.3.1" style="font-size:90%;">The Lancet Respiratory Medicine</span><span class="ltx_text" id="bib.bib32.4.2" style="font-size:90%;">, 8(11):1144–1153, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.1.1" style="font-size:90%;">
Piotr Woznicki, Fabian Christopher Laqua, Adam Al-Haj, Thorsten Bley, and Bettina Baeßler.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.2.1" style="font-size:90%;">Addressing challenges in radiomics research: systematic review and repository of open-access cancer imaging datasets.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.3.1" style="font-size:90%;">Insights into Imaging</span><span class="ltx_text" id="bib.bib33.4.2" style="font-size:90%;">, 14(1):216, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.1.1" style="font-size:90%;">
Xiaodan Xing, Fadong Shi, Jiahao Huang, Yinzhe Wu, Yang Nan, Sheng Zhang, Yingying Fang, Mike Roberts, Carola-Bibiane Schönlieb, Javier Del Ser, and Guang Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.2.1" style="font-size:90%;">When ai eats itself: On the caveats of data pollution in the era of generative ai, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.1.1" style="font-size:90%;">
Xiahai Zhuang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.2.1" style="font-size:90%;">Mm-whs: Multi-modality whole heart segmentation, 2019.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.3.1" style="font-size:90%;">Available at: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zmiclab.github.io/zxh/0/mmwhs/" style="font-size:90%;" title="">https://zmiclab.github.io/zxh/0/mmwhs/</a><span class="ltx_text" id="bib.bib35.4.2" style="font-size:90%;">; Accessed on 14th September 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.1.1" style="font-size:90%;">
Xiahai Zhuang, Lei Li, Christian Payer, Darko Štern, Martin Urschler, Mattias P Heinrich, Julien Oster, Chunliang Wang, Örjan Smedby, Cheng Bian, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.2.1" style="font-size:90%;">Evaluation of algorithms for multi-modality whole heart segmentation: an open-access grand challenge.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.3.1" style="font-size:90%;">Medical image analysis</span><span class="ltx_text" id="bib.bib36.4.2" style="font-size:90%;">, 58:101537, 2019.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Sep  4 21:21:05 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
