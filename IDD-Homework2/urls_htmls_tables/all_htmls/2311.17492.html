<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data</title>
<!--Generated on Fri Jan 12 14:17:29 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2311.17492v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S1" title="1 Introduction ‣ Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S2" title="2 Related Work ‣ Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS1" title="2.1 Low-Resource Machine Translation ‣ 2 Related Work ‣ Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Low-Resource Machine Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS2" title="2.2 Typological Similarities between Manchu and Korean ‣ 2 Related Work ‣ Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Typological Similarities between Manchu and Korean</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="3 Data ‣ Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Data</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS1" title="3.1 Materials ‣ 3 Data ‣ Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Materials</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS2" title="3.2 Romanization of Manchu script and Hangul ‣ 3 Data ‣ Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Romanization of Manchu script and Hangul</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS3" title="3.3 Data Augmentation ‣ 3 Data ‣ Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Data Augmentation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S4" title="4 Experiments ‣ Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS1" title="4.1 Task Details ‣ 4 Experiments ‣ Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Task Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS2" title="4.2 Model ‣ 4 Experiments ‣ Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS3" title="4.3 Results and Discussions ‣ 4 Experiments ‣ Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Results and Discussions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S5" title="5 Conclusion ‣ Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="#A1" title="Appendix A Example Appendix ‣ Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Example Appendix</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div aria-label="”Conversion" been="" class="package-alerts ltx_document" errors="" found”="" have="" role="“status”">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewbox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
</div><div class="section" id="target-section"><div id="license-tr">License: CC BY 4.0</div><div id="watermark-tr">arXiv:2311.17492v2 [cs.CL] 12 Jan 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text ltx_font_italic" id="id1.id1">Mergen</span>: The First Manchu-Korean Machine Translation Model
<br class="ltx_break"/>Trained on Augmented Data </h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jean Seo,  Sungjoo Byun,  Minha Kang, Sangah Lee 
<br class="ltx_break"/>Seoul National University 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.1.id1">{seemdog, byunsj, alsgk1123, sanalee}@snu.ac.kr</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">The Manchu language, with its roots in the historical Manchurian region of Northeast China, is now facing a critical threat of extinction, as there are very few speakers left. In our efforts to safeguard the Manchu language, we introduce <span class="ltx_text ltx_font_italic" id="id3.id1.1">Mergen</span>, the first-ever attempt at a Manchu-Korean Machine Translation (MT) model. To develop this model, we utilize valuable resources such as the Mǎnwén Lǎodàng(a historical book) and a Manchu-Korean dictionary. Due to the scarcity of a Manchu-Korean parallel dataset, we expand our data by employing word replacement guided by GloVe embeddings, trained on both monolingual and parallel texts. Our approach is built around an encoder-decoder neural machine translation model, incorporating a bi-directional Gated Recurrent Unit (GRU) layer. The experiments have yielded promising results, showcasing a significant enhancement in Manchu-Korean translation, with a remarkable 20-30 point increase in the BLEU score.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Efforts to conserve and revive endangered languages have surged, with modern advancements in Natural Language Processing (NLP) playing a pivotal role. <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="#bib.bib39" title="">2020</a>)</cite> introduce ChrEn, a Cherokee-English parallel dataset, and examine methodologies like Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="#bib.bib39" title="">2020</a>)</cite> aid the conservation of Cherokee, a critically endangered Native American dialect. On a similar note, <cite class="ltx_cite ltx_citemacro_citet">Luo et al. (<a class="ltx_ref" href="#bib.bib24" title="">2020</a>)</cite> present a decipherment model for lost languages that addresses challenges posed by non-segmented scripts and undetermined proximate languages, leveraging linguistic constraints and the International Phonetic Alphabet (IPA) for phonological patterns.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Manchu language, originated from the historical Manchurian region in Northeast China, stands as a highly endangered Tungusic language of East Asia <cite class="ltx_cite ltx_citemacro_citep">(Tsunoda, <a class="ltx_ref" href="#bib.bib35" title="">2006</a>)</cite>. There are merely few Manchu speakers left nowadays, leading Manchu to be labeled ‘nearly extinct’ by UNESCO <cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a class="ltx_ref" href="#bib.bib15" title="">2008</a>)</cite>. The Manchu spell checker <cite class="ltx_cite ltx_citemacro_citep">(You, <a class="ltx_ref" href="#bib.bib38" title="">2014</a>)</cite> and the Manchu corpus with morphological annotations <cite class="ltx_cite ltx_citemacro_citep">(Choi et al., <a class="ltx_ref" href="#bib.bib6" title="">2023a</a>, <a class="ltx_ref" href="#bib.bib7" title="">b</a>)</cite> are the only prior approaches to embrace Manchu in the field of NLP. We introduce <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">Mergen</span>, the first Manchu-Korean machine translation model, which marks the pioneering effort to apply MT to the Manchu language.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We employ two sets of parallel corpora for machine translation from Manchu to Korean, as detailed in <cite class="ltx_cite ltx_citemacro_citet">Kim et al. (<a class="ltx_ref" href="#bib.bib14" title="">2019</a>)</cite>. Initially, we train an adapted version of the NMT model <cite class="ltx_cite ltx_citemacro_citep">(Bahdanau et al., <a class="ltx_ref" href="#bib.bib2" title="">2016</a>)</cite>. Assuming the unexpectedly low performance is due to the scarcity of Manchu-Korean data, we augment the size of parallel data several fold utilizing GloVe <cite class="ltx_cite ltx_citemacro_citep">(Pennington et al., <a class="ltx_ref" href="#bib.bib29" title="">2014</a>)</cite>. Our findings suggest that this data augmentation methodology substantially enhances translation quality.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Despite the constrained availability of resources, our goal is to enhance Manchu-Korean machine translation performance. To symbolize our commitment to the field of Manchu NLP, we christen our model <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">Mergen</span>, denoting a sage or a wise individual in the Manchu lexicon. Our translation approach, which employs a data augmentation technique, not only seeks to improve Manchu-Korean translation performance but also aims to eventually serve as a potential model for addressing NLP challenges in other extremely low-resource scenarios as addressed in <cite class="ltx_cite ltx_citemacro_citet">King (<a class="ltx_ref" href="#bib.bib16" title="">2015</a>)</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Low-Resource Machine Translation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">MT necessitates parallel data of source and target languages to be trained effectively. However, the majority of language pairs face a scarcity of resources. As a result, there has been various research endeavors aimed at developing translation models in low-resource scenarios. Extended language models such as XLM-RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Conneau et al., <a class="ltx_ref" href="#bib.bib8" title="">2019</a>)</cite>, mBART <cite class="ltx_cite ltx_citemacro_citep">(Tang et al., <a class="ltx_ref" href="#bib.bib34" title="">2021</a>)</cite>, multilingual BERT (mBERT) <cite class="ltx_cite ltx_citemacro_citep">(Pires et al., <a class="ltx_ref" href="#bib.bib30" title="">2019</a>)</cite>, and mT5 <cite class="ltx_cite ltx_citemacro_citep">(Xue et al., <a class="ltx_ref" href="#bib.bib37" title="">2021</a>)</cite> are trained on diverse languages. Yet, most of these multilingual language models tend not to incorporate endangered languages. This leads to an increasing disparity in NLP resources, where less-resourced languages are further marginalized. Numerous strategies have been attempted in low-resource machine translation. <cite class="ltx_cite ltx_citemacro_citet">Gibadullin et al. (<a class="ltx_ref" href="#bib.bib11" title="">2019</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Siddhant et al. (<a class="ltx_ref" href="#bib.bib33" title="">2020</a>)</cite> employ monolingual data in low-resource NMT. Additionally, utilization of pre-trained word embeddings <cite class="ltx_cite ltx_citemacro_citep">(Qi et al., <a class="ltx_ref" href="#bib.bib31" title="">2018</a>)</cite> and application of transfer learning with pretrained language models like XLM <cite class="ltx_cite ltx_citemacro_citep">(Lample and Conneau, <a class="ltx_ref" href="#bib.bib20" title="">2019</a>)</cite> and mBART <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="#bib.bib23" title="">2020</a>)</cite> have been employed. Furthermore, <cite class="ltx_cite ltx_citemacro_citet">Lakew et al. (<a class="ltx_ref" href="#bib.bib19" title="">2018</a>)</cite> enhance the zero-shot translation capability of low-resource languages.</p>
</div>
<figure class="ltx_figure" id="S2.F1">
<p class="ltx_p ltx_align_center" id="S2.F1.1"><span class="ltx_text" id="S2.F1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="373" id="S2.F1.1.1.g1" src="extracted/5344446/image.png" width="598"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Our data augmentation methodology. First, we train ten versions of GloVe embedding models, varying in the minimum token length of source data and window size. Then, the presumable synonym for the target word is selected via comparing the frequency of outputs from each model. Finally, we augment data through replacing original words with synonyms if possible. The pair of original and substituted words are in the same color.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Typological Similarities between Manchu and Korean</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">There are several typological motivations for translating Manchu to Korean using a Machine Translation model. The genetic affinity between Manchu and Korean is not proven, but it is well-known that Manchu has a similar structure to that of Korean. The word order of Manchu and Korean mostly coincide, including the order of ‘noun-particle,’ ‘modifier-modified,’ and ‘object-verb,’ etc. <cite class="ltx_cite ltx_citemacro_citep">(Park, <a class="ltx_ref" href="#bib.bib28" title="">2018</a>)</cite>. Substitutes in Korean, <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">kes</span>, and Manchu, <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.2">-ngge</span>, have analogous grammatical functions and positions <cite class="ltx_cite ltx_citemacro_citep">(Choi, <a class="ltx_ref" href="#bib.bib5" title="">2009</a>)</cite>. The two languages both show factivity alternation by using the attitude verb ‘to know’ <cite class="ltx_cite ltx_citemacro_citep">(Lee, <a class="ltx_ref" href="#bib.bib21" title="">2019</a>)</cite> and have parallel subordinated clause structures <cite class="ltx_cite ltx_citemacro_citep">(Malchukov and Czerwinski, <a class="ltx_ref" href="#bib.bib25" title="">2020</a>)</cite>. These typological similarities between Manchu and Korean arouse interest in understanding and linguistically translating each other. In fact, studies of the Manchu language are active in Korea <cite class="ltx_cite ltx_citemacro_citep">(Ko, <a class="ltx_ref" href="#bib.bib17" title="">2023</a>)</cite>.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T1.1" style="width:433.6pt;height:322pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(95.6pt,-71.0pt) scale(1.78914578810923,1.78914578810923) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T1.1.1">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<td class="ltx_td ltx_align_right" id="S2.T1.1.1.1.1">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
<span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1">Monolingual data</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.2.1">Number of sentences</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.2.1">Mǎnwén Lǎodàng–Taizong</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.2.2">2,220</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.3">
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.3.1">Ilan gurun i bithe</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.3.2">41,904</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.4">
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.1">Gin ping mei bithe</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.2">21,376</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.5">
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.1">Yùzhì Qīngwénjiàn</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.2">11,954</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.6">
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.1">Yùzhì Zēngdìng Qīngwénjiàn</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.6.2">18,420</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.7">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.7.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.7.1.1">Parallel data (Man-Kor)</span></td>
<td class="ltx_td ltx_border_t" id="S2.T1.1.1.7.2"></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.8">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.8.1">Mǎnwén Lǎodàng–Taizu</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.8.2">22,578</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.9">
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.9.1">Manchu-Korean Dictionary</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.9.2">40,583</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.10">
<td class="ltx_td ltx_align_right" id="S2.T1.1.1.10.1">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td" id="S2.T1.1.1.10.2"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>The size of each material</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Data</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Materials</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The Manchu corpora used in this study comprise all of the digitized textual data available and can be categorized as either parallel or monolingual. The parallel corpora are <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">Mǎnwén Lǎodàng</span> (1774-1778) and the Manchu-Korean dictionary. These corpora consist of Manchu texts and their corresponding translations in Korean. We only utilize a section of the <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.2">Mǎnwén Lǎodàng</span> and its translations from <cite class="ltx_cite ltx_citemacro_citet">Kim et al. (<a class="ltx_ref" href="#bib.bib14" title="">2019</a>)</cite>, which details the history of Nurhaci, the Emperor Taizu of Qing dynasty. Additionally, we refer to the dictionary from <cite class="ltx_cite ltx_citemacro_citet">Lee (<a class="ltx_ref" href="#bib.bib22" title="">2017</a>)</cite> and select sentences with a minimum of three words.
</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The monolingual texts of Manchu include the remaining part of <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.1">Mǎnwén Lǎodàng</span>, Manchu-Manchu dictionaries, and several pieces of literature. The part of <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.2">Mǎnwén Lǎodàng</span> left over is the chronicle of Hong Taiji, the Emperor Taizong of Qing. The Manchu-Manchu dictionaries we use are <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.3">Yùzhì Qīngwénjiàn</span> (1708) and <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.4">Yùzhì Zēngdìng Qīngwénjiàn</span> (c.1771).</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">The other data is composed of novels, <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.1">Ilan gurun i bithe</span> (c.1723-1735) and <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.2">Gin ping mei bithe</span> (1708). <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.3">Ilan gurun i bithe</span> is the translated version of <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.4">The Romance of the Three Kingdoms</span>. <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.5">Gin ping mei bithe</span> is translated from the Chinese naturalistic novel, <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.1.6">The Plum in the Golden Vase</span>. The size description of each data can be found in Table <a class="ltx_ref" href="#S2.T1" title="Table 1 ‣ 2.2 Typological Similarities between Manchu and Korean ‣ 2 Related Work ‣ Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Romanization of Manchu script and Hangul</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To create a more sufficient translation model, the script of each language should be unified in one writing system. That is, both the source and target language should undergo transliteration to the Latin alphabet, so-called ‘romanization’. For the romanization of Manchu, we apply Abkai Latin transliteration. The Abkai romanization suggested by <cite class="ltx_cite ltx_citemacro_citet">An (<a class="ltx_ref" href="#bib.bib1" title="">1993</a>)</cite> is a Pinyin-based writing system. We also use the system of <cite class="ltx_cite ltx_citemacro_citet">Seong (<a class="ltx_ref" href="#bib.bib32" title="">1977</a>)</cite> for the special characters in the Manchu script. Transliteration of Manchu to the Latin alphabet is reversible except for a couple of letters. For the Latin transliteration of Korean, we employ Yale romanization system <cite class="ltx_cite ltx_citemacro_citep">(Martin, <a class="ltx_ref" href="#bib.bib26" title="">1992</a>)</cite> and develop the corresponding Python library<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/SeHaan/YaleKorean</span></span></span>. See Appendix  <a class="ltx_ref" href="#A1" title="Appendix A Example Appendix ‣ Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data"><span class="ltx_text ltx_ref_tag">A</span></a> for examples.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Data Augmentation</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The lack of available Manchu linguistic data poses challenges not only for the pre-training of transformer-based models but also for the training of simpler and more lightweight models, such as encoder-decoder models. Inspired by TinyBERT <cite class="ltx_cite ltx_citemacro_citep">(Jiao et al., <a class="ltx_ref" href="#bib.bib13" title="">2020</a>)</cite>, we adopt a novel data augmentation approach. While the data augmentation method in TinyBERT <cite class="ltx_cite ltx_citemacro_citep">(Jiao et al., <a class="ltx_ref" href="#bib.bib13" title="">2020</a>)</cite> combines both BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a class="ltx_ref" href="#bib.bib9" title="">2019</a>)</cite> and GloVe <cite class="ltx_cite ltx_citemacro_citep">(Pennington et al., <a class="ltx_ref" href="#bib.bib29" title="">2014</a>)</cite>, we exclusively employ GloVe embeddings. This decision stems from the absence of a pre-trained BERT model tailored to Manchu and the significant difficulty of pre-training a BERT model from scratch due to the limited amount of available textual data.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Our methodology involves training GloVe embedding models with two different versions of the dataset: (1) a dataset comprising sentences with at least 3 words, and (2) a dataset comprising sentences with at least 5 words. The dataset includes both monolingual and parallel text data. Various window sizes, specifically 1, 3, 5, 7, and 10, are used during the training process, resulting in a total of 10 distinct variations of GloVe embeddings.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">For each word in the training dataset, we gather the most similar word predicted by each individual GloVe embedding. Amongst the list of 10 words generated from these separate models, the word with the highest frequency is considered the most suitable synonym for the target word. Following this, we substitute a single word in each sentence from parallel text data with the identified synonym. The augmentation steps are described in Figure <a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ 2.1 Low-Resource Machine Translation ‣ 2 Related Work ‣ Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data"><span class="ltx_text ltx_ref_tag">1</span></a>. This procedure leads to the creation of two augmented versions of the original dataset: full augmentation and half augmentation. The first version involves replacing every word possible in each sentence with its corresponding synonym, significantly expanding the dataset size relative to the average sentence length. The second version is generated by replacing half of the words in each sentence with their respective synonyms, resulting in a dataset expansion about half the size of the first method. Additional details regarding the original and augmented dataset are available in Table <a class="ltx_ref" href="#S3.T2" title="Table 2 ‣ 3.3 Data Augmentation ‣ 3 Data ‣ Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T2.1" style="width:433.6pt;height:179.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(96.3pt,-39.8pt) scale(1.79904225283991,1.79904225283991) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<td class="ltx_td ltx_align_right" id="S3.T2.1.1.1.1">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
<span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.1">augmentation</span></td>
<td class="ltx_td ltx_align_justify" id="S3.T2.1.1.1.2" style="width:85.4pt;"><span class="ltx_text ltx_font_bold ltx_align_center ltx_align_top" id="S3.T2.1.1.1.2.1">Mǎnwén Lǎodàng
<br class="ltx_break"/>–Taizu (train)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.3.1">Man-Kor Dict</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.2.1.1">Before augmentation</span></td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.1.1.2.2" style="width:85.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.1.1.2.2.1">20,320</p>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.2.3">40,583</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.3">
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.3.1"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.3.1.1">Full augmentation</span></td>
<td class="ltx_td ltx_align_justify" id="S3.T2.1.1.3.2" style="width:85.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.1.1.3.2.1">179,843</p>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.3.3">154,404</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.4">
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.1"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.4.1.1">Half augmentation</span></td>
<td class="ltx_td ltx_align_justify" id="S3.T2.1.1.4.2" style="width:85.4pt;">
<p class="ltx_p ltx_align_top" id="S3.T2.1.1.4.2.1">99,506</p>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.4.3">100,694</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.5">
<td class="ltx_td ltx_align_right" id="S3.T2.1.1.5.1">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td ltx_align_justify" id="S3.T2.1.1.5.2" style="width:85.4pt;"></td>
<td class="ltx_td" id="S3.T2.1.1.5.3"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>The number of sentences of parallel text data before and after augmentation</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Task Details</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In the experiment, we merge <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">Mǎnwén Lǎodàng</span> with Manchu-Korean dictionary and shuffle them together. The combined dataset is then divided into training, validation, and testing subsets. These subsets are split in an 8:1:1 ratio. In the augmentation process, we first shuffle and then augment the data to even out the word distributions, finally splitting into subsets.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Model</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We adopt the sequence-to-sequence (seq2seq) framework, a deep learning approach designed to transform one sequence into another. Our model is based on the encoder-decoder structure of the NMT <cite class="ltx_cite ltx_citemacro_citep">(Bahdanau et al., <a class="ltx_ref" href="#bib.bib2" title="">2016</a>)</cite>, implemented with bi-directional Gated Recurrent Unit (GRU) layer <cite class="ltx_cite ltx_citemacro_citep">(Cho et al., <a class="ltx_ref" href="#bib.bib3" title="">2014</a>)</cite>. We incorporate two techniques to enhance the performance: packed padded sequences and masking. Packed padded sequences ensure that the RNN processes only the genuine elements of the input sentence, excluding the padded ones. Masking directs the model to deliberately overlook specific components, like attention weights assigned to padded sections.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results and Discussions</h3>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.1" style="width:433.6pt;height:650.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(96.7pt,-145.0pt) scale(1.80570170960043,1.80570170960043) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.1.1">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
<span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1">Train</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.2.1">Test</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.3.1">BLEU</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.4.1">PPL</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.2">
<td class="ltx_td ltx_align_left" colspan="2" id="S4.T3.1.1.2.1">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span> <span class="ltx_text ltx_font_bold" id="S4.T3.1.1.2.1.1">Before augmentation (No augmentation)</span></td>
<td class="ltx_td" id="S4.T3.1.1.2.2"></td>
<td class="ltx_td" id="S4.T3.1.1.2.3"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.3.1">Mǎnwén Lǎodàng</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.3.2">Mǎnwén Lǎodàng</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.3.3">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.3.4">72.50</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.4">
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.4.1">Man-Kor Dict</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.4.2">Man-Kor Dict</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.4.3">0.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.4.4">59.34</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.5.1" rowspan="3"><span class="ltx_text" id="S4.T3.1.1.5.1.1">Combined</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.5.2">Mǎnwén Lǎodàng</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.5.3">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.5.4">61.83</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.6">
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.1">Man-Kor Dict</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.2">0.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.3">61.16</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.7">
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.7.1">Combined</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.7.2">0.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.7.3">69.62</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.8">
<td class="ltx_td ltx_align_left" colspan="2" id="S4.T3.1.1.8.1">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span> <span class="ltx_text ltx_font_bold" id="S4.T3.1.1.8.1.1">Half augmentation</span></td>
<td class="ltx_td" id="S4.T3.1.1.8.2"></td>
<td class="ltx_td" id="S4.T3.1.1.8.3"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.9.1">Mǎnwén Lǎodàng</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.9.2">Mǎnwén Lǎodàng</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.9.3">38.38</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.9.4">147.07</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.10">
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.10.1">Man-Kor Dict</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.10.2">Man-Kor Dict</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.10.3">0.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.10.4">174.94</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.11.1" rowspan="3"><span class="ltx_text" id="S4.T3.1.1.11.1.1">Combined</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.11.2">Mǎnwén Lǎodàng</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.11.3">36.05</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.11.4">192.95</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.12">
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.12.1">Man-Kor Dict</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.12.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.12.2.1">2.37</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.12.3">36.14</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.13">
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.13.1">Combined</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.13.2">27.59</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.13.3">29.22</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.14">
<td class="ltx_td ltx_align_left" colspan="2" id="S4.T3.1.1.14.1">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span> <span class="ltx_text ltx_font_bold" id="S4.T3.1.1.14.1.1">Full augmentation</span></td>
<td class="ltx_td" id="S4.T3.1.1.14.2"></td>
<td class="ltx_td" id="S4.T3.1.1.14.3"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.15">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.15.1">Mǎnwén Lǎodàng</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.15.2">Mǎnwén Lǎodàng</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.15.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.15.3.1">38.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.15.4">1549.40</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.16">
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.16.1">Man-Kor Dict</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.16.2">Man-Kor Dict</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.16.3">0.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.16.4">158.25</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.17">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.17.1" rowspan="3"><span class="ltx_text" id="S4.T3.1.1.17.1.1">Combined</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.17.2">Mǎnwén Lǎodàng</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.17.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.17.3.1">37.17</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.17.4">447.59</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.18">
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.18.1">Man-Kor Dict</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.18.2">2.26</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.18.3">46.54</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.19">
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.19.1">Combined</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.19.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.19.2.1">28.00</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.19.3">41.97</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.20">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.20.1">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td ltx_border_t" id="S4.T3.1.1.20.2"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.1.1.20.3"></td>
<td class="ltx_td ltx_border_t" id="S4.T3.1.1.20.4"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Manchu-Korean Translation Performance</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We perform machine translation and evaluate the performance on all the available combinations of parallel corpora: <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">Mǎnwén Lǎodàng</span>, Manchu-Korean dictionary, and the combined dataset. In particular, we augment the training sets of each corpus to alleviate the data scarcity problem. <a class="ltx_ref ltx_refmacro_autoref" href="#S4.T3" title="Table 3 ‣ 4.3 Results and Discussions ‣ 4 Experiments ‣ Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data"><span class="ltx_text ltx_ref_tag">Table 3</span></a> shows the performance of our Manchu-Korean translation models, with BLEU score <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a class="ltx_ref" href="#bib.bib27" title="">2002</a>)</cite> and Perplexity (PPL) as the metrices. We train each model for 5 epochs and report the one with the best performance.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">The first block of <a class="ltx_ref ltx_refmacro_autoref" href="#S4.T3" title="Table 3 ‣ 4.3 Results and Discussions ‣ 4 Experiments ‣ Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data"><span class="ltx_text ltx_ref_tag">Table 3</span></a> shows the translation performance based on the original Manchu-Korean parallel corpora. All the experiments here show BLEU scores of 0.0, which represent that none of the test sentences are accurately translated. Most of the predicted translations include the special symbol ‘&lt;UNK&gt;’ instead of proper Korean tokens, possibly due to the small dataset and vocabulary size.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">The second block shows the experiment results from the augmented version of the parallel corpora, where up to 50% of the tokens in each sentence are replaced for data augmentation. The third block displays experiments on another augmented version where all tokens with substitutes are replaced. The augmentation procedure increases the size of the training set, resulting in a significant rise in the translation performance. BLEU scores exceed 38 on the <span class="ltx_text ltx_font_italic" id="S4.SS3.p3.1.1">Mǎnwén Lǎodàng</span> test set, and around 28 on the combined test set. The two versions of the augmented dataset show comparable performance, but replacing all the possible words in the corpus resulted in slightly higher BLEU scores.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">Due to data augmentation, the vocabulary for each model is expanded; for example, the original <span class="ltx_text ltx_font_italic" id="S4.SS3.p4.1.1">Mǎnwén Lǎodàng</span> vocabulary includes 4,335 words, while the full-augmented dataset constructs an expanded vocabulary with 11,089 words. A larger vocabulary and training set may have helped the language model’s representation and result in better translation performance. Additionally, most newly induced words are from the augmentation sources which include monolingual Manchu texts, different from our parallel corpora. This expansion of word diversity may have also affected the models’ perplexity to increase when they predicted the next words in each sentence.</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1">On the other hand, results on the Manchu-Korean dictionary are consistently very low, and this may have influenced the lower performance of the combined test set. We suppose that it is because the corpus is a dictionary, where each line is a unique word or phrase. The training set and the test set would have much fewer overlaps in their vocabularies, and this could cause a number of ‘&lt;UNK&gt;’ generations in the model prediction.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In our exploration of the critically endangered Manchu language, we have made significant strides towards development of low-resource NLP through the development of the Manchu-Korean MT system, "Mergen." Our endeavor to train this model, despite the challenges posed by the scarcity of a Manchu-Korean parallel dataset, demonstrates the potential of an innovative data augmentation strategy. This attempt is also significant in that we have collected all the digitized Manchu text data. By leveraging resources such as "Mǎnwén Làodǎng" and a Manchu-Korean dictionary, and by adopting a word substitution techniqus guided by GloVe embeddings, we have not only built a functional MT system but have also considerably enhanced its accuracy, as evidenced by the increase in the BLEU score. Our encoder-decoder NMT model, equipped with a bi-directional GRU layer, has shown promising results, offering hope for the preservation and accessibility of the Manchu language to future generations. We anticipate that this research will serve as a foundation for further innovations in the realm of endangered language preservation.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">The main limitation of this study is the scarcity of resources. Numerous Manchu literatures exist in East Asia <cite class="ltx_cite ltx_citemacro_citep">(Vovin, <a class="ltx_ref" href="#bib.bib36" title="">2023</a>)</cite>, including China <cite class="ltx_cite ltx_citemacro_citep">(Elliott, <a class="ltx_ref" href="#bib.bib10" title="">2001</a>)</cite>, Korea <cite class="ltx_cite ltx_citemacro_citep">(Ko and You, <a class="ltx_ref" href="#bib.bib18" title="">2012</a>)</cite>, and Mongolia <cite class="ltx_cite ltx_citemacro_citep">(Choi, <a class="ltx_ref" href="#bib.bib4" title="">2014</a>)</cite>. However, most of them lack an electronic version. The only publicly available Manchu language database is the Manchu Dictionary and Literature DB, created by Seoul National University and supported by the National Research Foundation of Korea.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>NFR-2012S1A5B4A01035397, available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://ffr.krm.or.kr/base/td037/intro_db.html" title="">http://ffr.krm.or.kr/base/td037/intro_db.html</a></span></span></span> Furthermore, the majority of these resources have not been translated into Korean. To address this gap, we intend to provide supplementary parallel texts translated into Korean for further study. In addition, we plan to implement a cutting-edge method of Transformer-based language model including Manchu language. Knowledge Distillation could be a way for modeling endangered languages, training a small student model based on those languages and improving it with a teacher model based on high-resource languages <cite class="ltx_cite ltx_citemacro_citep">(Heffernan et al., <a class="ltx_ref" href="#bib.bib12" title="">2022</a>)</cite>.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Ethics Statement</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">The Manchu language, classified as critically endangered, remains underrepresented due to its scarce resources. As such, it has yet to be incorporated into any multilingual language models. This study pioneers Manchu translation efforts, an endeavor previously uncharted. Our primary research objective as NLP practitioners is to prevent the extinction of Manchu language and ensure its preservation. We have no intention of commercializing the translation model. Instead, by making the model publicly available, we aim to facilitate and encourage as many individuals as possible to learn Manchu using our translator. We are committed to continuous collaboration with Manchu language researchers. We endeavor to enhance the performance of our translator and regularly update it with new Manchu data to ensure its accuracy.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">An (1993)</span>
<span class="ltx_bibblock">
Shuangcheng An. 1993.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Man Han Da Ci Dian</em>.

</span>
<span class="ltx_bibblock">Liaoning Minzhu Chubanshe, Shenyang.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahdanau et al. (2016)</span>
<span class="ltx_bibblock">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1409.0473" title="">Neural machine translation by
jointly learning to align and translate</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al. (2014)</span>
<span class="ltx_bibblock">
Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Fethi
Bougares, Holger Schwenk, and Yoshua Bengio. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1406.1078" title="">Learning phrase
representations using RNN encoder-decoder for statistical machine
translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">CoRR</em>, abs/1406.1078.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi (2014)</span>
<span class="ltx_bibblock">
Donggeun Choi. 2014.

</span>
<span class="ltx_bibblock">A study of Manchu language literatures in Mongolia.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">The journal of humanities</em>, 25:275–303.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi (2009)</span>
<span class="ltx_bibblock">
Dongguen Choi. 2009.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.17292/kams.2009..27.008" title="">A comparative
study of substitute - Korean <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1.1">keos</span>, Mongolian <span class="ltx_text ltx_font_italic" id="bib.bib5.2.2.2">yum</span>,
Manchu <span class="ltx_text ltx_font_italic" id="bib.bib5.3.3.3">-ngge</span> -</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.4.1">Mongolian Studies</em>, 27:205–228.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et al. (2023a)</span>
<span class="ltx_bibblock">
Woonho Choi, Sunghoon Jung, and Jeongup Do. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.15816/ask.2023..33.003" title="">Construction of
the Manchu corpus: focusing on <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1.1">Manwen laodang Taidzu</span></a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.2.1">Altai Hakpo</em>, 33:67–87.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et al. (2023b)</span>
<span class="ltx_bibblock">
Woonho Choi, Sunghoon Jung, and Jeongup Do. 2023b.

</span>
<span class="ltx_bibblock">Word embeddings for <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Manwen laodang</span> corpus with focus on
names of countries and articles.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.2.1">Proceedings of the 16th Seoul International Altaistic
Conference</em>, pages 189–204. The Altaic Society of Korea.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2019)</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and
Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1911.02116" title="">Unsupervised cross-lingual
representation learning at scale</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">CoRR</em>, abs/1911.02116.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1810.04805" title="">Bert: Pre-training of deep
bidirectional transformers for language understanding</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elliott (2001)</span>
<span class="ltx_bibblock">
Mark C Elliott. 2001.

</span>
<span class="ltx_bibblock">The manchu-language archives of the qing dynasty and the origins of
the palace memorial system.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Late Imperial China</em>, 22(1):1–70.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gibadullin et al. (2019)</span>
<span class="ltx_bibblock">
Ilshat Gibadullin, Aidar Valeev, Albina Khusainova, and Adil Khan. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1910.00373" title="">A survey of methods to
leverage monolingual data in low-resource neural machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">CoRR</em>, abs/1910.00373.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heffernan et al. (2022)</span>
<span class="ltx_bibblock">
Kevin Heffernan, Onur Çelebi, and Holger Schwenk. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2205.12654" title="">Bitext mining using
distilled sentence representations for low-resource languages</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiao et al. (2020)</span>
<span class="ltx_bibblock">
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang
Wang, and Qun Liu. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1909.10351" title="">Tinybert: Distilling bert
for natural language understanding</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2019)</span>
<span class="ltx_bibblock">
Juwon Kim, Dongho Ko, Gyeyeong Choe, Sangchul Park, Jeongup Do, Hyungmi Lee,
Hui Jin, and Jaehong Shim. 2019.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Tongki Fuka Sindaha Hergen i Dangse - The
Chronicles of Early Qing Dynasty: Taizu Vol. 1   2</em>.

</span>
<span class="ltx_bibblock">Seoul National University Press, Seoul.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2008)</span>
<span class="ltx_bibblock">
Juwon Kim, Dongho Ko, Youfeng Han, Lianyu Piao, and B. V. Boldyrev. 2008.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:127506280" title=""><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1.1">Materials of spoken Manchu</em></a>.

</span>
<span class="ltx_bibblock">unesco.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">King (2015)</span>
<span class="ltx_bibblock">
Benjamin Philip King. 2015.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Practical Natural Language Processing for Low-Resource
Languages.</em>
</span>
<span class="ltx_bibblock">Ph.D. thesis.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ko (2023)</span>
<span class="ltx_bibblock">
Dongho Ko. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002932384" title="">Manchu-tungus studies in korea: Focusing on the studies of third-generation
scholars</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Reosiahag</em>, 26:1–27.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ko and You (2012)</span>
<span class="ltx_bibblock">
Dongho Ko and Hyunjo You. 2012.

</span>
<span class="ltx_bibblock">For building a database of written Manchu.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Kenci Inmwunhak</em>, 8:5–30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lakew et al. (2018)</span>
<span class="ltx_bibblock">
Surafel Melaku Lakew, Quintino F. Lotito, Matteo Negri, Marco Turchi, and
Marcello Federico. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1811.01389" title="">Improving zero-shot
translation of low-resource languages</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">CoRR</em>, abs/1811.01389.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lample and Conneau (2019)</span>
<span class="ltx_bibblock">
Guillaume Lample and Alexis Conneau. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1901.07291" title="">Cross-lingual language model
pretraining</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">CoRR</em>, abs/1901.07291.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee (2019)</span>
<span class="ltx_bibblock">
Chungmin Lee. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.17791/jcs.2019.20.4.451" title="">Factivity
alternation of attitude ‘know’ in Korean, Mongolian, Uyghur,
Manchu, Azeri, etc. and content clausal nominals</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Journal of Cognitive Science</em>, 20(4):449–503.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee (2017)</span>
<span class="ltx_bibblock">
Hoon Lee. 2017.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Manju Solho Gisun Kamcibuha Buleku Bithe -
Manhan-sacen</em>.

</span>
<span class="ltx_bibblock">Korea University Press, Seoul.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2020)</span>
<span class="ltx_bibblock">
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan
Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00343" title="">Multilingual denoising
pre-training for neural machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Transactions of the Association for Computational Linguistics</em>,
8:726–742.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2020)</span>
<span class="ltx_bibblock">
Jiaming Luo, Frederik Hartmann, Enrico Santus, Yuan Cao, and Regina Barzilay.
2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2010.11054" title="">Deciphering undersegmented
ancient scripts using phonetic prior</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">CoRR</em>, abs/2010.11054.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malchukov and Czerwinski (2020)</span>
<span class="ltx_bibblock">
Andrej Malchukov and Patryk Czerwinski. 2020.

</span>
<span class="ltx_bibblock">Complex constructions in the Transeurasian languages.

</span>
<span class="ltx_bibblock">In Martine Robbeets and Alexander Savelyev, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">The
Oxford Guide to the Transeurasian Languages</em>, pages 625–644. Oxford
University Press, Oxford.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martin (1992)</span>
<span class="ltx_bibblock">
Samuel E. Martin. 1992.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">A Reference Grammar of Korean</em>.

</span>
<span class="ltx_bibblock">Charles E. Tuttle, Rutland, VT and Tokyo.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/1073083.1073135" title="">Bleu: a method for
automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics</em>, pages 311–318, Philadelphia, Pennsylvania,
USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park (2018)</span>
<span class="ltx_bibblock">
Sangchul Park. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.17290/jlsk.2018..81.243" title="">The function of
Modern Korean as in discourse</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Eoneohag</em>, 81:243–264.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington et al. (2014)</span>
<span class="ltx_bibblock">
Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/v1/D14-1162" title="">GloVe: Global
vectors for word representation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 1532–1543, Doha, Qatar.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pires et al. (2019)</span>
<span class="ltx_bibblock">
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1906.01502" title="">How multilingual is
multilingual devlin?</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">CoRR</em>, abs/1906.01502.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et al. (2018)</span>
<span class="ltx_bibblock">
Ye Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig.
2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N18-2084" title="">When and why are
pre-trained word embeddings useful for neural machine translation?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 2 (Short Papers)</em>, pages 529–535,
New Orleans, Louisiana. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seong (1977)</span>
<span class="ltx_bibblock">
Baegin Seong. 1977.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE02273168" title="">Romanization of the special letters of manchu</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Eoneohag</em>, 2:185–197.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Siddhant et al. (2020)</span>
<span class="ltx_bibblock">
Aditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Firat, Mia Xu Chen, Sneha Reddy
Kudugunta, Naveen Arivazhagan, and Yonghui Wu. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2005.04816" title="">Leveraging monolingual data
with self-supervision for multilingual neural machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">CoRR</em>, abs/2005.04816.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2021)</span>
<span class="ltx_bibblock">
Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary,
Jiatao Gu, and Angela Fan. 2021.

</span>
<span class="ltx_bibblock">Multilingual translation from denoising pre-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Findings of the Association for Computational Linguistics:
ACL-IJCNLP 2021</em>, pages 3450–3466.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsunoda (2006)</span>
<span class="ltx_bibblock">
Tasaku Tsunoda. 2006.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Language endangerment and language revitalization: An
introduction</em>.

</span>
<span class="ltx_bibblock">De Gruyter Mouton.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vovin (2023)</span>
<span class="ltx_bibblock">
Alexander Vovin. 2023.

</span>
<span class="ltx_bibblock">Written Manchu.

</span>
<span class="ltx_bibblock">In Alexander Vovin, José Andrés Alonso de la Fuente, and Juha
Janhunen, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">The Tungusic Languages</em>, pages 103–138.
Routledge, New York.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al. (2021)</span>
<span class="ltx_bibblock">
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, and Colin Raffel. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2010.11934" title="">mt5: A massively
multilingual pre-trained text-to-text transformer</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">You (2014)</span>
<span class="ltx_bibblock">
Hyun-Jo You. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.15816/ask.2014..24.003" title="">A manchu speller:
With a practical introduction to the natural language processing of minority
languages</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Altai Hakpo</em>, 24:39–67.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020)</span>
<span class="ltx_bibblock">
Shiyue Zhang, Benjamin Frey, and Mohit Bansal. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2010.04791" title="">Chren: Cherokee-english
machine translation for endangered language revitalization</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Example Appendix</h2>
<figure class="ltx_figure" id="A1.F2">
<p class="ltx_p ltx_align_center" id="A1.F2.1"><span class="ltx_text" id="A1.F2.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="260" id="A1.F2.1.1.g1" src="extracted/5344446/man_sen.png" width="598"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Example of Romanizations of Manchu text and Korean text</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Jan 12 14:17:29 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
