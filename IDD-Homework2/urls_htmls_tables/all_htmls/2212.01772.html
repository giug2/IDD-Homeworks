<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2212.01772] Brain Tumor Synthetic Data Generation with Adaptive StyleGANsThis preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this contribution is published in [insert volume title], and is available online at https://doi.org/doi to be updated . Upon publishing, final version will be available from Springer Nature Publishers (link to be updated). Pre-print of work submitted to 30th AICS conference.</title><meta property="og:description" content="Generative models have been very successful over the years and have received significant attention for synthetic data generation. As deep learning models are getting more and more complex, they require large amounts ofâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Brain Tumor Synthetic Data Generation with Adaptive StyleGANsThis preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this contribution is published in [insert volume title], and is available online at https://doi.org/doi to be updated . Upon publishing, final version will be available from Springer Nature Publishers (link to be updated). Pre-print of work submitted to 30th AICS conference.">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Brain Tumor Synthetic Data Generation with Adaptive StyleGANsThis preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this contribution is published in [insert volume title], and is available online at https://doi.org/doi to be updated . Upon publishing, final version will be available from Springer Nature Publishers (link to be updated). Pre-print of work submitted to 30th AICS conference.">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2212.01772">

<!--Generated on Fri Mar  1 12:45:53 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Brain tumor Deep learning Generative models Computer Vision MRI">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>National University of Computer and Emerging Sciences, Karachi, Pakistan.
</span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Department of Imaging Physics, MD ANDERSON Cancer Center, The University of Texas, Houston, Texas, USA.
</span></span></span><span id="id3" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>College of Science and Engineering, Hamad Bin Khalifa University, Qatar Foundation, Doha, Qatar.
<br class="ltx_break"><span id="id3.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">email: </span>K173810@nu.edu.pk, FRizwan@mdanderson.org, anaszafar98@gmail.com, danyalaftab97@gmail.com, JWu11@mdanderson.org, talam@hbku.edu.qa, zshah@hbku.edu.qa, haali2@hbku.edu.qa</span></span></span>
</span></span></span>
<h1 class="ltx_title ltx_title_document">Brain Tumor Synthetic Data Generation with Adaptive StyleGANs<span id="id1.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">thanks: </span>This preprint has not undergone peer review (when applicable) or any post-submission improvements or
corrections. The Version of Record of this contribution is published in [insert volume title],
and is available online at https://doi.org/doi to be updated
. Upon publishing, final version will be available from Springer Nature Publishers (link to be updated). Pre-print of work submitted to 30th AICS conference.</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Usama Tariq 
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rizwan Qureshi
</span><span class="ltx_author_notes">11 2 233</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anas Zafar
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Danyal Aftab
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jia Wu
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tanvir Alam
</span><span class="ltx_author_notes">33</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zubair Shah
</span><span class="ltx_author_notes">33</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hazrat Ali
</span><span class="ltx_author_notes">33</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Generative models have been very successful over the years and have received significant attention for synthetic data generation. As deep learning models are getting more and more complex, they require large amounts of data to perform accurately. In medical image analysis, such generative models play a crucial role as the available data is limited due to challenges related to data privacy, lack of data diversity, or uneven data distributions. In this paper, we present a method to generate brain tumor MRI images using generative adversarial networks. We have utilized StyleGAN2 with ADA methodology to generate high-quality brain MRI with tumors while using a significantly smaller amount of training data when compared to the existing approaches. We use three pre-trained models for transfer learning. Results demonstrate that the proposed method can learn the distributions of brain tumors. Furthermore, the model can generate high-quality synthetic brain MRI with a tumor that can limit the small sample size issues. The approach can addresses the limited data availability by generating realistic-looking brain MRI with tumors. The code is available at: Â <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/rizwanqureshi123/Brain-Tumor-Synthetic-Data</span>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Brain tumor Deep learning Generative models Computer Vision MRI
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Due to the advancements in computational power and a large amount of high-quality datasets, deep learning has become the state-of-the-art technology in computer vision, natural language processing, and othersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Deep learning has also made remarkable progress in all areas of medical image analysis, including segmentation, detection, and classificationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. However, deep learning models are trained on large datasets, which may not be available in the medical domain due to privacy and ethical concernsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Medical experts find it difficult to publicize the majority of medical images without patientsâ€™ consent. In addition, the public datasets are also small and lack expert annotations, thus, hindering their use for training deep neural networks. Furthermore, most of the available datasets might contain unbalanced classes that may hinder the performance of deep learning models and may not produce critical biological insights.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.4" class="ltx_p">To overcome the problem of data unavailability, many researchers use generative modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> to generate realistic synthetic images with diverse distributions for training complex deep learning models for medical analysis. Generative Adversarial Networks (GAN), a type of neural network, comprises two neural networks, one of which focuses on image production and the other on discrimination. The training of GAN involves a contest between the generator <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S1.p2.1.m1.1a"><mi id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><ci id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">G</annotation></semantics></math> and the discriminator <math id="S1.p2.2.m2.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S1.p2.2.m2.1a"><mi id="S1.p2.2.m2.1.1" xref="S1.p2.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S1.p2.2.m2.1b"><ci id="S1.p2.2.m2.1.1.cmml" xref="S1.p2.2.m2.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.2.m2.1c">D</annotation></semantics></math>. The discriminator <math id="S1.p2.3.m3.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S1.p2.3.m3.1a"><mi id="S1.p2.3.m3.1.1" xref="S1.p2.3.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S1.p2.3.m3.1b"><ci id="S1.p2.3.m3.1.1.cmml" xref="S1.p2.3.m3.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.3.m3.1c">D</annotation></semantics></math> is a binary classifier that determines if the data generated by <math id="S1.p2.4.m4.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S1.p2.4.m4.1a"><mi id="S1.p2.4.m4.1.1" xref="S1.p2.4.m4.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S1.p2.4.m4.1b"><ci id="S1.p2.4.m4.1.1.cmml" xref="S1.p2.4.m4.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.4.m4.1c">G</annotation></semantics></math> belongs to the training set or not (real versus unreal). GANs can be used to create synthetic medical images, image captioning, and cross-modality image generationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Due to the adversarial training schemeâ€™s success in creating new image samples and utility in preventing domain shift, GANs have drawn great interest from the research community. However, a GAN with insufficient training data leads to over-fitting the discriminator. The feedback to the generator becomes meaningless, and the training starts to divergeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. A common approach to overcome over-fitting is data augmentation. For instance, training an image classifier by including images with rotation, noise, or scaling may increase the classifierâ€™s invariance to certain semantics-preserving distortions, which is a very desired quality. On the other hand, a GAN trained with comparable dataset augmentations learns to produce the augmented distributionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Medical image analysis tasks such as brain tumor diagnosis Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> are critical where one would wish for minimum error from a computer model. Brain tumor refers to excessive growth of cells in regions of the brain. An early diagnosis of a brain tumor increases the effectiveness of the treatment and hence, the survival rates. Early diagnosis of a brain tumor is necessary in order to treat it properly; otherwise, it might cause severe damage to the brain that can eventually be fatal. Magnetic Resonance Imaging (MRI) is the most popular way to generate brain scans and detect tumors in different regions of the brain. Many deep learning modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> have been introduced recently to detect tumors in brain MRIs. However, progress is generally hindered by the lack of enough data. Traditional data augmentation methods, such as rotation, translation, mirror, and lightning, are not sufficient to generate a diverse, realistic dataset for brain tumor diagnosis. Synthetic images can be generated for this purpose which can address the problems associated with data acquisition, such as; privacy concerns, class imbalance, and small sample size.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.4" class="ltx_p">Generative Adversarial Networks (GANs) have been very popular for generating realistic diversified datasets. In 2018 StyleGANÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> was proposed, with the main aim to improve the existing generator architecture <math id="S1.p4.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S1.p4.1.m1.1a"><mi id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><ci id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">G</annotation></semantics></math>. StyleGAN mainly improved the existing architecture of the generator network in ProGANÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> for better performance and kept the discriminator <math id="S1.p4.2.m2.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S1.p4.2.m2.1a"><mi id="S1.p4.2.m2.1.1" xref="S1.p4.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S1.p4.2.m2.1b"><ci id="S1.p4.2.m2.1.1.cmml" xref="S1.p4.2.m2.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.2.m2.1c">D</annotation></semantics></math> network and loss functions constant. The latent code (<em id="S1.p4.4.1" class="ltx_emph ltx_font_italic">z</em>) is transformed into an intermediate latent code (<em id="S1.p4.4.2" class="ltx_emph ltx_font_italic">W</em>) prior to feeding it into the network. The synthesis network (<math id="S1.p4.3.m3.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S1.p4.3.m3.1a"><mi id="S1.p4.3.m3.1.1" xref="S1.p4.3.m3.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S1.p4.3.m3.1b"><ci id="S1.p4.3.m3.1.1.cmml" xref="S1.p4.3.m3.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.3.m3.1c">G</annotation></semantics></math>) is supervised by affine transforms through an adaptive instance that adds random noise maps to the space <math id="S1.p4.4.m4.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S1.p4.4.m4.1a"><mi id="S1.p4.4.m4.1.1" xref="S1.p4.4.m4.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S1.p4.4.m4.1b"><ci id="S1.p4.4.m4.1.1.cmml" xref="S1.p4.4.m4.1.1">ğ‘Š</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.4.m4.1c">W</annotation></semantics></math> resulting in much entangled latent space. The proposed model is capable of generating realistic, high-quality images and offers control over the new style of the generated image.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">StyleGAN2Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> architecture was presented to overcome issues present in the initial images generated by StyleGAN, such as blob and phase artifacts. Two causes were identified for the artifacts introduced in StyleGAN such as; fixed positions of eyes and nose and water droplet effects. Upon investigating the cause of common bob-like artifacts, it was observed that it was generated by the generator in response to an architectural design defect. A new design was proposed for the normalization used in the generator, which removed artifacts.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In this paper, we used StyleGAN2 with adaptive discriminator augmentation (ADA)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> for generating brain tumor MRI images of 512<math id="S1.p6.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.p6.1.m1.1a"><mo id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><times id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">\times</annotation></semantics></math>512 resolution while utilizing a significantly limited amount of training data when compared to the existing approaches. Our proposed approach effectively addresses the problem of data limitation by generating realistic brain MRI with tumor samples and can learn different data distributions from brain tumor raw images. The experiments are conducted on the brain tumor dataset.
We utilized pre-trained models trained on FFHQ datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, BreCaHaD datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, and AFHQ datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. The experimental results indicate that these models can generate superior quality superior MRI tumor samples that can be effectively utilized for medical analysis.
The remaining paper is organized as: Section <a href="#S2" title="2 Literature Review â€£ Brain Tumor Synthetic Data Generation with Adaptive StyleGANsThis preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this contribution is published in [insert volume title], and is available online at https://doi.org/doi to be updated . Upon publishing, final version will be available from Springer Nature Publishers (link to be updated). Pre-print of work submitted to 30th AICS conference." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides a review of the related literature. Section <a href="#S3" title="3 Methodology â€£ Brain Tumor Synthetic Data Generation with Adaptive StyleGANsThis preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this contribution is published in [insert volume title], and is available online at https://doi.org/doi to be updated . Upon publishing, final version will be available from Springer Nature Publishers (link to be updated). Pre-print of work submitted to 30th AICS conference." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> explains the methodology and the architecture. SectionÂ <a href="#S4" title="4 Experiments â€£ Brain Tumor Synthetic Data Generation with Adaptive StyleGANsThis preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this contribution is published in [insert volume title], and is available online at https://doi.org/doi to be updated . Upon publishing, final version will be available from Springer Nature Publishers (link to be updated). Pre-print of work submitted to 30th AICS conference." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents experiments, and SectionÂ <a href="#S4" title="4 Experiments â€£ Brain Tumor Synthetic Data Generation with Adaptive StyleGANsThis preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this contribution is published in [insert volume title], and is available online at https://doi.org/doi to be updated . Upon publishing, final version will be available from Springer Nature Publishers (link to be updated). Pre-print of work submitted to 30th AICS conference." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the results and discussion for the synthesis of brain MRI having tumors. Finally, SectionÂ <a href="#S6" title="6 Conclusion and Future Work â€£ Brain Tumor Synthetic Data Generation with Adaptive StyleGANsThis preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this contribution is published in [insert volume title], and is available online at https://doi.org/doi to be updated . Upon publishing, final version will be available from Springer Nature Publishers (link to be updated). Pre-print of work submitted to 30th AICS conference." class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> concludes the paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Literature Review</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The SytleGAN architecture generates style images while controlling different high-level attributes of the imagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. The generator architecture in this research was designed in such a way that helps to control the image synthesizing process by learning on a constant input of 4x4x512 and on each subsequent layer based on latent code for adjusting the style of the image. When noise is provided as an input to the network, this combined effect helps segregate high-level attributes from stochastic variation
in generated images and allows for better style mixing and interpolation. The datasets used in the work are FFHQÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, LSUNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and CelebA-HQÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. The concept of intermediate latent space was used, which significantly affected how variational factors are represented in the network and could be disentangled. Two metrics, i.e., the perceptual path length and linear separability, were used to estimate the degree of latent space disentanglement.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">StyleGAN2 was introduced inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> to address the characteristic artifacts and improve the output of StyleGAN. The first reason for the artifacts was the attempt of StyleGAN to evade a design flaw related to instance normalization used in AdaIN. The second type of artifact was related to progressive growth and was addressed in StyleGAN2 by changing the training method.
A method for mapping low-resolution medical images to high-resolution medical images using generative models is presented inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. InÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, considering the limitation of GANs to generate high-quality images for domains that have very little data, one of the very recent breakthroughs in generative modeling is a text-driven method that allows domain adaptation capability to the generator model for generating images across a multitude of domains. A text-driven method for out-of-domain image
synthesis is proposed. The domain shift was carried out by adjusting the generatorâ€™s weights in the direction of images aligned with the driving text. The network architecture is dependent on StyleGAN2 and Contrastive-Language-Image-Pre-training (CLIP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">CLIP model has been used for discovering semantically
rich and meaningful latent manipulations in order to generate images with styles defined through text based interface. In the first stage, an optimization task has been applied using CLIP-based loss to manipulate a latent input vector. In the next stage, a latent mapper for an optimized text-based manipulation given an input image has been used. Effectively, mapping the text-based inputs in the direction of StyleGAN style space results in effective text-based image manipulation. Motivated by the potential of the StayleGAN2 architecture to generate improved images of human faces, we use the StyleGAN2-ADA architecture to synthesize brain MRI, as explained in the following sections.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2212.01772/assets/img_ss.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="471" height="209" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.4.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.5.2" class="ltx_text" style="font-size:90%;">StyleGAN2-ADA (a) <span id="S2.F1.5.2.1" class="ltx_text ltx_font_bold">Generator.</span> Based on the incoming style, the modulation scales each input feature map of the convolution, and the demodulation module is used to remove the droplet artifacts.
(b) <span id="S2.F1.5.2.2" class="ltx_text ltx_font_bold">Discriminator.</span> After the input vectors of the components, StyleGAN2-ADA performs data augmentation.</span></figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We have utilized StyleGAN2 with ADA methodology to generate high-quality MRI brain tumor images while using a significantly limited amount of training data.
The proposed pipeline of StyleGANs with ADA is shown in FigureÂ <a href="#S2.F1" title="Figure 1 â€£ 2 Literature Review â€£ Brain Tumor Synthetic Data Generation with Adaptive StyleGANsThis preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this contribution is published in [insert volume title], and is available online at https://doi.org/doi to be updated . Upon publishing, final version will be available from Springer Nature Publishers (link to be updated). Pre-print of work submitted to 30th AICS conference." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. StyleGAN2Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> introduced several changes in the architecture to overcome the issues in StyleGAN. Many viewers observed distinctive artifacts in StyleGAN images. Two key issues were identified in the output of StyleGAN, and changes were introduced in the architecture and the training method accordingly. Upon investigating the cause of common blob-like artifacts, it was observed that the blobs were generated in response to an architectural design defect. A new design was proposed for the normalization used in the generator, which helped in removing the artifacts. It was concluded that the artifacts related to progressive growing have been quite effective at stabilizing high-resolution GAN training. Overall, the following major improvements were made in the <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">G</annotation></semantics></math> network considering issues in StyleGAN:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">StyleGAN used a constant input <em id="S3.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">c</em> as the model input directly, it was modified to input <em id="S3.I1.i1.p1.1.2" class="ltx_emph ltx_font_italic">C</em> by adding noise and bias.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Noise and bias were moved outside the style block.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Only the standard deviation value of every feature map was modified instead of modifying both the standard deviation and the mean values.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Demodulation module was introduced to overcome the droplet artifacts.</p>
</div>
</li>
</ul>
<p id="S3.p1.2" class="ltx_p"><span id="S3.p1.2.1" class="ltx_text ltx_font_bold">Weight Demodulation</span>: Similar to StyleGAN, StyleGAN2 makes use of a normalization technique to provide styles from the W vector using learning to transform <math id="S3.p1.2.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.p1.2.m1.1a"><mi id="S3.p1.2.m1.1.1" xref="S3.p1.2.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m1.1b"><ci id="S3.p1.2.m1.1.1.cmml" xref="S3.p1.2.m1.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m1.1c">A</annotation></semantics></math> into the source image. Here, the weight demodulation handles the droplet artifacts.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Lazy Regularization</span>:
StyleGAN2 computes regularization terms once after 16 mini-batches compared to StyleGAN, which
computes both the main loss function and regularization for every mini-batch with heavy memory consumption and high computation cost. This change in approach is to compute the cost function, which has no major effects in terms of
model efficiency but speeds up the training.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Path Length Regularization</span>:
Introducing path length regularizationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> allows the same displacement in the latent space that would
produce the same magnitude change in the image space regardless of the value of the latent factor.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">Removing Progressive Growing</span>:
Progressive growth in StyleGAN causes phase artifacts (location preference for facial features).
StyleGAN2 overcomes the issue by using a different network design
based on skip connections similar to that of ResNet architectures.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p"><span id="S3.p5.1.1" class="ltx_text ltx_font_bold">Adaptive Control Scheme</span>:
In order to have dynamic control over the augmentation strength parameter <math id="S3.p5.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.p5.1.m1.1a"><mi id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.1b"><ci id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.1c">p</annotation></semantics></math> to avoid over-fitting, an adaptive control scheme Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> has been used instead of manually tuning the augmentation strength. With the introduction of two heuristics to detect over-fitting in the discriminator, we are going to increase the magnitude of the augmentation to have dynamic scheduling.</p>
</div>
<div id="S3.p6" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.4" class="ltx_Math" alttext="r_{v}=E\left[D_{train}\right]-E\left[D_{validation}\right]/E\left[D_{train}\right]-E\left[D_{generated}\right]" display="block"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml"><msub id="S3.E1.m1.4.4.6" xref="S3.E1.m1.4.4.6.cmml"><mi id="S3.E1.m1.4.4.6.2" xref="S3.E1.m1.4.4.6.2.cmml">r</mi><mi id="S3.E1.m1.4.4.6.3" xref="S3.E1.m1.4.4.6.3.cmml">v</mi></msub><mo id="S3.E1.m1.4.4.5" xref="S3.E1.m1.4.4.5.cmml">=</mo><mrow id="S3.E1.m1.4.4.4" xref="S3.E1.m1.4.4.4.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.2.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.1.cmml">[</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml">D</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.3.1a" xref="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.4" xref="S3.E1.m1.1.1.1.1.1.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.3.1b" xref="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.5" xref="S3.E1.m1.1.1.1.1.1.1.1.3.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.3.1c" xref="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.6" xref="S3.E1.m1.1.1.1.1.1.1.1.3.6.cmml">n</mi></mrow></msub><mo id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S3.E1.m1.4.4.4.5" xref="S3.E1.m1.4.4.4.5.cmml">âˆ’</mo><mrow id="S3.E1.m1.3.3.3.3" xref="S3.E1.m1.3.3.3.3.cmml"><mrow id="S3.E1.m1.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.1.cmml"><mrow id="S3.E1.m1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.1.1.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.2.cmml">â€‹</mo><mrow id="S3.E1.m1.2.2.2.2.1.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.2.cmml"><mo id="S3.E1.m1.2.2.2.2.1.1.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.1.2.1.cmml">[</mo><msub id="S3.E1.m1.2.2.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.1.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.2.cmml">D</mi><mrow id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.2" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.1" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.3" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.1a" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.4" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.1b" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.5" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.1c" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.6" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.6.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.1d" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.7" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.1e" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.8" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.8.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.1f" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.9" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.9.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.1g" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.10" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.10.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.1h" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.11" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.11.cmml">n</mi></mrow></msub><mo id="S3.E1.m1.2.2.2.2.1.1.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S3.E1.m1.2.2.2.2.1.2" xref="S3.E1.m1.2.2.2.2.1.2.cmml">/</mo><mi id="S3.E1.m1.2.2.2.2.1.3" xref="S3.E1.m1.2.2.2.2.1.3.cmml">E</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.cmml">â€‹</mo><mrow id="S3.E1.m1.3.3.3.3.2.1" xref="S3.E1.m1.3.3.3.3.2.2.cmml"><mo id="S3.E1.m1.3.3.3.3.2.1.2" xref="S3.E1.m1.3.3.3.3.2.2.1.cmml">[</mo><msub id="S3.E1.m1.3.3.3.3.2.1.1" xref="S3.E1.m1.3.3.3.3.2.1.1.cmml"><mi id="S3.E1.m1.3.3.3.3.2.1.1.2" xref="S3.E1.m1.3.3.3.3.2.1.1.2.cmml">D</mi><mrow id="S3.E1.m1.3.3.3.3.2.1.1.3" xref="S3.E1.m1.3.3.3.3.2.1.1.3.cmml"><mi id="S3.E1.m1.3.3.3.3.2.1.1.3.2" xref="S3.E1.m1.3.3.3.3.2.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.3.2.1.1.3.1" xref="S3.E1.m1.3.3.3.3.2.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.3.3.3.3.2.1.1.3.3" xref="S3.E1.m1.3.3.3.3.2.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.3.2.1.1.3.1a" xref="S3.E1.m1.3.3.3.3.2.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.3.3.3.3.2.1.1.3.4" xref="S3.E1.m1.3.3.3.3.2.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.3.2.1.1.3.1b" xref="S3.E1.m1.3.3.3.3.2.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.3.3.3.3.2.1.1.3.5" xref="S3.E1.m1.3.3.3.3.2.1.1.3.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.3.2.1.1.3.1c" xref="S3.E1.m1.3.3.3.3.2.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.3.3.3.3.2.1.1.3.6" xref="S3.E1.m1.3.3.3.3.2.1.1.3.6.cmml">n</mi></mrow></msub><mo id="S3.E1.m1.3.3.3.3.2.1.3" xref="S3.E1.m1.3.3.3.3.2.2.1.cmml">]</mo></mrow></mrow><mo id="S3.E1.m1.4.4.4.5a" xref="S3.E1.m1.4.4.4.5.cmml">âˆ’</mo><mrow id="S3.E1.m1.4.4.4.4" xref="S3.E1.m1.4.4.4.4.cmml"><mi id="S3.E1.m1.4.4.4.4.3" xref="S3.E1.m1.4.4.4.4.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.4.4.2" xref="S3.E1.m1.4.4.4.4.2.cmml">â€‹</mo><mrow id="S3.E1.m1.4.4.4.4.1.1" xref="S3.E1.m1.4.4.4.4.1.2.cmml"><mo id="S3.E1.m1.4.4.4.4.1.1.2" xref="S3.E1.m1.4.4.4.4.1.2.1.cmml">[</mo><msub id="S3.E1.m1.4.4.4.4.1.1.1" xref="S3.E1.m1.4.4.4.4.1.1.1.cmml"><mi id="S3.E1.m1.4.4.4.4.1.1.1.2" xref="S3.E1.m1.4.4.4.4.1.1.1.2.cmml">D</mi><mrow id="S3.E1.m1.4.4.4.4.1.1.1.3" xref="S3.E1.m1.4.4.4.4.1.1.1.3.cmml"><mi id="S3.E1.m1.4.4.4.4.1.1.1.3.2" xref="S3.E1.m1.4.4.4.4.1.1.1.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.4.4.1.1.1.3.1" xref="S3.E1.m1.4.4.4.4.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.4.4.4.4.1.1.1.3.3" xref="S3.E1.m1.4.4.4.4.1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.4.4.1.1.1.3.1a" xref="S3.E1.m1.4.4.4.4.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.4.4.4.4.1.1.1.3.4" xref="S3.E1.m1.4.4.4.4.1.1.1.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.4.4.1.1.1.3.1b" xref="S3.E1.m1.4.4.4.4.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.4.4.4.4.1.1.1.3.5" xref="S3.E1.m1.4.4.4.4.1.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.4.4.1.1.1.3.1c" xref="S3.E1.m1.4.4.4.4.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.4.4.4.4.1.1.1.3.6" xref="S3.E1.m1.4.4.4.4.1.1.1.3.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.4.4.1.1.1.3.1d" xref="S3.E1.m1.4.4.4.4.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.4.4.4.4.1.1.1.3.7" xref="S3.E1.m1.4.4.4.4.1.1.1.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.4.4.1.1.1.3.1e" xref="S3.E1.m1.4.4.4.4.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.4.4.4.4.1.1.1.3.8" xref="S3.E1.m1.4.4.4.4.1.1.1.3.8.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.4.4.1.1.1.3.1f" xref="S3.E1.m1.4.4.4.4.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.4.4.4.4.1.1.1.3.9" xref="S3.E1.m1.4.4.4.4.1.1.1.3.9.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.4.4.1.1.1.3.1g" xref="S3.E1.m1.4.4.4.4.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.4.4.4.4.1.1.1.3.10" xref="S3.E1.m1.4.4.4.4.1.1.1.3.10.cmml">d</mi></mrow></msub><mo id="S3.E1.m1.4.4.4.4.1.1.3" xref="S3.E1.m1.4.4.4.4.1.2.1.cmml">]</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4"><eq id="S3.E1.m1.4.4.5.cmml" xref="S3.E1.m1.4.4.5"></eq><apply id="S3.E1.m1.4.4.6.cmml" xref="S3.E1.m1.4.4.6"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.6.1.cmml" xref="S3.E1.m1.4.4.6">subscript</csymbol><ci id="S3.E1.m1.4.4.6.2.cmml" xref="S3.E1.m1.4.4.6.2">ğ‘Ÿ</ci><ci id="S3.E1.m1.4.4.6.3.cmml" xref="S3.E1.m1.4.4.6.3">ğ‘£</ci></apply><apply id="S3.E1.m1.4.4.4.cmml" xref="S3.E1.m1.4.4.4"><minus id="S3.E1.m1.4.4.4.5.cmml" xref="S3.E1.m1.4.4.4.5"></minus><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3">ğ¸</ci><apply id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">ğ·</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3"><times id="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2">ğ‘¡</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3">ğ‘Ÿ</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.4">ğ‘</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.5.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.5">ğ‘–</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.6.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.6">ğ‘›</ci></apply></apply></apply></apply><apply id="S3.E1.m1.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3"><times id="S3.E1.m1.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3"></times><apply id="S3.E1.m1.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1"><divide id="S3.E1.m1.2.2.2.2.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.2"></divide><apply id="S3.E1.m1.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1"><times id="S3.E1.m1.2.2.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2"></times><ci id="S3.E1.m1.2.2.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3">ğ¸</ci><apply id="S3.E1.m1.2.2.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.2.2.2.2.1.1.1.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E1.m1.2.2.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.2">ğ·</ci><apply id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3"><times id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.1"></times><ci id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.2">ğ‘£</ci><ci id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.3">ğ‘</ci><ci id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.4.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.4">ğ‘™</ci><ci id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.5.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.5">ğ‘–</ci><ci id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.6.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.6">ğ‘‘</ci><ci id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.7.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.7">ğ‘</ci><ci id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.8.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.8">ğ‘¡</ci><ci id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.9.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.9">ğ‘–</ci><ci id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.10.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.10">ğ‘œ</ci><ci id="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.11.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.1.1.3.11">ğ‘›</ci></apply></apply></apply></apply><ci id="S3.E1.m1.2.2.2.2.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.3">ğ¸</ci></apply><apply id="S3.E1.m1.3.3.3.3.2.2.cmml" xref="S3.E1.m1.3.3.3.3.2.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.3.3.2.2.1.cmml" xref="S3.E1.m1.3.3.3.3.2.1.2">delimited-[]</csymbol><apply id="S3.E1.m1.3.3.3.3.2.1.1.cmml" xref="S3.E1.m1.3.3.3.3.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.3.2.1.1.1.cmml" xref="S3.E1.m1.3.3.3.3.2.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.3.3.2.1.1.2.cmml" xref="S3.E1.m1.3.3.3.3.2.1.1.2">ğ·</ci><apply id="S3.E1.m1.3.3.3.3.2.1.1.3.cmml" xref="S3.E1.m1.3.3.3.3.2.1.1.3"><times id="S3.E1.m1.3.3.3.3.2.1.1.3.1.cmml" xref="S3.E1.m1.3.3.3.3.2.1.1.3.1"></times><ci id="S3.E1.m1.3.3.3.3.2.1.1.3.2.cmml" xref="S3.E1.m1.3.3.3.3.2.1.1.3.2">ğ‘¡</ci><ci id="S3.E1.m1.3.3.3.3.2.1.1.3.3.cmml" xref="S3.E1.m1.3.3.3.3.2.1.1.3.3">ğ‘Ÿ</ci><ci id="S3.E1.m1.3.3.3.3.2.1.1.3.4.cmml" xref="S3.E1.m1.3.3.3.3.2.1.1.3.4">ğ‘</ci><ci id="S3.E1.m1.3.3.3.3.2.1.1.3.5.cmml" xref="S3.E1.m1.3.3.3.3.2.1.1.3.5">ğ‘–</ci><ci id="S3.E1.m1.3.3.3.3.2.1.1.3.6.cmml" xref="S3.E1.m1.3.3.3.3.2.1.1.3.6">ğ‘›</ci></apply></apply></apply></apply><apply id="S3.E1.m1.4.4.4.4.cmml" xref="S3.E1.m1.4.4.4.4"><times id="S3.E1.m1.4.4.4.4.2.cmml" xref="S3.E1.m1.4.4.4.4.2"></times><ci id="S3.E1.m1.4.4.4.4.3.cmml" xref="S3.E1.m1.4.4.4.4.3">ğ¸</ci><apply id="S3.E1.m1.4.4.4.4.1.2.cmml" xref="S3.E1.m1.4.4.4.4.1.1"><csymbol cd="latexml" id="S3.E1.m1.4.4.4.4.1.2.1.cmml" xref="S3.E1.m1.4.4.4.4.1.1.2">delimited-[]</csymbol><apply id="S3.E1.m1.4.4.4.4.1.1.1.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.4.4.1.1.1.1.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1">subscript</csymbol><ci id="S3.E1.m1.4.4.4.4.1.1.1.2.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.2">ğ·</ci><apply id="S3.E1.m1.4.4.4.4.1.1.1.3.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.3"><times id="S3.E1.m1.4.4.4.4.1.1.1.3.1.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.3.1"></times><ci id="S3.E1.m1.4.4.4.4.1.1.1.3.2.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.3.2">ğ‘”</ci><ci id="S3.E1.m1.4.4.4.4.1.1.1.3.3.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.3.3">ğ‘’</ci><ci id="S3.E1.m1.4.4.4.4.1.1.1.3.4.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.3.4">ğ‘›</ci><ci id="S3.E1.m1.4.4.4.4.1.1.1.3.5.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.3.5">ğ‘’</ci><ci id="S3.E1.m1.4.4.4.4.1.1.1.3.6.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.3.6">ğ‘Ÿ</ci><ci id="S3.E1.m1.4.4.4.4.1.1.1.3.7.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.3.7">ğ‘</ci><ci id="S3.E1.m1.4.4.4.4.1.1.1.3.8.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.3.8">ğ‘¡</ci><ci id="S3.E1.m1.4.4.4.4.1.1.1.3.9.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.3.9">ğ‘’</ci><ci id="S3.E1.m1.4.4.4.4.1.1.1.3.10.cmml" xref="S3.E1.m1.4.4.4.4.1.1.1.3.10">ğ‘‘</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">r_{v}=E\left[D_{train}\right]-E\left[D_{validation}\right]/E\left[D_{train}\right]-E\left[D_{generated}\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="r_{t}=E\left[sign(D_{train})\right]" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">r</mi><mi id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml">t</mi></msub><mo id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.2.cmml"><mo id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.1.cmml">[</mo><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2a" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.5" xref="S3.E2.m1.1.1.1.1.1.1.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2b" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.6" xref="S3.E2.m1.1.1.1.1.1.1.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2c" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml">D</mi><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1a" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1b" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.5" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1c" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.6" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.6.cmml">n</mi></mrow></msub><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"></eq><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">ğ‘Ÿ</ci><ci id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3">ğ‘¡</ci></apply><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><times id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3">ğ¸</ci><apply id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3">ğ‘ </ci><ci id="S3.E2.m1.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.4">ğ‘–</ci><ci id="S3.E2.m1.1.1.1.1.1.1.5.cmml" xref="S3.E2.m1.1.1.1.1.1.1.5">ğ‘”</ci><ci id="S3.E2.m1.1.1.1.1.1.1.6.cmml" xref="S3.E2.m1.1.1.1.1.1.1.6">ğ‘›</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2">ğ·</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2">ğ‘¡</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3">ğ‘Ÿ</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.4">ğ‘</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.5.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.5">ğ‘–</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.6.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.6">ğ‘›</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">r_{t}=E\left[sign(D_{train})\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.2" class="ltx_p">where <math id="S3.p7.1.m1.1" class="ltx_Math" alttext="r_{v}" display="inline"><semantics id="S3.p7.1.m1.1a"><msub id="S3.p7.1.m1.1.1" xref="S3.p7.1.m1.1.1.cmml"><mi id="S3.p7.1.m1.1.1.2" xref="S3.p7.1.m1.1.1.2.cmml">r</mi><mi id="S3.p7.1.m1.1.1.3" xref="S3.p7.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p7.1.m1.1b"><apply id="S3.p7.1.m1.1.1.cmml" xref="S3.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p7.1.m1.1.1.1.cmml" xref="S3.p7.1.m1.1.1">subscript</csymbol><ci id="S3.p7.1.m1.1.1.2.cmml" xref="S3.p7.1.m1.1.1.2">ğ‘Ÿ</ci><ci id="S3.p7.1.m1.1.1.3.cmml" xref="S3.p7.1.m1.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.1.m1.1c">r_{v}</annotation></semantics></math> is the first heuristic which refers to the validation set results relative to the training set and images generated given in Eq. (<a href="#S3.E1" title="In 3 Methodology â€£ Brain Tumor Synthetic Data Generation with Adaptive StyleGANsThis preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this contribution is published in [insert volume title], and is available online at https://doi.org/doi to be updated . Upon publishing, final version will be available from Springer Nature Publishers (link to be updated). Pre-print of work submitted to 30th AICS conference." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). <math id="S3.p7.2.m2.1" class="ltx_Math" alttext="r_{t}" display="inline"><semantics id="S3.p7.2.m2.1a"><msub id="S3.p7.2.m2.1.1" xref="S3.p7.2.m2.1.1.cmml"><mi id="S3.p7.2.m2.1.1.2" xref="S3.p7.2.m2.1.1.2.cmml">r</mi><mi id="S3.p7.2.m2.1.1.3" xref="S3.p7.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p7.2.m2.1b"><apply id="S3.p7.2.m2.1.1.cmml" xref="S3.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p7.2.m2.1.1.1.cmml" xref="S3.p7.2.m2.1.1">subscript</csymbol><ci id="S3.p7.2.m2.1.1.2.cmml" xref="S3.p7.2.m2.1.1.2">ğ‘Ÿ</ci><ci id="S3.p7.2.m2.1.1.3.cmml" xref="S3.p7.2.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.2.m2.1c">r_{t}</annotation></semantics></math> is the second heuristic that refers to the training set that generates positive discriminator outputs given in Eq. (<a href="#S3.E2" title="In 3 Methodology â€£ Brain Tumor Synthetic Data Generation with Adaptive StyleGANsThis preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this contribution is published in [insert volume title], and is available online at https://doi.org/doi to be updated . Upon publishing, final version will be available from Springer Nature Publishers (link to be updated). Pre-print of work submitted to 30th AICS conference." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Style transfer learning mechanism is used for model training. Transfer learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> is used to reduce the training data by using the weights of the model already trained on a datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We applied the models to the brain tumor datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, as available via Kaggle <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. This dataset includes 154 brain MRI samples and contains 3064 T1-weighted images with high contrast consisting of three kinds of brain tumors which are classified as Glioma, Meningioma, and Pituitary Tumor, as shown in FigureÂ <a href="#S4.F2" title="Figure 2 â€£ 4.1 Datasets â€£ 4 Experiments â€£ Brain Tumor Synthetic Data Generation with Adaptive StyleGANsThis preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this contribution is published in [insert volume title], and is available online at https://doi.org/doi to be updated . Upon publishing, final version will be available from Springer Nature Publishers (link to be updated). Pre-print of work submitted to 30th AICS conference." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2212.01772/assets/x1.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="197" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.3.2" class="ltx_text" style="font-size:90%;">Brain tumor dataset sample images. Each row represents one type of tumor.</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.2" class="ltx_p">We resize all training images to 512<math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mo id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><times id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\times</annotation></semantics></math>512 resolution.
We used <em id="S4.SS2.p1.2.1" class="ltx_emph ltx_font_italic">Google Colab Pro</em> platform for the training model as it allows access to faster GPUs which helps in speeding up the training. The model was trained on a Tesla P100 GPU with 25 GB RAM. For monitoring and managing GPU resources, NVIDIA System Management Interface (nvidia-smi) driver version <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="460.32.03" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mn id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">460.32.03</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><cn type="float" id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">460.32.03</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">460.32.03</annotation></semantics></math> and Cuda version 11.2 has been used for the management and monitoring of NVIDIA GPU devices. We converted all images into TFR records, enabling StyleGAN2 ADA to read data and improving the import pipelineâ€™s performance.
We utilized pre-trained models trained on FFHQ datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, BreCaHaD datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, AFHQ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Pre-trained Models</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">FFHQ512Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> pre-trained model is trained on Flickr-Faces high-quality images (FFHQ) dataset. The FFHQ is an image dataset containing high-quality images of human faces.
It offers 70,000 PNG images at 512<math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mo id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><times id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\times</annotation></semantics></math>512 resolution that display diverse ages, ethnicity, image backgrounds, and accessories like hats and eyeglasses.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">BreCaHaDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> pre-trained model is trained on a dataset consisting of 162 breast cancer histopathology images that are distributed into 1944 partially overlapping crops of 512<math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mo id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><times id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">\times</annotation></semantics></math>512. The dataset is widely used by the biomedical and computer vision research community to evaluate and develop novel methods for tumor detection and diagnosis of cancerous regions in breast cancer histopathology images.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Animal FacesHQÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> (AFHQ) pre-trained model is trained on a dataset of 15,000 high-quality animal face images at 512<math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mo id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><times id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">\times</annotation></semantics></math>512 resolution in three domains of cat, dog, and wildlife, with 5000 images per domain.
AFHQ sets a more challenging image-to-image translation problem by having three domains and diverse images of various breeds. The images are vertically and horizontally aligned. The low-quality images were manually discarded. We used weights from the AFHQ (Cat) and AFHQ (Wild) pre-trained models.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Evaluation Metrics</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.4" class="ltx_p">FrÃ©chet Inception Distance (FID)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> is a metric for quantifying the distance between two distributions of images <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="P_{r}" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><msub id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mi id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">P</mi><mi id="S4.SS4.p1.1.m1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.2">ğ‘ƒ</ci><ci id="S4.SS4.p1.1.m1.1.1.3.cmml" xref="S4.SS4.p1.1.m1.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">P_{r}</annotation></semantics></math> and <math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="P_{g}" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><msub id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml"><mi id="S4.SS4.p1.2.m2.1.1.2" xref="S4.SS4.p1.2.m2.1.1.2.cmml">P</mi><mi id="S4.SS4.p1.2.m2.1.1.3" xref="S4.SS4.p1.2.m2.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><apply id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.2.m2.1.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.p1.2.m2.1.1.2.cmml" xref="S4.SS4.p1.2.m2.1.1.2">ğ‘ƒ</ci><ci id="S4.SS4.p1.2.m2.1.1.3.cmml" xref="S4.SS4.p1.2.m2.1.1.3">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">P_{g}</annotation></semantics></math> where <math id="S4.SS4.p1.3.m3.1" class="ltx_Math" alttext="P_{r}" display="inline"><semantics id="S4.SS4.p1.3.m3.1a"><msub id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml"><mi id="S4.SS4.p1.3.m3.1.1.2" xref="S4.SS4.p1.3.m3.1.1.2.cmml">P</mi><mi id="S4.SS4.p1.3.m3.1.1.3" xref="S4.SS4.p1.3.m3.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><apply id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.3.m3.1.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS4.p1.3.m3.1.1.2.cmml" xref="S4.SS4.p1.3.m3.1.1.2">ğ‘ƒ</ci><ci id="S4.SS4.p1.3.m3.1.1.3.cmml" xref="S4.SS4.p1.3.m3.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">P_{r}</annotation></semantics></math> is the probability distribution of real images, and <math id="S4.SS4.p1.4.m4.1" class="ltx_Math" alttext="P_{g}" display="inline"><semantics id="S4.SS4.p1.4.m4.1a"><msub id="S4.SS4.p1.4.m4.1.1" xref="S4.SS4.p1.4.m4.1.1.cmml"><mi id="S4.SS4.p1.4.m4.1.1.2" xref="S4.SS4.p1.4.m4.1.1.2.cmml">P</mi><mi id="S4.SS4.p1.4.m4.1.1.3" xref="S4.SS4.p1.4.m4.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><apply id="S4.SS4.p1.4.m4.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.4.m4.1.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS4.p1.4.m4.1.1.2.cmml" xref="S4.SS4.p1.4.m4.1.1.2">ğ‘ƒ</ci><ci id="S4.SS4.p1.4.m4.1.1.3.cmml" xref="S4.SS4.p1.4.m4.1.1.3">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">P_{g}</annotation></semantics></math> is the probability distribution of generated images. It is used to evaluate the quality of generated images and the performance of GANs. FID is defined as:</p>
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.8" class="ltx_Math" alttext="\operatorname{FID}\left(\mu_{r},\Sigma_{r},\mu_{g},\Sigma_{g}\right)=\left\|\mu_{r}-\mu_{g}\right\|_{2}^{2}+\operatorname{Tr}\left(\Sigma_{r}+\Sigma_{g}-2\left(\Sigma_{r}\Sigma_{g}\right)^{1/2}\right)" display="block"><semantics id="S4.E3.m1.8a"><mrow id="S4.E3.m1.8.8" xref="S4.E3.m1.8.8.cmml"><mrow id="S4.E3.m1.6.6.4.4" xref="S4.E3.m1.6.6.4.5.cmml"><mi id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml">FID</mi><mo id="S4.E3.m1.6.6.4.4a" xref="S4.E3.m1.6.6.4.5.cmml">â¡</mo><mrow id="S4.E3.m1.6.6.4.4.4" xref="S4.E3.m1.6.6.4.5.cmml"><mo id="S4.E3.m1.6.6.4.4.4.5" xref="S4.E3.m1.6.6.4.5.cmml">(</mo><msub id="S4.E3.m1.3.3.1.1.1.1" xref="S4.E3.m1.3.3.1.1.1.1.cmml"><mi id="S4.E3.m1.3.3.1.1.1.1.2" xref="S4.E3.m1.3.3.1.1.1.1.2.cmml">Î¼</mi><mi id="S4.E3.m1.3.3.1.1.1.1.3" xref="S4.E3.m1.3.3.1.1.1.1.3.cmml">r</mi></msub><mo id="S4.E3.m1.6.6.4.4.4.6" xref="S4.E3.m1.6.6.4.5.cmml">,</mo><msub id="S4.E3.m1.4.4.2.2.2.2" xref="S4.E3.m1.4.4.2.2.2.2.cmml"><mi mathvariant="normal" id="S4.E3.m1.4.4.2.2.2.2.2" xref="S4.E3.m1.4.4.2.2.2.2.2.cmml">Î£</mi><mi id="S4.E3.m1.4.4.2.2.2.2.3" xref="S4.E3.m1.4.4.2.2.2.2.3.cmml">r</mi></msub><mo id="S4.E3.m1.6.6.4.4.4.7" xref="S4.E3.m1.6.6.4.5.cmml">,</mo><msub id="S4.E3.m1.5.5.3.3.3.3" xref="S4.E3.m1.5.5.3.3.3.3.cmml"><mi id="S4.E3.m1.5.5.3.3.3.3.2" xref="S4.E3.m1.5.5.3.3.3.3.2.cmml">Î¼</mi><mi id="S4.E3.m1.5.5.3.3.3.3.3" xref="S4.E3.m1.5.5.3.3.3.3.3.cmml">g</mi></msub><mo id="S4.E3.m1.6.6.4.4.4.8" xref="S4.E3.m1.6.6.4.5.cmml">,</mo><msub id="S4.E3.m1.6.6.4.4.4.4" xref="S4.E3.m1.6.6.4.4.4.4.cmml"><mi mathvariant="normal" id="S4.E3.m1.6.6.4.4.4.4.2" xref="S4.E3.m1.6.6.4.4.4.4.2.cmml">Î£</mi><mi id="S4.E3.m1.6.6.4.4.4.4.3" xref="S4.E3.m1.6.6.4.4.4.4.3.cmml">g</mi></msub><mo id="S4.E3.m1.6.6.4.4.4.9" xref="S4.E3.m1.6.6.4.5.cmml">)</mo></mrow></mrow><mo id="S4.E3.m1.8.8.7" xref="S4.E3.m1.8.8.7.cmml">=</mo><mrow id="S4.E3.m1.8.8.6" xref="S4.E3.m1.8.8.6.cmml"><msubsup id="S4.E3.m1.7.7.5.1" xref="S4.E3.m1.7.7.5.1.cmml"><mrow id="S4.E3.m1.7.7.5.1.1.1.1" xref="S4.E3.m1.7.7.5.1.1.1.2.cmml"><mo id="S4.E3.m1.7.7.5.1.1.1.1.2" xref="S4.E3.m1.7.7.5.1.1.1.2.1.cmml">â€–</mo><mrow id="S4.E3.m1.7.7.5.1.1.1.1.1" xref="S4.E3.m1.7.7.5.1.1.1.1.1.cmml"><msub id="S4.E3.m1.7.7.5.1.1.1.1.1.2" xref="S4.E3.m1.7.7.5.1.1.1.1.1.2.cmml"><mi id="S4.E3.m1.7.7.5.1.1.1.1.1.2.2" xref="S4.E3.m1.7.7.5.1.1.1.1.1.2.2.cmml">Î¼</mi><mi id="S4.E3.m1.7.7.5.1.1.1.1.1.2.3" xref="S4.E3.m1.7.7.5.1.1.1.1.1.2.3.cmml">r</mi></msub><mo id="S4.E3.m1.7.7.5.1.1.1.1.1.1" xref="S4.E3.m1.7.7.5.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S4.E3.m1.7.7.5.1.1.1.1.1.3" xref="S4.E3.m1.7.7.5.1.1.1.1.1.3.cmml"><mi id="S4.E3.m1.7.7.5.1.1.1.1.1.3.2" xref="S4.E3.m1.7.7.5.1.1.1.1.1.3.2.cmml">Î¼</mi><mi id="S4.E3.m1.7.7.5.1.1.1.1.1.3.3" xref="S4.E3.m1.7.7.5.1.1.1.1.1.3.3.cmml">g</mi></msub></mrow><mo id="S4.E3.m1.7.7.5.1.1.1.1.3" xref="S4.E3.m1.7.7.5.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="S4.E3.m1.7.7.5.1.1.3" xref="S4.E3.m1.7.7.5.1.1.3.cmml">2</mn><mn id="S4.E3.m1.7.7.5.1.3" xref="S4.E3.m1.7.7.5.1.3.cmml">2</mn></msubsup><mo id="S4.E3.m1.8.8.6.3" xref="S4.E3.m1.8.8.6.3.cmml">+</mo><mrow id="S4.E3.m1.8.8.6.2.1" xref="S4.E3.m1.8.8.6.2.2.cmml"><mi id="S4.E3.m1.2.2" xref="S4.E3.m1.2.2.cmml">Tr</mi><mo id="S4.E3.m1.8.8.6.2.1a" xref="S4.E3.m1.8.8.6.2.2.cmml">â¡</mo><mrow id="S4.E3.m1.8.8.6.2.1.1" xref="S4.E3.m1.8.8.6.2.2.cmml"><mo id="S4.E3.m1.8.8.6.2.1.1.2" xref="S4.E3.m1.8.8.6.2.2.cmml">(</mo><mrow id="S4.E3.m1.8.8.6.2.1.1.1" xref="S4.E3.m1.8.8.6.2.1.1.1.cmml"><mrow id="S4.E3.m1.8.8.6.2.1.1.1.3" xref="S4.E3.m1.8.8.6.2.1.1.1.3.cmml"><msub id="S4.E3.m1.8.8.6.2.1.1.1.3.2" xref="S4.E3.m1.8.8.6.2.1.1.1.3.2.cmml"><mi mathvariant="normal" id="S4.E3.m1.8.8.6.2.1.1.1.3.2.2" xref="S4.E3.m1.8.8.6.2.1.1.1.3.2.2.cmml">Î£</mi><mi id="S4.E3.m1.8.8.6.2.1.1.1.3.2.3" xref="S4.E3.m1.8.8.6.2.1.1.1.3.2.3.cmml">r</mi></msub><mo id="S4.E3.m1.8.8.6.2.1.1.1.3.1" xref="S4.E3.m1.8.8.6.2.1.1.1.3.1.cmml">+</mo><msub id="S4.E3.m1.8.8.6.2.1.1.1.3.3" xref="S4.E3.m1.8.8.6.2.1.1.1.3.3.cmml"><mi mathvariant="normal" id="S4.E3.m1.8.8.6.2.1.1.1.3.3.2" xref="S4.E3.m1.8.8.6.2.1.1.1.3.3.2.cmml">Î£</mi><mi id="S4.E3.m1.8.8.6.2.1.1.1.3.3.3" xref="S4.E3.m1.8.8.6.2.1.1.1.3.3.3.cmml">g</mi></msub></mrow><mo id="S4.E3.m1.8.8.6.2.1.1.1.2" xref="S4.E3.m1.8.8.6.2.1.1.1.2.cmml">âˆ’</mo><mrow id="S4.E3.m1.8.8.6.2.1.1.1.1" xref="S4.E3.m1.8.8.6.2.1.1.1.1.cmml"><mn id="S4.E3.m1.8.8.6.2.1.1.1.1.3" xref="S4.E3.m1.8.8.6.2.1.1.1.1.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.E3.m1.8.8.6.2.1.1.1.1.2" xref="S4.E3.m1.8.8.6.2.1.1.1.1.2.cmml">â€‹</mo><msup id="S4.E3.m1.8.8.6.2.1.1.1.1.1" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.cmml"><mrow id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.2" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.cmml"><msub id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.2" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.2.cmml"><mi mathvariant="normal" id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.2.2" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.2.2.cmml">Î£</mi><mi id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.2.3" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.2.3.cmml">r</mi></msub><mo lspace="0em" rspace="0em" id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.1" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.1.cmml">â€‹</mo><msub id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.3" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.3.cmml"><mi mathvariant="normal" id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.3.2" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.3.2.cmml">Î£</mi><mi id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.3.3" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.3.3.cmml">g</mi></msub></mrow><mo id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.3" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S4.E3.m1.8.8.6.2.1.1.1.1.1.3" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.3.cmml"><mn id="S4.E3.m1.8.8.6.2.1.1.1.1.1.3.2" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.3.2.cmml">1</mn><mo id="S4.E3.m1.8.8.6.2.1.1.1.1.1.3.1" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.3.1.cmml">/</mo><mn id="S4.E3.m1.8.8.6.2.1.1.1.1.1.3.3" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.3.3.cmml">2</mn></mrow></msup></mrow></mrow><mo id="S4.E3.m1.8.8.6.2.1.1.3" xref="S4.E3.m1.8.8.6.2.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.8b"><apply id="S4.E3.m1.8.8.cmml" xref="S4.E3.m1.8.8"><eq id="S4.E3.m1.8.8.7.cmml" xref="S4.E3.m1.8.8.7"></eq><apply id="S4.E3.m1.6.6.4.5.cmml" xref="S4.E3.m1.6.6.4.4"><ci id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1">FID</ci><apply id="S4.E3.m1.3.3.1.1.1.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.3.3.1.1.1.1.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1">subscript</csymbol><ci id="S4.E3.m1.3.3.1.1.1.1.2.cmml" xref="S4.E3.m1.3.3.1.1.1.1.2">ğœ‡</ci><ci id="S4.E3.m1.3.3.1.1.1.1.3.cmml" xref="S4.E3.m1.3.3.1.1.1.1.3">ğ‘Ÿ</ci></apply><apply id="S4.E3.m1.4.4.2.2.2.2.cmml" xref="S4.E3.m1.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S4.E3.m1.4.4.2.2.2.2.1.cmml" xref="S4.E3.m1.4.4.2.2.2.2">subscript</csymbol><ci id="S4.E3.m1.4.4.2.2.2.2.2.cmml" xref="S4.E3.m1.4.4.2.2.2.2.2">Î£</ci><ci id="S4.E3.m1.4.4.2.2.2.2.3.cmml" xref="S4.E3.m1.4.4.2.2.2.2.3">ğ‘Ÿ</ci></apply><apply id="S4.E3.m1.5.5.3.3.3.3.cmml" xref="S4.E3.m1.5.5.3.3.3.3"><csymbol cd="ambiguous" id="S4.E3.m1.5.5.3.3.3.3.1.cmml" xref="S4.E3.m1.5.5.3.3.3.3">subscript</csymbol><ci id="S4.E3.m1.5.5.3.3.3.3.2.cmml" xref="S4.E3.m1.5.5.3.3.3.3.2">ğœ‡</ci><ci id="S4.E3.m1.5.5.3.3.3.3.3.cmml" xref="S4.E3.m1.5.5.3.3.3.3.3">ğ‘”</ci></apply><apply id="S4.E3.m1.6.6.4.4.4.4.cmml" xref="S4.E3.m1.6.6.4.4.4.4"><csymbol cd="ambiguous" id="S4.E3.m1.6.6.4.4.4.4.1.cmml" xref="S4.E3.m1.6.6.4.4.4.4">subscript</csymbol><ci id="S4.E3.m1.6.6.4.4.4.4.2.cmml" xref="S4.E3.m1.6.6.4.4.4.4.2">Î£</ci><ci id="S4.E3.m1.6.6.4.4.4.4.3.cmml" xref="S4.E3.m1.6.6.4.4.4.4.3">ğ‘”</ci></apply></apply><apply id="S4.E3.m1.8.8.6.cmml" xref="S4.E3.m1.8.8.6"><plus id="S4.E3.m1.8.8.6.3.cmml" xref="S4.E3.m1.8.8.6.3"></plus><apply id="S4.E3.m1.7.7.5.1.cmml" xref="S4.E3.m1.7.7.5.1"><csymbol cd="ambiguous" id="S4.E3.m1.7.7.5.1.2.cmml" xref="S4.E3.m1.7.7.5.1">superscript</csymbol><apply id="S4.E3.m1.7.7.5.1.1.cmml" xref="S4.E3.m1.7.7.5.1"><csymbol cd="ambiguous" id="S4.E3.m1.7.7.5.1.1.2.cmml" xref="S4.E3.m1.7.7.5.1">subscript</csymbol><apply id="S4.E3.m1.7.7.5.1.1.1.2.cmml" xref="S4.E3.m1.7.7.5.1.1.1.1"><csymbol cd="latexml" id="S4.E3.m1.7.7.5.1.1.1.2.1.cmml" xref="S4.E3.m1.7.7.5.1.1.1.1.2">norm</csymbol><apply id="S4.E3.m1.7.7.5.1.1.1.1.1.cmml" xref="S4.E3.m1.7.7.5.1.1.1.1.1"><minus id="S4.E3.m1.7.7.5.1.1.1.1.1.1.cmml" xref="S4.E3.m1.7.7.5.1.1.1.1.1.1"></minus><apply id="S4.E3.m1.7.7.5.1.1.1.1.1.2.cmml" xref="S4.E3.m1.7.7.5.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.7.7.5.1.1.1.1.1.2.1.cmml" xref="S4.E3.m1.7.7.5.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E3.m1.7.7.5.1.1.1.1.1.2.2.cmml" xref="S4.E3.m1.7.7.5.1.1.1.1.1.2.2">ğœ‡</ci><ci id="S4.E3.m1.7.7.5.1.1.1.1.1.2.3.cmml" xref="S4.E3.m1.7.7.5.1.1.1.1.1.2.3">ğ‘Ÿ</ci></apply><apply id="S4.E3.m1.7.7.5.1.1.1.1.1.3.cmml" xref="S4.E3.m1.7.7.5.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.7.7.5.1.1.1.1.1.3.1.cmml" xref="S4.E3.m1.7.7.5.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E3.m1.7.7.5.1.1.1.1.1.3.2.cmml" xref="S4.E3.m1.7.7.5.1.1.1.1.1.3.2">ğœ‡</ci><ci id="S4.E3.m1.7.7.5.1.1.1.1.1.3.3.cmml" xref="S4.E3.m1.7.7.5.1.1.1.1.1.3.3">ğ‘”</ci></apply></apply></apply><cn type="integer" id="S4.E3.m1.7.7.5.1.1.3.cmml" xref="S4.E3.m1.7.7.5.1.1.3">2</cn></apply><cn type="integer" id="S4.E3.m1.7.7.5.1.3.cmml" xref="S4.E3.m1.7.7.5.1.3">2</cn></apply><apply id="S4.E3.m1.8.8.6.2.2.cmml" xref="S4.E3.m1.8.8.6.2.1"><ci id="S4.E3.m1.2.2.cmml" xref="S4.E3.m1.2.2">Tr</ci><apply id="S4.E3.m1.8.8.6.2.1.1.1.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1"><minus id="S4.E3.m1.8.8.6.2.1.1.1.2.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.2"></minus><apply id="S4.E3.m1.8.8.6.2.1.1.1.3.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.3"><plus id="S4.E3.m1.8.8.6.2.1.1.1.3.1.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.3.1"></plus><apply id="S4.E3.m1.8.8.6.2.1.1.1.3.2.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E3.m1.8.8.6.2.1.1.1.3.2.1.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.3.2">subscript</csymbol><ci id="S4.E3.m1.8.8.6.2.1.1.1.3.2.2.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.3.2.2">Î£</ci><ci id="S4.E3.m1.8.8.6.2.1.1.1.3.2.3.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.3.2.3">ğ‘Ÿ</ci></apply><apply id="S4.E3.m1.8.8.6.2.1.1.1.3.3.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E3.m1.8.8.6.2.1.1.1.3.3.1.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.3.3">subscript</csymbol><ci id="S4.E3.m1.8.8.6.2.1.1.1.3.3.2.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.3.3.2">Î£</ci><ci id="S4.E3.m1.8.8.6.2.1.1.1.3.3.3.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.3.3.3">ğ‘”</ci></apply></apply><apply id="S4.E3.m1.8.8.6.2.1.1.1.1.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.1"><times id="S4.E3.m1.8.8.6.2.1.1.1.1.2.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.1.2"></times><cn type="integer" id="S4.E3.m1.8.8.6.2.1.1.1.1.3.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.1.3">2</cn><apply id="S4.E3.m1.8.8.6.2.1.1.1.1.1.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.8.8.6.2.1.1.1.1.1.2.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1">superscript</csymbol><apply id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1"><times id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.1"></times><apply id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.2.2">Î£</ci><ci id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.2.3">ğ‘Ÿ</ci></apply><apply id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.3.2">Î£</ci><ci id="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.1.1.1.3.3">ğ‘”</ci></apply></apply><apply id="S4.E3.m1.8.8.6.2.1.1.1.1.1.3.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.3"><divide id="S4.E3.m1.8.8.6.2.1.1.1.1.1.3.1.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.3.1"></divide><cn type="integer" id="S4.E3.m1.8.8.6.2.1.1.1.1.1.3.2.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.3.2">1</cn><cn type="integer" id="S4.E3.m1.8.8.6.2.1.1.1.1.1.3.3.cmml" xref="S4.E3.m1.8.8.6.2.1.1.1.1.1.3.3">2</cn></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.8c">\operatorname{FID}\left(\mu_{r},\Sigma_{r},\mu_{g},\Sigma_{g}\right)=\left\|\mu_{r}-\mu_{g}\right\|_{2}^{2}+\operatorname{Tr}\left(\Sigma_{r}+\Sigma_{g}-2\left(\Sigma_{r}\Sigma_{g}\right)^{1/2}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S4.SS4.p1.9" class="ltx_p">where (<math id="S4.SS4.p1.5.m1.1" class="ltx_Math" alttext="\mu_{r}" display="inline"><semantics id="S4.SS4.p1.5.m1.1a"><msub id="S4.SS4.p1.5.m1.1.1" xref="S4.SS4.p1.5.m1.1.1.cmml"><mi id="S4.SS4.p1.5.m1.1.1.2" xref="S4.SS4.p1.5.m1.1.1.2.cmml">Î¼</mi><mi id="S4.SS4.p1.5.m1.1.1.3" xref="S4.SS4.p1.5.m1.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.5.m1.1b"><apply id="S4.SS4.p1.5.m1.1.1.cmml" xref="S4.SS4.p1.5.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.5.m1.1.1.1.cmml" xref="S4.SS4.p1.5.m1.1.1">subscript</csymbol><ci id="S4.SS4.p1.5.m1.1.1.2.cmml" xref="S4.SS4.p1.5.m1.1.1.2">ğœ‡</ci><ci id="S4.SS4.p1.5.m1.1.1.3.cmml" xref="S4.SS4.p1.5.m1.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.5.m1.1c">\mu_{r}</annotation></semantics></math>, <math id="S4.SS4.p1.6.m2.1" class="ltx_Math" alttext="\Sigma_{r}" display="inline"><semantics id="S4.SS4.p1.6.m2.1a"><msub id="S4.SS4.p1.6.m2.1.1" xref="S4.SS4.p1.6.m2.1.1.cmml"><mi mathvariant="normal" id="S4.SS4.p1.6.m2.1.1.2" xref="S4.SS4.p1.6.m2.1.1.2.cmml">Î£</mi><mi id="S4.SS4.p1.6.m2.1.1.3" xref="S4.SS4.p1.6.m2.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.6.m2.1b"><apply id="S4.SS4.p1.6.m2.1.1.cmml" xref="S4.SS4.p1.6.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.6.m2.1.1.1.cmml" xref="S4.SS4.p1.6.m2.1.1">subscript</csymbol><ci id="S4.SS4.p1.6.m2.1.1.2.cmml" xref="S4.SS4.p1.6.m2.1.1.2">Î£</ci><ci id="S4.SS4.p1.6.m2.1.1.3.cmml" xref="S4.SS4.p1.6.m2.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.6.m2.1c">\Sigma_{r}</annotation></semantics></math>) and (<math id="S4.SS4.p1.7.m3.2" class="ltx_Math" alttext="\mu_{g},\Sigma_{g}" display="inline"><semantics id="S4.SS4.p1.7.m3.2a"><mrow id="S4.SS4.p1.7.m3.2.2.2" xref="S4.SS4.p1.7.m3.2.2.3.cmml"><msub id="S4.SS4.p1.7.m3.1.1.1.1" xref="S4.SS4.p1.7.m3.1.1.1.1.cmml"><mi id="S4.SS4.p1.7.m3.1.1.1.1.2" xref="S4.SS4.p1.7.m3.1.1.1.1.2.cmml">Î¼</mi><mi id="S4.SS4.p1.7.m3.1.1.1.1.3" xref="S4.SS4.p1.7.m3.1.1.1.1.3.cmml">g</mi></msub><mo id="S4.SS4.p1.7.m3.2.2.2.3" xref="S4.SS4.p1.7.m3.2.2.3.cmml">,</mo><msub id="S4.SS4.p1.7.m3.2.2.2.2" xref="S4.SS4.p1.7.m3.2.2.2.2.cmml"><mi mathvariant="normal" id="S4.SS4.p1.7.m3.2.2.2.2.2" xref="S4.SS4.p1.7.m3.2.2.2.2.2.cmml">Î£</mi><mi id="S4.SS4.p1.7.m3.2.2.2.2.3" xref="S4.SS4.p1.7.m3.2.2.2.2.3.cmml">g</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.7.m3.2b"><list id="S4.SS4.p1.7.m3.2.2.3.cmml" xref="S4.SS4.p1.7.m3.2.2.2"><apply id="S4.SS4.p1.7.m3.1.1.1.1.cmml" xref="S4.SS4.p1.7.m3.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.7.m3.1.1.1.1.1.cmml" xref="S4.SS4.p1.7.m3.1.1.1.1">subscript</csymbol><ci id="S4.SS4.p1.7.m3.1.1.1.1.2.cmml" xref="S4.SS4.p1.7.m3.1.1.1.1.2">ğœ‡</ci><ci id="S4.SS4.p1.7.m3.1.1.1.1.3.cmml" xref="S4.SS4.p1.7.m3.1.1.1.1.3">ğ‘”</ci></apply><apply id="S4.SS4.p1.7.m3.2.2.2.2.cmml" xref="S4.SS4.p1.7.m3.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS4.p1.7.m3.2.2.2.2.1.cmml" xref="S4.SS4.p1.7.m3.2.2.2.2">subscript</csymbol><ci id="S4.SS4.p1.7.m3.2.2.2.2.2.cmml" xref="S4.SS4.p1.7.m3.2.2.2.2.2">Î£</ci><ci id="S4.SS4.p1.7.m3.2.2.2.2.3.cmml" xref="S4.SS4.p1.7.m3.2.2.2.2.3">ğ‘”</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.7.m3.2c">\mu_{g},\Sigma_{g}</annotation></semantics></math>) denote the mean vectors and covariance matrices of the Gaussian approximations for real and generated samples, respectively. The lower the FID value, the better the generated image quality.
Kernel Inception Distance (KID)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> is a metric which measures the dissimilarity between two probability distributions <math id="S4.SS4.p1.8.m4.1" class="ltx_Math" alttext="P_{r}" display="inline"><semantics id="S4.SS4.p1.8.m4.1a"><msub id="S4.SS4.p1.8.m4.1.1" xref="S4.SS4.p1.8.m4.1.1.cmml"><mi id="S4.SS4.p1.8.m4.1.1.2" xref="S4.SS4.p1.8.m4.1.1.2.cmml">P</mi><mi id="S4.SS4.p1.8.m4.1.1.3" xref="S4.SS4.p1.8.m4.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.8.m4.1b"><apply id="S4.SS4.p1.8.m4.1.1.cmml" xref="S4.SS4.p1.8.m4.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.8.m4.1.1.1.cmml" xref="S4.SS4.p1.8.m4.1.1">subscript</csymbol><ci id="S4.SS4.p1.8.m4.1.1.2.cmml" xref="S4.SS4.p1.8.m4.1.1.2">ğ‘ƒ</ci><ci id="S4.SS4.p1.8.m4.1.1.3.cmml" xref="S4.SS4.p1.8.m4.1.1.3">ğ‘Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.8.m4.1c">P_{r}</annotation></semantics></math> and <math id="S4.SS4.p1.9.m5.1" class="ltx_Math" alttext="P_{g}" display="inline"><semantics id="S4.SS4.p1.9.m5.1a"><msub id="S4.SS4.p1.9.m5.1.1" xref="S4.SS4.p1.9.m5.1.1.cmml"><mi id="S4.SS4.p1.9.m5.1.1.2" xref="S4.SS4.p1.9.m5.1.1.2.cmml">P</mi><mi id="S4.SS4.p1.9.m5.1.1.3" xref="S4.SS4.p1.9.m5.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.9.m5.1b"><apply id="S4.SS4.p1.9.m5.1.1.cmml" xref="S4.SS4.p1.9.m5.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.9.m5.1.1.1.cmml" xref="S4.SS4.p1.9.m5.1.1">subscript</csymbol><ci id="S4.SS4.p1.9.m5.1.1.2.cmml" xref="S4.SS4.p1.9.m5.1.1.2">ğ‘ƒ</ci><ci id="S4.SS4.p1.9.m5.1.1.3.cmml" xref="S4.SS4.p1.9.m5.1.1.3">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.9.m5.1c">P_{g}</annotation></semantics></math> using samples drawn independently from each distribution. KID is defined to be
the squared maximum mean discrepancy (MMD) between the Inception features of real and generated
images. A cubic polynomial kernel is used to map the real and generated images from the feature
space of the Inception network, which is defined as:</p>
<table id="S4.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E4.m1.3" class="ltx_Math" alttext="KID(x,y)=\left(\frac{1}{d}{x^{T}}y+1\right)\textsuperscript{3}\vspace{-5mm}" display="block"><semantics id="S4.E4.m1.3a"><mrow id="S4.E4.m1.3.3" xref="S4.E4.m1.3.3.cmml"><mrow id="S4.E4.m1.3.3.3" xref="S4.E4.m1.3.3.3.cmml"><mi id="S4.E4.m1.3.3.3.2" xref="S4.E4.m1.3.3.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.3.1" xref="S4.E4.m1.3.3.3.1.cmml">â€‹</mo><mi id="S4.E4.m1.3.3.3.3" xref="S4.E4.m1.3.3.3.3.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.3.1a" xref="S4.E4.m1.3.3.3.1.cmml">â€‹</mo><mi id="S4.E4.m1.3.3.3.4" xref="S4.E4.m1.3.3.3.4.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.3.1b" xref="S4.E4.m1.3.3.3.1.cmml">â€‹</mo><mrow id="S4.E4.m1.3.3.3.5.2" xref="S4.E4.m1.3.3.3.5.1.cmml"><mo stretchy="false" id="S4.E4.m1.3.3.3.5.2.1" xref="S4.E4.m1.3.3.3.5.1.cmml">(</mo><mi id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml">x</mi><mo id="S4.E4.m1.3.3.3.5.2.2" xref="S4.E4.m1.3.3.3.5.1.cmml">,</mo><mi id="S4.E4.m1.2.2" xref="S4.E4.m1.2.2.cmml">y</mi><mo stretchy="false" id="S4.E4.m1.3.3.3.5.2.3" xref="S4.E4.m1.3.3.3.5.1.cmml">)</mo></mrow></mrow><mo id="S4.E4.m1.3.3.2" xref="S4.E4.m1.3.3.2.cmml">=</mo><mrow id="S4.E4.m1.3.3.1" xref="S4.E4.m1.3.3.1.cmml"><mrow id="S4.E4.m1.3.3.1.1.1" xref="S4.E4.m1.3.3.1.1.1.1.cmml"><mo id="S4.E4.m1.3.3.1.1.1.2" xref="S4.E4.m1.3.3.1.1.1.1.cmml">(</mo><mrow id="S4.E4.m1.3.3.1.1.1.1" xref="S4.E4.m1.3.3.1.1.1.1.cmml"><mrow id="S4.E4.m1.3.3.1.1.1.1.2" xref="S4.E4.m1.3.3.1.1.1.1.2.cmml"><mfrac id="S4.E4.m1.3.3.1.1.1.1.2.2" xref="S4.E4.m1.3.3.1.1.1.1.2.2.cmml"><mn id="S4.E4.m1.3.3.1.1.1.1.2.2.2" xref="S4.E4.m1.3.3.1.1.1.1.2.2.2.cmml">1</mn><mi id="S4.E4.m1.3.3.1.1.1.1.2.2.3" xref="S4.E4.m1.3.3.1.1.1.1.2.2.3.cmml">d</mi></mfrac><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.1.1.1.1.2.1" xref="S4.E4.m1.3.3.1.1.1.1.2.1.cmml">â€‹</mo><msup id="S4.E4.m1.3.3.1.1.1.1.2.3" xref="S4.E4.m1.3.3.1.1.1.1.2.3.cmml"><mi id="S4.E4.m1.3.3.1.1.1.1.2.3.2" xref="S4.E4.m1.3.3.1.1.1.1.2.3.2.cmml">x</mi><mi id="S4.E4.m1.3.3.1.1.1.1.2.3.3" xref="S4.E4.m1.3.3.1.1.1.1.2.3.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.1.1.1.1.2.1a" xref="S4.E4.m1.3.3.1.1.1.1.2.1.cmml">â€‹</mo><mi id="S4.E4.m1.3.3.1.1.1.1.2.4" xref="S4.E4.m1.3.3.1.1.1.1.2.4.cmml">y</mi></mrow><mo id="S4.E4.m1.3.3.1.1.1.1.1" xref="S4.E4.m1.3.3.1.1.1.1.1.cmml">+</mo><mn id="S4.E4.m1.3.3.1.1.1.1.3" xref="S4.E4.m1.3.3.1.1.1.1.3.cmml">1</mn></mrow><mo id="S4.E4.m1.3.3.1.1.1.3" xref="S4.E4.m1.3.3.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.1.2" xref="S4.E4.m1.3.3.1.2.cmml">â€‹</mo><mtext id="S4.E4.m1.3.3.1.3" xref="S4.E4.m1.3.3.1.3b.cmml"><sup id="S4.E4.m1.3.3.1.3.1nest" class="ltx_sup">3</sup></mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.3b"><apply id="S4.E4.m1.3.3.cmml" xref="S4.E4.m1.3.3"><eq id="S4.E4.m1.3.3.2.cmml" xref="S4.E4.m1.3.3.2"></eq><apply id="S4.E4.m1.3.3.3.cmml" xref="S4.E4.m1.3.3.3"><times id="S4.E4.m1.3.3.3.1.cmml" xref="S4.E4.m1.3.3.3.1"></times><ci id="S4.E4.m1.3.3.3.2.cmml" xref="S4.E4.m1.3.3.3.2">ğ¾</ci><ci id="S4.E4.m1.3.3.3.3.cmml" xref="S4.E4.m1.3.3.3.3">ğ¼</ci><ci id="S4.E4.m1.3.3.3.4.cmml" xref="S4.E4.m1.3.3.3.4">ğ·</ci><interval closure="open" id="S4.E4.m1.3.3.3.5.1.cmml" xref="S4.E4.m1.3.3.3.5.2"><ci id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1">ğ‘¥</ci><ci id="S4.E4.m1.2.2.cmml" xref="S4.E4.m1.2.2">ğ‘¦</ci></interval></apply><apply id="S4.E4.m1.3.3.1.cmml" xref="S4.E4.m1.3.3.1"><times id="S4.E4.m1.3.3.1.2.cmml" xref="S4.E4.m1.3.3.1.2"></times><apply id="S4.E4.m1.3.3.1.1.1.1.cmml" xref="S4.E4.m1.3.3.1.1.1"><plus id="S4.E4.m1.3.3.1.1.1.1.1.cmml" xref="S4.E4.m1.3.3.1.1.1.1.1"></plus><apply id="S4.E4.m1.3.3.1.1.1.1.2.cmml" xref="S4.E4.m1.3.3.1.1.1.1.2"><times id="S4.E4.m1.3.3.1.1.1.1.2.1.cmml" xref="S4.E4.m1.3.3.1.1.1.1.2.1"></times><apply id="S4.E4.m1.3.3.1.1.1.1.2.2.cmml" xref="S4.E4.m1.3.3.1.1.1.1.2.2"><divide id="S4.E4.m1.3.3.1.1.1.1.2.2.1.cmml" xref="S4.E4.m1.3.3.1.1.1.1.2.2"></divide><cn type="integer" id="S4.E4.m1.3.3.1.1.1.1.2.2.2.cmml" xref="S4.E4.m1.3.3.1.1.1.1.2.2.2">1</cn><ci id="S4.E4.m1.3.3.1.1.1.1.2.2.3.cmml" xref="S4.E4.m1.3.3.1.1.1.1.2.2.3">ğ‘‘</ci></apply><apply id="S4.E4.m1.3.3.1.1.1.1.2.3.cmml" xref="S4.E4.m1.3.3.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S4.E4.m1.3.3.1.1.1.1.2.3.1.cmml" xref="S4.E4.m1.3.3.1.1.1.1.2.3">superscript</csymbol><ci id="S4.E4.m1.3.3.1.1.1.1.2.3.2.cmml" xref="S4.E4.m1.3.3.1.1.1.1.2.3.2">ğ‘¥</ci><ci id="S4.E4.m1.3.3.1.1.1.1.2.3.3.cmml" xref="S4.E4.m1.3.3.1.1.1.1.2.3.3">ğ‘‡</ci></apply><ci id="S4.E4.m1.3.3.1.1.1.1.2.4.cmml" xref="S4.E4.m1.3.3.1.1.1.1.2.4">ğ‘¦</ci></apply><cn type="integer" id="S4.E4.m1.3.3.1.1.1.1.3.cmml" xref="S4.E4.m1.3.3.1.1.1.1.3">1</cn></apply><ci id="S4.E4.m1.3.3.1.3b.cmml" xref="S4.E4.m1.3.3.1.3"><mtext id="S4.E4.m1.3.3.1.3.cmml" xref="S4.E4.m1.3.3.1.3"><sup id="S4.E4.m1.3.3.1.3.1anest" class="ltx_sup">3</sup></mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.3c">KID(x,y)=\left(\frac{1}{d}{x^{T}}y+1\right)\textsuperscript{3}\vspace{-5mm}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2212.01772/assets/comparisonfigures.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="264" height="157" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">Comparative analysis of generated brain tumor MRI samples using DCGANÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, WGANÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, FFHQ (Ours), and the original sample. Each row corresponds to images from three different classes, namely; Glioma, Meningioma, and Pituitary.</span></figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2212.01772/assets/x2.png" id="S4.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="353" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F4.sf1.3.2" class="ltx_text" style="font-size:90%;">Initial synthetic images using FFHQ</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2212.01772/assets/x3.png" id="S4.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="358" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F4.sf2.3.2" class="ltx_text" style="font-size:90%;">Initial synthetic images using AFHQ (Cat)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2212.01772/assets/x4.png" id="S4.F4.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="353" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F4.sf3.3.2" class="ltx_text" style="font-size:90%;">Initial synthetic images using AFHQ (Wild)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2212.01772/assets/x5.png" id="S4.F4.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="353" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S4.F4.sf4.3.2" class="ltx_text" style="font-size:90%;">Initial synthetic images using BreCaHaD (Histopathology)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Samples of initially generated images. Results show a visualization of the weights of the StyleGAN model trained on FFHQ, AFHQ (Cat), AFHQ (Wild), and BreCaHad images.</span></figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2212.01772/assets/x6.png" id="S4.F5.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="356" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">Synthetic MRI brain tumor images on best FID and KID for the FFHQ pre-trained model.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2212.01772/assets/x7.png" id="S4.F5.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="352" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">Synthetic MRI brain tumor images on best FID and KID values for the AFHQ (Cat) pre-trained model.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2212.01772/assets/x8.png" id="S4.F5.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="353" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F5.sf3.3.2" class="ltx_text" style="font-size:90%;">Synthetic MRI brain tumor images on best FID and KID values for the AFHQ (Wild) pre-trained model.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2212.01772/assets/x9.png" id="S4.F5.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="345" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S4.F5.sf4.3.2" class="ltx_text" style="font-size:90%;">Synthetic MRI brain tumor images on best FID and KID values for the BreCaHaD pre-trained model.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">Synthetic images generated for the best FID and KID values.</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">FID is a commonly used metric for assessing the quality of the images generated by a model. However, FID is prone to be dominated by the inherent bias when the number of real images is not large enough. Hence, we used KID as an additional metric for evaluating our model performance.
The trend for FID and KID using different pre-trained models is shown in FigureÂ <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:pre14</span>. FID and KID values were recorded on every ten tick intervals for FFHQ, BreCaHaD, and AFHQ models, where tick interval refers to the number of iterations after the training snapshot has been taken. The results are summarized in TableÂ <a href="#S5.T1" title="Table 1 â€£ 5 Results and Discussion â€£ Brain Tumor Synthetic Data Generation with Adaptive StyleGANsThis preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this contribution is published in [insert volume title], and is available online at https://doi.org/doi to be updated . Upon publishing, final version will be available from Springer Nature Publishers (link to be updated). Pre-print of work submitted to 30th AICS conference." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The trend indicates a decrease in FID and KID values as tick intervals increase for FFHQ and AFHQ (Cat) models. For BreCaHaD and AFHQ (Wild) models, a decrease can be observed from 0-30 tick intervals. After that, an increase can be seen for both FID and KID values. Amongst the models evaluated, the BrecaHaD model had the worse performance, having the highest FID and lowest KID values.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Qualitative results of initially synthetically generated brain tumor images by different models are shown in FigureÂ <a href="#S4.F4" title="Figure 4 â€£ 4.4 Evaluation Metrics â€£ 4 Experiments â€£ Brain Tumor Synthetic Data Generation with Adaptive StyleGANsThis preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this contribution is published in [insert volume title], and is available online at https://doi.org/doi to be updated . Upon publishing, final version will be available from Springer Nature Publishers (link to be updated). Pre-print of work submitted to 30th AICS conference." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
Using the best FID and KID of the pre-trained models, the brain MRI images generated by transfer learning are shown in FigureÂ <a href="#S4.F5" title="Figure 5 â€£ 4.4 Evaluation Metrics â€£ 4 Experiments â€£ Brain Tumor Synthetic Data Generation with Adaptive StyleGANsThis preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this contribution is published in [insert volume title], and is available online at https://doi.org/doi to be updated . Upon publishing, final version will be available from Springer Nature Publishers (link to be updated). Pre-print of work submitted to 30th AICS conference." class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. By analyzing our results, we find that FFHQ gives the lowest
FID of 58.1097 and KID of 0.00862, and generates better quality images
when compared with other pre-trained models. FigureÂ <a href="#S4.F3" title="Figure 3 â€£ 4.4 Evaluation Metrics â€£ 4 Experiments â€£ Brain Tumor Synthetic Data Generation with Adaptive StyleGANsThis preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this contribution is published in [insert volume title], and is available online at https://doi.org/doi to be updated . Upon publishing, final version will be available from Springer Nature Publishers (link to be updated). Pre-print of work submitted to 30th AICS conference." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows a comparison of the images generated using DCGANÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, WGANÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and Ours (FFHQ) model using the brain tumor dataset. The results indicate Ours (FFHQ) generates better quality images when compared with the other GAN models.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S5.T1.3.2" class="ltx_text" style="font-size:90%;">Results</span></figcaption>
<table id="S5.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.4.1.1" class="ltx_tr">
<th id="S5.T1.4.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T1.4.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.4.1.1.1.1.1" class="ltx_p" style="width:85.4pt;">Pre-trained models</span>
</span>
</th>
<th id="S5.T1.4.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S5.T1.4.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.4.1.1.2.1.1" class="ltx_p" style="width:56.9pt;">FID</span>
</span>
</th>
<th id="S5.T1.4.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S5.T1.4.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.4.1.1.3.1.1" class="ltx_p" style="width:85.4pt;">KID</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.4.2.1" class="ltx_tr">
<td id="S5.T1.4.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S5.T1.4.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.4.2.1.1.1.1" class="ltx_p" style="width:85.4pt;">FFHQ</span>
</span>
</td>
<td id="S5.T1.4.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.4.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.4.2.1.2.1.1" class="ltx_p" style="width:56.9pt;">58.1097</span>
</span>
</td>
<td id="S5.T1.4.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.4.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.4.2.1.3.1.1" class="ltx_p" style="width:85.4pt;">0.00862692</span>
</span>
</td>
</tr>
<tr id="S5.T1.4.3.2" class="ltx_tr">
<td id="S5.T1.4.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="S5.T1.4.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.4.3.2.1.1.1" class="ltx_p" style="width:85.4pt;">AFHQ Cat</span>
</span>
</td>
<td id="S5.T1.4.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T1.4.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.4.3.2.2.1.1" class="ltx_p" style="width:56.9pt;">60.9486</span>
</span>
</td>
<td id="S5.T1.4.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T1.4.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.4.3.2.3.1.1" class="ltx_p" style="width:85.4pt;">0.01049849</span>
</span>
</td>
</tr>
<tr id="S5.T1.4.4.3" class="ltx_tr">
<td id="S5.T1.4.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="S5.T1.4.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.4.4.3.1.1.1" class="ltx_p" style="width:85.4pt;">BreCaHaD</span>
</span>
</td>
<td id="S5.T1.4.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T1.4.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.4.4.3.2.1.1" class="ltx_p" style="width:56.9pt;">67.5336</span>
</span>
</td>
<td id="S5.T1.4.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S5.T1.4.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.4.4.3.3.1.1" class="ltx_p" style="width:85.4pt;">0.02081763</span>
</span>
</td>
</tr>
<tr id="S5.T1.4.5.4" class="ltx_tr">
<td id="S5.T1.4.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r">
<span id="S5.T1.4.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.4.5.4.1.1.1" class="ltx_p" style="width:85.4pt;">AFHQ Wild</span>
</span>
</td>
<td id="S5.T1.4.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S5.T1.4.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.4.5.4.2.1.1" class="ltx_p" style="width:56.9pt;">59.7498</span>
</span>
</td>
<td id="S5.T1.4.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S5.T1.4.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.4.5.4.3.1.1" class="ltx_p" style="width:85.4pt;">0.0109629</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Future Work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this work, we presented a useful application of Adaptive StyleGANS for synthetic brain MRI images. Our results show that high-quality realistic MRI brain tumor images can be generated using pre-trained GAN models.
By analyzing our results, we find that FFHQ gives the lowest FID and KID and generates better quality images when compared with other pre-trained models used in this research. This work will motivate other researchers to leverage the potential of StyleGAN in many applied domains of medical imaging research. For example, the models can be explored for modeling to detect the presence of tumors in body parts, perform tissue segmentation when training largely suffers due to the unavailability of high-quality data, and cross-modality medical image generation. The future work of this research is to explore the use of StyleGAN2-based architectures for the synthesis of high-quality medical images of other modalities such as Computed Tomography and histopathology images.
It would be interesting to evaluate the model performance with other smaller medical imaging datasets. Similarly, an interesting direction is to explore the use of StyleGAN2 with StyleCLIP Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> for generating medical images from the text description.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J.Â Ker, L.Â Wang, J.Â Rao, and T.Â Lim, â€œDeep learning applications in medical
image analysis,â€ <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Ieee Access</span>, vol.Â 6, pp.Â 9375â€“9389, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
G.Â An, M.Â Akiba, K.Â Omodaka, T.Â Nakazawa, and H.Â Yokota, â€œHierarchical deep
learning models using transfer learning for disease detection and
classification based on small number of medical images,â€ <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Sci reports.</span>,
vol.Â 11, no.Â 1, pp.Â 1â€“9, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
GoodfellowIan, Pouget-AbadieJean, MirzaMehdi, Xubing, Warde-FarleyDavid,
OzairSherjil, CourvilleAaron, and BengioYoshua, â€œGenerative adversarial
networks,â€ <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Communications of The ACM</span>, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
X.Â Yi, E.Â Walia, and P.Â Babyn, â€œGenerative adversarial network in medical
imaging: A review,â€ <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Medical image analysis</span>, vol.Â 58, p.Â 101552, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
H.Â Ali, R.Â Biswas, F.Â Ali, U.Â Shah, A.Â Alamgir, O.Â Mousa, and Z.Â Shah, â€œThe
role of generative adversarial networks in brain mri: a scoping review,â€
<span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Insights into Imaging</span>, vol.Â 13, no.Â 1, pp.Â 1â€“15, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
T.Â Karras, M.Â Aittala, J.Â Hellsten, S.Â Laine, J.Â Lehtinen, and T.Â Aila,
â€œTraining generative adversarial networks with limited data,â€ <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Advances
in Neural Information Processing Systems</span>, vol.Â 33, pp.Â 12104â€“12114, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y.Â Zhao, C.Â Li, P.Â Yu, J.Â Gao, and C.Â Chen, â€œFeature quantization improves gan
training,â€ <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.02088</span>, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A.Â IÅŸÄ±n, C.Â DirekoÄŸlu, and M.Â Åah, â€œReview of mri-based brain tumor image
segmentation using deep learning methods,â€ <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Procedia computer science</span>,
vol.Â 102, p.Â 317â€“324, 2016.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
F.Â J. DÃ­az-Pernas and M.Â MartÃ­nez-Zarzuela, â€œA deep learning approach for
brain tumor classification and segmentation using a multiscale convolutional
neural network,â€ <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Healthcare</span>, vol.Â 9, no.Â 2, p.Â 153, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
T.Â Karras, S.Â Laine, and T.Â Aila, â€œA style-based generator architecture for
generative adversarial networks,â€ <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">IEEE CVPR</span>, pp.Â 4396â€“4405, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
H.Â Gao, J.Â Pei, and H.Â Huang, â€œProgan: Network embedding via proximity
generative adversarial network,â€ in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proceedings of the 25th ACM SIGKDD
International Conference on Knowledge Discovery &amp; Data Mining</span>,
pp.Â 1308â€“1316, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
T.Â Karras, S.Â Laine, M.Â Aittala, J.Â Hellsten, J.Â Lehtinen, and T.Â Aila,
â€œAnalyzing and improving the image quality of stylegan,â€ <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">IEEE CVPR</span>,
pp.Â 8107â€“8116, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
T.Â Karras, M.Â Aittala, J.Â Hellsten, S.Â Laine, J.Â Lehtinen, and T.Â Aila,
â€œTraining generative adversarial networks with limited data,â€ in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, vol.Â 33,
pp.Â 12104â€“12114, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A.Â Aksac, D.Â J. Demetrick, T.Â Ozyer, and R.Â Alhajj, â€œBrecahad: a dataset for
breast cancer histopathological annotation and diagnosis,â€ <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">BMC research
notes</span>, vol.Â 12, no.Â 1, p.Â 82, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
N.Â Kumari, R.Â Zhang, E.Â Shechtman, and J.-Y. Zhu, â€œEnsembling off-the-shelf
models for gan training,â€ in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">IEEE CVPR</span>, pp.Â 10651â€“10662, 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Y.Â Choi, Y.Â Uh, J.Â Yoo, and J.-W. Ha, â€œStargan v2: Diverse image synthesis for
multiple domains,â€ <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">IEEE CVPR</span>, pp.Â 8185â€“8194, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
F.Â Yu, A.Â Seff, Y.Â Zhang, S.Â Song, T.Â Funkhouser, and J.Â Xiao, â€œLsun:
Construction of a large-scale image dataset using deep learning with humans
in the loop,â€ <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1506.03365</span>, 2015.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
T.Â Karras, T.Â Aila, S.Â Laine, and J.Â Lehtinen, â€œProgressive growing of gans
for improved quality, stability, and variation,â€ <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:1710.10196</span>, 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
W.Â Ahmad, H.Â Ali, Z.Â Shah, and S.Â Azmat, â€œA new generative adversarial network
for medical images super resolution,â€ <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Sci Rep.</span>, vol.Â 12, no.Â 1,
pp.Â 1â€“20, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
O.Â Patashnik, Z.Â Wu, E.Â Shechtman, D.Â Cohen-Or, and D.Â Lischinski, â€œStyleclip:
Text-driven manipulation of stylegan imagery,â€ <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">2021 IEEE/CVF
International Conference on Computer Vision (ICCV)</span>, pp.Â 2065â€“2074, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A.Â Radford, L.Â Metz, and S.Â Chintala, â€œUnsupervised representation learning
with deep convolutional generative adversarial networks,â€ <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">CoRR</span>,
vol.Â abs/1511.06434, 2016.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
A.Â Radford, J.Â W. Kim, C.Â Hallacy, A.Â Ramesh, G.Â Goh, S.Â Agarwal, G.Â Sastry,
A.Â Askell, P.Â Mishkin, J.Â Clark, <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">etÂ al.</span>, â€œLearning transferable visual
models from natural language supervision,â€ in <span id="bib.bib22.2.2" class="ltx_text ltx_font_italic">ICML</span>, pp.Â 8748â€“8763,
PMLR, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
W.Â Peebles, J.-Y. Zhu, R.Â Zhang, A.Â Torralba, A.Â A. Efros, and E.Â Shechtman,
â€œGan-supervised dense visual alignment,â€ in <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">IEEE CVPR</span>,
pp.Â 13470â€“13481, 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
X.Â Ma, R.Â Jin, K.-A. Sohn, J.-Y. Paik, and T.-S. Chung, â€œAn adaptive control
algorithm for stable training of generative adversarial networks,â€ <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">IEEE
access: practical innovations, open solutions</span>, vol.Â 7, p.Â 184103â€“184114,
2019.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
K.Â Weiss, T.Â M. Khoshgoftaar, and D.Â Wang, â€œA survey of transfer learning,â€
<span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Journal of Big data</span>, vol.Â 3, no.Â 1, pp.Â 1â€“40, 2016.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Y.Â Wang, A.Â Gonzalez-Garcia, D.Â Berga, L.Â Herranz, F.Â S. Khan, and J.Â v.Â d.
Weijer, â€œMinegan: effective knowledge transfer from gans to target domains
with few images,â€ in <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">IEEE CVPR</span>, pp.Â 9332â€“9341, 2020.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
T.Â Iqbal and H.Â Ali, â€œGenerative adversarial network for medical images
(mi-gan),â€ <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Journal of medical systems</span>, vol.Â 42, no.Â 11, pp.Â 1â€“11,
2018.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Y.Â Wang, C.Â Wu, L.Â Herranz, J.Â vanÂ de Weijer, A.Â Gonzalez-Garcia, and
B.Â Raducanu, â€œTransferring gans: generating images from limited data,â€ in
<span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision (ECCV)</span>,
pp.Â 218â€“234, 2018.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J.Â Cheng, W.Â Huang, S.Â Cao, R.Â Yang, W.Â Yang, Z.Â Yun, Z.Â Wang, and Q.Â Feng,
â€œEnhanced performance of brain tumor classification via tumor region
augmentation and partition,â€ <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">PloS one</span>, vol.Â 10, no.Â 10, p.Â e0140381,
2015.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
P.Â Singh, E.Â Sizikova, and J.Â Cirrone, â€œCASS: Cross architectural
self-supervision for medical image analysis,â€ <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">ArXiv</span>,
vol.Â abs/2206.04170, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
N.Â Chakrabarty, â€œBrain tumor dataset,â€ <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Kaggle</span>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
E.Â J. Nunn, P.Â Khadivi, and S.Â Samavi, â€œCompound frechet inception distance
for quality assessment of gan created images,â€ <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">arXiv [cs.CV]</span>, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
S.Â Knop, M.Â Mazur, P.Â Spurek, J.Â Tabor, and I.Â Podolak, â€œGenerative models
with kernel distance in data space,â€ <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Neurocomputing</span>, vol.Â 487,
pp.Â 119â€“129, 2022.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
M.Â Arjovsky, S.Â Chintala, and L.Â Bottou, â€œWasserstein generative adversarial
networks,â€ in <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">34th ICML</span>, vol.Â 70, pp.Â 214â€“223, PMLR, Aug 2017.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2212.01771" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2212.01772" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2212.01772">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2212.01772" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2212.01773" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 12:45:53 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
