<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2201.07980] Caring Without Sharing: A Federated Learning Crowdsensing Framework for Diversifying Representation of Cities</title><meta property="og:description" content="Mobile Crowdsensing has become main stream paradigm for researchers to collect behavioural data from citizens in large scales. This valuable data can be leveraged to create centralized repositories that can be used to …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Caring Without Sharing: A Federated Learning Crowdsensing Framework for Diversifying Representation of Cities">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Caring Without Sharing: A Federated Learning Crowdsensing Framework for Diversifying Representation of Cities">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2201.07980">

<!--Generated on Wed Mar  6 10:17:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Mobile Crowd Sensing,  Federated Learning,  Privacy">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Computing Software System, University of Washington, Bothell, Washington, USA 
<br class="ltx_break"><span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>mikec87@uw.edu</span></span></span>,<span id="id1.2" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>mashhadi@uw.edu</span></span></span>
<br class="ltx_break"></span></span></span>
<h1 class="ltx_title ltx_title_document">Caring Without Sharing: A Federated Learning Crowdsensing Framework for Diversifying Representation of Cities</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Michael Cho
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Afra Mashhadi
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Mobile Crowdsensing has become main stream paradigm for researchers to collect behavioural data from citizens in large scales. This valuable data can be leveraged to create centralized repositories that can be used to train advanced Artificial Intelligent (AI) models for various services that benefit society in all aspects. Although decades of research has explored the viability of Mobile Crowdsensing in terms of incentives and many attempts have been made to reduce the participation barriers, the overshadowing privacy concerns regarding sharing personal data still remain. Recently a new pathway has emerged to enable to shift MCS paradigm towards a more privacy-preserving collaborative learning, namely Federated Learning. In this paper, we posit a first of its kind framework for this emerging paradigm. We demonstrate the functionalities of our framework through a case study of diversifying two vision algorithms through to learn the representation of ordinary sidewalk obstacles as part of enhancing visually impaired navigation.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Mobile Crowd Sensing, Federated Learning, Privacy
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the past decade the Mobile Crowdsensing paradigm (MCS) have leveraged power of crowds for a range of applications to help researchers and practitioners enhance their understanding of the cities and citizens. MCS allows users to participate in a campaign by providing passive or active data through their sensor enabled mobile devices. For instance, within the context of smart cities, MCS has enabled the optimization for various <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">passive sensing</em> applications, such as pollution, public transportation, traffic congestion, road conditions, etc. Active sensing applications have also helped to advance understanding of the cities through the active contribution of the crowds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. Such applications include FixMyStreet<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.fixmystreet.com</span></span></span> where citizens actively report faults within their neighborhoods or others such as GeoNotify <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> where citizen report the sidewalk obstacles that could impact visually impaired navigation.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To this end, frameworks such as AWARE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, Sensus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, and SensingKit<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> have brought the feasibility of creating MCS tasks/campaigns to researchers and policy makers. For example AWARE framework allows users to design experiments and run data collection campaigns that tap into the smartphone sensors with a few lines of code. Indeed, common to all these frameworks is that they provide an easy interactive design for the scientific community to design the experiment through a web dashboard, and furthermore they offer storage and communication between the devices and the server.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, there still remains privacy challenges that could act as participation barriers for MCS users <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. These privacy concerns are in twofold: First, user’s data could contain sensitive personal information. Secondly, personal information can be concluded by analyzing the data provided by the user and through continuous monitoring. For example, by collecting sensory data related to the user location on the device, the user’s home location information can be obtained.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To overcome privacy concerns, the crowd-sensing community has recently started to explore alternatives and possibilities of a paradigm shift that would decouple the data collection and analysis from a centralized approach to a distributed setting. To this end, Federated Learning (FL) has emerged as a promising candidate for this paradigm shift <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. In FL schema each participant’s device holds on to their own data, and a FL server orchastrates a collaborative training by sending the shared model to the devices. In this way, the data always remains local to the client device.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Inspired by this trend, in this paper we present, FLOAT, a <em id="S1.p5.1.1" class="ltx_emph ltx_font_bold">F</em>ederated <em id="S1.p5.1.2" class="ltx_emph ltx_font_bold">L</em>earning framework f<em id="S1.p5.1.3" class="ltx_emph ltx_font_bold">o</em>r <em id="S1.p5.1.4" class="ltx_emph ltx_font_bold">A</em>ctive crowdsensing <em id="S1.p5.1.5" class="ltx_emph ltx_font_bold">T</em>asks. In the design of this framework we pay careful attention in the challenges and opportunities that incur in this paradigm shift. Our framework relies on Flower <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> to facilitate FL training on the device end and the orchestration on the server side. Our framework consists of an interactive dashboard allowing the researchers to setup their task and specify the training properties. On the device end, we design a range of functionalities to enable users to participate in a campaign. To demonstrate an application of our framework, we present algorithmic and system performance of an obstacle detection use case, where we train state-of-the-art vision models to enhance their representation of ordinary side walk objects. In summary, we make the following contributions:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose, the first of its kind, an end-to-end framework for bringing FL into crowdsensing tasks. Our entire framework would be open-sourced and available under Apache 2.0 license for the crowdsensing community to use in their research.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Using FLOAT as the underlying framework, we present experiments that explore both algorithmic and system-level aspects of federated mobile crowdsensing for an application of Obstacle Detection. We address important research questions as to: <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">How many users and how much data per user is required to learn representation of 5 ordinary sidewalk obstacles. </span></p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We propose a roadmap that we believe would empower the research community to address challenges that remain if we are to integrate FL into the MCS paradigm.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background and Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> has been proposed to provide a privacy-preserving mechanism to leverage de-centralized user data and computation resources to train machine learning models. Federated learning allows users to collaboratively train a shared model under the orchestration of a central server while keeping personal data on their devices. There are, in general, two steps in the FL training process (i) local model training on end devices and (ii) global aggregation of updated parameters in the FL server. The training process of such a FL system usually contains the following three steps as illustrated in Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Background and Related Work ‣ Caring Without Sharing: A Federated Learning Crowdsensing Framework for Diversifying Representation of Cities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>:</p>
</div>
<div id="S2.p2" class="ltx_para">
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">The server initializes the global weights, specifies the global model hyper parameters and the training process, and sends the task to selected participants.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Participants locally compute training gradients and send the gradients or updated weights to the server.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">The server performs aggregation and shares the new weights with participants. Steps 2–3 are repeated until the global loss function converges or a desirable training accuracy is achieved.</p>
</div>
</li>
</ol>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">To this end, incorporating Federated Learning paradigm into Mobile Crowdsensing tasks can address some of the long existing challenges of MCS and create new opportunities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. In Mobile Crowdsensing the use of personal smart devices that have enough processing capabilities are a prime candidate to integrate with Federated Learning. The benefit of such methodology can be seen in two major aspects. First, Federated Learning helps to preserve the privacy of the user by never uploading the raw collected data. Secondly, it can be leveraged as a means to diversify the representation of the data and lead to more inclusive machine learning models.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2201.07980/assets/arch2.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="669" height="287" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The overview of the interactions between the model owner and the devices under orchestration of the FL server.</figcaption>
</figure>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">It is worth noting that there also exists other great opportunities that arise from this paradigm shift and particularly from removing the burden of data collection and centralized training. For instance, in traditional MCS schemes the server must overlook the transmission, and the storage of data. The data is then pre-processed and used for a centralized training. These processes incur large overhead costs and maintenance and thus reducing those requirements by removing the need for centralized data repositories would lower the threshold for deployment campaigns. Moreover, MCS incurs large communication costs with the transfer of raw data into the server. This places a burden on the server itself to process this data and takes a large amount of bandwidth from the users as well. By bringing the model to the user and enabling local training, the need for resources in terms of bandwidth is greatly reduced. Finally, in terms of energy consumption, previous work has shown that federated learning can play a large role in reducing CO2 emission associated with training large AI models, by moving the training process into hand-held devices that do not require cooling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Furthermore, previous work has also shown that the energy and memory consumption that are required for on-device training for smart city applications is almost negligible <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Applications of Federated Learning in MCS </h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">To date, research in real-world applications of federated learning are still in infancy and limited to a handful examples. Smart security <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> is an emerging field that has seen most integration of federated learning in the context of smart cities. Based on machine learning, smart security can perform post event analysis and self-learning, constantly accumulating experience, and continuously improve pre-warning capabilities. Federated Learning offers a machine learning training scheme that allows the use of the large amounts of collected data in daily applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Outside of security applications Mashhadi et al.  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> proposed an application of federated learning for discovering urban communities. They showed that by using the GPS traces that are stored on each device, and collaboratively training a deep embedded clustering model, it is possible to detect meaningful urban communities without the need for location information to be shared.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Design Considerations</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section we take a critical view of some of the design challenges that incur due to the suggested paradigm shift and decoupling the data from the centralized approaches. We address how we design for responding to these challenges in the proposed framework.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Challenges</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In order to shift the MCS schema to a decentralized approach where the participants’ data is only held on local devices, various challenges need to be addressed.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Challenge 1:</span> Perhaps most challenging aspect of a FL MCS proposed schema is that it reduces the <span id="S3.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">exploratory</span> scopes of MCS tasks. Indeed one of the benefits of MCS data collection, has always been on enabling researchers to collect a large volume of data first and then ask what type of research hypothesis could be addressed and which portion of the data is indeed needed to answer those question. In contrast, a federated schema, reduces the scope of this exploratory analysis and enforces researchers to not only have a very well defined research question and hypothesis, but most importantly a <span id="S3.I1.i1.p1.1.3" class="ltx_text ltx_font_italic">model</span> prior to deployment.</p>
</div>
<div id="S3.I1.i1.p2" class="ltx_para">
<p id="S3.I1.i1.p2.1" class="ltx_p">This property means that many phases of exploratory analysis needs to be shifted into a pre-training stage where the model is implemented and trained on a proxy dataset. Such proxy dataset may or may not be an accurate representation of the participant’s data. Indeed, in this vein <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> showed that it is possible to <span id="S3.I1.i1.p2.1.1" class="ltx_text ltx_font_italic">pre-</span>train an urban community detection model on an aggregated mobility dataset and then dispatch it to be re-trained under the FL setting on client’s fine grain location data.</p>
</div>
<div id="S3.I1.i1.p3" class="ltx_para">
<p id="S3.I1.i1.p3.1" class="ltx_p"><span id="S3.I1.i1.p3.1.1" class="ltx_text ltx_font_bold">Design Goal 1:</span> In our framework we design for this functionality by enabling any pre-trained model and weights to be loaded to FLOAT. Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Background and Related Work ‣ Caring Without Sharing: A Federated Learning Crowdsensing Framework for Diversifying Representation of Cities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents the proposed schema where the user (i.e., model owner) shares a pre-trained model with the framework which then gets pushed towards the client devices. Furthermore, we also cater for the applications of transfer learning. Transfer learning focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. In many cases where the algorithms are pre-trained to learn a representation of a proxy dataset, it is possible to only retrain the final layer of the model to account for the specific task. Such approach also significantly reduces the convergence time and thus reduces the training at the local devices. In FLOAT, we provide an option to indicate whether the weights of the loaded model are to be fully re-trained or only fine tuned at the final layer.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Challenge 2:</span> A second challenge in designing for a federated MCS schema is the lack of <span id="S3.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">transparency</span>. That is because participants’ data is unseen, it is difficult to assess the outcome of the training. Therefore, questions arise on “how much contribution did each participant make?” or “How many rounds of training would be needed for the model to converge?”.</p>
</div>
<div id="S3.I1.i2.p2" class="ltx_para">
<p id="S3.I1.i2.p2.1" class="ltx_p"><span id="S3.I1.i2.p2.1.1" class="ltx_text ltx_font_bold">Design Goal 2:</span> To address this challenge, we design our framework with an interactive interface which enables the model owner to quantify the outcome of their model after each round. More specifically we provide two ways of validation:</p>
</div>
<div id="S3.I1.i2.p3" class="ltx_para">
<p id="S3.I1.i2.p3.1" class="ltx_p"><span id="S3.I1.i2.p3.1.1" class="ltx_text ltx_font_bold">Server Validation</span>: The model owner is able to specify a path to to a centralized dataset that could be used to evaluate the accuracy of the updated model after each round of training.</p>
</div>
<div id="S3.I1.i2.p4" class="ltx_para">
<p id="S3.I1.i2.p4.1" class="ltx_p"><span id="S3.I1.i2.p4.1.1" class="ltx_text ltx_font_bold">Client Validation</span>: The validation is entirely on the clients devices. For the cases that the model owner does not have a centralized validation dataset to validate the updated model against it, our framework enables client validation where a portion of participants are selected for validating the model.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Opportunities</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">There exists multiple great opportunities that arise from removing the burden of data collection and centralized training. For instance, in traditional MCS schemes the server must overlook the transmission and storage of data. The data is then pre-processed and used for a centralized training. These processes incur large overhead costs and maintenance. Reducing those requirements by removing the need for centralized data repositories would lower the threshold for deployment campaigns.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Moreover, MCS incurs large communication costs with the transfer of raw data into the server. This places a burden on the server itself to process this data and takes a large amount of bandwidth from the users as well. By bringing the model to the user and enabling local training, the need for resources in terms of bandwidth is greatly reduced.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Finally, in terms of energy consumption, previous work has shown that federated learning can play a large role in reducing CO2 emission associated with training large AI models, by moving the training process into hand-held devices that do not require cooling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Furthermore, previous work has also shown that the energy and memory consumption that are required for on-device training for smart city applications is almost negligible <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2201.07980/assets/arch.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="617" height="380" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Architecture of FLOAT depicting the server and the client part of the framework.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Overview of FLOAT Framework</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Opportunities ‣ 3 Design Considerations ‣ Caring Without Sharing: A Federated Learning Crowdsensing Framework for Diversifying Representation of Cities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the overall architecture design of our framework. As can be seen our framework consist of the server and the client end. Both these parts of the framework rely on Flower <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> as an underlying Federated Learning platform. We thus first describe Flower for the sake of clarity before moving on to describe each component of our framework and the interactions amongst them.</p>
</div>
<section id="S4.SS0.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.0.1 </span>Flower.</h4>

<div id="S4.SS0.SSS1.p1" class="ltx_para">
<p id="S4.SS0.SSS1.p1.1" class="ltx_p">Our framework relies on Flower <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> as an agnostic and scale-able solution for federated learning. Flower offers a stable, language and ML framework-agnostic implementation of the core components of a federated learning system. In particular by using Flower as an underlying FL implementation, FLOAT is able to inherit the agnostic properties of Flower. That is our framework is able to support Machine Learning Models written in either Tensorflow or Pytorch.</p>
</div>
<div id="S4.SS0.SSS1.p2" class="ltx_para">
<p id="S4.SS0.SSS1.p2.1" class="ltx_p">Furthermore, Flower allows for rapid transition of existing ML training pipelines into a FL setup to evaluate their convergence properties and training time in a federated setting. This includes various strategies from aggregation methods (e.g., FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, QffedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, and many others) and evaluation methods.</p>
</div>
<div id="S4.SS0.SSS1.p3" class="ltx_para">
<p id="S4.SS0.SSS1.p3.1" class="ltx_p">Most importantly, we chose Flower as it provides support for extending FL implementations to mobile and wireless clients, with heterogeneous compute, memory, and network resources (e.g., phone, tablet, embedded) and thus ideal for the smart city applications.</p>
</div>
</section>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Server</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The back-end server of our framework is responsible for taking the design of the experiment from the end user, communicating it with devices, and initiating the experiment.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Dashboard.</span> To facilitate an easy interaction our framework has an interactive dashboard (Figure <a href="#S4.F3" title="Figure 3 ‣ 4.1 Server ‣ 4 Overview of FLOAT Framework ‣ Caring Without Sharing: A Federated Learning Crowdsensing Framework for Diversifying Representation of Cities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) that enables researchers to load their own model and specify the training parameters such as the number of training devices (Design Goal 1), the number of data points per each participant, and the hyper-parameters of the experiment. We developed this dashabord in python-based Flask server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> which enables us to build up the web-application and easily modify the components of the interface.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">In addition to taking input from the user, FLOAT dashboard is designed to provide transparency into training (Design Goal 2) by integrating a live visualization dashboard implemented in TensorBoard <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Figure <a href="#S4.F4" title="Figure 4 ‣ 4.1 Server ‣ 4 Overview of FLOAT Framework ‣ Caring Without Sharing: A Federated Learning Crowdsensing Framework for Diversifying Representation of Cities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents this component of the dashboard. An alternative tab on the dashboard allows the users to switch to a debugging interface where the underlying Flower messages presenting INFO, DEBUG, and ERROR that are happening during the training round as observed by the RPC communication channel are displayed.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2201.07980/assets/dash1.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="251" height="240" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Part of the FLOAT Dashboard that acts as an interface with the user to set up the task.</figcaption>
</figure>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">FL server.</span> To start an instance of the FL server, the information from the dashboard is directly communicated to the FL server python code. More specifically these are the following parameters: i) rounds of training; ii) aggregation strategy, iii) evaluation strategy and path to a validation dataset (if applicable), and iv) number of clients.
The FL (Flower) server configures the next round of FL by sending the required configurations to the clients via bi-directional gRPC channel, receives the resulting client updates (or failures) from the clients using the same gRPC, and aggregates the results using the strategy.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p"><span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_bold">Web API module.</span> Additionally in order to communicate the experiment setting with the clients devices, the settings that are entered in the dashboard are saved as a <span id="S4.SS1.p5.1.2" class="ltx_text ltx_font_italic">config.json</span> and sent directly to the clients device. This configuration file includes information regarding the hyper-parameters of the model and local training instructions: i) the model and the initialized weights (if applicable), ii) instructions on whether the model is to be fully retrained or fine-tuned, iii) number of data points per class, iv) finally number and description of classes.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2201.07980/assets/tesnsorboard2.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="203" height="195" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The output component of the dashboard visualizing live results of the training.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Client</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The client component of the FLOAT is currently designed to support Andriod devices. As depicted in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Opportunities ‣ 3 Design Considerations ‣ Caring Without Sharing: A Federated Learning Crowdsensing Framework for Diversifying Representation of Cities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, this component has two main parts: the Backend and the User Interface. We describe the interaction between each part next.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Backend Modules</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">The modules in the backend part of the app are responsible to bridge between the FL server and the user interface. These are <span id="S4.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_italic">Experiment Manager</span>, <span id="S4.SS2.SSS1.p1.1.2" class="ltx_text ltx_font_italic">FL Client</span>, <span id="S4.SS2.SSS1.p1.1.3" class="ltx_text ltx_font_italic">Resource Manager</span> and finally the <span id="S4.SS2.SSS1.p1.1.4" class="ltx_text ltx_font_italic">Sensor Module</span>.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p"><span id="S4.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Experiment Manager.</span> As depicted in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Opportunities ‣ 3 Design Considerations ‣ Caring Without Sharing: A Federated Learning Crowdsensing Framework for Diversifying Representation of Cities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, this module is in charge of receiving the training instructions, parameters, and model from the server. It then communicates the information about the task to the participant through the Task View component of the UI. It is also responsible for setting up the FL client and initializing the training.</p>
</div>
<div id="S4.SS2.SSS1.p3" class="ltx_para">
<p id="S4.SS2.SSS1.p3.1" class="ltx_p"><span id="S4.SS2.SSS1.p3.1.1" class="ltx_text ltx_font_bold">FL Client. </span> The FL client is responsible for training the shared model. As described earlier we use Flower as the underlying framework to support the on-device training. The flexibility of the flower architecture enables our framework to receive any models and weights and simply assemble a FL client code. Furthermore because Flower is designed for heterogeneous devices, the FL client module can be later integrated into different devices and platforms (e.g., iOS). As depicted in the Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Opportunities ‣ 3 Design Considerations ‣ Caring Without Sharing: A Federated Learning Crowdsensing Framework for Diversifying Representation of Cities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the FL client code also has direct access to user’s local dataset. FL Client connects to the gRPC channel which is responsible for monitoring these connections and for sending and receiving Flower Protocol messages.</p>
</div>
<div id="S4.SS2.SSS1.p4" class="ltx_para">
<p id="S4.SS2.SSS1.p4.1" class="ltx_p"><span id="S4.SS2.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Resource Manager.</span> The resource manager is responsible for monitoring the current device resources and communicating that with the flower client and server.</p>
</div>
<div id="S4.SS2.SSS1.p5" class="ltx_para">
<p id="S4.SS2.SSS1.p5.1" class="ltx_p"><span id="S4.SS2.SSS1.p5.1.1" class="ltx_text ltx_font_bold">Sensor Module.</span> This module is responsible for enable specific data to be used as part of the training tasks. This is done by direct communication with the Data Collection Module of the UI as we describe next.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>User Interface</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">Through the designed UI, participants are able to view the information and description of the MCS task and provide consent in taking part in the campaign. Furthermore, we design the UI with the vision of enabling the participant to indicate their participation criteria such as minimum available resources or stable connectivity (WiFi). Finally the Data Collection module of the UI enables the participant to specifically collect data that is required for the active MCS task. This module therefore directly communicates with the sensor module of the client backend and has access both to hardware resources (e.g., camera, GPS etc) and the on-device database.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Case Study: Obstacle Detection</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section we present one of many possible use cases for our framework. The use case here is motivated by GeoNotify an application that was designed to assist visually impaired with navigation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. GeoNotify is a crowd-sourcing application that notifies the users about the obstacles on the sidewalks and re-routes them to avoid those obstacles using audio messages. To map the obstacles the system relies on crowdsourced information from the users by enabling them to take a photo of sidewalk obstacles and an audio description and submitting it to a centralized repository. The underlying back-end server learns: 1) the GPS location of the obstacle and uses this information to reroute other users; 2) the image representation of the obstacle. The focus of this section is on the latter component of the system which aims to retrain vision models to learn representation of everyday sidewalk obstacles.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Obstacle Detection Model</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">A large scale study by the Royal Institute of Blind People <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> interviewed 500 visually impaired participants for over three months. As part of this report the institute highlights that often the ordinary sidewalk obstacles are the most common causes of injury daily navigation of those with partial vision impairment. This report identifies five most common obstacles that impact visually impaired daily: cars parked on the pavement, advertising-boards, bins and recycling boxes on pavements, street furniture (such as chair and tables). Some of these classes (e.g., chair and table) are common objects that are labeled in image recognition datasets such as ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and thus are detectable using existing Convolutional Neural Network (CNN) models. However, the presentations of these common objects across the world can vary significantly. Others such as boards, sidewalk signs and potholes are specific to the context of this report and are not present as part of ImageNet. Indeed, earlier studies showed that the accuracy of the state-of-the-art models in detecting common sidewalk obstacles ranges between 10-40% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">To enable vision models to learn a diverse representations of the sidewalk objects, we experiment with two state-of-the-art light weight models, namely MobileNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and SqueezeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Furthermore, as the system is designed to rely on the crowds’ contribution to enhance these models, it is important to quantify the number of images that are required to train the models to learn the representation of the sidewalk objects. We thus seek to answer the following research questions:</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">RQ1: How many participants are needed to collaboratively train each model?</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">RQ2: How many photos per participant is required to enhance the model?</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p">RQ3: How much resources per client device is required to collaboratively train the model?</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Experiment Setting</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In order to answer the above research questions, we use FLOAT as underlying framework to simulate the described active crowdsensing task amongst devices. The details of our experiments are as follows:</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">Data.</span> We used the manually curated dataset as published by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. We segmented the training portion of this dataset into equal and uniform distribution amongst the clients in an IID fashion. That is each client has an equal number of photos per class. Our dataset consists of 5 classes with a varying number of images, <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mi id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><ci id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">x</annotation></semantics></math>, per class. We kept the validation part of this dataset, which includes 100 images per each class, as the validation dataset on the server.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">Training.</span> We experiment with pre-trained MobileNet and SqueezeNet models (weights of pretraining on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>) and study the impact of transfer learning of each model on algorithmic and system performance. To do so we freeze the learning on the earlier layers of the model where the weights are transferred from the pre-trained weights and then retrain the final layer of the network (i.e., the fully connected layer) to learn the representations of our domain specific images. We trained the each model for 20 rounds under the FL setting. During each of the 20 rounds, clients perform one epoch of SGD (batch size 32, momentum 0.9 and learning rate of 0.001) before sending the updated model parameters back to the server. The aggregation strategy was configured as FedAvg.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p"><span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_bold">Platform.</span> We ran an instance of the FLOAT framework on a local server with 4 GPU GeForce RTX 2080 Ti with CUDA 9. For answering the first two research questions we <em id="S5.SS2.p4.1.2" class="ltx_emph ltx_font_italic">simulated</em> the clients on the same server each with their own data private repositories. To answer the third research question and compute the system performance on the mobile devices, we ran an end-to-end instance of training on an Android device and measured the system usage of this device.</p>
</div>
<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2201.07980/assets/1.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="138" height="139" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2201.07980/assets/2.png" id="S5.F5.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="138" height="138" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Results of the algorithmic performance of MobileNet (left) and Squeezenet(right) for varying number of clients and images.</figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Experimental Results</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">In this section we present the algorithmic performance and system measurements of the described study.</p>
</div>
<section id="S5.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.1 </span>Algorithmic Performance</h4>

<div id="S5.SS3.SSS1.p1" class="ltx_para">
<p id="S5.SS3.SSS1.p1.5" class="ltx_p">Figure <a href="#S5.F5" title="Figure 5 ‣ 5.2 Experiment Setting ‣ 5 Case Study: Obstacle Detection ‣ Caring Without Sharing: A Federated Learning Crowdsensing Framework for Diversifying Representation of Cities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> reports on the algorithmic performance of MobileNet and Squeezenet respectively for varying number of clients and varying data size. To answer the first research question on how many participants are needed to collaboratively contribute to the model, we can see that as little as 10 clients can improve the accuracy of the model to 0.4-0.8 for MobileNet and 0.6-0.8 for SqueezeNet. To answer the second research question on how many images (<math id="S5.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S5.SS3.SSS1.p1.1.m1.1a"><mi id="S5.SS3.SSS1.p1.1.m1.1.1" xref="S5.SS3.SSS1.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p1.1.m1.1b"><ci id="S5.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S5.SS3.SSS1.p1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p1.1.m1.1c">x</annotation></semantics></math>) each participant requires to have, we can observe that the more images that users have (per class) the quicker the models converges. Particularly in the case of SqueezeNet we can see that the accuracy <span id="S5.SS3.SSS1.p1.5.1" class="ltx_text ltx_font_italic">exponentially</span> increases for <math id="S5.SS3.SSS1.p1.2.m2.1" class="ltx_Math" alttext="x=20" display="inline"><semantics id="S5.SS3.SSS1.p1.2.m2.1a"><mrow id="S5.SS3.SSS1.p1.2.m2.1.1" xref="S5.SS3.SSS1.p1.2.m2.1.1.cmml"><mi id="S5.SS3.SSS1.p1.2.m2.1.1.2" xref="S5.SS3.SSS1.p1.2.m2.1.1.2.cmml">x</mi><mo id="S5.SS3.SSS1.p1.2.m2.1.1.1" xref="S5.SS3.SSS1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS3.SSS1.p1.2.m2.1.1.3" xref="S5.SS3.SSS1.p1.2.m2.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p1.2.m2.1b"><apply id="S5.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S5.SS3.SSS1.p1.2.m2.1.1"><eq id="S5.SS3.SSS1.p1.2.m2.1.1.1.cmml" xref="S5.SS3.SSS1.p1.2.m2.1.1.1"></eq><ci id="S5.SS3.SSS1.p1.2.m2.1.1.2.cmml" xref="S5.SS3.SSS1.p1.2.m2.1.1.2">𝑥</ci><cn type="integer" id="S5.SS3.SSS1.p1.2.m2.1.1.3.cmml" xref="S5.SS3.SSS1.p1.2.m2.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p1.2.m2.1c">x=20</annotation></semantics></math>, making the number of the participants not as important of a factor as when <math id="S5.SS3.SSS1.p1.3.m3.1" class="ltx_Math" alttext="x=10" display="inline"><semantics id="S5.SS3.SSS1.p1.3.m3.1a"><mrow id="S5.SS3.SSS1.p1.3.m3.1.1" xref="S5.SS3.SSS1.p1.3.m3.1.1.cmml"><mi id="S5.SS3.SSS1.p1.3.m3.1.1.2" xref="S5.SS3.SSS1.p1.3.m3.1.1.2.cmml">x</mi><mo id="S5.SS3.SSS1.p1.3.m3.1.1.1" xref="S5.SS3.SSS1.p1.3.m3.1.1.1.cmml">=</mo><mn id="S5.SS3.SSS1.p1.3.m3.1.1.3" xref="S5.SS3.SSS1.p1.3.m3.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p1.3.m3.1b"><apply id="S5.SS3.SSS1.p1.3.m3.1.1.cmml" xref="S5.SS3.SSS1.p1.3.m3.1.1"><eq id="S5.SS3.SSS1.p1.3.m3.1.1.1.cmml" xref="S5.SS3.SSS1.p1.3.m3.1.1.1"></eq><ci id="S5.SS3.SSS1.p1.3.m3.1.1.2.cmml" xref="S5.SS3.SSS1.p1.3.m3.1.1.2">𝑥</ci><cn type="integer" id="S5.SS3.SSS1.p1.3.m3.1.1.3.cmml" xref="S5.SS3.SSS1.p1.3.m3.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p1.3.m3.1c">x=10</annotation></semantics></math>. For MobileNet we see that the model performes very poorly in learning the representation of the objects when participants images is <math id="S5.SS3.SSS1.p1.4.m4.1" class="ltx_Math" alttext="x=10" display="inline"><semantics id="S5.SS3.SSS1.p1.4.m4.1a"><mrow id="S5.SS3.SSS1.p1.4.m4.1.1" xref="S5.SS3.SSS1.p1.4.m4.1.1.cmml"><mi id="S5.SS3.SSS1.p1.4.m4.1.1.2" xref="S5.SS3.SSS1.p1.4.m4.1.1.2.cmml">x</mi><mo id="S5.SS3.SSS1.p1.4.m4.1.1.1" xref="S5.SS3.SSS1.p1.4.m4.1.1.1.cmml">=</mo><mn id="S5.SS3.SSS1.p1.4.m4.1.1.3" xref="S5.SS3.SSS1.p1.4.m4.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p1.4.m4.1b"><apply id="S5.SS3.SSS1.p1.4.m4.1.1.cmml" xref="S5.SS3.SSS1.p1.4.m4.1.1"><eq id="S5.SS3.SSS1.p1.4.m4.1.1.1.cmml" xref="S5.SS3.SSS1.p1.4.m4.1.1.1"></eq><ci id="S5.SS3.SSS1.p1.4.m4.1.1.2.cmml" xref="S5.SS3.SSS1.p1.4.m4.1.1.2">𝑥</ci><cn type="integer" id="S5.SS3.SSS1.p1.4.m4.1.1.3.cmml" xref="S5.SS3.SSS1.p1.4.m4.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p1.4.m4.1c">x=10</annotation></semantics></math> and after 20 rounds of training it leads to only 0.5 accuracy. SqueezeNet on the other hand reaches 0.8 accuracy with the larger number of clients (<math id="S5.SS3.SSS1.p1.5.m5.1" class="ltx_Math" alttext="n=100" display="inline"><semantics id="S5.SS3.SSS1.p1.5.m5.1a"><mrow id="S5.SS3.SSS1.p1.5.m5.1.1" xref="S5.SS3.SSS1.p1.5.m5.1.1.cmml"><mi id="S5.SS3.SSS1.p1.5.m5.1.1.2" xref="S5.SS3.SSS1.p1.5.m5.1.1.2.cmml">n</mi><mo id="S5.SS3.SSS1.p1.5.m5.1.1.1" xref="S5.SS3.SSS1.p1.5.m5.1.1.1.cmml">=</mo><mn id="S5.SS3.SSS1.p1.5.m5.1.1.3" xref="S5.SS3.SSS1.p1.5.m5.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p1.5.m5.1b"><apply id="S5.SS3.SSS1.p1.5.m5.1.1.cmml" xref="S5.SS3.SSS1.p1.5.m5.1.1"><eq id="S5.SS3.SSS1.p1.5.m5.1.1.1.cmml" xref="S5.SS3.SSS1.p1.5.m5.1.1.1"></eq><ci id="S5.SS3.SSS1.p1.5.m5.1.1.2.cmml" xref="S5.SS3.SSS1.p1.5.m5.1.1.2">𝑛</ci><cn type="integer" id="S5.SS3.SSS1.p1.5.m5.1.1.3.cmml" xref="S5.SS3.SSS1.p1.5.m5.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p1.5.m5.1c">n=100</annotation></semantics></math>).</p>
</div>
</section>
<section id="S5.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.2 </span>System Performance</h4>

<div id="S5.SS3.SSS2.p1" class="ltx_para">
<p id="S5.SS3.SSS2.p1.1" class="ltx_p">Finally in terms of resources we measured memory, CPU, and energy consumption of the described experiments on a Samsung S21 Ultra, with 8GB RAM and 8 CPU cores. We experimented running each model with and without Transfer Learning (TF) on this device using a training dataset that is composed of 20 pictures per class. The client only performed the training and no validation was done on the client. Figure <a href="#S5.F6" title="Figure 6 ‣ 5.3.2 System Performance ‣ 5.3 Experimental Results ‣ 5 Case Study: Obstacle Detection ‣ Caring Without Sharing: A Federated Learning Crowdsensing Framework for Diversifying Representation of Cities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates the CPU and memory usage for MobileNet with and without TF respectively.
We observe that the maximum memory usage during the entire training remains low. Indeed our comparison of the two models suggest that the memory usage is approximately between 300MB-700MB (3-7% of the total RAM) for Squeezenet and MobileNet respectively when the models are leveraging transfer learning. Table <a href="#S5.T1" title="Table 1 ‣ 5.3.2 System Performance ‣ 5.3 Experimental Results ‣ 5 Case Study: Obstacle Detection ‣ Caring Without Sharing: A Federated Learning Crowdsensing Framework for Diversifying Representation of Cities" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the memory and training time for both models. In terms of energy consumption, one round of training (of MobileNet without TL) consumed 10.3 mAh on average that corresponds to less than 0.15% of the total available battery on a fully charged device.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Model</th>
<th id="S5.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Time (per one round)</th>
<th id="S5.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Max Memory</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.1.2.1" class="ltx_tr">
<td id="S5.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">MobileNet (TL)</td>
<td id="S5.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">180 s</td>
<td id="S5.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">728 MB</td>
</tr>
<tr id="S5.T1.1.3.2" class="ltx_tr">
<td id="S5.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">MobileNet (No TL)</td>
<td id="S5.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">340 s</td>
<td id="S5.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.8 GB</td>
</tr>
<tr id="S5.T1.1.4.3" class="ltx_tr">
<td id="S5.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">SqueezeNet (TL)</td>
<td id="S5.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7 s</td>
<td id="S5.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">384 MB</td>
</tr>
<tr id="S5.T1.1.5.4" class="ltx_tr">
<td id="S5.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">SqueezeNet (No TL)</td>
<td id="S5.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">32 s</td>
<td id="S5.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1.2GB</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>On device measurements of memory usage and training time for one round of local training. </figcaption>
</figure>
<div id="S5.SS3.SSS2.p2" class="ltx_para">
<p id="S5.SS3.SSS2.p2.1" class="ltx_p">Bringing these results together with the algorithmic performance we can see that our framework is a viable option for the described MCS task and can enhance the representation of the objects in as few as 10 rounds of training with very little participation burden (number of images and computational burden of the devices (time, energy, memory, and CPU).</p>
</div>
<figure id="S5.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2201.07980/assets/with.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="157" height="96" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2201.07980/assets/without.png" id="S5.F6.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="159" height="96" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>CPU and Memory usage of one round of training of MobileNet on Samsung S21 Ultra with and without Transfer Learning.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section we first describe some limitation and future extension to our work before putting forward a future road map on what we believe are the important challenges for the research community to tackle to ensure the adoption of the federated learning beyond theory and opportunities for integration in real-world studies.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Limitation</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">One limitation of the evaluation presented here is that the participation of users was simulated and we did not evaluate our framework on images <span id="S6.SS1.p1.1.1" class="ltx_text ltx_font_italic">collected by</span> actual users who may present a more diverse representation of the objects across the world. Furthermore, as these images were collected from Internet, they might have a have higher quality and fail to represent the heterogeneity of the smart-phones devices, and the impact it might impose on the training.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">Finally, although we designed our framework with paying particular attention to some of the design challenges, there are others that we have not addressed yet. One of such challenges is the impact of the <span id="S6.SS1.p2.1.1" class="ltx_text ltx_font_italic">noisy labels</span>. That is where the participant intentionally (i.e., poison attacks) or unintentionally associate a wrong label with the input data. Noisy label detection is a hard task as they are ubiquitous in the real world and prominent in crowd-sourcing applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. Deep neural networks, including CNNs, have a high capacity to fit noisy labels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. That is, these models can memorize <em id="S6.SS1.p2.1.2" class="ltx_emph ltx_font_italic">easy</em> instances first and gradually adapt to hard instances as training epochs become large. When noisy labels frequently exist, deep learning models will eventually memorize these wrong labels, which leads to poor generalization performance. In our future work we plan to tackle this challenge by integrating a <math id="S6.SS1.p2.1.m1.1" class="ltx_Math" alttext="TrustModule" display="inline"><semantics id="S6.SS1.p2.1.m1.1a"><mrow id="S6.SS1.p2.1.m1.1.1" xref="S6.SS1.p2.1.m1.1.1.cmml"><mi id="S6.SS1.p2.1.m1.1.1.2" xref="S6.SS1.p2.1.m1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p2.1.m1.1.1.1" xref="S6.SS1.p2.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS1.p2.1.m1.1.1.3" xref="S6.SS1.p2.1.m1.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p2.1.m1.1.1.1a" xref="S6.SS1.p2.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS1.p2.1.m1.1.1.4" xref="S6.SS1.p2.1.m1.1.1.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p2.1.m1.1.1.1b" xref="S6.SS1.p2.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS1.p2.1.m1.1.1.5" xref="S6.SS1.p2.1.m1.1.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p2.1.m1.1.1.1c" xref="S6.SS1.p2.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS1.p2.1.m1.1.1.6" xref="S6.SS1.p2.1.m1.1.1.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p2.1.m1.1.1.1d" xref="S6.SS1.p2.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS1.p2.1.m1.1.1.7" xref="S6.SS1.p2.1.m1.1.1.7.cmml">M</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p2.1.m1.1.1.1e" xref="S6.SS1.p2.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS1.p2.1.m1.1.1.8" xref="S6.SS1.p2.1.m1.1.1.8.cmml">o</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p2.1.m1.1.1.1f" xref="S6.SS1.p2.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS1.p2.1.m1.1.1.9" xref="S6.SS1.p2.1.m1.1.1.9.cmml">d</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p2.1.m1.1.1.1g" xref="S6.SS1.p2.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS1.p2.1.m1.1.1.10" xref="S6.SS1.p2.1.m1.1.1.10.cmml">u</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p2.1.m1.1.1.1h" xref="S6.SS1.p2.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS1.p2.1.m1.1.1.11" xref="S6.SS1.p2.1.m1.1.1.11.cmml">l</mi><mo lspace="0em" rspace="0em" id="S6.SS1.p2.1.m1.1.1.1i" xref="S6.SS1.p2.1.m1.1.1.1.cmml">​</mo><mi id="S6.SS1.p2.1.m1.1.1.12" xref="S6.SS1.p2.1.m1.1.1.12.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.1.m1.1b"><apply id="S6.SS1.p2.1.m1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1"><times id="S6.SS1.p2.1.m1.1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1.1"></times><ci id="S6.SS1.p2.1.m1.1.1.2.cmml" xref="S6.SS1.p2.1.m1.1.1.2">𝑇</ci><ci id="S6.SS1.p2.1.m1.1.1.3.cmml" xref="S6.SS1.p2.1.m1.1.1.3">𝑟</ci><ci id="S6.SS1.p2.1.m1.1.1.4.cmml" xref="S6.SS1.p2.1.m1.1.1.4">𝑢</ci><ci id="S6.SS1.p2.1.m1.1.1.5.cmml" xref="S6.SS1.p2.1.m1.1.1.5">𝑠</ci><ci id="S6.SS1.p2.1.m1.1.1.6.cmml" xref="S6.SS1.p2.1.m1.1.1.6">𝑡</ci><ci id="S6.SS1.p2.1.m1.1.1.7.cmml" xref="S6.SS1.p2.1.m1.1.1.7">𝑀</ci><ci id="S6.SS1.p2.1.m1.1.1.8.cmml" xref="S6.SS1.p2.1.m1.1.1.8">𝑜</ci><ci id="S6.SS1.p2.1.m1.1.1.9.cmml" xref="S6.SS1.p2.1.m1.1.1.9">𝑑</ci><ci id="S6.SS1.p2.1.m1.1.1.10.cmml" xref="S6.SS1.p2.1.m1.1.1.10">𝑢</ci><ci id="S6.SS1.p2.1.m1.1.1.11.cmml" xref="S6.SS1.p2.1.m1.1.1.11">𝑙</ci><ci id="S6.SS1.p2.1.m1.1.1.12.cmml" xref="S6.SS1.p2.1.m1.1.1.12">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.1c">TrustModule</annotation></semantics></math> in both the client side (in order to detect noisy labels) and on the server side (in order to select reputable participants).</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Research Road Map</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p"><span id="S6.SS2.p1.1.1" class="ltx_text ltx_font_bold">User Acceptance:</span> We believe first and foremost important challenge with the proposed paradigm shift is to study and assess users’ acceptance. Great amount of literature exists on user’s privacy concerns in various platforms from Social Media <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> to IoT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and MCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. However, little is known on how users will perceive systems that rely on FL schema. Would such schema actually ease their privacy concerns? What type of awareness and education is needed for the users to understand the underlying benefits of such schema? We believe this is an urgent qualitative research question that the research community needs to answer if we are to see more real-world use cases and applications of federated MCS.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p"><span id="S6.SS2.p2.1.1" class="ltx_text ltx_font_bold">Participants Selection: </span> The vast majority of federated methods are passive in the sense that they do not aim to influence which devices participate, or only select the participants based on the available resources (e.g., connected to WiFi and battery level). We believe that if we are to use a federated MCS in real-world use cases, it is important to account for the diversity not only at the point of aggregation but at the point of participant selection. We believe more research initiative is needed to enable participant selection based on alternative metrics, such as fairness criteria <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, that could enable increasing fairness of the overall model.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p"><span id="S6.SS2.p3.1.1" class="ltx_text ltx_font_bold">Human Data Interaction:</span> Finally we believe important conversations and debates are needed to take place if FL systems are to become more applicable in training ML models. For instance how to adapt policies such as <span id="S6.SS2.p3.1.2" class="ltx_text ltx_font_italic">right to be forgotten</span> to federated schema. In other words, “how can a model forget users’ contribution to it”. We believe stepping away from centralized datasets, brings all new set of regulatory challenges and now is the crucial time for the research community to start on creating forums for these types of conversations.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this paper we presented FLOAT, a federated learning framework for adapting active MCS tasks into a privacy-preserving FL schema. Through a use case, we demonstrated how FLOAT can be used to learn a diverse representation of sidewalk obstacles using as little as 10 images per object and 20 rounds of training. We also demonstrated the viability of our proposed framework through measuring resource consumption on an ordinary smart phone.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Aledhari, M., Razzak, R., Parizi, R.M., Saeed, F.: Federated learning: A survey
on enabling technologies, protocols, and applications. IEEE Access
<span id="bib.bib1.1.1" class="ltx_text ltx_font_bold">8</span>, 140699–140725 (2020)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Badii, C., Bellini, P., Difino, A., Nesi, P.: Smart city iot platform
respecting gdpr privacy and security aspects. IEEE Access <span id="bib.bib2.1.1" class="ltx_text ltx_font_bold">8</span>,
23601–23623 (2020)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Baig, Z.A., Szewczyk, P., Valli, C., Rabadia, P., Hannay, P., Chernyshev, M.,
Johnstone, M., Kerai, P., Ibrahim, A., Sansurooah, K., et al.: Future
challenges for smart cities: Cyber-security and digital forensics. Digital
Investigation <span id="bib.bib3.1.1" class="ltx_text ltx_font_bold">22</span>, 3–13 (2017)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Barocas, S., Hardt, M., Narayanan, A.: Fairness and Machine Learning.
fairmlbook.org (2019), <a target="_blank" href="http://www.fairmlbook.org" title="" class="ltx_ref">http://www.fairmlbook.org</a>

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Beigi, G., Liu, H.: A survey on privacy in social media: identification,
mitigation, and applications. ACM Transactions on Data Science
<span id="bib.bib5.1.1" class="ltx_text ltx_font_bold">1</span>(1), 1–38 (2020)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Beutel, D.J., Topal, T., Mathur, A., Qiu, X., Parcollet, T., de Gusmão,
P.P., Lane, N.D.: Flower: A friendly federated learning research framework.
arXiv preprint arXiv:2007.14390 (2020)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Binns, R.: On the apparent conflict between individual and group fairness. In:
Proceedings of the 2020 Conference on Fairness, Accountability, and
Transparency. pp. 514–524 (2020)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A
large-scale hierarchical image database. In: 2009 IEEE conference on computer
vision and pattern recognition. pp. 248–255. Ieee (2009)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Diamantopoulou, V., Androutsopoulou, A., Gritzalis, S., Charalabidis, Y.: An
assessment of privacy preservation in crowdsourcing approaches: Towards gdpr
compliance. In: 2018 12th International Conference on Research Challenges in
Information Science (RCIS). pp. 1–9. IEEE (2018)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Ferreira, D., Kostakos, V.: Aware: Open-source context instrumentation
framework for everyone. URL: https://awareframework. com/.(Zugriff: 02.09.
2020) (2018)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Flask: Flask project (2021), <a target="_blank" href="https://flask.palletsprojects.com/en/2.0.x/" title="" class="ltx_ref">https://flask.palletsprojects.com/en/2.0.x/</a>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Gustarini, M., Wac, K., Dey, A.K.: Anonymous smartphone data collection:
factors influencing the users’ acceptance in mobile crowd sensing. Personal
and Ubiquitous Computing <span id="bib.bib12.1.1" class="ltx_text ltx_font_bold">20</span>(1), 65–82 (2016)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T.,
Andreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks
for mobile vision applications. arXiv preprint arXiv:1704.04861 (2017)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.:
Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb
model size. arXiv preprint arXiv:1602.07360 (2016)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Jiang, J.C., Kantarci, B., Oktug, S., Soyata, T.: Federated learning in smart
city sensing: Challenges and opportunities. Sensors <span id="bib.bib15.1.1" class="ltx_text ltx_font_bold">20</span>(21),  6230
(2020)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Kairouz, P., Mcmahan, B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A.N.,
Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al.: Advances and
open problems in federated learning (2021)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Katevas, K., Haddadi, H., Tokarchuk, L.: Sensingkit: Evaluating the sensor
power consumption in ios devices. In: 2016 12th International Conference on
Intelligent Environments (IE). pp. 222–225. IEEE (2016)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Kim, E., Sterner, J., Mashhadi, A.: A crowd-sourced obstacle detection and
navigation app for visually impaired. In: International Summit Smart City
360°. pp. 571–579. Springer (2020)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Konečnỳ, J., McMahan, H.B., Ramage, D., Richtárik, P.: Federated
optimization: Distributed machine learning for on-device intelligence. arXiv
preprint arXiv:1610.02527 (2016)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Li, T., Sanjabi, M., Beirami, A., Smith, V.: Fair resource allocation in
federated learning. arXiv preprint arXiv:1905.10497 (2019)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Liu, J., Shen, H., Narman, H.S., Chung, W., Lin, Z.: A survey of mobile
crowdsensing techniques: A critical component for the internet of things. ACM
Transactions on Cyber-Physical Systems <span id="bib.bib21.1.1" class="ltx_text ltx_font_bold">2</span>(3), 1–26 (2018)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Mashhadi, A., Sterner, J., Murray, J.: Deep embedded clustering of urban
communities using federated learning (2021)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
McMahan, B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A.:
Communication-efficient learning of deep networks from decentralized data.
In: Artificial Intelligence and Statistics. pp. 1273–1282 (2017)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Preuveneers, D., Rimmer, V., Tsingenopoulos, I., Spooren, J., Joosen, W.,
Ilie-Zudor, E.: Chained anomaly detection models for federated learning: An
intrusion detection case study. Applied Sciences <span id="bib.bib24.1.1" class="ltx_text ltx_font_bold">8</span>(12),  2663
(2018)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Qiu, X., Parcollet, T., Beutel, D., Topal, T., Mathur, A., Lane, N.: Can
federated learning save the planet? In: NeurIPS-Tackling Climate Change with
Machine Learning (2020)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
RIBP: The royal institute for blind people (2016),
<a target="_blank" href="https://www.rnib.org.uk/sites/default/files/Who%20put%20that%20there%20Report%0AFebruary%202015.pdf" title="" class="ltx_ref">https://www.rnib.org.uk/sites/default/files/Who put that there Report
February 2015.pdf</a>

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Savazzi, S., Kianoush, S., Rampa, V., Bennis, M.: A framework for energy and
carbon footprint analysis of distributed and federated edge learning. arXiv
preprint arXiv:2103.10346 (2021)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Smith, M., Szongott, C., Henne, B., Von Voigt, G.: Big data privacy issues in
public social media. In: 2012 6th IEEE international conference on digital
ecosystems and technologies (DEST). pp. 1–6. IEEE (2012)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Tensorflow: Tensorboard (2021), <a target="_blank" href="https://www.tensorflow.org/tensorboard" title="" class="ltx_ref">https://www.tensorflow.org/tensorboard</a>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Xiong, H., Huang, Y., Barnes, L.E., Gerber, M.S.: Sensus: a cross-platform,
general-purpose system for mobile crowdsensing in human-subject studies. In:
Proceedings of the 2016 ACM international joint conference on pervasive and
ubiquitous computing. pp. 415–426 (2016)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Yan, Y., Rosales, R., Fung, G., Subramanian, R., Dy, J.: Learning from multiple
annotators with varying expertise. Machine learning <span id="bib.bib31.1.1" class="ltx_text ltx_font_bold">95</span>(3),
291–327 (2014)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Yu, X., Liu, T., Gong, M., Tao, D.: Learning with biased complementary labels.
In: Proceedings of the European Conference on Computer Vision (ECCV). pp.
68–83 (2018)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Zhang, C., Bengio, S., Hardt, M., Recht, B., Vinyals, O.: Understanding deep
learning requires rethinking generalization. arXiv preprint arXiv:1611.03530
(2016)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Zhong, Y., Kobayashi, M., Matsubara, M., Morishima, A.: A survey of visually
impaired workers in japanese and us crowdsourcing platforms. In Proceedings
of HCOMP (2020)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2201.07979" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2201.07980" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2201.07980">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2201.07980" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2201.07981" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  6 10:17:42 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
