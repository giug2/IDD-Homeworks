<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2201.06629] Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860.</title><meta property="og:description" content="Object detection is increasingly used onboard Unmanned Aerial Vehicles (UAV) for various applications; however, the machine learning (ML) models for UAV-based detection are often validated using data curated for tasks ‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860.">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860.">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2201.06629">

<!--Generated on Fri Mar  8 18:18:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\authorinfo</span>
<p id="p1.2" class="ltx_p">Further author information: (Send correspondence to Eung-Joo Lee)
<br class="ltx_break">Eung-Joo Lee.: E-mail: eungjoo.y.lee@gmail.com</p>
</div>
<h1 class="ltx_title ltx_title_document">Validation of object detection in UAV-based images using synthetic data <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span> This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860.</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Eung-Joo Lee
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">University of Maryland, ECE Department and UMIACS, College Park, MD 20742, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Damon M. Conover
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">DEVCOM Army Research Laboratory, Adelphi, MD
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuvra S. Bhattacharyya
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">University of Maryland, ECE Department and UMIACS, College Park, MD 20742, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Heesung Kwon
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">DEVCOM Army Research Laboratory, Adelphi, MD
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Jason Hill
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Defense Threat Reduction Agency (DTRA), Fort Belvoir, VA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kenneth Evensen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Defense Threat Reduction Agency (DTRA), Fort Belvoir, VA
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Object detection is increasingly used onboard Unmanned Aerial Vehicles (UAV) for various applications; however, the machine learning (ML) models for UAV-based detection are often validated using data curated for tasks unrelated to the UAV application. This is a concern because training neural networks on large-scale benchmarks have shown excellent capability in generic object detection tasks, yet conventional training approaches can lead to large inference errors for UAV-based images. Such errors arise due to differences in imaging conditions between images from UAVs and images in training. To overcome this problem, we characterize boundary conditions of ML models, beyond which the models exhibit rapid degradation in detection accuracy. Our work is focused on understanding the impact of different UAV-based imaging conditions on detection performance by using synthetic data generated using a game engine. Properties of the game engine are exploited to populate the synthetic datasets with realistic and annotated images. Specifically, it enables the fine control of various parameters, such as camera position, view angle, illumination conditions, and object pose. Using the synthetic datasets, we analyze detection accuracy in different imaging conditions as a function of the above parameters. We use three well-known neural network models with different model complexity in our work. In our experiment, we observe and quantify the following: 1) how detection accuracy drops as the camera moves toward the nadir-view region; 2) how detection accuracy varies depending on different object poses, and 3) the degree to which the robustness of the models changes as illumination conditions vary.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Object detection, UAV-based image, Synthetic data generation, Domain adaptation, Model validation.
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Rapid and accurate detection of objects of interest from onboard Unmanned Aerial Vehicles (UAV) has become an increasingly important component of practical solutions for various applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Although machine learning models recently have shown successful results in generic object detection tasks due to large-scale benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, detection in UAV-based images has unique challenges <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. This is primarily because of significant differences in imaging conditions between images from UAVs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and images in the large-scale benchmarks used for training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. To obtain accurate detection results, the training dataset needs to be large and diverse and represent the scenes in which the UAVs will fly <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">For example, a training set should contain thousands of images captured at different distances, viewing angles, orientations, and under different illumination conditions with various targets and backgrounds. To rigorously account for all combinations of these attributes at high levels of granularity is a laborious and costly endeavor. Additionally, manually annotating each image requires additional costs.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To tackle this problem, we focus on an alternative approach that can effectively substitute for the large-scale collection of real data. As an alternative to large-scale UAV-based data collections, synthetic data can be generated using a game engine <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. The game engine that we used is the Unity Real-Time Development Platform, which enables the creation of scenes using various terrains and people 3D models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Within Unity, we also have the flexibility to adjust the visibility and lighting within the scene. In this study, we demonstrate that a synthetic dataset can be used to validate machine learning models.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.2" class="ltx_p">Using our generated synthetic dataset, we characterize the performance of multiple machine learning models, with different levels of architectural and model complexity, with respect to the camera position. This allows us to visualize the expected performance for each classifier for a given distance (<math id="S1.p4.1.m1.1" class="ltx_Math" alttext="\it{d}" display="inline"><semantics id="S1.p4.1.m1.1a"><mi id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><ci id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1">ùëë</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">\it{d}</annotation></semantics></math>) and pitch angle (<math id="S1.p4.2.m2.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S1.p4.2.m2.1a"><mi id="S1.p4.2.m2.1.1" xref="S1.p4.2.m2.1.1.cmml">Œ∏</mi><annotation-xml encoding="MathML-Content" id="S1.p4.2.m2.1b"><ci id="S1.p4.2.m2.1.1.cmml" xref="S1.p4.2.m2.1.1">ùúÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.2.m2.1c">\theta</annotation></semantics></math>) between the camera and the target (see Figure ¬†<a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), as the camera circles and remains pointed inward at the target, as shown in Figure ¬†<a href="#S1.F2" title="Figure 2 ‚Ä£ 1 Introduction ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. In our experiments, we compare the performance of the classifiers to one another and characterize boundary conditions for each model where it exhibits rapid degradation in detection performance.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2201.06629/assets/camera.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="156" height="149" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The relationship between the camera and target object</figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2201.06629/assets/circle.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="180" height="179" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Full 360¬∞ collections of a stationary target by moving a single camera</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Method</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The development of accurate and robust deep learning models requires large amounts of diverse training data representing the environments where the models will be deployed; however, it is not always practical to collect real data in those environments. This could be due to the cost of the data collection, inclement weather, the timeframe in which the models are needed, or lack of access to the targets of interest. Therefore, the ability to augment existing real datasets with data from synthetic scenes constructed to simulate mission-relevant tasks and terrains is desirable. The synthetic scenes in this project were constructed using the Unity game engine, and C# scripts were written to iterate through different combinations of camera positions relative to the target and different sun angles. The resulting data contained multiple targets and target poses and was automatically annotated.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Virtual Environment</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">A game engine consists of a set of tools that allows a developer to construct virtual environments from individual, interacting game objects. The game engine enables the control of aspects of the game objects, such as their appearance and actions, and then renders the result. For this project, the Unity game engine was used to construct 3D scenes from game objects purchased from the Unity Asset Store. For the results shown below, a desert terrain asset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and several human character assets<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> were used, as shown in Figure <a href="#S2.F3" title="Figure 3 ‚Ä£ 2.1 Virtual Environment ‚Ä£ 2 Method ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2201.06629/assets/fig1.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="211" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>PBR Desert Landscape asset from the game developer Banjo (facebook.com/BanjoGameArt) (Left), Citizen‚Äôs Pro 2019 asset package from the AGLOBEX-Mobile design team (aglobex.com) (Right)</figcaption>
</figure>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Simulation</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">Our process for generating a synthetic dataset starts with creating a Unity project and configuring the lighting and camera. A terrain asset and one or more target assets, such as those shown in Figure¬†<a href="#S2.F4" title="Figure 4 ‚Ä£ 2.1.1 Simulation ‚Ä£ 2.1 Virtual Environment ‚Ä£ 2 Method ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, are then added to the terrain. The targets are the objects that we wish to annotate for use in training, testing, or validating deep learning models. Next, a unique tag is assigned to each target asset, and Rigidbody and Skinned Mesh Renderer components are attached. It is the tag that allows the targets to be independently segmented when they are later annotated.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2201.06629/assets/characters3.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="146" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Four different characters from the Citizen‚Äôs Pro 2019 asset package</figcaption>
</figure>
<div id="S2.SS1.SSS1.p2" class="ltx_para">
<p id="S2.SS1.SSS1.p2.1" class="ltx_p">A C# script then controls the position and pose of a camera as it flies circling around the target. At each step, the camera is pointed at the center of the target (LookAt method). The script iterates the altitude of the camera, the radius of the circle, and the angle of the camera relative to the target to produce a range of poses, camera-to-target distances, and camera pitch angles, as shown in Figure <a href="#S2.F5" title="Figure 5 ‚Ä£ 2.1.1 Simulation ‚Ä£ 2.1 Virtual Environment ‚Ä£ 2 Method ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#S2.F6" title="Figure 6 ‚Ä£ 2.1.1 Simulation ‚Ä£ 2.1 Virtual Environment ‚Ä£ 2 Method ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="S2.F5" class="ltx_figure"><img src="/html/2201.06629/assets/rot.png" id="S2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="586" height="169" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The camera flies circling around the target and captures an image every 2¬∞: (1) angle=0¬∞, (2) angle=90¬∞, (3) angle=180¬∞, (4) angle=270¬∞</figcaption>
</figure>
<figure id="S2.F6" class="ltx_figure"><img src="/html/2201.06629/assets/dist.png" id="S2.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="586" height="171" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The camera flies in circles of different radii around the target at different altitudes: (1) radius=20 m, altitude=25m, (2) radius=25m, altitude=40 m, (3) radius=35 m, altitude=25 m, (4) radius=50m, altitude=50 m</figcaption>
</figure>
<div id="S2.SS1.SSS1.p3" class="ltx_para">
<p id="S2.SS1.SSS1.p3.1" class="ltx_p">Additionally, the sun angle was varied so that data could be generated at different times of day, as shown in Figure ¬†<a href="#S2.F7" title="Figure 7 ‚Ä£ 2.1.1 Simulation ‚Ä£ 2.1 Virtual Environment ‚Ä£ 2 Method ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. This resulted in large sets of data that had variations in target position relative to the camera and time of day. For example, in one synthetic data generation trial, for a single virtual character, the altitude of the camera was varied from 5-50 meters in 5 meter increments, the radius of the circle was also varied from 5-30 meters in 5 meter increments, the angle of the camera relative to the character was varied from 0-358¬∞
<br class="ltx_break">in 2¬∞increments, and four different sun angles were simulated for a total of 43,200 images.</p>
</div>
<figure id="S2.F7" class="ltx_figure"><img src="/html/2201.06629/assets/illum.png" id="S2.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="586" height="163" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Data was collected with different sun angles (different times of day)</figcaption>
</figure>
<div id="S2.SS1.SSS1.p4" class="ltx_para">
<p id="S2.SS1.SSS1.p4.1" class="ltx_p">A synthetic data generation trial is a set of images that are produced without any user intervention after the initial setup. The initial setup involves selecting the terrain, the targets, and the target positions and poses. Our desert terrain dataset includes 8 different targets, each in 3 different poses. The 3 poses are standing, prone, and squatting, as shown in Figure <a href="#S2.F8" title="Figure 8 ‚Ä£ 2.1.1 Simulation ‚Ä£ 2.1 Virtual Environment ‚Ä£ 2 Method ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. Therefore, the total number of images across the 24 trials is about 1 million.</p>
</div>
<figure id="S2.F8" class="ltx_figure"><img src="/html/2201.06629/assets/pose.png" id="S2.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="148" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Target poses: (Left) standing, (Center) prone, (Right) squatting</figcaption>
</figure>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Automatic Annotation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">To synthesize the images from the virtual scene, we used a repository called Image Synthesis for Machine Learning<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. The repository contains C# code for generating annotated training sets in Unity. Specifically, the code produces an image segmentation mask where each object is assigned a unique color and the synthetic image, as shown in Figure <a href="#S2.F9" title="Figure 9 ‚Ä£ 2.2 Automatic Annotation ‚Ä£ 2 Method ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure id="S2.F9" class="ltx_figure"><img src="/html/2201.06629/assets/mask_bb.png" id="S2.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="586" height="166" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>(1) Synthetic image, (2) Image segmentation mask, (3) Training data image with bounding box, (4) Annotations associated with the image</figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">To generate the annotations, we have written a Python script to parse each mask file, identify each target, and measure the center coordinates, width, and height of the smallest rectangle that encompasses the target, as shown in Figure <a href="#S2.F9" title="Figure 9 ‚Ä£ 2.2 Automatic Annotation ‚Ä£ 2 Method ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. Additionally, the object label, label category, camera altitude (m), the orientation of the target relative to the camera (degrees), distance of the target from the camera (m), the pitch angle of the camera (degrees), and the number of pixels included in the image segmentation mask are recorded in a single JSON file for each trial.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Experimental Setup</h3>

<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Experimental Datasets</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para">
<p id="S2.SS3.SSS1.p1.1" class="ltx_p">As described in section ¬†<a href="#S2.SS1" title="2.1 Virtual Environment ‚Ä£ 2 Method ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>, we populate synthetic datasets in various camera positions, viewing angles, illumination conditions, and target object appearances and poses. The image frame consists of a 512 √ó 512 pixel array with a background terrain containing desert and mountains, and the target object is placed in the center of each image frame. For our experiment, we use a person class to validate machine learning models. We validated the detection performance of machine learning models as a function of the various image parameters. In our experiment, we use average precision (AP) of an IOU (Intersection of Union) threshold of 50% as the indicator of detection performance.</p>
</div>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Detection Models</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para">
<p id="S2.SS3.SSS2.p1.1" class="ltx_p">For detectors, we use three well-known machine learning models: Tiny-YOLO, YOLOv3, and RetinaNet. These models are representative one-stage detectors that can achieve high-speed inference. Using multiple models, we characterize the performance of detection models, which have different levels of architectural and model complexity.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Results</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We visualize detection performance for each model for a given distance and view angle between the camera and the target object, and compare the performance of the models to one another in different target poses. The analysis is to demonstrate if a trained detector has a bias toward specific poses. Additionally, we present detection performances in four different illumination conditions that are determined by the sun angles. In this work, we refer to four illumination conditions as follows: early morning, around noon, mid-afternoon, and late afternoon. We also present a histogram that describes how detection accuracy changes as the camera flies in circles around the target object as shown in Figure ¬†<a href="#S2.F5" title="Figure 5 ‚Ä£ 2.1.1 Simulation ‚Ä£ 2.1 Virtual Environment ‚Ä£ 2 Method ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Standing Position</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Overall detection performance</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">Figure ¬†<a href="#S3.F10" title="Figure 10 ‚Ä£ 3.1.1 Overall detection performance ‚Ä£ 3.1 Standing Position ‚Ä£ 3 Experimental Results ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> illustrates detection performance for the standing position in different imaging conditions. In the figure, detection accuracy is binned by the camera height and the radius of the circular path of the camera as it moves around the object within the image. We also provide the mean of AP values in each illumination condition for performance comparisons.</p>
</div>
<figure id="S3.F10" class="ltx_figure"><img src="/html/2201.06629/assets/s_l.png" id="S3.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="550" height="425" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Detection performance of the three models in the standing position with different illumination conditions. Each bar in the plots indicates an AP value at the corresponding camera height and radius of the circle. The numbers in the parentheses are mAP (mean Average Precision) values.</figcaption>
</figure>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">Figure ¬†<a href="#S3.F11" title="Figure 11 ‚Ä£ 3.1.1 Overall detection performance ‚Ä£ 3.1 Standing Position ‚Ä£ 3 Experimental Results ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> shows the overall detection accuracy from the three detectors in the standing position. The detection results of all three models are unsatisfactory for images that are captured with a high viewing angle (approaches the nadir view). This may be partly the result of the training dataset not containing nadir or near-nadir views, or bird-eye views, of image scenes captured by the camera at high altitudes and on the circles of small radii. In the same dataset, the detection results for images captured from short distances have relatively high AP values. This is because the target object takes up a large percentage of the image frame. Additionally, the detection accuracy increases for a given altitude as the camera is moved farther away. This is likely because the pre-trained detection model uses ground imagery, so the detection accuracy increases as the images appear more similar to the image characteristics of ground imagery. Figure ¬†<a href="#S3.F11" title="Figure 11 ‚Ä£ 3.1.1 Overall detection performance ‚Ä£ 3.1 Standing Position ‚Ä£ 3 Experimental Results ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> shows that the overall performance of the three models that were compared and indicates that RetinaNet is more robust than that of YOLO3 and Tiny-Yolo in general due to advanced model architecture requiring more computational resources.</p>
</div>
<figure id="S3.F11" class="ltx_figure"><img src="/html/2201.06629/assets/s1.png" id="S3.F11.g1" class="ltx_graphics ltx_centering ltx_img_square" width="299" height="255" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Overall detection performance in the standing position. 1) large target object, 2) nadir-view image, 3) small target object, 4) eye-level-view image</figcaption>
</figure>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p">The overall detection results can be classified into four large regions: 1) large target images, 2) nadir-view images, 3) small target images, and 4) eye-level-view images. In the first region, the target objects are large where the image frames are captured at a low altitude and small radius. In another region, target objects are small where the image frames are captured at a high altitude and large radius. Nadir-view images are those captured when the camera is at a high altitude and the circle radius is small. Eye-level-view images are those captured when the camera is at a low altitude and the circle radius is large and are characterized by small pitch angles. Image frames from the eye-level view are well covered by large benchmark datasets, resulting in high overall detection performance.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Angular dependency plot</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">Figure ¬†<a href="#S3.F12" title="Figure 12 ‚Ä£ 3.1.2 Angular dependency plot ‚Ä£ 3.1 Standing Position ‚Ä£ 3 Experimental Results ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> is a histogram illustrating the number of positive detections at each camera view angle. The only variable that changes in each histogram is the camera angle relative to the object within the image as the camera moves along the circle. There is a bias in the detection results towards the person in the front and back view, which may indicate that a similar image dataset is included in the dataset used for training the model.</p>
</div>
<figure id="S3.F12" class="ltx_figure"><img src="/html/2201.06629/assets/s_a.png" id="S3.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="432" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Angular dependency plots for the standing position: H and R indicate height and radius, respectively. 1) nadir view, 2) outside nadir view</figcaption>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Squatting Position</h3>

<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Overall detection performance</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Figure ¬†<a href="#S3.F13" title="Figure 13 ‚Ä£ 3.2.1 Overall detection performance ‚Ä£ 3.2 Squatting Position ‚Ä£ 3 Experimental Results ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> shows the detection performance in the squatting position with four different illumination conditions.</p>
</div>
<figure id="S3.F13" class="ltx_figure"><img src="/html/2201.06629/assets/q_l.png" id="S3.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="424" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Detection performances of three models in the squatting position with different illumination conditions. Each bar in the plots indicates an AP value at the corresponding camera height and radius of the circle. The numbers in the parentheses are mAP (mean Average Precision) values.</figcaption>
</figure>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">As presented in Figure ¬†<a href="#S3.F14" title="Figure 14 ‚Ä£ 3.2.1 Overall detection performance ‚Ä£ 3.2 Squatting Position ‚Ä£ 3 Experimental Results ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>, overall detection performance in the squatting position has similar results to those observed for the standing position as a function of height and radius parameters. However, the overall detection accuracy is lower because the size of target object is smaller in the squatting position, which degrades the detection performance, especially in long-distance and steep viewing angle conditions.</p>
</div>
<figure id="S3.F14" class="ltx_figure"><img src="/html/2201.06629/assets/q1.png" id="S3.F14.g1" class="ltx_graphics ltx_centering ltx_img_square" width="299" height="267" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Overall detection performance in the squatting position. 1) large target object, 2) nadir-view image, 3) small target object, 4) eye-level-view image</figcaption>
</figure>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Angular dependency plot</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Figure ¬†<a href="#S3.F15" title="Figure 15 ‚Ä£ 3.2.2 Angular dependency plot ‚Ä£ 3.2 Squatting Position ‚Ä£ 3 Experimental Results ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> presents the number of positive detection at each camera view angle in the squatting position.</p>
</div>
<figure id="S3.F15" class="ltx_figure"><img src="/html/2201.06629/assets/q_a.png" id="S3.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="402" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Angular dependency plots for the squatting position: H and R indicate height and radius, respectively. 1) nadir view, 2) outside nadir view</figcaption>
</figure>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Prone Position</h3>

<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Overall detection performance</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">Figure ¬†<a href="#S3.F16" title="Figure 16 ‚Ä£ 3.3.1 Overall detection performance ‚Ä£ 3.3 Prone Position ‚Ä£ 3 Experimental Results ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> presents the detection performance of three models in the prone position with different illumination conditions.</p>
</div>
<figure id="S3.F16" class="ltx_figure"><img src="/html/2201.06629/assets/l_l.png" id="S3.F16.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="423" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Detection performances of three models in the prone position with different illumination conditions. Each bar in the plots indicates an AP value at the corresponding camera height and radius of the circle. The numbers in the parentheses are mAP (mean Average Precision) values.</figcaption>
</figure>
<div id="S3.SS3.SSS1.p2" class="ltx_para">
<p id="S3.SS3.SSS1.p2.1" class="ltx_p">Figure ¬†<a href="#S3.F17" title="Figure 17 ‚Ä£ 3.3.1 Overall detection performance ‚Ä£ 3.3 Prone Position ‚Ä£ 3 Experimental Results ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a> illustrates the overall detection accuracy of three models in the prone position. As illustrated in Figure ¬†<a href="#S3.F17" title="Figure 17 ‚Ä£ 3.3.1 Overall detection performance ‚Ä£ 3.3 Prone Position ‚Ä£ 3 Experimental Results ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>, the prone position detection performance over the non-nadir-view region shows a significant reduction in accuracy compared to that of the standing and squatting positions. This is primarily because the prone position is a relatively unique pose unfamaliar to the neural network models pretrained on large-scale benchmarks.</p>
</div>
<figure id="S3.F17" class="ltx_figure"><img src="/html/2201.06629/assets/l1.png" id="S3.F17.g1" class="ltx_graphics ltx_centering ltx_img_square" width="299" height="267" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Overall detection performance in the prone position. 1) large target object, 2) nadir-view image, 3) small target object, 4) eye-level-view image</figcaption>
</figure>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Angular dependency plot</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">Figure ¬†<a href="#S3.F18" title="Figure 18 ‚Ä£ 3.3.2 Angular dependency plot ‚Ä£ 3.3 Prone Position ‚Ä£ 3 Experimental Results ‚Ä£ Validation of object detection in UAV-based images using synthetic data 1footnote 11footnote 1 This a pre-publication draft of a paper that is published in the Proceedings of SPIE Defense and Commercial Sensing. The final version of the paper is available from the SPIE digital library. Please cite as: E.-J. Lee, D. Conover, H. Kwon, S. S. Bhattacharyya, J. Hill, and K. Evensen, ‚ÄùValidation of object detection in UAV-based images using synthetic data‚Äù, Proceedings Volume 11746, Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications III; 117462A (2021) https://doi.org/10.1117/12.2586860." class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a> presents detection results for angular dependency in the prone position. We observe that there are biased detection results towards the 180 ¬∞position both in nadir view and outside of nadir view image frames. This is because the target object looks like the person in the back-side view, which is contained in the training dataset.</p>
</div>
<figure id="S3.F18" class="ltx_figure"><img src="/html/2201.06629/assets/l_a.png" id="S3.F18.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="432" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>Angular dependency plots for the prone position: H and R indicate height and radius, respectively. 1) nadir view, 2) outside nadir view</figcaption>
</figure>
<div id="S3.SS3.SSS2.p2" class="ltx_para">
<p id="S3.SS3.SSS2.p2.1" class="ltx_p">Our analysis has shown that the detection performance of all three models are lower for image frames collected directly above the target object. The results indicate that the models should be retrained if the goal is to use them with aerial imagery. We also observe that lower illumination conditions degrade the overall detection performance.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this study, we generate synthetic data using a game engine to characterize detection accuracy of machine learning models in various conditions of UAV-based imaging systems. By applying neural network models with different model complexity to the synthetic data, we quantitatively show how detection accuracy varies as imaging conditions change. Additionally, we characterize boundary conditions for the neural network models beyond which the models exhibit rapid degradation in detection accuracy. The proposed work provides valuable information regarding the accuracy and usability of UAV-based classifiers onboard UAVs at the edge by characterizing practical limits under which the classifiers can be reliably applied. Also, our analysis presented in this study will allow the user to select the optimal classifier for a given set of imaging parameters.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Acknowledgment</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This research was sponsored by the Defense Threat Reduction Agency (DTRA). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or DTRA.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Semsch, E., Jakob, M., Pavlicek, D., and Pechoucek, M., ‚ÄúAutonomous uav
surveillance in complex urban environments,‚Äù in [<span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">2009 IEEE/WIC/ACM
International Joint Conference on Web Intelligence and Intelligent Agent
Technology</span>‚ÄÜ], <span id="bib.bib1.2.2" class="ltx_text ltx_font_bold">2</span>, 82‚Äì85, IEEE (2009).

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Honkavaara, E., Saari, H., Kaivosoja, J., P√∂l√∂nen, I., Hakala, T.,
Litkey, P., M√§kynen, J., and Pesonen, L., ‚ÄúProcessing and assessment of
spectrometric, stereoscopic imagery collected using a lightweight uav
spectral camera for precision agriculture,‚Äù <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Remote Sensing</span>¬†<span id="bib.bib2.2.2" class="ltx_text ltx_font_bold">5</span>(10), 5006‚Äì5039 (2013).

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Erdelj, M. and Natalizio, E., ‚ÄúUav-assisted disaster management: Applications
and open issues,‚Äù in [<span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">2016 international conference on computing,
networking and communications (ICNC)</span>‚ÄÜ], 1‚Äì5,
IEEE (2016).

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
VidalMata, R.¬†G., Banerjee, S., RichardWebster, B., Albright, M., Davalos, P.,
McCloskey, S., Miller, B., Tambo, A., Ghosh, S., Nagesh, S., et¬†al.,
‚ÄúBridging the gap between computational photography and visual
recognition,‚Äù <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1901.09482</span> (2019).

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Farhadi, A. and Redmon, J., ‚ÄúYolov3: An incremental improvement,‚Äù <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Computer Vision and Pattern Recognition, cite as</span> (2018).

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Doll√°r, P., ‚ÄúFocal loss
for dense object detection,‚Äù in [<span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international
conference on computer vision</span>‚ÄÜ], 2980‚Äì2988
(2017).

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Lee, H., Eum, S., and Kwon, H., ‚ÄúMe r-cnn: Multi-expert r-cnn for object
detection,‚Äù <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Image Processing</span>¬†<span id="bib.bib7.2.2" class="ltx_text ltx_font_bold">29</span>,
1030‚Äì1044 (2019).

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick, R., and Sun, J., ‚ÄúFaster r-cnn: Towards real-time
object detection with region proposal networks,‚Äù <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:1506.01497</span> (2015).

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Wu, Z., Suresh, K., Narayanan, P., Xu, H., Kwon, H., and Wang, Z., ‚ÄúDelving
into robust object detection from unmanned aerial vehicles: A deep nuisance
disentanglement approach,‚Äù in [<span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF
International Conference on Computer Vision</span>‚ÄÜ],
1201‚Äì1210 (2019).

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Du, D., Qi, Y., Yu, H., Yang, Y., Duan, K., Li, G., Zhang, W., Huang, Q., and
Tian, Q., ‚ÄúThe unmanned aerial vehicle benchmark: Object detection and
tracking,‚Äù in [<span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer
Vision (ECCV)</span>‚ÄÜ], 370‚Äì386 (2018).

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Zhu, P., Wen, L., Du, D., Bian, X., Ling, H., Hu, Q., Nie, Q., Cheng, H., Liu,
C., Liu, X., et¬†al., ‚ÄúVisdrone-det2018: The vision meets drone object
detection in image challenge results,‚Äù in [<span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proceedings of the European
Conference on Computer Vision (ECCV) Workshops</span>‚ÄÜ],
0‚Äì0 (2018).

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Bozcan, I. and Kayacan, E., ‚ÄúAu-air: A multi-modal unmanned aerial vehicle
dataset for low altitude traffic surveillance,‚Äù in [<span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">2020 IEEE
International Conference on Robotics and Automation
(ICRA)</span>‚ÄÜ], 8504‚Äì8510, IEEE (2020).

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Barekatain, M., Mart√≠, M., Shih, H.-F., Murray, S., Nakayama, K., Matsuo,
Y., and Prendinger, H., ‚ÄúOkutama-action: An aerial view video dataset for
concurrent human action detection,‚Äù in [<span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition
workshops</span>‚ÄÜ], 28‚Äì35 (2017).

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Everingham, M., Eslami, S.¬†A., Van¬†Gool, L., Williams, C.¬†K., Winn, J., and
Zisserman, A., ‚ÄúThe pascal visual object classes challenge: A
retrospective,‚Äù <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">International journal of computer vision</span>¬†<span id="bib.bib14.2.2" class="ltx_text ltx_font_bold">111</span>(1),
98‚Äì136 (2015).

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
Doll√°r, P., and Zitnick, C.¬†L., ‚ÄúMicrosoft coco: Common objects in
context,‚Äù in [<span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">European conference on computer
vision</span>‚ÄÜ], 740‚Äì755, Springer (2014).

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L., ‚ÄúImagenet:
A large-scale hierarchical image database,‚Äù in [<span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">2009 IEEE conference on
computer vision and pattern recognition</span>‚ÄÜ],
248‚Äì255, Ieee (2009).

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Narayanan, P., Borel-Donohue, C., Lee, H., Kwon, H., and Rao, R., ‚ÄúA real-time
object detection framework for aerial imagery using deep neural networks and
synthetic training images,‚Äù in [<span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Signal Processing, Sensor/Information
Fusion, and Target Recognition XXVII</span>‚ÄÜ], <span id="bib.bib17.2.2" class="ltx_text ltx_font_bold">10646</span>, 1064614, International Society for Optics and Photonics (2018).

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Tremblay, J., Prakash, A., Acuna, D., Brophy, M., Jampani, V., Anil, C., To,
T., Cameracci, E., Boochoon, S., and Birchfield, S., ‚ÄúTraining deep networks
with synthetic data: Bridging the reality gap by domain randomization,‚Äù in
[<span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition Workshops</span>‚ÄÜ], 969‚Äì977 (2018).

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Kar, A., Prakash, A., Liu, M.-Y., Cameracci, E., Yuan, J., Rusiniak, M., Acuna,
D., Torralba, A., and Fidler, S., ‚ÄúMeta-sim: Learning to generate synthetic
datasets,‚Äù in [<span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>‚ÄÜ], 4551‚Äì4560 (2019).

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Saleh, F.¬†S., Aliakbarian, M.¬†S., Salzmann, M., Petersson, L., and Alvarez,
J.¬†M., ‚ÄúEffective use of synthetic data for urban scene semantic
segmentation,‚Äù in [<span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer
Vision (ECCV)</span>‚ÄÜ], 84‚Äì100 (2018).

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Yan, J., Lee, E.-J., Conover, D., and Kwon, H., ‚ÄúSynthetic dataset generation
and adaptation for human detection,‚Äù Tech. Rep. ARL-TR-9112, US Army
Research Laboratory (2020).

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Banjo, ‚ÄúPbr desert landscape,‚Äù (2017).

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
AGLOBEX-Mobile, ‚ÄúCitizens pro 2019,‚Äù (2020).

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
U3DC, ‚ÄúMl-imagesynthesis,‚Äù (2017).

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2201.06628" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2201.06629" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2201.06629">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2201.06629" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2201.06630" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  8 18:18:55 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
