<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2003.10286] PathVQA: 30000+ Questions for Medical Visual Question Answering</title><meta property="og:description" content="Is it possible to develop an “AI Pathologist" to pass the board-certified examination of the American Board of Pathology? To achieve this goal, the first step is to create a visual question answering (VQA) dataset wher…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PathVQA: 30000+ Questions for Medical Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="PathVQA: 30000+ Questions for Medical Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2003.10286">

<!--Generated on Thu Mar  7 07:44:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">PathVQA: 30000+ Questions for Medical Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Xuehai He 
<br class="ltx_break">University of California San Diego
<br class="ltx_break"><span id="id1.1.id1" class="ltx_ERROR undefined">\And</span>Yichen Zhang 
<br class="ltx_break">University of California San Diego
<br class="ltx_break"><span id="id2.2.id2" class="ltx_ERROR undefined">\And</span>Luntian Mou 
<br class="ltx_break">Beijing University of Technology
<br class="ltx_break"><span id="id3.3.id3" class="ltx_ERROR undefined">\And</span>Eric Xing 
<br class="ltx_break">Carnegie Mellon University
<br class="ltx_break"><span id="id4.4.id4" class="ltx_ERROR undefined">\And</span>Pengtao Xie 
<br class="ltx_break">University of California San Diego
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Is it possible to develop an “AI Pathologist" to pass the board-certified examination of the American Board of Pathology? To achieve this goal, the first step is to create a visual question answering (VQA) dataset where the AI agent is presented with a pathology image together with a question and is asked to give the correct answer. Our work makes the first attempt to build such a dataset. Different from creating general-domain VQA datasets where the images are widely accessible and there are many crowdsourcing workers available and capable of generating question-answer pairs, developing a medical VQA dataset is much more challenging. First, due to privacy concerns, pathology images are usually not publicly available. Second, only well-trained pathologists can understand pathology images, but they barely have time to help create datasets for AI research. To address these challenges, we resort to pathology textbooks and online digital libraries. We develop a semi-automated pipeline to extract pathology images and captions from textbooks and generate question-answer pairs from captions using natural language processing. We collect 32,799 open-ended questions from 4,998 pathology images where each question is manually checked to ensure correctness. To our best knowledge, this is the first dataset for pathology VQA. Our dataset will be released publicly to promote research in medical VQA.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.1" class="ltx_p"><em id="p1.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">K</em><span id="p1.1.2" class="ltx_text ltx_font_bold">eywords</span> Visual question answering, dataset, pathology, healthcare</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Pathology studies the causes and effects of diseases or injuries. It underpins every aspect of patient care, from diagnostic testing and treatment advice to using cutting-edge genetic technologies and preventing diseases. Medical professionals practicing pathology are called pathologists, who examine bodies and body tissues. To become a board-certificated pathologist in the US, a medical professional needs to pass a certification examination organized by the American Board of Pathology (ABP), which is a very challenging task. We are interested in asking: whether an artificial intelligence (AI) system can be developed to pass the ABP examination? It is an important step towards achieving AI-aided clinical decision support and clinical education.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Among the ABP test questions, one major type is to understand the pathology images. Given a pathology image and a question, the examinees are asked to select a correct answer. Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows an example. To train an AI system to pass this exam, we need to collect a dataset containing questions similar to those in the ABP test. ABP provides some sample questions, but they are too few to be useful for training data-driven models. Some commercial institutes provide a larger number of practice questions, but they are very expensive to buy and they cannot be shared with the public due to copyright issues.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2003.10286/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="300" height="74" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An example of ABP test questions</figcaption>
</figure>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">To address these limitations, we aim to create a pathology visual question answering (VQA) dataset that contains questions similar to those in the ABP tests and can be shared with the broad research community on AI for healthcare. To our best knowledge, this is the first dataset for pathology VQA. VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is an interdisciplinary research problem that has drawn extensive attention recently. Given an image (e.g., an image showing a dog is chasing a ball) and a question asked about the visual content of the image (e.g., “what is the dog chasing?"), VQA aims to develop AI algorithms to infer the correct answer (e.g., “ball"). VQA requires a deep comprehension of both images and textual questions, as well as the relationship between visual objects and textual entities, which is technically very demanding.
While there have been several datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> for general domain VQA, datasets for medical VQA are very rare.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">It is much more challenging to build medical VQA datasets than general domain VQA datasets. First, many human workers in crowdsourcing platforms such as Amazon Mechanical Turk are available to generate questions and answers from general domain images. These images contain contents (e.g., dog, cat, lake) easily understandable to human. There is almost no barrier to comprehend the images, ask proper questions about the visual objects, and give correct answers. However, medical images such as pathology images are highly domain-specific, which can only be interpreted by well-educated medical professionals. It is very difficult and expensive to hire medical professionals to help create medical VQA datasets. Second, to create a VQA dataset, one first needs to collect an image dataset. While images in the general domain are pervasive, medical images are very difficult to obtain due to privacy concerns.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2003.10286/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="231" height="228" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An example of image/caption pairs from the “Textbook of Pathology”</figcaption>
</figure>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">To address these challenges, we resort to pathology textbooks, especially those that are freely accessible online, as well as online digital libraries. These textbooks contain a lot of pathology images, covering the entire domain of pathology. Each image has a caption that describes pathological findings present in the image (as shown in Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). The caption is carefully worded and clinically precise. We extract images and captions from the textbooks and online digital libraries and develop a semi-automated pipeline to generate question-answer pairs from each caption. We have manually checked the automatically-generated questions and answers and fixed small grammatical issues. In the end, we collected a pathology VQA dataset containing 4,998 images and 32,799 question-answer pairs.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">The major contributions of this paper are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We create a pathology visual question answering (VQA) dataset containing 4998 pathology images and 32,799 question-answer pairs to foster the research of medical VQA. To our best knowledge, this is the first dataset for pathology VQA.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We develop an semi-automated pipeline to efficiently create medical VQA datasets from medical textbooks and online digital libraries. Our pipeline can be widely applied to other medical imaging domains beyond pathology, such as radiology, ultrasound, etc.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i3.p1.1" class="ltx_p">We apply several well-established and state-of-the-art VQA methods to our dataset and generate a set of baseline results for other researchers to benchmark with.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p7" class="ltx_para ltx_noindent">
<p id="S1.p7.1" class="ltx_p">The rest of the paper is organized as follows. Section 2 presents an overview of existing VQA datasets. Section 3 describes our pipeline for constructing pathology VQA datasets from pathology textbooks and online digital libraries. Section 4 presents the statistics of our dataset. Section 5 introduces baselines VQA models and results achieved on our dataset. Section 6 concludes the paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Datasets</h3>

<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of VQA datasets</figcaption>
<div id="S2.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:264.3pt;height:145.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-14.7pt,8.1pt) scale(0.9,0.9) ;">
<table id="S2.T1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></td>
<td id="S2.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Domain</td>
<td id="S2.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"># images</td>
<td id="S2.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"># QA pairs</td>
<td id="S2.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Answer type</td>
</tr>
<tr id="S2.T1.1.1.2.2" class="ltx_tr">
<td id="S2.T1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">DAQUAR</td>
<td id="S2.T1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">General</td>
<td id="S2.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1,449</td>
<td id="S2.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12,468</td>
<td id="S2.T1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Open</td>
</tr>
<tr id="S2.T1.1.1.3.3" class="ltx_tr">
<td id="S2.T1.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">VQA</td>
<td id="S2.T1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">General</td>
<td id="S2.T1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">204K</td>
<td id="S2.T1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">614K</td>
<td id="S2.T1.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Open/MC</td>
</tr>
<tr id="S2.T1.1.1.4.4" class="ltx_tr">
<td id="S2.T1.1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">VQA v2</td>
<td id="S2.T1.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">General</td>
<td id="S2.T1.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">204K</td>
<td id="S2.T1.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.1M</td>
<td id="S2.T1.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Open/MC</td>
</tr>
<tr id="S2.T1.1.1.5.5" class="ltx_tr">
<td id="S2.T1.1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">COCO-QA</td>
<td id="S2.T1.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">General</td>
<td id="S2.T1.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">123K</td>
<td id="S2.T1.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">118K</td>
<td id="S2.T1.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Open/MC</td>
</tr>
<tr id="S2.T1.1.1.6.6" class="ltx_tr">
<td id="S2.T1.1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">CLEVR</td>
<td id="S2.T1.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">General</td>
<td id="S2.T1.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">100K</td>
<td id="S2.T1.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">999K</td>
<td id="S2.T1.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Open</td>
</tr>
<tr id="S2.T1.1.1.7.7" class="ltx_tr">
<td id="S2.T1.1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">VQA-Med</td>
<td id="S2.T1.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Medical</td>
<td id="S2.T1.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">4,200</td>
<td id="S2.T1.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">15,292</td>
<td id="S2.T1.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Open/MC</td>
</tr>
<tr id="S2.T1.1.1.8.8" class="ltx_tr">
<td id="S2.T1.1.1.8.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">VQA-RAD</td>
<td id="S2.T1.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Medical</td>
<td id="S2.T1.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">315</td>
<td id="S2.T1.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3,515</td>
<td id="S2.T1.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Open/MC</td>
</tr>
<tr id="S2.T1.1.1.9.9" class="ltx_tr">
<td id="S2.T1.1.1.9.9.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Ours</td>
<td id="S2.T1.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Medical</td>
<td id="S2.T1.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">4,998</td>
<td id="S2.T1.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">32,799</td>
<td id="S2.T1.1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Open</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">To our best knowledge, there are two existing datasets for medical visual question
answering.
The VQA-Med <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> dataset is created on 4,200 radiology images and has 15,292 question-answer pairs. There are four categories of clinical questions: modality, plane, organ system, and abnormality. For the first three categories, the QA is in multiple-choice (MC) style where the number of possible answers is fixed (36, 16, and 10 respectively). Consequently, the QA tasks can be equivalently formulated as multi-way classification problems with 36, 16, and 10 classes respectively. This makes the difficulty of this dataset significantly lower. Questions in the abnormality category are truly challenging open-ended questions. However, there are only 2408 such questions (15.7%). VQA-RAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is a manually-crafted dataset where questions and answers are given by clinicians on radiology images. It has 3515 questions of 11 types, e.g. modality, plane, etc. 58% of the questions are in MC style and the rest are open-ended.
Our dataset differs from VQA-Med and VQA-RAD in two-fold. First, our dataset is about pathology while VQA-Med and VQA-RAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> are both about radiology. Second, our dataset is a truly challenging QA dataset where most of the questions are open-ended while in VQA-Med and VQA-RAD the majority of questions have a fixed number of candidate answers and can be answered by multi-way classification. Besides, the number of questions in our dataset is much larger than that in VQA-Med and VQA-RAD.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p">A number of visual question answering datasets have been developed in the general domain. DAQUAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> is built on top of the NYU-Depth V2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> which contains RGBD images of indoor scenes. DAQUAR consists of (1) synthetic question-answer pairs that are automatically generated based on textual templates and (2) human-created question-answer pairs produced by five annotators. The VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is developed on real images in MS COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and abstract scene images
in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. The
question-answer pairs are created by human annotators who are encouraged to ask “interesting" and “diverse" questions. VQA v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> is extended from the VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> dataset to achieve more balance between visual and textual information, by collecting complementary images in a way that each question is associated with a pair of similar images with different answers. In the COCO-QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> dataset, the question-answer pairs are automatically generated from image captions based on syntactic parsing and linguistic rules. CLEVR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> is a dataset developed on rendered images of spatially related objects (including cube, sphere, and cylinder) with different sizes, materials, and colors. The locations and attributes of objects are annotated for each image. The questions are automatically generated from the annotations.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.1" class="ltx_p">Table <a href="#S2.T1" title="Table 1 ‣ 2.1 Datasets ‣ 2 Related Works ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents a comparison of different VQA datasets. The first five datasets are in the general domain while the last three are in the medical domain. Not surprisingly, the size of general-domain datasets (including the number of images and question-answer pairs) is much larger than that of medical datasets since general-domain images are much more available publicly and there are many qualified human annotators to generate QA pairs on general images.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Automatic Construction of Question-Answer Pairs</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">Existing datasets have used automated methods for constructing question-answer pairs. In DAQUAR, questions are generated with templates, such as “How many {object} are in {image_id}?". These templates are instantiated with ground-truth facts from the database. In COCO-QA, the authors develop a question generation algorithm based on the Stanford syntactic parser <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, and they form four types of questions—“object", “number", “color", and “location" using hand-crafted rules.
In CLEVR, the locations and attributes of objects in each image are fully annotated, based on which the questions are generated by an automated algorithm. Their algorithm cannot be applied to natural images where detailed annotation of objects and scenes are very difficult to obtain. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, the authors develop a conditional auto-encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> model to automatically generate questions from images. To train such a model, image-question pairs are needed, which incurs a chicken-and-egg problem: the goal is to generate questions, but realizing this goal needs generated questions.
In VQA-Med, the authors collect medical images along with associated side information (e.g., captions, modalities, planes) from the MedPix<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://medpix.nlm.nih.gov</span></span></span> database and generate question-answer pairs based on manually-defined patterns in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
To ensure correctness of questions in the test set, two doctors were asked to perform manual validation.
</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset Collection</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">We develop a semi-automated pipeline to generate a pathology VQA dataset from pathology textbooks and online digital libraries. We manually check the automatically-generated question-answer pairs to fix grammatical errors. The automated pipeline consists of two steps: (1) extracting pathology images and their captions from electronic pathology textbooks and the Pathology Education Informational Resource (PEIR) Digital Library<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>http://peir.path.uab.edu/library/index.php?/category/2</span></span></span> website; (2) generating questions-answer pairs from captions.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Extracting Pathology Images and Captions</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.3" class="ltx_p">Given a pathology textbook that is in the PDF format and available online publicly, we use two third-party tools PyPDF2 <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/mstamy2/PyPDF2</span></span></span> and PDFMiner<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://github.com/pdfminer/pdfminer.six</span></span></span> to extract images and the associated captions therefrom. PyPDF2 provides APIs to access the “Resources" object in each PDF page where the “XObject" gives information about images.
PDFMiner allows one to obtain text along with its exact location in a page.
To extract image captions from text in each page, we use regular expressions to search for snippets with prefixes of “Fig." or “Figure" followed by figure numbers and caption texts. For a page containing multiple images, we order them based on their locations; the same for the captions. Images and locations are matched based on their order.
Given an online pathology digital library such as PEIR, we use two third-party tools Requests<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://requests.readthedocs.io/en/master/</span></span></span> and Beautiful Soup<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://www.crummy.com/software/BeautifulSoup/</span></span></span> to crawl images and the associated captions. Requests is an HTTP library built using Python and provides APIs to send HTTP/1.1 requests. Beautiful Soup generates the ‘http.parser’ and can access the urls and tags of the images on the website pages. Given a set of urls, we use Requests to read website pages and use Beautiful Soup to find images under the targeted HTML tags including the Content Division element <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\left&lt;div\right&gt;" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.2.cmml"><mo id="S3.SS1.p1.1.m1.1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.1.cmml">⟨</mo><mrow id="S3.SS1.p1.1.m1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.1.1.2.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.p1.1.m1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.1.1.1a" xref="S3.SS1.p1.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.p1.1.m1.1.1.1.1.4" xref="S3.SS1.p1.1.m1.1.1.1.1.4.cmml">v</mi></mrow><mo id="S3.SS1.p1.1.m1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.2.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.1.m1.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.2">delimited-⟨⟩</csymbol><apply id="S3.SS1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.1"></times><ci id="S3.SS1.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.2">𝑑</ci><ci id="S3.SS1.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.3">𝑖</ci><ci id="S3.SS1.p1.1.m1.1.1.1.1.4.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.4">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\left&lt;div\right&gt;</annotation></semantics></math>, the unordered list element <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="\left&lt;ul\right&gt;" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.2.cmml"><mo id="S3.SS1.p1.2.m2.1.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.1.cmml">⟨</mo><mrow id="S3.SS1.p1.2.m2.1.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.1.1.2" xref="S3.SS1.p1.2.m2.1.1.1.1.2.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.2.m2.1.1.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.p1.2.m2.1.1.1.1.3" xref="S3.SS1.p1.2.m2.1.1.1.1.3.cmml">l</mi></mrow><mo id="S3.SS1.p1.2.m2.1.1.1.3" xref="S3.SS1.p1.2.m2.1.1.2.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1.2">delimited-⟨⟩</csymbol><apply id="S3.SS1.p1.2.m2.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1"><times id="S3.SS1.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.1"></times><ci id="S3.SS1.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.2">𝑢</ci><ci id="S3.SS1.p1.2.m2.1.1.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.1.1.3">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\left&lt;ul\right&gt;</annotation></semantics></math>, and the <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="\left&lt;li\right&gt;" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.2.cmml"><mo id="S3.SS1.p1.3.m3.1.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.1.cmml">⟨</mo><mrow id="S3.SS1.p1.3.m3.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.1.1.2" xref="S3.SS1.p1.3.m3.1.1.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.3.m3.1.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.1.cmml">​</mo><mi id="S3.SS1.p1.3.m3.1.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.1.1.3.cmml">i</mi></mrow><mo id="S3.SS1.p1.3.m3.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.2.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.2">delimited-⟨⟩</csymbol><apply id="S3.SS1.p1.3.m3.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1"><times id="S3.SS1.p1.3.m3.1.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1"></times><ci id="S3.SS1.p1.3.m3.1.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.2">𝑙</ci><ci id="S3.SS1.p1.3.m3.1.1.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\left&lt;li\right&gt;</annotation></semantics></math> element. Then we can download images with Requests and write their captions directly to local files. Given the extracted image-caption pairs, we perform post-processing including (1) removing images that are not pathology images, such as flow charts and portraits; (2) correcting erroneous matching between images and captions. 
<br class="ltx_break"></p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Question Generation</h3>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2003.10286/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="90" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The framework of generating questions from captions</figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">In this section, we discuss how to semi-automatically generate questions from captions. Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2 Question Generation ‣ 3 Dataset Collection ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the overall framework. We perform natural language processing of the captions using the Stanford CoreNLP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> toolkit, including sentence split, tokenization, part-of-speech (POS) tagging, named entity recognition (NER), constituent parsing, and dependency parsing. Many sentences are long, with complicated syntactic structures. We perform sentence simplification to break a long sentence into several short ones. Given the subjects, verbs, clauses, etc. labeled by POS tagging and syntactic parsing, we rearrange them using the rules proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> to achieve simplification.
Figure <a href="#S3.F4" title="Figure 4 ‣ 3.2 Question Generation ‣ 3 Dataset Collection ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows an example.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2003.10286/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="313" height="169" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Sentence simplification</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p">Given the POS tags and named entities of the simplified sentences, we generate questions for them: including “when"-type of questions for date and time entities and phrases such as “in/during … stage/period", “before …", and “after …"; “how much/how many"-type of questions for words tagged as numbers; “whose" questions for possessive pronouns (e.g., “its", “their"); “where" questions for location entities and prepositional phrases starting with “inner", “within", “on the right/left of"; “how" questions for adjective words and phrases starting with “using", “via", “with", and “through", and “what" questions for the remaining noun phrases. Table <a href="#S3.T2" title="Table 2 ‣ 3.2 Question Generation ‣ 3 Dataset Collection ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows an example for each type of questions.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Examples of generated questions for different types</figcaption>
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:392.9pt;height:112.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-148.2pt,42.6pt) scale(0.57,0.57) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Type</th>
<th id="S3.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Original sentence</th>
<th id="S3.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Question</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.2.1" class="ltx_tr">
<td id="S3.T2.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S3.T2.1.1.2.1.1.1" class="ltx_text">What</span></td>
<td id="S3.T2.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T2.1.1.2.1.2.1" class="ltx_text ltx_font_bold">The end of the
long bone</span> is expanded</td>
<td id="S3.T2.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">What is expanded</td>
</tr>
<tr id="S3.T2.1.1.3.2" class="ltx_tr">
<td id="S3.T2.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r">in the region of epiphysis.</td>
<td id="S3.T2.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">in the region of epiphysis?</td>
</tr>
<tr id="S3.T2.1.1.4.3" class="ltx_tr">
<td id="S3.T2.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.1.1.4.3.1.1" class="ltx_text">Where</span></td>
<td id="S3.T2.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">The left ventricle is <span id="S3.T2.1.1.4.3.2.1" class="ltx_text ltx_font_bold">on the lower right</span>
</td>
<td id="S3.T2.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Where is the left ventricle</td>
</tr>
<tr id="S3.T2.1.1.5.4" class="ltx_tr">
<td id="S3.T2.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="S3.T2.1.1.5.4.1.1" class="ltx_text">Where</span></td>
<td id="S3.T2.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">in this
apical four-chamber view of the heart.</td>
<td id="S3.T2.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">in this apical four-chamber view of the heart?</td>
</tr>
<tr id="S3.T2.1.1.6.5" class="ltx_tr">
<td id="S3.T2.1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">When</td>
<td id="S3.T2.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T2.1.1.6.5.2.1" class="ltx_text ltx_font_bold">After 1 year of abstinence</span>, most scars are gone.</td>
<td id="S3.T2.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">When are most scars gone?</td>
</tr>
<tr id="S3.T2.1.1.7.6" class="ltx_tr">
<td id="S3.T2.1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">How much/How many</td>
<td id="S3.T2.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S3.T2.1.1.7.6.2.1" class="ltx_text ltx_font_bold">Two</span> multi-faceted gallstones are present in the lumen.</td>
<td id="S3.T2.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">How many multi-faceted gallstones are present in the lumen?</td>
</tr>
<tr id="S3.T2.1.1.8.7" class="ltx_tr">
<td id="S3.T2.1.1.8.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S3.T2.1.1.8.7.1.1" class="ltx_text">Whose</span></td>
<td id="S3.T2.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">The tumor cells and <span id="S3.T2.1.1.8.7.2.1" class="ltx_text ltx_font_bold">their</span> nuclei
are fairly uniform,</td>
<td id="S3.T2.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">The tumor cells and whose nuclei are fairly uniform,</td>
</tr>
<tr id="S3.T2.1.1.9.8" class="ltx_tr">
<td id="S3.T2.1.1.9.8.1" class="ltx_td ltx_align_center ltx_border_r">giving a monotonous appearance.</td>
<td id="S3.T2.1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r">giving a monotonous appearance?</td>
</tr>
<tr id="S3.T2.1.1.10.9" class="ltx_tr">
<td id="S3.T2.1.1.10.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.1.1.10.9.1.1" class="ltx_text">How</span></td>
<td id="S3.T2.1.1.10.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">The trabecular bone
forming the marrow space shows trabeculae</td>
<td id="S3.T2.1.1.10.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">How does the trabecular bone</td>
</tr>
<tr id="S3.T2.1.1.11.10" class="ltx_tr">
<td id="S3.T2.1.1.11.10.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r"><span id="S3.T2.1.1.11.10.1.1" class="ltx_text">How</span></td>
<td id="S3.T2.1.1.11.10.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">
<span id="S3.T2.1.1.11.10.2.1" class="ltx_text ltx_font_bold">with osteoclastic activity at the margins</span>.</td>
<td id="S3.T2.1.1.11.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">forming the marrow space show trabeculae?</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">We use Tregex from Stanford CoreNLP tools <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>,
a tree query language including various relational operators based on the primitive relations of
immediate dominance and immediate precedence, to implement the rules <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> for transforming declarative sentences (captions) into questions.
To reduce grammatical errors, we avoid generating questions on sentences with adverbial clauses such as “chronic inflammation in the lung, showing all three characteristic histologic features". The question transducer mainly contains three steps. First, we perform the main verb decomposition based on the tense of the verb. For instance, we decompose “shows" to “does show". It is worth noting that for passive sentences with a structure of “be+shown/presented/demonstrated", we keep their original forms rather than performing the verb decomposition. Second, we perform subject-auxiliary inversion. We invert the subject and the auxiliary verb in the declarative sentences to form the interrogative sentence. After the inversion, the binary “yes/no" questions are generated, for instance, as shown in Figure <a href="#S3.F5" title="Figure 5 ‣ 3.2 Question Generation ‣ 3 Dataset Collection ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the sentence “microscopy shows coagulative necrosis of the affected bowel wall and thrombosed vessels" is inverted to “does microscopy show coagulative necrosis of the affected bowel wall and thrombosed vessels?". To generate questions whose answers are “no", we randomly select a phrase with the same POS tagging from other captions to replace the head words in the original question. For example, we replace “coagulative necrosis" in the sentence “does microscopy show coagulative necrosis of the affected bowel wall and thrombosed vessels" with other noun phrases. Third, we remove the target answer phrases and insert the question phrase obtained previously to generate open-ended questions belonging to types of “what", “where", “when", “whose", “how", and
“how much/how many" as shown in Table <a href="#S3.T2" title="Table 2 ‣ 3.2 Question Generation ‣ 3 Dataset Collection ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
For instance, we transduce “microscopy shows coagulative necrosis of the affected bowel wall and thrombosed vessels" to “what of the affected bowel wall and thrombosed vessels does microscopy show?" as shown in Figure <a href="#S3.F5" title="Figure 5 ‣ 3.2 Question Generation ‣ 3 Dataset Collection ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2003.10286/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="155" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span> Implementation of syntactic transformation rules</figcaption>
</figure>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p">Given the automatically generated questions which may contain syntactic and semantic errors, we perform post-processing to fix those issues. We manually proofread all questions to correct misspellings, syntactic errors, and semantic inconsistencies. The questions and answers are further cleaned by removing extra spaces and irrelevant symbols. Questions that are too short or vague are removed. Articles appearing at the beginning of answers are stripped.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Dataset Statistics</h2>

<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Statistics of our dataset</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Maximum</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Average</th>
<th id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Minimum</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<th id="S4.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"># questions per image</th>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14</td>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.6</td>
<td id="S4.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<th id="S4.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"># words per question</th>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">28</td>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.5</td>
<td id="S4.T3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<th id="S4.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t"># words per answer</th>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">10</td>
<td id="S4.T3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2.5</td>
<td id="S4.T3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">Our PathVQA dataset consists of 32,799 question-answer pairs generated from 1,670 pathology images collected from two pathology textbooks: “Textbook of Pathology" and “Basic Pathology", and 3,328 pathology images collected from the PEIR<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="http://peir.path.uab.edu/library/index.php?/category/2" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://peir.path.uab.edu/library/index.php?/category/2</a></span></span></span> digital library. Figure <a href="#S4" title="4 Dataset Statistics ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows some examples. On average, each image has 6.6 questions. The maximum and minimum number of questions for a single image is 14 and 1 respectively. The average number of words per question and per answer is 9.5 and 2.5 respectively. Table <a href="#S4.T3" title="Table 3 ‣ 4 Dataset Statistics ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> summarizes these statistics. There are 7 categories of questions: what, where, when, whose, how, how much/how many, and yes/no. Table <a href="#S4" title="4 Dataset Statistics ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the number of questions and percentage of each category. The questions in the first 6 categories are open-ended: 16,465 in total and accounting for 50.2% of all questions. The rest are close-ended “yes/no" questions. The number of “yes" and “no" answers are balanced, which is 8,145 and 8,189 respectively. The questions cover various aspects of the visual contents, including color, location, appearance, shape, etc. Such clinical diversity poses great challenges for AI models to solve this pathology VQA problem.</p>
</div>
<div id="S4.2" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<figure id="S4.1.1" class="ltx_figure ltx_minipage ltx_align_middle" style="width:303.5pt;"><img src="/html/2003.10286/assets/x6.png" id="S4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="489" height="323" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Two exemplar images with semi-automatically generated questions. Both images have three types of questions: “what", “where", and “yes/no".</figcaption>
</figure>
<figure id="S4.2.fig1" class="ltx_figure ltx_minipage ltx_align_middle" style="width:130.1pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Table 4: </span>Frequency of questions in different categories</figcaption>
<div id="S4.2.fig1.1" class="ltx_inline-block ltx_transformed_outer" style="width:162.6pt;height:137.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-14.3pt,12.1pt) scale(0.85,0.85) ;">
<table id="S4.2.fig1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.2.fig1.1.1.1.1" class="ltx_tr">
<td id="S4.2.fig1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.2.fig1.1.1.1.1.1.1" class="ltx_text">Question type</span></td>
<td id="S4.2.fig1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Total number</td>
</tr>
<tr id="S4.2.fig1.1.1.2.2" class="ltx_tr">
<td id="S4.2.fig1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r"><span id="S4.2.fig1.1.1.2.2.1.1" class="ltx_text">Question type</span></td>
<td id="S4.2.fig1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r">and percentage</td>
</tr>
<tr id="S4.2.fig1.1.1.3.3" class="ltx_tr">
<td id="S4.2.fig1.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Yes/No</td>
<td id="S4.2.fig1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16,334 (49.8%)</td>
</tr>
<tr id="S4.2.fig1.1.1.4.4" class="ltx_tr">
<td id="S4.2.fig1.1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">What</td>
<td id="S4.2.fig1.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13,402 (40.9%)</td>
</tr>
<tr id="S4.2.fig1.1.1.5.5" class="ltx_tr">
<td id="S4.2.fig1.1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Where</td>
<td id="S4.2.fig1.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1,268 (4.0%)</td>
</tr>
<tr id="S4.2.fig1.1.1.6.6" class="ltx_tr">
<td id="S4.2.fig1.1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">How</td>
<td id="S4.2.fig1.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1,014 (3.0%)</td>
</tr>
<tr id="S4.2.fig1.1.1.7.7" class="ltx_tr">
<td id="S4.2.fig1.1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">How much/How many</td>
<td id="S4.2.fig1.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">294 (0.9%)</td>
</tr>
<tr id="S4.2.fig1.1.1.8.8" class="ltx_tr">
<td id="S4.2.fig1.1.1.8.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">When</td>
<td id="S4.2.fig1.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">285 (0.9%)</td>
</tr>
<tr id="S4.2.fig1.1.1.9.9" class="ltx_tr">
<td id="S4.2.fig1.1.1.9.9.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Whose</td>
<td id="S4.2.fig1.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">202 (0.6%)</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p">Figure <a href="#S4.F7" title="Figure 7 ‣ 4 Dataset Statistics ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the frequencies of different
answers for open-ended questions. The x-axis shows 70 most common answers and y-axis shows the frequency of each answer. As can be seen, the answer frequency has a long-tail distribution: a few answers have very high frequency while most answers have low frequency. Majority of answers have one or two words.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2003.10286/assets/unnormalized_frequency_no_background.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="219" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Frequencies of answers for open-ended questions</figcaption>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">To standardize the performance comparison on this dataset, we create an “official" split. We randomly partition the images along with the associated questions into a training set, validation set, and testing set with a ratio of 0.5, 0.3, and 0.2. The statistics are summarized in Table 
<a href="#S4.T5" title="Table 5 ‣ 4 Dataset Statistics ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Statistics of data split</figcaption>
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"></th>
<th id="S4.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Training set</th>
<th id="S4.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Validation set</th>
<th id="S4.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Test set</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.2.1" class="ltx_tr">
<td id="S4.T5.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"># images</td>
<td id="S4.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2,499</td>
<td id="S4.T5.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1,499</td>
<td id="S4.T5.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1,000</td>
</tr>
<tr id="S4.T5.1.3.2" class="ltx_tr">
<td id="S4.T5.1.3.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"># QA pairs</td>
<td id="S4.T5.1.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">17,325</td>
<td id="S4.T5.1.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">9,462</td>
<td id="S4.T5.1.3.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">6,012</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Benchmark VQA Performance</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">In this section, we apply existing well-established and state-of-the-art VQA methods to our PathVQA dataset to obtain some baseline performance numbers for the research community to benchmark with.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Models</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">We use three well-known VQA methods to generate the benchmark results.</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para ltx_noindent">
<p id="S5.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Method 1</span>: The method proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> uses a Gated Recurrent Unit (GRU) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> recurrent network and a Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> network to embed the question and the image.
It learns bilinear attention distributions using the bilinear attention networks (BAN) and uses low rank approximation techniques to approximate the bilinear interaction between question embeddings and image embeddings.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Method 2</span>: In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, a CNN is used to encode the image, and an LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> network is used to encode the questions and answers. A multimodal compact bilinear pooling mechanism is proposed to match the image encoding and question encoding and an attention mechanism is leveraged to infer the answer.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S5.I1.i3.p1.1" class="ltx_p"><span id="S5.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Method 3</span>: The stacked attention network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> embeds images and questions/answers using CNN and LSTM respectively and leverages a stacked attention mechanism to locate image regions that are relevant to answering the question. It queries the image multiple times to progressively narrow the regions to be attended.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Experimental Settings</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p">Given the questions and answers, we perform standard pre-processing, including removing punctuation and stop words, tokenization, and converting to lower-cases. For question encoding and answer decoding, we create a vocabulary of 2200 words that have the highest frequencies.
Data augmentation is applied to the images, including shifting, scaling, and shearing. In Method 1-3, we follow the original model configurations used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, where the extraction of visual features are conducted using Faster R-CNN, ResNet-152 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, and VGGNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> respectively, with the Faster R-CNN pre-trained on Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and the later two both pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Words in questions and answers are represented using GloVe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> vectors pre-trained on general-domain corpora such as Wikipedia, Twitter, etc.
In Method 1, the dropout <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> rate for the linear mapping was set to 0.2 while for the classifier it was set to 0.5. The initial learning
rate was set to 0.005 with the Adamax optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> used. The batch size was set to 512.
In Method 2, dropout was applied to the LSTM layers with a probability of 0.4. We set the feature dimension to 2048 in multimodal compact bilinear pooling. The optimizer was Adam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> with an initial learning rate of 0.0001 and a mini-batch size of 32.
In Method 3, the number of attention layers and LSTM layers were both set to 2 and the hidden dimensionality of
the LSTMs was set to 512. The weight parameters were learned using Stochastic Gradient Descent (SGD) with a momentum of 0.9, a learning rate of 0.1, and a mini-batch size of 100. As a comparison to Method 1 and Method 2, we change the image encoder in Method 3 to Faster R-CNN and ResNet-152 respectively. We refer to these two baseline models as Method 3 + Faster R-CNN and Method 3 + ResNet respectively.
</p>
</div>
<div id="S5.SS2.2" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<figure id="S5.SS2.1.1" class="ltx_figure ltx_minipage ltx_align_middle" style="width:238.5pt;"><img src="/html/2003.10286/assets/x7.png" id="S5.SS2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="438" height="322" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Two qualitative examples of VQA</figcaption>
</figure>
<figure id="S5.SS2.2.fig1" class="ltx_figure ltx_minipage ltx_align_middle" style="width:186.5pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Table 6: </span>Accuracy on “yes/no" questions</figcaption>
<table id="S5.SS2.2.fig1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.SS2.2.fig1.1.1.1" class="ltx_tr">
<th id="S5.SS2.2.fig1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Method</th>
<th id="S5.SS2.2.fig1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Accuracy (%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.SS2.2.fig1.1.2.1" class="ltx_tr">
<td id="S5.SS2.2.fig1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Method 1</td>
<td id="S5.SS2.2.fig1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">68.2</td>
</tr>
<tr id="S5.SS2.2.fig1.1.3.2" class="ltx_tr">
<td id="S5.SS2.2.fig1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Method 2</td>
<td id="S5.SS2.2.fig1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">57.6</td>
</tr>
<tr id="S5.SS2.2.fig1.1.4.3" class="ltx_tr">
<td id="S5.SS2.2.fig1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Method 3</td>
<td id="S5.SS2.2.fig1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">59.4</td>
</tr>
<tr id="S5.SS2.2.fig1.1.5.4" class="ltx_tr">
<td id="S5.SS2.2.fig1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Method 3 + Faster R-CNN</td>
<td id="S5.SS2.2.fig1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">62.0</td>
</tr>
<tr id="S5.SS2.2.fig1.1.6.5" class="ltx_tr">
<td id="S5.SS2.2.fig1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Method 3 + ResNet</td>
<td id="S5.SS2.2.fig1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">60.1</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p">For “yes/no" questions, we evaluate using accuracy. For open-ended questions, we evaluate using three metrics: (1) exact match <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, which measures the percentage of inferred answers that match exactly with the ground-truth; (2) Macro-averaged F1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, which measures the average overlap between the predicted answers and ground-truth, where the answers are treated as bag of tokens; (3) BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, which measures the similarity of predicted answers and ground-truth by matching <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mi id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><ci id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">n</annotation></semantics></math>-grams.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Results</h3>

<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.p1.1" class="ltx_p">Table <a href="#S5.SS2" title="5.2 Experimental Settings ‣ 5 Benchmark VQA Performance ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a> shows the accuracy achieved by different methods on the “yes/no" questions. All methods perform better than random guess (where the accuracy is 50%). This indicates that this dataset is clinically meaningful, which allows VQA models to be learnable.
Among Method 1-3, Method 1 performs the best. One primary reason is that it uses the bottom-up mechanism to propose candidate image regions and extract region-specific visual features. Typically the answer is only relevant to a small region of the entire pathology image. Method 1 effectively localizes images regions that are most helpful in inferring the correct answer. This can be further verified by comparing Method 3 + Faster R-CNN with Method 3, where the former outperforms the latter. Method 3 + Faster R-CNN extracts region-specific features while Method 3 extracts holistic features of the entire image. Besides, the use of residual learning of attention and the superiority of bilinear attention over other co-attention approaches also contribute to the highest accuracy of Method 1.
Another observation is that Method 3 outperforms Method 2. This is because Method 3 utilizes multiple layers of attention to progressively learn where to attend, therefore achieving better performance than Method 2 which utilizes a single layer of attention. Method 3 + ResNet works better than Method 3, due to the reason that ResNet can extract better visual features than VGGNet.
</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.3" class="ltx_p">Table <a href="#S5.T7" title="Table 7 ‣ 5.3 Results ‣ 5 Benchmark VQA Performance ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the exact match scores, F1, and BLEU-(1,2,3) scores on open-ended questions belonging to the following categories: what, where, how, whose, and when. As can be seen, these scores are low in general, which indicates that our dataset is very challenging for medical VQA. As a reference, we summarize the exact match scores achieved by these baseline methods on general-domain VQA datasets in Table <a href="#S5.T8" title="Table 8 ‣ 5.3 Results ‣ 5 Benchmark VQA Performance ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. As can be seen, those numbers are much higher. The reasons that our dataset is so challenging lie in the following facts. First, most questions in our dataset are open-ended where the number of possible answers is <math id="S5.SS3.p2.1.m1.1" class="ltx_Math" alttext="O(V^{L})" display="inline"><semantics id="S5.SS3.p2.1.m1.1a"><mrow id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml"><mi id="S5.SS3.p2.1.m1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S5.SS3.p2.1.m1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.2.cmml">​</mo><mrow id="S5.SS3.p2.1.m1.1.1.1.1" xref="S5.SS3.p2.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS3.p2.1.m1.1.1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.1.1.1.cmml">(</mo><msup id="S5.SS3.p2.1.m1.1.1.1.1.1" xref="S5.SS3.p2.1.m1.1.1.1.1.1.cmml"><mi id="S5.SS3.p2.1.m1.1.1.1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.1.1.1.2.cmml">V</mi><mi id="S5.SS3.p2.1.m1.1.1.1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.1.1.1.3.cmml">L</mi></msup><mo stretchy="false" id="S5.SS3.p2.1.m1.1.1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><apply id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1"><times id="S5.SS3.p2.1.m1.1.1.2.cmml" xref="S5.SS3.p2.1.m1.1.1.2"></times><ci id="S5.SS3.p2.1.m1.1.1.3.cmml" xref="S5.SS3.p2.1.m1.1.1.3">𝑂</ci><apply id="S5.SS3.p2.1.m1.1.1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.1.m1.1.1.1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1.1.1">superscript</csymbol><ci id="S5.SS3.p2.1.m1.1.1.1.1.1.2.cmml" xref="S5.SS3.p2.1.m1.1.1.1.1.1.2">𝑉</ci><ci id="S5.SS3.p2.1.m1.1.1.1.1.1.3.cmml" xref="S5.SS3.p2.1.m1.1.1.1.1.1.3">𝐿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">O(V^{L})</annotation></semantics></math>, where <math id="S5.SS3.p2.2.m2.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S5.SS3.p2.2.m2.1a"><mi id="S5.SS3.p2.2.m2.1.1" xref="S5.SS3.p2.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.1b"><ci id="S5.SS3.p2.2.m2.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.1c">V</annotation></semantics></math> is the vocabulary size and <math id="S5.SS3.p2.3.m3.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S5.SS3.p2.3.m3.1a"><mi id="S5.SS3.p2.3.m3.1.1" xref="S5.SS3.p2.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.3.m3.1b"><ci id="S5.SS3.p2.3.m3.1.1.cmml" xref="S5.SS3.p2.3.m3.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.3.m3.1c">L</annotation></semantics></math> is the expected length of answers. This easily incurs the out-of-vocabulary issue, where the words in test examples may never occur in the training examples.
Second, the size of our dataset is much smaller, compared with general domain VQA datasets. More innovations of VQA models are needed to bridge the performance gap.</p>
</div>
<figure id="S5.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>BLEU-(1,2,3), exact match scores, and F1 on open-ended questions</figcaption>
<table id="S5.T7.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T7.1.1.1" class="ltx_tr">
<th id="S5.T7.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Method</th>
<th id="S5.T7.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="5">Evaluation metric</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T7.1.2.1" class="ltx_tr">
<td id="S5.T7.1.2.1.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></td>
<td id="S5.T7.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">BLEU-1</td>
<td id="S5.T7.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">BLEU-2</td>
<td id="S5.T7.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">BLEU-3</td>
<td id="S5.T7.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Exact match (%)</td>
<td id="S5.T7.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">F1 (%)</td>
</tr>
<tr id="S5.T7.1.3.2" class="ltx_tr">
<td id="S5.T7.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Method 1</td>
<td id="S5.T7.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">32.4</td>
<td id="S5.T7.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22.8</td>
<td id="S5.T7.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17.4</td>
<td id="S5.T7.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.9</td>
<td id="S5.T7.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24.0</td>
</tr>
<tr id="S5.T7.1.4.3" class="ltx_tr">
<td id="S5.T7.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Method 2</td>
<td id="S5.T7.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13.3</td>
<td id="S5.T7.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9.5</td>
<td id="S5.T7.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.8</td>
<td id="S5.T7.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.4</td>
<td id="S5.T7.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12.5</td>
</tr>
<tr id="S5.T7.1.5.4" class="ltx_tr">
<td id="S5.T7.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Method 3</td>
<td id="S5.T7.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">19.2</td>
<td id="S5.T7.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17.9</td>
<td id="S5.T7.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15.8</td>
<td id="S5.T7.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.6</td>
<td id="S5.T7.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">19.7</td>
</tr>
<tr id="S5.T7.1.6.5" class="ltx_tr">
<td id="S5.T7.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Method 3 + Faster R-CNN</td>
<td id="S5.T7.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24.7</td>
<td id="S5.T7.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">19.1</td>
<td id="S5.T7.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16.5</td>
<td id="S5.T7.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.9</td>
<td id="S5.T7.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21.2</td>
</tr>
<tr id="S5.T7.1.7.6" class="ltx_tr">
<td id="S5.T7.1.7.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Method 3 + ResNet</td>
<td id="S5.T7.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">19.9</td>
<td id="S5.T7.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">18.0</td>
<td id="S5.T7.1.7.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">16.0</td>
<td id="S5.T7.1.7.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1.6</td>
<td id="S5.T7.1.7.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">19.8</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Exact match (%) scores on general-domain VQA datasets</figcaption>
<table id="S5.T8.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T8.1.1.1" class="ltx_tr">
<th id="S5.T8.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Method</th>
<th id="S5.T8.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="4">Dataset</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T8.1.2.1" class="ltx_tr">
<td id="S5.T8.1.2.1.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></td>
<td id="S5.T8.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">DAQUAR</td>
<td id="S5.T8.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">VQA-real</td>
<td id="S5.T8.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">VQA v2</td>
<td id="S5.T8.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">COCO-QA</td>
</tr>
<tr id="S5.T8.1.3.2" class="ltx_tr">
<td id="S5.T8.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Method 1</td>
<td id="S5.T8.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✗</td>
<td id="S5.T8.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✗</td>
<td id="S5.T8.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">66.0</td>
<td id="S5.T8.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✗</td>
</tr>
<tr id="S5.T8.1.4.3" class="ltx_tr">
<td id="S5.T8.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Method 2</td>
<td id="S5.T8.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✗</td>
<td id="S5.T8.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">61.1</td>
<td id="S5.T8.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">66.5</td>
<td id="S5.T8.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✗</td>
</tr>
<tr id="S5.T8.1.5.4" class="ltx_tr">
<td id="S5.T8.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Method 3</td>
<td id="S5.T8.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">45.5</td>
<td id="S5.T8.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">58.7</td>
<td id="S5.T8.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">✗</td>
<td id="S5.T8.1.5.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">61.6</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS3.p3" class="ltx_para ltx_noindent">
<p id="S5.SS3.p3.1" class="ltx_p">The exact match scores on open-ended questions are much lower than the accuracy scores on “yes/no" questions. This is not surprising since the number of candidate answers for open-ended questions is vast while that of “yes/no" questions is only 2. Similar to the conclusions drawn from Table <a href="#S5.T7" title="Table 7 ‣ 5.3 Results ‣ 5 Benchmark VQA Performance ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, bilinear attention based Method 1 achieves the best performance.
Method 3 works better than Method 2 by utilizing a stack of attention mechanisms.</p>
</div>
<figure id="S5.F9" class="ltx_figure"><img src="/html/2003.10286/assets/F1.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="357" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>F1 scores achieved on 20 most frequent answers by Method 1-3 </figcaption>
</figure>
<div id="S5.SS3.p4" class="ltx_para ltx_noindent">
<p id="S5.SS3.p4.1" class="ltx_p">Figure <a href="#S5.F9" title="Figure 9 ‣ 5.3 Results ‣ 5 Benchmark VQA Performance ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows the individual F1 scores achieved on 20 most frequent answers by Method 1-3. As can be seen, Method 1 outperforms Method 2 and 3 on most answers. This is consistent with the results in Table <a href="#S5.SS2" title="5.2 Experimental Settings ‣ 5 Benchmark VQA Performance ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a> and <a href="#S5.T7" title="Table 7 ‣ 5.3 Results ‣ 5 Benchmark VQA Performance ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<div id="S5.SS3.p5" class="ltx_para ltx_noindent">
<p id="S5.SS3.p5.1" class="ltx_p">We show two qualitative examples of VQA results achieved by Method 1-3
in Figure <a href="#S5.SS2" title="5.2 Experimental Settings ‣ 5 Benchmark VQA Performance ‣ PathVQA: 30000+ Questions for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>. In the left example, all methods fail to give the correct answer since this answer is an infrequent one. But the three answers are semantically very relevant to the image, indicating that the models can learn something meaningful. In the right example, both Method 1 and Method 3 predict the answers correctly while Method 2 fails. This suggests that these two methods have certain advantages over Method 2 in that their effective attention mechanisms allow them to better recognize image regions of interest, which helps to give the correct answer.</p>
</div>
<section id="S5.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Suggestions for model improvement</h4>

<div id="S5.SS3.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.SSS0.Px1.p1.1" class="ltx_p">The visual feature extractors used in the baseline methods are pre-trained on general-domain images, which have a domain discrepancy with pathology images. One way to improve is to collect publicly available medical images (preferably pathology images) from textbooks, website, etc., whose domain is closer to that of the images in our dataset, then pre-train the CNNs using these medical images. Similarly for the word embeddings which were pre-trained on general-domain corpora, they may not be able to effectively capture the semantics relevant to pathology. To improve, we can pre-train the word embeddings on medical literature, such as medical textbooks, clinical guidelines, medical publications, etc.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Future Works</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">In this paper, towards the goal of developing AI systems to pass the board-certificated examinations of the American Board of Pathology and fostering research in medical visual question answering, we build a pathology VQA dataset that contains 32,799 question-answer pairs of 7 categories, generated from 4,998 images. Majority of questions in our dataset are open-ended, posing great challenges for the medical VQA research. Our dataset is publicly available.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p">For future studies, there are several aspects to improve. First, the questions in our dataset are not yet totally aligned with those in the ABP tests. In ABP test questions, each image is associated with a short text describing the medical history and demographics of the patient. These information are useful in determining the answers. To bridge this gap, we plan to create medical VQA datasets from the MedPix dataset where each image is associated with a caption and a text describing medical history and patient demographics. Second, in our current method, the creation of question/answer pairs from captions are mostly based on linguistic rules, which may not be diverse or robust enough. We plan to develop deep generative models that learn how to generate QA pairs from captions. Third, we plan to apply our automated pipeline to create VQA datasets for other types of medical images, such as radiology, ultrasound, CT scans, etc. Besides the board of pathology, other medical imaging domains have their own boards as well, organizing different types of board-certificated examinations. It would be interesting to build AI systems to pass those examinations as well.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">ICCV</span>, 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Mateusz Malinowski and Mario Fritz.

</span>
<span class="ltx_bibblock">A multi-world approach to question answering about real-world scenes
based on uncertain input.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">NIPS</span>, 2014.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Mengye Ren, Ryan Kiros, and Richard Zemel.

</span>
<span class="ltx_bibblock">Image question answering: A visual semantic embedding model and a new
dataset.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">NIPS</span>, 2015.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei,
C Lawrence Zitnick, and Ross Girshick.

</span>
<span class="ltx_bibblock">Clevr: A diagnostic dataset for compositional language and elementary
visual reasoning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2017.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2017.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Asma Ben Abacha, Sadid A Hasan, Vivek V Datla, Joey Liu, Dina Demner-Fushman,
and Henning Müller.

</span>
<span class="ltx_bibblock">Vqa-med: Overview of the medical visual question answering task at
imageclef 2019.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">CLEF2019 Working Notes.</span>, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman.

</span>
<span class="ltx_bibblock">A dataset of clinically generated visual questions and answers about
radiology images.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Scientific data</span>, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus.

</span>
<span class="ltx_bibblock">Indoor segmentation and support inference from rgbd images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2012.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2014.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Stanislaw Antol, C Lawrence Zitnick, and Devi Parikh.

</span>
<span class="ltx_bibblock">Zero-shot learning via visual abstraction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2014.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
C Lawrence Zitnick and Devi Parikh.

</span>
<span class="ltx_bibblock">Bringing semantics into focus using visual abstraction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2013.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi,
and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Are you smarter than a sixth grader? textbook question answering for
multimodal machine comprehension.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2017.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Dan Klein and Christopher D Manning.

</span>
<span class="ltx_bibblock">Accurate unlexicalized parsing.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">ACL</span>, 2003.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Zhihao Fan, Zhongyu Wei, Piji Li, Yanyan Lan, and Xuanjing Huang.

</span>
<span class="ltx_bibblock">A question type driven framework to diversify visual question
generation.

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Diederik P Kingma and Max Welling.

</span>
<span class="ltx_bibblock">Auto-encoding variational bayes.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1312.6114</span>, 2013.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Kristina Toutanova, Chris Brockett, Michael Gamon, Jagadeesh Jagarlamudi,
Hisami Suzuki, and Lucy Vanderwende.

</span>
<span class="ltx_bibblock">The pythy summarization system: Microsoft research at duc 2007.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proc. of DUC</span>, 2007.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Bonnie Dorr, David Zajic, and Richard Schwartz.

</span>
<span class="ltx_bibblock">Hedge trimmer: A parse-and-trim approach to headline generation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">HLT-NAACL workshop</span>, 2003.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard,
and David McClosky.

</span>
<span class="ltx_bibblock">The Stanford CoreNLP natural language processing toolkit.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">ACL</span>, 2014.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Michael Heilman and Noah A Smith.

</span>
<span class="ltx_bibblock">Question generation via overgenerating transformations and ranking.

</span>
<span class="ltx_bibblock">Technical report, 2009.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang.

</span>
<span class="ltx_bibblock">Bilinear attention networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">NIPS</span>, 2018.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Learning phrase representations using rnn encoder-decoder for
statistical machine translation.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1406.1078</span>, 2014.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with region proposal
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
91–99, 2015.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and
Marcus Rohrbach.

</span>
<span class="ltx_bibblock">Multimodal compact bilinear pooling for visual question answering and
visual grounding.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.01847</span>, 2016.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Sepp Hochreiter and Jürgen Schmidhuber.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Neural computation</span>, 1997.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola.

</span>
<span class="ltx_bibblock">Stacked attention networks for image question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2016.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2016.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Karen Simonyan and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock">2014.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, 2017.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2009.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Jeffrey Pennington, Richard Socher, and Christopher Manning.

</span>
<span class="ltx_bibblock">Glove: Global vectors for word representation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">EMNLP</span>, 2014.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">NIPS</span>, 2012.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Diederik P Kingma and Jimmy Ba.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1412.6980</span>, 2014.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Cyril Goutte and Eric Gaussier.

</span>
<span class="ltx_bibblock">A probabilistic interpretation of precision, recall and f-score, with
implication for evaluation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">European Conference on Information Retrieval</span>, 2005.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">ACL</span>, 2002.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2003.10285" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2003.10286" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2003.10286">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2003.10286" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2003.10287" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar  7 07:44:33 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
