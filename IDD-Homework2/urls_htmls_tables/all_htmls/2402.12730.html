<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation</title>
<!--Generated on Fri Apr 12 00:49:31 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2402.12730v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S1" title="1 Introduction ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S2" title="2 System Overview ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>System Overview</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S2.SS1" title="2.1 Translation to English &amp; Data Augmentation ‣ 2 System Overview ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Translation to English &amp; Data Augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S2.SS2" title="2.2 TranSem Model ‣ 2 System Overview ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span><span class="ltx_text ltx_font_italic">TranSem</span> Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S2.SS3" title="2.3 FineSem Model ‣ 2 System Overview ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span><span class="ltx_text ltx_font_italic">FineSem</span> Model</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S3" title="3 Experimental Setup ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S3.SS1" title="3.1 HyperParameters ‣ 3 Experimental Setup ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>HyperParameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S3.SS2" title="3.2 Infrastructure ‣ 3 Experimental Setup ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Infrastructure</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S4" title="4 Results ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S4.SS1" title="4.1 Track A Languages ‣ 4 Results ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Track A Languages</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S4.SS1.SSS1" title="4.1.1 Effect of Batch Size ‣ 4.1 Track A Languages ‣ 4 Results ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Effect of Batch Size</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S4.SS1.SSS2" title="4.1.2 Effect of Encoder Pooling ‣ 4.1 Track A Languages ‣ 4 Results ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Effect of Encoder Pooling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S4.SS1.SSS3" title="4.1.3 Effect of Sentence Embedding Models ‣ 4.1 Track A Languages ‣ 4 Results ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Effect of Sentence Embedding Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S4.SS1.SSS4" title="4.1.4 Usefulness of Machine Translation and Direct Fine-tuning ‣ 4.1 Track A Languages ‣ 4 Results ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.4 </span>Usefulness of Machine Translation and Direct Fine-tuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S4.SS2" title="4.2 Track C Languages ‣ 4 Results ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Track C Languages</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S5" title="5 Related Work ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S5.SS1" title="5.1 Machine Translation ‣ 5 Related Work ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Machine Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S5.SS2" title="5.2 Sentence Embedding ‣ 5 Related Work ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Sentence Embedding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S5.SS3" title="5.3 Semantic Similarity ‣ 5 Related Work ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Semantic Similarity</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S6" title="6 Conclusion &amp; Future Work ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion &amp; Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S7" title="7 Disclaimer ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Disclaimer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S8" title="8 Limitations ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Limitations</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<div aria-label="Conversion errors have been found" class="package-alerts ltx_document" role="status">
<button aria-label="Dismiss alert" onclick="closePopup()">
<span aria-hidden="true"><svg aria-hidden="true" focusable="false" height="20" role="presentation" viewbox="0 0 44 44" width="20">
<path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
<path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
</svg></span>
</button>
<p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
<ul arial-label="Unsupported packages used in this paper">
<li>failed: inconsolata</li>
<li>failed: arydshln</li>
</ul>
<p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
</div><div class="section" id="target-section"><div id="license-tr">License: arXiv.org perpetual non-exclusive license</div><div id="watermark-tr">arXiv:2402.12730v2 [cs.CL] 12 Apr 2024</div></div>
<script>
            function closePopup() {
                document.querySelector('.package-alerts').style.display = 'none';
            }
        </script>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shubhashis Roy Dipta <span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">2</span></span></span></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sai Vallurupalli <span class="ltx_note ltx_role_footnotemark" id="footnotex2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">2</span></span></span></span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id3.1.id1">{sroydip1,kolli}@umbc.edu</span>
<br class="ltx_break"/>Department of Computer Science and Electrical Engineering 
<br class="ltx_break"/>University of Maryland, Baltimore County 
<br class="ltx_break"/>Baltimore, MD 21250 USA 
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.2">The aim of SemEval-2024 Task 1, “Semantic Textual Relatedness for African and Asian Languages” is to develop models for identifying semantic textual relatedness (STR) between two sentences using multiple languages (14 African and Asian languages) and settings (supervised, unsupervised, and cross-lingual). Large language models (LLMs) have shown impressive performance on several natural language understanding tasks such as multilingual machine translation (MMT), semantic similarity (STS), and encoding sentence embeddings. Using a combination of LLMs that perform well on these tasks, we developed two STR models, <span class="ltx_text ltx_font_italic" id="id2.2.1">TranSem</span> and <span class="ltx_text ltx_font_italic" id="id2.2.2">FineSem</span>, for the supervised and cross-lingual settings. We explore the effectiveness of several training methods and the usefulness of machine translation. We find that direct fine-tuning on the task is comparable to using sentence embeddings and translating to English leads to better performance for some languages.
In the supervised setting, our model performance is better than the official baseline for 3 languages with the remaining 4 performing on par. In the cross-lingual setting, our model performance is better than the baseline for 3 languages (leading to <math alttext="1^{st}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mn id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml">1</mn><mrow id="id1.1.m1.1.1.3" xref="id1.1.m1.1.1.3.cmml"><mi id="id1.1.m1.1.1.3.2" xref="id1.1.m1.1.1.3.2.cmml">s</mi><mo id="id1.1.m1.1.1.3.1" xref="id1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="id1.1.m1.1.1.3.3" xref="id1.1.m1.1.1.3.3.cmml">t</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><csymbol cd="ambiguous" id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1">superscript</csymbol><cn id="id1.1.m1.1.1.2.cmml" type="integer" xref="id1.1.m1.1.1.2">1</cn><apply id="id1.1.m1.1.1.3.cmml" xref="id1.1.m1.1.1.3"><times id="id1.1.m1.1.1.3.1.cmml" xref="id1.1.m1.1.1.3.1"></times><ci id="id1.1.m1.1.1.3.2.cmml" xref="id1.1.m1.1.1.3.2">𝑠</ci><ci id="id1.1.m1.1.1.3.3.cmml" xref="id1.1.m1.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">1^{st}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">1 start_POSTSUPERSCRIPT italic_s italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> place for Africaans and <math alttext="2^{nd}" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><msup id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mn id="id2.2.m2.1.1.2" xref="id2.2.m2.1.1.2.cmml">2</mn><mrow id="id2.2.m2.1.1.3" xref="id2.2.m2.1.1.3.cmml"><mi id="id2.2.m2.1.1.3.2" xref="id2.2.m2.1.1.3.2.cmml">n</mi><mo id="id2.2.m2.1.1.3.1" xref="id2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="id2.2.m2.1.1.3.3" xref="id2.2.m2.1.1.3.3.cmml">d</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><csymbol cd="ambiguous" id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1">superscript</csymbol><cn id="id2.2.m2.1.1.2.cmml" type="integer" xref="id2.2.m2.1.1.2">2</cn><apply id="id2.2.m2.1.1.3.cmml" xref="id2.2.m2.1.1.3"><times id="id2.2.m2.1.1.3.1.cmml" xref="id2.2.m2.1.1.3.1"></times><ci id="id2.2.m2.1.1.3.2.cmml" xref="id2.2.m2.1.1.3.2">𝑛</ci><ci id="id2.2.m2.1.1.3.3.cmml" xref="id2.2.m2.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">2^{nd}</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">2 start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> place for Indonesian), is on par for 2 languages and performs poorly on the remaining 7 languages.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p ltx_align_center ltx_align_bottom" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1">UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation</span></p>
</div>
<div class="ltx_para" id="p2">
<br class="ltx_break"/>
<p class="ltx_p" id="p2.1"><span class="ltx_text" id="p2.1.1" style="width:433.6pt;"><span class="ltx_text" id="p2.1.1.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_guessed_headers ltx_align_top" id="p2.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="p2.1.1.1.1.1.1">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column" id="p2.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p2.1.1.1.1.1.1.1.1">Shubhashis Roy Dipta <span class="ltx_note ltx_role_footnotemark" id="footnotex3"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex3.1.1.1">2</span></span></span></span></span> </span>and<span class="ltx_text ltx_font_bold" id="p2.1.1.1.1.1.1.1.2"> Sai Vallurupalli <span class="ltx_note ltx_role_footnotemark" id="footnotex4"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex4.1.1.1">2</span></span></span></span></span></span></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="p2.1.1.1.1.2.1">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.2.1.1"><span class="ltx_text ltx_font_typewriter" id="p2.1.1.1.1.2.1.1.1">{sroydip1,kolli}@umbc.edu</span></span></span>
<span class="ltx_tr" id="p2.1.1.1.1.3.2">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.3.2.1">Department of Computer Science and Electrical Engineering</span></span>
<span class="ltx_tr" id="p2.1.1.1.1.4.3">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.4.3.1">University of Maryland, Baltimore County</span></span>
<span class="ltx_tr" id="p2.1.1.1.1.5.4">
<span class="ltx_td ltx_align_center" id="p2.1.1.1.1.5.4.1">Baltimore, MD 21250 USA</span></span>
</span>
</span></span> </span></p>
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span>These authors contributed equally to this work</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The objective of the SemEval 2024 Task 1 is to build and evaluate models capable of identifying relatedness between a sentence pair. Sentence pairs from 14 African and Asian languages belonging to 5 language groups are annotated for relatedness and released for model development. The task is divided into 3 tracks targeting different types of model training: supervised (Track A), unsupervised (Track B), and cross-lingual (Track C). Each track targets a different subset of languages. Extensive details about the languages, language groups, and the data collection process are provided in the task description paper <cite class="ltx_cite ltx_citemacro_cite">Ousidhoum et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib25" title="">2024a</a>)</cite>. A detailed description of the shared task, tracks, and datasets are provided in the shared task description paper <cite class="ltx_cite ltx_citemacro_cite">Ousidhoum et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib26" title="">2024b</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Semantic relatedness helps with understanding language meaning <cite class="ltx_cite ltx_citemacro_citep">(Jarmasz and Szpakowicz, <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib16" title="">2012</a>; Miller, <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib21" title="">1995</a>; Antony et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib2" title="">2022</a>; Osgood, <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib24" title="">1949</a>)</cite> and is useful in many areas of natural language processing such as word-sense disambiguation <cite class="ltx_cite ltx_citemacro_citep">(Banerjee and Pedersen, <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib4" title="">2003</a>)</cite>, machine translation <cite class="ltx_cite ltx_citemacro_citep">(Bracken et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib6" title="">2017</a>)</cite> and sentence representation <cite class="ltx_cite ltx_citemacro_citep">(Reimers et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib30" title="">2019</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib34" title="">2022</a>)</cite> which have numerous applications. Until recently, semantic relatedness has been mostly restricted to finding word relatedness <cite class="ltx_cite ltx_citemacro_citep">(Feng et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib14" title="">2017</a>; Budanitsky and Hirst, <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib7" title="">2006</a>)</cite>, leading to a lack of sentence-relatedness datasets. At the sentence level, relatedness has been limited to similarity, providing a restricted view of STR <cite class="ltx_cite ltx_citemacro_citep">(Abdalla et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib1" title="">2021</a>)</cite>. The current shared task aims to broaden the scope of sentence relatedness and extend it to several languages with the goal of encouraging model and resource development in these languages <cite class="ltx_cite ltx_citemacro_citep">(Ousidhoum et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib26" title="">2024b</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Recent advancements in multi-lingual translation and the availability of models for obtaining high-quality sentence embeddings allowed us to explore the effectiveness of machine translated data. Using various sentence embedding models to encode data translated into English, we trained a model, <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">TranSem</span>, to find the relatedness score between sentence pairs. Although the task requires the sentence pair to be from the same language, our model can handle sentences from two different languages. Our second model, <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">FineSem</span> directly fine-tunes a T5 model (T5 is already fine-tuned on the STS benchmark) on the STR task using both untranslated and translated data to explore the usefulness of translation. We use both these models to evaluate languages in Track A. For Track C languages, we use a T5 model fine-tuned only on the english STR data. For evaluating the English dataset in the cross-lingual track, we use a T5 model fine-tuned on the Spanish dataset.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Our contributions to the STR task are as follows: We 1) develop unified models for STR to work with all languages.
2) participate in supervised and cross-lingual tracks.
3) explore the usefulness of machine translation.
4) explore data augmentation using machine translation.
Our code is publicly available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/dipta007/SemEval24-Task8" title="">https://github.com/dipta007/SemEval24-Task8</a></p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>System Overview</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">After exploring models and datasets available in the languages we understand<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Of the 14 languages, the authors are proficient in English, Hindi &amp; Telugu languages.</span></span></span>, we realized the dearth of resources available in these languages. To leverage resources available in English, we translated the 13 non-English languages into English.
Assuming the translated data accurately reflects the semantic meaning of the source language, the derived relatedness value from our model for a translated sentence pair should reflect the STR between the sentence pair in the source language. This section describes our machine translation process and models, <span class="ltx_text ltx_font_italic" id="S2.p1.1.1">TranSem</span> and <span class="ltx_text ltx_font_italic" id="S2.p1.1.2">FineSem</span>. Besides using different training strategies, these models can use both translated and untranslated data.
</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Translation to English &amp; Data Augmentation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">We use Meta’s “No Language Left Behind (NLLB) open-source models” that provide direct high-quality translations for 200 languages with many low-resource languages <cite class="ltx_cite ltx_citemacro_citep">(Costa-jussà et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib10" title="">2022</a>)</cite>. We use four of the translation models<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>
<a class="ltx_ref ltx_href" href="https://huggingface.co/facebook/nllb-200-3.3B" title="">facebook/nllb-200-3.3B</a>,
<a class="ltx_ref ltx_href" href="https://huggingface.co/facebook/nllb-200-1.3B" title="">facebook/nllb-200-1.3B</a>,
<a class="ltx_ref ltx_href" href="https://huggingface.co/facebook/nllb-200-distilled-1.3B" title="">facebook/nllb-200-distilled-1.3B</a>,
<a class="ltx_ref ltx_href" href="https://huggingface.co/facebook/nllb-200-distilled-600M" title="">facebook/nllb-200-distilled-600M</a>
</span></span></span>
ranging from 600 million to 3.3 billion parameters. Using each of the 4 models, we translated the training data for all languages in track A, except Amharic and Algerian Arabic, and obtained 4 translated datasets for each language. None of the 4 models we used supported Amharic, Algerian Arabic or Punjabi. We decided against translating track C languages with 3 of 12 unsupported languages. Using 4 different model translations gave us a 4-fold augmentation of the training data. We translated the test data using only the largest model (<em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.1">facebook/all-200-3.3 B</em>) to obtain the best translation features.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span><span class="ltx_text ltx_font_italic" id="S2.SS2.1.1">TranSem</span> Model</h3>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="374" id="S2.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of TranSem model architecture (Inspired by <cite class="ltx_cite ltx_citemacro_citet">Reimers et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib30" title="">2019</a>)</cite>). The encoder (<math alttext="\theta" class="ltx_Math" display="inline" id="S2.F1.4.m1.1"><semantics id="S2.F1.4.m1.1b"><mi id="S2.F1.4.m1.1.1" xref="S2.F1.4.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S2.F1.4.m1.1c"><ci id="S2.F1.4.m1.1.1.cmml" xref="S2.F1.4.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.4.m1.1d">\theta</annotation><annotation encoding="application/x-llamapun" id="S2.F1.4.m1.1e">italic_θ</annotation></semantics></math>) is shared, and the diamond box represents the loss function. The encoded sentence pairs (<math alttext="x_{1},x_{2}" class="ltx_Math" display="inline" id="S2.F1.5.m2.2"><semantics id="S2.F1.5.m2.2b"><mrow id="S2.F1.5.m2.2.2.2" xref="S2.F1.5.m2.2.2.3.cmml"><msub id="S2.F1.5.m2.1.1.1.1" xref="S2.F1.5.m2.1.1.1.1.cmml"><mi id="S2.F1.5.m2.1.1.1.1.2" xref="S2.F1.5.m2.1.1.1.1.2.cmml">x</mi><mn id="S2.F1.5.m2.1.1.1.1.3" xref="S2.F1.5.m2.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.F1.5.m2.2.2.2.3" xref="S2.F1.5.m2.2.2.3.cmml">,</mo><msub id="S2.F1.5.m2.2.2.2.2" xref="S2.F1.5.m2.2.2.2.2.cmml"><mi id="S2.F1.5.m2.2.2.2.2.2" xref="S2.F1.5.m2.2.2.2.2.2.cmml">x</mi><mn id="S2.F1.5.m2.2.2.2.2.3" xref="S2.F1.5.m2.2.2.2.2.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.F1.5.m2.2c"><list id="S2.F1.5.m2.2.2.3.cmml" xref="S2.F1.5.m2.2.2.2"><apply id="S2.F1.5.m2.1.1.1.1.cmml" xref="S2.F1.5.m2.1.1.1.1"><csymbol cd="ambiguous" id="S2.F1.5.m2.1.1.1.1.1.cmml" xref="S2.F1.5.m2.1.1.1.1">subscript</csymbol><ci id="S2.F1.5.m2.1.1.1.1.2.cmml" xref="S2.F1.5.m2.1.1.1.1.2">𝑥</ci><cn id="S2.F1.5.m2.1.1.1.1.3.cmml" type="integer" xref="S2.F1.5.m2.1.1.1.1.3">1</cn></apply><apply id="S2.F1.5.m2.2.2.2.2.cmml" xref="S2.F1.5.m2.2.2.2.2"><csymbol cd="ambiguous" id="S2.F1.5.m2.2.2.2.2.1.cmml" xref="S2.F1.5.m2.2.2.2.2">subscript</csymbol><ci id="S2.F1.5.m2.2.2.2.2.2.cmml" xref="S2.F1.5.m2.2.2.2.2.2">𝑥</ci><cn id="S2.F1.5.m2.2.2.2.2.3.cmml" type="integer" xref="S2.F1.5.m2.2.2.2.2.3">2</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.5.m2.2d">x_{1},x_{2}</annotation><annotation encoding="application/x-llamapun" id="S2.F1.5.m2.2e">italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>) and the label (<math alttext="y" class="ltx_Math" display="inline" id="S2.F1.6.m3.1"><semantics id="S2.F1.6.m3.1b"><mi id="S2.F1.6.m3.1.1" xref="S2.F1.6.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.F1.6.m3.1c"><ci id="S2.F1.6.m3.1.1.cmml" xref="S2.F1.6.m3.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.6.m3.1d">y</annotation><annotation encoding="application/x-llamapun" id="S2.F1.6.m3.1e">italic_y</annotation></semantics></math>) are the input to the cosine similarity loss.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.5">Inspired by <cite class="ltx_cite ltx_citemacro_citet">Reimers et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib30" title="">2019</a>)</cite>, we used the Siamese model architecture (shown in <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S2.F1" title="Figure 1 ‣ 2.2 TranSem Model ‣ 2 System Overview ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>). For a given pair of sentences (<math alttext="x_{1},x_{2}" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.2"><semantics id="S2.SS2.p1.1.m1.2a"><mrow id="S2.SS2.p1.1.m1.2.2.2" xref="S2.SS2.p1.1.m1.2.2.3.cmml"><msub id="S2.SS2.p1.1.m1.1.1.1.1" xref="S2.SS2.p1.1.m1.1.1.1.1.cmml"><mi id="S2.SS2.p1.1.m1.1.1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.1.1.2.cmml">x</mi><mn id="S2.SS2.p1.1.m1.1.1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS2.p1.1.m1.2.2.2.3" xref="S2.SS2.p1.1.m1.2.2.3.cmml">,</mo><msub id="S2.SS2.p1.1.m1.2.2.2.2" xref="S2.SS2.p1.1.m1.2.2.2.2.cmml"><mi id="S2.SS2.p1.1.m1.2.2.2.2.2" xref="S2.SS2.p1.1.m1.2.2.2.2.2.cmml">x</mi><mn id="S2.SS2.p1.1.m1.2.2.2.2.3" xref="S2.SS2.p1.1.m1.2.2.2.2.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.2b"><list id="S2.SS2.p1.1.m1.2.2.3.cmml" xref="S2.SS2.p1.1.m1.2.2.2"><apply id="S2.SS2.p1.1.m1.1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S2.SS2.p1.1.m1.1.1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.1.1.2">𝑥</ci><cn id="S2.SS2.p1.1.m1.1.1.1.1.3.cmml" type="integer" xref="S2.SS2.p1.1.m1.1.1.1.1.3">1</cn></apply><apply id="S2.SS2.p1.1.m1.2.2.2.2.cmml" xref="S2.SS2.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.2.2.2.2.1.cmml" xref="S2.SS2.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S2.SS2.p1.1.m1.2.2.2.2.2.cmml" xref="S2.SS2.p1.1.m1.2.2.2.2.2">𝑥</ci><cn id="S2.SS2.p1.1.m1.2.2.2.2.3.cmml" type="integer" xref="S2.SS2.p1.1.m1.2.2.2.2.3">2</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.2c">x_{1},x_{2}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.2d">italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>) and their semantic relatedness score (<math alttext="y" class="ltx_Math" display="inline" id="S2.SS2.p1.2.m2.1"><semantics id="S2.SS2.p1.2.m2.1a"><mi id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><ci id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">y</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.2.m2.1d">italic_y</annotation></semantics></math>), we encode each sentence with a sentence encoder (<math alttext="\theta" class="ltx_Math" display="inline" id="S2.SS2.p1.3.m3.1"><semantics id="S2.SS2.p1.3.m3.1a"><mi id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><ci id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.3.m3.1d">italic_θ</annotation></semantics></math>). The embeddings for the sentences go through a pooling operation (<math alttext="P" class="ltx_Math" display="inline" id="S2.SS2.p1.4.m4.1"><semantics id="S2.SS2.p1.4.m4.1a"><mi id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><ci id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">P</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.4.m4.1d">italic_P</annotation></semantics></math>) to produce sentence embeddings (<math alttext="s_{1},s_{2}" class="ltx_Math" display="inline" id="S2.SS2.p1.5.m5.2"><semantics id="S2.SS2.p1.5.m5.2a"><mrow id="S2.SS2.p1.5.m5.2.2.2" xref="S2.SS2.p1.5.m5.2.2.3.cmml"><msub id="S2.SS2.p1.5.m5.1.1.1.1" xref="S2.SS2.p1.5.m5.1.1.1.1.cmml"><mi id="S2.SS2.p1.5.m5.1.1.1.1.2" xref="S2.SS2.p1.5.m5.1.1.1.1.2.cmml">s</mi><mn id="S2.SS2.p1.5.m5.1.1.1.1.3" xref="S2.SS2.p1.5.m5.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS2.p1.5.m5.2.2.2.3" xref="S2.SS2.p1.5.m5.2.2.3.cmml">,</mo><msub id="S2.SS2.p1.5.m5.2.2.2.2" xref="S2.SS2.p1.5.m5.2.2.2.2.cmml"><mi id="S2.SS2.p1.5.m5.2.2.2.2.2" xref="S2.SS2.p1.5.m5.2.2.2.2.2.cmml">s</mi><mn id="S2.SS2.p1.5.m5.2.2.2.2.3" xref="S2.SS2.p1.5.m5.2.2.2.2.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.2b"><list id="S2.SS2.p1.5.m5.2.2.3.cmml" xref="S2.SS2.p1.5.m5.2.2.2"><apply id="S2.SS2.p1.5.m5.1.1.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.5.m5.1.1.1.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1.1.1">subscript</csymbol><ci id="S2.SS2.p1.5.m5.1.1.1.1.2.cmml" xref="S2.SS2.p1.5.m5.1.1.1.1.2">𝑠</ci><cn id="S2.SS2.p1.5.m5.1.1.1.1.3.cmml" type="integer" xref="S2.SS2.p1.5.m5.1.1.1.1.3">1</cn></apply><apply id="S2.SS2.p1.5.m5.2.2.2.2.cmml" xref="S2.SS2.p1.5.m5.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.p1.5.m5.2.2.2.2.1.cmml" xref="S2.SS2.p1.5.m5.2.2.2.2">subscript</csymbol><ci id="S2.SS2.p1.5.m5.2.2.2.2.2.cmml" xref="S2.SS2.p1.5.m5.2.2.2.2.2">𝑠</ci><cn id="S2.SS2.p1.5.m5.2.2.2.2.3.cmml" type="integer" xref="S2.SS2.p1.5.m5.2.2.2.2.3">2</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.2c">s_{1},s_{2}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.5.m5.2d">italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>). The cosine similarity of the encoded embeddings is trained to match the semantic relatedness score using the mean-squared error loss:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}=MSE(\text{cos-sim}(P\theta(x_{1}),P\theta(x_{2})),y)" class="ltx_Math" display="block" id="S2.E1.m1.2"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.2.2.3" xref="S2.E1.m1.2.2.3.cmml">ℒ</mi><mo id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.2.cmml">=</mo><mrow id="S2.E1.m1.2.2.1" xref="S2.E1.m1.2.2.1.cmml"><mi id="S2.E1.m1.2.2.1.3" xref="S2.E1.m1.2.2.1.3.cmml">M</mi><mo id="S2.E1.m1.2.2.1.2" xref="S2.E1.m1.2.2.1.2.cmml">⁢</mo><mi id="S2.E1.m1.2.2.1.4" xref="S2.E1.m1.2.2.1.4.cmml">S</mi><mo id="S2.E1.m1.2.2.1.2a" xref="S2.E1.m1.2.2.1.2.cmml">⁢</mo><mi id="S2.E1.m1.2.2.1.5" xref="S2.E1.m1.2.2.1.5.cmml">E</mi><mo id="S2.E1.m1.2.2.1.2b" xref="S2.E1.m1.2.2.1.2.cmml">⁢</mo><mrow id="S2.E1.m1.2.2.1.1.1" xref="S2.E1.m1.2.2.1.1.2.cmml"><mo id="S2.E1.m1.2.2.1.1.1.2" stretchy="false" xref="S2.E1.m1.2.2.1.1.2.cmml">(</mo><mrow id="S2.E1.m1.2.2.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.cmml"><mtext id="S2.E1.m1.2.2.1.1.1.1.4" xref="S2.E1.m1.2.2.1.1.1.1.4a.cmml">cos-sim</mtext><mo id="S2.E1.m1.2.2.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.3.cmml">⁢</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.2.2" xref="S2.E1.m1.2.2.1.1.1.1.2.3.cmml"><mo id="S2.E1.m1.2.2.1.1.1.1.2.2.3" stretchy="false" xref="S2.E1.m1.2.2.1.1.1.1.2.3.cmml">(</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml">P</mi><mo id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml">⁢</mo><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.4" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.4.cmml">θ</mi><mo id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2a" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mn id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.2.2.1.1.1.1.2.2.4" xref="S2.E1.m1.2.2.1.1.1.1.2.3.cmml">,</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.2.2.2" xref="S2.E1.m1.2.2.1.1.1.1.2.2.2.cmml"><mi id="S2.E1.m1.2.2.1.1.1.1.2.2.2.3" xref="S2.E1.m1.2.2.1.1.1.1.2.2.2.3.cmml">P</mi><mo id="S2.E1.m1.2.2.1.1.1.1.2.2.2.2" xref="S2.E1.m1.2.2.1.1.1.1.2.2.2.2.cmml">⁢</mo><mi id="S2.E1.m1.2.2.1.1.1.1.2.2.2.4" xref="S2.E1.m1.2.2.1.1.1.1.2.2.2.4.cmml">θ</mi><mo id="S2.E1.m1.2.2.1.1.1.1.2.2.2.2a" xref="S2.E1.m1.2.2.1.1.1.1.2.2.2.2.cmml">⁢</mo><mrow id="S2.E1.m1.2.2.1.1.1.1.2.2.2.1.1" xref="S2.E1.m1.2.2.1.1.1.1.2.2.2.1.1.1.cmml"><mo id="S2.E1.m1.2.2.1.1.1.1.2.2.2.1.1.2" stretchy="false" xref="S2.E1.m1.2.2.1.1.1.1.2.2.2.1.1.1.cmml">(</mo><msub id="S2.E1.m1.2.2.1.1.1.1.2.2.2.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.2.2.2.1.1.1.cmml"><mi id="S2.E1.m1.2.2.1.1.1.1.2.2.2.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.2.2.2.1.1.1.2.cmml">x</mi><mn id="S2.E1.m1.2.2.1.1.1.1.2.2.2.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.2.2.2.1.1.1.3.cmml">2</mn></msub><mo id="S2.E1.m1.2.2.1.1.1.1.2.2.2.1.1.3" stretchy="false" xref="S2.E1.m1.2.2.1.1.1.1.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.2.2.1.1.1.1.2.2.5" stretchy="false" xref="S2.E1.m1.2.2.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.2.2.1.1.1.3" xref="S2.E1.m1.2.2.1.1.2.cmml">,</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">y</mi><mo id="S2.E1.m1.2.2.1.1.1.4" stretchy="false" xref="S2.E1.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2"><eq id="S2.E1.m1.2.2.2.cmml" xref="S2.E1.m1.2.2.2"></eq><ci id="S2.E1.m1.2.2.3.cmml" xref="S2.E1.m1.2.2.3">ℒ</ci><apply id="S2.E1.m1.2.2.1.cmml" xref="S2.E1.m1.2.2.1"><times id="S2.E1.m1.2.2.1.2.cmml" xref="S2.E1.m1.2.2.1.2"></times><ci id="S2.E1.m1.2.2.1.3.cmml" xref="S2.E1.m1.2.2.1.3">𝑀</ci><ci id="S2.E1.m1.2.2.1.4.cmml" xref="S2.E1.m1.2.2.1.4">𝑆</ci><ci id="S2.E1.m1.2.2.1.5.cmml" xref="S2.E1.m1.2.2.1.5">𝐸</ci><interval closure="open" id="S2.E1.m1.2.2.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1"><apply id="S2.E1.m1.2.2.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1"><times id="S2.E1.m1.2.2.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.3"></times><ci id="S2.E1.m1.2.2.1.1.1.1.4a.cmml" xref="S2.E1.m1.2.2.1.1.1.1.4"><mtext id="S2.E1.m1.2.2.1.1.1.1.4.cmml" xref="S2.E1.m1.2.2.1.1.1.1.4">cos-sim</mtext></ci><interval closure="open" id="S2.E1.m1.2.2.1.1.1.1.2.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.2.2"><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1"><times id="S2.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.2"></times><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.3">𝑃</ci><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.4.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.4">𝜃</ci><apply id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><cn id="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S2.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3">1</cn></apply></apply><apply id="S2.E1.m1.2.2.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.2.2.2"><times id="S2.E1.m1.2.2.1.1.1.1.2.2.2.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.2.2.2.2"></times><ci id="S2.E1.m1.2.2.1.1.1.1.2.2.2.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.2.2.2.3">𝑃</ci><ci id="S2.E1.m1.2.2.1.1.1.1.2.2.2.4.cmml" xref="S2.E1.m1.2.2.1.1.1.1.2.2.2.4">𝜃</ci><apply id="S2.E1.m1.2.2.1.1.1.1.2.2.2.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.2.2.2.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.1.1.2.2.2.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1.2.2.2.1.1">subscript</csymbol><ci id="S2.E1.m1.2.2.1.1.1.1.2.2.2.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.2.2.2.1.1.1.2">𝑥</ci><cn id="S2.E1.m1.2.2.1.1.1.1.2.2.2.1.1.1.3.cmml" type="integer" xref="S2.E1.m1.2.2.1.1.1.1.2.2.2.1.1.1.3">2</cn></apply></apply></interval></apply><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">𝑦</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">\mathcal{L}=MSE(\text{cos-sim}(P\theta(x_{1}),P\theta(x_{2})),y)</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.2d">caligraphic_L = italic_M italic_S italic_E ( cos-sim ( italic_P italic_θ ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , italic_P italic_θ ( italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ) , italic_y )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">We experimented with several sentence encoding models for encoding our translated and augmented training dataset. We chose DistilRoberta<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_href" href="https://huggingface.co/sentence-transformers/all-distilroberta-v1" title="">sentence-transformers/all-distilroberta-v1</a></span></span></span> to submit results for the competition leaderboard based on our primary validation (details on <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S4.SS1.SSS1" title="4.1.1 Effect of Batch Size ‣ 4.1 Track A Languages ‣ 4 Results ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">4.1.1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S4.SS1.SSS2" title="4.1.2 Effect of Encoder Pooling ‣ 4.1 Track A Languages ‣ 4 Results ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">4.1.2</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S4.SS1.SSS3" title="4.1.3 Effect of Sentence Embedding Models ‣ 4.1 Track A Languages ‣ 4 Results ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">4.1.3</span></a>). This is a distilled version of RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib19" title="">2019</a>)</cite> fine-tuned on sentence-level datasets and suitable for clustering and semantic searches, which we further fine-tuned on our translated and augmented dataset. The sentence-t5-xl embedding model was chosen to compare the effectiveness of sentence embeddings as opposed to the direct fine-tuning used in the <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.1">FineSem</span> model. After experimenting with different pooling mechanisms of mean, max, and CLS tokens, we found that mean pooling works well for our setting. This aligns with earlier findings, which show that mean pooling produced encodings lead to better performance on downstream tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span><span class="ltx_text ltx_font_italic" id="S2.SS3.1.1">FineSem</span> Model</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">T5 <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib28" title="">2020</a>)</cite> is a transformer model that uses transfer learning; the model trained on “Colossal Clean Crawled Corpus” is fine-tuned on a mixture of 8 downstream unsupervised and supervised tasks converting them into a unified text-to-text task setting. The T5 model is available in several sizes, of which we use the base, large, and XL models ranging from (660 million to 3 billion parameters). One of the supervised tasks used in the T5 model training is the semantic textual similarity benchmark (STS-B) dataset trained as a regression classification problem. We use the STS task setting to train on the track A STR training datasets using 3 different options: separate T5 models trained on individual languages, a single model trained on all 14 languages (without translation), and a single model trained on the translated and augmented dataset (for 12 languages). These settings allow us to contrast the effectiveness of direct fine-tuning with the sentence embedding-based <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.1">transem</span> model and the usefulness of machine translation.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">From the T5 models fine-tuned on the individual languages, we use the English and Spanish fine-tuned models (we refer to these models as the English and Spanish models) for evaluating the cross-lingual Track C languages. We use the English model to evaluate development and test data from all languages except English and the Spanish model to evaluate the English data.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setup</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>HyperParameters</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We train our models using AdamW <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov and Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib20" title="">2017</a>)</cite> optimizer with a learning rate of 1e-5 and weight decay of 0.01. We use an effective batch size of 32 (batch size 16 with 2 steps of gradient accumulation) (for TransSem) and a batch size of 16 (for FineSem).</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">We train our <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.1">transem</span> model infinitely with an early stopping patience of 10 on the validation Spearman Correlation score. We train the <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.2">finesem</span> model for 10 epochs (2 epochs for the model trained on the translated and augmented model) and checkpoint the models at the end of every epoch. We evaluate the dev sets for each language against these 10 checkpoints. We evaluate the corresponding test data using the checkpoint which provides the best performance on the dev data for a language.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Infrastructure</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">All experiments were conducted on an NVIDIA Quadro RTX 8000 with 48GB of VRAM and A100 80GB. We utilize the PyTorch Lightning library <span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lightning.ai/" title="">https://lightning.ai/</a></span></span></span> for conducting the experiments and Weight &amp; Biases <span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://wandb.ai/" title="">https://wandb.ai/</a></span></span></span> for logging purposes (for the <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">TransSem</span> Model) and HuggingFace Transformers (for the <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2">FineSem</span> Model).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We first report our results and analysis on Track A languages (<a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S4.SS1" title="4.1 Track A Languages ‣ 4 Results ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>), and then on Track C (<a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S4.SS2" title="4.2 Track C Languages ‣ 4 Results ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>). We use the official baseline (<cite class="ltx_cite ltx_citemacro_citet">Ousidhoum et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib26" title="">2024b</a>)</cite>) that used LaBSE <cite class="ltx_cite ltx_citemacro_citep">(Feng et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib13" title="">2020</a>)</cite> fine-tuned on the provided training dataset and refer to this model as <math alttext="baseline" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mrow id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">b</mi><mo id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml">a</mi><mo id="S4.p1.1.m1.1.1.1a" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.4" xref="S4.p1.1.m1.1.1.4.cmml">s</mi><mo id="S4.p1.1.m1.1.1.1b" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.5" xref="S4.p1.1.m1.1.1.5.cmml">e</mi><mo id="S4.p1.1.m1.1.1.1c" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.6" xref="S4.p1.1.m1.1.1.6.cmml">l</mi><mo id="S4.p1.1.m1.1.1.1d" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.7" xref="S4.p1.1.m1.1.1.7.cmml">i</mi><mo id="S4.p1.1.m1.1.1.1e" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.8" xref="S4.p1.1.m1.1.1.8.cmml">n</mi><mo id="S4.p1.1.m1.1.1.1f" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.9" xref="S4.p1.1.m1.1.1.9.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><times id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1"></times><ci id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">𝑏</ci><ci id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3">𝑎</ci><ci id="S4.p1.1.m1.1.1.4.cmml" xref="S4.p1.1.m1.1.1.4">𝑠</ci><ci id="S4.p1.1.m1.1.1.5.cmml" xref="S4.p1.1.m1.1.1.5">𝑒</ci><ci id="S4.p1.1.m1.1.1.6.cmml" xref="S4.p1.1.m1.1.1.6">𝑙</ci><ci id="S4.p1.1.m1.1.1.7.cmml" xref="S4.p1.1.m1.1.1.7">𝑖</ci><ci id="S4.p1.1.m1.1.1.8.cmml" xref="S4.p1.1.m1.1.1.8">𝑛</ci><ci id="S4.p1.1.m1.1.1.9.cmml" xref="S4.p1.1.m1.1.1.9">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">baseline</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">italic_b italic_a italic_s italic_e italic_l italic_i italic_n italic_e</annotation></semantics></math>.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Track A Languages</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">This section discusses our findings using various model settings with the <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">TranSem</span> model.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Effect of Batch Size</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">Comparing performance with various batch sizes (results are shown in <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S4.T1" title="Table 1 ‣ 4.1.1 Effect of Batch Size ‣ 4.1 Track A Languages ‣ 4 Results ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>), we show that our batch size selections are fairly good (32 for task <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p1.1.1">TranSem</span> and 8 for <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p1.1.2">FineSem</span>).</p>
</div>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.5" style="width:433.6pt;height:281.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(87.7pt,-56.9pt) scale(1.6792466633079,1.6792466633079) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.5.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row" id="S4.T1.5.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.5.1.1.1.2">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.1.1.2.1">
<tr class="ltx_tr" id="S4.T1.5.1.1.1.2.1.1">
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.1.1.2.1.1.1">Batch</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.1.1.2.1.2">
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.1.1.2.1.2.1">Size</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.5.1.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.1.1.3.1">
<tr class="ltx_tr" id="S4.T1.5.1.1.1.3.1.1">
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.1.1.3.1.1.1">A3</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.1.1.3.1.2">
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.1.1.3.1.2.1">eng</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.5.1.1.1.4">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.1.1.4.1">
<tr class="ltx_tr" id="S4.T1.5.1.1.1.4.1.1">
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.1.1.4.1.1.1">A4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.1.1.4.1.2">
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.1.1.4.1.2.1">hau</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.5.1.1.1.5">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.1.1.5.1">
<tr class="ltx_tr" id="S4.T1.5.1.1.1.5.1.1">
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.1.1.5.1.1.1">A5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.1.1.5.1.2">
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.1.1.5.1.2.1">kin</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.5.1.1.1.6">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.1.1.6.1">
<tr class="ltx_tr" id="S4.T1.5.1.1.1.6.1.1">
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.1.1.6.1.1.1">A6</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.1.1.6.1.2">
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.1.1.6.1.2.1">mar</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.5.1.1.1.7">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.1.1.7.1">
<tr class="ltx_tr" id="S4.T1.5.1.1.1.7.1.1">
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.1.1.7.1.1.1">A7</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.1.1.7.1.2">
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.1.1.7.1.2.1">ary</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.5.1.1.1.8">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.1.1.8.1">
<tr class="ltx_tr" id="S4.T1.5.1.1.1.8.1.1">
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.1.1.8.1.1.1">A8</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.1.1.8.1.2">
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.1.1.8.1.2.1">esp</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.5.1.1.1.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.5.1.1.1.9.1">
<tr class="ltx_tr" id="S4.T1.5.1.1.1.9.1.1">
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.1.1.9.1.1.1">A9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.1.1.9.1.2">
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.1.1.9.1.2.1">tel</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.5.1.1.1.10">avg</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.5.1.2.1.1">TranSem+</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.2.1.2">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.2.1.3">.8102</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.2.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.2.1.4.1">.6857</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.2.1.5">.6886</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.2.1.6"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.2.1.6.1">.8515</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.2.1.7">.7376</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.2.1.8"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.2.1.8.1">.6519</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.2.1.9"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.2.1.9.1">.8342</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.1.2.1.10"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.2.1.10.1">.7514</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.5.1.3.2.1">TranSem+</th>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.3.2.2">4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.3.2.3">.5415</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.3.2.4">.1355</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.3.2.5">.0643</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.3.2.6">.5394</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.3.2.7">.3038</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.3.2.8">.5998</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.3.2.9">.4231</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.3.2.10">.3725</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.5.1.4.3.1">TranSem+</th>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.4.3.2">8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.4.3.3">.8132</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.4.3.4">.6519</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.4.3.5">.6642</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.4.3.6">.8348</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.4.3.7">.7340</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.4.3.8">.6355</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.4.3.9">.8187</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.4.3.10">.7360</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.5.1.5.4.1">TranSem+</th>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.5.4.2">16</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.5.4.3">.8093</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.5.4.4">.6377</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.5.4.5"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.5.4.5.1">.6997</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.5.4.6">.8429</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.5.4.7"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.5.4.7.1">.7500</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.5.4.8">.6462</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.5.4.9">.8257</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.5.4.10">.7445</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.5.1.6.5.1">TranSem+</th>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.6.5.2">64</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.6.5.3">.8092</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.6.5.4">.6589</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.6.5.5">.6456</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.6.5.6">.8271</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.6.5.7">.7247</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.6.5.8">.6234</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.6.5.9">.8131</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.6.5.10">.7289</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.5.1.7.6.1">TranSem+</th>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.7.6.2">128</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.7.6.3">.8129</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.7.6.4">.6787</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.7.6.5">.6659</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.7.6.6">.8417</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.7.6.7">.7411</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.7.6.8">.6396</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.7.6.9">.8324</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.7.6.10">.7446</td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.5.1.8.7.1">TranSem+</th>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.8.7.2">256</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.8.7.3"><span class="ltx_text ltx_font_bold" id="S4.T1.5.1.8.7.3.1">.8152</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.8.7.4">.6716</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.8.7.5">.6589</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.8.7.6">.8357</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.8.7.7">.7197</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.8.7.8">.6353</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.8.7.9">.8189</td>
<td class="ltx_td ltx_align_center" id="S4.T1.5.1.8.7.10">.7365</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The effect of batch size on <math alttext="TranSem" class="ltx_Math" display="inline" id="S4.T1.3.m1.1"><semantics id="S4.T1.3.m1.1b"><mrow id="S4.T1.3.m1.1.1" xref="S4.T1.3.m1.1.1.cmml"><mi id="S4.T1.3.m1.1.1.2" xref="S4.T1.3.m1.1.1.2.cmml">T</mi><mo id="S4.T1.3.m1.1.1.1" xref="S4.T1.3.m1.1.1.1.cmml">⁢</mo><mi id="S4.T1.3.m1.1.1.3" xref="S4.T1.3.m1.1.1.3.cmml">r</mi><mo id="S4.T1.3.m1.1.1.1b" xref="S4.T1.3.m1.1.1.1.cmml">⁢</mo><mi id="S4.T1.3.m1.1.1.4" xref="S4.T1.3.m1.1.1.4.cmml">a</mi><mo id="S4.T1.3.m1.1.1.1c" xref="S4.T1.3.m1.1.1.1.cmml">⁢</mo><mi id="S4.T1.3.m1.1.1.5" xref="S4.T1.3.m1.1.1.5.cmml">n</mi><mo id="S4.T1.3.m1.1.1.1d" xref="S4.T1.3.m1.1.1.1.cmml">⁢</mo><mi id="S4.T1.3.m1.1.1.6" xref="S4.T1.3.m1.1.1.6.cmml">S</mi><mo id="S4.T1.3.m1.1.1.1e" xref="S4.T1.3.m1.1.1.1.cmml">⁢</mo><mi id="S4.T1.3.m1.1.1.7" xref="S4.T1.3.m1.1.1.7.cmml">e</mi><mo id="S4.T1.3.m1.1.1.1f" xref="S4.T1.3.m1.1.1.1.cmml">⁢</mo><mi id="S4.T1.3.m1.1.1.8" xref="S4.T1.3.m1.1.1.8.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.3.m1.1c"><apply id="S4.T1.3.m1.1.1.cmml" xref="S4.T1.3.m1.1.1"><times id="S4.T1.3.m1.1.1.1.cmml" xref="S4.T1.3.m1.1.1.1"></times><ci id="S4.T1.3.m1.1.1.2.cmml" xref="S4.T1.3.m1.1.1.2">𝑇</ci><ci id="S4.T1.3.m1.1.1.3.cmml" xref="S4.T1.3.m1.1.1.3">𝑟</ci><ci id="S4.T1.3.m1.1.1.4.cmml" xref="S4.T1.3.m1.1.1.4">𝑎</ci><ci id="S4.T1.3.m1.1.1.5.cmml" xref="S4.T1.3.m1.1.1.5">𝑛</ci><ci id="S4.T1.3.m1.1.1.6.cmml" xref="S4.T1.3.m1.1.1.6">𝑆</ci><ci id="S4.T1.3.m1.1.1.7.cmml" xref="S4.T1.3.m1.1.1.7">𝑒</ci><ci id="S4.T1.3.m1.1.1.8.cmml" xref="S4.T1.3.m1.1.1.8">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.m1.1d">TranSem</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.m1.1e">italic_T italic_r italic_a italic_n italic_S italic_e italic_m</annotation></semantics></math> for different batch sizes <math alttext="\{2,4,8,16,64,128,256\}" class="ltx_Math" display="inline" id="S4.T1.4.m2.7"><semantics id="S4.T1.4.m2.7b"><mrow id="S4.T1.4.m2.7.8.2" xref="S4.T1.4.m2.7.8.1.cmml"><mo id="S4.T1.4.m2.7.8.2.1" stretchy="false" xref="S4.T1.4.m2.7.8.1.cmml">{</mo><mn id="S4.T1.4.m2.1.1" xref="S4.T1.4.m2.1.1.cmml">2</mn><mo id="S4.T1.4.m2.7.8.2.2" xref="S4.T1.4.m2.7.8.1.cmml">,</mo><mn id="S4.T1.4.m2.2.2" xref="S4.T1.4.m2.2.2.cmml">4</mn><mo id="S4.T1.4.m2.7.8.2.3" xref="S4.T1.4.m2.7.8.1.cmml">,</mo><mn id="S4.T1.4.m2.3.3" xref="S4.T1.4.m2.3.3.cmml">8</mn><mo id="S4.T1.4.m2.7.8.2.4" xref="S4.T1.4.m2.7.8.1.cmml">,</mo><mn id="S4.T1.4.m2.4.4" xref="S4.T1.4.m2.4.4.cmml">16</mn><mo id="S4.T1.4.m2.7.8.2.5" xref="S4.T1.4.m2.7.8.1.cmml">,</mo><mn id="S4.T1.4.m2.5.5" xref="S4.T1.4.m2.5.5.cmml">64</mn><mo id="S4.T1.4.m2.7.8.2.6" xref="S4.T1.4.m2.7.8.1.cmml">,</mo><mn id="S4.T1.4.m2.6.6" xref="S4.T1.4.m2.6.6.cmml">128</mn><mo id="S4.T1.4.m2.7.8.2.7" xref="S4.T1.4.m2.7.8.1.cmml">,</mo><mn id="S4.T1.4.m2.7.7" xref="S4.T1.4.m2.7.7.cmml">256</mn><mo id="S4.T1.4.m2.7.8.2.8" stretchy="false" xref="S4.T1.4.m2.7.8.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.4.m2.7c"><set id="S4.T1.4.m2.7.8.1.cmml" xref="S4.T1.4.m2.7.8.2"><cn id="S4.T1.4.m2.1.1.cmml" type="integer" xref="S4.T1.4.m2.1.1">2</cn><cn id="S4.T1.4.m2.2.2.cmml" type="integer" xref="S4.T1.4.m2.2.2">4</cn><cn id="S4.T1.4.m2.3.3.cmml" type="integer" xref="S4.T1.4.m2.3.3">8</cn><cn id="S4.T1.4.m2.4.4.cmml" type="integer" xref="S4.T1.4.m2.4.4">16</cn><cn id="S4.T1.4.m2.5.5.cmml" type="integer" xref="S4.T1.4.m2.5.5">64</cn><cn id="S4.T1.4.m2.6.6.cmml" type="integer" xref="S4.T1.4.m2.6.6">128</cn><cn id="S4.T1.4.m2.7.7.cmml" type="integer" xref="S4.T1.4.m2.7.7">256</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.m2.7d">\{2,4,8,16,64,128,256\}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.m2.7e">{ 2 , 4 , 8 , 16 , 64 , 128 , 256 }</annotation></semantics></math></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Effect of Encoder Pooling</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">In <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S4.T2" title="Table 2 ‣ 4.1.2 Effect of Encoder Pooling ‣ 4.1 Track A Languages ‣ 4 Results ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>, we compare performance using 3 different pooling operations. We used mean pooling for the official results we submitted, as it showed good performance in most languages.
</p>
</div>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.3" style="width:433.6pt;height:161.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(88.5pt,-32.9pt) scale(1.68924112570502,1.68924112570502) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.3.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row" id="S4.T2.3.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.3.1.1.1.2">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.3.1.1.1.2.1">
<tr class="ltx_tr" id="S4.T2.3.1.1.1.2.1.1">
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.1.1.2.1.1.1">Pool</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.1.1.1.2.1.2">
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.1.1.2.1.2.1">ing</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.3.1.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.3.1.1.1.3.1">
<tr class="ltx_tr" id="S4.T2.3.1.1.1.3.1.1">
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.1.1.3.1.1.1">A3</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.1.1.1.3.1.2">
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.1.1.3.1.2.1">eng</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.3.1.1.1.4">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.3.1.1.1.4.1">
<tr class="ltx_tr" id="S4.T2.3.1.1.1.4.1.1">
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.1.1.4.1.1.1">A4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.1.1.1.4.1.2">
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.1.1.4.1.2.1">hau</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.3.1.1.1.5">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.3.1.1.1.5.1">
<tr class="ltx_tr" id="S4.T2.3.1.1.1.5.1.1">
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.1.1.5.1.1.1">A5</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.1.1.1.5.1.2">
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.1.1.5.1.2.1">kin</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.3.1.1.1.6">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.3.1.1.1.6.1">
<tr class="ltx_tr" id="S4.T2.3.1.1.1.6.1.1">
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.1.1.6.1.1.1">A6</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.1.1.1.6.1.2">
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.1.1.6.1.2.1">mar</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.3.1.1.1.7">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.3.1.1.1.7.1">
<tr class="ltx_tr" id="S4.T2.3.1.1.1.7.1.1">
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.1.1.7.1.1.1">A7</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.1.1.1.7.1.2">
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.1.1.7.1.2.1">ary</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.3.1.1.1.8">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.3.1.1.1.8.1">
<tr class="ltx_tr" id="S4.T2.3.1.1.1.8.1.1">
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.1.1.8.1.1.1">A8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.1.1.1.8.1.2">
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.1.1.8.1.2.1">esp</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.3.1.1.1.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.3.1.1.1.9.1">
<tr class="ltx_tr" id="S4.T2.3.1.1.1.9.1.1">
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.1.1.9.1.1.1">A9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.1.1.1.9.1.2">
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.1.1.9.1.2.1">tel</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.3.1.1.1.10">avg</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.3.1.2.1.1">TranSem+</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.1.2.1.2">CLS</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.2.1.3.1">.8133</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.1.2.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.2.1.4.1">.6737</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.1.2.1.5">.6655</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.1.2.1.6">.8339</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.1.2.1.7">.7376</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.1.2.1.8">.6363</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.1.2.1.9"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.2.1.9.1">.8309</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.1.2.1.10">.7416</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.1.3.2.1">TranSem</th>
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.3.2.2">Mean</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.3.2.3">.8125</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.3.2.4">.6403</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.3.2.5"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.3.2.5.1">.6807</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.3.2.6"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.3.2.6.1">.8406</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.3.2.7"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.3.2.7.1">.7448</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.3.2.8"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.3.2.8.1">.7211</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.3.2.9">.8255</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.3.2.10"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.3.2.10.1">.7522</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.1.4.3.1">TranSem+</th>
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.4.3.2">Max</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.4.3.3">.7960</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.4.3.4">.6157</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.4.3.5">.5809</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.4.3.6">.8227</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.4.3.7">.6643</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.4.3.8">.6075</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.4.3.9">.7997</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.1.4.3.10">.6981</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The effect of pooling on <math alttext="TranSem" class="ltx_Math" display="inline" id="S4.T2.2.m1.1"><semantics id="S4.T2.2.m1.1b"><mrow id="S4.T2.2.m1.1.1" xref="S4.T2.2.m1.1.1.cmml"><mi id="S4.T2.2.m1.1.1.2" xref="S4.T2.2.m1.1.1.2.cmml">T</mi><mo id="S4.T2.2.m1.1.1.1" xref="S4.T2.2.m1.1.1.1.cmml">⁢</mo><mi id="S4.T2.2.m1.1.1.3" xref="S4.T2.2.m1.1.1.3.cmml">r</mi><mo id="S4.T2.2.m1.1.1.1b" xref="S4.T2.2.m1.1.1.1.cmml">⁢</mo><mi id="S4.T2.2.m1.1.1.4" xref="S4.T2.2.m1.1.1.4.cmml">a</mi><mo id="S4.T2.2.m1.1.1.1c" xref="S4.T2.2.m1.1.1.1.cmml">⁢</mo><mi id="S4.T2.2.m1.1.1.5" xref="S4.T2.2.m1.1.1.5.cmml">n</mi><mo id="S4.T2.2.m1.1.1.1d" xref="S4.T2.2.m1.1.1.1.cmml">⁢</mo><mi id="S4.T2.2.m1.1.1.6" xref="S4.T2.2.m1.1.1.6.cmml">S</mi><mo id="S4.T2.2.m1.1.1.1e" xref="S4.T2.2.m1.1.1.1.cmml">⁢</mo><mi id="S4.T2.2.m1.1.1.7" xref="S4.T2.2.m1.1.1.7.cmml">e</mi><mo id="S4.T2.2.m1.1.1.1f" xref="S4.T2.2.m1.1.1.1.cmml">⁢</mo><mi id="S4.T2.2.m1.1.1.8" xref="S4.T2.2.m1.1.1.8.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.m1.1c"><apply id="S4.T2.2.m1.1.1.cmml" xref="S4.T2.2.m1.1.1"><times id="S4.T2.2.m1.1.1.1.cmml" xref="S4.T2.2.m1.1.1.1"></times><ci id="S4.T2.2.m1.1.1.2.cmml" xref="S4.T2.2.m1.1.1.2">𝑇</ci><ci id="S4.T2.2.m1.1.1.3.cmml" xref="S4.T2.2.m1.1.1.3">𝑟</ci><ci id="S4.T2.2.m1.1.1.4.cmml" xref="S4.T2.2.m1.1.1.4">𝑎</ci><ci id="S4.T2.2.m1.1.1.5.cmml" xref="S4.T2.2.m1.1.1.5">𝑛</ci><ci id="S4.T2.2.m1.1.1.6.cmml" xref="S4.T2.2.m1.1.1.6">𝑆</ci><ci id="S4.T2.2.m1.1.1.7.cmml" xref="S4.T2.2.m1.1.1.7">𝑒</ci><ci id="S4.T2.2.m1.1.1.8.cmml" xref="S4.T2.2.m1.1.1.8">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.m1.1d">TranSem</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.m1.1e">italic_T italic_r italic_a italic_n italic_S italic_e italic_m</annotation></semantics></math> using different pooling mechanisms (CLS Token, Mean, Max)</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Effect of Sentence Embedding Models</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">In <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S4.T3" title="Table 3 ‣ 4.1.3 Effect of Sentence Embedding Models ‣ 4.1 Track A Languages ‣ 4 Results ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>, we provide contrastive results with several sentence embedding models used in <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p1.1.1">TranSem</span>.
For official results, we submitted results from the distilroberta-v1 sentence embedding model (results for some of the languages are from the <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p1.1.2">FineSem</span> model fine-tuned on individual STR training datasets).</p>
</div>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.5" style="width:424.9pt;height:305.1pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-6.0pt,4.3pt) scale(0.972519756742635,0.972519756742635) ;">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T3.5.1" style="width:433.6pt;height:312.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(63.3pt,-45.7pt) scale(1.41243933031684,1.41243933031684) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.5.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.5.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.5.1.1.1.1.1">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.5.1.1.1.1.1.1">
<tr class="ltx_tr" id="S4.T3.5.1.1.1.1.1.1.1">
<td class="ltx_td ltx_align_center" id="S4.T3.5.1.1.1.1.1.1.1.1">Model</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.5.1.1.1.1.2">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.5.1.1.1.1.2.1">
<tr class="ltx_tr" id="S4.T3.5.1.1.1.1.2.1.1">
<td class="ltx_td ltx_align_center" id="S4.T3.5.1.1.1.1.2.1.1.1">A3</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.1.1.1.1.2.1.2">
<td class="ltx_td ltx_align_center" id="S4.T3.5.1.1.1.1.2.1.2.1">eng</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.5.1.1.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.5.1.1.1.1.3.1">
<tr class="ltx_tr" id="S4.T3.5.1.1.1.1.3.1.1">
<td class="ltx_td ltx_align_center" id="S4.T3.5.1.1.1.1.3.1.1.1">A4</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.1.1.1.1.3.1.2">
<td class="ltx_td ltx_align_center" id="S4.T3.5.1.1.1.1.3.1.2.1">hau</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.5.1.1.1.1.4">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.5.1.1.1.1.4.1">
<tr class="ltx_tr" id="S4.T3.5.1.1.1.1.4.1.1">
<td class="ltx_td ltx_align_center" id="S4.T3.5.1.1.1.1.4.1.1.1">A5</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.1.1.1.1.4.1.2">
<td class="ltx_td ltx_align_center" id="S4.T3.5.1.1.1.1.4.1.2.1">kin</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.5.1.1.1.1.5">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.5.1.1.1.1.5.1">
<tr class="ltx_tr" id="S4.T3.5.1.1.1.1.5.1.1">
<td class="ltx_td ltx_align_center" id="S4.T3.5.1.1.1.1.5.1.1.1">A6</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.1.1.1.1.5.1.2">
<td class="ltx_td ltx_align_center" id="S4.T3.5.1.1.1.1.5.1.2.1">mar</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.5.1.1.1.1.6">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.5.1.1.1.1.6.1">
<tr class="ltx_tr" id="S4.T3.5.1.1.1.1.6.1.1">
<td class="ltx_td ltx_align_center" id="S4.T3.5.1.1.1.1.6.1.1.1">A7</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.1.1.1.1.6.1.2">
<td class="ltx_td ltx_align_center" id="S4.T3.5.1.1.1.1.6.1.2.1">ary</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.5.1.1.1.1.7">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.5.1.1.1.1.7.1">
<tr class="ltx_tr" id="S4.T3.5.1.1.1.1.7.1.1">
<td class="ltx_td ltx_align_center" id="S4.T3.5.1.1.1.1.7.1.1.1">A8</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.1.1.1.1.7.1.2">
<td class="ltx_td ltx_align_center" id="S4.T3.5.1.1.1.1.7.1.2.1">esp</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.5.1.1.1.1.8">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.5.1.1.1.1.8.1">
<tr class="ltx_tr" id="S4.T3.5.1.1.1.1.8.1.1">
<td class="ltx_td ltx_align_center" id="S4.T3.5.1.1.1.1.8.1.1.1">A9</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.1.1.1.1.8.1.2">
<td class="ltx_td ltx_align_center" id="S4.T3.5.1.1.1.1.8.1.2.1">tel</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.5.1.1.1.1.9">avg</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.5.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.5.1.1.2.1.1">baseline</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.5.1.1.2.1.2.1">.8300</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.2.1.3"><span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T3.5.1.1.2.1.3.1">.6900</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.2.1.4"><span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T3.5.1.1.2.1.4.1">.7200</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.2.1.5"><span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T3.5.1.1.2.1.5.1">.8800</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.2.1.6"><span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T3.5.1.1.2.1.6.1">.7700</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.2.1.7">.7000</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.2.1.8">.8200</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.2.1.9"><span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T3.5.1.1.2.1.9.1">.7729</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T3.5.1.1.3.2.1">distilroberta-v1 (TranSem)</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.3.2.2">.8125</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.3.2.3">.6403</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.3.2.4">.6807</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.3.2.5">.8406</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.3.2.6">.7448</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.3.2.7"><span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T3.5.1.1.3.2.7.1">.7211</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.3.2.8">.8255</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.3.2.9">.7522</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.5.1.1.4.3.1">mpnet-base-v2</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.4.3.2">.8104</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.4.3.3">.6692</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.4.3.4">.6971</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.4.3.5"><span class="ltx_text ltx_font_bold" id="S4.T3.5.1.1.4.3.5.1">.8568</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.4.3.6">.7297</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.4.3.7">.6518</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.4.3.8">.8250</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.4.3.9">.7486</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T3.5.1.1.5.4.1">roberta-large-v1</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.5.4.2"><span class="ltx_text ltx_font_bold" id="S4.T3.5.1.1.5.4.2.1">.8260</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.5.4.3">.6750</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.5.4.4"><span class="ltx_text ltx_font_bold" id="S4.T3.5.1.1.5.4.4.1">.7056</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.5.4.5">.8461</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.5.4.6">.7480</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.5.4.7">.6298</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.5.4.8"><span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T3.5.1.1.5.4.8.1">.8394</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.5.4.9"><span class="ltx_text ltx_font_bold" id="S4.T3.5.1.1.5.4.9.1">.7528</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T3.5.1.1.6.5.1">sentence-t5-xl</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.6.5.2">.8236</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.6.5.3">.6440</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.6.5.4">.6720</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.6.5.5">.8324</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.6.5.6">.7124</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.6.5.7">.6277</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.6.5.8">.8250</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.6.5.9">.7339</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T3.5.1.1.7.6.1">multi-qa-mpnet-base-dot-v1</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.7.6.2">.8111</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.7.6.3"><span class="ltx_text ltx_font_bold" id="S4.T3.5.1.1.7.6.3.1">.6852</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.7.6.4">.7041</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.7.6.5">.8482</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.7.6.6">.7163</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.7.6.7">.6586</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.7.6.8">.8245</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.7.6.9">.7497</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T3.5.1.1.8.7.1">all-MiniLM-L12-v2</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.8.7.2">.8237</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.8.7.3">.6474</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.8.7.4">.7026</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.8.7.5">.8522</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.8.7.6"><span class="ltx_text ltx_font_bold" id="S4.T3.5.1.1.8.7.6.1">.7605</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.8.7.7">.6141</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.8.7.8">.8247</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.8.7.9">.7465</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.1.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.5.1.1.9.8.1">FineSem-Individual</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.9.8.2">.8385</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.9.8.3">.6335</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.9.8.4"><span class="ltx_text ltx_font_bold" id="S4.T3.5.1.1.9.8.4.1">.7175</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.9.8.5">.2211</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.9.8.6"><span class="ltx_text ltx_font_bold" id="S4.T3.5.1.1.9.8.6.1">.7647</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.9.8.7">.6900</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.9.8.8">.6085</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.1.1.9.8.9">.6391</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.1.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T3.5.1.1.10.9.1">FineSem-Unified</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.10.9.2"><span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T3.5.1.1.10.9.2.1">.8438</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.10.9.3">.6369</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.10.9.4">.6837</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.10.9.5">.3878</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.10.9.6">.6265</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.10.9.7"><span class="ltx_text ltx_font_bold" id="S4.T3.5.1.1.10.9.7.1">.7040</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.10.9.8">.6993</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.5.1.1.10.9.9">.6546</td>
</tr>
<tr class="ltx_tr" id="S4.T3.5.1.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S4.T3.5.1.1.11.10.1">FineSem-Translated</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T3.5.1.1.11.10.2">.8105</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T3.5.1.1.11.10.3"><span class="ltx_text ltx_font_bold" id="S4.T3.5.1.1.11.10.3.1">.6383</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T3.5.1.1.11.10.4">.7133</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T3.5.1.1.11.10.5"><span class="ltx_text ltx_font_bold" id="S4.T3.5.1.1.11.10.5.1">.8608</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T3.5.1.1.11.10.6">.7403</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T3.5.1.1.11.10.7">.6663</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T3.5.1.1.11.10.8"><span class="ltx_text ltx_font_bold" id="S4.T3.5.1.1.11.10.8.1">.8152</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T3.5.1.1.11.10.9"><span class="ltx_text ltx_font_bold" id="S4.T3.5.1.1.11.10.9.1">.7493</span></td>
</tr>
</tbody>
</table>
</span></div>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Model Performance (Spearman Correlation Coefficient) on Subtask A test set. <math alttext="TranSem" class="ltx_Math" display="inline" id="S4.T3.3.m1.1"><semantics id="S4.T3.3.m1.1b"><mrow id="S4.T3.3.m1.1.1" xref="S4.T3.3.m1.1.1.cmml"><mi id="S4.T3.3.m1.1.1.2" xref="S4.T3.3.m1.1.1.2.cmml">T</mi><mo id="S4.T3.3.m1.1.1.1" xref="S4.T3.3.m1.1.1.1.cmml">⁢</mo><mi id="S4.T3.3.m1.1.1.3" xref="S4.T3.3.m1.1.1.3.cmml">r</mi><mo id="S4.T3.3.m1.1.1.1b" xref="S4.T3.3.m1.1.1.1.cmml">⁢</mo><mi id="S4.T3.3.m1.1.1.4" xref="S4.T3.3.m1.1.1.4.cmml">a</mi><mo id="S4.T3.3.m1.1.1.1c" xref="S4.T3.3.m1.1.1.1.cmml">⁢</mo><mi id="S4.T3.3.m1.1.1.5" xref="S4.T3.3.m1.1.1.5.cmml">n</mi><mo id="S4.T3.3.m1.1.1.1d" xref="S4.T3.3.m1.1.1.1.cmml">⁢</mo><mi id="S4.T3.3.m1.1.1.6" xref="S4.T3.3.m1.1.1.6.cmml">S</mi><mo id="S4.T3.3.m1.1.1.1e" xref="S4.T3.3.m1.1.1.1.cmml">⁢</mo><mi id="S4.T3.3.m1.1.1.7" xref="S4.T3.3.m1.1.1.7.cmml">e</mi><mo id="S4.T3.3.m1.1.1.1f" xref="S4.T3.3.m1.1.1.1.cmml">⁢</mo><mi id="S4.T3.3.m1.1.1.8" xref="S4.T3.3.m1.1.1.8.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.3.m1.1c"><apply id="S4.T3.3.m1.1.1.cmml" xref="S4.T3.3.m1.1.1"><times id="S4.T3.3.m1.1.1.1.cmml" xref="S4.T3.3.m1.1.1.1"></times><ci id="S4.T3.3.m1.1.1.2.cmml" xref="S4.T3.3.m1.1.1.2">𝑇</ci><ci id="S4.T3.3.m1.1.1.3.cmml" xref="S4.T3.3.m1.1.1.3">𝑟</ci><ci id="S4.T3.3.m1.1.1.4.cmml" xref="S4.T3.3.m1.1.1.4">𝑎</ci><ci id="S4.T3.3.m1.1.1.5.cmml" xref="S4.T3.3.m1.1.1.5">𝑛</ci><ci id="S4.T3.3.m1.1.1.6.cmml" xref="S4.T3.3.m1.1.1.6">𝑆</ci><ci id="S4.T3.3.m1.1.1.7.cmml" xref="S4.T3.3.m1.1.1.7">𝑒</ci><ci id="S4.T3.3.m1.1.1.8.cmml" xref="S4.T3.3.m1.1.1.8">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.m1.1d">TranSem</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.m1.1e">italic_T italic_r italic_a italic_n italic_S italic_e italic_m</annotation></semantics></math> shows results submitted before the official deadline, <math alttext="baseline" class="ltx_Math" display="inline" id="S4.T3.4.m2.1"><semantics id="S4.T3.4.m2.1b"><mrow id="S4.T3.4.m2.1.1" xref="S4.T3.4.m2.1.1.cmml"><mi id="S4.T3.4.m2.1.1.2" xref="S4.T3.4.m2.1.1.2.cmml">b</mi><mo id="S4.T3.4.m2.1.1.1" xref="S4.T3.4.m2.1.1.1.cmml">⁢</mo><mi id="S4.T3.4.m2.1.1.3" xref="S4.T3.4.m2.1.1.3.cmml">a</mi><mo id="S4.T3.4.m2.1.1.1b" xref="S4.T3.4.m2.1.1.1.cmml">⁢</mo><mi id="S4.T3.4.m2.1.1.4" xref="S4.T3.4.m2.1.1.4.cmml">s</mi><mo id="S4.T3.4.m2.1.1.1c" xref="S4.T3.4.m2.1.1.1.cmml">⁢</mo><mi id="S4.T3.4.m2.1.1.5" xref="S4.T3.4.m2.1.1.5.cmml">e</mi><mo id="S4.T3.4.m2.1.1.1d" xref="S4.T3.4.m2.1.1.1.cmml">⁢</mo><mi id="S4.T3.4.m2.1.1.6" xref="S4.T3.4.m2.1.1.6.cmml">l</mi><mo id="S4.T3.4.m2.1.1.1e" xref="S4.T3.4.m2.1.1.1.cmml">⁢</mo><mi id="S4.T3.4.m2.1.1.7" xref="S4.T3.4.m2.1.1.7.cmml">i</mi><mo id="S4.T3.4.m2.1.1.1f" xref="S4.T3.4.m2.1.1.1.cmml">⁢</mo><mi id="S4.T3.4.m2.1.1.8" xref="S4.T3.4.m2.1.1.8.cmml">n</mi><mo id="S4.T3.4.m2.1.1.1g" xref="S4.T3.4.m2.1.1.1.cmml">⁢</mo><mi id="S4.T3.4.m2.1.1.9" xref="S4.T3.4.m2.1.1.9.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.4.m2.1c"><apply id="S4.T3.4.m2.1.1.cmml" xref="S4.T3.4.m2.1.1"><times id="S4.T3.4.m2.1.1.1.cmml" xref="S4.T3.4.m2.1.1.1"></times><ci id="S4.T3.4.m2.1.1.2.cmml" xref="S4.T3.4.m2.1.1.2">𝑏</ci><ci id="S4.T3.4.m2.1.1.3.cmml" xref="S4.T3.4.m2.1.1.3">𝑎</ci><ci id="S4.T3.4.m2.1.1.4.cmml" xref="S4.T3.4.m2.1.1.4">𝑠</ci><ci id="S4.T3.4.m2.1.1.5.cmml" xref="S4.T3.4.m2.1.1.5">𝑒</ci><ci id="S4.T3.4.m2.1.1.6.cmml" xref="S4.T3.4.m2.1.1.6">𝑙</ci><ci id="S4.T3.4.m2.1.1.7.cmml" xref="S4.T3.4.m2.1.1.7">𝑖</ci><ci id="S4.T3.4.m2.1.1.8.cmml" xref="S4.T3.4.m2.1.1.8">𝑛</ci><ci id="S4.T3.4.m2.1.1.9.cmml" xref="S4.T3.4.m2.1.1.9">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.m2.1d">baseline</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.m2.1e">italic_b italic_a italic_s italic_e italic_l italic_i italic_n italic_e</annotation></semantics></math> shows official baseline results, and the rest are contrastive results for our various models. The best scores within each section are <span class="ltx_text ltx_font_bold" id="S4.T3.8.1">bolded</span>, and best scores across all sections are <span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T3.9.2">underlined</span>.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4 </span>Usefulness of Machine Translation and Direct Fine-tuning</h4>
<div class="ltx_para" id="S4.SS1.SSS4.p1">
<p class="ltx_p" id="S4.SS1.SSS4.p1.1">We compare the performance of the <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p1.1.1">FineSem</span> models fine-tuned using the 3 data options (results are shown in <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S4.T3" title="Table 3 ‣ 4.1.3 Effect of Sentence Embedding Models ‣ 4.1 Track A Languages ‣ 4 Results ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>). FineSem-Individual shows the performance of T5-XL models fine-tuned on the individual datasets. Unified and Translated models show the performance of the two T5-XL models fine-tuned on the untranslated data and translated<math alttext="+" class="ltx_Math" display="inline" id="S4.SS1.SSS4.p1.1.m1.1"><semantics id="S4.SS1.SSS4.p1.1.m1.1a"><mo id="S4.SS1.SSS4.p1.1.m1.1.1" xref="S4.SS1.SSS4.p1.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS4.p1.1.m1.1b"><plus id="S4.SS1.SSS4.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS4.p1.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS4.p1.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS4.p1.1.m1.1d">+</annotation></semantics></math>augmented data. The model trained on untranslated data performs poorly on the Marathi dataset, but performs on par with the other models indicating that we may not need to translate all languages to English. We find that direct fine-tuning with the translated and augmented data is comparable with the <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p1.1.2">TranSem</span> model using various sentence embeddings.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Track C Languages</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S4.T4" title="Table 4 ‣ 4.2 Track C Languages ‣ 4 Results ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>, we compare the performance of various T5 models on the track C languages. We submitted official results using our T5-XL based FineSem model (FineSem-LB) where the results are obtained using the checkpoint after the third epoch. With the same model we also report with the approach where we use the checkpoint which results in the best performance on the development data for a given language. These results are shown as FineSem-XL. We compare these results with the T5-base and T5-large based FineSem models. We bold the best scores for easy readability but underline scores that are better than the baseline. Among our models the overall performance of the XL model is better and this model improves upon the baseline for Afrikaans, Indonesian and Spanish.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.1" style="width:424.9pt;height:145.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(28.7pt,-9.8pt) scale(1.15631181739161,1.15631181739161) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1.2">C1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1.3">C2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1.4">C3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1.5">C4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1.6">C5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1.7">C6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1.8">C7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1.9">C8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1.10">C9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1.11">C10</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1.12">C11</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1.13">C12</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1.14"></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T4.1.1.2.2.1">Model</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.2.2.2">Afr</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.2.2.3">Arq</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.2.2.4">Amh</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.2.2.5">Eng</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.2.2.6">Hau</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.2.2.7">Hin</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.2.2.8">Ind</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.2.2.9">Kin</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.2.2.10">Arb</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.2.2.11">Ary</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.2.2.12">Pan</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.2.2.13">Esp</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.2.2.14">avg</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.1.3.3.1">Baseline</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.3.2">.7900</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.3.3"><span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T4.1.1.3.3.3.1">.4600</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.3.4">.<span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T4.1.1.3.3.4.1">8400</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.3.5"><span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T4.1.1.3.3.5.1">.8000</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.3.6"><span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T4.1.1.3.3.6.1">.6200</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.3.7"><span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T4.1.1.3.3.7.1">.7600</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.3.8">.4700</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.3.9"><span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T4.1.1.3.3.9.1">.5700</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.3.10"><span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T4.1.1.3.3.10.1">.6100</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.3.11"><span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T4.1.1.3.3.11.1">.4000</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.3.12"><span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T4.1.1.3.3.12.1">-.050</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.3.13">.6200</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.3.3.14">.5742</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T4.1.1.4.4.1">FineSem-LB</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.4.4.2"><span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T4.1.1.4.4.2.1">.8223</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.4.4.3">1263</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.4.4.4">.0430</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.4.4.5">.7875</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.4.4.6">.4569</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.4.4.7">.1552</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.4.4.8"><span class="ltx_text ltx_ulem_uline" id="S4.T4.1.1.4.4.8.1">.5153</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.4.4.9">.4836</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.4.4.10">.0354</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.4.4.11">-.0375</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.4.4.12">-.0775</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.4.4.13">.6089</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.4.4.14"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.4.4.14.1">.3266</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.1.5.5.1">FineSem-XL</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.5.5.2"><span class="ltx_text ltx_ulem_uline" id="S4.T4.1.1.5.5.2.1">.8164</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.5.5.3">1023</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.5.5.4">.0373</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.5.5.5"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.5.5.5.1">.7889</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.5.5.6">.4561</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.5.5.7">.1594</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.5.5.8"><span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T4.1.1.5.5.8.1">.5279</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.5.5.9">.4128</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.5.5.10">.0000</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.5.5.11">.0219</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.5.5.12">-.0817</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.5.5.13"><span class="ltx_text ltx_ulem_uline" id="S4.T4.1.1.5.5.13.1">.6259</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.1.5.5.14">.3246</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S4.T4.1.1.6.6.1">FineSem-L</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.6.6.2">.8007</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.6.6.3">-.0515</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.6.6.4">.0112</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.6.6.5">.7752</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.6.6.6"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.6.6.6.1">.4831</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.6.6.7">.1764</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.6.6.8">.4419</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.6.6.9"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.6.6.9.1">.5094</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.6.6.10">.0154</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.6.6.11"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.6.6.11.1">.0331</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.6.6.12">-.0591</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.6.6.13">.6605</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.1.6.6.14">.3164</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S4.T4.1.1.7.7.1">FineSem-B</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T4.1.1.7.7.2">.7802</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T4.1.1.7.7.3"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.7.7.3.1">.1799</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T4.1.1.7.7.4"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.7.7.4.1">.2543</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T4.1.1.7.7.5">.7448</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T4.1.1.7.7.6">.4784</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T4.1.1.7.7.7"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.7.7.7.1">.2404</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T4.1.1.7.7.8">.4517</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T4.1.1.7.7.9">.3861</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T4.1.1.7.7.10"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.7.7.10.1">.0527</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T4.1.1.7.7.11">.0268</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T4.1.1.7.7.12"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.7.7.12.1">-.0520</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T4.1.1.7.7.13"><span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T4.1.1.7.7.13.1">.6289</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T4.1.1.7.7.14">.3477</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Model performance (Spearman Correlation Coefficient) on Track C test sets. All language test sets (except English) use the FineSem models trained on the English training set. The English test set uses the FineSem models trained on the Spanish training set. The best scores among our models are <span class="ltx_text ltx_font_bold" id="S4.T4.4.1">bolded</span>. Scores better than baseline are <span class="ltx_text ltx_ulem_uline ltx_font_bold" id="S4.T4.5.2">Underlined</span>.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we present previous research conducted in the fields of Machine Translation (see <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S5.SS1" title="5.1 Machine Translation ‣ 5 Related Work ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.1</span></a>), Sentence Embedding (see <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S5.SS2" title="5.2 Sentence Embedding ‣ 5 Related Work ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.2</span></a>) and Semantic Similarity (see <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#S5.SS3" title="5.3 Semantic Similarity ‣ 5 Related Work ‣ UMBCLU at SemEval-2024 Task 1: Semantic Textual Relatedness with and without machine translation"><span class="ltx_text ltx_ref_tag">§</span> <span class="ltx_text ltx_ref_tag">5.3</span></a>).</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Machine Translation</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Machine translation has evolved in the last 75 years from rule-based systems to statistical-based systems to the current neural machine translation (NMT) systems. In the 10 years since the first sequence-to-sequence NMT model <cite class="ltx_cite ltx_citemacro_citep">(Bahdanau et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib3" title="">2014</a>)</cite>, machine translation reached a point where translations from models for high-resource languages rival human translators <cite class="ltx_cite ltx_citemacro_citep">(Läubli et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib18" title="">2018</a>; Popel et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib27" title="">2020</a>)</cite>. This was possible due to the amount of bilingual data pairs available for training in these languages <cite class="ltx_cite ltx_citemacro_citep">(Haddow et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib15" title="">2022</a>)</cite>. Translation systems for medium and low resource languages that lacked the scale of these resources either developed cross-lingual models <cite class="ltx_cite ltx_citemacro_citep">(Nguyen and Chiang, <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib22" title="">2017</a>; Zoph et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib37" title="">2016</a>)</cite> or developed datasets <cite class="ltx_cite ltx_citemacro_citep">(Bañón et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib5" title="">2020</a>; Schwenk et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib32" title="">2019</a>)</cite>. Current state-of-the-art translation models use a many-to-many approach to handle a large number of medium to low-resource languages <cite class="ltx_cite ltx_citemacro_citep">(Costa-jussà et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib10" title="">2022</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Sentence Embedding</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Generating a sentence-level embedding is useful for semantic searches and clustering. Since the first transformer model <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib33" title="">2017</a>)</cite>, several encoder-only models such as BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib12" title="">2018</a>)</cite>, RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib19" title="">2019</a>)</cite>, XLNet <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib35" title="">2018</a>)</cite> were used to learn effective sentence embeddings that also performed well on downstream NLP tasks <cite class="ltx_cite ltx_citemacro_citep">(Cer et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib9" title="">2018</a>; Roy Dipta et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib31" title="">2023</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Reimers et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib30" title="">2019</a>)</cite> developed a Siamese-like network architecture with two BERT sentence embedding models that improved semantic search systems. Using T5 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib28" title="">2020</a>)</cite> models in a similar architecture, sentence embeddings produced by T5 were shown to be superior to the encoder-only model embeddings with performance gains in downstream tasks <cite class="ltx_cite ltx_citemacro_cite">Ni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib23" title="">2021</a>)</cite>. A more recent work <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib29" title="">2020</a>)</cite> showed that a teacher-student model can be efficiently used to develop a sentence embedding system for many low-resource languages.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Semantic Similarity</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Semantic textual similarity captures a type of semantic relatedness requiring similarity on all aspects between a sentence pair. SemEval tasks on semantic textual similarity from 2012 to 2017 resulted in the STS benchmark <cite class="ltx_cite ltx_citemacro_citep">(Cer et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib8" title="">2017</a>)</cite>. Recently, <cite class="ltx_cite ltx_citemacro_citet">Deshpande et al. (<a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib11" title="">2023</a>)</cite> proposed conditional semantic textual similarity to explore semantic relatedness.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion &amp; Future Work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We developed two different models and showed how the models performed in supervised and cross-domain training tasks in 14 languages. We explored using machine translation, sentence encoders, and SST-B style training with T5 models. Our models improved over the official baseline for some of the languages. For computational purposes, we have excluded using more recent models like mistral-7b <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib17" title="">2023</a>)</cite>, which have outperformed most of the open-source and close-source models in various benchmarks <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2402.12730v2#bib.bib36" title="">2024</a>)</cite>. For future work, we intend to explore prompting for STR and prompt-based LLMs <span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chat.openai.com/" title="">https://chat.openai.com/</a></span></span></span> for translation.
</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Disclaimer</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">We did not use AI assistants to write any part of our paper or code. All writing is original and produced by the authors.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Limitations</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">We acknowledge our work has the following limitations. We use several pre-trained LLMs in our experiments. It is well known that these models can echo biases and misinformation either implicitly or explicitly. We did not control for any of these when training them on the STR datasets. In addition, the STR datasets may also echo several biases related to social groups, cultural groups, race, gender, behavioral, and perceptual differences of annotators. We did not explore or control for any of these biases in our work. As a result, our work carries the limitations of both the models and the datasets we used.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdalla et al. (2021)</span>
<span class="ltx_bibblock">
Mohamed Abdalla, Krishnapriya Vishnubhotla, and Saif M. Mohammad. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2110.04845" title="">What makes sentences semantically related: A textual relatedness dataset and empirical study</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">CoRR</em>, abs/2110.04845.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antony et al. (2022)</span>
<span class="ltx_bibblock">
James W Antony, America Romero, Anthony H Vierra, Rebecca S Luenser, Robert D Hawkins, and Kelly A Bennion. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.7554/eLife.72519" title="">Semantic relatedness retroactively boosts memory and promotes memory interdependence across episodes</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">eLife</em>, 11:e72519.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahdanau et al. (2014)</span>
<span class="ltx_bibblock">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014.

</span>
<span class="ltx_bibblock">Neural machine translation by jointly learning to align and translate.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:1409.0473</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banerjee and Pedersen (2003)</span>
<span class="ltx_bibblock">
Satanjeev Banerjee and Ted Pedersen. 2003.

</span>
<span class="ltx_bibblock">Extended gloss overlaps as a measure of semantic relatedness.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 18th International Joint Conference on Artificial Intelligence</em>, IJCAI’03, page 805–810, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bañón et al. (2020)</span>
<span class="ltx_bibblock">
Marta Bañón, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Esplà-Gomis, Mikel Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, et al. 2020.

</span>
<span class="ltx_bibblock">Paracrawl: Web-scale acquisition of parallel corpora.

</span>
<span class="ltx_bibblock">Association for Computational Linguistics (ACL).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bracken et al. (2017)</span>
<span class="ltx_bibblock">
Jennifer Bracken, Tamar Degani, and Natasha Tokowicz Chelsea Eddington. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1017/S1366728916000274" title="">Translation semantic variability: How semantic relatedness affects learning of translation-ambiguous words</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Bilingualism: Language and Cognition</em>, 20(4):783–794.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Budanitsky and Hirst (2006)</span>
<span class="ltx_bibblock">
Alexander Budanitsky and Graeme Hirst. 2006.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/coli.2006.32.1.13" title="">Evaluating WordNet-based Measures of Lexical Semantic Relatedness</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Computational Linguistics</em>, 32(1):13–47.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cer et al. (2017)</span>
<span class="ltx_bibblock">
Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/S17-2001" title="">SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</em>, pages 1–14, Vancouver, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cer et al. (2018)</span>
<span class="ltx_bibblock">
Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, et al. 2018.

</span>
<span class="ltx_bibblock">Universal sentence encoder.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:1803.11175</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costa-jussà et al. (2022)</span>
<span class="ltx_bibblock">
Marta R Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022.

</span>
<span class="ltx_bibblock">No language left behind: Scaling human-centered machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2207.04672</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deshpande et al. (2023)</span>
<span class="ltx_bibblock">
Ameet Deshpande, Carlos E. Jimenez, Howard Chen, Vishvak Murahari, Victoria Graf, Tanmay Rajpurohit, Ashwin Kalyan, Danqi Chen, and Karthik Narasimhan. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.15093" title="">C-sts: Conditional semantic textual similarity</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. (2020)</span>
<span class="ltx_bibblock">
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2020.

</span>
<span class="ltx_bibblock">Language-agnostic bert sentence embedding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2007.01852</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. (2017)</span>
<span class="ltx_bibblock">
Yue Feng, Ebrahim Bagheri, Faezeh Ensan, and Jelena Jovanovic. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1017/S0269888917000029" title="">The state of the art in semantic relatedness: a framework for comparison</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">The Knowledge Engineering Review</em>, 32:e10.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haddow et al. (2022)</span>
<span class="ltx_bibblock">
Barry Haddow, Rachel Bawden, Antonio Valerio Miceli Barone, Jindřich Helcl, and Alexandra Birch. 2022.

</span>
<span class="ltx_bibblock">Survey of low-resource machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Computational Linguistics</em>, 48(3):673–732.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jarmasz and Szpakowicz (2012)</span>
<span class="ltx_bibblock">
Mario Jarmasz and Stanialaw Szpakowicz. 2012.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:1189582" title="">Roget’s thesaurus and semantic similarity</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Recent Advances in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023)</span>
<span class="ltx_bibblock">
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2310.06825</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Läubli et al. (2018)</span>
<span class="ltx_bibblock">
Samuel Läubli, Rico Sennrich, and Martin Volk. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-1512" title="">Has machine translation achieved human parity? a case for document-level evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 4791–4796, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:1907.11692</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2017)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2017.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:1711.05101</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller (1995)</span>
<span class="ltx_bibblock">
George A. Miller. 1995.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/219717.219748" title="">Wordnet: a lexical database for english</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Commun. ACM</em>, 38(11):39–41.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen and Chiang (2017)</span>
<span class="ltx_bibblock">
Toan Q Nguyen and David Chiang. 2017.

</span>
<span class="ltx_bibblock">Transfer learning across low-resource, related languages for neural machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:1708.09803</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ni et al. (2021)</span>
<span class="ltx_bibblock">
Jianmo Ni, Gustavo Hernández Ábrego, Noah Constant, Ji Ma, Keith B. Hall, Daniel Cer, and Yinfei Yang. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2108.08877" title="">Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">CoRR</em>, abs/2108.08877.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Osgood (1949)</span>
<span class="ltx_bibblock">
C. E. Osgood. 1949.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1037/h0057488" title="">The similarity paradox in human learning: a resolution</a>.

</span>
<span class="ltx_bibblock">56:132–143.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ousidhoum et al. (2024a)</span>
<span class="ltx_bibblock">
Nedjma Ousidhoum, Shamsuddeen Hassan Muhammad, Mohamed Abdalla, Idris Abdulmumin, Ibrahim Said Ahmad, Sanchit Ahuja, Alham Fikri Aji, Vladimir Araujo, Abinew Ali Ayele, Pavan Baswani, Meriem Beloucif, Chris Biemann, Sofia Bourhim, Christine De Kock, Genet Shanko Dekebo, Oumaima Hourrane, Gopichand Kanumolu, Lokesh Madasu, Samuel Rutunda, Manish Shrivastava, Thamar Solorio, Nirmal Surange, Hailegnaw Getaneh Tilaye, Krishnapriya Vishnubhotla, Genta Winata, Seid Muhie Yimam, and Saif M. Mohammad. 2024a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2402.08638" title="">Semrel2024: A collection of semantic textual relatedness datasets for 14 languages</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ousidhoum et al. (2024b)</span>
<span class="ltx_bibblock">
Nedjma Ousidhoum, Shamsuddeen Hassan Muhammad, Mohamed Abdalla, Idris Abdulmumin, Ibrahim Said Ahmad, Sanchit Ahuja, Alham Fikri Aji, Vladimir Araujo, Meriem Beloucif, Christine De Kock, Oumaima Hourrane, Manish Shrivastava, Thamar Solorio, Nirmal Surange, Krishnapriya Vishnubhotla, Seid Muhie Yimam, and Saif M. Mohammad. 2024b.

</span>
<span class="ltx_bibblock">SemEval-2024 task 1: Semantic textual relatedness for african and asian languages.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)</em>. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popel et al. (2020)</span>
<span class="ltx_bibblock">
Martin Popel, Marketa Tomkova, Jakub Tomek, Łukasz Kaiser, Jakob Uszkoreit, Ondřej Bojar, and Zdeněk Žabokrtský. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41467-020-18073-9" title="">Transforming machine translation: a deep learning system reaches news translation quality comparable to human professionals</a>.

</span>
<span class="ltx_bibblock">11.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://jmlr.org/papers/v21/20-074.html" title="">Exploring the limits of transfer learning with a unified text-to-text transformer</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Journal of Machine Learning Research</em>, 21(140):1–67.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2020)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2004.09813" title="">Making monolingual sentence embeddings multilingual using knowledge distillation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</em>. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers et al. (2019)</span>
<span class="ltx_bibblock">
Nils Reimers, Nils Reimers, Iryna Gurevych, and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/d19-1410" title="">Sentence-bert: Sentence embeddings using siamese bert-networks</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv: Computation and Language</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roy Dipta et al. (2023)</span>
<span class="ltx_bibblock">
Shubhashis Roy Dipta, Mehdi Rezaee, and Francis Ferraro. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.starsem-1.31" title="">Semantically-informed hierarchical event modeling</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)</em>, pages 353–369, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwenk et al. (2019)</span>
<span class="ltx_bibblock">
Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, and Armand Joulin. 2019.

</span>
<span class="ltx_bibblock">Ccmatrix: Mining billions of high-quality parallel sentences on the web.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:1911.04944</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Bin Wang, C.-C. Jay Kuo, and Haizhou Li. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.acl-long.419" title="">Just rank: Rethinking evaluation with word and sentence similarities</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 6060–6077, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Yinfei Yang, Steve Yuan, Daniel Cer, Sheng-yi Kong, Noah Constant, Petr Pilar, Heming Ge, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-3022" title="">Learning semantic textual similarity from conversations</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the Third Workshop on Representation Learning for NLP</em>, pages 164–174, Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2024)</span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zoph et al. (2016)</span>
<span class="ltx_bibblock">
Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. 2016.

</span>
<span class="ltx_bibblock">Transfer learning for low-resource neural machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:1604.02201</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Apr 12 00:49:31 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
