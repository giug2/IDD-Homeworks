<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations</title>
<!--Generated on Wed Sep  4 13:46:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Low Resource Machine Translation,  Cross-Lingual Sentence Representations,  Indic Languages,  Multilingual Natural Language Processing
" lang="en" name="keywords"/>
<base href="/html/2409.02712v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S1" title="In A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction and Motivation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S2" title="In A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Literature Survey</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S3" title="In A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S3.SS1" title="In III Methodology ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic"> <span class="ltx_text ltx_ref_tag">2</span>Overview of our proposed approach</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S3.SS1.SSS1" title="In III-A 2Overview of our proposed approach ‣ III Methodology ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>1 </span><span class="ltx_text ltx_font_bold">Original Data</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S3.SS1.SSS2" title="In III-A 2Overview of our proposed approach ‣ III Methodology ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>2 </span><span class="ltx_text ltx_font_bold">Discrepancies found in the Dataset</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S3.SS1.SSS3" title="In III-A 2Overview of our proposed approach ‣ III Methodology ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>3 </span><span class="ltx_text ltx_font_bold">Data selection using multilingual IndicSBERT</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S3.SS1.SSS4" title="In III-A 2Overview of our proposed approach ‣ III Methodology ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>4 </span><span class="ltx_text ltx_font_bold">Model Training</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S3.SS1.SSS5" title="In III-A 2Overview of our proposed approach ‣ III Methodology ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span>5 </span><span class="ltx_text ltx_font_bold">Model Evaluation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S3.SS2" title="In III Methodology ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_bold">Dataset details</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S3.SS3" title="In III Methodology ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_bold">Models:</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S3.SS3.SSS1" title="In III-C Models: ‣ III Methodology ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>1 </span><span class="ltx_text ltx_font_bold">IndicBART Model</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S3.SS3.SSS2" title="In III-C Models: ‣ III Methodology ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>2 </span><span class="ltx_text ltx_font_bold">IndicSBERT Model</span></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S4" title="In A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Gold Testset Curation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S5" title="In A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Results and Discussion</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S5.SS1" title="In V Results and Discussion ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Evaluation Metrics</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S5.SS2" title="In V Results and Discussion ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Observations from Evaluation Metrics</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S5.SS3" title="In V Results and Discussion ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Observations from Sample Translations</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S6" title="In A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion and Future Scope</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S7" title="In A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Acknowledgement</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations
<br class="ltx_break"/>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Nidhi Kowtal<sup class="ltx_sup" id="id1.1.id1"> * </sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id2.2.id1">SCTR’s Pune Institute of Computer Technology
<br class="ltx_break"/></span>Pune, India 
<br class="ltx_break"/>kowtalnidhi@gmail.com
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tejas Deshpande<sup class="ltx_sup" id="id3.1.id1"> * </sup>
<span class="ltx_text" id="id4.2.id2"></span><span class="ltx_text" id="id5.3.id3"></span> <span class="ltx_ERROR undefined" id="id6.4.id4">{@IEEEauthorhalign}</span>
Raviraj Joshi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id7.5.id1">SCTR’s Pune Institute of Computer Technology
<br class="ltx_break"/></span>Pune, India 
<br class="ltx_break"/>tejasdeshpande1112@gmail.com
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id8.6.id1">Indian Institute of Technology Madras, India
<br class="ltx_break"/>L3Cube Labs, Pune</span>
<br class="ltx_break"/>Pune, India 
<br class="ltx_break"/>ravirajoshi@gmail.com
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id9.id1">Machine translation in low-resource language pairs faces significant challenges due to the scarcity of parallel corpora and linguistic resources. This study focuses on the case of English-Marathi language pairs, where existing datasets are notably noisy, impeding the performance of machine translation models. To mitigate the impact of data quality issues, we propose a data filtering approach based on cross-lingual sentence representations.</p>
<p class="ltx_p" id="id10.id2">Our methodology leverages a multilingual SBERT model to filter out problematic translations in the training data. Specifically, we employ an IndicSBERT similarity model to assess the semantic equivalence between original and translated sentences, allowing us to retain linguistically correct translations while discarding instances with substantial deviations. The results demonstrate a significant improvement in translation quality over the baseline post-filtering with IndicSBERT. This illustrates how cross-lingual sentence representations can reduce errors in machine translation scenarios with limited resources.
By integrating multilingual sentence BERT models into the translation pipeline, this research contributes to advancing machine translation techniques in low-resource environments. The proposed method not only addresses the challenges in English-Marathi language pairs but also provides a valuable framework for enhancing translation quality in other low-resource language translation tasks.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Low Resource Machine Translation, Cross-Lingual Sentence Representations, Indic Languages, Multilingual Natural Language Processing

</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_publicationid" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">publicationid: </span>pubid: 
<span class="ltx_p ltx_minipage ltx_align_middle" id="id1.1" style="width:433.6pt;"><span class="ltx_text" id="id1.1.1" style="font-size:80%;">* Authors contributed equally.</span></span></span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction and Motivation</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Machine Translation in low-resource language pairs encounters several challenges, with the most significant being the scarcity of parallel corpora and linguistic resources. To overcome these obstacles, datasets are often automatically generated, simplifying the training of translation models. However, datasets produced through these techniques often contain inherent noise, presenting a significant challenge to the creation of reliable and accurate translations. While translating from a high-level language to a low-level language, often it is found there are grammatical errors, and a few words are skipped while translating. This can contribute to noise in these datasets. 
<br class="ltx_break"/></p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="55" id="S1.F1.g1" src="extracted/5833309/introduction.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Process of Filtering the Data</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">A few of the methods of automated dataset generation include -</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1"><span class="ltx_text ltx_font_bold" id="S1.p3.1.1">Parallel Corpora Extraction:</span>
In parallel corpora extraction, texts in multiple languages are aligned using algorithms to match sentences for training translation models. These texts are usually sourced from translated books, articles, or official records. The scarcity of high-quality parallel corpora, particularly for specific language pairs, restricts the effectiveness of this approach and may hinder the size and diversity of the training dataset.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1"><span class="ltx_text ltx_font_bold" id="S1.p4.1.1">Back Transition:</span>
Utilizing an established translation model, back translation generates a synthetic parallel dataset by translating monolingual data back into the original language. The model’s generalization is influenced by potential artefacts and biases introduced from either the translation model or the original training set. Despite this, the method proves effective, especially in scenarios where parallel corpora in the target language pair are limited.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1"><span class="ltx_text ltx_font_bold" id="S1.p5.1.1">Data Augmentation:</span>
By employing automated methods like paraphrasing, data augmentation can be achieved to enhance the diversity of training data. This aids the model in learning from a broader spectrum of examples, thereby increasing its proficiency in handling linguistic variations. However, excessive and aggressive data augmentation may lead to the generation of nonsensical examples, causing confusion for the model rather than contributing to its generalization.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><span class="ltx_text ltx_font_bold" id="S1.p6.1.1">Web Scraping:</span>
Text content in multiple languages is extracted from online sources using web scraping. Through the use of automated data extraction from websites, we can gather a wide range of texts. To be more precise, we use web scraping to gather parallel translations that are accessible on multilingual websites. Through our website navigation, we can extract sentences that align in different languages, thereby building useful parallel translation corpora. These corpora, which comprise similar sentences in several languages, turn into an essential tool for machine translation model evaluation and training.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1"><span class="ltx_text ltx_font_bold" id="S1.p7.1.1">Pretrained Embeddings:</span>
The pre-trained embedding method makes use of embeddings produced by pre-existing models that have been trained on linguistic datasets. Semantic relationships between words and sentences are captured by these embeddings. These pre-trained embeddings improve machine translation models’ comprehension and representation of linguistic nuances. Through transfer learning, the embeddings help models understand patterns without requiring a lot of task-specific training. By using this technique, the models become more adept at managing a variety of language pairs and translating texts more accurately overall.

<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">To tackle these issues, we concentrate on the English-Marathi language pair. For machine translation systems, in particular, the noise captures linguistic complexities that present significant challenges. Because of the inherent challenges in generating high-quality translations from these datasets, reliable filtering mechanisms are essential. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">Cross-lingual similarity models have been used to construct the datasets under review to replicate the complexities of real-world translation scenarios. Still, we use a stronger model to improve the quality of the filtering process, realizing the need for a more resilient filtering mechanism. The drive to improve translation output accuracy and dependability is driving this switch to a more sophisticated model, which will ultimately help to advance machine translation capabilities in language environments with limited resources. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S1.p10">
<p class="ltx_p" id="S1.p10.1">Our study explores the difficulties that noisy datasets present for low-resource machine translation and highlights the significance of efficient noise reduction techniques. By eliminating problematic sentences, we enhance the quality of translation outputs through the use of the multilingual IndicSBERT model. 
<br class="ltx_break"/></p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Literature Survey</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Machine translation, which has a long history that dates back to the middle of the 20<sup class="ltx_sup" id="S2.p1.1.1">th</sup> century, has experienced tremendous evolution over time. The first machine translation attempts were rule-based, translating text between languages using linguistic structures and rules. Unfortunately, these systems had trouble processing the nuances of natural language, which made it difficult to translate words accurately in most cases. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#bib.bib26" title="">26</a>]</cite> by Vaswani et al. in 2017, is responsible for the development of neural machine translation (NMT) and the popularity of models such as the Transformer architecture. By presenting a self-attention mechanism that could successfully capture complex linguistic patterns and long-range dependencies, the Transformer architecture described in this paper revolutionized the field of machine translation. Using high-resource language pairs, the Transformer architecture’s effectiveness was shown, exhibiting notable gains in training efficiency and translation quality. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Machine translation has been impacted by the new era of natural language processing brought about by BERT (Bidirectional Encoder Representations from Transformers). Devlin et al. (2018) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#bib.bib12" title="">12</a>]</cite> pioneered BERT, a paradigm-shifting approach that pre-trains a deep bidirectional representation of language on an enormous volume of unlabeled text. Researchers have looked into integrating BERT-based models into machine translation architectures. In their investigation into the use of BERT in neural machine translation, Shavarani and Sarkar (2021) concentrated on gathering important linguistic information from BERT to improve the quality of the translated text. Their research highlighted BERT’s capacity to identify complex language patterns, enhancing the model’s comprehension of semantics and context. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Zhu et al. (2020) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#bib.bib27" title="">27</a>]</cite> expanded on the investigation of BERT integration into neural machine translation by looking into the advantages of incorporating pre-trained contextual embeddings from BERT. The goal of this study was to improve the representation of source and target language sentences in the translation model by capturing more comprehensive semantic information. The study demonstrated the utility of pre-trained language models in advancing machine translation capabilities by demonstrating the potential of BERT to improve translation accuracy and fluency. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">Low-resource language pairs like English to Khasi presented unique challenges that spurred the development of creative methods for achieving efficient machine translation. A Transformer-based method for low-resource neural machine translation from English to Khasi was presented by Thabah and Purkayastha in 2021. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#bib.bib24" title="">24</a>]</cite> The approach focuses on utilizing the Transformer architecture to improve translation capabilities for underrepresented languages. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">In 2021, Gowtham Ramesh <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#bib.bib22" title="">22</a>]</cite> and his associates unveiled the Samanantar project, which aims to solve a persistent problem in machine translation for Indian languages: the lack of parallel corpora. Machine translation models that are trained effectively require parallel corpora, which are collections of aligned texts in multiple languages. Offering the largest collection of parallel corpora for 11 Indian languages that is publicly available, Samanantar stands out as a trailblazing project. To ensure representation from a range of domains, the project sources diverse texts, which are then aligned to create parallel datasets. This comprehensive resource has grown to be essential for scholars and professionals involved in machine translation into the Indian language. Samanantar greatly aids in overcoming the challenges posed by data scarcity by offering a sizable and varied collection of parallel corpora. This makes it possible to develop and assess machine translation models with better language coverage and quality. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S2.p7">
<p class="ltx_p" id="S2.p7.1"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#bib.bib8" title="">8</a>]</cite> improved the method with semantically weighted back translation for morphologically rich and low-resource languages in the context of unsupervised machine translation. Their study sought to improve unsupervised neural machine translation efficiency while taking into account the unique difficulties presented by the linguistic peculiarities of Indian languages. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S2.p8">
<p class="ltx_p" id="S2.p8.1">One major development in the field was the paradigm shift towards statistical machine translation. Systems such as METEOR and BLEU metrics were developed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#bib.bib6" title="">6</a>]</cite> for automated assessment, offering numerical values to gauge the calibre of translations. SMT performed well in language pairs with abundant resources, but its low-resource performance—particularly for Indian languages—remained difficult to achieve because of the scarcity of parallel corpora. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S2.p9">
<p class="ltx_p" id="S2.p9.1">Presented in 2023 by AI4Bharat and partners <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#bib.bib4" title="">4</a>]</cite>, the IndicTrans2 project is an all-encompassing endeavour to fulfil the translation requirements of all 22 scheduled Indian languages. Recognizing India’s linguistic diversity, the project seeks to create machine translation models that are both accessible and of excellent quality. To make these models accessible and efficient for all scheduled Indian languages, IndicTrans2 expands on the achievements and difficulties seen in the machine translation field. To emphasize the value of linguistic diversity in the Indian context, the initiative involves the development of specialized models that are suited to the linguistic features of each language. IndicTrans2, with its emphasis on quality and accessibility, stands out as a noteworthy addition to the advancement of machine translation capabilities for all scheduled Indian languages, promoting inclusivity and linguistic representation in the digital sphere. 
<br class="ltx_break"/></p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Methodology</span>
</h2>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Discrepancies in the Dataset</figcaption><img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="S3.T1.g1" src="extracted/5833309/discrepancies.png" width="598"/>
</figure>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="266" id="S3.F2.g1" src="extracted/5833309/ict_paper_2.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Stages of the proposed approach</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2"> <a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S3.F2" title="Figure 2 ‣ III Methodology ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_tag">2</span></a>Overview of our proposed approach</span>
</h3>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS1.5.1.1">III-A</span>1 </span><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.6.2">Original Data</span>
</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">The initial dataset consists of 3.6 million sentences which are the ai4bharat’s BPCC Mined Dataset, representing the original noisy data. These sentences are noisy since they are mined and are not manually annotated.
This dataset includes diverse sentences with potential variations in grammar, context, and translation quality.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS2.5.1.1">III-A</span>2 </span><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.6.2">Discrepancies found in the Dataset</span>
</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">After evaluating the 3.6 million corpus, we found out that the dataset contained duplicates. So the sentence pairs which had the same translations were removed, and the language pairs which had different translations in any one language were retained, since the model would get attenuated to different contexts in a language.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S3.T1" title="TABLE I ‣ III Methodology ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_tag">I</span></a>, we present the discrepancies found in the dataset.
Following a manual assessment of 200 randomly chosen sample sentences from the dataset, various kinds of differences between the Marathi and English translations were found. These include situations where the Marathi translation did not fully convey subtleties from the English sentence. There have been instances where the translated text conveyed a different meaning due to cases of different meanings. Certain translations lacked specificity and were utterly ambiguous. Inconsistencies also appeared in details and missing contextual information, as well as sentences with similar contexts but distinct meanings.

<br class="ltx_break"/>The fact that nearly 50% of the sampled data showed these kinds of inconsistencies is notable and underscores the difficulties in preserving accurate translations across the dataset.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS3.5.1.1">III-A</span>3 </span><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS3.6.2">Data selection using multilingual IndicSBERT</span>
</h4>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.1">The IndicSBERT model  <a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S1.F1" title="Figure 1 ‣ I Introduction and Motivation ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_tag">1</span></a> is incorporated into the methodology as a sophisticated tool for measuring sentence similarity. The IndicSBERT model is used to get the similarity score between the English sentence and its corresponding Marathi sentence. The similarity score ranges between 0 to 1. We expelled the sentences whose similarity score was below 0.7. We trained the model only on the filtered sentences with a similarity score greater than 0.7. Thus, we procured high-quality dataset of 1.5 million corpus.
<br class="ltx_break"/></p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS4.5.1.1">III-A</span>4 </span><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS4.6.2">Model Training</span>
</h4>
<div class="ltx_para" id="S3.SS1.SSS4.p1">
<p class="ltx_p" id="S3.SS1.SSS4.p1.1">We trained our on top of IndicBART model, using 1 Million sentences after pre-processing the dataset. We found some shortcomings in the translations.
<br class="ltx_break"/>During the translation process, we noticed that some translations were grammatically incorrect and included English words. After reviewing the dataset, we found it to be noisy with several discrepancies. Since manually correcting such a large dataset wasn’t feasible, we decided to use the IndicSBERT model to address these issues.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS4.p2">
<p class="ltx_p" id="S3.SS1.SSS4.p2.1">The IndicSBERT model is incorporated into the methodology as a sophisticated tool for measuring sentence similarity. The IndicSBERT model was used to get the similarity score between the English sentence and its corresponding Marathi sentence. The similarity score ranges between 0 to 1. We expelled the sentences whose similarity score was below 0.7. We trained the model only on the filtered sentences with a similarity score greater than 0.7. The outputs of all three models—the original IndicBART, the fine-tuned IndicBART, and the filtered dataset using IndicSBERT are thus analyzed.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS4.p3">
<p class="ltx_p" id="S3.SS1.SSS4.p3.1">By filtering sentences based on the similarity score, we included 1 million sentences in the training dataset. After training the model on this dataset, the results showed significant improvement compared to the previous model. The translations were grammatically correct and no longer contained any English words.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS1.SSS5.5.1.1">III-A</span>5 </span><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS5.6.2">Model Evaluation</span>
</h4>
<div class="ltx_para" id="S3.SS1.SSS5.p1">
<p class="ltx_p" id="S3.SS1.SSS5.p1.1">The baseline model and our model are evaluated on the BLEU score, METEOR Score, CHRF Score, CHRF++ Score and IndicSBERT Score, and are mentioned in the table.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_bold" id="S3.SS2.6.2">Dataset details</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We chose ”AI4Bharat’s BPCC” Dataset for our model trainng.
BPCC is an extensive collection of parallel corpora created for eleven different Indic languages. The collection includes parallel texts written in Assamese, Bengali, Marathi, Gujarati, Oriya, Tamil, Telugu, Malayalam, Bengali, Bengal, and Hindi. It is a valuable tool for tackling the problem of sparse parallel corpora for low-resource languages.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_bold" id="S3.SS3.6.2">Models:</span>
</h3>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS1.5.1.1">III-C</span>1 </span><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.6.2">IndicBART Model</span>
</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">Specifically designed for Indian languages, the IndicBART model is a transformer-based architecture. It serves a wide linguistic spectrum, having been pre-trained on a multilingual corpus that includes 11 major Indian languages. Perfected for tasks like machine translation from English to Marathi, the model is excellent at interpreting the subtle differences between these two languages. Its proficiency in producing accurate and contextually relevant translations for this particular language is ensured by its specialization in Marathi. It is a useful tool for researchers and developers working on Indian language natural language processing tasks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS2.5.1.1">III-C</span>2 </span><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS2.6.2">IndicSBERT Model</span>
</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">Indic-Sentence-BERT (SBERT) model optimized for Indian language sentence similarity is created by L3Cube and has been trained on a wide range of Indian languages, such as Bengali, Tamil, Telugu, Kannada, Malayalam, Hindi, Marathi, and Gujarati. It is especially good at capturing semantic relationships between sentences by utilizing the SBERT architecture. This makes it useful for tasks like filtering out dissimilar sentences in a multilingual context. The model is skilled at recognizing the contextual subtleties that affect sentence similarity because it has been fine-tuned for the complexities of Indian languages. Sentences in Hindi, Marathi, or any other supported Indian language can be used to measure the semantic similarity between sentence pairs using the embeddings provided by this model. It provides a practical and approachable way for developers and researchers to include sentence similarity measurement in applications and research projects about Indian languages.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Some translations performed by previous model v/s the fine-tuned model</figcaption><img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="224" id="S3.T2.g1" src="extracted/5833309/translations.png" width="598"/>
</figure>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Evaluation Metrics</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S3.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.1.1" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T3.1.1.1.2" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.1.2.1">
<span class="ltx_p" id="S3.T3.1.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.2.1.1.1">English to Marathi</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T3.1.1.1.3" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.1.3.1">
<span class="ltx_p" id="S3.T3.1.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.3.1.1.1">English to Marathi</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.2.2.1" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.2.2.1.1">
<span class="ltx_p" id="S3.T3.1.2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.2.2.1.1.1.1">Metrics</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T3.1.2.2.2" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.2.2.2.1">
<span class="ltx_p" id="S3.T3.1.2.2.2.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.2.2.2.1.1.1">Mean value of metric before filtering the dataset</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T3.1.2.2.3" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.2.2.3.1">
<span class="ltx_p" id="S3.T3.1.2.2.3.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.2.2.3.1.1.1">Mean value of metric after filtering the dataset</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.3.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.3.3.1" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.3.3.1.1">
<span class="ltx_p" id="S3.T3.1.3.3.1.1.1">IndicSBERT Score</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T3.1.3.3.2" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.3.3.2.1">
<span class="ltx_p" id="S3.T3.1.3.3.2.1.1">75.3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T3.1.3.3.3" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.3.3.3.1">
<span class="ltx_p" id="S3.T3.1.3.3.3.1.1">78.2</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.4.4">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r" id="S3.T3.1.4.4.1" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T3.1.4.4.2" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T3.1.4.4.3" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.5.5">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.5.5.1" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.5.5.1.1">
<span class="ltx_p" id="S3.T3.1.5.5.1.1.1">BLEU Score</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T3.1.5.5.2" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.5.5.2.1">
<span class="ltx_p" id="S3.T3.1.5.5.2.1.1">28.9</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T3.1.5.5.3" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.5.5.3.1">
<span class="ltx_p" id="S3.T3.1.5.5.3.1.1">35.6</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.6.6">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r" id="S3.T3.1.6.6.1" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T3.1.6.6.2" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T3.1.6.6.3" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.7.7">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.7.7.1" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.7.7.1.1">
<span class="ltx_p" id="S3.T3.1.7.7.1.1.1">Meteor Score</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T3.1.7.7.2" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.7.7.2.1">
<span class="ltx_p" id="S3.T3.1.7.7.2.1.1">28.7</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T3.1.7.7.3" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.7.7.3.1">
<span class="ltx_p" id="S3.T3.1.7.7.3.1.1">34.1</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.8.8">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r" id="S3.T3.1.8.8.1" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T3.1.8.8.2" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T3.1.8.8.3" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.9.9">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.9.9.1" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.9.9.1.1">
<span class="ltx_p" id="S3.T3.1.9.9.1.1.1">CHRF Score</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T3.1.9.9.2" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.9.9.2.1">
<span class="ltx_p" id="S3.T3.1.9.9.2.1.1">37.044</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T3.1.9.9.3" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.9.9.3.1">
<span class="ltx_p" id="S3.T3.1.9.9.3.1.1">43.674</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.10.10">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r" id="S3.T3.1.10.10.1" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T3.1.10.10.2" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S3.T3.1.10.10.3" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.11.11">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.11.11.1" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.11.11.1.1">
<span class="ltx_p" id="S3.T3.1.11.11.1.1.1">CHRF++ Score</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T3.1.11.11.2" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.11.11.2.1">
<span class="ltx_p" id="S3.T3.1.11.11.2.1.1">37.860</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S3.T3.1.11.11.3" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.11.11.3.1">
<span class="ltx_p" id="S3.T3.1.11.11.3.1.1">44.223</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.12.12">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_l ltx_border_r" id="S3.T3.1.12.12.1" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r" id="S3.T3.1.12.12.2" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r" id="S3.T3.1.12.12.3" style="width:85.4pt;padding-top:2.5pt;padding-bottom:2.5pt;"></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Gold Testset Curation</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">It was observed that the dataset which was being used as a test dataset previously had some errors in the translation. So we considered the news dataset available on the MahaNLP corpus. Initially, we randomly considered 10000 sentences from the dataset. Then we manually went through these 10000 sentences and selected the top 1500 sentences which were translated accurately. We then calculated the metrics of our translation model using this manually curated test dataset. To check our model’s performance, we have considered the following metrics.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Results and Discussion</span>
</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Evaluation Metrics</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<ol class="ltx_enumerate" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">IndicSBERT Score -

<br class="ltx_break"/>The IndicSBERT model helps to check the similarity score between translated sentences with their respective Marathi sentence from the dataset. Our fine-tuned model are evaluated on a manually curated test dataset, using the IndicSBERT score as a metric. The mean IndicSBERT score of both the models is given in  <a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S3.T3" title="TABLE III ‣ III-C2 IndicSBERT Model ‣ III-C Models: ‣ III Methodology ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_tag">III</span></a> table.

<br class="ltx_break"/></p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">BLEU Score -

<br class="ltx_break"/>The BLEU Score is the benchmark for evaluating the quality of computer translation. It compares the translations produced by the ML model and the actual translations. It examines word clusters, such as one word, two words together, and so forth. It counts the number of words that are translated the same in both the model and the actual translations for each group. It then determines a score. A score of one indicates flawless translation. If it is zero, then no word was correctly predicted by the model. Higher BLEU Scores indicate that the translation produced by the model is more accurate than the original. Table  <a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S3.T3" title="TABLE III ‣ III-C2 IndicSBERT Model ‣ III-C Models: ‣ III Methodology ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_tag">III</span></a> indicates the BLEU score of both models.

<br class="ltx_break"/></p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1">Meteor Score -

<br class="ltx_break"/>The METEOR score is a metric used to measure the quality of machine-generated translations by comparing them to reference translations, which are considered the gold standard. It considers various aspects like precision, recall, and alignment to evaluate how well the generated translation captures the meaning and nuances of the original text. METEOR is particularly useful in machine translation evaluation because it goes beyond simple word matching and considers the overall fluency and correctness of the translated sentences. Higher METEOR scores indicate more accurate and contextually relevant translations, providing a quantitative measure for assessing the performance of machine translation models. Table  <a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S3.T3" title="TABLE III ‣ III-C2 IndicSBERT Model ‣ III-C Models: ‣ III Methodology ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_tag">III</span></a> indicates the Meteor score of both the models.

<br class="ltx_break"/></p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1">CHRF and CHRF++ Score -

<br class="ltx_break"/>These metrics function at the character level, as opposed to conventional metrics that concentrate on words. Essentially, they use matching character n-gram analysis to systematically check if translations produced by machines and humans are consistent.</p>
</div>
<div class="ltx_para" id="S5.I1.i4.p2">
<p class="ltx_p" id="S5.I1.i4.p2.1">A higher CHRF score in this case indicates improved performance by showing a good alignment between the machine translation and the human reference. An improved version of CHRF, called CHRF++, expands its analysis to include different character n-gram lengths, offering a more complex evaluation of the translation quality. 
<br class="ltx_break"/></p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Observations from Evaluation Metrics</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">The five metrics mentioned above namely IndicSBERT score, BLEU Score, Meteor Score, CHRF Score and CHRF++ Score were used for the evaluation. The scores  <a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S3.T3" title="TABLE III ‣ III-C2 IndicSBERT Model ‣ III-C Models: ‣ III Methodology ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_tag">III</span></a> of all the metrics have improved significantly after training the model on a filtered dataset. IndicSBERT Score increased by 2.%. BLEU Score improved by 6.7%. Meteor Score improved by 5.4%. CHRF Score was increased by 6.63% and CHRF++ Score was increased from 6.4%.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.5.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.6.2">Observations from Sample Translations</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Observations from the examples in Table  <a class="ltx_ref" href="https://arxiv.org/html/2409.02712v1#S3.T2" title="TABLE II ‣ III-C2 IndicSBERT Model ‣ III-C Models: ‣ III Methodology ‣ A Data Selection Approach for Enhancing Low Resource Machine Translation Using Cross-Lingual Sentence Representations"><span class="ltx_text ltx_ref_tag">II</span></a> :
<br class="ltx_break"/>1) In the 1st example, the translation of the fine-tuned model is grammatically correct.
<br class="ltx_break"/>2) In sentences 2 and 3, the translations of the previous model included some English words, but the fine-tuned model’s translation doesn’t contain any English words. 
<br class="ltx_break"/>3) In sentence number 4, the translation of the previous model contains a part of the English sentence as it is, whereas the fine-tuned model’s translation is accurate
<br class="ltx_break"/>4) In the 5th sentence, the translation of the previous model contains the same name thrice and is not accurate, whereas the translation of the finetuned model is accurate
<br class="ltx_break"/>5) In the 6th sentence, the translation of the previous model is incomplete, whereas the translation of the fine-tuned model is complete.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion and Future Scope</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we have highlighted a method to filter the noisy, low-resource dataset. It was concluded that training the model after filtering out the noisy sentences from the dataset improved the performance of the model.

<br class="ltx_break"/>Our long-term goal is to obtain additional high-quality data for languages with scarce resources, which has been a recurring problem in our work. We want to work with linguists and institutions to gather large, varied datasets so that our models can be trained more effectively.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Acknowledgement</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This work was done under the mentorship of Mr. Raviraj Joshi (Mentor, L3Cube Pune). We would like to express our gratitude towards him for his continuous support and encouragement.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Anvita Abbi.

</span>
<span class="ltx_bibblock">Languages of india and india and as a linguistic area.

</span>
<span class="ltx_bibblock">2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe.

</span>
<span class="ltx_bibblock">Semantic textual similarity, monolingual and cross-lingual evaluation.

</span>
<span class="ltx_bibblock">2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Roee Aharoni, Melvin Johnson, and Orhan Firat.

</span>
<span class="ltx_bibblock">Massively multilingual neural machine translation.

</span>
<span class="ltx_bibblock">2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
AI4Bharat, Jay Gala, Pranjal A. Chitale, Raghavan AK, Sumanth Doddapaneni, Varun Gumma, Aswanth Kumar, Janki Nawale, Anupama Sujatha, Ratish Puduppully, Vivek Raghavan, Pratyush Kumar, Mitesh M. Khapra, Raj Dabre, and Anoop Kunchukuttan.

</span>
<span class="ltx_bibblock">Indictrans2: Towards high-quality and accessible machine translation models for all 22 scheduled indian languages.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Paul Baker, Andrew Hardie, Tony McEnery, Hamish Cunningham, and Rob Gaizauskas.

</span>
<span class="ltx_bibblock">Emille: A 67-million word corpus of indic languages: Data collection, mark-up and harmonisation.

</span>
<span class="ltx_bibblock">2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Satanjeev Banerjee and Alon Lavie.

</span>
<span class="ltx_bibblock">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments.

</span>
<span class="ltx_bibblock">2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Nicola Bertoldi, Madalina Barbaiani, Marcello Federico, and Roldano Cattoni.

</span>
<span class="ltx_bibblock">Phrase-based statistical machine translation with pivot languages.

</span>
<span class="ltx_bibblock">2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Shweta Chauhan, Shefali Saxena, and Philemon Daniel.

</span>
<span class="ltx_bibblock">Improved unsupervised neural machine translation with semantically weighted back translation for morphologically rich and low resource languages.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Marjan Ghazvininejad, Hila Gonen, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Dictionary-based phrase-level prompting of large language models for machine translation.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Amruta Godase and Sharvari Govilkar.

</span>
<span class="ltx_bibblock">Machine translation development for indian languages and its approaches.

</span>
<span class="ltx_bibblock">2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan.

</span>
<span class="ltx_bibblock">The flores-101 evaluation benchmark for low-resource and multilingual machine translation.

</span>
<span class="ltx_bibblock">2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O.K. Li.

</span>
<span class="ltx_bibblock">Universal neural machine translation for extremely low resource languages.

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Tahmid Hasan, Abhik Bhattacharjee, Kazi Samin, Masum Hasan, Madhusudan Basak, M. Sohel Rahman, and Rifat Shahriyar.

</span>
<span class="ltx_bibblock">Not low-resource anymore: Aligner ensembling, batch filtering, and new datasets for bengali-english machine translation.

</span>
<span class="ltx_bibblock">2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Kenji Imamura and Eiichiro Sumita.

</span>
<span class="ltx_bibblock">Recycling a pre-trained bert encoder for neural machine translation.

</span>
<span class="ltx_bibblock">2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Piyush Jha, Rashi Kumar, and Vineet Sahula.

</span>
<span class="ltx_bibblock">Filtering and extended vocabulary based translation for low-resource language pair of sanskrit-hindi.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Nadeem Jadoon Khan, Waqas Anwar, and Nadir Durrani.

</span>
<span class="ltx_bibblock">Machine translation approaches and survey for indian languages.

</span>
<span class="ltx_bibblock">2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Yunsu Kim, Petre Petrov, Pavel Petrushkov, Shahram Khadivi, and Hermann Ney.

</span>
<span class="ltx_bibblock">Pivot-based transfer learning for neural machine translation between non-english languages.

</span>
<span class="ltx_bibblock">2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
En-Shiun Annie Lee, Sarubi Thillainathan, Shravan Nayak, Surangika Ranathunga, David Ifeoluwa Adelani, Ruisi Su, and Arya D. McCarthy.

</span>
<span class="ltx_bibblock">Pre-trained multilingual sequence-to-sequence models: A hope for low-resource language translation?

</span>
<span class="ltx_bibblock">2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zh.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Chanjun Park, Yeongwook Yang, Kinam Park, and Heuiseok Lim.

</span>
<span class="ltx_bibblock">Decoding strategies for improving low-resource machine translation.

</span>
<span class="ltx_bibblock">2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
B. Premjith, M. Anand Kumar, and K.P. Soman.

</span>
<span class="ltx_bibblock">Neural machine translation system for english to indian language translation using mtil parallel corpus.

</span>
<span class="ltx_bibblock">2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Gowtham Ramesh, Sumanth Doddapaneni, Aravinth Bheemaraj, Mayank Jobanputra, Raghavan AK, Ajitesh Sharma, Sujit Sahoo, Harshita Diddee, Mahalakshmi J, Divyanshu Kakwani, Navneet Kumar, Aswin Pradeep, Srihari Nagaraj, Kumar Deepak, Vivek Raghavan, Anoop Kunchukuttan, Pratyush Kumar, and Mitesh Shantadevi Khapra.

</span>
<span class="ltx_bibblock">Samanantar: The largest publicly available parallel corpora collection for 11 indic languages.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Hassan S. Shavarani and Anoop Sarkar.

</span>
<span class="ltx_bibblock">Better neural machine translation by extracting linguistic information from bert.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
N. Donald Jefferson Thabah and Bipul Syam Purkayastha.

</span>
<span class="ltx_bibblock">Low resource neural machine translation from english to khasi: A transformer-based approach.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Spoorthi Thammaiah, Vinaya Manchaiah, Vijayalakshmi Easwar, and Rajalakshmi Krishna.

</span>
<span class="ltx_bibblock">Translation and adaptation of five english language self-report health measures to south indian kannada language.

</span>
<span class="ltx_bibblock">2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, and Tie-Yan Liu.

</span>
<span class="ltx_bibblock">Incorporating bert into neural machine translation.

</span>
<span class="ltx_bibblock">2020.

</span>
</li>
</ul>
</section>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1"></p>
</div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Sep  4 13:46:51 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
