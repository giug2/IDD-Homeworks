<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)</title>
<!--Generated on Thu Oct  3 10:15:37 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
speaker verification,  weak-supervision,  pre-trained models
" lang="en" name="keywords"/>
<base href="/html/2410.02364v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#S1" title="In State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#S2" title="In State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Proposed method</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#S2.SS1" title="In II Proposed method ‚Ä£ State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Weakly-supervised approach for speaker verification</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#S2.SS2" title="In II Proposed method ‚Ä£ State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Exploiting pre-trained models with MHFA</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#S2.SS3" title="In II Proposed method ‚Ä£ State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Exploring speaker diarization</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#S2.SS4" title="In II Proposed method ‚Ä£ State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">Utilization of unknown speakers‚Äô segments</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#S3" title="In State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Experimental Setup</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#S4" title="In State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiments</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#S4.SS1" title="In IV Experiments ‚Ä£ State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">First stage</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#S4.SS2" title="In IV Experiments ‚Ä£ State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Evaluation of method validity</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#S4.SS3" title="In IV Experiments ‚Ä£ State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Second Stage</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#S5" title="In State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusions</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data 
<br class="ltx_break"/><span class="ltx_note ltx_role_thanks" id="id3.id1"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">thanks: </span>This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the
Spanish Ministerio de Ciencia e Innovacion,
Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sara Barahona1, Ladislav Mo≈°ner2, Themos Stafylakis3, Old≈ôich Plchot2, Junyi Peng2, 
<br class="ltx_break"/>Luk√°≈° Burget2, and Jan ƒåernock√Ω2
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">1AUDIAS Research Group, Universidad Aut√≥noma de Madrid, Madrid, Spain
</span>
<span class="ltx_contact ltx_role_affiliation">2Brno University of Technology, Faculty of Information Technology, Speech@FIT, Czechia
</span>
<span class="ltx_contact ltx_role_affiliation">3Athens University of Economics and Business <math alttext="|" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mo fence="false" id="id1.1.m1.1.1" stretchy="false" xref="id1.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">|</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">|</annotation></semantics></math> Omilia <math alttext="|" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mo fence="false" id="id2.2.m2.1.1" stretchy="false" xref="id2.2.m2.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><ci id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">|</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">|</annotation></semantics></math> Archimedes/Athena R.C, Greece
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">In this paper, we refine and validate our method for training speaker embedding extractors using weak annotations.
More specifically, we use only the audio stream of the source VoxCeleb videos and the names of the celebrities without knowing the time intervals in which they appear in the recording.
We experiment with hyperparameters and embedding extractors based on ResNet and WavLM. We show that the method achieves state-of-the-art results in speaker verification, comparable with training the extractors in a standard supervised way on the VoxCeleb dataset. We also extend it by considering segments belonging to unknown speakers appearing alongside the celebrities, which are typically being discarded. Overall, our approach can be used for directly training state-of-the-art embedding extractors or as an alternative to the VoxCeleb-like pipeline for dataset creation without needing image modality.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
speaker verification, weak-supervision, pre-trained models

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Extracting speaker embeddings from audio has multiple uses in various speech tasks such as speaker verification, diarization, separation, speech enhancement, voice conversion, multi-speaker automatic speech recognition (ASR), etc. Speaker embedding is a low-dimensional vector that characterizes a person‚Äôs voice attributes and is extracted from an audio sample typically via large neural networks such as ResNet or TDNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib3" title="">3</a>]</cite>. More recently, with the advent of self-supervised learning (SSL), the research community also employs transformer-based models such as WavLM, HuBERT, or Wav2Vec2<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib6" title="">6</a>]</cite> that are either simply used as feature extractors for the mentioned CNN models or combined with a lighter speaker classification backend<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib7" title="">7</a>]</cite> which allows fast convergence. All of these architectures are typically trained in a supervised way, with a training set consisting of several thousands of speakers and multiple recordings containing only the speech of a speaker corresponding to the label.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Creating human-labeled datasets beyond the purpose of the evaluation is prohibitively costly, and the research field has practically moved towards harvesting the data found online while leveraging multiple speech and image-processing technologies to segment it into training samples. One such dataset is VoxCeleb <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib9" title="">9</a>]</cite>, which comprises over 2,000 hours of speech recordings from approximately 7,000 celebrities. Interviews with them are sourced by querying YouTube, selecting audio that overlaps with the celebrity speaking, using SyncNet for audio-visual synchronization and a ResNet50 for face recognition. By design, this pipeline is not only dependent on processing video, but it also discards a lot of parts when the corresponding face does not appear clearly in the image.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address this limitation, our previous research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib10" title="">10</a>]</cite> explored training a speaker embedding extractor employing source audio from the VoxCeleb dataset, relying only on recording-level labels while considering the time boundaries where celebrities appear as unknown. Our proposed two-stage method begins by training a speaker embedding extractor employing segments from clusters of segments obtained with a basic diarization algorithm<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib11" title="">11</a>]</cite>.
The trained embedding extractor is then employed to identify which diarized segments belong to the target speaker. These selected segments are then used to train the second stage of the model in a fully supervised way.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In comparison to our previous work, where we concentrated on formally introducing the method and providing initial results, we now present a comprehensive ablation study of our method on speaker verification task with a newly re-implemented pipeline that is based on PyTorch and the WeSpeaker toolkit<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib13" title="">13</a>]</cite>. Motivated by current research with SSL models and the fact that they align well with the philosophy of loosening the labeling requirements of our method, we have extended the toolkit to include our approach based on WavLM employing Multi-Head Factorized Attention (MHFA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib7" title="">7</a>]</cite> as back-end. Naturally, we also experiment with embedding extractors based on ResNet, as they still constitute the state-of-the-art approach for speaker embedding extraction. We also propose a novel extension to our method that takes into account a portion of segments from unknown speakers, which increases the utilization of the training data. This modification brings further marginal improvements, but more importantly, it shows an interesting and relatively unexplored research direction. Lastly, we validate our method by comparing the performance when employing a state-of-the-art diarization model from the Pyannote toolkit.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Proposed method</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Weakly-supervised approach for speaker verification</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Our method for exploiting weakly-labeled data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib10" title="">10</a>]</cite> involves a two-stage process.
In the first stage, a diarization algorithm is employed to cluster segments from potential sources, assuming that at least one of them belongs to the target celebrity. The resulting clusters are characterized by high speaker purity but low speaker coverage, which frequently causes speakers to be assigned to multiple clusters. Following a multi-instance learning strategy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib14" title="">14</a>]</cite>, our embedding extractor is trained employing a <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.1">bag</span> of segments from each recording.
When creating each mini-batch, we ensure that at least one segment from each cluster is included. Therefore, its size will depend on the number of clusters within each recording. During the feed-forward step, each segment is processed independently to extract individual embeddings. We then compute the similarity between these normalized embeddings and speaker-specific weights (speaker prototypes) using the dot product.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.2">To obtain recording-level similarities, we aggregate segment-level similarities. We explored the use of max-pooling, which focuses on the segment with the highest similarity while excluding the others. Considering that when employing max-pooling, gradients are propagated only through the highest-similarity segment, it might be hard to train in the warm-up phase. Therefore, we also considered its soft version, log-sum-exp (LSE) function, which is smoothed by a temperature parameter <math alttext="\tau" class="ltx_Math" display="inline" id="S2.SS1.p2.1.m1.1"><semantics id="S2.SS1.p2.1.m1.1a"><mi id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">œÑ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">ùúè</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.1.m1.1d">italic_œÑ</annotation></semantics></math>. These recording-level similarities range between <math alttext="[-1,1]" class="ltx_Math" display="inline" id="S2.SS1.p2.2.m2.2"><semantics id="S2.SS1.p2.2.m2.2a"><mrow id="S2.SS1.p2.2.m2.2.2.1" xref="S2.SS1.p2.2.m2.2.2.2.cmml"><mo id="S2.SS1.p2.2.m2.2.2.1.2" stretchy="false" xref="S2.SS1.p2.2.m2.2.2.2.cmml">[</mo><mrow id="S2.SS1.p2.2.m2.2.2.1.1" xref="S2.SS1.p2.2.m2.2.2.1.1.cmml"><mo id="S2.SS1.p2.2.m2.2.2.1.1a" xref="S2.SS1.p2.2.m2.2.2.1.1.cmml">‚àí</mo><mn id="S2.SS1.p2.2.m2.2.2.1.1.2" xref="S2.SS1.p2.2.m2.2.2.1.1.2.cmml">1</mn></mrow><mo id="S2.SS1.p2.2.m2.2.2.1.3" xref="S2.SS1.p2.2.m2.2.2.2.cmml">,</mo><mn id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">1</mn><mo id="S2.SS1.p2.2.m2.2.2.1.4" stretchy="false" xref="S2.SS1.p2.2.m2.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.2b"><interval closure="closed" id="S2.SS1.p2.2.m2.2.2.2.cmml" xref="S2.SS1.p2.2.m2.2.2.1"><apply id="S2.SS1.p2.2.m2.2.2.1.1.cmml" xref="S2.SS1.p2.2.m2.2.2.1.1"><minus id="S2.SS1.p2.2.m2.2.2.1.1.1.cmml" xref="S2.SS1.p2.2.m2.2.2.1.1"></minus><cn id="S2.SS1.p2.2.m2.2.2.1.1.2.cmml" type="integer" xref="S2.SS1.p2.2.m2.2.2.1.1.2">1</cn></apply><cn id="S2.SS1.p2.2.m2.1.1.cmml" type="integer" xref="S2.SS1.p2.2.m2.1.1">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.2c">[-1,1]</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.2.m2.2d">[ - 1 , 1 ]</annotation></semantics></math>, making
the Additive Angular Margin (AAM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib15" title="">15</a>]</cite> loss applicable.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">In the second stage, the segments used for fully-supervised training are selected by the first-stage embedding extractor. We feed-forward all segments obtained from the diarization algorithm, where a segment is associated with the target celebrity if, and only if, the network classifies it as such.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Exploiting pre-trained models with MHFA</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Training a speaker verification system using our weakly-supervised approach has higher computational requirements compared to fully-supervised methods. This is attributed not only to the two-stage training process, but also to the substantially larger amount of data employed, especially in the first phase, where embedding extractors are trained. During our experiments with pre-trained SSL models and the MHFA backend<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib7" title="">7</a>]</cite>, we saw that we need to do substantially fewer training epochs than training ResNet-based systems from scratch while achieving state-of-the-art results. This property is also held here, especially in the second stage of our method.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.4.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.5.2">Exploring speaker diarization</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">To reduce the reliance on annotations, our previous work¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib10" title="">10</a>]</cite> used a very baseline diarization system with no required training, implemented with the SIDEKIT toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib16" title="">16</a>]</cite>. As a first stage, this algorithm employed a speaker change detector followed by a BIC-based agglomerative hierarchical clustering with Gaussians on MFCC features for modeling features, and followed by Viterbi-based boundary refinement using Gaussian Mixture Models. To explore whether the quality of segments can determine the performance of our weakly-supervised pipeline, we explore employing a state-of-the-art pre-trained diarization model from the Pyannote toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib18" title="">18</a>]</cite>. This model is composed of three stages: first, a neural-based local speaker diarization is performed on short sliding windows; second, local speaker embeddings are extracted for each window; and finally, these embeddings are clustered to assign each local speaker to a global cluster.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.4.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.5.2">Utilization of unknown speakers‚Äô segments</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">Aiming to utilize segments containing speech of speakers different from the target ones, we devise a new training method. It enables training with an extra unknown class and, as a result, allows us to increase the size of the training corpus.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.5">Consider that <math alttext="T" class="ltx_Math" display="inline" id="S2.SS4.p2.1.m1.1"><semantics id="S2.SS4.p2.1.m1.1a"><mi id="S2.SS4.p2.1.m1.1.1" xref="S2.SS4.p2.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.1.m1.1b"><ci id="S2.SS4.p2.1.m1.1.1.cmml" xref="S2.SS4.p2.1.m1.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.1.m1.1d">italic_T</annotation></semantics></math> is a set of numerical labels of known speakers and <math alttext="\mathcal{F}_{\mathrm{id}}" class="ltx_Math" display="inline" id="S2.SS4.p2.2.m2.1"><semantics id="S2.SS4.p2.2.m2.1a"><msub id="S2.SS4.p2.2.m2.1.1" xref="S2.SS4.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS4.p2.2.m2.1.1.2" xref="S2.SS4.p2.2.m2.1.1.2.cmml">‚Ñ±</mi><mi id="S2.SS4.p2.2.m2.1.1.3" xref="S2.SS4.p2.2.m2.1.1.3.cmml">id</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.2.m2.1b"><apply id="S2.SS4.p2.2.m2.1.1.cmml" xref="S2.SS4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS4.p2.2.m2.1.1.1.cmml" xref="S2.SS4.p2.2.m2.1.1">subscript</csymbol><ci id="S2.SS4.p2.2.m2.1.1.2.cmml" xref="S2.SS4.p2.2.m2.1.1.2">‚Ñ±</ci><ci id="S2.SS4.p2.2.m2.1.1.3.cmml" xref="S2.SS4.p2.2.m2.1.1.3">id</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.2.m2.1c">\mathcal{F}_{\mathrm{id}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.2.m2.1d">caligraphic_F start_POSTSUBSCRIPT roman_id end_POSTSUBSCRIPT</annotation></semantics></math> maps the example to its corresponding label. For each mini-batch, a second-stage model provides logits <math alttext="\mathbf{L}\in\mathbb{R}^{|B|\times|T|}" class="ltx_Math" display="inline" id="S2.SS4.p2.3.m3.2"><semantics id="S2.SS4.p2.3.m3.2a"><mrow id="S2.SS4.p2.3.m3.2.3" xref="S2.SS4.p2.3.m3.2.3.cmml"><mi id="S2.SS4.p2.3.m3.2.3.2" xref="S2.SS4.p2.3.m3.2.3.2.cmml">ùêã</mi><mo id="S2.SS4.p2.3.m3.2.3.1" xref="S2.SS4.p2.3.m3.2.3.1.cmml">‚àà</mo><msup id="S2.SS4.p2.3.m3.2.3.3" xref="S2.SS4.p2.3.m3.2.3.3.cmml"><mi id="S2.SS4.p2.3.m3.2.3.3.2" xref="S2.SS4.p2.3.m3.2.3.3.2.cmml">‚Ñù</mi><mrow id="S2.SS4.p2.3.m3.2.2.2" xref="S2.SS4.p2.3.m3.2.2.2.cmml"><mrow id="S2.SS4.p2.3.m3.2.2.2.4.2" xref="S2.SS4.p2.3.m3.2.2.2.4.1.cmml"><mo id="S2.SS4.p2.3.m3.2.2.2.4.2.1" stretchy="false" xref="S2.SS4.p2.3.m3.2.2.2.4.1.1.cmml">|</mo><mi id="S2.SS4.p2.3.m3.1.1.1.1" xref="S2.SS4.p2.3.m3.1.1.1.1.cmml">B</mi><mo id="S2.SS4.p2.3.m3.2.2.2.4.2.2" rspace="0.055em" stretchy="false" xref="S2.SS4.p2.3.m3.2.2.2.4.1.1.cmml">|</mo></mrow><mo id="S2.SS4.p2.3.m3.2.2.2.3" rspace="0.222em" xref="S2.SS4.p2.3.m3.2.2.2.3.cmml">√ó</mo><mrow id="S2.SS4.p2.3.m3.2.2.2.5.2" xref="S2.SS4.p2.3.m3.2.2.2.5.1.cmml"><mo id="S2.SS4.p2.3.m3.2.2.2.5.2.1" stretchy="false" xref="S2.SS4.p2.3.m3.2.2.2.5.1.1.cmml">|</mo><mi id="S2.SS4.p2.3.m3.2.2.2.2" xref="S2.SS4.p2.3.m3.2.2.2.2.cmml">T</mi><mo id="S2.SS4.p2.3.m3.2.2.2.5.2.2" stretchy="false" xref="S2.SS4.p2.3.m3.2.2.2.5.1.1.cmml">|</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.3.m3.2b"><apply id="S2.SS4.p2.3.m3.2.3.cmml" xref="S2.SS4.p2.3.m3.2.3"><in id="S2.SS4.p2.3.m3.2.3.1.cmml" xref="S2.SS4.p2.3.m3.2.3.1"></in><ci id="S2.SS4.p2.3.m3.2.3.2.cmml" xref="S2.SS4.p2.3.m3.2.3.2">ùêã</ci><apply id="S2.SS4.p2.3.m3.2.3.3.cmml" xref="S2.SS4.p2.3.m3.2.3.3"><csymbol cd="ambiguous" id="S2.SS4.p2.3.m3.2.3.3.1.cmml" xref="S2.SS4.p2.3.m3.2.3.3">superscript</csymbol><ci id="S2.SS4.p2.3.m3.2.3.3.2.cmml" xref="S2.SS4.p2.3.m3.2.3.3.2">‚Ñù</ci><apply id="S2.SS4.p2.3.m3.2.2.2.cmml" xref="S2.SS4.p2.3.m3.2.2.2"><times id="S2.SS4.p2.3.m3.2.2.2.3.cmml" xref="S2.SS4.p2.3.m3.2.2.2.3"></times><apply id="S2.SS4.p2.3.m3.2.2.2.4.1.cmml" xref="S2.SS4.p2.3.m3.2.2.2.4.2"><abs id="S2.SS4.p2.3.m3.2.2.2.4.1.1.cmml" xref="S2.SS4.p2.3.m3.2.2.2.4.2.1"></abs><ci id="S2.SS4.p2.3.m3.1.1.1.1.cmml" xref="S2.SS4.p2.3.m3.1.1.1.1">ùêµ</ci></apply><apply id="S2.SS4.p2.3.m3.2.2.2.5.1.cmml" xref="S2.SS4.p2.3.m3.2.2.2.5.2"><abs id="S2.SS4.p2.3.m3.2.2.2.5.1.1.cmml" xref="S2.SS4.p2.3.m3.2.2.2.5.2.1"></abs><ci id="S2.SS4.p2.3.m3.2.2.2.2.cmml" xref="S2.SS4.p2.3.m3.2.2.2.2">ùëá</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.3.m3.2c">\mathbf{L}\in\mathbb{R}^{|B|\times|T|}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.3.m3.2d">bold_L ‚àà blackboard_R start_POSTSUPERSCRIPT | italic_B | √ó | italic_T | end_POSTSUPERSCRIPT</annotation></semantics></math>, with B representing a set of mini-batch examples. Our proposed method extends the logits by adding one class without introducing a new prototype leading to logits <math alttext="\mathbf{L}^{\prime}\in\mathbb{R}^{|B|\times(|T|+1)}" class="ltx_Math" display="inline" id="S2.SS4.p2.4.m4.3"><semantics id="S2.SS4.p2.4.m4.3a"><mrow id="S2.SS4.p2.4.m4.3.4" xref="S2.SS4.p2.4.m4.3.4.cmml"><msup id="S2.SS4.p2.4.m4.3.4.2" xref="S2.SS4.p2.4.m4.3.4.2.cmml"><mi id="S2.SS4.p2.4.m4.3.4.2.2" xref="S2.SS4.p2.4.m4.3.4.2.2.cmml">ùêã</mi><mo id="S2.SS4.p2.4.m4.3.4.2.3" xref="S2.SS4.p2.4.m4.3.4.2.3.cmml">‚Ä≤</mo></msup><mo id="S2.SS4.p2.4.m4.3.4.1" xref="S2.SS4.p2.4.m4.3.4.1.cmml">‚àà</mo><msup id="S2.SS4.p2.4.m4.3.4.3" xref="S2.SS4.p2.4.m4.3.4.3.cmml"><mi id="S2.SS4.p2.4.m4.3.4.3.2" xref="S2.SS4.p2.4.m4.3.4.3.2.cmml">‚Ñù</mi><mrow id="S2.SS4.p2.4.m4.3.3.3" xref="S2.SS4.p2.4.m4.3.3.3.cmml"><mrow id="S2.SS4.p2.4.m4.3.3.3.5.2" xref="S2.SS4.p2.4.m4.3.3.3.5.1.cmml"><mo id="S2.SS4.p2.4.m4.3.3.3.5.2.1" stretchy="false" xref="S2.SS4.p2.4.m4.3.3.3.5.1.1.cmml">|</mo><mi id="S2.SS4.p2.4.m4.1.1.1.1" xref="S2.SS4.p2.4.m4.1.1.1.1.cmml">B</mi><mo id="S2.SS4.p2.4.m4.3.3.3.5.2.2" rspace="0.055em" stretchy="false" xref="S2.SS4.p2.4.m4.3.3.3.5.1.1.cmml">|</mo></mrow><mo id="S2.SS4.p2.4.m4.3.3.3.4" rspace="0.222em" xref="S2.SS4.p2.4.m4.3.3.3.4.cmml">√ó</mo><mrow id="S2.SS4.p2.4.m4.3.3.3.3.1" xref="S2.SS4.p2.4.m4.3.3.3.3.1.1.cmml"><mo id="S2.SS4.p2.4.m4.3.3.3.3.1.2" stretchy="false" xref="S2.SS4.p2.4.m4.3.3.3.3.1.1.cmml">(</mo><mrow id="S2.SS4.p2.4.m4.3.3.3.3.1.1" xref="S2.SS4.p2.4.m4.3.3.3.3.1.1.cmml"><mrow id="S2.SS4.p2.4.m4.3.3.3.3.1.1.2.2" xref="S2.SS4.p2.4.m4.3.3.3.3.1.1.2.1.cmml"><mo id="S2.SS4.p2.4.m4.3.3.3.3.1.1.2.2.1" stretchy="false" xref="S2.SS4.p2.4.m4.3.3.3.3.1.1.2.1.1.cmml">|</mo><mi id="S2.SS4.p2.4.m4.2.2.2.2" xref="S2.SS4.p2.4.m4.2.2.2.2.cmml">T</mi><mo id="S2.SS4.p2.4.m4.3.3.3.3.1.1.2.2.2" stretchy="false" xref="S2.SS4.p2.4.m4.3.3.3.3.1.1.2.1.1.cmml">|</mo></mrow><mo id="S2.SS4.p2.4.m4.3.3.3.3.1.1.1" xref="S2.SS4.p2.4.m4.3.3.3.3.1.1.1.cmml">+</mo><mn id="S2.SS4.p2.4.m4.3.3.3.3.1.1.3" xref="S2.SS4.p2.4.m4.3.3.3.3.1.1.3.cmml">1</mn></mrow><mo id="S2.SS4.p2.4.m4.3.3.3.3.1.3" stretchy="false" xref="S2.SS4.p2.4.m4.3.3.3.3.1.1.cmml">)</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.4.m4.3b"><apply id="S2.SS4.p2.4.m4.3.4.cmml" xref="S2.SS4.p2.4.m4.3.4"><in id="S2.SS4.p2.4.m4.3.4.1.cmml" xref="S2.SS4.p2.4.m4.3.4.1"></in><apply id="S2.SS4.p2.4.m4.3.4.2.cmml" xref="S2.SS4.p2.4.m4.3.4.2"><csymbol cd="ambiguous" id="S2.SS4.p2.4.m4.3.4.2.1.cmml" xref="S2.SS4.p2.4.m4.3.4.2">superscript</csymbol><ci id="S2.SS4.p2.4.m4.3.4.2.2.cmml" xref="S2.SS4.p2.4.m4.3.4.2.2">ùêã</ci><ci id="S2.SS4.p2.4.m4.3.4.2.3.cmml" xref="S2.SS4.p2.4.m4.3.4.2.3">‚Ä≤</ci></apply><apply id="S2.SS4.p2.4.m4.3.4.3.cmml" xref="S2.SS4.p2.4.m4.3.4.3"><csymbol cd="ambiguous" id="S2.SS4.p2.4.m4.3.4.3.1.cmml" xref="S2.SS4.p2.4.m4.3.4.3">superscript</csymbol><ci id="S2.SS4.p2.4.m4.3.4.3.2.cmml" xref="S2.SS4.p2.4.m4.3.4.3.2">‚Ñù</ci><apply id="S2.SS4.p2.4.m4.3.3.3.cmml" xref="S2.SS4.p2.4.m4.3.3.3"><times id="S2.SS4.p2.4.m4.3.3.3.4.cmml" xref="S2.SS4.p2.4.m4.3.3.3.4"></times><apply id="S2.SS4.p2.4.m4.3.3.3.5.1.cmml" xref="S2.SS4.p2.4.m4.3.3.3.5.2"><abs id="S2.SS4.p2.4.m4.3.3.3.5.1.1.cmml" xref="S2.SS4.p2.4.m4.3.3.3.5.2.1"></abs><ci id="S2.SS4.p2.4.m4.1.1.1.1.cmml" xref="S2.SS4.p2.4.m4.1.1.1.1">ùêµ</ci></apply><apply id="S2.SS4.p2.4.m4.3.3.3.3.1.1.cmml" xref="S2.SS4.p2.4.m4.3.3.3.3.1"><plus id="S2.SS4.p2.4.m4.3.3.3.3.1.1.1.cmml" xref="S2.SS4.p2.4.m4.3.3.3.3.1.1.1"></plus><apply id="S2.SS4.p2.4.m4.3.3.3.3.1.1.2.1.cmml" xref="S2.SS4.p2.4.m4.3.3.3.3.1.1.2.2"><abs id="S2.SS4.p2.4.m4.3.3.3.3.1.1.2.1.1.cmml" xref="S2.SS4.p2.4.m4.3.3.3.3.1.1.2.2.1"></abs><ci id="S2.SS4.p2.4.m4.2.2.2.2.cmml" xref="S2.SS4.p2.4.m4.2.2.2.2">ùëá</ci></apply><cn id="S2.SS4.p2.4.m4.3.3.3.3.1.1.3.cmml" type="integer" xref="S2.SS4.p2.4.m4.3.3.3.3.1.1.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.4.m4.3c">\mathbf{L}^{\prime}\in\mathbb{R}^{|B|\times(|T|+1)}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.4.m4.3d">bold_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT | italic_B | √ó ( | italic_T | + 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math>. For each example <math alttext="b\in B" class="ltx_Math" display="inline" id="S2.SS4.p2.5.m5.1"><semantics id="S2.SS4.p2.5.m5.1a"><mrow id="S2.SS4.p2.5.m5.1.1" xref="S2.SS4.p2.5.m5.1.1.cmml"><mi id="S2.SS4.p2.5.m5.1.1.2" xref="S2.SS4.p2.5.m5.1.1.2.cmml">b</mi><mo id="S2.SS4.p2.5.m5.1.1.1" xref="S2.SS4.p2.5.m5.1.1.1.cmml">‚àà</mo><mi id="S2.SS4.p2.5.m5.1.1.3" xref="S2.SS4.p2.5.m5.1.1.3.cmml">B</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.5.m5.1b"><apply id="S2.SS4.p2.5.m5.1.1.cmml" xref="S2.SS4.p2.5.m5.1.1"><in id="S2.SS4.p2.5.m5.1.1.1.cmml" xref="S2.SS4.p2.5.m5.1.1.1"></in><ci id="S2.SS4.p2.5.m5.1.1.2.cmml" xref="S2.SS4.p2.5.m5.1.1.2">ùëè</ci><ci id="S2.SS4.p2.5.m5.1.1.3.cmml" xref="S2.SS4.p2.5.m5.1.1.3">ùêµ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.5.m5.1c">b\in B</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.5.m5.1d">italic_b ‚àà italic_B</annotation></semantics></math> in a mini-batch, we set the logit of the additional class as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{L}^{\prime}_{b,|T|+1}=\begin{cases}0&amp;\mathrm{if}~{}~{}\mathcal{F}_{%
\mathrm{id}}(b)\in T\\
\frac{1}{|B_{T}|}\sum_{i\in B_{T}}\mathbf{L}_{i,\mathcal{F}_{\mathrm{id}}(i)}&amp;%
\mathrm{otherwise}\\
\end{cases}," class="ltx_Math" display="block" id="S2.E1.m1.8"><semantics id="S2.E1.m1.8a"><mrow id="S2.E1.m1.8.8.1" xref="S2.E1.m1.8.8.1.1.cmml"><mrow id="S2.E1.m1.8.8.1.1" xref="S2.E1.m1.8.8.1.1.cmml"><msubsup id="S2.E1.m1.8.8.1.1.2" xref="S2.E1.m1.8.8.1.1.2.cmml"><mi id="S2.E1.m1.8.8.1.1.2.2.2" xref="S2.E1.m1.8.8.1.1.2.2.2.cmml">ùêã</mi><mrow id="S2.E1.m1.7.7.3.3" xref="S2.E1.m1.7.7.3.4.cmml"><mi id="S2.E1.m1.6.6.2.2" xref="S2.E1.m1.6.6.2.2.cmml">b</mi><mo id="S2.E1.m1.7.7.3.3.2" xref="S2.E1.m1.7.7.3.4.cmml">,</mo><mrow id="S2.E1.m1.7.7.3.3.1" xref="S2.E1.m1.7.7.3.3.1.cmml"><mrow id="S2.E1.m1.7.7.3.3.1.2.2" xref="S2.E1.m1.7.7.3.3.1.2.1.cmml"><mo id="S2.E1.m1.7.7.3.3.1.2.2.1" stretchy="false" xref="S2.E1.m1.7.7.3.3.1.2.1.1.cmml">|</mo><mi id="S2.E1.m1.5.5.1.1" xref="S2.E1.m1.5.5.1.1.cmml">T</mi><mo id="S2.E1.m1.7.7.3.3.1.2.2.2" stretchy="false" xref="S2.E1.m1.7.7.3.3.1.2.1.1.cmml">|</mo></mrow><mo id="S2.E1.m1.7.7.3.3.1.1" xref="S2.E1.m1.7.7.3.3.1.1.cmml">+</mo><mn id="S2.E1.m1.7.7.3.3.1.3" xref="S2.E1.m1.7.7.3.3.1.3.cmml">1</mn></mrow></mrow><mo id="S2.E1.m1.8.8.1.1.2.2.3" xref="S2.E1.m1.8.8.1.1.2.2.3.cmml">‚Ä≤</mo></msubsup><mo id="S2.E1.m1.8.8.1.1.1" xref="S2.E1.m1.8.8.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.4.4" xref="S2.E1.m1.8.8.1.1.3.1.cmml"><mo id="S2.E1.m1.4.4.5" xref="S2.E1.m1.8.8.1.1.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S2.E1.m1.4.4.4" rowspacing="0pt" xref="S2.E1.m1.8.8.1.1.3.1.cmml"><mtr id="S2.E1.m1.4.4.4a" xref="S2.E1.m1.8.8.1.1.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E1.m1.4.4.4b" xref="S2.E1.m1.8.8.1.1.3.1.cmml"><mn id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml">0</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E1.m1.4.4.4c" xref="S2.E1.m1.8.8.1.1.3.1.cmml"><mrow id="S2.E1.m1.2.2.2.2.2.1" xref="S2.E1.m1.2.2.2.2.2.1.cmml"><mrow id="S2.E1.m1.2.2.2.2.2.1.3" xref="S2.E1.m1.2.2.2.2.2.1.3.cmml"><mi id="S2.E1.m1.2.2.2.2.2.1.3.2" xref="S2.E1.m1.2.2.2.2.2.1.3.2.cmml">if</mi><mo id="S2.E1.m1.2.2.2.2.2.1.3.1" lspace="0.660em" xref="S2.E1.m1.2.2.2.2.2.1.3.1.cmml">‚Å¢</mo><msub id="S2.E1.m1.2.2.2.2.2.1.3.3" xref="S2.E1.m1.2.2.2.2.2.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.2.2.2.2.2.1.3.3.2" xref="S2.E1.m1.2.2.2.2.2.1.3.3.2.cmml">‚Ñ±</mi><mi id="S2.E1.m1.2.2.2.2.2.1.3.3.3" xref="S2.E1.m1.2.2.2.2.2.1.3.3.3.cmml">id</mi></msub><mo id="S2.E1.m1.2.2.2.2.2.1.3.1a" xref="S2.E1.m1.2.2.2.2.2.1.3.1.cmml">‚Å¢</mo><mrow id="S2.E1.m1.2.2.2.2.2.1.3.4.2" xref="S2.E1.m1.2.2.2.2.2.1.3.cmml"><mo id="S2.E1.m1.2.2.2.2.2.1.3.4.2.1" stretchy="false" xref="S2.E1.m1.2.2.2.2.2.1.3.cmml">(</mo><mi id="S2.E1.m1.2.2.2.2.2.1.1" xref="S2.E1.m1.2.2.2.2.2.1.1.cmml">b</mi><mo id="S2.E1.m1.2.2.2.2.2.1.3.4.2.2" stretchy="false" xref="S2.E1.m1.2.2.2.2.2.1.3.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.2.2.2.2.2.1.2" xref="S2.E1.m1.2.2.2.2.2.1.2.cmml">‚àà</mo><mi id="S2.E1.m1.2.2.2.2.2.1.4" xref="S2.E1.m1.2.2.2.2.2.1.4.cmml">T</mi></mrow></mtd></mtr><mtr id="S2.E1.m1.4.4.4d" xref="S2.E1.m1.8.8.1.1.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E1.m1.4.4.4e" xref="S2.E1.m1.8.8.1.1.3.1.cmml"><mrow id="S2.E1.m1.3.3.3.3.1.1" xref="S2.E1.m1.3.3.3.3.1.1.cmml"><mstyle displaystyle="false" id="S2.E1.m1.3.3.3.3.1.1.1" xref="S2.E1.m1.3.3.3.3.1.1.1.cmml"><mfrac id="S2.E1.m1.3.3.3.3.1.1.1a" xref="S2.E1.m1.3.3.3.3.1.1.1.cmml"><mn id="S2.E1.m1.3.3.3.3.1.1.1.3" xref="S2.E1.m1.3.3.3.3.1.1.1.3.cmml">1</mn><mrow id="S2.E1.m1.3.3.3.3.1.1.1.1.1" xref="S2.E1.m1.3.3.3.3.1.1.1.1.2.cmml"><mo id="S2.E1.m1.3.3.3.3.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.3.3.3.3.1.1.1.1.2.1.cmml">|</mo><msub id="S2.E1.m1.3.3.3.3.1.1.1.1.1.1" xref="S2.E1.m1.3.3.3.3.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.3.3.3.3.1.1.1.1.1.1.2" xref="S2.E1.m1.3.3.3.3.1.1.1.1.1.1.2.cmml">B</mi><mi id="S2.E1.m1.3.3.3.3.1.1.1.1.1.1.3" xref="S2.E1.m1.3.3.3.3.1.1.1.1.1.1.3.cmml">T</mi></msub><mo id="S2.E1.m1.3.3.3.3.1.1.1.1.1.3" stretchy="false" xref="S2.E1.m1.3.3.3.3.1.1.1.1.2.1.cmml">|</mo></mrow></mfrac></mstyle><mo id="S2.E1.m1.3.3.3.3.1.1.5" xref="S2.E1.m1.3.3.3.3.1.1.5.cmml">‚Å¢</mo><mrow id="S2.E1.m1.3.3.3.3.1.1.6" xref="S2.E1.m1.3.3.3.3.1.1.6.cmml"><mstyle displaystyle="false" id="S2.E1.m1.3.3.3.3.1.1.6.1" xref="S2.E1.m1.3.3.3.3.1.1.6.1.cmml"><msub id="S2.E1.m1.3.3.3.3.1.1.6.1a" xref="S2.E1.m1.3.3.3.3.1.1.6.1.cmml"><mo id="S2.E1.m1.3.3.3.3.1.1.6.1.2" xref="S2.E1.m1.3.3.3.3.1.1.6.1.2.cmml">‚àë</mo><mrow id="S2.E1.m1.3.3.3.3.1.1.6.1.3" xref="S2.E1.m1.3.3.3.3.1.1.6.1.3.cmml"><mi id="S2.E1.m1.3.3.3.3.1.1.6.1.3.2" xref="S2.E1.m1.3.3.3.3.1.1.6.1.3.2.cmml">i</mi><mo id="S2.E1.m1.3.3.3.3.1.1.6.1.3.1" xref="S2.E1.m1.3.3.3.3.1.1.6.1.3.1.cmml">‚àà</mo><msub id="S2.E1.m1.3.3.3.3.1.1.6.1.3.3" xref="S2.E1.m1.3.3.3.3.1.1.6.1.3.3.cmml"><mi id="S2.E1.m1.3.3.3.3.1.1.6.1.3.3.2" xref="S2.E1.m1.3.3.3.3.1.1.6.1.3.3.2.cmml">B</mi><mi id="S2.E1.m1.3.3.3.3.1.1.6.1.3.3.3" xref="S2.E1.m1.3.3.3.3.1.1.6.1.3.3.3.cmml">T</mi></msub></mrow></msub></mstyle><msub id="S2.E1.m1.3.3.3.3.1.1.6.2" xref="S2.E1.m1.3.3.3.3.1.1.6.2.cmml"><mi id="S2.E1.m1.3.3.3.3.1.1.6.2.2" xref="S2.E1.m1.3.3.3.3.1.1.6.2.2.cmml">ùêã</mi><mrow id="S2.E1.m1.3.3.3.3.1.1.4.3.3" xref="S2.E1.m1.3.3.3.3.1.1.4.3.4.cmml"><mi id="S2.E1.m1.3.3.3.3.1.1.3.2.2" xref="S2.E1.m1.3.3.3.3.1.1.3.2.2.cmml">i</mi><mo id="S2.E1.m1.3.3.3.3.1.1.4.3.3.2" xref="S2.E1.m1.3.3.3.3.1.1.4.3.4.cmml">,</mo><mrow id="S2.E1.m1.3.3.3.3.1.1.4.3.3.1" xref="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.cmml"><msub id="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.2" xref="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.2.2" xref="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.2.2.cmml">‚Ñ±</mi><mi id="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.2.3" xref="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.2.3.cmml">id</mi></msub><mo id="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.1" xref="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.1.cmml">‚Å¢</mo><mrow id="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.3.2" xref="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.cmml"><mo id="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.3.2.1" stretchy="false" xref="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.cmml">(</mo><mi id="S2.E1.m1.3.3.3.3.1.1.2.1.1" xref="S2.E1.m1.3.3.3.3.1.1.2.1.1.cmml">i</mi><mo id="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.3.2.2" stretchy="false" xref="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.cmml">)</mo></mrow></mrow></mrow></msub></mrow></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E1.m1.4.4.4f" xref="S2.E1.m1.8.8.1.1.3.1.cmml"><mi id="S2.E1.m1.4.4.4.4.2.1" xref="S2.E1.m1.4.4.4.4.2.1.cmml">otherwise</mi></mtd></mtr></mtable></mrow></mrow><mo id="S2.E1.m1.8.8.1.2" xref="S2.E1.m1.8.8.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.8b"><apply id="S2.E1.m1.8.8.1.1.cmml" xref="S2.E1.m1.8.8.1"><eq id="S2.E1.m1.8.8.1.1.1.cmml" xref="S2.E1.m1.8.8.1.1.1"></eq><apply id="S2.E1.m1.8.8.1.1.2.cmml" xref="S2.E1.m1.8.8.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.8.8.1.1.2.1.cmml" xref="S2.E1.m1.8.8.1.1.2">subscript</csymbol><apply id="S2.E1.m1.8.8.1.1.2.2.cmml" xref="S2.E1.m1.8.8.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.8.8.1.1.2.2.1.cmml" xref="S2.E1.m1.8.8.1.1.2">superscript</csymbol><ci id="S2.E1.m1.8.8.1.1.2.2.2.cmml" xref="S2.E1.m1.8.8.1.1.2.2.2">ùêã</ci><ci id="S2.E1.m1.8.8.1.1.2.2.3.cmml" xref="S2.E1.m1.8.8.1.1.2.2.3">‚Ä≤</ci></apply><list id="S2.E1.m1.7.7.3.4.cmml" xref="S2.E1.m1.7.7.3.3"><ci id="S2.E1.m1.6.6.2.2.cmml" xref="S2.E1.m1.6.6.2.2">ùëè</ci><apply id="S2.E1.m1.7.7.3.3.1.cmml" xref="S2.E1.m1.7.7.3.3.1"><plus id="S2.E1.m1.7.7.3.3.1.1.cmml" xref="S2.E1.m1.7.7.3.3.1.1"></plus><apply id="S2.E1.m1.7.7.3.3.1.2.1.cmml" xref="S2.E1.m1.7.7.3.3.1.2.2"><abs id="S2.E1.m1.7.7.3.3.1.2.1.1.cmml" xref="S2.E1.m1.7.7.3.3.1.2.2.1"></abs><ci id="S2.E1.m1.5.5.1.1.cmml" xref="S2.E1.m1.5.5.1.1">ùëá</ci></apply><cn id="S2.E1.m1.7.7.3.3.1.3.cmml" type="integer" xref="S2.E1.m1.7.7.3.3.1.3">1</cn></apply></list></apply><apply id="S2.E1.m1.8.8.1.1.3.1.cmml" xref="S2.E1.m1.4.4"><csymbol cd="latexml" id="S2.E1.m1.8.8.1.1.3.1.1.cmml" xref="S2.E1.m1.4.4.5">cases</csymbol><cn id="S2.E1.m1.1.1.1.1.1.1.cmml" type="integer" xref="S2.E1.m1.1.1.1.1.1.1">0</cn><apply id="S2.E1.m1.2.2.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.2.2.1"><in id="S2.E1.m1.2.2.2.2.2.1.2.cmml" xref="S2.E1.m1.2.2.2.2.2.1.2"></in><apply id="S2.E1.m1.2.2.2.2.2.1.3.cmml" xref="S2.E1.m1.2.2.2.2.2.1.3"><times id="S2.E1.m1.2.2.2.2.2.1.3.1.cmml" xref="S2.E1.m1.2.2.2.2.2.1.3.1"></times><ci id="S2.E1.m1.2.2.2.2.2.1.3.2.cmml" xref="S2.E1.m1.2.2.2.2.2.1.3.2">if</ci><apply id="S2.E1.m1.2.2.2.2.2.1.3.3.cmml" xref="S2.E1.m1.2.2.2.2.2.1.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.2.1.3.3.1.cmml" xref="S2.E1.m1.2.2.2.2.2.1.3.3">subscript</csymbol><ci id="S2.E1.m1.2.2.2.2.2.1.3.3.2.cmml" xref="S2.E1.m1.2.2.2.2.2.1.3.3.2">‚Ñ±</ci><ci id="S2.E1.m1.2.2.2.2.2.1.3.3.3.cmml" xref="S2.E1.m1.2.2.2.2.2.1.3.3.3">id</ci></apply><ci id="S2.E1.m1.2.2.2.2.2.1.1.cmml" xref="S2.E1.m1.2.2.2.2.2.1.1">ùëè</ci></apply><ci id="S2.E1.m1.2.2.2.2.2.1.4.cmml" xref="S2.E1.m1.2.2.2.2.2.1.4">ùëá</ci></apply><apply id="S2.E1.m1.3.3.3.3.1.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1"><times id="S2.E1.m1.3.3.3.3.1.1.5.cmml" xref="S2.E1.m1.3.3.3.3.1.1.5"></times><apply id="S2.E1.m1.3.3.3.3.1.1.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1.1"><divide id="S2.E1.m1.3.3.3.3.1.1.1.2.cmml" xref="S2.E1.m1.3.3.3.3.1.1.1"></divide><cn id="S2.E1.m1.3.3.3.3.1.1.1.3.cmml" type="integer" xref="S2.E1.m1.3.3.3.3.1.1.1.3">1</cn><apply id="S2.E1.m1.3.3.3.3.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.3.3.1.1.1.1.1"><abs id="S2.E1.m1.3.3.3.3.1.1.1.1.2.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1.1.1.1.2"></abs><apply id="S2.E1.m1.3.3.3.3.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.3.3.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.3.3.3.3.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.3.3.1.1.1.1.1.1.2">ùêµ</ci><ci id="S2.E1.m1.3.3.3.3.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.3.3.3.3.1.1.1.1.1.1.3">ùëá</ci></apply></apply></apply><apply id="S2.E1.m1.3.3.3.3.1.1.6.cmml" xref="S2.E1.m1.3.3.3.3.1.1.6"><apply id="S2.E1.m1.3.3.3.3.1.1.6.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1.6.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.3.3.1.1.6.1.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1.6.1">subscript</csymbol><sum id="S2.E1.m1.3.3.3.3.1.1.6.1.2.cmml" xref="S2.E1.m1.3.3.3.3.1.1.6.1.2"></sum><apply id="S2.E1.m1.3.3.3.3.1.1.6.1.3.cmml" xref="S2.E1.m1.3.3.3.3.1.1.6.1.3"><in id="S2.E1.m1.3.3.3.3.1.1.6.1.3.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1.6.1.3.1"></in><ci id="S2.E1.m1.3.3.3.3.1.1.6.1.3.2.cmml" xref="S2.E1.m1.3.3.3.3.1.1.6.1.3.2">ùëñ</ci><apply id="S2.E1.m1.3.3.3.3.1.1.6.1.3.3.cmml" xref="S2.E1.m1.3.3.3.3.1.1.6.1.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.3.3.1.1.6.1.3.3.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1.6.1.3.3">subscript</csymbol><ci id="S2.E1.m1.3.3.3.3.1.1.6.1.3.3.2.cmml" xref="S2.E1.m1.3.3.3.3.1.1.6.1.3.3.2">ùêµ</ci><ci id="S2.E1.m1.3.3.3.3.1.1.6.1.3.3.3.cmml" xref="S2.E1.m1.3.3.3.3.1.1.6.1.3.3.3">ùëá</ci></apply></apply></apply><apply id="S2.E1.m1.3.3.3.3.1.1.6.2.cmml" xref="S2.E1.m1.3.3.3.3.1.1.6.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.3.3.1.1.6.2.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1.6.2">subscript</csymbol><ci id="S2.E1.m1.3.3.3.3.1.1.6.2.2.cmml" xref="S2.E1.m1.3.3.3.3.1.1.6.2.2">ùêã</ci><list id="S2.E1.m1.3.3.3.3.1.1.4.3.4.cmml" xref="S2.E1.m1.3.3.3.3.1.1.4.3.3"><ci id="S2.E1.m1.3.3.3.3.1.1.3.2.2.cmml" xref="S2.E1.m1.3.3.3.3.1.1.3.2.2">ùëñ</ci><apply id="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1.4.3.3.1"><times id="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.1"></times><apply id="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.2.cmml" xref="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.2.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.2">subscript</csymbol><ci id="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.2.2.cmml" xref="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.2.2">‚Ñ±</ci><ci id="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.2.3.cmml" xref="S2.E1.m1.3.3.3.3.1.1.4.3.3.1.2.3">id</ci></apply><ci id="S2.E1.m1.3.3.3.3.1.1.2.1.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1.2.1.1">ùëñ</ci></apply></list></apply></apply></apply><ci id="S2.E1.m1.4.4.4.4.2.1.cmml" xref="S2.E1.m1.4.4.4.4.2.1">otherwise</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.8c">\mathbf{L}^{\prime}_{b,|T|+1}=\begin{cases}0&amp;\mathrm{if}~{}~{}\mathcal{F}_{%
\mathrm{id}}(b)\in T\\
\frac{1}{|B_{T}|}\sum_{i\in B_{T}}\mathbf{L}_{i,\mathcal{F}_{\mathrm{id}}(i)}&amp;%
\mathrm{otherwise}\\
\end{cases},</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.8d">bold_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_b , | italic_T | + 1 end_POSTSUBSCRIPT = { start_ROW start_CELL 0 end_CELL start_CELL roman_if caligraphic_F start_POSTSUBSCRIPT roman_id end_POSTSUBSCRIPT ( italic_b ) ‚àà italic_T end_CELL end_ROW start_ROW start_CELL divide start_ARG 1 end_ARG start_ARG | italic_B start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT | end_ARG ‚àë start_POSTSUBSCRIPT italic_i ‚àà italic_B start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT bold_L start_POSTSUBSCRIPT italic_i , caligraphic_F start_POSTSUBSCRIPT roman_id end_POSTSUBSCRIPT ( italic_i ) end_POSTSUBSCRIPT end_CELL start_CELL roman_otherwise end_CELL end_ROW ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS4.p2.6">where <math alttext="B_{T}=\{i|i\in B\land\mathcal{F}_{\mathrm{id}}(i)\in T\}" class="ltx_Math" display="inline" id="S2.SS4.p2.6.m1.3"><semantics id="S2.SS4.p2.6.m1.3a"><mrow id="S2.SS4.p2.6.m1.3.3" xref="S2.SS4.p2.6.m1.3.3.cmml"><msub id="S2.SS4.p2.6.m1.3.3.3" xref="S2.SS4.p2.6.m1.3.3.3.cmml"><mi id="S2.SS4.p2.6.m1.3.3.3.2" xref="S2.SS4.p2.6.m1.3.3.3.2.cmml">B</mi><mi id="S2.SS4.p2.6.m1.3.3.3.3" xref="S2.SS4.p2.6.m1.3.3.3.3.cmml">T</mi></msub><mo id="S2.SS4.p2.6.m1.3.3.2" xref="S2.SS4.p2.6.m1.3.3.2.cmml">=</mo><mrow id="S2.SS4.p2.6.m1.3.3.1.1" xref="S2.SS4.p2.6.m1.3.3.1.2.cmml"><mo id="S2.SS4.p2.6.m1.3.3.1.1.2" stretchy="false" xref="S2.SS4.p2.6.m1.3.3.1.2.1.cmml">{</mo><mi id="S2.SS4.p2.6.m1.2.2" xref="S2.SS4.p2.6.m1.2.2.cmml">i</mi><mo id="S2.SS4.p2.6.m1.3.3.1.1.3" lspace="0em" rspace="0em" xref="S2.SS4.p2.6.m1.3.3.1.2.1.cmml">|</mo><mrow id="S2.SS4.p2.6.m1.3.3.1.1.1" xref="S2.SS4.p2.6.m1.3.3.1.1.1.cmml"><mi id="S2.SS4.p2.6.m1.3.3.1.1.1.2" xref="S2.SS4.p2.6.m1.3.3.1.1.1.2.cmml">i</mi><mo id="S2.SS4.p2.6.m1.3.3.1.1.1.3" xref="S2.SS4.p2.6.m1.3.3.1.1.1.3.cmml">‚àà</mo><mrow id="S2.SS4.p2.6.m1.3.3.1.1.1.4" xref="S2.SS4.p2.6.m1.3.3.1.1.1.4.cmml"><mi id="S2.SS4.p2.6.m1.3.3.1.1.1.4.2" xref="S2.SS4.p2.6.m1.3.3.1.1.1.4.2.cmml">B</mi><mo id="S2.SS4.p2.6.m1.3.3.1.1.1.4.1" xref="S2.SS4.p2.6.m1.3.3.1.1.1.4.1.cmml">‚àß</mo><mrow id="S2.SS4.p2.6.m1.3.3.1.1.1.4.3" xref="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.cmml"><msub id="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.2" xref="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.2.2" xref="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.2.2.cmml">‚Ñ±</mi><mi id="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.2.3" xref="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.2.3.cmml">id</mi></msub><mo id="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.1" xref="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.1.cmml">‚Å¢</mo><mrow id="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.3.2" xref="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.cmml"><mo id="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.3.2.1" stretchy="false" xref="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.cmml">(</mo><mi id="S2.SS4.p2.6.m1.1.1" xref="S2.SS4.p2.6.m1.1.1.cmml">i</mi><mo id="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.3.2.2" stretchy="false" xref="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.cmml">)</mo></mrow></mrow></mrow><mo id="S2.SS4.p2.6.m1.3.3.1.1.1.5" xref="S2.SS4.p2.6.m1.3.3.1.1.1.5.cmml">‚àà</mo><mi id="S2.SS4.p2.6.m1.3.3.1.1.1.6" xref="S2.SS4.p2.6.m1.3.3.1.1.1.6.cmml">T</mi></mrow><mo id="S2.SS4.p2.6.m1.3.3.1.1.4" stretchy="false" xref="S2.SS4.p2.6.m1.3.3.1.2.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.6.m1.3b"><apply id="S2.SS4.p2.6.m1.3.3.cmml" xref="S2.SS4.p2.6.m1.3.3"><eq id="S2.SS4.p2.6.m1.3.3.2.cmml" xref="S2.SS4.p2.6.m1.3.3.2"></eq><apply id="S2.SS4.p2.6.m1.3.3.3.cmml" xref="S2.SS4.p2.6.m1.3.3.3"><csymbol cd="ambiguous" id="S2.SS4.p2.6.m1.3.3.3.1.cmml" xref="S2.SS4.p2.6.m1.3.3.3">subscript</csymbol><ci id="S2.SS4.p2.6.m1.3.3.3.2.cmml" xref="S2.SS4.p2.6.m1.3.3.3.2">ùêµ</ci><ci id="S2.SS4.p2.6.m1.3.3.3.3.cmml" xref="S2.SS4.p2.6.m1.3.3.3.3">ùëá</ci></apply><apply id="S2.SS4.p2.6.m1.3.3.1.2.cmml" xref="S2.SS4.p2.6.m1.3.3.1.1"><csymbol cd="latexml" id="S2.SS4.p2.6.m1.3.3.1.2.1.cmml" xref="S2.SS4.p2.6.m1.3.3.1.1.2">conditional-set</csymbol><ci id="S2.SS4.p2.6.m1.2.2.cmml" xref="S2.SS4.p2.6.m1.2.2">ùëñ</ci><apply id="S2.SS4.p2.6.m1.3.3.1.1.1.cmml" xref="S2.SS4.p2.6.m1.3.3.1.1.1"><and id="S2.SS4.p2.6.m1.3.3.1.1.1a.cmml" xref="S2.SS4.p2.6.m1.3.3.1.1.1"></and><apply id="S2.SS4.p2.6.m1.3.3.1.1.1b.cmml" xref="S2.SS4.p2.6.m1.3.3.1.1.1"><in id="S2.SS4.p2.6.m1.3.3.1.1.1.3.cmml" xref="S2.SS4.p2.6.m1.3.3.1.1.1.3"></in><ci id="S2.SS4.p2.6.m1.3.3.1.1.1.2.cmml" xref="S2.SS4.p2.6.m1.3.3.1.1.1.2">ùëñ</ci><apply id="S2.SS4.p2.6.m1.3.3.1.1.1.4.cmml" xref="S2.SS4.p2.6.m1.3.3.1.1.1.4"><and id="S2.SS4.p2.6.m1.3.3.1.1.1.4.1.cmml" xref="S2.SS4.p2.6.m1.3.3.1.1.1.4.1"></and><ci id="S2.SS4.p2.6.m1.3.3.1.1.1.4.2.cmml" xref="S2.SS4.p2.6.m1.3.3.1.1.1.4.2">ùêµ</ci><apply id="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.cmml" xref="S2.SS4.p2.6.m1.3.3.1.1.1.4.3"><times id="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.1.cmml" xref="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.1"></times><apply id="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.2.cmml" xref="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.2"><csymbol cd="ambiguous" id="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.2.1.cmml" xref="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.2">subscript</csymbol><ci id="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.2.2.cmml" xref="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.2.2">‚Ñ±</ci><ci id="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.2.3.cmml" xref="S2.SS4.p2.6.m1.3.3.1.1.1.4.3.2.3">id</ci></apply><ci id="S2.SS4.p2.6.m1.1.1.cmml" xref="S2.SS4.p2.6.m1.1.1">ùëñ</ci></apply></apply></apply><apply id="S2.SS4.p2.6.m1.3.3.1.1.1c.cmml" xref="S2.SS4.p2.6.m1.3.3.1.1.1"><in id="S2.SS4.p2.6.m1.3.3.1.1.1.5.cmml" xref="S2.SS4.p2.6.m1.3.3.1.1.1.5"></in><share href="https://arxiv.org/html/2410.02364v1#S2.SS4.p2.6.m1.3.3.1.1.1.4.cmml" id="S2.SS4.p2.6.m1.3.3.1.1.1d.cmml" xref="S2.SS4.p2.6.m1.3.3.1.1.1"></share><ci id="S2.SS4.p2.6.m1.3.3.1.1.1.6.cmml" xref="S2.SS4.p2.6.m1.3.3.1.1.1.6">ùëá</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.6.m1.3c">B_{T}=\{i|i\in B\land\mathcal{F}_{\mathrm{id}}(i)\in T\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.6.m1.3d">italic_B start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT = { italic_i | italic_i ‚àà italic_B ‚àß caligraphic_F start_POSTSUBSCRIPT roman_id end_POSTSUBSCRIPT ( italic_i ) ‚àà italic_T }</annotation></semantics></math> represents a set of examples in a mini-batch with known speaker identities. For examples with known labels, we set the expected logit of the unknown class to zero. Recalling that logits are scaled cosine similarities, zero corresponds to the perpendicularity of extracted embedding and imagined embedding of the other class. Empirically, embeddings that are perpendicular to each other belong to different speakers. In the case of an example for which the label is unknown, we set its logit to an expected logit the network would yield for the target class at a given point of training. We estimate the expectation over one mini-batch. In turn, a loss for the example with an unknown label can be decreased by decreasing the probability of all known classes. Hence, the network is encouraged to model the differences between those known speakers having similar characteristics with the unknown speakers, further increasing the discriminability of the embeddings.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Experimental Setup</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">For better reproducibility and straightforward experimentation, we integrated our approach to weakly-supervised training into a WeSpeaker toolkit¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib13" title="">13</a>]</cite>. The main contributions to the toolkit are the data loader and the versions of AAM with aggregation. Notably, due to the nature of the data partitioned into multiple clusters, our data-loading implementation provides mini-batches of dynamic size fluctuating up to <math alttext="\pm" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mo id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><csymbol cd="latexml" id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">¬±</annotation></semantics></math>10% around the target mini-batch size, which should be kept in mind w.r.t. GPU resources.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison of VoxCeleb2 sets</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1">Dev. Set</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1">Celebrities</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.1">Recordings</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.4.1">Hours</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.2.1.1.1">Original</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1.2">5,994</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1.3">145,569</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1.4">2,369.0</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.3.2.1.1">Restricted</span></th>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.2.2">5,987</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.2.3">110,940</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.2.4">1,884.3</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S3.T1.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.4.3.1.1">Uncut</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.1.4.3.2">5,987</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.1.4.3.3">110,940</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T1.1.4.3.4">10,211.6</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Our training corpus <span class="ltx_text ltx_font_italic" id="S3.p2.1.1">VoxCeleb2 Uncut dev</span> has been acquired by downloading full-length audio recordings from YouTube, without its visual components. As some video clips are no longer available, some speakers have been lost compared to the original VoxCeleb2 dev set, as shown in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#S3.T1" title="TABLE I ‚Ä£ III Experimental Setup ‚Ä£ State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)"><span class="ltx_text ltx_ref_tag">I</span></a>. For comparison purposes, we have created a <span class="ltx_text ltx_font_italic" id="S3.p2.1.2">restricted</span> version in which the number of speakers and recordings has been limited to match the <span class="ltx_text ltx_font_italic" id="S3.p2.1.3">uncut</span> version.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">As the backbone of our embedding extractor, we use ResNet34 and MHFA along with WavLM features. For ResNet34, we employ 2D convolutional layers with 64, 128, 256, and 256 filters. The original block structure has been modified following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib19" title="">19</a>]</cite>. Since our diarized segments may contain non-speech content, we employ instant normalization instead of batch normalization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib20" title="">20</a>]</cite>. The input to our ResNet models consists of 400 frames of 80-dimensional log Mel-filter bank energy (fbank) features.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">In experiments employing pre-trained models, we opt for a WavLM Base+ for its size and performance of downstream models that build on top of it in the SUPERB benchmark¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib21" title="">21</a>]</cite>. The input duration is 4‚Äâs. The MHFA back-end comprises 64 query heads and provides 256-dimensional embeddings.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">For AAM, we employ a scale <math alttext="s=30" class="ltx_Math" display="inline" id="S3.p5.1.m1.1"><semantics id="S3.p5.1.m1.1a"><mrow id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml"><mi id="S3.p5.1.m1.1.1.2" xref="S3.p5.1.m1.1.1.2.cmml">s</mi><mo id="S3.p5.1.m1.1.1.1" xref="S3.p5.1.m1.1.1.1.cmml">=</mo><mn id="S3.p5.1.m1.1.1.3" xref="S3.p5.1.m1.1.1.3.cmml">30</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.1b"><apply id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1"><eq id="S3.p5.1.m1.1.1.1.cmml" xref="S3.p5.1.m1.1.1.1"></eq><ci id="S3.p5.1.m1.1.1.2.cmml" xref="S3.p5.1.m1.1.1.2">ùë†</ci><cn id="S3.p5.1.m1.1.1.3.cmml" type="integer" xref="S3.p5.1.m1.1.1.3">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.1c">s=30</annotation><annotation encoding="application/x-llamapun" id="S3.p5.1.m1.1d">italic_s = 30</annotation></semantics></math> for all models. For the first stage, the margin setting will be discussed in the experimental part; for the second stage, we scheduled the margin from 0.1 to 0.3 or 0.2 for ResNet34- and MHFA-based models, respectively, following our previous empirical experience. The models are trained employing SGD with a momentum of 0.9. After a warm-up phase, we exponentially decay a learning rate. The maximum learning rate is set to 0.2 and 0.01, the final value is set to 5e-5 and 4.4e-3 for ResNet34 and MHFA-based models, respectively.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experiments</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">First stage</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Our previous research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib10" title="">10</a>]</cite> focused on establishing a viable pipeline for training with weak labels, but it was limited in terms of hyperparameter exploration and their mutual influence. Therefore, in this section, we provide an ablation study of our version of the AAM with aggregation. Analyzing the results in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#S4.T2" title="TABLE II ‚Ä£ IV-A First stage ‚Ä£ IV Experiments ‚Ä£ State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)"><span class="ltx_text ltx_ref_tag">II</span></a>, we reinforce our previous findings on the superiority of the LSE with a scheduled <math alttext="\tau" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">œÑ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">ùúè</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">italic_œÑ</annotation></semantics></math> when using a margin of 0.1. Enforcing a margin during training with strong labels (such as VoxCeleb) helps to make speaker clusters in the embedding space more separable. However, in our case, the examples do not contain only the target speaker‚Äôs speech, which makes them more difficult. Then, enforcing the margin (which impedes training) is questionable. It turns out that releasing the margin constraint increases performance when using the max-pooling (Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#S4.T2" title="TABLE II ‚Ä£ IV-A First stage ‚Ä£ IV Experiments ‚Ä£ State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)"><span class="ltx_text ltx_ref_tag">II</span></a>). We observe an interesting trade-off between the two hyperparameters: the use of margin calls for smoothing provided by LSE; removed margin allows for a more strict pooling method.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Additionally, Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#S4.T3" title="TABLE III ‚Ä£ IV-A First stage ‚Ä£ IV Experiments ‚Ä£ State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)"><span class="ltx_text ltx_ref_tag">III</span></a> presents the impact of using features from the pre-trained WavLM model and the MHFA back-end. This approach outperformed the use of ResNet34 as the backbone architecture keeping a similar training time. Following previous ResNet34 results, models employing max-pooling lead to superior performance than those using LSE, with the improvement being even more pronounced in this case. In contrast to the results observed with ResNet34, max-pooling aggregation benefits from adding margin to the AAM loss. We hypothesize that the WavLM model provides speaker-enriched features resulting from pre-training that allow for stricter loss. Although we achieve more competitive results with WavLM + MHFA, they are still inferior compared to strong supervision training, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#S4.T4" title="TABLE IV ‚Ä£ IV-A First stage ‚Ä£ IV Experiments ‚Ä£ State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)"><span class="ltx_text ltx_ref_tag">IV</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Speaker verification results for the first stage employing ResNet34 as backend model. Results are reported on the three trials of VoxCeleb1, employing EER(%) metric. A model denoted as p uses Pyannote diarization.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.4.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T2.2.2.2.3"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1">Aggregation /<math alttext="\tau" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.1.m1.1a"><mi id="S4.T2.1.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.1.m1.1.1.cmml">œÑ</mi><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1">ùúè</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.1.m1.1d">bold_italic_œÑ</annotation></semantics></math></span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.2"><math alttext="m" class="ltx_Math" display="inline" id="S4.T2.2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.2.m1.1a"><mi id="S4.T2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.cmml">ùíé</mi><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1">ùíé</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">m</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.2.m1.1d">bold_italic_m</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.4.1">Vox1-O</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.5.1">Vox1-E</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.6.1">Vox1-H</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.5.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T2.4.4.5.1.1">m1</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.4.4.5.1.2">max / -</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.5.1.3" rowspan="3"><span class="ltx_text" id="S4.T2.4.4.5.1.3.1">0.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.5.1.4">4.44</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.5.1.5">4.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.5.1.6">7.43</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.6.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.4.4.6.2.1">m2</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.4.4.6.2.2">LSE / 0.5</th>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.6.2.3">4.33</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.6.2.4">4.33</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.6.2.5">7.01</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.3.3.3.2">m3</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.3.3.3.1">LSE / 0.5 <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T2.3.3.3.1.m1.1"><semantics id="S4.T2.3.3.3.1.m1.1a"><mo id="S4.T2.3.3.3.1.m1.1.1" stretchy="false" xref="S4.T2.3.3.3.1.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.1.m1.1b"><ci id="S4.T2.3.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.1.m1.1d">‚Üí</annotation></semantics></math> 0.1</th>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3.3">3.85</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3.4">3.90</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.3.5">6.30</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.7.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T2.4.4.7.3.1">m4</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.4.4.7.3.2">max / -</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.7.3.3" rowspan="3"><span class="ltx_text" id="S4.T2.4.4.7.3.3.1">0.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.7.3.4"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.7.3.4.1">3.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.7.3.5"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.7.3.5.1">2.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.7.3.6"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.7.3.6.1">4.68</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.8.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.4.4.8.4.1">m5</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.4.4.8.4.2">LSE / 0.5</th>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.8.4.3">4.34</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.8.4.4">4.44</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.8.4.5">7.29</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.4.4.4.2">m6</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.4.4.4.1">LSE / 0.5 <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.T2.4.4.4.1.m1.1"><semantics id="S4.T2.4.4.4.1.m1.1a"><mo id="S4.T2.4.4.4.1.m1.1.1" stretchy="false" xref="S4.T2.4.4.4.1.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.1.m1.1b"><ci id="S4.T2.4.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.4.1.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.4.1.m1.1d">‚Üí</annotation></semantics></math> 0.1</th>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.3">4.86</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.4">5.20</td>
<td class="ltx_td ltx_align_center" id="S4.T2.4.4.4.5">8.25</td>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4.9.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S4.T2.4.4.9.5.1">p</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S4.T2.4.4.9.5.2">max / -</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.4.4.9.5.3">0.0</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.4.4.9.5.4">2.55</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.4.4.9.5.5">2.37</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.4.4.9.5.6">3.93</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Effects of employing WavLM features along with MHFA for training the embedding extractor during the first stage.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.2.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T3.2.2.2.3"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1">Aggregation /<math alttext="\tau" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.1.m1.1a"><mi id="S4.T3.1.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.1.m1.1.1.cmml">œÑ</mi><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1">ùúè</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.1.m1.1d">bold_italic_œÑ</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.2.2.2.2"><math alttext="m" class="ltx_Math" display="inline" id="S4.T3.2.2.2.2.m1.1"><semantics id="S4.T3.2.2.2.2.m1.1a"><mi id="S4.T3.2.2.2.2.m1.1.1" xref="S4.T3.2.2.2.2.m1.1.1.cmml">ùíé</mi><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.2.m1.1b"><ci id="S4.T3.2.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.2.m1.1.1">ùíé</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.2.m1.1c">m</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.2.m1.1d">bold_italic_m</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.2.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.2.4.1">Vox1-O</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.2.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.2.5.1">Vox1-E</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.2.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.2.6.1">Vox1-H</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.2.2.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T3.2.2.3.1.1">m7</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.2.2.3.1.2">max / -</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.3.1.3" rowspan="2"><span class="ltx_text" id="S4.T3.2.2.3.1.3.1">0.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.3.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.3.1.4.1">1.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.3.1.5"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.3.1.5.1">1.36</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.3.1.6"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.3.1.6.1">2.55</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T3.2.2.4.2.1">m8</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.2.4.2.2">LSE / 0.5</th>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.4.2.3">3.82</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.4.2.4">3.71</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.4.2.5">5.86</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T3.2.2.5.3.1">m9</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.2.2.5.3.2">max / -</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T3.2.2.5.3.3" rowspan="2"><span class="ltx_text" id="S4.T3.2.2.5.3.3.1">0.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.5.3.4">1.51</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.5.3.5">1.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.5.3.6">3.36</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.6.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S4.T3.2.2.6.4.1">m10</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T3.2.2.6.4.2">LSE / 0.5</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.2.2.6.4.3">3.59</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.2.2.6.4.4">3.10</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.2.2.6.4.5">5.60</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Results for the second stage. Margin was scheduled from 0.1 to 0.3 to every experiment. For the self-labeled data, the trained models used for selected the segments is indicated in brackets. For minDCF, target trial probability is set to 0.05.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.1">Supervision</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.2.1">Data</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.1.1.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.3.1">Speakers</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.1.1.4" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.4.1">Model</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T4.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.5.1">Vox1-O</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T4.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.6.1">Vox1-E</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T4.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.7.1">Vox1-H</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.2.2">
<td class="ltx_td ltx_align_center" id="S4.T4.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.2.2.1.1">EER</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.2.2.2.1">minDCF</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T4.1.2.2.3.1">EER</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S4.T4.1.2.2.4.1">minDCF</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S4.T4.1.2.2.5.1">EER</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T4.1.2.2.6.1">minDCF</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.3.3.1" rowspan="3"><span class="ltx_text" id="S4.T4.1.3.3.1.1">strong</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.3.3.2" rowspan="3"><span class="ltx_text" id="S4.T4.1.3.3.2.1">original</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.3.3.3" rowspan="3"><span class="ltx_text" id="S4.T4.1.3.3.3.1">5,994</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.3.3.4">ResNet34</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.3.5">0.87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.3.6">‚Äì</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.3.7">1.05</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.3.8">‚Äì</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.3.9">1.96</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.3.10">‚Äì</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.4.4.1">ResNet293</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.4.2">0.56</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.4.3">‚Äì</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.4.4">0.76</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.4.5">‚Äì</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.4.6">1.43</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.4.7">‚Äì</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.5.5.1">WavLM + MHFA</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.5.2">0.86</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.5.3">0.067</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.5.4">0.88</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.5.5">0.056</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.5.6">1.77</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.5.7">0.107</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.6.6.1" rowspan="3"><span class="ltx_text" id="S4.T4.1.6.6.1.1">strong</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.6.6.2" rowspan="3"><span class="ltx_text" id="S4.T4.1.6.6.2.1">restricted</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.6.6.3" rowspan="3"><span class="ltx_text" id="S4.T4.1.6.6.3.1">5,987</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.6.6.4">ResNet34</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.6.6.5">1.11</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.6.6.6">0.064</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.6.6.7">1.13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.6.6.8">0.073</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.6.6.9">2.07</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.6.6.10">0.121</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.7.7.1">ResNet152</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.7.7.2">0.69</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.7.7.3">0.048</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.7.7.4">0.82</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.7.7.5">0.052</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.7.7.6">1.57</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.7.7.7">0.092</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.8.8.1">WavLM + MHFA</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.8.8.2">0.86</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.8.8.3">0.065</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.8.8.4">0.86</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.8.8.5">0.057</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.8.8.6">1.78</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.8.8.7">0.111</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.9.9.1" rowspan="5"><span class="ltx_text" id="S4.T4.1.9.9.1.1">pseudo</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.9.9.2">self-labeled (m3)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.9.9.3">5,945</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.9.9.4" rowspan="3"><span class="ltx_text" id="S4.T4.1.9.9.4.1">ResNet34</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.9.9.5">0.97</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.9.9.6">0.056</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.9.9.7">1.06</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.9.9.8">0.066</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.9.9.9">1.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.9.9.10">0.111</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.10.10.1">self-labeled (m4)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T4.1.10.10.2">5,987</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.10.10.3">0.99</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.10.10.4">0.054</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.10.10.5">1.02</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.10.10.6">0.065</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.10.10.7">1.83</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.10.10.8">0.107</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.11.11.1">self-labeled (p)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T4.1.11.11.2">5,987</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.11.11.3">1.17</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.11.11.4">0.069</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.11.11.5">1.18</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.11.11.6">0.077</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.11.11.7">2.13</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.11.11.8">0.127</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.12.12.1">self-labeled (m4)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T4.1.12.12.2">5,987</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.12.12.3">ResNet152</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.12.12.4">0.84</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.12.12.5"><span class="ltx_text ltx_font_bold" id="S4.T4.1.12.12.5.1">0.051</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.12.12.6"><span class="ltx_text ltx_font_bold" id="S4.T4.1.12.12.6.1">0.85</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.12.12.7"><span class="ltx_text ltx_font_bold" id="S4.T4.1.12.12.7.1">0.053</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.12.12.8"><span class="ltx_text ltx_font_bold" id="S4.T4.1.12.12.8.1">1.55</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.12.12.9"><span class="ltx_text ltx_font_bold" id="S4.T4.1.12.12.9.1">0.091</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T4.1.13.13.1" rowspan="2"><span class="ltx_text" id="S4.T4.1.13.13.1.1">self-labeled (m10)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S4.T4.1.13.13.2" rowspan="2"><span class="ltx_text" id="S4.T4.1.13.13.2.1">5,987</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.13.13.3">WavLM + MHFA</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.13.13.4"><span class="ltx_text ltx_font_bold" id="S4.T4.1.13.13.4.1">0.83</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.13.13.5">0.062</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.13.13.6">0.92</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.13.13.7">0.062</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.13.13.8">1.91</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.13.13.9">0.121</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.14.14">
<th class="ltx_td ltx_th ltx_th_row ltx_border_b" id="S4.T4.1.14.14.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T4.1.14.14.2">+ unknown class</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.14.14.3">0.83</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.14.14.4">0.061</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.14.14.5">0.92</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.14.14.6">0.058</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.14.14.7">1.86</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T4.1.14.14.8">0.117</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Evaluation of method validity</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In this section, we evaluate the efficacy of our proposed method for training an embedding extractor employing weakly-labeled data. Previous experiments employed a basic BIC-based HAC algorithm to generate input segments, which tend to overestimate the number of clusters within each recording. To assess the dependence of method performance on segment quality, we train with segments obtained with a state-of-the-art pre-trained diarization model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#bib.bib17" title="">17</a>]</cite> from Pyannote Toolkit. This advanced diarization system not only provided segments with higher speaker purity, removing segments containing only noise and music, but also allowed us to constrain the maximum number of speakers to 4.
Although the introduction of higher-quality segments provided better results, the improvement was not as significant as expected (see m4 and p in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#S4.T2" title="TABLE II ‚Ä£ IV-A First stage ‚Ä£ IV Experiments ‚Ä£ State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)"><span class="ltx_text ltx_ref_tag">II</span></a>). This suggests that our method is stable enough and capable of extracting robust speaker embeddings, even when the quality of the input segments is suboptimal, as demonstrated by the baseline diarization approach.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">To further assess the validity of our method, we compare the quality of segments selected by our best first-stage model (m7) w.r.t. the oracle ones. Since ground-truth annotations are not available for segments provided by our diarization, we employed state-of-the-art ResNet293 trained in a supervised way on the VoxCeleb2 dataset (using the Wespeaker toolkit) to provide oracle labels. We found out that our method selected segments of all 5987 speakers totaling 6262.1 h of speech, whereas the oracle segments provide 6304.5 h of audio. We also confirm a high overlap of segments as precision and recall w.r.t. the oracle ones are 94.16 and 93.68, respectively. These findings support the validity of our method.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.4.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.5.2">Second Stage</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#S4.T4" title="TABLE IV ‚Ä£ IV-A First stage ‚Ä£ IV Experiments ‚Ä£ State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)"><span class="ltx_text ltx_ref_tag">IV</span></a> we present a comparative analysis of state-of-the-art and our second-stage systems, which were trained employing self-labeled data provided from the first stage. We include results for ResNet34 and ResNet293 models trained on the original VoxCeleb2 dataset, following the WeSpeaker recipe with their proposed tuned hyperparamenters<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>WeSpeaker Official VoxCeleb results: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/wenet-e2e/wespeaker/tree/master/examples/voxceleb/v2</span></span></span></span>. However, for comparison purposes, models trained on the <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">restricted</span>
subset follow the same ResNet architecture and hyperparameters aforementioned. Our models trained with pseudo-labeles perform much better than their counterparts trained during the first stage. When it comes to ResNet34 backbone, training with segments from m3 and m4 leads to similar results, despite the loss of some speakers when selecting segments with m3. Both of them outperform training with fully-supervised with the restricted set, and achieve similar results to the official results from WeSpeaker, even outperforming results in VoxCeleb1-H. Therefore, we can conclude the efficacy of our method, by which training with weakly-labeled data leads to a higher amount of pseudo labels which benefits the training during the second stage. We show that strong results are also obtained with a model based on WavLM and MHFA. The benefit of such a system is that it is trained approximately four times faster compared to the ResNet34-based model.
Due to the limited capability of our servers, we could not train ResNet293 from scratch. Despite not outperforming its SOTA results employing the full VoxCeleb2, we obtain very good results employing the smaller ResNet152 and training with seven speakers less.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Finally, in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2410.02364v1#S4.T4" title="TABLE IV ‚Ä£ IV-A First stage ‚Ä£ IV Experiments ‚Ä£ State-of-the-art Embeddings with Video-free Segmentation of the Source VoxCeleb Data This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by project PID2021-125943OB-I00, MCIN/AEI/10.13039/501100011033/FEDER, UE from the Spanish Ministerio de Ciencia e Innovacion, Fondo Europeo de Desarrollo Regional and by Tencent AI Lab Rhino-Bird Gift Fund. Collaboration between BUT and Omilia was supported by Horizon 2020 Marie Sklodowska-Curie grant ESPERANTO, No. 101007666. Computing was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID: 90254)"><span class="ltx_text ltx_ref_tag">IV</span></a>, we also show our preliminary results with an approach employing unlabeled segments. We used it in the second stage, where the extra class was added in the middle of the training. We followed a specific selection process with filtering to focus on difficult segments of unknown speakers. If the target speaker was among the 10-best predictions of the first-stage model, we discarded a corresponding segment, as it may contain a target speaker‚Äôs speech (and it would be misleading when considered as belonging to the unknown class). Out of the remaining segments that do not correspond to the target speaker, we selected 5% with the highest LSE of logits not to overweigh examples of a newly added class.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusions</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we advanced our previous work on training speaker verification embedding extractor using only recording-level speaker labels. By reimplementing our pipeline in PyTorch employing the WeSpeaker toolkit and optimizing hyperparameters, we achieved significant performance gains compared to our previous ResNet results, outperforming fully-supervised models trained with the same number of speakers. We further enhanced our results by exploiting SSL models, employing WavLM along with a MHFA backend, which also allowed us to decrease training time.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We demonstrated that our embedding extractor‚Äôs segment selection performance closely matched an oracle model. Additionally, varying the quality of segments employed for training the embedding extractor led to minimal degradation, showing the robustness of our method. These findings confirm the validity of our approach, evidencing the benefits of exploiting the whole audio recording from the source of VoxCeleb.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
D.¬†Snyder, D.¬†Garcia-Romero, G.¬†Sell, D.¬†Povey, and S.¬†Khudanpur, ‚ÄúX-vectors: Robust dnn embeddings for speaker recognition,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2018, pp. 5329‚Äì5333.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
B.¬†Desplanques, J.¬†Thienpondt, and K.¬†Demuynck, ‚ÄúEcapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Interspeech 2020</em>, 2020, pp. 3830‚Äì3834.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J.¬†S. Chung, A.¬†Nagrani, and A.¬†Zisserman, ‚ÄúVoxceleb2: Deep speaker recognition,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Interspeech 2018</em>, 2018, pp. 1086‚Äì1090.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S.¬†Chen, C.¬†Wang, Z.¬†Chen, Y.¬†Wu, S.¬†Liu, Z.¬†Chen, J.¬†Li, N.¬†Kanda, T.¬†Yoshioka, X.¬†Xiao, J.¬†Wu, L.¬†Zhou, S.¬†Ren, Y.¬†Qian, Y.¬†Qian, M.¬†Zeng, X.¬†Yu, and F.¬†Wei, ‚ÄúWavlm: Large-scale self-supervised pre-training for full stack speech processing,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">IEEE Journal of Selected Topics in Signal Processing</em>, vol.¬†16, pp. 1‚Äì14, 10 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
W.-N. Hsu, B.¬†Bolte, Y.-H.¬†H. Tsai, K.¬†Lakhotia, R.¬†Salakhutdinov, and A.¬†Mohamed, ‚ÄúHubert: Self-supervised speech representation learning by masked prediction of hidden units,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.¬†29, pp. 3451‚Äì3460, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A.¬†Baevski, Y.¬†Zhou, A.¬†Mohamed, and M.¬†Auli, ‚Äúwav2vec 2.0: A framework for self-supervised learning of speech representations,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Advances in Neural Information Processing Systems</em>, H.¬†Larochelle, M.¬†Ranzato, R.¬†Hadsell, M.¬†Balcan, and H.¬†Lin, Eds., vol.¬†33.¬†¬†¬†Curran Associates, Inc., 2020, pp. 12‚Äâ449‚Äì12‚Äâ460.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J.¬†Peng, O.¬†Plchot, T.¬†Stafylakis, L.¬†Mo≈°ner, L.¬†Burget, and J.¬†ƒåernock√Ω, ‚ÄúAn attention-based backend allowing efficient fine-tuning of transformer models for speaker verification,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">2022 IEEE Spoken Language Technology Workshop (SLT)</em>, 2023, pp. 555‚Äì562.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A.¬†Nagrani, J.¬†S. Chung, and A.¬†Zisserman, ‚ÄúVoxceleb: A large-scale speaker identification dataset,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Interspeech 2017</em>, 2017, pp. 2616‚Äì2620.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J.¬†S. Chung, A.¬†Nagrani, and A.¬†Zisserman, ‚ÄúVoxceleb2: Deep speaker recognition,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Interspeech 2018</em>, 2018, pp. 1086‚Äì1090.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
T.¬†Stafylakis, L.¬†Mosner, O.¬†Plchot, J.¬†Rohdin, A.¬†Silnova, L.¬†Burget, and J.¬†ƒåernock√Ω, ‚ÄúTraining speaker embedding extractors using multi-speaker audio with unknown speaker boundaries,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Interspeech 2022</em>, 2022, pp. 605‚Äì609.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
P.-A. Broux, F.¬†Desnous, A.¬†Larcher, S.¬†Petitrenaud, J.¬†Carrive, and S.¬†Meignier, ‚ÄúS4D: Speaker diarization toolkit in python,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proc. Interspeech 2018</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
H.¬†Wang, C.¬†Liang, S.¬†Wang, Z.¬†Chen, B.¬†Zhang, X.¬†Xiang, Y.¬†Deng, and Y.¬†Qian, ‚ÄúWespeaker: A research and production oriented speaker embedding learning toolkit,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.¬†¬†¬†IEEE, 2023, pp. 1‚Äì5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
S.¬†Wang, Z.¬†Chen, B.¬†Han, H.¬†Wang, C.¬†Liang, B.¬†Zhang, X.¬†Xiang, W.¬†Ding, J.¬†Rohdin, A.¬†Silnova <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">et¬†al.</em>, ‚ÄúAdvancing speaker embedding learning: Wespeaker toolkit for research and production,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib13.2.2">Speech Communication</em>, vol. 162, p. 103104, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
T.¬†G. Dietterich, R.¬†H. Lathrop, and T.¬†Lozano-P√©rez, ‚ÄúSolving the multiple instance problem with axis-parallel rectangles,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Artificial Intelligence</em>, vol.¬†89, no.¬†1, pp. 31‚Äì71, 1997. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.sciencedirect.com/science/article/pii/S0004370296000343</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J.¬†Deng, J.¬†Guo, N.¬†Xue, and S.¬†Zafeiriou, ‚ÄúArcface: Additive angular margin loss for deep face recognition,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019, pp. 4685‚Äì4694.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
P.-A. Broux, F.¬†Desnous, A.¬†Larcher, S.¬†Petitrenaud, J.¬†Carrive, and S.¬†Meignier, ‚ÄúS4d: Speaker diarization toolkit in python,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Interspeech 2018</em>, 2018, pp. 1368‚Äì1372.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
H.¬†Bredin, ‚Äúpyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proc. INTERSPEECH 2023</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
A.¬†Plaquet and H.¬†Bredin, ‚ÄúPowerset multi-class cross entropy loss for neural speaker diarization,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proc. INTERSPEECH 2023</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
K.¬†He, X.¬†Zhang, S.¬†Ren, and J.¬†Sun, ‚ÄúIdentity mappings in deep residual networks,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Computer Vision ‚Äì EECV 2016</em>, September 2016, pp. 630‚Äì645.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
D.¬†Ulyanov, A.¬†Vedaldi, and V.¬†Lempitsky, ‚ÄúInstance normalization: The missing ingredient for fast stylization,‚Äù 2017, arXiv 1607.08022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
S.¬†wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I.¬†J. Lai, K.¬†Lakhotia, Y.¬†Y. Lin, A.¬†T. Liu, J.¬†Shi, X.¬†Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K.¬†tik Lee, D.-R. Liu, Z.¬†Huang, S.¬†Dong, S.-W. Li, S.¬†Watanabe, A.¬†Mohamed, and H.¬†yi¬†Lee, ‚ÄúSUPERB: Speech Processing Universal PERformance Benchmark,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Interspeech 2021</em>, 2021, pp. 1194‚Äì1198.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct  3 10:15:37 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
