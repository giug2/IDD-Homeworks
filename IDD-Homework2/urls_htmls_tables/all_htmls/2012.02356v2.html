<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2012.02356] WeaQA: Weak Supervision via Captions for Visual Question Answering</title><meta property="og:description" content="Methodologies for training visual question answering (VQA) models assume the availability of datasets with human-annotated Image-Question-Answer (I-Q-A) triplets.
This has led to heavy reliance on datasets and a lack o…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="WeaQA: Weak Supervision via Captions for Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="WeaQA: Weak Supervision via Captions for Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2012.02356">

<!--Generated on Thu Mar  7 23:37:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
WeaQA: Weak Supervision via Captions for Visual Question Answering
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Pratyay Banerjee  Tejas Gokhale  Yezhou Yang  Chitta Baral 
<br class="ltx_break">Arizona State University 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">pbanerj6, tgokhale, yz.yang, chitta</span>@asu.edu 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Methodologies for training visual question answering (VQA) models assume the availability of datasets with human-annotated <span id="id2.id1.1" class="ltx_text ltx_font_italic">Image-Question-Answer</span> (I-Q-A) triplets.
This has led to heavy reliance on datasets and a lack of generalization to new types of questions and scenes.
Linguistic priors along with biases and errors due to annotator subjectivity have been shown to percolate into VQA models trained on such samples.
We study whether models can be trained without any human-annotated Q-A pairs, but only with images and their associated textual descriptions or captions.
We present a method to train models with synthetic Q-A pairs generated procedurally from captions.
Additionally, we demonstrate the efficacy of spatial-pyramid image patches as a simple but effective alternative to dense and costly object bounding box annotations used in existing VQA models.
Our experiments on three VQA benchmarks demonstrate the efficacy of this weakly-supervised approach, especially on the VQA-CP challenge, which tests performance under changing linguistic priors.
</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Since Visual Question Answering (VQA) was first proposed as a Turing test <cite class="ltx_cite ltx_citemacro_cite">Malinowski and Fritz (<a href="#bib.bib50" title="" class="ltx_ref">2014</a>)</cite>, several human-annotated datasets <cite class="ltx_cite ltx_citemacro_cite">Mogadala et al. (<a href="#bib.bib52" title="" class="ltx_ref">2019</a>)</cite>
have been used to train and evaluate VQA models.
Unfortunately, heavy reliance on these datasets for training has the unwanted side-effects of bias towards answer styles, question-types <cite class="ltx_cite ltx_citemacro_cite">Chao et al. (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>, and spurious correlations with language priors <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite>.
Similar findings have been reported for natural language tasks <cite class="ltx_cite ltx_citemacro_cite">Gururangan et al. (<a href="#bib.bib25" title="" class="ltx_ref">2018</a>); Niven and Kao (<a href="#bib.bib53" title="" class="ltx_ref">2019</a>); Kaushik et al. (<a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite>.
Evaluating VQA models on test-sets that are very similar to training sets is deceptive and inadequate and not an accurate measure of robustness.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To address this, one line of work has focused on balancing, de-biasing, and diversifying samples <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib23" title="" class="ltx_ref">2017</a>); Zhang et al. (<a href="#bib.bib90" title="" class="ltx_ref">2016</a>)</cite>.
However, crowd-sourcing “unbiased” labels is difficult and costly; it requires a well-designed annotation interface and a large-scale annotation effort with dedicated and able annotators <cite class="ltx_cite ltx_citemacro_cite">Sakaguchi et al. (<a href="#bib.bib65" title="" class="ltx_ref">2020</a>)</cite>.
The alternative (that this paper aligns itself with) is to avoid the use of explicit human annotations and instead to train models in an unsupervised manner by synthesizing training data.
These techniques, coined <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">unsupervised<span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span id="footnotex1.1.1.1" class="ltx_text ltx_font_upright">1</span></span></span></span></span></span>,
come with many advantages – human bias and subjectivity are reduced; the techniques are largely domain-agnostic and can be transferred from one language to another (low resource languages) or from one visual domain to another.
For instance, template-based Q-A generation developed for synthetic blocks-world images in CLEVR <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a href="#bib.bib34" title="" class="ltx_ref">2017</a>)</cite> can also be used to generate Q-A pairs for natural complex scenes in GQA <cite class="ltx_cite ltx_citemacro_cite">Hudson and Manning (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite> or the referring-expressions task <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib48" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we train VQA models without using human-annotated Q-A pairs.
Instead, we rely on weak supervision from image-captioning datasets, which provide multi-perspective, concise, and less subjective descriptions of visible objects in an image.
We procedurally generate Q-A pairs from these captions and train models using this synthetic data, and <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">only evaluate</span> them on established human-annotated VQA benchmarks.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p"><span id="S1.p4.1.1" class="ltx_text ltx_font_bold">Why Captions?</span> Image captioning, like VQA, has been a central area of vision-and-language research.
Datasets such as MS-COCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib47" title="" class="ltx_ref">2014</a>); Chen et al. (<a href="#bib.bib11" title="" class="ltx_ref">2015</a>)</cite> contain captions that describe objects and actions in images of everyday scenes.
During the construction of MS-COCO, human captioners were instructed to refrain from describing past and future events or “what a person might say”.
On the other hand, annotators of VQA <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite> were instructed to ask questions that <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">“a smart robot cannot answer, but a human can”</span> and “interesting” questions that may require “commonsense”.
Different sets of annotators provided answers to these questions and were allowed to speculate or even guess an answer that <span id="S1.p4.1.3" class="ltx_text ltx_font_italic">most people would agree on</span>.
It has also been shown that multiple answers may exist for questions in common VQA datasets <cite class="ltx_cite ltx_citemacro_cite">Bhattacharya et al. (<a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In Figure <a href="#S2.F2" title="Figure 2 ‣ 2 Related Work ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the first VQA-v2 question asks how many doors the car has.
Although commonsense (and linguistic priors) would suggest that “Most cars have <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">four</span> doors”, only two doors can be seen in the image. What should the model predict, <span id="S1.p5.1.2" class="ltx_text ltx_font_italic">two</span> or <span id="S1.p5.1.3" class="ltx_text ltx_font_italic">four</span>?
The second question is subjective and has multiple contradicting answers from different annotators (where one should draw the line between opaque, transparent, or reflective is not very clear).
Similarly, the first GQA question is ambiguous and could refer to either the skier or the photographer.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Thus the very nature of the data-collection procedure and instructions for VQA brings in human subjectivity and linguistic bias as compared to caption annotations, which are designed to be simple, precise, and non-speculative. Motivated by this, we study the benefits of using captions to synthesize Q-A pairs, using three types of methods:
</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">template-based methods similar to <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib60" title="" class="ltx_ref">2015a</a>); Gokhale et al. (<a href="#bib.bib22" title="" class="ltx_ref">2020b</a>)</cite>,</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">paraphrasing and back-translation <cite class="ltx_cite ltx_citemacro_cite">Sennrich et al. (<a href="#bib.bib68" title="" class="ltx_ref">2016</a>)</cite> which provide linguistic variation,</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">synthesis of questions about image semantics using the QA-SRL <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib27" title="" class="ltx_ref">2015</a>)</cite> approach.</p>
</div>
</li>
</ol>
<p id="S1.p6.2" class="ltx_p">Since our Q-A pairs are created synthetically, there does exist a domain shift as well as label (answer) shift from evaluation datasets such as VQA-v2 and GQA as shown in Figure <a href="#S2.F2" title="Figure 2 ‣ 2 Related Work ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, thus posing challenges to this weakly-supervised method.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">We evaluate two models, UpDown <cite class="ltx_cite ltx_citemacro_cite">Anderson et al. (<a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite> and a transformer-encoder <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a href="#bib.bib81" title="" class="ltx_ref">2017</a>)</cite> based model pre-trained on synthetic Q-A pairs and image-caption matching task.
To remove the dependence on object bounding-boxes and labels needed to extract object features,
we propose spatial pyramids of image patches as a simple and effective alternative.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">To the best of our knowledge, this is the first work on the unsupervised<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>adhering to the usage of this term in <cite class="ltx_cite ltx_citemacro_citet">Lewis et al. (<a href="#bib.bib43" title="" class="ltx_ref">2019a</a>)</cite>.</span></span></span> visual question answering, with the following contributions:</p>
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p">We introduce a framework for synthesizing <span id="S1.I2.i1.p1.1.1" class="ltx_text ltx_font_italic">(Question, Answer)</span> pairs from captions.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p">Since synthetic samples (unlike popular benchmarks) include multi-word answer phrases, we propose a sub-phrase weighted-answer loss to mitigate bias towards such multi-word answers.</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p">We propose pre-training tasks that use spatial pyramids of image-patches instead of object bounding-boxes, further removing the dependence on human annotations.</p>
</div>
</li>
<li id="S1.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i4.p1" class="ltx_para">
<p id="S1.I2.i4.p1.1" class="ltx_p">Extensive experiments and analyses under zero-shot transfer and fully-supervised settings on VQA-v2, VQA-CP, and GQA show our model’s efficacy and establish a strong baseline for future work on unsupervised visual question answering.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2012.02356/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="155" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Aspects of generalization in VQA.
</figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.6" class="ltx_p"><span id="S2.p1.6.1" class="ltx_text ltx_font_bold">Robustness in VQA</span>
can be defined as shown in Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Related Work ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> under two situations: domain shift and label shift.
Under domain shift, generalization to a new input domain (such as different styles of questions or novel scenes) is desired, characterized by <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="S\cap T\neq T" display="inline"><semantics id="S2.p1.1.m1.1a"><mrow id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><mrow id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml"><mi id="S2.p1.1.m1.1.1.2.2" xref="S2.p1.1.m1.1.1.2.2.cmml">S</mi><mo id="S2.p1.1.m1.1.1.2.1" xref="S2.p1.1.m1.1.1.2.1.cmml">∩</mo><mi id="S2.p1.1.m1.1.1.2.3" xref="S2.p1.1.m1.1.1.2.3.cmml">T</mi></mrow><mo id="S2.p1.1.m1.1.1.1" xref="S2.p1.1.m1.1.1.1.cmml">≠</mo><mi id="S2.p1.1.m1.1.1.3" xref="S2.p1.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><neq id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1.1"></neq><apply id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2"><intersect id="S2.p1.1.m1.1.1.2.1.cmml" xref="S2.p1.1.m1.1.1.2.1"></intersect><ci id="S2.p1.1.m1.1.1.2.2.cmml" xref="S2.p1.1.m1.1.1.2.2">𝑆</ci><ci id="S2.p1.1.m1.1.1.2.3.cmml" xref="S2.p1.1.m1.1.1.2.3">𝑇</ci></apply><ci id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">S\cap T\neq T</annotation></semantics></math> where <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.p1.2.m2.1a"><mi id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">S</annotation></semantics></math> and <math id="S2.p1.3.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.p1.3.m3.1a"><mi id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">T</annotation></semantics></math> denote the train and test input domains.
Under label shift, generalization to novel answers is desired (predicting answers not seen during training), characterized by <math id="S2.p1.4.m4.1" class="ltx_Math" alttext="A_{S}\cap A_{T}\neq A_{T}" display="inline"><semantics id="S2.p1.4.m4.1a"><mrow id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml"><mrow id="S2.p1.4.m4.1.1.2" xref="S2.p1.4.m4.1.1.2.cmml"><msub id="S2.p1.4.m4.1.1.2.2" xref="S2.p1.4.m4.1.1.2.2.cmml"><mi id="S2.p1.4.m4.1.1.2.2.2" xref="S2.p1.4.m4.1.1.2.2.2.cmml">A</mi><mi id="S2.p1.4.m4.1.1.2.2.3" xref="S2.p1.4.m4.1.1.2.2.3.cmml">S</mi></msub><mo id="S2.p1.4.m4.1.1.2.1" xref="S2.p1.4.m4.1.1.2.1.cmml">∩</mo><msub id="S2.p1.4.m4.1.1.2.3" xref="S2.p1.4.m4.1.1.2.3.cmml"><mi id="S2.p1.4.m4.1.1.2.3.2" xref="S2.p1.4.m4.1.1.2.3.2.cmml">A</mi><mi id="S2.p1.4.m4.1.1.2.3.3" xref="S2.p1.4.m4.1.1.2.3.3.cmml">T</mi></msub></mrow><mo id="S2.p1.4.m4.1.1.1" xref="S2.p1.4.m4.1.1.1.cmml">≠</mo><msub id="S2.p1.4.m4.1.1.3" xref="S2.p1.4.m4.1.1.3.cmml"><mi id="S2.p1.4.m4.1.1.3.2" xref="S2.p1.4.m4.1.1.3.2.cmml">A</mi><mi id="S2.p1.4.m4.1.1.3.3" xref="S2.p1.4.m4.1.1.3.3.cmml">T</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><apply id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1"><neq id="S2.p1.4.m4.1.1.1.cmml" xref="S2.p1.4.m4.1.1.1"></neq><apply id="S2.p1.4.m4.1.1.2.cmml" xref="S2.p1.4.m4.1.1.2"><intersect id="S2.p1.4.m4.1.1.2.1.cmml" xref="S2.p1.4.m4.1.1.2.1"></intersect><apply id="S2.p1.4.m4.1.1.2.2.cmml" xref="S2.p1.4.m4.1.1.2.2"><csymbol cd="ambiguous" id="S2.p1.4.m4.1.1.2.2.1.cmml" xref="S2.p1.4.m4.1.1.2.2">subscript</csymbol><ci id="S2.p1.4.m4.1.1.2.2.2.cmml" xref="S2.p1.4.m4.1.1.2.2.2">𝐴</ci><ci id="S2.p1.4.m4.1.1.2.2.3.cmml" xref="S2.p1.4.m4.1.1.2.2.3">𝑆</ci></apply><apply id="S2.p1.4.m4.1.1.2.3.cmml" xref="S2.p1.4.m4.1.1.2.3"><csymbol cd="ambiguous" id="S2.p1.4.m4.1.1.2.3.1.cmml" xref="S2.p1.4.m4.1.1.2.3">subscript</csymbol><ci id="S2.p1.4.m4.1.1.2.3.2.cmml" xref="S2.p1.4.m4.1.1.2.3.2">𝐴</ci><ci id="S2.p1.4.m4.1.1.2.3.3.cmml" xref="S2.p1.4.m4.1.1.2.3.3">𝑇</ci></apply></apply><apply id="S2.p1.4.m4.1.1.3.cmml" xref="S2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S2.p1.4.m4.1.1.3.1.cmml" xref="S2.p1.4.m4.1.1.3">subscript</csymbol><ci id="S2.p1.4.m4.1.1.3.2.cmml" xref="S2.p1.4.m4.1.1.3.2">𝐴</ci><ci id="S2.p1.4.m4.1.1.3.3.cmml" xref="S2.p1.4.m4.1.1.3.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">A_{S}\cap A_{T}\neq A_{T}</annotation></semantics></math>, where <math id="S2.p1.5.m5.1" class="ltx_Math" alttext="A_{S}" display="inline"><semantics id="S2.p1.5.m5.1a"><msub id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml"><mi id="S2.p1.5.m5.1.1.2" xref="S2.p1.5.m5.1.1.2.cmml">A</mi><mi id="S2.p1.5.m5.1.1.3" xref="S2.p1.5.m5.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><apply id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.p1.5.m5.1.1.1.cmml" xref="S2.p1.5.m5.1.1">subscript</csymbol><ci id="S2.p1.5.m5.1.1.2.cmml" xref="S2.p1.5.m5.1.1.2">𝐴</ci><ci id="S2.p1.5.m5.1.1.3.cmml" xref="S2.p1.5.m5.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">A_{S}</annotation></semantics></math> and <math id="S2.p1.6.m6.1" class="ltx_Math" alttext="A_{T}" display="inline"><semantics id="S2.p1.6.m6.1a"><msub id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml"><mi id="S2.p1.6.m6.1.1.2" xref="S2.p1.6.m6.1.1.2.cmml">A</mi><mi id="S2.p1.6.m6.1.1.3" xref="S2.p1.6.m6.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.1b"><apply id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.p1.6.m6.1.1.1.cmml" xref="S2.p1.6.m6.1.1">subscript</csymbol><ci id="S2.p1.6.m6.1.1.2.cmml" xref="S2.p1.6.m6.1.1.2">𝐴</ci><ci id="S2.p1.6.m6.1.1.3.cmml" xref="S2.p1.6.m6.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m6.1c">A_{T}</annotation></semantics></math> are the set of answers seen during training and test-time.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2012.02356/assets/images/synth_data.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="177" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples of images and human-annotated Q-A pairs from VQA and GQA and our synthetic Q-A pairs.</figcaption>
</figure>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Performance under <span id="S2.p2.1.1" class="ltx_text ltx_font_bold">domain shift</span> has been evaluated for new domains of test questions with unseen words and objects <cite class="ltx_cite ltx_citemacro_cite">Teney and Hengel (<a href="#bib.bib78" title="" class="ltx_ref">2016</a>); Ramakrishnan et al. (<a href="#bib.bib58" title="" class="ltx_ref">2017</a>)</cite>,
novel compositions <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a href="#bib.bib34" title="" class="ltx_ref">2017</a>); Agrawal et al. (<a href="#bib.bib2" title="" class="ltx_ref">2017</a>)</cite>,
logical connectives <cite class="ltx_cite ltx_citemacro_cite">Gokhale et al. (<a href="#bib.bib22" title="" class="ltx_ref">2020b</a>)</cite>, as well as questions that are
implied <cite class="ltx_cite ltx_citemacro_cite">Ribeiro et al. (<a href="#bib.bib63" title="" class="ltx_ref">2019</a>)</cite>, entailed <cite class="ltx_cite ltx_citemacro_cite">Ray et al. (<a href="#bib.bib59" title="" class="ltx_ref">2019</a>)</cite> or sub-questions <cite class="ltx_cite ltx_citemacro_cite">Selvaraju et al. (<a href="#bib.bib67" title="" class="ltx_ref">2020</a>)</cite>; or for datasets with varying linguistic styles <cite class="ltx_cite ltx_citemacro_cite">Chao et al. (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>); Xu et al. (<a href="#bib.bib86" title="" class="ltx_ref">2020</a>); Shrestha et al. (<a href="#bib.bib70" title="" class="ltx_ref">2019</a>)</cite>
and different reasoning capabilities <cite class="ltx_cite ltx_citemacro_cite">Kafle and Kanan (<a href="#bib.bib35" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Label shift</span> or Prior Probability Shift <cite class="ltx_cite ltx_citemacro_cite">Storkey (<a href="#bib.bib73" title="" class="ltx_ref">2009</a>)</cite> has been implicitly explored in VQA-CP <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite>, where the conditional probabilities of answers given the question type deviate at test-time.
<cite class="ltx_cite ltx_citemacro_citet">Teney et al. (<a href="#bib.bib79" title="" class="ltx_ref">2020c</a>)</cite> have identified several pitfalls associated with the models and evaluation criteria for VQA-CP.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Unsupervised Extractive QA</span>
in which aligned <span id="S2.p4.1.2" class="ltx_text ltx_font_italic">(context, question, answer)</span> triplets are not available, has been studied <cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a href="#bib.bib44" title="" class="ltx_ref">2019b</a>); Banerjee and Baral (<a href="#bib.bib5" title="" class="ltx_ref">2020</a>); Rennie et al. (<a href="#bib.bib62" title="" class="ltx_ref">2020</a>); Fabbri et al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>); Li et al. (<a href="#bib.bib46" title="" class="ltx_ref">2020</a>); Banerjee et al. (<a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite> by training models on procedurally generated Q-A pairs.
Captions have been used to generate Q-A pairs for logical understanding <cite class="ltx_cite ltx_citemacro_cite">Gokhale et al. (<a href="#bib.bib22" title="" class="ltx_ref">2020b</a>)</cite> and commonsense video understanding <cite class="ltx_cite ltx_citemacro_cite">Fang et al. (<a href="#bib.bib17" title="" class="ltx_ref">2020a</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib45" title="" class="ltx_ref">2018</a>); Krishna et al. (<a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite> have explored Visual Question Generation from an input image and answer.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">Weak supervision</span> is an active area of research; for instance in action/object localization <cite class="ltx_cite ltx_citemacro_cite">Song et al. (<a href="#bib.bib72" title="" class="ltx_ref">2014</a>); Zhou et al. (<a href="#bib.bib91" title="" class="ltx_ref">2016</a>)</cite> and semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">Khoreva et al. (<a href="#bib.bib37" title="" class="ltx_ref">2017</a>); Zhang et al. (<a href="#bib.bib89" title="" class="ltx_ref">2017</a>)</cite> without pixel-level annotations, but only class labels.
There is also interest growing in leveraging natural language captions or textual queries as weak supervision for visual grounding tasks <cite class="ltx_cite ltx_citemacro_cite">Hendricks et al. (<a href="#bib.bib29" title="" class="ltx_ref">2017</a>); Mithun et al. (<a href="#bib.bib51" title="" class="ltx_ref">2019</a>); Fang et al. (<a href="#bib.bib18" title="" class="ltx_ref">2020b</a>)</cite>.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p"><span id="S2.p6.1.1" class="ltx_text ltx_font_bold">Visual Feature Extractors</span> such as
VGG <cite class="ltx_cite ltx_citemacro_cite">Simonyan and Zisserman (<a href="#bib.bib71" title="" class="ltx_ref">2015</a>)</cite> and
ResNet <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib26" title="" class="ltx_ref">2016</a>)</cite> have been widely used for many computer vision tasks.
Object-based features such as RCNN <cite class="ltx_cite ltx_citemacro_cite">Girshick et al. (<a href="#bib.bib20" title="" class="ltx_ref">2014</a>)</cite> and Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib61" title="" class="ltx_ref">2015b</a>)</cite>
have become the standard for V&amp; L tasks <cite class="ltx_cite ltx_citemacro_cite">Anderson et al. (<a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Framework for Synthesizing Q-A Pairs</h2>

<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.1" class="ltx_figure ltx_figure_panel"><img src="/html/2012.02356/assets/images/all-pretrain-overlap.png" id="S3.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="106" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F3.fig1" class="ltx_figure ltx_figure_panel">
<div id="S3.F3.fig1.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:114.1pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-72.8pt,19.0pt) scale(0.748711734421338,0.748711734421338) ;">
<table id="S3.F3.fig1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.F3.fig1.1.1.1.1" class="ltx_tr">
<td id="S3.F3.fig1.1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S3.F3.fig1.1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F3.fig1.1.1.1.1.1.1.1" class="ltx_p" style="width:99.6pt;"></span>
</span>
</td>
<th id="S3.F3.fig1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S3.F3.fig1.1.1.1.1.2.1" class="ltx_text"></span> <span id="S3.F3.fig1.1.1.1.1.2.2" class="ltx_text">
<span id="S3.F3.fig1.1.1.1.1.2.2.1" class="ltx_tabular ltx_align_bottom">
<span id="S3.F3.fig1.1.1.1.1.2.2.1.1" class="ltx_tr">
<span id="S3.F3.fig1.1.1.1.1.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.F3.fig1.1.1.1.1.2.2.1.1.1.1" class="ltx_text ltx_font_bold">Template-based</span></span></span>
</span></span><span id="S3.F3.fig1.1.1.1.1.2.3" class="ltx_text"></span></th>
<th id="S3.F3.fig1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S3.F3.fig1.1.1.1.1.3.1" class="ltx_text"></span> <span id="S3.F3.fig1.1.1.1.1.3.2" class="ltx_text">
<span id="S3.F3.fig1.1.1.1.1.3.2.1" class="ltx_tabular ltx_align_bottom">
<span id="S3.F3.fig1.1.1.1.1.3.2.1.1" class="ltx_tr">
<span id="S3.F3.fig1.1.1.1.1.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.F3.fig1.1.1.1.1.3.2.1.1.1.1" class="ltx_text ltx_font_bold">Paraphrase &amp;</span></span></span>
<span id="S3.F3.fig1.1.1.1.1.3.2.1.2" class="ltx_tr">
<span id="S3.F3.fig1.1.1.1.1.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.F3.fig1.1.1.1.1.3.2.1.2.1.1" class="ltx_text ltx_font_bold">Back-translate</span></span></span>
</span></span><span id="S3.F3.fig1.1.1.1.1.3.3" class="ltx_text"></span></th>
<th id="S3.F3.fig1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S3.F3.fig1.1.1.1.1.4.1" class="ltx_text"></span> <span id="S3.F3.fig1.1.1.1.1.4.2" class="ltx_text">
<span id="S3.F3.fig1.1.1.1.1.4.2.1" class="ltx_tabular ltx_align_bottom">
<span id="S3.F3.fig1.1.1.1.1.4.2.1.1" class="ltx_tr">
<span id="S3.F3.fig1.1.1.1.1.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.F3.fig1.1.1.1.1.4.2.1.1.1.1" class="ltx_text ltx_font_bold">QA-SRL</span></span></span>
</span></span><span id="S3.F3.fig1.1.1.1.1.4.3" class="ltx_text"></span></th>
<th id="S3.F3.fig1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S3.F3.fig1.1.1.1.1.5.1" class="ltx_text"></span> <span id="S3.F3.fig1.1.1.1.1.5.2" class="ltx_text">
<span id="S3.F3.fig1.1.1.1.1.5.2.1" class="ltx_tabular ltx_align_bottom">
<span id="S3.F3.fig1.1.1.1.1.5.2.1.1" class="ltx_tr">
<span id="S3.F3.fig1.1.1.1.1.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.F3.fig1.1.1.1.1.5.2.1.1.1.1" class="ltx_text ltx_font_bold">VQA-v2</span></span></span>
</span></span><span id="S3.F3.fig1.1.1.1.1.5.3" class="ltx_text"></span></th>
<th id="S3.F3.fig1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S3.F3.fig1.1.1.1.1.6.1" class="ltx_text"></span> <span id="S3.F3.fig1.1.1.1.1.6.2" class="ltx_text">
<span id="S3.F3.fig1.1.1.1.1.6.2.1" class="ltx_tabular ltx_align_bottom">
<span id="S3.F3.fig1.1.1.1.1.6.2.1.1" class="ltx_tr">
<span id="S3.F3.fig1.1.1.1.1.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.F3.fig1.1.1.1.1.6.2.1.1.1.1" class="ltx_text ltx_font_bold">GQA</span></span></span>
</span></span><span id="S3.F3.fig1.1.1.1.1.6.3" class="ltx_text"></span></th>
<th id="S3.F3.fig1.1.1.1.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S3.F3.fig1.1.1.1.1.7.1" class="ltx_text"></span> <span id="S3.F3.fig1.1.1.1.1.7.2" class="ltx_text">
<span id="S3.F3.fig1.1.1.1.1.7.2.1" class="ltx_tabular ltx_align_bottom">
<span id="S3.F3.fig1.1.1.1.1.7.2.1.1" class="ltx_tr">
<span id="S3.F3.fig1.1.1.1.1.7.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.F3.fig1.1.1.1.1.7.2.1.1.1.1" class="ltx_text ltx_font_bold">VQA-CP</span></span></span>
</span></span><span id="S3.F3.fig1.1.1.1.1.7.3" class="ltx_text"></span></th>
</tr>
<tr id="S3.F3.fig1.1.1.2.2" class="ltx_tr">
<td id="S3.F3.fig1.1.1.2.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.F3.fig1.1.1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F3.fig1.1.1.2.2.1.1.1" class="ltx_p" style="width:99.6pt;"># of Questions</span>
</span>
</td>
<td id="S3.F3.fig1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">600K</td>
<td id="S3.F3.fig1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">400K</td>
<td id="S3.F3.fig1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">2.5M</td>
<td id="S3.F3.fig1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">438K / 214K</td>
<td id="S3.F3.fig1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">943K / 132K</td>
<td id="S3.F3.fig1.1.1.2.2.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">245K / 220K</td>
</tr>
<tr id="S3.F3.fig1.1.1.3.3" class="ltx_tr">
<td id="S3.F3.fig1.1.1.3.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.F3.fig1.1.1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F3.fig1.1.1.3.3.1.1.1" class="ltx_p" style="width:99.6pt;"># of Answers</span>
</span>
</td>
<td id="S3.F3.fig1.1.1.3.3.2" class="ltx_td ltx_align_center">5K</td>
<td id="S3.F3.fig1.1.1.3.3.3" class="ltx_td ltx_align_center">5K</td>
<td id="S3.F3.fig1.1.1.3.3.4" class="ltx_td ltx_align_center">90K</td>
<td id="S3.F3.fig1.1.1.3.3.5" class="ltx_td ltx_align_center">3.5K</td>
<td id="S3.F3.fig1.1.1.3.3.6" class="ltx_td ltx_align_center">1878</td>
<td id="S3.F3.fig1.1.1.3.3.7" class="ltx_td ltx_nopad_r ltx_align_center">3.5K</td>
</tr>
<tr id="S3.F3.fig1.1.1.4.4" class="ltx_tr">
<td id="S3.F3.fig1.1.1.4.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.F3.fig1.1.1.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F3.fig1.1.1.4.4.1.1.1" class="ltx_p" style="width:99.6pt;">Mean Question Length</span>
</span>
</td>
<td id="S3.F3.fig1.1.1.4.4.2" class="ltx_td ltx_align_center">7.9</td>
<td id="S3.F3.fig1.1.1.4.4.3" class="ltx_td ltx_align_center">8.1</td>
<td id="S3.F3.fig1.1.1.4.4.4" class="ltx_td ltx_align_center">4.8</td>
<td id="S3.F3.fig1.1.1.4.4.5" class="ltx_td ltx_align_center">6.4</td>
<td id="S3.F3.fig1.1.1.4.4.6" class="ltx_td ltx_align_center">10.6</td>
<td id="S3.F3.fig1.1.1.4.4.7" class="ltx_td ltx_nopad_r ltx_align_center">6.4</td>
</tr>
<tr id="S3.F3.fig1.1.1.5.5" class="ltx_tr">
<td id="S3.F3.fig1.1.1.5.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.F3.fig1.1.1.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F3.fig1.1.1.5.5.1.1.1" class="ltx_p" style="width:99.6pt;">Mean Answer Length</span>
</span>
</td>
<td id="S3.F3.fig1.1.1.5.5.2" class="ltx_td ltx_align_center">1.4</td>
<td id="S3.F3.fig1.1.1.5.5.3" class="ltx_td ltx_align_center">1.4</td>
<td id="S3.F3.fig1.1.1.5.5.4" class="ltx_td ltx_align_center">6.3</td>
<td id="S3.F3.fig1.1.1.5.5.5" class="ltx_td ltx_align_center">1.1</td>
<td id="S3.F3.fig1.1.1.5.5.6" class="ltx_td ltx_align_center">1.3</td>
<td id="S3.F3.fig1.1.1.5.5.7" class="ltx_td ltx_nopad_r ltx_align_center">1.1</td>
</tr>
<tr id="S3.F3.fig1.1.1.6.6" class="ltx_tr">
<td id="S3.F3.fig1.1.1.6.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.F3.fig1.1.1.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F3.fig1.1.1.6.6.1.1.1" class="ltx_p" style="width:99.6pt;">Image Source</span>
</span>
</td>
<td id="S3.F3.fig1.1.1.6.6.2" class="ltx_td ltx_align_center">COCO</td>
<td id="S3.F3.fig1.1.1.6.6.3" class="ltx_td ltx_align_center">COCO</td>
<td id="S3.F3.fig1.1.1.6.6.4" class="ltx_td ltx_align_center">COCO</td>
<td id="S3.F3.fig1.1.1.6.6.5" class="ltx_td ltx_align_center">COCO</td>
<td id="S3.F3.fig1.1.1.6.6.6" class="ltx_td ltx_align_center">COCO,VG,Flickr</td>
<td id="S3.F3.fig1.1.1.6.6.7" class="ltx_td ltx_nopad_r ltx_align_center">COCO</td>
</tr>
<tr id="S3.F3.fig1.1.1.7.7" class="ltx_tr">
<td id="S3.F3.fig1.1.1.7.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.F3.fig1.1.1.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.F3.fig1.1.1.7.7.1.1.1" class="ltx_p" style="width:99.6pt;">Image Counts</span>
</span>
</td>
<td id="S3.F3.fig1.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_bb">120K</td>
<td id="S3.F3.fig1.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_bb">120K</td>
<td id="S3.F3.fig1.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_bb">120K</td>
<td id="S3.F3.fig1.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_bb">120K</td>
<td id="S3.F3.fig1.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_bb">113K</td>
<td id="S3.F3.fig1.1.1.7.7.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">120K</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Discrepancy between VQA-v2, GQA, and synthetic samples. Left: t-SNE plot of question
embeddings. Right: Dataset statistics for our generated Q-A pairs with Train/Val. splits for benchmark datasets.
</figcaption>
</figure>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Problem Statement:</h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">Consider a dataset containing images and associated captions
as shown in Figure <a href="#S2.F2" title="Figure 2 ‣ 2 Related Work ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
Our work deals with learning VQA using these image-caption data, without any labeled Q-A pairs, and answer questions about unseen images.</p>
</div>
</section>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Question Generation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Several studies <cite class="ltx_cite ltx_citemacro_cite">Du et al. (<a href="#bib.bib15" title="" class="ltx_ref">2017</a>); Lewis et al. (<a href="#bib.bib43" title="" class="ltx_ref">2019a</a>)</cite> have been dedicated to the complex domain of question generation.
We approach it conservatively, using template-based methods and semantic role labeling, with paraphrasing and back-translation for improving the linguistic diversity of template-based questions.
We begin by extracting object words from the caption by using simple heuristics such as extracting noun-phrases and using numerical quantifiers in the caption as soft approximations of objects’ cardinality.
If object-words are available explicitly, we used them as is.
Questions are categorized based on answer types; <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">Yes-No</span>, <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">Number</span>, <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_italic">Color</span>, <span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_italic">Location</span>, <span id="S3.SS1.p1.1.5" class="ltx_text ltx_font_italic">Object</span>, and <span id="S3.SS1.p1.1.6" class="ltx_text ltx_font_italic">Phrases</span>.</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Template-based:</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">To create <span id="S3.SS1.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">Yes-No</span> questions,
modal verbs are removed from the caption, and a randomly chosen question prefix such as <span id="S3.SS1.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">“is there”, “is this”</span> is attached.
For instance, the caption “A man is wearing a hat and sitting” is converted to “<span id="S3.SS1.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_italic">Is there a</span> man wearing a hat and sitting”, with the answer “Yes”.
To create the corresponding question with the answer “No”, we use either negation or replace the object-word with an adversarial word or antonym, thus obtaining “Is there a dog wearing a hat and sitting” for which the answer is “No”.
An adversarial word refers to an object absent in the image but similar to objects in the image.
To compute similarity, we use Glove <cite class="ltx_cite ltx_citemacro_cite">Pennington et al. (<a href="#bib.bib56" title="" class="ltx_ref">2014</a>)</cite> word-vectors.</p>
</div>
<div id="S3.SS1.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p2.1" class="ltx_p">For <span id="S3.SS1.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_italic">Object</span>, <span id="S3.SS1.SSS0.Px1.p2.1.2" class="ltx_text ltx_font_italic">Number</span>, <span id="S3.SS1.SSS0.Px1.p2.1.3" class="ltx_text ltx_font_italic">Location</span>, and <span id="S3.SS1.SSS0.Px1.p2.1.4" class="ltx_text ltx_font_italic">Color</span> questions, we follow a procedure similar to <cite class="ltx_cite ltx_citemacro_citet">Ren et al. (<a href="#bib.bib60" title="" class="ltx_ref">2015a</a>)</cite>.
To create “<span id="S3.SS1.SSS0.Px1.p2.1.5" class="ltx_text ltx_font_italic">what</span>” questions for the <span id="S3.SS1.SSS0.Px1.p2.1.6" class="ltx_text ltx_font_italic">Object</span> type, we extract objects and noun phrases from captions as potential answers and replace them with <span id="S3.SS1.SSS0.Px1.p2.1.7" class="ltx_text ltx_font_italic">what</span>.
The question is rephrased by splitting long sentences into shorter ones and converting indefinite determiners to definite.
A similar procedure is used for <span id="S3.SS1.SSS0.Px1.p2.1.8" class="ltx_text ltx_font_italic">Number</span> questions; numeric quantifiers of noun phrases are extracted and replaced by “how many” and “what is the count” to form the question.
<span id="S3.SS1.SSS0.Px1.p2.1.9" class="ltx_text ltx_font_italic">Color</span> questions are generated by locating the color adjective and the corresponding noun phrase and replacing them in a templated question: “What is the color of the object?”.
<span id="S3.SS1.SSS0.Px1.p2.1.10" class="ltx_text ltx_font_italic">Location</span> questions are similar to <span id="S3.SS1.SSS0.Px1.p2.1.11" class="ltx_text ltx_font_italic">Object</span> questions, but we extract phrases with “in”, “within” to extract locations, with places, scenes, and containers as answers.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Semantic Role Labeling:</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">QA-SRL <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib27" title="" class="ltx_ref">2015</a>)</cite> was proposed as a paradigm to use natural language to annotate data by using Q-A pairs to specify textual arguments and their roles.
Consider the caption <span id="S3.SS1.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">“A girl in a red shirt holding an apple sitting in an empty open field”</span>.
Using QA-SRL with B-I-O span detection and sequence-to-sequence models <cite class="ltx_cite ltx_citemacro_cite">FitzGerald et al. (<a href="#bib.bib19" title="" class="ltx_ref">2018</a>)</cite>, for the <span id="S3.SS1.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">“when”, “what”, “where”, and “who”</span> questions, we obtain Q-A pairs belonging to the <span id="S3.SS1.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_italic">Phrases</span> category such as:</p>
</div>
<div id="S3.SS1.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<div id="S3.SS1.SSS0.Px2.p2.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:33pt;vertical-align:-13.1pt;"><span class="ltx_transformed_inner" style="transform:translate(-6.7pt,0.3pt) scale(0.970168443221065,0.970168443221065) ;">
<p id="S3.SS1.SSS0.Px2.p2.1.1" class="ltx_p">

<span id="S3.SS1.SSS0.Px2.p2.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:433.6pt;">
<span id="S3.SS1.SSS0.Px2.p2.1.1.1.1" class="ltx_p"><span id="S3.SS1.SSS0.Px2.p2.1.1.1.1.1" class="ltx_text ltx_font_italic">(what is someone holding?,    an apple)
<br class="ltx_break">(who is sitting?,    girl in a red shirt holding an apple)
<br class="ltx_break">(where is someone sitting?,    an empty open field)</span></span>
</span>
</p>
</span></div>
</div>
<div id="S3.SS1.SSS0.Px2.p3" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p3.1" class="ltx_p">These examples illustrate that QA-SRL questions are short and use generic descriptors such as <span id="S3.SS1.SSS0.Px2.p3.1.1" class="ltx_text ltx_font_italic">something</span> and <span id="S3.SS1.SSS0.Px2.p3.1.2" class="ltx_text ltx_font_italic">someone</span> instead of elaborate references, while the expected answer phrases are longer and descriptive.
Thus to answer these, better semantic image understanding is required.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Paraphrasing and Back-Translation (P&amp;B):</h4>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">We apply two natural language data augmentation techniques, paraphrasing, and back-translation to increase the linguistic variation in the questions.
To paraphrase questions, we train a T5 <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a href="#bib.bib57" title="" class="ltx_ref">2019</a>)</cite> text generation model on the Quora Question Pairs Corpus <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib32" title="" class="ltx_ref">Iyer et al. </a></cite>.
For back-translation, we train another T5 text generation model on the Opus corpus <cite class="ltx_cite ltx_citemacro_cite">Tiedemann (<a href="#bib.bib80" title="" class="ltx_ref">2012</a>)</cite>, translate the question to an intermediate language (Français, Deutsche, or Español), and translate the question back to English.
For example:</p>
</div>
<div id="S3.SS1.SSS0.Px3.p2" class="ltx_para ltx_noindent">
<div id="S3.SS1.SSS0.Px3.p2.2" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:61.4pt;vertical-align:-27.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-6.7pt,0.5pt) scale(0.970168443221065,0.970168443221065) ;">
<p id="S3.SS1.SSS0.Px3.p2.2.2" class="ltx_p">

<span id="S3.SS1.SSS0.Px3.p2.2.2.2.2" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:433.6pt;">
<span id="S3.SS1.SSS0.Px3.p2.2.2.2.2.2" class="ltx_p"><span id="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2" class="ltx_text ltx_font_italic">Is the girl who is to the left of the sailboats wearing a backpack?
<br class="ltx_break"><math id="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\Big{\downarrow}{\text{Espa\~{n}ol}}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1a"><mrow id="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1.1" xref="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1.1.2.cmml"></mi><mo mathsize="160%" stretchy="false" id="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1.1.1" xref="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1.1.1.cmml">↓</mo><mtext id="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1.1.3a.cmml">Español</mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1b"><apply id="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1.1"><ci id="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1.1.1">↓</ci><csymbol cd="latexml" id="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1.1.2">absent</csymbol><ci id="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1.1.3a.cmml" xref="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1.1.3"><mtext id="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1.1.3">Español</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p2.1.1.1.1.1.1.m1.1c">\Big{\downarrow}{\text{Espa\~{n}ol}}</annotation></semantics></math>
<br class="ltx_break">La chica que está a la izquierda de los veleros lleva mochila?
<br class="ltx_break"><math id="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1" class="ltx_Math" alttext="\Big{\downarrow}{\text{English}}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1a"><mrow id="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1.1" xref="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1.1.2" xref="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1.1.2.cmml"></mi><mo mathsize="160%" stretchy="false" id="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1.1.1" xref="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1.1.1.cmml">↓</mo><mtext id="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1.1.3" xref="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1.1.3a.cmml">English</mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1b"><apply id="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1.1"><ci id="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1.1.1">↓</ci><csymbol cd="latexml" id="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1.1.2">absent</csymbol><ci id="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1.1.3a.cmml" xref="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1.1.3"><mtext id="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1.1.3">English</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p2.2.2.2.2.2.2.m2.1c">\Big{\downarrow}{\text{English}}</annotation></semantics></math>
<br class="ltx_break">Does the girl to the left of the sailboats carry a backpack?</span></span>
</span>
</p>
</span></div>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Domain Shift w.r.t. VQA-v2 and GQA</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.2" class="ltx_p">Compared to current VQA benchmarks (which typically contain one-word answers), answers to QA-SRL questions are more descriptive and contain adjectives, adverbs, determiners, and quantifiers, as seen in Figure <a href="#S2.F2" title="Figure 2 ‣ 2 Related Work ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
On the other hand, synthetic questions have less descriptive subjects due to the use of pronouns.
Our synthetic data contains <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="90k" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mn id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">90</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><times id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">90</cn><ci id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">90k</annotation></semantics></math> unique answer phrases, compared to <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="3.2k" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mn id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">3.2</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><times id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></times><cn type="float" id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">3.2</cn><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">3.2k</annotation></semantics></math> in VQA and 3k in GQA.
Around 200 answers from VQA are not present in our answer phrases, such as time (11:00) and proper nouns (LA Clippers), both of which are not present in caption descriptions.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Moreover, our training data contains Q-A pair such as <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_italic">(“Where is the man standing?, “to the left of the table”)</span>, generated by QA-SRL with long phrases as answers. However, the test set contains questions such as <span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_italic">(“Which side of the car is the tree?”, “left”)</span>, which expects only <span id="S3.SS2.p2.1.3" class="ltx_text ltx_font_italic">“left”</span> as the answer. So although the word <span id="S3.SS2.p2.1.4" class="ltx_text ltx_font_italic">“left”</span> is seen as a sub-phrase of our training answers, it is not explicitly seen as an only correct answer.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Some of our synthetic template-based questions about counting and object presence are similar in style to those in VQA and GQA.
However, QA-SRL questions require a semantic understanding of the actions depicted in the image, which are rare in VQA and GQA.
We quantify this by plotting the t-SNE components of document vector embeddings of the questions from VQA, GQA, and our synthetic data, in Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Framework for Synthesizing Q-A Pairs ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, and observe
that our synthetic questions are a distinct cluster, while VQA and GQA overlap with each other.
As such, a linguistic domain shift exists between these synthetic source questions and human-annotated target questions.
In this paper, we address the challenge of learning VQA on a synthetically generated dataset and evaluating models on conventional benchmarks which have questions and answers that deviate linguistically from synthetic training samples.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2012.02356/assets/x2.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="106" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Our model architecture makes the use of spatial pyramids of image patches as inputs to the Encoder, which is trained for three pre-training tasks as shown.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Method</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.2" class="ltx_p">Recently, multiple deep transformer-based architectures have been proposed <cite class="ltx_cite ltx_citemacro_cite">Tan and Bansal (<a href="#bib.bib74" title="" class="ltx_ref">2019</a>); Lu et al. (<a href="#bib.bib49" title="" class="ltx_ref">2019</a>); Chen et al. (<a href="#bib.bib12" title="" class="ltx_ref">2019</a>)</cite>, that are pretrained on a combination of multiple VQA and image captioning datasets such as Conceptual Captions <cite class="ltx_cite ltx_citemacro_cite">Sharma et al. (<a href="#bib.bib69" title="" class="ltx_ref">2018</a>)</cite>, SBU Captions <cite class="ltx_cite ltx_citemacro_cite">Ordonez et al. (<a href="#bib.bib54" title="" class="ltx_ref">2011</a>)</cite>, Visual Genome <cite class="ltx_cite ltx_citemacro_cite">Krishna et al. (<a href="#bib.bib40" title="" class="ltx_ref">2017</a>)</cite>, and MSCOCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib47" title="" class="ltx_ref">2014</a>)</cite>.
These models are resource intensive as they are trained on a huge collection of data with <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="~{}3" display="inline"><semantics id="S4.p1.1.m1.1a"><mn id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><cn type="integer" id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">~{}3</annotation></semantics></math> million images.
We train our models only on MS-COCO captions and images (<math id="S4.p1.2.m2.1" class="ltx_Math" alttext="{\sim}204" display="inline"><semantics id="S4.p1.2.m2.1a"><mrow id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml"><mi id="S4.p1.2.m2.1.1.2" xref="S4.p1.2.m2.1.1.2.cmml"></mi><mo id="S4.p1.2.m2.1.1.1" xref="S4.p1.2.m2.1.1.1.cmml">∼</mo><mn id="S4.p1.2.m2.1.1.3" xref="S4.p1.2.m2.1.1.3.cmml">204</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><apply id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.p1.2.m2.1.1.1.cmml" xref="S4.p1.2.m2.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.p1.2.m2.1.1.2.cmml" xref="S4.p1.2.m2.1.1.2">absent</csymbol><cn type="integer" id="S4.p1.2.m2.1.1.3.cmml" xref="S4.p1.2.m2.1.1.3">204</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">{\sim}204</annotation></semantics></math>k), without access to any human-authored Q-A pairs or object bounding boxes.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Spatial Pyramid Patches</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">“Bottom-Up” object features <cite class="ltx_cite ltx_citemacro_cite">Anderson et al. (<a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite> extracted from Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib61" title="" class="ltx_ref">2015b</a>)</cite> have become the de-facto features used in state-of-the-art VQA models.
These VQA models thus only use features of detected objects as input, and ignore the rest of the image.
Although object features are discriminative, dense annotations are required for training and additional large deep networks for extraction.
Object detection can be imperfect for small and rare objects <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib82" title="" class="ltx_ref">2019</a>)</cite>; for instance
if an object detection model detects only four out of six bananas in an image, features of the other two bananas will not be used by VQA models.
This creates a performance bottle-neck for questions about counting or rare objects.
</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">We take a step back and postulate that the use of features of the entire image in context could reduce this bottleneck.
Image features extracted from a ResNet <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib26" title="" class="ltx_ref">2016</a>)</cite> trained for the ImageNet <cite class="ltx_cite ltx_citemacro_cite">Russakovsky et al. (<a href="#bib.bib64" title="" class="ltx_ref">2015</a>)</cite> classification task, which is widely used for computer vision tasks, have been previously used for VQA models <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib23" title="" class="ltx_ref">2017</a>)</cite>.
Unfortunately, since ImageNet contains iconic (single-object) images, using these features for non-iconic VQA images is restrictive since many questions refer to multiple objects and backgrounds in the image.
Inspired by Spatial Pyramid Matching <cite class="ltx_cite ltx_citemacro_cite">Lazebnik et al. (<a href="#bib.bib42" title="" class="ltx_ref">2006</a>)</cite> for image classification, we propose <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">spatial pyramid patch features</span> to represent the input VQA image into a sequence of features at different scales.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.4" class="ltx_p">We divide each image <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mi id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><ci id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">I</annotation></semantics></math> into a set of image patches <math id="S4.SS1.p3.2.m2.3" class="ltx_Math" alttext="\{I_{k_{1}},\dots,I_{k_{n}}\}" display="inline"><semantics id="S4.SS1.p3.2.m2.3a"><mrow id="S4.SS1.p3.2.m2.3.3.2" xref="S4.SS1.p3.2.m2.3.3.3.cmml"><mo stretchy="false" id="S4.SS1.p3.2.m2.3.3.2.3" xref="S4.SS1.p3.2.m2.3.3.3.cmml">{</mo><msub id="S4.SS1.p3.2.m2.2.2.1.1" xref="S4.SS1.p3.2.m2.2.2.1.1.cmml"><mi id="S4.SS1.p3.2.m2.2.2.1.1.2" xref="S4.SS1.p3.2.m2.2.2.1.1.2.cmml">I</mi><msub id="S4.SS1.p3.2.m2.2.2.1.1.3" xref="S4.SS1.p3.2.m2.2.2.1.1.3.cmml"><mi id="S4.SS1.p3.2.m2.2.2.1.1.3.2" xref="S4.SS1.p3.2.m2.2.2.1.1.3.2.cmml">k</mi><mn id="S4.SS1.p3.2.m2.2.2.1.1.3.3" xref="S4.SS1.p3.2.m2.2.2.1.1.3.3.cmml">1</mn></msub></msub><mo id="S4.SS1.p3.2.m2.3.3.2.4" xref="S4.SS1.p3.2.m2.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">…</mi><mo id="S4.SS1.p3.2.m2.3.3.2.5" xref="S4.SS1.p3.2.m2.3.3.3.cmml">,</mo><msub id="S4.SS1.p3.2.m2.3.3.2.2" xref="S4.SS1.p3.2.m2.3.3.2.2.cmml"><mi id="S4.SS1.p3.2.m2.3.3.2.2.2" xref="S4.SS1.p3.2.m2.3.3.2.2.2.cmml">I</mi><msub id="S4.SS1.p3.2.m2.3.3.2.2.3" xref="S4.SS1.p3.2.m2.3.3.2.2.3.cmml"><mi id="S4.SS1.p3.2.m2.3.3.2.2.3.2" xref="S4.SS1.p3.2.m2.3.3.2.2.3.2.cmml">k</mi><mi id="S4.SS1.p3.2.m2.3.3.2.2.3.3" xref="S4.SS1.p3.2.m2.3.3.2.2.3.3.cmml">n</mi></msub></msub><mo stretchy="false" id="S4.SS1.p3.2.m2.3.3.2.6" xref="S4.SS1.p3.2.m2.3.3.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.3b"><set id="S4.SS1.p3.2.m2.3.3.3.cmml" xref="S4.SS1.p3.2.m2.3.3.2"><apply id="S4.SS1.p3.2.m2.2.2.1.1.cmml" xref="S4.SS1.p3.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.2.m2.2.2.1.1.1.cmml" xref="S4.SS1.p3.2.m2.2.2.1.1">subscript</csymbol><ci id="S4.SS1.p3.2.m2.2.2.1.1.2.cmml" xref="S4.SS1.p3.2.m2.2.2.1.1.2">𝐼</ci><apply id="S4.SS1.p3.2.m2.2.2.1.1.3.cmml" xref="S4.SS1.p3.2.m2.2.2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p3.2.m2.2.2.1.1.3.1.cmml" xref="S4.SS1.p3.2.m2.2.2.1.1.3">subscript</csymbol><ci id="S4.SS1.p3.2.m2.2.2.1.1.3.2.cmml" xref="S4.SS1.p3.2.m2.2.2.1.1.3.2">𝑘</ci><cn type="integer" id="S4.SS1.p3.2.m2.2.2.1.1.3.3.cmml" xref="S4.SS1.p3.2.m2.2.2.1.1.3.3">1</cn></apply></apply><ci id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">…</ci><apply id="S4.SS1.p3.2.m2.3.3.2.2.cmml" xref="S4.SS1.p3.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S4.SS1.p3.2.m2.3.3.2.2.1.cmml" xref="S4.SS1.p3.2.m2.3.3.2.2">subscript</csymbol><ci id="S4.SS1.p3.2.m2.3.3.2.2.2.cmml" xref="S4.SS1.p3.2.m2.3.3.2.2.2">𝐼</ci><apply id="S4.SS1.p3.2.m2.3.3.2.2.3.cmml" xref="S4.SS1.p3.2.m2.3.3.2.2.3"><csymbol cd="ambiguous" id="S4.SS1.p3.2.m2.3.3.2.2.3.1.cmml" xref="S4.SS1.p3.2.m2.3.3.2.2.3">subscript</csymbol><ci id="S4.SS1.p3.2.m2.3.3.2.2.3.2.cmml" xref="S4.SS1.p3.2.m2.3.3.2.2.3.2">𝑘</ci><ci id="S4.SS1.p3.2.m2.3.3.2.2.3.3.cmml" xref="S4.SS1.p3.2.m2.3.3.2.2.3.3">𝑛</ci></apply></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.3c">\{I_{k_{1}},\dots,I_{k_{n}}\}</annotation></semantics></math>, each <math id="S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="I_{k_{i}}" display="inline"><semantics id="S4.SS1.p3.3.m3.1a"><msub id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml"><mi id="S4.SS1.p3.3.m3.1.1.2" xref="S4.SS1.p3.3.m3.1.1.2.cmml">I</mi><msub id="S4.SS1.p3.3.m3.1.1.3" xref="S4.SS1.p3.3.m3.1.1.3.cmml"><mi id="S4.SS1.p3.3.m3.1.1.3.2" xref="S4.SS1.p3.3.m3.1.1.3.2.cmml">k</mi><mi id="S4.SS1.p3.3.m3.1.1.3.3" xref="S4.SS1.p3.3.m3.1.1.3.3.cmml">i</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><apply id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.3.m3.1.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.p3.3.m3.1.1.2.cmml" xref="S4.SS1.p3.3.m3.1.1.2">𝐼</ci><apply id="S4.SS1.p3.3.m3.1.1.3.cmml" xref="S4.SS1.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p3.3.m3.1.1.3.1.cmml" xref="S4.SS1.p3.3.m3.1.1.3">subscript</csymbol><ci id="S4.SS1.p3.3.m3.1.1.3.2.cmml" xref="S4.SS1.p3.3.m3.1.1.3.2">𝑘</ci><ci id="S4.SS1.p3.3.m3.1.1.3.3.cmml" xref="S4.SS1.p3.3.m3.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">I_{k_{i}}</annotation></semantics></math> being a <math id="S4.SS1.p3.4.m4.1" class="ltx_Math" alttext="k_{i}\times k_{i}" display="inline"><semantics id="S4.SS1.p3.4.m4.1a"><mrow id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml"><msub id="S4.SS1.p3.4.m4.1.1.2" xref="S4.SS1.p3.4.m4.1.1.2.cmml"><mi id="S4.SS1.p3.4.m4.1.1.2.2" xref="S4.SS1.p3.4.m4.1.1.2.2.cmml">k</mi><mi id="S4.SS1.p3.4.m4.1.1.2.3" xref="S4.SS1.p3.4.m4.1.1.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p3.4.m4.1.1.1" xref="S4.SS1.p3.4.m4.1.1.1.cmml">×</mo><msub id="S4.SS1.p3.4.m4.1.1.3" xref="S4.SS1.p3.4.m4.1.1.3.cmml"><mi id="S4.SS1.p3.4.m4.1.1.3.2" xref="S4.SS1.p3.4.m4.1.1.3.2.cmml">k</mi><mi id="S4.SS1.p3.4.m4.1.1.3.3" xref="S4.SS1.p3.4.m4.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.1b"><apply id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1"><times id="S4.SS1.p3.4.m4.1.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1.1"></times><apply id="S4.SS1.p3.4.m4.1.1.2.cmml" xref="S4.SS1.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p3.4.m4.1.1.2.1.cmml" xref="S4.SS1.p3.4.m4.1.1.2">subscript</csymbol><ci id="S4.SS1.p3.4.m4.1.1.2.2.cmml" xref="S4.SS1.p3.4.m4.1.1.2.2">𝑘</ci><ci id="S4.SS1.p3.4.m4.1.1.2.3.cmml" xref="S4.SS1.p3.4.m4.1.1.2.3">𝑖</ci></apply><apply id="S4.SS1.p3.4.m4.1.1.3.cmml" xref="S4.SS1.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p3.4.m4.1.1.3.1.cmml" xref="S4.SS1.p3.4.m4.1.1.3">subscript</csymbol><ci id="S4.SS1.p3.4.m4.1.1.3.2.cmml" xref="S4.SS1.p3.4.m4.1.1.3.2">𝑘</ci><ci id="S4.SS1.p3.4.m4.1.1.3.3.cmml" xref="S4.SS1.p3.4.m4.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.1c">k_{i}\times k_{i}</annotation></semantics></math> grid of patches, and extract ResNet features for each patch.
Larger patches encode global features and relations, while smaller patches encode local and low-level features.
</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Encoder:</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.3" class="ltx_p">Our Encoder model is similar to the UNITER single-stream transformer, where the sequence of word tokens <math id="S4.SS1.SSS0.Px1.p1.1.m1.3" class="ltx_Math" alttext="w=\{w_{1},...,w_{T}\}" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.1.m1.3a"><mrow id="S4.SS1.SSS0.Px1.p1.1.m1.3.3" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3.cmml"><mi id="S4.SS1.SSS0.Px1.p1.1.m1.3.3.4" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3.4.cmml">w</mi><mo id="S4.SS1.SSS0.Px1.p1.1.m1.3.3.3" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3.3.cmml">=</mo><mrow id="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.3.cmml"><mo stretchy="false" id="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.2.3" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.3.cmml">{</mo><msub id="S4.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.cmml"><mi id="S4.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.2.cmml">w</mi><mn id="S4.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.3" xref="S4.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.2.4" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S4.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">…</mi><mo id="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.2.5" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.3.cmml">,</mo><msub id="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.2.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.2.2.cmml"><mi id="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.2.2.2" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.2.2.2.cmml">w</mi><mi id="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.2.2.3" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.2.2.3.cmml">T</mi></msub><mo stretchy="false" id="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.2.6" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.1.m1.3b"><apply id="S4.SS1.SSS0.Px1.p1.1.m1.3.3.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3"><eq id="S4.SS1.SSS0.Px1.p1.1.m1.3.3.3.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3.3"></eq><ci id="S4.SS1.SSS0.Px1.p1.1.m1.3.3.4.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3.4">𝑤</ci><set id="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.3.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.2"><apply id="S4.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.2">𝑤</ci><cn type="integer" id="S4.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.2.2.1.1.1.3">1</cn></apply><ci id="S4.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.1.1">…</ci><apply id="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.2.2.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.2.2.1.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.2.2.2.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.2.2.2">𝑤</ci><ci id="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.2.2.3.cmml" xref="S4.SS1.SSS0.Px1.p1.1.m1.3.3.2.2.2.3">𝑇</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.1.m1.3c">w=\{w_{1},...,w_{T}\}</annotation></semantics></math> and the sequence of image patch features <math id="S4.SS1.SSS0.Px1.p1.2.m2.3" class="ltx_Math" alttext="v=\{v_{1},...,v_{K}\}" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.2.m2.3a"><mrow id="S4.SS1.SSS0.Px1.p1.2.m2.3.3" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.cmml"><mi id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.4" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.4.cmml">v</mi><mo id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.3" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.3.cmml">=</mo><mrow id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.3.cmml"><mo stretchy="false" id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.3" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.3.cmml">{</mo><msub id="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1" xref="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.cmml"><mi id="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.2" xref="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.2.cmml">v</mi><mn id="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.3" xref="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.4" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S4.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">…</mi><mo id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.5" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.3.cmml">,</mo><msub id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.cmml"><mi id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.2" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.2.cmml">v</mi><mi id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.3" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.3.cmml">K</mi></msub><mo stretchy="false" id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.6" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.2.m2.3b"><apply id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3"><eq id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.3.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.3"></eq><ci id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.4.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.4">𝑣</ci><set id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.3.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2"><apply id="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.2.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.2">𝑣</ci><cn type="integer" id="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.3.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.3">1</cn></apply><ci id="S4.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.1.1">…</ci><apply id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.1.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.2.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.2">𝑣</ci><ci id="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.3.cmml" xref="S4.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.3">𝐾</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.2.m2.3c">v=\{v_{1},...,v_{K}\}</annotation></semantics></math> are taken as input.
We tokenize the text using a WordPieces <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib85" title="" class="ltx_ref">2016</a>)</cite> tokenizer similar to BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite>, and embed the text tokens through a text-embedder <cite class="ltx_cite ltx_citemacro_cite">Sanh et al. (<a href="#bib.bib66" title="" class="ltx_ref">2019</a>)</cite>.
The visual features are projected to a shared embedding space using a fully-connected layer.
A projected visual position encoding, indicating the patch region (top-right, bottom-left) is added to the visual features.
We concatenate both sequences of features and feed them
to <math id="S4.SS1.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S4.SS1.SSS0.Px1.p1.3.m3.1a"><mi id="S4.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px1.p1.3.m3.1b"><ci id="S4.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS0.Px1.p1.3.m3.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px1.p1.3.m3.1c">L</annotation></semantics></math> cross-modality attention layers.
Parameters between the cross-modality attention layers are shared to reduce parameter count and increase training stability <cite class="ltx_cite ltx_citemacro_cite">Lan et al. (<a href="#bib.bib41" title="" class="ltx_ref">2020</a>)</cite>,
and a residual connection and layer normalization is added after cross-modal attention layer similar to <cite class="ltx_cite ltx_citemacro_citet">Vaswani et al. (<a href="#bib.bib81" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Pre-training Tasks and Loss Functions</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We train the Encoder model using three pre-training tasks: Masked Language Modeling, Masked Question Answering, and Image-Text Matching.</p>
</div>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Masked Language Modeling (MLM):</h4>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">We randomly mask 15% of the word tokens from the caption and ask the model to predict them.
For the caption “There is a man wearing a hat”, the model gets the input “There is <span id="S4.SS2.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_typewriter">[MASK]</span> wearing a hat”.
Without the image, there can be multiple plausible choices for the <span id="S4.SS2.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_typewriter">[MASK]</span> token, such as “woman”, “man”, “girl”, but given the image the model should predict “man”.
This task has been shown to effectively learn cross-modal features <cite class="ltx_cite ltx_citemacro_cite">Tan and Bansal (<a href="#bib.bib74" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Masked Question Answering (MQA):</h4>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p1.1" class="ltx_p">In this task, the answer tokens are masked, and the model is trained to predict the answer tokens.
For example in Figure <a href="#S2.F2" title="Figure 2 ‣ 2 Related Work ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, for the input “ When is someone competing? <span id="S4.SS2.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_typewriter">[MASK]</span> <span id="S4.SS2.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_typewriter">[MASK]</span>”, the model should predict, “at night”.
To answer such questions, the model needs to interpret the image.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Image-Text Matching (ITM):</h4>

<div id="S4.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px3.p1.1" class="ltx_p">We use the five captions provided by MS-COCO as positive samples for each image.
To obtain negative samples, we randomly sample captions from other images that contain a different set of objects.
We train the model on a binary classification task (matching / not matching) for each image-caption pair.</p>
</div>
<div id="S4.SS2.SSS0.Px3.p2" class="ltx_para">
<p id="S4.SS2.SSS0.Px3.p2.1" class="ltx_p">For VQA and ITM, we use the final layer representation <math id="S4.SS2.SSS0.Px3.p2.1.m1.1" class="ltx_Math" alttext="z^{\textit{{[CLS]}}}" display="inline"><semantics id="S4.SS2.SSS0.Px3.p2.1.m1.1a"><msup id="S4.SS2.SSS0.Px3.p2.1.m1.1.1" xref="S4.SS2.SSS0.Px3.p2.1.m1.1.1.cmml"><mi id="S4.SS2.SSS0.Px3.p2.1.m1.1.1.2" xref="S4.SS2.SSS0.Px3.p2.1.m1.1.1.2.cmml">z</mi><mtext class="ltx_mathvariant_monospace" id="S4.SS2.SSS0.Px3.p2.1.m1.1.1.3" xref="S4.SS2.SSS0.Px3.p2.1.m1.1.1.3a.cmml">[CLS]</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px3.p2.1.m1.1b"><apply id="S4.SS2.SSS0.Px3.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px3.p2.1.m1.1.1.1.cmml" xref="S4.SS2.SSS0.Px3.p2.1.m1.1.1">superscript</csymbol><ci id="S4.SS2.SSS0.Px3.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS0.Px3.p2.1.m1.1.1.2">𝑧</ci><ci id="S4.SS2.SSS0.Px3.p2.1.m1.1.1.3a.cmml" xref="S4.SS2.SSS0.Px3.p2.1.m1.1.1.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS2.SSS0.Px3.p2.1.m1.1.1.3.cmml" xref="S4.SS2.SSS0.Px3.p2.1.m1.1.1.3">[CLS]</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px3.p2.1.m1.1c">z^{\textit{{[CLS]}}}</annotation></semantics></math> of <span id="S4.SS2.SSS0.Px3.p2.1.1" class="ltx_text ltx_font_typewriter">[CLS]</span> token
, followed by a feed-forward and softmax layer.
For MLM and MQA we feed corresponding token representations to a different feed-forward layer.
We train the model using cross-entropy loss for all three tasks.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Sub-phrase Weighted Answer Loss:</h4>

<div id="S4.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px4.p1.3" class="ltx_p">As observed before, the questions generated in QA-SRL have long answer phrases.
For instance “What is parked?” has the answer “two black cars”.
We extract all possible sub-phrases that can be alternate answers, but assign them a lower weight than the complete phrase, computed as <math id="S4.SS2.SSS0.Px4.p1.1.m1.2" class="ltx_Math" alttext="W_{sub}=\textit{WordCount}(sub)/\textit{WordCount}(ans)" display="inline"><semantics id="S4.SS2.SSS0.Px4.p1.1.m1.2a"><mrow id="S4.SS2.SSS0.Px4.p1.1.m1.2.2" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.cmml"><msub id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.cmml"><mi id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.2" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.2.cmml">W</mi><mrow id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3.cmml"><mi id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3.2" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3.1" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3.1.cmml">​</mo><mi id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3.3" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3.1a" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3.1.cmml">​</mo><mi id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3.4" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3.4.cmml">b</mi></mrow></msub><mo id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.3" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.3.cmml">=</mo><mrow id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.cmml"><mrow id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.cmml"><mrow id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.3" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.3a.cmml">WordCount</mtext><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.2" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.2" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.2" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.1" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.3" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.1a" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.4" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.4.cmml">b</mi></mrow><mo stretchy="false" id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.3" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.2" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.2.cmml">/</mo><mtext class="ltx_mathvariant_italic" id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.3" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.3a.cmml">WordCount</mtext></mrow><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.3" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.3.cmml">​</mo><mrow id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.2" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.cmml">(</mo><mrow id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.cmml"><mi id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.2" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.1" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.1.cmml">​</mo><mi id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.3" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.1a" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.1.cmml">​</mo><mi id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.4" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.4.cmml">s</mi></mrow><mo stretchy="false" id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.3" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px4.p1.1.m1.2b"><apply id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2"><eq id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.3.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.3"></eq><apply id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.1.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4">subscript</csymbol><ci id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.2.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.2">𝑊</ci><apply id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3"><times id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3.1.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3.1"></times><ci id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3.2.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3.2">𝑠</ci><ci id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3.3.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3.3">𝑢</ci><ci id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3.4.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.4.3.4">𝑏</ci></apply></apply><apply id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2"><times id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.3.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.3"></times><apply id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1"><divide id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.2.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.2"></divide><apply id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1"><times id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.2"></times><ci id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.3a.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.3">WordCount</mtext></ci><apply id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1"><times id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.1"></times><ci id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.2">𝑠</ci><ci id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.3">𝑢</ci><ci id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.1.1.1.1.4">𝑏</ci></apply></apply><ci id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.3a.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.3.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.1.1.1.1.3">WordCount</mtext></ci></apply><apply id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1"><times id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.1"></times><ci id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.2.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.2">𝑎</ci><ci id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.3.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.3">𝑛</ci><ci id="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.4.cmml" xref="S4.SS2.SSS0.Px4.p1.1.m1.2.2.2.2.1.1.4">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px4.p1.1.m1.2c">W_{sub}=\textit{WordCount}(sub)/\textit{WordCount}(ans)</annotation></semantics></math>.
Thus “two black cars” has a weight <math id="S4.SS2.SSS0.Px4.p1.2.m2.1" class="ltx_Math" alttext="1.0" display="inline"><semantics id="S4.SS2.SSS0.Px4.p1.2.m2.1a"><mn id="S4.SS2.SSS0.Px4.p1.2.m2.1.1" xref="S4.SS2.SSS0.Px4.p1.2.m2.1.1.cmml">1.0</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px4.p1.2.m2.1b"><cn type="float" id="S4.SS2.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.2.m2.1.1">1.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px4.p1.2.m2.1c">1.0</annotation></semantics></math>, while the extracted sub-phrases and weights are: (two, 0.33), (2, 0.33), (black, 0.33), (cars, 0.33), (two cars, 0.66), (2 cars, 0.66), (black cars, 0.66), (car, 0.33).
This enforces a distribution over the probable answer space instead of a strict “single true answer”
training.
We train the model with this additional binary cross-entropy loss, where the model predicts a weighted distribution <math id="S4.SS2.SSS0.Px4.p1.3.m3.1" class="ltx_Math" alttext="y_{wa}" display="inline"><semantics id="S4.SS2.SSS0.Px4.p1.3.m3.1a"><msub id="S4.SS2.SSS0.Px4.p1.3.m3.1.1" xref="S4.SS2.SSS0.Px4.p1.3.m3.1.1.cmml"><mi id="S4.SS2.SSS0.Px4.p1.3.m3.1.1.2" xref="S4.SS2.SSS0.Px4.p1.3.m3.1.1.2.cmml">y</mi><mrow id="S4.SS2.SSS0.Px4.p1.3.m3.1.1.3" xref="S4.SS2.SSS0.Px4.p1.3.m3.1.1.3.cmml"><mi id="S4.SS2.SSS0.Px4.p1.3.m3.1.1.3.2" xref="S4.SS2.SSS0.Px4.p1.3.m3.1.1.3.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS0.Px4.p1.3.m3.1.1.3.1" xref="S4.SS2.SSS0.Px4.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.SS2.SSS0.Px4.p1.3.m3.1.1.3.3" xref="S4.SS2.SSS0.Px4.p1.3.m3.1.1.3.3.cmml">a</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px4.p1.3.m3.1b"><apply id="S4.SS2.SSS0.Px4.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS0.Px4.p1.3.m3.1.1.1.cmml" xref="S4.SS2.SSS0.Px4.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.SSS0.Px4.p1.3.m3.1.1.2.cmml" xref="S4.SS2.SSS0.Px4.p1.3.m3.1.1.2">𝑦</ci><apply id="S4.SS2.SSS0.Px4.p1.3.m3.1.1.3.cmml" xref="S4.SS2.SSS0.Px4.p1.3.m3.1.1.3"><times id="S4.SS2.SSS0.Px4.p1.3.m3.1.1.3.1.cmml" xref="S4.SS2.SSS0.Px4.p1.3.m3.1.1.3.1"></times><ci id="S4.SS2.SSS0.Px4.p1.3.m3.1.1.3.2.cmml" xref="S4.SS2.SSS0.Px4.p1.3.m3.1.1.3.2">𝑤</ci><ci id="S4.SS2.SSS0.Px4.p1.3.m3.1.1.3.3.cmml" xref="S4.SS2.SSS0.Px4.p1.3.m3.1.1.3.3">𝑎</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px4.p1.3.m3.1c">y_{wa}</annotation></semantics></math> over the answer vocabulary. The vocabulary is defined from the synthetic QA answer-space.</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{SWA}=\mathcal{L}_{BCE}(\sigma(z^{\textit{{[CLS]}}}),y_{wa})." display="block"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><mrow id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><msub id="S4.E1.m1.1.1.1.1.4" xref="S4.E1.m1.1.1.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.1.1.4.2" xref="S4.E1.m1.1.1.1.1.4.2.cmml">ℒ</mi><mrow id="S4.E1.m1.1.1.1.1.4.3" xref="S4.E1.m1.1.1.1.1.4.3.cmml"><mi id="S4.E1.m1.1.1.1.1.4.3.2" xref="S4.E1.m1.1.1.1.1.4.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.4.3.1" xref="S4.E1.m1.1.1.1.1.4.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.1.1.4.3.3" xref="S4.E1.m1.1.1.1.1.4.3.3.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.4.3.1a" xref="S4.E1.m1.1.1.1.1.4.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.1.1.4.3.4" xref="S4.E1.m1.1.1.1.1.4.3.4.cmml">A</mi></mrow></msub><mo id="S4.E1.m1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.3.cmml">=</mo><mrow id="S4.E1.m1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.2.cmml"><msub id="S4.E1.m1.1.1.1.1.2.4" xref="S4.E1.m1.1.1.1.1.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.1.1.2.4.2" xref="S4.E1.m1.1.1.1.1.2.4.2.cmml">ℒ</mi><mrow id="S4.E1.m1.1.1.1.1.2.4.3" xref="S4.E1.m1.1.1.1.1.2.4.3.cmml"><mi id="S4.E1.m1.1.1.1.1.2.4.3.2" xref="S4.E1.m1.1.1.1.1.2.4.3.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.2.4.3.1" xref="S4.E1.m1.1.1.1.1.2.4.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.1.1.2.4.3.3" xref="S4.E1.m1.1.1.1.1.2.4.3.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.2.4.3.1a" xref="S4.E1.m1.1.1.1.1.2.4.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.1.1.2.4.3.4" xref="S4.E1.m1.1.1.1.1.2.4.3.4.cmml">E</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.2.3" xref="S4.E1.m1.1.1.1.1.2.3.cmml">​</mo><mrow id="S4.E1.m1.1.1.1.1.2.2.2" xref="S4.E1.m1.1.1.1.1.2.2.3.cmml"><mo stretchy="false" id="S4.E1.m1.1.1.1.1.2.2.2.3" xref="S4.E1.m1.1.1.1.1.2.2.3.cmml">(</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msup id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">z</mi><mtext class="ltx_mathvariant_monospace" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3a.cmml">[CLS]</mtext></msup><mo stretchy="false" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.1.1.1.1.2.2.2.4" xref="S4.E1.m1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S4.E1.m1.1.1.1.1.2.2.2.2" xref="S4.E1.m1.1.1.1.1.2.2.2.2.cmml"><mi id="S4.E1.m1.1.1.1.1.2.2.2.2.2" xref="S4.E1.m1.1.1.1.1.2.2.2.2.2.cmml">y</mi><mrow id="S4.E1.m1.1.1.1.1.2.2.2.2.3" xref="S4.E1.m1.1.1.1.1.2.2.2.2.3.cmml"><mi id="S4.E1.m1.1.1.1.1.2.2.2.2.3.2" xref="S4.E1.m1.1.1.1.1.2.2.2.2.3.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.1.1.2.2.2.2.3.1" xref="S4.E1.m1.1.1.1.1.2.2.2.2.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.1.1.2.2.2.2.3.3" xref="S4.E1.m1.1.1.1.1.2.2.2.2.3.3.cmml">a</mi></mrow></msub><mo stretchy="false" id="S4.E1.m1.1.1.1.1.2.2.2.5" xref="S4.E1.m1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S4.E1.m1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"><eq id="S4.E1.m1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.3"></eq><apply id="S4.E1.m1.1.1.1.1.4.cmml" xref="S4.E1.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.4.1.cmml" xref="S4.E1.m1.1.1.1.1.4">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.4.2.cmml" xref="S4.E1.m1.1.1.1.1.4.2">ℒ</ci><apply id="S4.E1.m1.1.1.1.1.4.3.cmml" xref="S4.E1.m1.1.1.1.1.4.3"><times id="S4.E1.m1.1.1.1.1.4.3.1.cmml" xref="S4.E1.m1.1.1.1.1.4.3.1"></times><ci id="S4.E1.m1.1.1.1.1.4.3.2.cmml" xref="S4.E1.m1.1.1.1.1.4.3.2">𝑆</ci><ci id="S4.E1.m1.1.1.1.1.4.3.3.cmml" xref="S4.E1.m1.1.1.1.1.4.3.3">𝑊</ci><ci id="S4.E1.m1.1.1.1.1.4.3.4.cmml" xref="S4.E1.m1.1.1.1.1.4.3.4">𝐴</ci></apply></apply><apply id="S4.E1.m1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.2"><times id="S4.E1.m1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.1.1.2.3"></times><apply id="S4.E1.m1.1.1.1.1.2.4.cmml" xref="S4.E1.m1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.2.4.1.cmml" xref="S4.E1.m1.1.1.1.1.2.4">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.2.4.2.cmml" xref="S4.E1.m1.1.1.1.1.2.4.2">ℒ</ci><apply id="S4.E1.m1.1.1.1.1.2.4.3.cmml" xref="S4.E1.m1.1.1.1.1.2.4.3"><times id="S4.E1.m1.1.1.1.1.2.4.3.1.cmml" xref="S4.E1.m1.1.1.1.1.2.4.3.1"></times><ci id="S4.E1.m1.1.1.1.1.2.4.3.2.cmml" xref="S4.E1.m1.1.1.1.1.2.4.3.2">𝐵</ci><ci id="S4.E1.m1.1.1.1.1.2.4.3.3.cmml" xref="S4.E1.m1.1.1.1.1.2.4.3.3">𝐶</ci><ci id="S4.E1.m1.1.1.1.1.2.4.3.4.cmml" xref="S4.E1.m1.1.1.1.1.2.4.3.4">𝐸</ci></apply></apply><interval closure="open" id="S4.E1.m1.1.1.1.1.2.2.3.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2"><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1"><times id="S4.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.2"></times><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.3">𝜎</ci><apply id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2">𝑧</ci><ci id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3a.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3">[CLS]</mtext></ci></apply></apply><apply id="S4.E1.m1.1.1.1.1.2.2.2.2.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.2.2">𝑦</ci><apply id="S4.E1.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.2.3"><times id="S4.E1.m1.1.1.1.1.2.2.2.2.3.1.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.2.3.1"></times><ci id="S4.E1.m1.1.1.1.1.2.2.2.2.3.2.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.2.3.2">𝑤</ci><ci id="S4.E1.m1.1.1.1.1.2.2.2.2.3.3.cmml" xref="S4.E1.m1.1.1.1.1.2.2.2.2.3.3">𝑎</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">\mathcal{L}_{SWA}=\mathcal{L}_{BCE}(\sigma(z^{\textit{{[CLS]}}}),y_{wa}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS2.SSS0.Px4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS0.Px4.p2.1" class="ltx_p">The total loss, with scalar coefficients <math id="S4.SS2.SSS0.Px4.p2.1.m1.4" class="ltx_Math" alttext="\alpha,\beta\in(0,1]" display="inline"><semantics id="S4.SS2.SSS0.Px4.p2.1.m1.4a"><mrow id="S4.SS2.SSS0.Px4.p2.1.m1.4.5" xref="S4.SS2.SSS0.Px4.p2.1.m1.4.5.cmml"><mrow id="S4.SS2.SSS0.Px4.p2.1.m1.4.5.2.2" xref="S4.SS2.SSS0.Px4.p2.1.m1.4.5.2.1.cmml"><mi id="S4.SS2.SSS0.Px4.p2.1.m1.3.3" xref="S4.SS2.SSS0.Px4.p2.1.m1.3.3.cmml">α</mi><mo id="S4.SS2.SSS0.Px4.p2.1.m1.4.5.2.2.1" xref="S4.SS2.SSS0.Px4.p2.1.m1.4.5.2.1.cmml">,</mo><mi id="S4.SS2.SSS0.Px4.p2.1.m1.4.4" xref="S4.SS2.SSS0.Px4.p2.1.m1.4.4.cmml">β</mi></mrow><mo id="S4.SS2.SSS0.Px4.p2.1.m1.4.5.1" xref="S4.SS2.SSS0.Px4.p2.1.m1.4.5.1.cmml">∈</mo><mrow id="S4.SS2.SSS0.Px4.p2.1.m1.4.5.3.2" xref="S4.SS2.SSS0.Px4.p2.1.m1.4.5.3.1.cmml"><mo stretchy="false" id="S4.SS2.SSS0.Px4.p2.1.m1.4.5.3.2.1" xref="S4.SS2.SSS0.Px4.p2.1.m1.4.5.3.1.cmml">(</mo><mn id="S4.SS2.SSS0.Px4.p2.1.m1.1.1" xref="S4.SS2.SSS0.Px4.p2.1.m1.1.1.cmml">0</mn><mo id="S4.SS2.SSS0.Px4.p2.1.m1.4.5.3.2.2" xref="S4.SS2.SSS0.Px4.p2.1.m1.4.5.3.1.cmml">,</mo><mn id="S4.SS2.SSS0.Px4.p2.1.m1.2.2" xref="S4.SS2.SSS0.Px4.p2.1.m1.2.2.cmml">1</mn><mo stretchy="false" id="S4.SS2.SSS0.Px4.p2.1.m1.4.5.3.2.3" xref="S4.SS2.SSS0.Px4.p2.1.m1.4.5.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px4.p2.1.m1.4b"><apply id="S4.SS2.SSS0.Px4.p2.1.m1.4.5.cmml" xref="S4.SS2.SSS0.Px4.p2.1.m1.4.5"><in id="S4.SS2.SSS0.Px4.p2.1.m1.4.5.1.cmml" xref="S4.SS2.SSS0.Px4.p2.1.m1.4.5.1"></in><list id="S4.SS2.SSS0.Px4.p2.1.m1.4.5.2.1.cmml" xref="S4.SS2.SSS0.Px4.p2.1.m1.4.5.2.2"><ci id="S4.SS2.SSS0.Px4.p2.1.m1.3.3.cmml" xref="S4.SS2.SSS0.Px4.p2.1.m1.3.3">𝛼</ci><ci id="S4.SS2.SSS0.Px4.p2.1.m1.4.4.cmml" xref="S4.SS2.SSS0.Px4.p2.1.m1.4.4">𝛽</ci></list><interval closure="open-closed" id="S4.SS2.SSS0.Px4.p2.1.m1.4.5.3.1.cmml" xref="S4.SS2.SSS0.Px4.p2.1.m1.4.5.3.2"><cn type="integer" id="S4.SS2.SSS0.Px4.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px4.p2.1.m1.1.1">0</cn><cn type="integer" id="S4.SS2.SSS0.Px4.p2.1.m1.2.2.cmml" xref="S4.SS2.SSS0.Px4.p2.1.m1.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px4.p2.1.m1.4c">\alpha,\beta\in(0,1]</annotation></semantics></math> is given by:</p>
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.1" class="ltx_Math" alttext="\mathcal{L}=\mathcal{L}_{MLM}+\mathcal{L}_{MQA}+\alpha\cdot\mathcal{L}_{ITM}+\beta\cdot\mathcal{L}_{SWA}." display="block"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.2.cmml">ℒ</mi><mo id="S4.E2.m1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.cmml">=</mo><mrow id="S4.E2.m1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.3.cmml"><msub id="S4.E2.m1.1.1.1.1.3.2" xref="S4.E2.m1.1.1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.1.1.1.1.3.2.2" xref="S4.E2.m1.1.1.1.1.3.2.2.cmml">ℒ</mi><mrow id="S4.E2.m1.1.1.1.1.3.2.3" xref="S4.E2.m1.1.1.1.1.3.2.3.cmml"><mi id="S4.E2.m1.1.1.1.1.3.2.3.2" xref="S4.E2.m1.1.1.1.1.3.2.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.3.2.3.1" xref="S4.E2.m1.1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S4.E2.m1.1.1.1.1.3.2.3.3" xref="S4.E2.m1.1.1.1.1.3.2.3.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.3.2.3.1a" xref="S4.E2.m1.1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S4.E2.m1.1.1.1.1.3.2.3.4" xref="S4.E2.m1.1.1.1.1.3.2.3.4.cmml">M</mi></mrow></msub><mo id="S4.E2.m1.1.1.1.1.3.1" xref="S4.E2.m1.1.1.1.1.3.1.cmml">+</mo><msub id="S4.E2.m1.1.1.1.1.3.3" xref="S4.E2.m1.1.1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.1.1.1.1.3.3.2" xref="S4.E2.m1.1.1.1.1.3.3.2.cmml">ℒ</mi><mrow id="S4.E2.m1.1.1.1.1.3.3.3" xref="S4.E2.m1.1.1.1.1.3.3.3.cmml"><mi id="S4.E2.m1.1.1.1.1.3.3.3.2" xref="S4.E2.m1.1.1.1.1.3.3.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.3.3.3.1" xref="S4.E2.m1.1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S4.E2.m1.1.1.1.1.3.3.3.3" xref="S4.E2.m1.1.1.1.1.3.3.3.3.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.3.3.3.1a" xref="S4.E2.m1.1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S4.E2.m1.1.1.1.1.3.3.3.4" xref="S4.E2.m1.1.1.1.1.3.3.3.4.cmml">A</mi></mrow></msub><mo id="S4.E2.m1.1.1.1.1.3.1a" xref="S4.E2.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S4.E2.m1.1.1.1.1.3.4" xref="S4.E2.m1.1.1.1.1.3.4.cmml"><mi id="S4.E2.m1.1.1.1.1.3.4.2" xref="S4.E2.m1.1.1.1.1.3.4.2.cmml">α</mi><mo lspace="0.222em" rspace="0.222em" id="S4.E2.m1.1.1.1.1.3.4.1" xref="S4.E2.m1.1.1.1.1.3.4.1.cmml">⋅</mo><msub id="S4.E2.m1.1.1.1.1.3.4.3" xref="S4.E2.m1.1.1.1.1.3.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.1.1.1.1.3.4.3.2" xref="S4.E2.m1.1.1.1.1.3.4.3.2.cmml">ℒ</mi><mrow id="S4.E2.m1.1.1.1.1.3.4.3.3" xref="S4.E2.m1.1.1.1.1.3.4.3.3.cmml"><mi id="S4.E2.m1.1.1.1.1.3.4.3.3.2" xref="S4.E2.m1.1.1.1.1.3.4.3.3.2.cmml">I</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.3.4.3.3.1" xref="S4.E2.m1.1.1.1.1.3.4.3.3.1.cmml">​</mo><mi id="S4.E2.m1.1.1.1.1.3.4.3.3.3" xref="S4.E2.m1.1.1.1.1.3.4.3.3.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.3.4.3.3.1a" xref="S4.E2.m1.1.1.1.1.3.4.3.3.1.cmml">​</mo><mi id="S4.E2.m1.1.1.1.1.3.4.3.3.4" xref="S4.E2.m1.1.1.1.1.3.4.3.3.4.cmml">M</mi></mrow></msub></mrow><mo id="S4.E2.m1.1.1.1.1.3.1b" xref="S4.E2.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S4.E2.m1.1.1.1.1.3.5" xref="S4.E2.m1.1.1.1.1.3.5.cmml"><mi id="S4.E2.m1.1.1.1.1.3.5.2" xref="S4.E2.m1.1.1.1.1.3.5.2.cmml">β</mi><mo lspace="0.222em" rspace="0.222em" id="S4.E2.m1.1.1.1.1.3.5.1" xref="S4.E2.m1.1.1.1.1.3.5.1.cmml">⋅</mo><msub id="S4.E2.m1.1.1.1.1.3.5.3" xref="S4.E2.m1.1.1.1.1.3.5.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.1.1.1.1.3.5.3.2" xref="S4.E2.m1.1.1.1.1.3.5.3.2.cmml">ℒ</mi><mrow id="S4.E2.m1.1.1.1.1.3.5.3.3" xref="S4.E2.m1.1.1.1.1.3.5.3.3.cmml"><mi id="S4.E2.m1.1.1.1.1.3.5.3.3.2" xref="S4.E2.m1.1.1.1.1.3.5.3.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.3.5.3.3.1" xref="S4.E2.m1.1.1.1.1.3.5.3.3.1.cmml">​</mo><mi id="S4.E2.m1.1.1.1.1.3.5.3.3.3" xref="S4.E2.m1.1.1.1.1.3.5.3.3.3.cmml">W</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.3.5.3.3.1a" xref="S4.E2.m1.1.1.1.1.3.5.3.3.1.cmml">​</mo><mi id="S4.E2.m1.1.1.1.1.3.5.3.3.4" xref="S4.E2.m1.1.1.1.1.3.5.3.3.4.cmml">A</mi></mrow></msub></mrow></mrow></mrow><mo lspace="0em" id="S4.E2.m1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"><eq id="S4.E2.m1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1"></eq><ci id="S4.E2.m1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2">ℒ</ci><apply id="S4.E2.m1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.3"><plus id="S4.E2.m1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.1"></plus><apply id="S4.E2.m1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.3.2.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.3.2.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2.2">ℒ</ci><apply id="S4.E2.m1.1.1.1.1.3.2.3.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3"><times id="S4.E2.m1.1.1.1.1.3.2.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3.1"></times><ci id="S4.E2.m1.1.1.1.1.3.2.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3.2">𝑀</ci><ci id="S4.E2.m1.1.1.1.1.3.2.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3.3">𝐿</ci><ci id="S4.E2.m1.1.1.1.1.3.2.3.4.cmml" xref="S4.E2.m1.1.1.1.1.3.2.3.4">𝑀</ci></apply></apply><apply id="S4.E2.m1.1.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.3.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.3.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.3.2">ℒ</ci><apply id="S4.E2.m1.1.1.1.1.3.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.3.3"><times id="S4.E2.m1.1.1.1.1.3.3.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.3.3.1"></times><ci id="S4.E2.m1.1.1.1.1.3.3.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.3.3.2">𝑀</ci><ci id="S4.E2.m1.1.1.1.1.3.3.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.3.3.3">𝑄</ci><ci id="S4.E2.m1.1.1.1.1.3.3.3.4.cmml" xref="S4.E2.m1.1.1.1.1.3.3.3.4">𝐴</ci></apply></apply><apply id="S4.E2.m1.1.1.1.1.3.4.cmml" xref="S4.E2.m1.1.1.1.1.3.4"><ci id="S4.E2.m1.1.1.1.1.3.4.1.cmml" xref="S4.E2.m1.1.1.1.1.3.4.1">⋅</ci><ci id="S4.E2.m1.1.1.1.1.3.4.2.cmml" xref="S4.E2.m1.1.1.1.1.3.4.2">𝛼</ci><apply id="S4.E2.m1.1.1.1.1.3.4.3.cmml" xref="S4.E2.m1.1.1.1.1.3.4.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.3.4.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.4.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.3.4.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.4.3.2">ℒ</ci><apply id="S4.E2.m1.1.1.1.1.3.4.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.4.3.3"><times id="S4.E2.m1.1.1.1.1.3.4.3.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.4.3.3.1"></times><ci id="S4.E2.m1.1.1.1.1.3.4.3.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.4.3.3.2">𝐼</ci><ci id="S4.E2.m1.1.1.1.1.3.4.3.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.4.3.3.3">𝑇</ci><ci id="S4.E2.m1.1.1.1.1.3.4.3.3.4.cmml" xref="S4.E2.m1.1.1.1.1.3.4.3.3.4">𝑀</ci></apply></apply></apply><apply id="S4.E2.m1.1.1.1.1.3.5.cmml" xref="S4.E2.m1.1.1.1.1.3.5"><ci id="S4.E2.m1.1.1.1.1.3.5.1.cmml" xref="S4.E2.m1.1.1.1.1.3.5.1">⋅</ci><ci id="S4.E2.m1.1.1.1.1.3.5.2.cmml" xref="S4.E2.m1.1.1.1.1.3.5.2">𝛽</ci><apply id="S4.E2.m1.1.1.1.1.3.5.3.cmml" xref="S4.E2.m1.1.1.1.1.3.5.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.3.5.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.5.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.3.5.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.5.3.2">ℒ</ci><apply id="S4.E2.m1.1.1.1.1.3.5.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.5.3.3"><times id="S4.E2.m1.1.1.1.1.3.5.3.3.1.cmml" xref="S4.E2.m1.1.1.1.1.3.5.3.3.1"></times><ci id="S4.E2.m1.1.1.1.1.3.5.3.3.2.cmml" xref="S4.E2.m1.1.1.1.1.3.5.3.3.2">𝑆</ci><ci id="S4.E2.m1.1.1.1.1.3.5.3.3.3.cmml" xref="S4.E2.m1.1.1.1.1.3.5.3.3.3">𝑊</ci><ci id="S4.E2.m1.1.1.1.1.3.5.3.3.4.cmml" xref="S4.E2.m1.1.1.1.1.3.5.3.3.4">𝐴</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">\mathcal{L}=\mathcal{L}_{MLM}+\mathcal{L}_{MQA}+\alpha\cdot\mathcal{L}_{ITM}+\beta\cdot\mathcal{L}_{SWA}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Setup</h2>

<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Datasets:</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">We evaluate our methods on the three popular visual question answering benchmarks: VQA-v2, VQA-CP-v2, and GQA.
Answering questions in VQA-v2 and VQA-CP v2 requires image and question understanding, whereas GQA further requires spatial understanding such as compositionality and relations between objects.
We evaluate our methods under <span id="S5.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">zero-shot</span> transfer (trained only on procedurally generated samples), and <span id="S5.SS0.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">fully-supervised</span> (where we finetune our model using the associated train annotations) settings.
We use exact-match accuracies for GQA, and use VQA-metric <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a href="#bib.bib2" title="" class="ltx_ref">2017</a>)</cite> for VQA.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Training:</h4>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.4" class="ltx_p">Our Encoder has 8 cross-modal layers with a hidden dimension of <math id="S5.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="768" display="inline"><semantics id="S5.SS0.SSS0.Px2.p1.1.m1.1a"><mn id="S5.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">768</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.1.m1.1b"><cn type="integer" id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1">768</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.1.m1.1c">768</annotation></semantics></math>. The weights are initialized using the standard definition as provided in the Huggingface repository <cite class="ltx_cite ltx_citemacro_cite">Wolf et al. (<a href="#bib.bib83" title="" class="ltx_ref">2019</a>)</cite>.
Our models are pre-trained for 40 epochs with a learning rate of <math id="S5.SS0.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="1\mathrm{e}{-5}" display="inline"><semantics id="S5.SS0.SSS0.Px2.p1.2.m2.1a"><mrow id="S5.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.cmml"><mrow id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml"><mn id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2.2" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2.1" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2.3" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.cmml">e</mi></mrow><mo id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.1" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml">−</mo><mn id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.3" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.2.m2.1b"><apply id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1"><minus id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.1"></minus><apply id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2"><times id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2.1.cmml" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2.1"></times><cn type="integer" id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2.2.cmml" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2.2">1</cn><ci id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2.3.cmml" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2.3">e</ci></apply><cn type="integer" id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.2.m2.1c">1\mathrm{e}{-5}</annotation></semantics></math>, batch size of 256, using Adam optimizer.
For finetuning, we use a learning rate of <math id="S5.SS0.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="1\mathrm{e}{-5}" display="inline"><semantics id="S5.SS0.SSS0.Px2.p1.3.m3.1a"><mrow id="S5.SS0.SSS0.Px2.p1.3.m3.1.1" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.cmml"><mrow id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.2" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml"><mn id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.2.2" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.2.1" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.2.3" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.2.3.cmml">e</mi></mrow><mo id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.1" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.1.cmml">−</mo><mn id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.3" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.3.m3.1b"><apply id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1"><minus id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.1"></minus><apply id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.2"><times id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.2.1.cmml" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.2.1"></times><cn type="integer" id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.2.2.cmml" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.2.2">1</cn><ci id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.2.3.cmml" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.2.3">e</ci></apply><cn type="integer" id="S5.SS0.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S5.SS0.SSS0.Px2.p1.3.m3.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.3.m3.1c">1\mathrm{e}{-5}</annotation></semantics></math> or <math id="S5.SS0.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="5\mathrm{e}{-5}" display="inline"><semantics id="S5.SS0.SSS0.Px2.p1.4.m4.1a"><mrow id="S5.SS0.SSS0.Px2.p1.4.m4.1.1" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.cmml"><mrow id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml"><mn id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2.2" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2.1" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2.3" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2.3.cmml">e</mi></mrow><mo id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.1" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml">−</mo><mn id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.3" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.4.m4.1b"><apply id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1"><minus id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.1"></minus><apply id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2"><times id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2.1.cmml" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2.1"></times><cn type="integer" id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2.2.cmml" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2.2">5</cn><ci id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2.3.cmml" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.2.3">e</ci></apply><cn type="integer" id="S5.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S5.SS0.SSS0.Px2.p1.4.m4.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.4.m4.1c">5\mathrm{e}{-5}</annotation></semantics></math> and batch size of 32 for 10 epochs.
We use a ResNet-50 pretrained on ImageNet to extract features from image patches with 50% overlap, and Faster R-CNN pretrained on Visual Genome to extract object features. We evaluate both frozen and finetuned ResNet, and observe finetuning the feature extractor to perform better.
All our models are trained using 4 Nvidia V100 16 GB GPUs. All results in the fully supervised setting are reported for from-scratch trained final classification layers.
</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Baselines:</h4>

<div id="S5.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p1.1" class="ltx_p">To measure the improvements due to our proposed image patch features and SWA loss, we compare our methods to the UpDown model <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib3" title="" class="ltx_ref">Anderson et al.</a></cite>, which uses object bounding-box features.
For the Zero-shot transfer setting, we compare our Encoder with UpDown when trained with spatial features as well as object features.
Pre-trained transformers such as UNITER use large V&amp;L corpora, dense human annotations for objects and Q-A pairs
and supervised loss functions over these.
Comparisons with such models are therefore not fair in a ZSL setting; instead, we perform these comparisons in a fully-supervised (FSL) setting.
</p>
</div>
<figure id="S5.T1" class="ltx_table">
<div id="S5.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:306.6pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-12.9pt,9.1pt) scale(0.943654711286896,0.943654711286896) ;">
<table id="S5.T1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T1.1.1.1" class="ltx_tr">
<td id="S5.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S5.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S5.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">All</span></td>
<td id="S5.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Yes-No</span></td>
<td id="S5.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Num</span></td>
<td id="S5.T1.1.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt"><span id="S5.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">Others</span></td>
</tr>
<tr id="S5.T1.1.1.2" class="ltx_tr" style="background-color:#E0FFFF;">
<td id="S5.T1.1.1.2.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S5.T1.1.1.2.1.1" class="ltx_text" style="background-color:#E0FFFF;">SAN <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib87" title="" class="ltx_ref">2016</a>)</cite></span></td>
<td id="S5.T1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.1.1.2.2.1" class="ltx_text" style="background-color:#E0FFFF;">25.0</span></td>
<td id="S5.T1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.1.1.2.3.1" class="ltx_text" style="background-color:#E0FFFF;">38.4</span></td>
<td id="S5.T1.1.1.2.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.1.1.2.4.1" class="ltx_text" style="background-color:#E0FFFF;">11.1</span></td>
<td id="S5.T1.1.1.2.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt"><span id="S5.T1.1.1.2.5.1" class="ltx_text" style="background-color:#E0FFFF;">21.7</span></td>
</tr>
<tr id="S5.T1.1.1.3" class="ltx_tr" style="background-color:#E0FFFF;">
<td id="S5.T1.1.1.3.1" class="ltx_td ltx_align_left"><span id="S5.T1.1.1.3.1.1" class="ltx_text" style="background-color:#E0FFFF;">GVQA <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite></span></td>
<td id="S5.T1.1.1.3.2" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.3.2.1" class="ltx_text" style="background-color:#E0FFFF;">31.3</span></td>
<td id="S5.T1.1.1.3.3" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.3.3.1" class="ltx_text" style="background-color:#E0FFFF;">58.0</span></td>
<td id="S5.T1.1.1.3.4" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.3.4.1" class="ltx_text" style="background-color:#E0FFFF;">13.7</span></td>
<td id="S5.T1.1.1.3.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.1.1.3.5.1" class="ltx_text" style="background-color:#E0FFFF;">22.1</span></td>
</tr>
<tr id="S5.T1.1.1.4" class="ltx_tr" style="background-color:#E0FFFF;">
<td id="S5.T1.1.1.4.1" class="ltx_td ltx_align_left"><span id="S5.T1.1.1.4.1.1" class="ltx_text" style="background-color:#E0FFFF;">UpDown <cite class="ltx_cite ltx_citemacro_cite">Anderson et al. (<a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite></span></td>
<td id="S5.T1.1.1.4.2" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.4.2.1" class="ltx_text" style="background-color:#E0FFFF;">39.1</span></td>
<td id="S5.T1.1.1.4.3" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.4.3.1" class="ltx_text" style="background-color:#E0FFFF;">62.4</span></td>
<td id="S5.T1.1.1.4.4" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.4.4.1" class="ltx_text" style="background-color:#E0FFFF;">15.1</span></td>
<td id="S5.T1.1.1.4.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.1.1.4.5.1" class="ltx_text" style="background-color:#E0FFFF;">34.5</span></td>
</tr>
<tr id="S5.T1.1.1.5" class="ltx_tr" style="background-color:#E0FFFF;">
<td id="S5.T1.1.1.5.1" class="ltx_td ltx_align_left"><span id="S5.T1.1.1.5.1.1" class="ltx_text" style="background-color:#E0FFFF;">AReg<cite class="ltx_cite ltx_citemacro_cite">Ramakrishnan et al. (<a href="#bib.bib58" title="" class="ltx_ref">2017</a>)</cite></span></td>
<td id="S5.T1.1.1.5.2" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.5.2.1" class="ltx_text" style="background-color:#E0FFFF;">42.0</span></td>
<td id="S5.T1.1.1.5.3" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.5.3.1" class="ltx_text" style="background-color:#E0FFFF;">65.5</span></td>
<td id="S5.T1.1.1.5.4" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.5.4.1" class="ltx_text" style="background-color:#E0FFFF;">15.9</span></td>
<td id="S5.T1.1.1.5.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.1.1.5.5.1" class="ltx_text" style="background-color:#E0FFFF;">36.6</span></td>
</tr>
<tr id="S5.T1.1.1.6" class="ltx_tr" style="background-color:#E0FFFF;">
<td id="S5.T1.1.1.6.1" class="ltx_td ltx_align_left"><span id="S5.T1.1.1.6.1.1" class="ltx_text" style="background-color:#E0FFFF;">AdvReg <cite class="ltx_cite ltx_citemacro_cite">Grand and Belinkov (<a href="#bib.bib24" title="" class="ltx_ref">2019</a>)</cite></span></td>
<td id="S5.T1.1.1.6.2" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.6.2.1" class="ltx_text" style="background-color:#E0FFFF;">42.3</span></td>
<td id="S5.T1.1.1.6.3" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.6.3.1" class="ltx_text" style="background-color:#E0FFFF;">59.7</span></td>
<td id="S5.T1.1.1.6.4" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.6.4.1" class="ltx_text" style="background-color:#E0FFFF;">14.8</span></td>
<td id="S5.T1.1.1.6.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.1.1.6.5.1" class="ltx_text" style="background-color:#E0FFFF;">40.8</span></td>
</tr>
<tr id="S5.T1.1.1.7" class="ltx_tr" style="background-color:#E0FFFF;">
<td id="S5.T1.1.1.7.1" class="ltx_td ltx_align_left"><span id="S5.T1.1.1.7.1.1" class="ltx_text" style="background-color:#E0FFFF;">RUBi <cite class="ltx_cite ltx_citemacro_cite">Cadène et al. (<a href="#bib.bib8" title="" class="ltx_ref">2019</a>)</cite></span></td>
<td id="S5.T1.1.1.7.2" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.7.2.1" class="ltx_text" style="background-color:#E0FFFF;">47.1</span></td>
<td id="S5.T1.1.1.7.3" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.7.3.1" class="ltx_text" style="background-color:#E0FFFF;">68.7</span></td>
<td id="S5.T1.1.1.7.4" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.7.4.1" class="ltx_text" style="background-color:#E0FFFF;">20.3</span></td>
<td id="S5.T1.1.1.7.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.1.1.7.5.1" class="ltx_text" style="background-color:#E0FFFF;">43.2</span></td>
</tr>
<tr id="S5.T1.1.1.8" class="ltx_tr" style="background-color:#E0FFFF;">
<td id="S5.T1.1.1.8.1" class="ltx_td ltx_align_left"><cite class="ltx_cite ltx_citemacro_citet">Teney and van den Hengel <span id="S5.T1.1.1.8.1.1.1.1.1" class="ltx_text" style="background-color:#E0FFFF;">(</span><a href="#bib.bib77" title="" class="ltx_ref">2019</a><span id="S5.T1.1.1.8.1.2.2.2.1" class="ltx_text" style="background-color:#E0FFFF;">)</span></cite></td>
<td id="S5.T1.1.1.8.2" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.8.2.1" class="ltx_text" style="background-color:#E0FFFF;">46.0</span></td>
<td id="S5.T1.1.1.8.3" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.8.3.1" class="ltx_text" style="background-color:#E0FFFF;">58.2</span></td>
<td id="S5.T1.1.1.8.4" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.8.4.1" class="ltx_text" style="background-color:#E0FFFF;">29.5</span></td>
<td id="S5.T1.1.1.8.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.1.1.8.5.1" class="ltx_text" style="background-color:#E0FFFF;">44.3</span></td>
</tr>
<tr id="S5.T1.1.1.9" class="ltx_tr" style="background-color:#E0FFFF;">
<td id="S5.T1.1.1.9.1" class="ltx_td ltx_align_left"><span id="S5.T1.1.1.9.1.1" class="ltx_text" style="background-color:#E0FFFF;">Unshuffling <cite class="ltx_cite ltx_citemacro_cite">Teney et al. (<a href="#bib.bib76" title="" class="ltx_ref">2020b</a>)</cite></span></td>
<td id="S5.T1.1.1.9.2" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.9.2.1" class="ltx_text" style="background-color:#E0FFFF;">42.4</span></td>
<td id="S5.T1.1.1.9.3" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.9.3.1" class="ltx_text" style="background-color:#E0FFFF;">47.7</span></td>
<td id="S5.T1.1.1.9.4" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.9.4.1" class="ltx_text" style="background-color:#E0FFFF;">14.4</span></td>
<td id="S5.T1.1.1.9.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.1.1.9.5.1" class="ltx_text" style="background-color:#E0FFFF;">47.3</span></td>
</tr>
<tr id="S5.T1.1.1.10" class="ltx_tr" style="background-color:#E0FFFF;">
<td id="S5.T1.1.1.10.1" class="ltx_td ltx_align_left"><span id="S5.T1.1.1.10.1.1" class="ltx_text" style="background-color:#E0FFFF;">UpDn+CE+GS <cite class="ltx_cite ltx_citemacro_cite">Teney et al. (<a href="#bib.bib75" title="" class="ltx_ref">2020a</a>)</cite></span></td>
<td id="S5.T1.1.1.10.2" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.10.2.1" class="ltx_text" style="background-color:#E0FFFF;">46.8</span></td>
<td id="S5.T1.1.1.10.3" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.10.3.1" class="ltx_text" style="background-color:#E0FFFF;">64.5</span></td>
<td id="S5.T1.1.1.10.4" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.10.4.1" class="ltx_text" style="background-color:#E0FFFF;">15.4</span></td>
<td id="S5.T1.1.1.10.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.1.1.10.5.1" class="ltx_text" style="background-color:#E0FFFF;">45.9</span></td>
</tr>
<tr id="S5.T1.1.1.11" class="ltx_tr" style="background-color:#E0FFFF;">
<td id="S5.T1.1.1.11.1" class="ltx_td ltx_align_left"><span id="S5.T1.1.1.11.1.1" class="ltx_text" style="background-color:#E0FFFF;">LXMERT <cite class="ltx_cite ltx_citemacro_cite">Tan and Bansal (<a href="#bib.bib74" title="" class="ltx_ref">2019</a>)</cite></span></td>
<td id="S5.T1.1.1.11.2" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.11.2.1" class="ltx_text" style="background-color:#E0FFFF;">46.2</span></td>
<td id="S5.T1.1.1.11.3" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.11.3.1" class="ltx_text" style="background-color:#E0FFFF;">42.8</span></td>
<td id="S5.T1.1.1.11.4" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.11.4.1" class="ltx_text" style="background-color:#E0FFFF;">18.9</span></td>
<td id="S5.T1.1.1.11.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.1.1.11.5.1" class="ltx_text" style="background-color:#E0FFFF;">55.5</span></td>
</tr>
<tr id="S5.T1.1.1.12" class="ltx_tr">
<td id="S5.T1.1.1.12.1" class="ltx_td ltx_align_left">SCR <cite class="ltx_cite ltx_citemacro_cite">Wu and Mooney (<a href="#bib.bib84" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S5.T1.1.1.12.2" class="ltx_td ltx_align_center">48.4</td>
<td id="S5.T1.1.1.12.3" class="ltx_td ltx_align_center">70.4</td>
<td id="S5.T1.1.1.12.4" class="ltx_td ltx_align_center" style="background-color:#FFE8E8;"><span id="S5.T1.1.1.12.4.1" class="ltx_text" style="background-color:#FFE8E8;">10.4</span></td>
<td id="S5.T1.1.1.12.5" class="ltx_td ltx_nopad_r ltx_align_center">47.3</td>
</tr>
<tr id="S5.T1.1.1.13" class="ltx_tr">
<td id="S5.T1.1.1.13.1" class="ltx_td ltx_align_left">LMH <cite class="ltx_cite ltx_citemacro_cite">Clark et al. (<a href="#bib.bib13" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S5.T1.1.1.13.2" class="ltx_td ltx_align_center">52.4</td>
<td id="S5.T1.1.1.13.3" class="ltx_td ltx_align_center" style="background-color:#FFE8E8;"><span id="S5.T1.1.1.13.3.1" class="ltx_text" style="background-color:#FFE8E8;">69.8</span></td>
<td id="S5.T1.1.1.13.4" class="ltx_td ltx_align_center">44.5</td>
<td id="S5.T1.1.1.13.5" class="ltx_td ltx_nopad_r ltx_align_center">45.5</td>
</tr>
<tr id="S5.T1.1.1.14" class="ltx_tr">
<td id="S5.T1.1.1.14.1" class="ltx_td ltx_align_left">CSS <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>*</td>
<td id="S5.T1.1.1.14.2" class="ltx_td ltx_align_center">58.9</td>
<td id="S5.T1.1.1.14.3" class="ltx_td ltx_align_center">84.4</td>
<td id="S5.T1.1.1.14.4" class="ltx_td ltx_align_center">49.4</td>
<td id="S5.T1.1.1.14.5" class="ltx_td ltx_nopad_r ltx_align_center">48.2</td>
</tr>
<tr id="S5.T1.1.1.15" class="ltx_tr">
<td id="S5.T1.1.1.15.1" class="ltx_td ltx_align_left">MUTANT <cite class="ltx_cite ltx_citemacro_cite">Gokhale et al. (<a href="#bib.bib21" title="" class="ltx_ref">2020a</a>)</cite>*</td>
<td id="S5.T1.1.1.15.2" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.15.2.1" class="ltx_text ltx_font_bold">69.5</span></td>
<td id="S5.T1.1.1.15.3" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.15.3.1" class="ltx_text ltx_font_bold">93.2</span></td>
<td id="S5.T1.1.1.15.4" class="ltx_td ltx_align_center"><span id="S5.T1.1.1.15.4.1" class="ltx_text ltx_font_bold">67.2</span></td>
<td id="S5.T1.1.1.15.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T1.1.1.15.5.1" class="ltx_text ltx_font_bold">57.8</span></td>
</tr>
<tr id="S5.T1.1.1.16" class="ltx_tr">
<td id="S5.T1.1.1.16.1" class="ltx_td ltx_align_left ltx_border_t">ZSL+Objects+UpDown</td>
<td id="S5.T1.1.1.16.2" class="ltx_td ltx_align_center ltx_border_t">40.8</td>
<td id="S5.T1.1.1.16.3" class="ltx_td ltx_align_center ltx_border_t">67.4</td>
<td id="S5.T1.1.1.16.4" class="ltx_td ltx_align_center ltx_border_t">28.6</td>
<td id="S5.T1.1.1.16.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">30.2</td>
</tr>
<tr id="S5.T1.1.1.17" class="ltx_tr">
<td id="S5.T1.1.1.17.1" class="ltx_td ltx_align_left">ZSL+Patches+UpDown</td>
<td id="S5.T1.1.1.17.2" class="ltx_td ltx_align_center">41.2</td>
<td id="S5.T1.1.1.17.3" class="ltx_td ltx_align_center">68.5</td>
<td id="S5.T1.1.1.17.4" class="ltx_td ltx_align_center">29.8</td>
<td id="S5.T1.1.1.17.5" class="ltx_td ltx_nopad_r ltx_align_center">30.0</td>
</tr>
<tr id="S5.T1.1.1.18" class="ltx_tr">
<td id="S5.T1.1.1.18.1" class="ltx_td ltx_align_left ltx_border_bb">ZSL+Patches+Encoder</td>
<td id="S5.T1.1.1.18.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.1.1.18.2.1" class="ltx_text ltx_framed ltx_framed_underline">47.3</span></td>
<td id="S5.T1.1.1.18.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.1.1.18.3.1" class="ltx_text ltx_framed ltx_framed_underline">73.4</span></td>
<td id="S5.T1.1.1.18.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.1.1.18.4.1" class="ltx_text ltx_framed ltx_framed_underline">39.8</span></td>
<td id="S5.T1.1.1.18.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S5.T1.1.1.18.5.1" class="ltx_text ltx_framed ltx_framed_underline">35.6</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>
Unsupervised accuracy on VQA-CP-v2 test set.
All baselines are <span id="S5.T1.2.1" class="ltx_text ltx_font_italic">supervised</span> methods trained on the train split. * use further additional supervised training samples.
Cyan: our model is better overall.
Red: our model is better on specific categories.<span id="footnotex2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>
ZSL refers to zero-shot transfer setting and FSL refers to our models further finetuned on the respective train split.
<span id="footnotex2.1" class="ltx_text ltx_framed ltx_framed_underline">Underline</span><math id="footnotex4.m1a.1" class="ltx_Math" alttext="{\Rightarrow}" display="inline"><semantics id="footnotex4.m1a.1c"><mo stretchy="false" id="footnotex4.m1a.1.1" xref="footnotex4.m1a.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="footnotex4.m1a.1d"><ci id="footnotex4.m1a.1.1.cmml" xref="footnotex4.m1a.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="footnotex4.m1a.1e">{\Rightarrow}</annotation></semantics></math>unsupervised best, <span id="footnotex4.1" class="ltx_text ltx_font_bold">bold<math id="footnotex4.1.m1a.1" class="ltx_Math" alttext="{\Rightarrow}" display="inline"><semantics id="footnotex4.1.m1a.1c"><mo stretchy="false" id="footnotex4.1.m1a.1.1" xref="footnotex4.1.m1a.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="footnotex4.1.m1a.1d"><ci id="footnotex4.1.m1a.1.1.cmml" xref="footnotex4.1.m1a.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="footnotex4.1.m1a.1e">{\Rightarrow}</annotation></semantics></math></span>overall best. Baselines are trained on train-split, our models on synthetic data.</span></span></span>
</figcaption>
</figure>
<figure id="S5.T2" class="ltx_table">
<div id="S5.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:232.6pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-19.0pt,10.1pt) scale(0.919503261440766,0.919503261440766) ;">
<table id="S5.T2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<td id="S5.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S5.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S5.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">All</span></td>
<td id="S5.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Yes-No</span></td>
<td id="S5.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">Num</span></td>
<td id="S5.T2.1.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt"><span id="S5.T2.1.1.1.5.1" class="ltx_text ltx_font_bold">Others</span></td>
</tr>
<tr id="S5.T2.1.1.2" class="ltx_tr">
<td id="S5.T2.1.1.2.1" class="ltx_td ltx_align_left ltx_border_tt">GVQA <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S5.T2.1.1.2.2" class="ltx_td ltx_align_center ltx_border_tt">48.2</td>
<td id="S5.T2.1.1.2.3" class="ltx_td ltx_align_center ltx_border_tt">72.0</td>
<td id="S5.T2.1.1.2.4" class="ltx_td ltx_align_center ltx_border_tt">31.1</td>
<td id="S5.T2.1.1.2.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">34.7</td>
</tr>
<tr id="S5.T2.1.1.3" class="ltx_tr">
<td id="S5.T2.1.1.3.1" class="ltx_td ltx_align_left">UpDown <cite class="ltx_cite ltx_citemacro_cite">Anderson et al. (<a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S5.T2.1.1.3.2" class="ltx_td ltx_align_center">65.3</td>
<td id="S5.T2.1.1.3.3" class="ltx_td ltx_align_center">81.8</td>
<td id="S5.T2.1.1.3.4" class="ltx_td ltx_align_center">44.2</td>
<td id="S5.T2.1.1.3.5" class="ltx_td ltx_nopad_r ltx_align_center">56.1</td>
</tr>
<tr id="S5.T2.1.1.4" class="ltx_tr">
<td id="S5.T2.1.1.4.1" class="ltx_td ltx_align_left">RUBi <cite class="ltx_cite ltx_citemacro_cite">Cadène et al. (<a href="#bib.bib8" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S5.T2.1.1.4.2" class="ltx_td ltx_align_center">63.1</td>
<td id="S5.T2.1.1.4.3" class="ltx_td ltx_align_center">*</td>
<td id="S5.T2.1.1.4.4" class="ltx_td ltx_align_center">*</td>
<td id="S5.T2.1.1.4.5" class="ltx_td ltx_nopad_r ltx_align_center">*</td>
</tr>
<tr id="S5.T2.1.1.5" class="ltx_tr">
<td id="S5.T2.1.1.5.1" class="ltx_td ltx_align_left">MCAN <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib88" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S5.T2.1.1.5.2" class="ltx_td ltx_align_center">70.4</td>
<td id="S5.T2.1.1.5.3" class="ltx_td ltx_align_center">85.8</td>
<td id="S5.T2.1.1.5.4" class="ltx_td ltx_align_center">53.7</td>
<td id="S5.T2.1.1.5.5" class="ltx_td ltx_nopad_r ltx_align_center">60.7</td>
</tr>
<tr id="S5.T2.1.1.6" class="ltx_tr">
<td id="S5.T2.1.1.6.1" class="ltx_td ltx_align_left">VilBERT <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib49" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S5.T2.1.1.6.2" class="ltx_td ltx_align_center">70.5</td>
<td id="S5.T2.1.1.6.3" class="ltx_td ltx_align_center">*</td>
<td id="S5.T2.1.1.6.4" class="ltx_td ltx_align_center">*</td>
<td id="S5.T2.1.1.6.5" class="ltx_td ltx_nopad_r ltx_align_center">*</td>
</tr>
<tr id="S5.T2.1.1.7" class="ltx_tr">
<td id="S5.T2.1.1.7.1" class="ltx_td ltx_align_left">LXMERT <cite class="ltx_cite ltx_citemacro_cite">Tan and Bansal (<a href="#bib.bib74" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S5.T2.1.1.7.2" class="ltx_td ltx_align_center">72.5</td>
<td id="S5.T2.1.1.7.3" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.7.3.1" class="ltx_text ltx_font_bold">88.2</span></td>
<td id="S5.T2.1.1.7.4" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.7.4.1" class="ltx_text ltx_font_bold">54.2</span></td>
<td id="S5.T2.1.1.7.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.7.5.1" class="ltx_text ltx_font_bold">63.1</span></td>
</tr>
<tr id="S5.T2.1.1.8" class="ltx_tr">
<td id="S5.T2.1.1.8.1" class="ltx_td ltx_align_left">UNITER <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib12" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S5.T2.1.1.8.2" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.8.2.1" class="ltx_text ltx_font_bold">72.7</span></td>
<td id="S5.T2.1.1.8.3" class="ltx_td ltx_align_center">*</td>
<td id="S5.T2.1.1.8.4" class="ltx_td ltx_align_center">*</td>
<td id="S5.T2.1.1.8.5" class="ltx_td ltx_nopad_r ltx_align_center">*</td>
</tr>
<tr id="S5.T2.1.1.9" class="ltx_tr">
<td id="S5.T2.1.1.9.1" class="ltx_td ltx_align_left ltx_border_t">ZSL + Objects + UpDown</td>
<td id="S5.T2.1.1.9.2" class="ltx_td ltx_align_center ltx_border_t">41.4</td>
<td id="S5.T2.1.1.9.3" class="ltx_td ltx_align_center ltx_border_t">68.1</td>
<td id="S5.T2.1.1.9.4" class="ltx_td ltx_align_center ltx_border_t">27.6</td>
<td id="S5.T2.1.1.9.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">29.4</td>
</tr>
<tr id="S5.T2.1.1.10" class="ltx_tr">
<td id="S5.T2.1.1.10.1" class="ltx_td ltx_align_left">ZSL + Patches + UpDown</td>
<td id="S5.T2.1.1.10.2" class="ltx_td ltx_align_center">40.6</td>
<td id="S5.T2.1.1.10.3" class="ltx_td ltx_align_center">67.8</td>
<td id="S5.T2.1.1.10.4" class="ltx_td ltx_align_center">28.4</td>
<td id="S5.T2.1.1.10.5" class="ltx_td ltx_nopad_r ltx_align_center">29.2</td>
</tr>
<tr id="S5.T2.1.1.11" class="ltx_tr">
<td id="S5.T2.1.1.11.1" class="ltx_td ltx_align_left">ZSL + Patches + Encoder</td>
<td id="S5.T2.1.1.11.2" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.11.2.1" class="ltx_text ltx_framed ltx_framed_underline">46.8</span></td>
<td id="S5.T2.1.1.11.3" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.11.3.1" class="ltx_text ltx_framed ltx_framed_underline">72.1</span></td>
<td id="S5.T2.1.1.11.4" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.11.4.1" class="ltx_text ltx_framed ltx_framed_underline">34.4</span></td>
<td id="S5.T2.1.1.11.5" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T2.1.1.11.5.1" class="ltx_text ltx_framed ltx_framed_underline">34.1</span></td>
</tr>
<tr id="S5.T2.1.1.12" class="ltx_tr">
<td id="S5.T2.1.1.12.1" class="ltx_td ltx_align_left ltx_border_t">FSL + Objects + UpDown</td>
<td id="S5.T2.1.1.12.2" class="ltx_td ltx_align_center ltx_border_t">66.8**</td>
<td id="S5.T2.1.1.12.3" class="ltx_td ltx_align_center ltx_border_t">82.4**</td>
<td id="S5.T2.1.1.12.4" class="ltx_td ltx_align_center ltx_border_t">45.1**</td>
<td id="S5.T2.1.1.12.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">56.4**</td>
</tr>
<tr id="S5.T2.1.1.13" class="ltx_tr">
<td id="S5.T2.1.1.13.1" class="ltx_td ltx_align_left">FSL + Patches + UpDown</td>
<td id="S5.T2.1.1.13.2" class="ltx_td ltx_align_center">63.4</td>
<td id="S5.T2.1.1.13.3" class="ltx_td ltx_align_center">80.2</td>
<td id="S5.T2.1.1.13.4" class="ltx_td ltx_align_center">45.2</td>
<td id="S5.T2.1.1.13.5" class="ltx_td ltx_nopad_r ltx_align_center">52.1</td>
</tr>
<tr id="S5.T2.1.1.14" class="ltx_tr">
<td id="S5.T2.1.1.14.1" class="ltx_td ltx_align_left ltx_border_bb">FSL + Patches + Encoder</td>
<td id="S5.T2.1.1.14.2" class="ltx_td ltx_align_center ltx_border_bb">65.3</td>
<td id="S5.T2.1.1.14.3" class="ltx_td ltx_align_center ltx_border_bb">80.5</td>
<td id="S5.T2.1.1.14.4" class="ltx_td ltx_align_center ltx_border_bb">48.94</td>
<td id="S5.T2.1.1.14.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">56.2</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>VQA-v2 Test-standard accuracies<span id="footnotex3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>
ZSL refers to zero-shot transfer setting and FSL refers to our models further finetuned on the respective train split.
<span id="footnotex3.1" class="ltx_text ltx_framed ltx_framed_underline">Underline</span><math id="footnotex4.m1b.1" class="ltx_Math" alttext="{\Rightarrow}" display="inline"><semantics id="footnotex4.m1b.1c"><mo stretchy="false" id="footnotex4.m1b.1.1" xref="footnotex4.m1b.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="footnotex4.m1b.1d"><ci id="footnotex4.m1b.1.1.cmml" xref="footnotex4.m1b.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="footnotex4.m1b.1e">{\Rightarrow}</annotation></semantics></math>unsupervised best, <span id="footnotex4.1a" class="ltx_text ltx_font_bold">bold<math id="footnotex4.1.m1b.1" class="ltx_Math" alttext="{\Rightarrow}" display="inline"><semantics id="footnotex4.1.m1b.1c"><mo stretchy="false" id="footnotex4.1.m1b.1.1" xref="footnotex4.1.m1b.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="footnotex4.1.m1b.1d"><ci id="footnotex4.1.m1b.1.1.cmml" xref="footnotex4.1.m1b.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="footnotex4.1.m1b.1e">{\Rightarrow}</annotation></semantics></math></span>overall best. Baselines are trained on train-split, our models on synthetic data.</span></span></span>. FSL models are pretrained on synthetic samples, and further finetuned on VQA-v2 train split. * - Scores are not available, ** - Validation split scores. </figcaption>
</figure>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T3.1.1" class="ltx_tr">
<td id="S5.T3.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S5.T3.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S5.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.1.1.2.1" class="ltx_text ltx_font_bold">All</span></td>
<td id="S5.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.1.1.3.1" class="ltx_text ltx_font_bold">Binary</span></td>
<td id="S5.T3.1.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt"><span id="S5.T3.1.1.4.1" class="ltx_text ltx_font_bold">Open</span></td>
</tr>
<tr id="S5.T3.1.2" class="ltx_tr">
<td id="S5.T3.1.2.1" class="ltx_td ltx_align_left ltx_border_tt">CNN + LSTM <cite class="ltx_cite ltx_citemacro_cite">Hudson and Manning (<a href="#bib.bib30" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S5.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_tt">46.6</td>
<td id="S5.T3.1.2.3" class="ltx_td ltx_align_center ltx_border_tt">61.9</td>
<td id="S5.T3.1.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">22.7</td>
</tr>
<tr id="S5.T3.1.3" class="ltx_tr">
<td id="S5.T3.1.3.1" class="ltx_td ltx_align_left">UpDown <cite class="ltx_cite ltx_citemacro_cite">Anderson et al. (<a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S5.T3.1.3.2" class="ltx_td ltx_align_center">49.7</td>
<td id="S5.T3.1.3.3" class="ltx_td ltx_align_center">66.6</td>
<td id="S5.T3.1.3.4" class="ltx_td ltx_nopad_r ltx_align_center">34.8</td>
</tr>
<tr id="S5.T3.1.4" class="ltx_tr">
<td id="S5.T3.1.4.1" class="ltx_td ltx_align_left">MAC <cite class="ltx_cite ltx_citemacro_cite">Hudson and Manning (<a href="#bib.bib30" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S5.T3.1.4.2" class="ltx_td ltx_align_center">54.1</td>
<td id="S5.T3.1.4.3" class="ltx_td ltx_align_center">71.2</td>
<td id="S5.T3.1.4.4" class="ltx_td ltx_nopad_r ltx_align_center">38.9</td>
</tr>
<tr id="S5.T3.1.5" class="ltx_tr">
<td id="S5.T3.1.5.1" class="ltx_td ltx_align_left">BAN <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib38" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="S5.T3.1.5.2" class="ltx_td ltx_align_center">57.1</td>
<td id="S5.T3.1.5.3" class="ltx_td ltx_align_center">76.0</td>
<td id="S5.T3.1.5.4" class="ltx_td ltx_nopad_r ltx_align_center">40.4</td>
</tr>
<tr id="S5.T3.1.6" class="ltx_tr">
<td id="S5.T3.1.6.1" class="ltx_td ltx_align_left">LXMERT <cite class="ltx_cite ltx_citemacro_cite">Tan and Bansal (<a href="#bib.bib74" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S5.T3.1.6.2" class="ltx_td ltx_align_center"><span id="S5.T3.1.6.2.1" class="ltx_text ltx_font_bold">60.3</span></td>
<td id="S5.T3.1.6.3" class="ltx_td ltx_align_center"><span id="S5.T3.1.6.3.1" class="ltx_text ltx_font_bold">77.8</span></td>
<td id="S5.T3.1.6.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T3.1.6.4.1" class="ltx_text ltx_font_bold">45.0</span></td>
</tr>
<tr id="S5.T3.1.7" class="ltx_tr">
<td id="S5.T3.1.7.1" class="ltx_td ltx_align_left ltx_border_t">ZSL + Objects + UpDown</td>
<td id="S5.T3.1.7.2" class="ltx_td ltx_align_center ltx_border_t">30.7</td>
<td id="S5.T3.1.7.3" class="ltx_td ltx_align_center ltx_border_t">50.8</td>
<td id="S5.T3.1.7.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">17.6</td>
</tr>
<tr id="S5.T3.1.8" class="ltx_tr">
<td id="S5.T3.1.8.1" class="ltx_td ltx_align_left">ZSL + Patches + UpDown</td>
<td id="S5.T3.1.8.2" class="ltx_td ltx_align_center">31.1</td>
<td id="S5.T3.1.8.3" class="ltx_td ltx_align_center">52.3</td>
<td id="S5.T3.1.8.4" class="ltx_td ltx_nopad_r ltx_align_center">16.8</td>
</tr>
<tr id="S5.T3.1.9" class="ltx_tr">
<td id="S5.T3.1.9.1" class="ltx_td ltx_align_left">ZSL + Patches + Encoder</td>
<td id="S5.T3.1.9.2" class="ltx_td ltx_align_center"><span id="S5.T3.1.9.2.1" class="ltx_text ltx_framed ltx_framed_underline">33.7</span></td>
<td id="S5.T3.1.9.3" class="ltx_td ltx_align_center"><span id="S5.T3.1.9.3.1" class="ltx_text ltx_framed ltx_framed_underline">55.5</span></td>
<td id="S5.T3.1.9.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S5.T3.1.9.4.1" class="ltx_text ltx_framed ltx_framed_underline">21.2</span></td>
</tr>
<tr id="S5.T3.1.10" class="ltx_tr">
<td id="S5.T3.1.10.1" class="ltx_td ltx_align_left ltx_border_t">FSL + Objects + UpDown</td>
<td id="S5.T3.1.10.2" class="ltx_td ltx_align_center ltx_border_t">50.4</td>
<td id="S5.T3.1.10.3" class="ltx_td ltx_align_center ltx_border_t">67.5</td>
<td id="S5.T3.1.10.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">35.1</td>
</tr>
<tr id="S5.T3.1.11" class="ltx_tr">
<td id="S5.T3.1.11.1" class="ltx_td ltx_align_left">FSL + Patches + UpDown</td>
<td id="S5.T3.1.11.2" class="ltx_td ltx_align_center">46.4</td>
<td id="S5.T3.1.11.3" class="ltx_td ltx_align_center">64.3</td>
<td id="S5.T3.1.11.4" class="ltx_td ltx_nopad_r ltx_align_center">31.4</td>
</tr>
<tr id="S5.T3.1.12" class="ltx_tr">
<td id="S5.T3.1.12.1" class="ltx_td ltx_align_left ltx_border_bb">FSL + Patches + Encoder</td>
<td id="S5.T3.1.12.2" class="ltx_td ltx_align_center ltx_border_bb">55.2</td>
<td id="S5.T3.1.12.3" class="ltx_td ltx_align_center ltx_border_bb">73.6</td>
<td id="S5.T3.1.12.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">38.8</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>GQA Validation split accuracies.<span id="footnotex5" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">2</span></span></span></span></figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results<span id="footnotex6" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">2</span></span></span></span>
</h2>

<section id="S6.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Unsupervised Question Answering:</h4>

<div id="S6.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px1.p1.7" class="ltx_p">Tables <a href="#footnotex2" title="footnote 2 ‣ Table 1 ‣ Baselines: ‣ 5 Experimental Setup ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, <a href="#footnotex3" title="footnote 2 ‣ Table 2 ‣ Baselines: ‣ 5 Experimental Setup ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#footnotex5" title="footnote 2 ‣ Table 3 ‣ Baselines: ‣ 5 Experimental Setup ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> summarize our results on the three benchmark datasets.
We can observe that our method outperforms specially designed supervised methods for bias removal in VQA-CP; our model with UpDown is <math id="S6.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="1.1\%" display="inline"><semantics id="S6.SS0.SSS0.Px1.p1.1.m1.1a"><mrow id="S6.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">1.1</mn><mo id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.1" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="latexml" id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2">1.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p1.1.m1.1c">1.1\%</annotation></semantics></math> better than the supervised UpDown.
Under the ZSL setting for VQA-CP, our Encoder model is <math id="S6.SS0.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="6.1\%" display="inline"><semantics id="S6.SS0.SSS0.Px1.p1.2.m2.1a"><mrow id="S6.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S6.SS0.SSS0.Px1.p1.2.m2.1.1.cmml"><mn id="S6.SS0.SSS0.Px1.p1.2.m2.1.1.2" xref="S6.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml">6.1</mn><mo id="S6.SS0.SSS0.Px1.p1.2.m2.1.1.1" xref="S6.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="S6.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="latexml" id="S6.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S6.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S6.SS0.SSS0.Px1.p1.2.m2.1.1.2">6.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p1.2.m2.1c">6.1\%</annotation></semantics></math> better than UpDown with patches, and <math id="S6.SS0.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="6.5\%" display="inline"><semantics id="S6.SS0.SSS0.Px1.p1.3.m3.1a"><mrow id="S6.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S6.SS0.SSS0.Px1.p1.3.m3.1.1.cmml"><mn id="S6.SS0.SSS0.Px1.p1.3.m3.1.1.2" xref="S6.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml">6.5</mn><mo id="S6.SS0.SSS0.Px1.p1.3.m3.1.1.1" xref="S6.SS0.SSS0.Px1.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p1.3.m3.1b"><apply id="S6.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="latexml" id="S6.SS0.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.3.m3.1.1.1">percent</csymbol><cn type="float" id="S6.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S6.SS0.SSS0.Px1.p1.3.m3.1.1.2">6.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p1.3.m3.1c">6.5\%</annotation></semantics></math> better than UpDown with Object features, for VQA-v2: <math id="S6.SS0.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="6.2\%" display="inline"><semantics id="S6.SS0.SSS0.Px1.p1.4.m4.1a"><mrow id="S6.SS0.SSS0.Px1.p1.4.m4.1.1" xref="S6.SS0.SSS0.Px1.p1.4.m4.1.1.cmml"><mn id="S6.SS0.SSS0.Px1.p1.4.m4.1.1.2" xref="S6.SS0.SSS0.Px1.p1.4.m4.1.1.2.cmml">6.2</mn><mo id="S6.SS0.SSS0.Px1.p1.4.m4.1.1.1" xref="S6.SS0.SSS0.Px1.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p1.4.m4.1b"><apply id="S6.SS0.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="latexml" id="S6.SS0.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.4.m4.1.1.1">percent</csymbol><cn type="float" id="S6.SS0.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S6.SS0.SSS0.Px1.p1.4.m4.1.1.2">6.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p1.4.m4.1c">6.2\%</annotation></semantics></math>, <math id="S6.SS0.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="5.4\%" display="inline"><semantics id="S6.SS0.SSS0.Px1.p1.5.m5.1a"><mrow id="S6.SS0.SSS0.Px1.p1.5.m5.1.1" xref="S6.SS0.SSS0.Px1.p1.5.m5.1.1.cmml"><mn id="S6.SS0.SSS0.Px1.p1.5.m5.1.1.2" xref="S6.SS0.SSS0.Px1.p1.5.m5.1.1.2.cmml">5.4</mn><mo id="S6.SS0.SSS0.Px1.p1.5.m5.1.1.1" xref="S6.SS0.SSS0.Px1.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p1.5.m5.1b"><apply id="S6.SS0.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.5.m5.1.1"><csymbol cd="latexml" id="S6.SS0.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.5.m5.1.1.1">percent</csymbol><cn type="float" id="S6.SS0.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="S6.SS0.SSS0.Px1.p1.5.m5.1.1.2">5.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p1.5.m5.1c">5.4\%</annotation></semantics></math> respectively, and for GQA: <math id="S6.SS0.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="2.2\%" display="inline"><semantics id="S6.SS0.SSS0.Px1.p1.6.m6.1a"><mrow id="S6.SS0.SSS0.Px1.p1.6.m6.1.1" xref="S6.SS0.SSS0.Px1.p1.6.m6.1.1.cmml"><mn id="S6.SS0.SSS0.Px1.p1.6.m6.1.1.2" xref="S6.SS0.SSS0.Px1.p1.6.m6.1.1.2.cmml">2.2</mn><mo id="S6.SS0.SSS0.Px1.p1.6.m6.1.1.1" xref="S6.SS0.SSS0.Px1.p1.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p1.6.m6.1b"><apply id="S6.SS0.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.6.m6.1.1"><csymbol cd="latexml" id="S6.SS0.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.6.m6.1.1.1">percent</csymbol><cn type="float" id="S6.SS0.SSS0.Px1.p1.6.m6.1.1.2.cmml" xref="S6.SS0.SSS0.Px1.p1.6.m6.1.1.2">2.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p1.6.m6.1c">2.2\%</annotation></semantics></math>, <math id="S6.SS0.SSS0.Px1.p1.7.m7.1" class="ltx_Math" alttext="3.0\%" display="inline"><semantics id="S6.SS0.SSS0.Px1.p1.7.m7.1a"><mrow id="S6.SS0.SSS0.Px1.p1.7.m7.1.1" xref="S6.SS0.SSS0.Px1.p1.7.m7.1.1.cmml"><mn id="S6.SS0.SSS0.Px1.p1.7.m7.1.1.2" xref="S6.SS0.SSS0.Px1.p1.7.m7.1.1.2.cmml">3.0</mn><mo id="S6.SS0.SSS0.Px1.p1.7.m7.1.1.1" xref="S6.SS0.SSS0.Px1.p1.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p1.7.m7.1b"><apply id="S6.SS0.SSS0.Px1.p1.7.m7.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.7.m7.1.1"><csymbol cd="latexml" id="S6.SS0.SSS0.Px1.p1.7.m7.1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.7.m7.1.1.1">percent</csymbol><cn type="float" id="S6.SS0.SSS0.Px1.p1.7.m7.1.1.2.cmml" xref="S6.SS0.SSS0.Px1.p1.7.m7.1.1.2">3.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p1.7.m7.1c">3.0\%</annotation></semantics></math> respectively.</p>
</div>
<div id="S6.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S6.SS0.SSS0.Px1.p2.1" class="ltx_p">For VQA-CP, our procedurally generated Q-A pairs and patch-features when used with either UpDown or Encoder are better than the baseline supervised UpDown model, showing the improvements are model-agnostic.
This also shows the merits of using our Q-A generation methods when train and test-sets deviate linguistically.</p>
</div>
<div id="S6.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S6.SS0.SSS0.Px1.p3.1" class="ltx_p">Most GQA questions require understanding spatial relationships between objects.
Such questions are infrequent in our synthetic training data since captions do not contain detailed spatial relationships among objects.
Thus, the ZSL performance is not as competitive for GQA when compared to our performance on VQA and VQA-CP.
Improving spatial and compositional question-answering with weak supervision is an interesting future pursuit.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Fully Supervised Question Answering:</h4>

<div id="S6.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px2.p1.1" class="ltx_p">In the FSL setting, our methods’ performance is not far from SOTA methods, even though our method uses significantly fewer annotations (no access to object bounding boxes).
In GQA, the Encoder model performs on par with MAC <cite class="ltx_cite ltx_citemacro_cite">Hudson and Manning (<a href="#bib.bib30" title="" class="ltx_ref">2018</a>)</cite> and BAN <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib38" title="" class="ltx_ref">2018</a>)</cite>, which unlike us, use object relationship annotations.
This suggests that cross-modal transformer layers can learn spatial relations from spatial pyramidal features.</p>
</div>
<figure id="S6.T4" class="ltx_table">
<div id="S6.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:256.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(80.0pt,-47.3pt) scale(1.58443909143806,1.58443909143806) ;">
<table id="S6.T4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S6.T4.1.1.1" class="ltx_tr">
<td id="S6.T4.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S6.T4.1.1.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Question Generation</span></td>
<td id="S6.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">VQA-v2</span></td>
<td id="S6.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T4.1.1.1.4.1" class="ltx_text ltx_font_bold">VQA-CP</span></td>
<td id="S6.T4.1.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt"><span id="S6.T4.1.1.1.5.1" class="ltx_text ltx_font_bold">GQA</span></td>
</tr>
<tr id="S6.T4.1.1.2" class="ltx_tr">
<td id="S6.T4.1.1.2.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="4"><span id="S6.T4.1.1.2.1.1" class="ltx_text">
<span id="S6.T4.1.1.2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.9pt;height:24.4pt;vertical-align:-9.7pt;"><span class="ltx_transformed_inner" style="width:24.4pt;transform:translate(-7.78pt,2.92pt) rotate(-90deg) ;">
<span id="S6.T4.1.1.2.1.1.1.1" class="ltx_p">Updn</span>
</span></span></span></td>
<td id="S6.T4.1.1.2.2" class="ltx_td ltx_align_left ltx_border_tt">Template</td>
<td id="S6.T4.1.1.2.3" class="ltx_td ltx_align_center ltx_border_tt">26.2</td>
<td id="S6.T4.1.1.2.4" class="ltx_td ltx_align_center ltx_border_tt">25.7</td>
<td id="S6.T4.1.1.2.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">11.6</td>
</tr>
<tr id="S6.T4.1.1.3" class="ltx_tr">
<td id="S6.T4.1.1.3.1" class="ltx_td ltx_align_left">Template + Para&amp;Back</td>
<td id="S6.T4.1.1.3.2" class="ltx_td ltx_align_center">28.5</td>
<td id="S6.T4.1.1.3.3" class="ltx_td ltx_align_center">27.1</td>
<td id="S6.T4.1.1.3.4" class="ltx_td ltx_nopad_r ltx_align_center">14.8</td>
</tr>
<tr id="S6.T4.1.1.4" class="ltx_tr">
<td id="S6.T4.1.1.4.1" class="ltx_td ltx_align_left">QA-SRL</td>
<td id="S6.T4.1.1.4.2" class="ltx_td ltx_align_center">31.1</td>
<td id="S6.T4.1.1.4.3" class="ltx_td ltx_align_center">33.8</td>
<td id="S6.T4.1.1.4.4" class="ltx_td ltx_nopad_r ltx_align_center">18.9</td>
</tr>
<tr id="S6.T4.1.1.5" class="ltx_tr">
<td id="S6.T4.1.1.5.1" class="ltx_td ltx_align_left">All</td>
<td id="S6.T4.1.1.5.2" class="ltx_td ltx_align_center">41.4</td>
<td id="S6.T4.1.1.5.3" class="ltx_td ltx_align_center">40.2</td>
<td id="S6.T4.1.1.5.4" class="ltx_td ltx_nopad_r ltx_align_center">31.1</td>
</tr>
<tr id="S6.T4.1.1.6" class="ltx_tr">
<td id="S6.T4.1.1.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="4"><span id="S6.T4.1.1.6.1.1" class="ltx_text">
<span id="S6.T4.1.1.6.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:36pt;vertical-align:-14.5pt;"><span class="ltx_transformed_inner" style="width:36.0pt;transform:translate(-14.53pt,0pt) rotate(-90deg) ;">
<span id="S6.T4.1.1.6.1.1.1.1" class="ltx_p">Encoder</span>
</span></span></span></td>
<td id="S6.T4.1.1.6.2" class="ltx_td ltx_align_left ltx_border_t">Template</td>
<td id="S6.T4.1.1.6.3" class="ltx_td ltx_align_center ltx_border_t">32.5</td>
<td id="S6.T4.1.1.6.4" class="ltx_td ltx_align_center ltx_border_t">31.3</td>
<td id="S6.T4.1.1.6.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">18.5</td>
</tr>
<tr id="S6.T4.1.1.7" class="ltx_tr">
<td id="S6.T4.1.1.7.1" class="ltx_td ltx_align_left">Template + Para&amp;Back</td>
<td id="S6.T4.1.1.7.2" class="ltx_td ltx_align_center">34.8</td>
<td id="S6.T4.1.1.7.3" class="ltx_td ltx_align_center">33.6</td>
<td id="S6.T4.1.1.7.4" class="ltx_td ltx_nopad_r ltx_align_center">23.6</td>
</tr>
<tr id="S6.T4.1.1.8" class="ltx_tr">
<td id="S6.T4.1.1.8.1" class="ltx_td ltx_align_left">QA-SRL</td>
<td id="S6.T4.1.1.8.2" class="ltx_td ltx_align_center">40.3</td>
<td id="S6.T4.1.1.8.3" class="ltx_td ltx_align_center">39.8</td>
<td id="S6.T4.1.1.8.4" class="ltx_td ltx_nopad_r ltx_align_center">21.4</td>
</tr>
<tr id="S6.T4.1.1.9" class="ltx_tr">
<td id="S6.T4.1.1.9.1" class="ltx_td ltx_align_left ltx_border_bb">All</td>
<td id="S6.T4.1.1.9.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T4.1.1.9.2.1" class="ltx_text ltx_font_bold">47.1</span></td>
<td id="S6.T4.1.1.9.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T4.1.1.9.3.1" class="ltx_text ltx_font_bold">46.8</span></td>
<td id="S6.T4.1.1.9.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S6.T4.1.1.9.4.1" class="ltx_text ltx_font_bold">33.7</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Effect of different pre-training data sources on ZSL Validation split accuracies.</figcaption>
</figure>
</section>
<section id="S6.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Impact of each question-generation technique:</h4>

<div id="S6.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px3.p1.1" class="ltx_p">In Table <a href="#S6.T4" title="Table 4 ‣ Fully Supervised Question Answering: ‣ 6 Results2footnote 22footnote 2 ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> we can observe the effect of different question generation techniques.
All models use spatial image patch features. QA-SRL based questions and the SWA-Loss contribute the most towards gains in performance, and the paraphrased questions provide larger linguistic variation.</p>
</div>
<figure id="S6.T5" class="ltx_table">
<table id="S6.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T5.1.1" class="ltx_tr">
<td id="S6.T5.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S6.T5.1.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T5.1.1.2.1" class="ltx_text ltx_font_bold">Patch Resolutions</span></td>
<td id="S6.T5.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T5.1.1.3.1" class="ltx_text ltx_font_bold">VQA-v2</span></td>
<td id="S6.T5.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T5.1.1.4.1" class="ltx_text ltx_font_bold">VQA-CP</span></td>
<td id="S6.T5.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt"><span id="S6.T5.1.1.5.1" class="ltx_text ltx_font_bold">GQA</span></td>
</tr>
<tr id="S6.T5.1.2" class="ltx_tr">
<td id="S6.T5.1.2.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="5"><span id="S6.T5.1.2.1.1" class="ltx_text">
<span id="S6.T5.1.2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:26.3pt;vertical-align:-10.7pt;"><span class="ltx_transformed_inner" style="width:26.3pt;transform:translate(-8.74pt,2.92pt) rotate(-90deg) ;">
<span id="S6.T5.1.2.1.1.1.1" class="ltx_p">UpDn</span>
</span></span></span></td>
<td id="S6.T5.1.2.2" class="ltx_td ltx_align_left ltx_border_t">{1}</td>
<td id="S6.T5.1.2.3" class="ltx_td ltx_align_center ltx_border_t">18.8</td>
<td id="S6.T5.1.2.4" class="ltx_td ltx_align_center ltx_border_t">19.7</td>
<td id="S6.T5.1.2.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">11.3</td>
</tr>
<tr id="S6.T5.1.3" class="ltx_tr">
<td id="S6.T5.1.3.1" class="ltx_td ltx_align_left">{1, 3}</td>
<td id="S6.T5.1.3.2" class="ltx_td ltx_align_center">36.7</td>
<td id="S6.T5.1.3.3" class="ltx_td ltx_align_center">35.9</td>
<td id="S6.T5.1.3.4" class="ltx_td ltx_nopad_r ltx_align_center">24.5</td>
</tr>
<tr id="S6.T5.1.4" class="ltx_tr">
<td id="S6.T5.1.4.1" class="ltx_td ltx_align_left">{1, 3, 5}</td>
<td id="S6.T5.1.4.2" class="ltx_td ltx_align_center">40.1</td>
<td id="S6.T5.1.4.3" class="ltx_td ltx_align_center">39.7</td>
<td id="S6.T5.1.4.4" class="ltx_td ltx_nopad_r ltx_align_center">29.5</td>
</tr>
<tr id="S6.T5.1.5" class="ltx_tr">
<td id="S6.T5.1.5.1" class="ltx_td ltx_align_left">{1, 3, 5, 7}</td>
<td id="S6.T5.1.5.2" class="ltx_td ltx_align_center"><span id="S6.T5.1.5.2.1" class="ltx_text ltx_font_bold">41.4</span></td>
<td id="S6.T5.1.5.3" class="ltx_td ltx_align_center"><span id="S6.T5.1.5.3.1" class="ltx_text ltx_font_bold">40.2</span></td>
<td id="S6.T5.1.5.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.1.5.4.1" class="ltx_text ltx_font_bold">31.1</span></td>
</tr>
<tr id="S6.T5.1.6" class="ltx_tr">
<td id="S6.T5.1.6.1" class="ltx_td ltx_align_left">{1, 3, 5, 7, 9}</td>
<td id="S6.T5.1.6.2" class="ltx_td ltx_align_center">39.8</td>
<td id="S6.T5.1.6.3" class="ltx_td ltx_align_center">38.4</td>
<td id="S6.T5.1.6.4" class="ltx_td ltx_nopad_r ltx_align_center">29.3</td>
</tr>
<tr id="S6.T5.1.7" class="ltx_tr">
<td id="S6.T5.1.7.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="5"><span id="S6.T5.1.7.1.1" class="ltx_text">
<span id="S6.T5.1.7.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:36pt;vertical-align:-14.5pt;"><span class="ltx_transformed_inner" style="width:36.0pt;transform:translate(-14.53pt,0pt) rotate(-90deg) ;">
<span id="S6.T5.1.7.1.1.1.1" class="ltx_p">Encoder</span>
</span></span></span></td>
<td id="S6.T5.1.7.2" class="ltx_td ltx_align_left ltx_border_t">{1}</td>
<td id="S6.T5.1.7.3" class="ltx_td ltx_align_center ltx_border_t">26.4</td>
<td id="S6.T5.1.7.4" class="ltx_td ltx_align_center ltx_border_t">27.7</td>
<td id="S6.T5.1.7.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">15.3</td>
</tr>
<tr id="S6.T5.1.8" class="ltx_tr">
<td id="S6.T5.1.8.1" class="ltx_td ltx_align_left">{1, 3}</td>
<td id="S6.T5.1.8.2" class="ltx_td ltx_align_center">42.6</td>
<td id="S6.T5.1.8.3" class="ltx_td ltx_align_center">43.1</td>
<td id="S6.T5.1.8.4" class="ltx_td ltx_nopad_r ltx_align_center">28.8</td>
</tr>
<tr id="S6.T5.1.9" class="ltx_tr">
<td id="S6.T5.1.9.1" class="ltx_td ltx_align_left">{1, 3, 5}</td>
<td id="S6.T5.1.9.2" class="ltx_td ltx_align_center">44.3</td>
<td id="S6.T5.1.9.3" class="ltx_td ltx_align_center">45.2</td>
<td id="S6.T5.1.9.4" class="ltx_td ltx_nopad_r ltx_align_center">30.9</td>
</tr>
<tr id="S6.T5.1.10" class="ltx_tr">
<td id="S6.T5.1.10.1" class="ltx_td ltx_align_left">{1, 3, 5, 7}</td>
<td id="S6.T5.1.10.2" class="ltx_td ltx_align_center"><span id="S6.T5.1.10.2.1" class="ltx_text ltx_font_bold">47.1</span></td>
<td id="S6.T5.1.10.3" class="ltx_td ltx_align_center"><span id="S6.T5.1.10.3.1" class="ltx_text ltx_font_bold">46.8</span></td>
<td id="S6.T5.1.10.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S6.T5.1.10.4.1" class="ltx_text ltx_font_bold">33.7</span></td>
</tr>
<tr id="S6.T5.1.11" class="ltx_tr">
<td id="S6.T5.1.11.1" class="ltx_td ltx_align_left ltx_border_bb">{1, 3, 5, 7, 9}</td>
<td id="S6.T5.1.11.2" class="ltx_td ltx_align_center ltx_border_bb">46.2</td>
<td id="S6.T5.1.11.3" class="ltx_td ltx_align_center ltx_border_bb">45.4</td>
<td id="S6.T5.1.11.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">31.2</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Effect of the number of spatial patches on ZSL performance
{3,5} implies division of the image into a 3x3 and 5x5 grid of patches.</figcaption>
</figure>
</section>
<section id="S6.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Effect of Spatial Pyramids:</h4>

<div id="S6.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px4.p1.5" class="ltx_p">We study the effect of progressively increasing the number of spatial image patches (i.e., decreasing the patch size).
Table <a href="#S6.T5" title="Table 5 ‣ Impact of each question-generation technique: ‣ 6 Results2footnote 22footnote 2 ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows that an optimum exists at grid-size of <math id="S6.SS0.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="7\times 7" display="inline"><semantics id="S6.SS0.SSS0.Px4.p1.1.m1.1a"><mrow id="S6.SS0.SSS0.Px4.p1.1.m1.1.1" xref="S6.SS0.SSS0.Px4.p1.1.m1.1.1.cmml"><mn id="S6.SS0.SSS0.Px4.p1.1.m1.1.1.2" xref="S6.SS0.SSS0.Px4.p1.1.m1.1.1.2.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S6.SS0.SSS0.Px4.p1.1.m1.1.1.1" xref="S6.SS0.SSS0.Px4.p1.1.m1.1.1.1.cmml">×</mo><mn id="S6.SS0.SSS0.Px4.p1.1.m1.1.1.3" xref="S6.SS0.SSS0.Px4.p1.1.m1.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px4.p1.1.m1.1b"><apply id="S6.SS0.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S6.SS0.SSS0.Px4.p1.1.m1.1.1"><times id="S6.SS0.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S6.SS0.SSS0.Px4.p1.1.m1.1.1.1"></times><cn type="integer" id="S6.SS0.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S6.SS0.SSS0.Px4.p1.1.m1.1.1.2">7</cn><cn type="integer" id="S6.SS0.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="S6.SS0.SSS0.Px4.p1.1.m1.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px4.p1.1.m1.1c">7\times 7</annotation></semantics></math> after which the addition of smaller patches is detrimental.
Similarly, only using patches of large size does not allow models to focus on specific image regions.
Thus a trade-off exists between global context and region-specific features.
Changing the feature extractor from ResNet-50 to ResNet-101 only results in a minor improvement of <math id="S6.SS0.SSS0.Px4.p1.2.m2.1" class="ltx_Math" alttext="0.01\%" display="inline"><semantics id="S6.SS0.SSS0.Px4.p1.2.m2.1a"><mrow id="S6.SS0.SSS0.Px4.p1.2.m2.1.1" xref="S6.SS0.SSS0.Px4.p1.2.m2.1.1.cmml"><mn id="S6.SS0.SSS0.Px4.p1.2.m2.1.1.2" xref="S6.SS0.SSS0.Px4.p1.2.m2.1.1.2.cmml">0.01</mn><mo id="S6.SS0.SSS0.Px4.p1.2.m2.1.1.1" xref="S6.SS0.SSS0.Px4.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px4.p1.2.m2.1b"><apply id="S6.SS0.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S6.SS0.SSS0.Px4.p1.2.m2.1.1"><csymbol cd="latexml" id="S6.SS0.SSS0.Px4.p1.2.m2.1.1.1.cmml" xref="S6.SS0.SSS0.Px4.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S6.SS0.SSS0.Px4.p1.2.m2.1.1.2.cmml" xref="S6.SS0.SSS0.Px4.p1.2.m2.1.1.2">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px4.p1.2.m2.1c">0.01\%</annotation></semantics></math> to <math id="S6.SS0.SSS0.Px4.p1.3.m3.1" class="ltx_Math" alttext="0.30\%" display="inline"><semantics id="S6.SS0.SSS0.Px4.p1.3.m3.1a"><mrow id="S6.SS0.SSS0.Px4.p1.3.m3.1.1" xref="S6.SS0.SSS0.Px4.p1.3.m3.1.1.cmml"><mn id="S6.SS0.SSS0.Px4.p1.3.m3.1.1.2" xref="S6.SS0.SSS0.Px4.p1.3.m3.1.1.2.cmml">0.30</mn><mo id="S6.SS0.SSS0.Px4.p1.3.m3.1.1.1" xref="S6.SS0.SSS0.Px4.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px4.p1.3.m3.1b"><apply id="S6.SS0.SSS0.Px4.p1.3.m3.1.1.cmml" xref="S6.SS0.SSS0.Px4.p1.3.m3.1.1"><csymbol cd="latexml" id="S6.SS0.SSS0.Px4.p1.3.m3.1.1.1.cmml" xref="S6.SS0.SSS0.Px4.p1.3.m3.1.1.1">percent</csymbol><cn type="float" id="S6.SS0.SSS0.Px4.p1.3.m3.1.1.2.cmml" xref="S6.SS0.SSS0.Px4.p1.3.m3.1.1.2">0.30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px4.p1.3.m3.1c">0.30\%</annotation></semantics></math>.
Removing visual position embeddings has a significant effect on performance, with a drop of <math id="S6.SS0.SSS0.Px4.p1.4.m4.1" class="ltx_Math" alttext="4.60\%" display="inline"><semantics id="S6.SS0.SSS0.Px4.p1.4.m4.1a"><mrow id="S6.SS0.SSS0.Px4.p1.4.m4.1.1" xref="S6.SS0.SSS0.Px4.p1.4.m4.1.1.cmml"><mn id="S6.SS0.SSS0.Px4.p1.4.m4.1.1.2" xref="S6.SS0.SSS0.Px4.p1.4.m4.1.1.2.cmml">4.60</mn><mo id="S6.SS0.SSS0.Px4.p1.4.m4.1.1.1" xref="S6.SS0.SSS0.Px4.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px4.p1.4.m4.1b"><apply id="S6.SS0.SSS0.Px4.p1.4.m4.1.1.cmml" xref="S6.SS0.SSS0.Px4.p1.4.m4.1.1"><csymbol cd="latexml" id="S6.SS0.SSS0.Px4.p1.4.m4.1.1.1.cmml" xref="S6.SS0.SSS0.Px4.p1.4.m4.1.1.1">percent</csymbol><cn type="float" id="S6.SS0.SSS0.Px4.p1.4.m4.1.1.2.cmml" xref="S6.SS0.SSS0.Px4.p1.4.m4.1.1.2">4.60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px4.p1.4.m4.1c">4.60\%</annotation></semantics></math> to <math id="S6.SS0.SSS0.Px4.p1.5.m5.1" class="ltx_Math" alttext="8.00\%" display="inline"><semantics id="S6.SS0.SSS0.Px4.p1.5.m5.1a"><mrow id="S6.SS0.SSS0.Px4.p1.5.m5.1.1" xref="S6.SS0.SSS0.Px4.p1.5.m5.1.1.cmml"><mn id="S6.SS0.SSS0.Px4.p1.5.m5.1.1.2" xref="S6.SS0.SSS0.Px4.p1.5.m5.1.1.2.cmml">8.00</mn><mo id="S6.SS0.SSS0.Px4.p1.5.m5.1.1.1" xref="S6.SS0.SSS0.Px4.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px4.p1.5.m5.1b"><apply id="S6.SS0.SSS0.Px4.p1.5.m5.1.1.cmml" xref="S6.SS0.SSS0.Px4.p1.5.m5.1.1"><csymbol cd="latexml" id="S6.SS0.SSS0.Px4.p1.5.m5.1.1.1.cmml" xref="S6.SS0.SSS0.Px4.p1.5.m5.1.1.1">percent</csymbol><cn type="float" id="S6.SS0.SSS0.Px4.p1.5.m5.1.1.2.cmml" xref="S6.SS0.SSS0.Px4.p1.5.m5.1.1.2">8.00</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px4.p1.5.m5.1c">8.00\%</annotation></semantics></math> in both ZSL and FSL settings.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Impact of Pre-training Tasks:</h4>

<figure id="S6.T6" class="ltx_table">
<table id="S6.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T6.1.1" class="ltx_tr">
<td id="S6.T6.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S6.T6.1.1.1.1" class="ltx_text ltx_font_bold">Pre-Training Task</span></td>
<td id="S6.T6.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T6.1.1.2.1" class="ltx_text ltx_font_bold">VQA-v2</span></td>
<td id="S6.T6.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T6.1.1.3.1" class="ltx_text ltx_font_bold">VQA-CP</span></td>
<td id="S6.T6.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T6.1.1.4.1" class="ltx_text ltx_font_bold">GQA</span></td>
</tr>
<tr id="S6.T6.1.2" class="ltx_tr">
<td id="S6.T6.1.2.1" class="ltx_td ltx_align_left ltx_border_t">SWA</td>
<td id="S6.T6.1.2.2" class="ltx_td ltx_align_center ltx_border_t">39.1</td>
<td id="S6.T6.1.2.3" class="ltx_td ltx_align_center ltx_border_t">38.3</td>
<td id="S6.T6.1.2.4" class="ltx_td ltx_align_center ltx_border_t">25.4</td>
</tr>
<tr id="S6.T6.1.3" class="ltx_tr">
<td id="S6.T6.1.3.1" class="ltx_td ltx_align_left">MLM+SWA</td>
<td id="S6.T6.1.3.2" class="ltx_td ltx_align_center">42.4</td>
<td id="S6.T6.1.3.3" class="ltx_td ltx_align_center">41.5</td>
<td id="S6.T6.1.3.4" class="ltx_td ltx_align_center">27.8</td>
</tr>
<tr id="S6.T6.1.4" class="ltx_tr">
<td id="S6.T6.1.4.1" class="ltx_td ltx_align_left">MQA+SWA</td>
<td id="S6.T6.1.4.2" class="ltx_td ltx_align_center">42.0</td>
<td id="S6.T6.1.4.3" class="ltx_td ltx_align_center">41.2</td>
<td id="S6.T6.1.4.4" class="ltx_td ltx_align_center">26.6</td>
</tr>
<tr id="S6.T6.1.5" class="ltx_tr">
<td id="S6.T6.1.5.1" class="ltx_td ltx_align_left">MLM+MQA+SWA</td>
<td id="S6.T6.1.5.2" class="ltx_td ltx_align_center">45.6</td>
<td id="S6.T6.1.5.3" class="ltx_td ltx_align_center">44.9</td>
<td id="S6.T6.1.5.4" class="ltx_td ltx_align_center">29.7</td>
</tr>
<tr id="S6.T6.1.6" class="ltx_tr">
<td id="S6.T6.1.6.1" class="ltx_td ltx_align_left">MLM+ITM+SWA</td>
<td id="S6.T6.1.6.2" class="ltx_td ltx_align_center">44.7</td>
<td id="S6.T6.1.6.3" class="ltx_td ltx_align_center">43.6</td>
<td id="S6.T6.1.6.4" class="ltx_td ltx_align_center">28.9</td>
</tr>
<tr id="S6.T6.1.7" class="ltx_tr">
<td id="S6.T6.1.7.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="S6.T6.1.7.1.1" class="ltx_text ltx_font_bold">All</span></td>
<td id="S6.T6.1.7.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S6.T6.1.7.2.1" class="ltx_text ltx_font_bold">46.2</span></td>
<td id="S6.T6.1.7.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S6.T6.1.7.3.1" class="ltx_text ltx_font_bold">45.4</span></td>
<td id="S6.T6.1.7.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S6.T6.1.7.4.1" class="ltx_text ltx_font_bold">31.2</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Effect of different pre-training tasks on the ZSL performance for the Encoder model.</figcaption>
</figure>
<div id="S6.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px5.p1.1" class="ltx_p">Table <a href="#S6.T6" title="Table 6 ‣ Impact of Pre-training Tasks: ‣ 6 Results2footnote 22footnote 2 ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the effect of different pretraining tasks on the downstream zero-shot transfer VQA task.
We need the SWA task, as it is used to perform the zero-shot QA task.
The combination of MLM, MQA, and ITM, all of which need image understanding, shows improved performance on the downstream task, indicating better cross-modal representations.</p>
</div>
<figure id="S6.F5" class="ltx_figure"><img src="/html/2012.02356/assets/images/lcwide.png" id="S6.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="255" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Learning Curve showing validation accuracy vs. number of synthetically generated training samples.</figcaption>
</figure>
</section>
<section id="S6.SS0.SSS0.Px6" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Effect of size of synthetic training set:</h4>

<div id="S6.SS0.SSS0.Px6.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px6.p1.2" class="ltx_p">Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Related Work ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows our Encoder model’s learning curve for the zero-shot transfer setting trained on our synthetic Q-A pairs.
The performance stagnates after a critical threshold of <math id="S6.SS0.SSS0.Px6.p1.1.m1.1" class="ltx_Math" alttext="10^{6}" display="inline"><semantics id="S6.SS0.SSS0.Px6.p1.1.m1.1a"><msup id="S6.SS0.SSS0.Px6.p1.1.m1.1.1" xref="S6.SS0.SSS0.Px6.p1.1.m1.1.1.cmml"><mn id="S6.SS0.SSS0.Px6.p1.1.m1.1.1.2" xref="S6.SS0.SSS0.Px6.p1.1.m1.1.1.2.cmml">10</mn><mn id="S6.SS0.SSS0.Px6.p1.1.m1.1.1.3" xref="S6.SS0.SSS0.Px6.p1.1.m1.1.1.3.cmml">6</mn></msup><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px6.p1.1.m1.1b"><apply id="S6.SS0.SSS0.Px6.p1.1.m1.1.1.cmml" xref="S6.SS0.SSS0.Px6.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS0.SSS0.Px6.p1.1.m1.1.1.1.cmml" xref="S6.SS0.SSS0.Px6.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S6.SS0.SSS0.Px6.p1.1.m1.1.1.2.cmml" xref="S6.SS0.SSS0.Px6.p1.1.m1.1.1.2">10</cn><cn type="integer" id="S6.SS0.SSS0.Px6.p1.1.m1.1.1.3.cmml" xref="S6.SS0.SSS0.Px6.p1.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px6.p1.1.m1.1c">10^{6}</annotation></semantics></math> samples is reached.
Our experiments also suggest that randomly sampling a set of questions for each image per epoch leads to a <math id="S6.SS0.SSS0.Px6.p1.2.m2.1" class="ltx_Math" alttext="4\%" display="inline"><semantics id="S6.SS0.SSS0.Px6.p1.2.m2.1a"><mrow id="S6.SS0.SSS0.Px6.p1.2.m2.1.1" xref="S6.SS0.SSS0.Px6.p1.2.m2.1.1.cmml"><mn id="S6.SS0.SSS0.Px6.p1.2.m2.1.1.2" xref="S6.SS0.SSS0.Px6.p1.2.m2.1.1.2.cmml">4</mn><mo id="S6.SS0.SSS0.Px6.p1.2.m2.1.1.1" xref="S6.SS0.SSS0.Px6.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px6.p1.2.m2.1b"><apply id="S6.SS0.SSS0.Px6.p1.2.m2.1.1.cmml" xref="S6.SS0.SSS0.Px6.p1.2.m2.1.1"><csymbol cd="latexml" id="S6.SS0.SSS0.Px6.p1.2.m2.1.1.1.cmml" xref="S6.SS0.SSS0.Px6.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S6.SS0.SSS0.Px6.p1.2.m2.1.1.2.cmml" xref="S6.SS0.SSS0.Px6.p1.2.m2.1.1.2">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px6.p1.2.m2.1c">4\%</annotation></semantics></math> gain compared to training on the entire set.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px7" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Error Analysis:</h4>

<div id="S6.SS0.SSS0.Px7.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px7.p1.2" class="ltx_p">Our ZSL method is pretrained on longer phrases and hence tends to generate more detailed answers, such as “red car” instead of “car”.
Although the SWA loss is designed to encourage a distribution over the shorter phrases, the bias is not entirely removed.
On automated evaluation, we observe that for <math id="S6.SS0.SSS0.Px7.p1.1.m1.1" class="ltx_Math" alttext="42\%" display="inline"><semantics id="S6.SS0.SSS0.Px7.p1.1.m1.1a"><mrow id="S6.SS0.SSS0.Px7.p1.1.m1.1.1" xref="S6.SS0.SSS0.Px7.p1.1.m1.1.1.cmml"><mn id="S6.SS0.SSS0.Px7.p1.1.m1.1.1.2" xref="S6.SS0.SSS0.Px7.p1.1.m1.1.1.2.cmml">42</mn><mo id="S6.SS0.SSS0.Px7.p1.1.m1.1.1.1" xref="S6.SS0.SSS0.Px7.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px7.p1.1.m1.1b"><apply id="S6.SS0.SSS0.Px7.p1.1.m1.1.1.cmml" xref="S6.SS0.SSS0.Px7.p1.1.m1.1.1"><csymbol cd="latexml" id="S6.SS0.SSS0.Px7.p1.1.m1.1.1.1.cmml" xref="S6.SS0.SSS0.Px7.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S6.SS0.SSS0.Px7.p1.1.m1.1.1.2.cmml" xref="S6.SS0.SSS0.Px7.p1.1.m1.1.1.2">42</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px7.p1.1.m1.1c">42\%</annotation></semantics></math> of questions, the target answer is a sub-phrase of our predicted answer.
Manual evaluation of 100 such samples shows that <math id="S6.SS0.SSS0.Px7.p1.2.m2.1" class="ltx_Math" alttext="87\%" display="inline"><semantics id="S6.SS0.SSS0.Px7.p1.2.m2.1a"><mrow id="S6.SS0.SSS0.Px7.p1.2.m2.1.1" xref="S6.SS0.SSS0.Px7.p1.2.m2.1.1.cmml"><mn id="S6.SS0.SSS0.Px7.p1.2.m2.1.1.2" xref="S6.SS0.SSS0.Px7.p1.2.m2.1.1.2.cmml">87</mn><mo id="S6.SS0.SSS0.Px7.p1.2.m2.1.1.1" xref="S6.SS0.SSS0.Px7.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px7.p1.2.m2.1b"><apply id="S6.SS0.SSS0.Px7.p1.2.m2.1.1.cmml" xref="S6.SS0.SSS0.Px7.p1.2.m2.1.1"><csymbol cd="latexml" id="S6.SS0.SSS0.Px7.p1.2.m2.1.1.1.cmml" xref="S6.SS0.SSS0.Px7.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S6.SS0.SSS0.Px7.p1.2.m2.1.1.2.cmml" xref="S6.SS0.SSS0.Px7.p1.2.m2.1.1.2">87</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px7.p1.2.m2.1c">87\%</annotation></semantics></math> of such detailed predicted answers are plausible.
This shows the relevance of learning from captions and quantifies the bias towards short “true” answers in human-annotated benchmarks, calling for better evaluation metrics that do not penalize VQA systems for producing descriptive or alternative accurate answers.</p>
</div>
<div id="S6.SS0.SSS0.Px7.p2" class="ltx_para">
<p id="S6.SS0.SSS0.Px7.p2.1" class="ltx_p">In the FSL setting, we either finetune our pre-trained QA classifier with the SWA Loss or train a separate feedforward layer from scratch for the task.
The pre-trained QA classifier predicts longer phrases as answers, leading to a drop in accuracy.
The feedforward layer performs better (<math id="S6.SS0.SSS0.Px7.p2.1.m1.1" class="ltx_Math" alttext="+6\%" display="inline"><semantics id="S6.SS0.SSS0.Px7.p2.1.m1.1a"><mrow id="S6.SS0.SSS0.Px7.p2.1.m1.1.1" xref="S6.SS0.SSS0.Px7.p2.1.m1.1.1.cmml"><mo id="S6.SS0.SSS0.Px7.p2.1.m1.1.1a" xref="S6.SS0.SSS0.Px7.p2.1.m1.1.1.cmml">+</mo><mrow id="S6.SS0.SSS0.Px7.p2.1.m1.1.1.2" xref="S6.SS0.SSS0.Px7.p2.1.m1.1.1.2.cmml"><mn id="S6.SS0.SSS0.Px7.p2.1.m1.1.1.2.2" xref="S6.SS0.SSS0.Px7.p2.1.m1.1.1.2.2.cmml">6</mn><mo id="S6.SS0.SSS0.Px7.p2.1.m1.1.1.2.1" xref="S6.SS0.SSS0.Px7.p2.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px7.p2.1.m1.1b"><apply id="S6.SS0.SSS0.Px7.p2.1.m1.1.1.cmml" xref="S6.SS0.SSS0.Px7.p2.1.m1.1.1"><plus id="S6.SS0.SSS0.Px7.p2.1.m1.1.1.1.cmml" xref="S6.SS0.SSS0.Px7.p2.1.m1.1.1"></plus><apply id="S6.SS0.SSS0.Px7.p2.1.m1.1.1.2.cmml" xref="S6.SS0.SSS0.Px7.p2.1.m1.1.1.2"><csymbol cd="latexml" id="S6.SS0.SSS0.Px7.p2.1.m1.1.1.2.1.cmml" xref="S6.SS0.SSS0.Px7.p2.1.m1.1.1.2.1">percent</csymbol><cn type="integer" id="S6.SS0.SSS0.Px7.p2.1.m1.1.1.2.2.cmml" xref="S6.SS0.SSS0.Px7.p2.1.m1.1.1.2.2">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px7.p2.1.m1.1c">+6\%</annotation></semantics></math>), indicating our Encoder captures relevant features necessary to generalize to the benchmark answer-space.
Note that we do not use object annotations during training, unlike existing methods.</p>
</div>
<div id="S6.SS0.SSS0.Px7.p3" class="ltx_para">
<p id="S6.SS0.SSS0.Px7.p3.1" class="ltx_p">Our error analysis and Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Framework for Synthesizing Q-A Pairs ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> show the shift in question-space and answer-space between synthetic and human-authored Q-A pairs.
These (along with inadequate evaluation metrics) act as the primary sources explaining the performance-gap between weakly-supervised methods and the fully-supervised setting.
It remains to be seen whether more sophisticated question generation can be developed to reduce the performance gap further and mitigate the heavy reliance on human annotations.
</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Discussion and Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Prior work <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib12" title="" class="ltx_ref">2019</a>); Jiang et al. (<a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite> has demonstrated that the use of object bounding-boxes and region features leads to significant improvements on downstream tasks such as captioning and VQA.
However, little effort has been dedicated to developing alternative methods that can approach similar performance without relying on dense annotations.
We argue that weakly supervised learning coupled with data synthesis strategies could be the pathway for the V&amp;L community towards a “post-dataset era”.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>A. Efros, <span id="footnote2.1" class="ltx_text ltx_font_italic">Imagining a post-dataset era</span>, ICML’20 Talk.</span></span></span>
In this work, we take a step towards that goal.
We address the problem of weakly-supervised VQA with a framework for the procedural synthesis of Q-A pairs from captions for training VQA models, where benchmark datasets can be used only for evaluation.
We use spatial pyramids of patch features to increase the annotation efficiency of our methods.
Our experiments and analyses show the potential of patch-features and procedural data synthesis and reveal problems with existing evaluation metrics.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Ethical Considerations</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Captions and Question-Answer pairs are both annotated by humans in existing image captioning and visual question answering datasets.
However, captions arguably contain a lesser degree of subjectivity, ambiguity, and linguistic biases than VQA annotations, due to the design of annotation prompts that limit the introduction of these biases.
Our work points to the potential of procedurally generated annotations in providing robustness improvements under changing linguistic priors in VQA test sets (Table <a href="#footnotex2" title="footnote 2 ‣ Table 1 ‣ Baselines: ‣ 5 Experimental Setup ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
<cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib28" title="" class="ltx_ref">Hendricks et al.</a></cite> find that gender bias exists in image-captioning datasets and is <span id="Sx1.p1.1.1" class="ltx_text ltx_font_italic">amplified</span> by models; further research in self-supervised data synthesis could potentially help alleviate such social biases.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">The authors acknowledge support from the DARPA SAIL-ON program W911NF2020006, ONR award N00014-20-1-2332, and NSF grant 1816039, and the anonymous reviewers for their insightful discussion.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2018)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2018.00522" title="" class="ltx_ref ltx_href">Don’t just assume;
look and answer: Overcoming priors for visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">2018 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018</em>, pages
4971–4980. IEEE Computer Society.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2017)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Aniruddha Kembhavi, Dhruv Batra, and Devi Parikh. 2017.

</span>
<span class="ltx_bibblock">C-vqa: A compositional split of the visual question answering (vqa)
v1.0 dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1704.08243</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al. (2018)</span>
<span class="ltx_bibblock">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2018.00636" title="" class="ltx_ref ltx_href">Bottom-up and
top-down attention for image captioning and visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">2018 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018</em>, pages
6077–6086. IEEE Computer Society.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C. Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV.2015.279" title="" class="ltx_ref ltx_href">VQA: visual question
answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">2015 IEEE International Conference on Computer Vision,
ICCV 2015, Santiago, Chile, December 7-13, 2015</em>, pages 2425–2433. IEEE
Computer Society.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banerjee and Baral (2020)</span>
<span class="ltx_bibblock">
Pratyay Banerjee and Chitta Baral. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.11" title="" class="ltx_ref ltx_href">Self-supervised knowledge triplet learning for zero-shot question
answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 151–162, Online. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banerjee et al. (2021)</span>
<span class="ltx_bibblock">
Pratyay Banerjee, Tejas Gokhale, and Chitta Baral. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.aclweb.org/anthology/2021.naacl-main.95" title="" class="ltx_ref ltx_href">Self-supervised test-time learning for reading comprehension</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 1200–1211, Online. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhattacharya et al. (2019)</span>
<span class="ltx_bibblock">
Nilavra Bhattacharya, Qing Li, and Danna Gurari. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV.2019.00437" title="" class="ltx_ref ltx_href">Why does a visual
question have different answers?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">2019 IEEE/CVF International Conference on Computer Vision,
ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019</em>, pages
4270–4279. IEEE.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cadène et al. (2019)</span>
<span class="ltx_bibblock">
Rémi Cadène, Corentin Dancette, Hedi Ben-younes, Matthieu Cord,
and Devi Parikh. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2019/hash/51d92be1c60d1db1d2e5e7a07da55b26-Abstract.html" title="" class="ltx_ref ltx_href">Rubi: Reducing unimodal biases for visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 32: Annual
Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
December 8-14, 2019, Vancouver, BC, Canada</em>, pages 839–850.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chao et al. (2018)</span>
<span class="ltx_bibblock">
Wei-Lun Chao, Hexiang Hu, and Fei Sha. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2018.00599" title="" class="ltx_ref ltx_href">Cross-dataset
adaptation for visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">2018 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018</em>, pages
5716–5725. IEEE Computer Society.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Long Chen, Xin Yan, Jun Xiao, Hanwang Zhang, Shiliang Pu, and Yueting Zhuang.
2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR42600.2020.01081" title="" class="ltx_ref ltx_href">Counterfactual
samples synthesizing for robust visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020</em>, pages
10797–10806. IEEE.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2015)</span>
<span class="ltx_bibblock">
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr
Dollár, and C Lawrence Zitnick. 2015.

</span>
<span class="ltx_bibblock">Microsoft coco captions: Data collection and evaluation server.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1504.00325</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2019)</span>
<span class="ltx_bibblock">
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan,
Yu Cheng, and Jingjing Liu. 2019.

</span>
<span class="ltx_bibblock">Uniter: Learning universal image-text representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.11740</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et al. (2019)</span>
<span class="ltx_bibblock">
Christopher Clark, Mark Yatskar, and Luke Zettlemoyer. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1418" title="" class="ltx_ref ltx_href">Don’t take the easy
way out: Ensemble based methods for avoiding known dataset biases</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 4069–4082, Hong Kong,
China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N19-1423" title="" class="ltx_ref ltx_href">BERT: Pre-training of
deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186,
Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. (2017)</span>
<span class="ltx_bibblock">
Xinya Du, Junru Shao, and Claire Cardie. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P17-1123" title="" class="ltx_ref ltx_href">Learning to ask: Neural
question generation for reading comprehension</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1342–1352,
Vancouver, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fabbri et al. (2020)</span>
<span class="ltx_bibblock">
Alexander Fabbri, Patrick Ng, Zhiguo Wang, Ramesh Nallapati, and Bing Xiang.
2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.413" title="" class="ltx_ref ltx_href">Template-based
question generation from retrieved sentences for improved unsupervised
question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 4508–4513, Online. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. (2020a)</span>
<span class="ltx_bibblock">
Zhiyuan Fang, Tejas Gokhale, Pratyay Banerjee, Chitta Baral, and Yezhou Yang.
2020a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.61" title="" class="ltx_ref ltx_href">Video2Commonsense: Generating commonsense descriptions to enrich video
captioning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 840–860, Online. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. (2020b)</span>
<span class="ltx_bibblock">
Zhiyuan Fang, Shu Kong, Zhe Wang, Charless Fowlkes, and Yezhou Yang.
2020b.

</span>
<span class="ltx_bibblock">Weak supervision and referring attention for temporal-textual
association learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.11747</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">FitzGerald et al. (2018)</span>
<span class="ltx_bibblock">
Nicholas FitzGerald, Julian Michael, Luheng He, and Luke Zettlemoyer. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P18-1191" title="" class="ltx_ref ltx_href">Large-scale QA-SRL
parsing</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 2051–2060,
Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Girshick et al. (2014)</span>
<span class="ltx_bibblock">
Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2014.81" title="" class="ltx_ref ltx_href">Rich feature
hierarchies for accurate object detection and semantic segmentation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">2014 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014</em>, pages
580–587. IEEE Computer Society.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gokhale et al. (2020a)</span>
<span class="ltx_bibblock">
Tejas Gokhale, Pratyay Banerjee, Chitta Baral, and Yezhou Yang.
2020a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.63" title="" class="ltx_ref ltx_href">MUTANT: A
training paradigm for out-of-distribution generalization in visual question
answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 878–892, Online. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gokhale et al. (2020b)</span>
<span class="ltx_bibblock">
Tejas Gokhale, Pratyay Banerjee, Chitta Baral, and Yezhou Yang.
2020b.

</span>
<span class="ltx_bibblock">Vqa-lol: Visual question answering under the lens of logic.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision (ECCV)</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2017)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2017.670" title="" class="ltx_ref ltx_href">Making the V in
VQA matter: Elevating the role of image understanding in visual question
answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017</em>, pages
6325–6334. IEEE Computer Society.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grand and Belinkov (2019)</span>
<span class="ltx_bibblock">
Gabriel Grand and Yonatan Belinkov. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/W19-1801" title="" class="ltx_ref ltx_href">Adversarial
regularization for visual question answering: Strengths, shortcomings, and
side effects</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Second Workshop on Shortcomings in Vision
and Language</em>, pages 1–13, Minneapolis, Minnesota. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gururangan et al. (2018)</span>
<span class="ltx_bibblock">
Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman,
and Noah A. Smith. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N18-2017" title="" class="ltx_ref ltx_href">Annotation artifacts in
natural language inference data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 2 (Short Papers)</em>, pages 107–112, New Orleans,
Louisiana. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2016.90" title="" class="ltx_ref ltx_href">Deep residual learning
for image recognition</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016</em>, pages
770–778. IEEE Computer Society.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2015)</span>
<span class="ltx_bibblock">
Luheng He, Mike Lewis, and Luke Zettlemoyer. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D15-1076" title="" class="ltx_ref ltx_href">Question-answer driven
semantic role labeling: Using natural language to annotate natural language</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing</em>, pages 643–653, Lisbon, Portugal. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendricks et al. (2018)</span>
<span class="ltx_bibblock">
Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor Darrell, and Anna
Rohrbach. 2018.

</span>
<span class="ltx_bibblock">Women also snowboard: Overcoming bias in captioning models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 793–811.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendricks et al. (2017)</span>
<span class="ltx_bibblock">
Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell,
and Bryan C. Russell. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV.2017.618" title="" class="ltx_ref ltx_href">Localizing moments in
video with natural language</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Computer Vision, ICCV
2017, Venice, Italy, October 22-29, 2017</em>, pages 5804–5813. IEEE Computer
Society.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning (2018)</span>
<span class="ltx_bibblock">
Drew A. Hudson and Christopher D. Manning. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=S1Euwz-Rb" title="" class="ltx_ref ltx_href">Compositional
attention networks for machine reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
Proceedings</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning (2019)</span>
<span class="ltx_bibblock">
Drew A. Hudson and Christopher D. Manning. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2019.00686" title="" class="ltx_ref ltx_href">GQA: A new
dataset for real-world visual reasoning and compositional question
answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019</em>, pages
6700–6709. Computer Vision Foundation / IEEE.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(32)</span>
<span class="ltx_bibblock">
Shankar Iyer, Nikhil Dandekar, and Kornel Csernai.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs" title="" class="ltx_ref ltx_href">First
quora dataset release: Question pairs</a>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2020)</span>
<span class="ltx_bibblock">
Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik G. Learned-Miller, and
Xinlei Chen. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR42600.2020.01028" title="" class="ltx_ref ltx_href">In defense of
grid features for visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020</em>, pages
10264–10273. IEEE.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2017)</span>
<span class="ltx_bibblock">
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei,
C. Lawrence Zitnick, and Ross B. Girshick. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2017.215" title="" class="ltx_ref ltx_href">CLEVR: A
diagnostic dataset for compositional language and elementary visual
reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017</em>, pages
1988–1997. IEEE Computer Society.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kafle and Kanan (2017)</span>
<span class="ltx_bibblock">
Kushal Kafle and Christopher Kanan. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV.2017.217" title="" class="ltx_ref ltx_href">An analysis of visual
question answering algorithms</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Computer Vision, ICCV
2017, Venice, Italy, October 22-29, 2017</em>, pages 1983–1991. IEEE Computer
Society.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaushik et al. (2020)</span>
<span class="ltx_bibblock">
Divyansh Kaushik, Eduard H. Hovy, and Zachary Chase Lipton. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=Sklgs0NFvr" title="" class="ltx_ref ltx_href">Learning the
difference that makes A difference with counterfactually-augmented data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khoreva et al. (2017)</span>
<span class="ltx_bibblock">
Anna Khoreva, Rodrigo Benenson, Jan Hendrik Hosang, Matthias Hein, and Bernt
Schiele. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2017.181" title="" class="ltx_ref ltx_href">Simple does it: Weakly
supervised instance and semantic segmentation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017</em>, pages
1665–1674. IEEE Computer Society.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2018)</span>
<span class="ltx_bibblock">
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2018/hash/96ea64f3a1aa2fd00c72faacf0cb8ac9-Abstract.html" title="" class="ltx_ref ltx_href">Bilinear attention networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 31: Annual
Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
December 3-8, 2018, Montréal, Canada</em>, pages 1571–1581.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. (2019)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Michael Bernstein, and Li Fei-Fei. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2019.00211" title="" class="ltx_ref ltx_href">Information
maximizing visual question generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019</em>, pages
2008–2018. Computer Vision Foundation / IEEE.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. (2017)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.
2017.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">International journal of computer vision</em>, 123(1):32–73.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lan et al. (2020)</span>
<span class="ltx_bibblock">
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
Radu Soricut. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=H1eA7AEtvS" title="" class="ltx_ref ltx_href">ALBERT: A
lite BERT for self-supervised learning of language representations</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lazebnik et al. (2006)</span>
<span class="ltx_bibblock">
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. 2006.

</span>
<span class="ltx_bibblock">Beyond bags of features: Spatial pyramid matching for recognizing
natural scene categories.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2019a)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ludovic Denoyer, and Sebastian Riedel. 2019a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P19-1484" title="" class="ltx_ref ltx_href">Unsupervised question
answering by cloze translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 4896–4910, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2019b)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ludovic Denoyer, and Sebastian Riedel. 2019b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P19-1484" title="" class="ltx_ref ltx_href">Unsupervised question
answering by cloze translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 4896–4910, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2018)</span>
<span class="ltx_bibblock">
Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, Xiaogang Wang, and
Ming Zhou. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2018.00640" title="" class="ltx_ref ltx_href">Visual question
generation as dual task of visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">2018 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018</em>, pages
6116–6124. IEEE Computer Society.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020)</span>
<span class="ltx_bibblock">
Zhongli Li, Wenhui Wang, Li Dong, Furu Wei, and Ke Xu. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.600" title="" class="ltx_ref ltx_href">Harvesting and
refining question-answer pairs for unsupervised QA</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 6719–6728, Online. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pages 740–755.
Springer.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Runtao Liu, Chenxi Liu, Yutong Bai, and Alan L. Yuille. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2019.00431" title="" class="ltx_ref ltx_href">Clevr-ref+:
Diagnosing visual reasoning with referring expressions</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019</em>, pages
4185–4194. Computer Vision Foundation / IEEE.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2019)</span>
<span class="ltx_bibblock">
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html" title="" class="ltx_ref ltx_href">Vilbert: Pretraining task-agnostic visiolinguistic representations for
vision-and-language tasks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 32: Annual
Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
December 8-14, 2019, Vancouver, BC, Canada</em>, pages 13–23.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski and Fritz (2014)</span>
<span class="ltx_bibblock">
Mateusz Malinowski and Mario Fritz. 2014.

</span>
<span class="ltx_bibblock">Towards a visual turing challenge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Learning Semantics 2014</em>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mithun et al. (2019)</span>
<span class="ltx_bibblock">
Niluthpol Chowdhury Mithun, Sujoy Paul, and Amit K. Roy-Chowdhury. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2019.01186" title="" class="ltx_ref ltx_href">Weakly supervised
video moment retrieval from text queries</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019</em>, pages
11592–11601. Computer Vision Foundation / IEEE.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mogadala et al. (2019)</span>
<span class="ltx_bibblock">
Aditya Mogadala, Marimuthu Kalimuthu, and Dietrich Klakow. 2019.

</span>
<span class="ltx_bibblock">Trends in integration of vision and language research: A survey of
tasks, datasets, and methods.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.09358</em>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Niven and Kao (2019)</span>
<span class="ltx_bibblock">
Timothy Niven and Hung-Yu Kao. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P19-1459" title="" class="ltx_ref ltx_href">Probing neural network
comprehension of natural language arguments</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 4658–4664, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ordonez et al. (2011)</span>
<span class="ltx_bibblock">
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. 2011.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2011/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html" title="" class="ltx_ref ltx_href">Im2text: Describing images using 1 million captioned photographs</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 24: 25th
Annual Conference on Neural Information Processing Systems 2011. Proceedings
of a meeting held 12-14 December 2011, Granada, Spain</em>, pages 1143–1151.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke et al. (2019)</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
Desmaison, Andreas Köpf, Edward Yang, Zachary DeVito, Martin Raison,
Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html" title="" class="ltx_ref ltx_href">Pytorch: An imperative style, high-performance deep learning library</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 32: Annual
Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
December 8-14, 2019, Vancouver, BC, Canada</em>, pages 8024–8035.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington et al. (2014)</span>
<span class="ltx_bibblock">
Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3115/v1/D14-1162" title="" class="ltx_ref ltx_href">GloVe: Global
vectors for word representation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2014 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 1532–1543, Doha, Qatar.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2019)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.10683</em>.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramakrishnan et al. (2017)</span>
<span class="ltx_bibblock">
Santhosh K. Ramakrishnan, Ambar Pal, Gaurav Sharma, and Anurag Mittal. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2017.773" title="" class="ltx_ref ltx_href">An empirical
evaluation of visual question answering for novel objects</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017</em>, pages
7312–7321. IEEE Computer Society.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ray et al. (2019)</span>
<span class="ltx_bibblock">
Arijit Ray, Karan Sikka, Ajay Divakaran, Stefan Lee, and Giedrius Burachas.
2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1596" title="" class="ltx_ref ltx_href">Sunny and dark
outside?! improving answer consistency in VQA through entailed question
generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 5860–5865, Hong Kong,
China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2015a)</span>
<span class="ltx_bibblock">
Mengye Ren, Ryan Kiros, and Richard S. Zemel. 2015a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2015/hash/831c2f88a604a07ca94314b56a4921b8-Abstract.html" title="" class="ltx_ref ltx_href">Exploring models and data for image question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 28: Annual
Conference on Neural Information Processing Systems 2015, December 7-12,
2015, Montreal, Quebec, Canada</em>, pages 2953–2961.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2015b)</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html" title="" class="ltx_ref ltx_href">Faster R-CNN: towards real-time object detection with region proposal
networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 28: Annual
Conference on Neural Information Processing Systems 2015, December 7-12,
2015, Montreal, Quebec, Canada</em>, pages 91–99.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rennie et al. (2020)</span>
<span class="ltx_bibblock">
Steven Rennie, Etienne Marcheret, Neil Mallinar, David Nahamoo, and Vaibhava
Goel. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.87" title="" class="ltx_ref ltx_href">Unsupervised
adaptation of question answering systems via generative self-training</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 1148–1157, Online. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ribeiro et al. (2019)</span>
<span class="ltx_bibblock">
Marco Tulio Ribeiro, Carlos Guestrin, and Sameer Singh. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P19-1621" title="" class="ltx_ref ltx_href">Are red roses red?
evaluating consistency of question-answering models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 6174–6184, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Russakovsky et al. (2015)</span>
<span class="ltx_bibblock">
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.
2015.

</span>
<span class="ltx_bibblock">Imagenet large scale visual recognition challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">International journal of computer vision</em>, 115(3).

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sakaguchi et al. (2020)</span>
<span class="ltx_bibblock">
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020.

</span>
<span class="ltx_bibblock">Winogrande: An adversarial winograd schema challenge at scale.

</span>
<span class="ltx_bibblock">In <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">AAAI</em>.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et al. (2019)</span>
<span class="ltx_bibblock">
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019.

</span>
<span class="ltx_bibblock">Distilbert, a distilled version of bert: smaller, faster, cheaper and
lighter.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.01108</em>.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Selvaraju et al. (2020)</span>
<span class="ltx_bibblock">
Ramprasaath R. Selvaraju, Purva Tendulkar, Devi Parikh, Eric Horvitz,
Marco Túlio Ribeiro, Besmira Nushi, and Ece Kamar. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR42600.2020.01002" title="" class="ltx_ref ltx_href">Squinting at
VQA models: Introspecting VQA models with sub-questions</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020</em>, pages
10000–10008. IEEE.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et al. (2016)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P16-1009" title="" class="ltx_ref ltx_href">Improving neural
machine translation models with monolingual data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 86–96, Berlin,
Germany. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al. (2018)</span>
<span class="ltx_bibblock">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P18-1238" title="" class="ltx_ref ltx_href">Conceptual captions: A
cleaned, hypernymed, image alt-text dataset for automatic image captioning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 2556–2565,
Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shrestha et al. (2019)</span>
<span class="ltx_bibblock">
Robik Shrestha, Kushal Kafle, and Christopher Kanan. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2019.01072" title="" class="ltx_ref ltx_href">Answer them all!
toward universal visual question answering models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019</em>, pages
10472–10481. Computer Vision Foundation / IEEE.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and Zisserman (2015)</span>
<span class="ltx_bibblock">
Karen Simonyan and Andrew Zisserman. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1409.1556" title="" class="ltx_ref ltx_href">Very deep convolutional
networks for large-scale image recognition</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings</em>.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2014)</span>
<span class="ltx_bibblock">
Hyun Oh Song, Ross B. Girshick, Stefanie Jegelka, Julien Mairal, Zaïd
Harchaoui, and Trevor Darrell. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://proceedings.mlr.press/v32/songb14.html" title="" class="ltx_ref ltx_href">On learning to
localize objects with minimal supervision</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 31th International Conference on Machine
Learning, ICML 2014, Beijing, China, 21-26 June 2014</em>, volume 32 of
<em id="bib.bib72.2.2" class="ltx_emph ltx_font_italic">JMLR Workshop and Conference Proceedings</em>, pages 1611–1619.
JMLR.org.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Storkey (2009)</span>
<span class="ltx_bibblock">
Amos Storkey. 2009.

</span>
<span class="ltx_bibblock">When training and test sets are different: characterizing learning
transfer.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">Dataset shift in machine learning</em>, pages 3–28.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Bansal (2019)</span>
<span class="ltx_bibblock">
Hao Tan and Mohit Bansal. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1514" title="" class="ltx_ref ltx_href">LXMERT: Learning
cross-modality encoder representations from transformers</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 5100–5111, Hong Kong,
China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teney et al. (2020a)</span>
<span class="ltx_bibblock">
Damien Teney, Ehsan Abbasnedjad, and Anton van den Hengel. 2020a.

</span>
<span class="ltx_bibblock">Learning what makes a difference from counterfactual examples and
gradient supervision.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.09034</em>.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teney et al. (2020b)</span>
<span class="ltx_bibblock">
Damien Teney, Ehsan Abbasnejad, and Anton van den Hengel. 2020b.

</span>
<span class="ltx_bibblock">Unshuffling data for improved generalization.

</span>
<span class="ltx_bibblock"><em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.11894</em>.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teney and van den Hengel (2019)</span>
<span class="ltx_bibblock">
Damien Teney and Anton van den Hengel. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2019.00204" title="" class="ltx_ref ltx_href">Actively seeking and
learning from live data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019</em>, pages
1940–1949. Computer Vision Foundation / IEEE.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teney and Hengel (2016)</span>
<span class="ltx_bibblock">
Damien Teney and Anton van den Hengel. 2016.

</span>
<span class="ltx_bibblock">Zero-shot visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1611.05546</em>.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teney et al. (2020c)</span>
<span class="ltx_bibblock">
Damien Teney, Kushal Kafle, Robik Shrestha, Ehsan Abbasnejad, Christopher
Kanan, and Anton van den Hengel. 2020c.

</span>
<span class="ltx_bibblock">On the value of out-of-distribution testing: An example of goodhart’s
law.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2005.09241</em>.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann (2012)</span>
<span class="ltx_bibblock">
Jörg Tiedemann. 2012.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf" title="" class="ltx_ref ltx_href">Parallel
data, tools and interfaces in OPUS</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC’12)</em>, pages 2214–2218, Istanbul,
Turkey. European Language Resources Association (ELRA).

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" title="" class="ltx_ref ltx_href">Attention is all you need</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 30: Annual
Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, USA</em>, pages 5998–6008.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2019)</span>
<span class="ltx_bibblock">
Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV.2019.01002" title="" class="ltx_ref ltx_href">Meta-learning to
detect rare objects</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">2019 IEEE/CVF International Conference on Computer Vision,
ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019</em>, pages
9924–9933. IEEE.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2019)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, R’emi Louf, Morgan Funtowicz, and
Jamie Brew. 2019.

</span>
<span class="ltx_bibblock">Huggingface’s transformers: State-of-the-art natural language
processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1910.03771.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Mooney (2019)</span>
<span class="ltx_bibblock">
Jialin Wu and Raymond J. Mooney. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2019/hash/33b879e7ab79f56af1e88359f9314a10-Abstract.html" title="" class="ltx_ref ltx_href">Self-critical reasoning for robust visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 32: Annual
Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
December 8-14, 2019, Vancouver, BC, Canada</em>, pages 8601–8611.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2016)</span>
<span class="ltx_bibblock">
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016.

</span>
<span class="ltx_bibblock">Google’s neural machine translation system: Bridging the gap between
human and machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1609.08144</em>.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2020)</span>
<span class="ltx_bibblock">
Yiming Xu, Lin Chen, Zhongwei Cheng, Lixin Duan, and Jiebo Luo. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.findings-emnlp.34" title="" class="ltx_ref ltx_href">Open-ended visual question answering by multi-modal domain adaptation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2020</em>, pages 367–376, Online. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2016)</span>
<span class="ltx_bibblock">
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alexander J. Smola. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2016.10" title="" class="ltx_ref ltx_href">Stacked attention
networks for image question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016</em>, pages
21–29. IEEE Computer Society.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2019)</span>
<span class="ltx_bibblock">
Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2019.00644" title="" class="ltx_ref ltx_href">Deep modular
co-attention networks for visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019</em>, pages
6281–6290. Computer Vision Foundation / IEEE.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2017)</span>
<span class="ltx_bibblock">
Hanwang Zhang, Zawlin Kyaw, Jinyang Yu, and Shih-Fu Chang. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV.2017.454" title="" class="ltx_ref ltx_href">PPR-FCN: weakly
supervised visual relation detection via parallel pairwise R-FCN</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Computer Vision, ICCV
2017, Venice, Italy, October 22-29, 2017</em>, pages 4243–4251. IEEE Computer
Society.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2016)</span>
<span class="ltx_bibblock">
Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2016.542" title="" class="ltx_ref ltx_href">Yin and yang:
Balancing and answering binary visual questions</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016</em>, pages
5014–5022. IEEE Computer Society.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2016)</span>
<span class="ltx_bibblock">
Bolei Zhou, Aditya Khosla, Àgata Lapedriza, Aude Oliva, and Antonio
Torralba. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2016.319" title="" class="ltx_ref ltx_href">Learning deep features
for discriminative localization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016</em>, pages
2921–2929. IEEE Computer Society.

</span>
</li>
</ul>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Appendix</h2>

</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Synthesized Samples</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Table <a href="#A1.T7" title="Table 7 ‣ Appendix A Synthesized Samples ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows illustrative examples of Q-A pairs procedurally generated from the image caption using template-based method.</p>
</div>
<figure id="A1.T7" class="ltx_table">
<div id="A1.T7.7" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:618.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(73.3pt,-104.5pt) scale(1.51060321758785,1.51060321758785) ;">
<table id="A1.T7.7.7" class="ltx_tabular ltx_align_middle">
<tr id="A1.T7.7.7.8" class="ltx_tr">
<td id="A1.T7.7.7.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T7.7.7.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.7.7.8.1.1.1" class="ltx_p" style="width:71.1pt;"><span id="A1.T7.7.7.8.1.1.1.1" class="ltx_text ltx_font_bold">Image</span></span>
</span>
</td>
<td id="A1.T7.7.7.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T7.7.7.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.7.7.8.2.1.1" class="ltx_p" style="width:136.6pt;"><span id="A1.T7.7.7.8.2.1.1.1" class="ltx_text ltx_font_bold">Question</span></span>
</span>
</td>
<td id="A1.T7.7.7.8.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T7.7.7.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.7.7.8.3.1.1" class="ltx_p" style="width:42.7pt;"><span id="A1.T7.7.7.8.3.1.1.1" class="ltx_text ltx_font_bold">Answer</span></span>
</span>
</td>
</tr>
<tr id="A1.T7.1.1.1" class="ltx_tr">
<td id="A1.T7.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T7.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.1.1.1.1.1" class="ltx_p" style="width:71.1pt;">
<span id="A1.T7.1.1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:71.1pt;"><img src="/html/2012.02356/assets/images/supp_tables/bags.png" id="A1.T7.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="99" height="73" alt="[Uncaptioned image]">
</span></span>
</span>
</td>
<td id="A1.T7.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T7.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.1.1.2.1.1" class="ltx_p" style="width:136.6pt;">What are set on the sidewalk outside a veterinary hospital?</span>
</span>
</td>
<td id="A1.T7.1.1.1.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T7.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.1.1.1.3.1.1" class="ltx_p" style="width:42.7pt;">bags</span>
</span>
</td>
</tr>
<tr id="A1.T7.2.2.2" class="ltx_tr">
<td id="A1.T7.2.2.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.2.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.2.2.2.1.1.1" class="ltx_p" style="width:71.1pt;">
<span id="A1.T7.2.2.2.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:71.1pt;"><img src="/html/2012.02356/assets/images/supp_tables/phone.png" id="A1.T7.2.2.2.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="99" height="73" alt="[Uncaptioned image]">
</span></span>
</span>
</td>
<td id="A1.T7.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.2.2.2.2.1.1" class="ltx_p" style="width:136.6pt;">What is the young man holding up in front of his face ?</span>
</span>
</td>
<td id="A1.T7.2.2.2.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.2.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.2.2.2.3.1.1" class="ltx_p" style="width:42.7pt;">phone</span>
</span>
</td>
</tr>
<tr id="A1.T7.3.3.3" class="ltx_tr">
<td id="A1.T7.3.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.3.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.3.3.3.1.1.1" class="ltx_p" style="width:71.1pt;">
<span id="A1.T7.3.3.3.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:71.1pt;"><img src="/html/2012.02356/assets/images/supp_tables/wine.png" id="A1.T7.3.3.3.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="99" height="79" alt="[Uncaptioned image]">
</span></span>
</span>
</td>
<td id="A1.T7.3.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.3.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.3.3.3.2.1.1" class="ltx_p" style="width:136.6pt;">What is almost empty on the table</span>
</span>
</td>
<td id="A1.T7.3.3.3.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.3.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.3.3.3.3.1.1" class="ltx_p" style="width:42.7pt;">glass</span>
</span>
</td>
</tr>
<tr id="A1.T7.4.4.4" class="ltx_tr">
<td id="A1.T7.4.4.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.4.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.4.4.4.1.1.1" class="ltx_p" style="width:71.1pt;">
<span id="A1.T7.4.4.4.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:71.1pt;"><img src="/html/2012.02356/assets/images/supp_tables/carriage.png" id="A1.T7.4.4.4.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="99" height="65" alt="[Uncaptioned image]">
</span></span>
</span>
</td>
<td id="A1.T7.4.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.4.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.4.4.4.2.1.1" class="ltx_p" style="width:136.6pt;">What drawn carriage with passengers in the city</span>
</span>
</td>
<td id="A1.T7.4.4.4.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.4.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.4.4.4.3.1.1" class="ltx_p" style="width:42.7pt;">horse</span>
</span>
</td>
</tr>
<tr id="A1.T7.5.5.5" class="ltx_tr">
<td id="A1.T7.5.5.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.5.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.5.5.5.1.1.1" class="ltx_p" style="width:71.1pt;">
<span id="A1.T7.5.5.5.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:71.1pt;"><img src="/html/2012.02356/assets/images/supp_tables/table.png" id="A1.T7.5.5.5.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="99" height="79" alt="[Uncaptioned image]">
</span></span>
</span>
</td>
<td id="A1.T7.5.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.5.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.5.5.5.2.1.1" class="ltx_p" style="width:136.6pt;">What is the color of the table ?</span>
</span>
</td>
<td id="A1.T7.5.5.5.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.5.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.5.5.5.3.1.1" class="ltx_p" style="width:42.7pt;">white</span>
</span>
</td>
</tr>
<tr id="A1.T7.6.6.6" class="ltx_tr">
<td id="A1.T7.6.6.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.6.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.6.6.6.1.1.1" class="ltx_p" style="width:71.1pt;">
<span id="A1.T7.6.6.6.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:71.1pt;"><img src="/html/2012.02356/assets/images/supp_tables/cat.png" id="A1.T7.6.6.6.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="99" height="65" alt="[Uncaptioned image]">
</span></span>
</span>
</td>
<td id="A1.T7.6.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.6.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.6.6.6.2.1.1" class="ltx_p" style="width:136.6pt;">What is the color of the eyes ?</span>
</span>
</td>
<td id="A1.T7.6.6.6.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.6.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.6.6.6.3.1.1" class="ltx_p" style="width:42.7pt;">blue</span>
</span>
</td>
</tr>
<tr id="A1.T7.7.7.7" class="ltx_tr">
<td id="A1.T7.7.7.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A1.T7.7.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.7.7.7.1.1.1" class="ltx_p" style="width:71.1pt;">
<span id="A1.T7.7.7.7.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:71.1pt;"><img src="/html/2012.02356/assets/x3.jpg" id="A1.T7.7.7.7.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="109" height="58" alt="[Uncaptioned image]">
</span></span>
</span>
</td>
<td id="A1.T7.7.7.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A1.T7.7.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.7.7.7.2.1.1" class="ltx_p" style="width:136.6pt;">How many boats anchored by ropes close to shore?</span>
</span>
</td>
<td id="A1.T7.7.7.7.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A1.T7.7.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.7.7.7.3.1.1" class="ltx_p" style="width:42.7pt;">8</span>
</span>
</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Examples of template-based data synthesis</figcaption>
</figure>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.3" class="ltx_p">Table <a href="#A1.T8" title="Table 8 ‣ Appendix A Synthesized Samples ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows the use of two transformations (T): negation and adversarial words <cite class="ltx_cite ltx_citemacro_cite">Gokhale et al. (<a href="#bib.bib22" title="" class="ltx_ref">2020b</a>)</cite> two generate more sentences.
Thus the negation of <math id="A1.p2.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="A1.p2.1.m1.1a"><mi id="A1.p2.1.m1.1.1" xref="A1.p2.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="A1.p2.1.m1.1b"><ci id="A1.p2.1.m1.1.1.cmml" xref="A1.p2.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.1.m1.1c">Q</annotation></semantics></math> or substitution of a word in <math id="A1.p2.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="A1.p2.2.m2.1a"><mi id="A1.p2.2.m2.1.1" xref="A1.p2.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="A1.p2.2.m2.1b"><ci id="A1.p2.2.m2.1.1.cmml" xref="A1.p2.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.2.m2.1c">Q</annotation></semantics></math> with an adversarial word results in the new question-answer pair <math id="A1.p2.3.m3.2" class="ltx_Math" alttext="Q_{new},A_{new}" display="inline"><semantics id="A1.p2.3.m3.2a"><mrow id="A1.p2.3.m3.2.2.2" xref="A1.p2.3.m3.2.2.3.cmml"><msub id="A1.p2.3.m3.1.1.1.1" xref="A1.p2.3.m3.1.1.1.1.cmml"><mi id="A1.p2.3.m3.1.1.1.1.2" xref="A1.p2.3.m3.1.1.1.1.2.cmml">Q</mi><mrow id="A1.p2.3.m3.1.1.1.1.3" xref="A1.p2.3.m3.1.1.1.1.3.cmml"><mi id="A1.p2.3.m3.1.1.1.1.3.2" xref="A1.p2.3.m3.1.1.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="A1.p2.3.m3.1.1.1.1.3.1" xref="A1.p2.3.m3.1.1.1.1.3.1.cmml">​</mo><mi id="A1.p2.3.m3.1.1.1.1.3.3" xref="A1.p2.3.m3.1.1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="A1.p2.3.m3.1.1.1.1.3.1a" xref="A1.p2.3.m3.1.1.1.1.3.1.cmml">​</mo><mi id="A1.p2.3.m3.1.1.1.1.3.4" xref="A1.p2.3.m3.1.1.1.1.3.4.cmml">w</mi></mrow></msub><mo id="A1.p2.3.m3.2.2.2.3" xref="A1.p2.3.m3.2.2.3.cmml">,</mo><msub id="A1.p2.3.m3.2.2.2.2" xref="A1.p2.3.m3.2.2.2.2.cmml"><mi id="A1.p2.3.m3.2.2.2.2.2" xref="A1.p2.3.m3.2.2.2.2.2.cmml">A</mi><mrow id="A1.p2.3.m3.2.2.2.2.3" xref="A1.p2.3.m3.2.2.2.2.3.cmml"><mi id="A1.p2.3.m3.2.2.2.2.3.2" xref="A1.p2.3.m3.2.2.2.2.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="A1.p2.3.m3.2.2.2.2.3.1" xref="A1.p2.3.m3.2.2.2.2.3.1.cmml">​</mo><mi id="A1.p2.3.m3.2.2.2.2.3.3" xref="A1.p2.3.m3.2.2.2.2.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="A1.p2.3.m3.2.2.2.2.3.1a" xref="A1.p2.3.m3.2.2.2.2.3.1.cmml">​</mo><mi id="A1.p2.3.m3.2.2.2.2.3.4" xref="A1.p2.3.m3.2.2.2.2.3.4.cmml">w</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="A1.p2.3.m3.2b"><list id="A1.p2.3.m3.2.2.3.cmml" xref="A1.p2.3.m3.2.2.2"><apply id="A1.p2.3.m3.1.1.1.1.cmml" xref="A1.p2.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="A1.p2.3.m3.1.1.1.1.1.cmml" xref="A1.p2.3.m3.1.1.1.1">subscript</csymbol><ci id="A1.p2.3.m3.1.1.1.1.2.cmml" xref="A1.p2.3.m3.1.1.1.1.2">𝑄</ci><apply id="A1.p2.3.m3.1.1.1.1.3.cmml" xref="A1.p2.3.m3.1.1.1.1.3"><times id="A1.p2.3.m3.1.1.1.1.3.1.cmml" xref="A1.p2.3.m3.1.1.1.1.3.1"></times><ci id="A1.p2.3.m3.1.1.1.1.3.2.cmml" xref="A1.p2.3.m3.1.1.1.1.3.2">𝑛</ci><ci id="A1.p2.3.m3.1.1.1.1.3.3.cmml" xref="A1.p2.3.m3.1.1.1.1.3.3">𝑒</ci><ci id="A1.p2.3.m3.1.1.1.1.3.4.cmml" xref="A1.p2.3.m3.1.1.1.1.3.4">𝑤</ci></apply></apply><apply id="A1.p2.3.m3.2.2.2.2.cmml" xref="A1.p2.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="A1.p2.3.m3.2.2.2.2.1.cmml" xref="A1.p2.3.m3.2.2.2.2">subscript</csymbol><ci id="A1.p2.3.m3.2.2.2.2.2.cmml" xref="A1.p2.3.m3.2.2.2.2.2">𝐴</ci><apply id="A1.p2.3.m3.2.2.2.2.3.cmml" xref="A1.p2.3.m3.2.2.2.2.3"><times id="A1.p2.3.m3.2.2.2.2.3.1.cmml" xref="A1.p2.3.m3.2.2.2.2.3.1"></times><ci id="A1.p2.3.m3.2.2.2.2.3.2.cmml" xref="A1.p2.3.m3.2.2.2.2.3.2">𝑛</ci><ci id="A1.p2.3.m3.2.2.2.2.3.3.cmml" xref="A1.p2.3.m3.2.2.2.2.3.3">𝑒</ci><ci id="A1.p2.3.m3.2.2.2.2.3.4.cmml" xref="A1.p2.3.m3.2.2.2.2.3.4">𝑤</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.3.m3.2c">Q_{new},A_{new}</annotation></semantics></math>.
To increase the linguistic diversity of the questions we use paraphrasing as shown in Table <a href="#A2.T11" title="Table 11 ‣ Appendix B Dataset Analysis ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
<figure id="A1.T8" class="ltx_table">
<div id="A1.T8.8" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:459.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(56.2pt,-59.6pt) scale(1.35027781272795,1.35027781272795) ;">
<table id="A1.T8.8.8" class="ltx_tabular ltx_align_middle">
<tr id="A1.T8.2.2.2" class="ltx_tr">
<td id="A1.T8.2.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T8.2.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.2.2.2.3.1.1" class="ltx_p" style="width:2.8pt;"><span id="A1.T8.2.2.2.3.1.1.1" class="ltx_text ltx_font_bold">T</span></span>
</span>
</td>
<td id="A1.T8.2.2.2.4" class="ltx_td ltx_border_tt"></td>
<td id="A1.T8.2.2.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T8.2.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.2.2.2.5.1.1" class="ltx_p" style="width:56.9pt;"><span id="A1.T8.2.2.2.5.1.1.1" class="ltx_text ltx_font_bold">Image</span></span>
</span>
</td>
<td id="A1.T8.2.2.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T8.2.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.2.2.2.6.1.1" class="ltx_p" style="width:62.6pt;"><span id="A1.T8.2.2.2.6.1.1.1" class="ltx_text ltx_font_bold">Q</span></span>
</span>
</td>
<td id="A1.T8.2.2.2.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T8.2.2.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.2.2.2.7.1.1" class="ltx_p" style="width:22.8pt;"><span id="A1.T8.2.2.2.7.1.1.1" class="ltx_text ltx_font_bold">A</span></span>
</span>
</td>
<td id="A1.T8.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T8.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.1.1.1.1.1.1" class="ltx_p" style="width:62.6pt;"><math id="A1.T8.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{Q_{new}}" display="inline"><semantics id="A1.T8.1.1.1.1.1.1.m1.1a"><msub id="A1.T8.1.1.1.1.1.1.m1.1.1" xref="A1.T8.1.1.1.1.1.1.m1.1.1.cmml"><mi id="A1.T8.1.1.1.1.1.1.m1.1.1.2" xref="A1.T8.1.1.1.1.1.1.m1.1.1.2.cmml">𝐐</mi><mi id="A1.T8.1.1.1.1.1.1.m1.1.1.3" xref="A1.T8.1.1.1.1.1.1.m1.1.1.3.cmml">𝐧𝐞𝐰</mi></msub><annotation-xml encoding="MathML-Content" id="A1.T8.1.1.1.1.1.1.m1.1b"><apply id="A1.T8.1.1.1.1.1.1.m1.1.1.cmml" xref="A1.T8.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="A1.T8.1.1.1.1.1.1.m1.1.1.1.cmml" xref="A1.T8.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="A1.T8.1.1.1.1.1.1.m1.1.1.2.cmml" xref="A1.T8.1.1.1.1.1.1.m1.1.1.2">𝐐</ci><ci id="A1.T8.1.1.1.1.1.1.m1.1.1.3.cmml" xref="A1.T8.1.1.1.1.1.1.m1.1.1.3">𝐧𝐞𝐰</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.1.1.1.1.1.1.m1.1c">\mathbf{Q_{new}}</annotation></semantics></math></span>
</span>
</td>
<td id="A1.T8.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T8.2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.2.2.2.2.1.1" class="ltx_p" style="width:22.8pt;"><math id="A1.T8.2.2.2.2.1.1.m1.1" class="ltx_Math" alttext="\mathbf{A_{new}}" display="inline"><semantics id="A1.T8.2.2.2.2.1.1.m1.1a"><msub id="A1.T8.2.2.2.2.1.1.m1.1.1" xref="A1.T8.2.2.2.2.1.1.m1.1.1.cmml"><mi id="A1.T8.2.2.2.2.1.1.m1.1.1.2" xref="A1.T8.2.2.2.2.1.1.m1.1.1.2.cmml">𝐀</mi><mi id="A1.T8.2.2.2.2.1.1.m1.1.1.3" xref="A1.T8.2.2.2.2.1.1.m1.1.1.3.cmml">𝐧𝐞𝐰</mi></msub><annotation-xml encoding="MathML-Content" id="A1.T8.2.2.2.2.1.1.m1.1b"><apply id="A1.T8.2.2.2.2.1.1.m1.1.1.cmml" xref="A1.T8.2.2.2.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="A1.T8.2.2.2.2.1.1.m1.1.1.1.cmml" xref="A1.T8.2.2.2.2.1.1.m1.1.1">subscript</csymbol><ci id="A1.T8.2.2.2.2.1.1.m1.1.1.2.cmml" xref="A1.T8.2.2.2.2.1.1.m1.1.1.2">𝐀</ci><ci id="A1.T8.2.2.2.2.1.1.m1.1.1.3.cmml" xref="A1.T8.2.2.2.2.1.1.m1.1.1.3">𝐧𝐞𝐰</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.2.2.2.2.1.1.m1.1c">\mathbf{A_{new}}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="A1.T8.3.3.3" class="ltx_tr">
<td id="A1.T8.3.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" rowspan="3">
<span id="A1.T8.3.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.3.3.3.2.1.1" class="ltx_p" style="width:2.8pt;"><span id="A1.T8.3.3.3.2.1.1.1" class="ltx_text">
<span id="A1.T8.3.3.3.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:39.1pt;vertical-align:-17.1pt;"><span class="ltx_transformed_inner" style="width:39.2pt;transform:translate(-15.19pt,2.92pt) rotate(-90deg) ;">
<span id="A1.T8.3.3.3.2.1.1.1.1.1" class="ltx_p">Negation</span>
</span></span></span></span>
</span>
</td>
<td id="A1.T8.3.3.3.3" class="ltx_td ltx_border_tt"></td>
<td id="A1.T8.3.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T8.3.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.3.3.3.1.1.1" class="ltx_p" style="width:56.9pt;">
<span id="A1.T8.3.3.3.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:56.9pt;"><img src="/html/2012.02356/assets/images/supp_tables/bread.png" id="A1.T8.3.3.3.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="79" height="69" alt="[Uncaptioned image]">
</span></span>
</span>
</td>
<td id="A1.T8.3.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T8.3.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.3.3.3.4.1.1" class="ltx_p" style="width:62.6pt;">Is this bread?</span>
</span>
</td>
<td id="A1.T8.3.3.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T8.3.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.3.3.3.5.1.1" class="ltx_p" style="width:22.8pt;">yes</span>
</span>
</td>
<td id="A1.T8.3.3.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T8.3.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.3.3.3.6.1.1" class="ltx_p" style="width:62.6pt;">Is this not bread</span>
</span>
</td>
<td id="A1.T8.3.3.3.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T8.3.3.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.3.3.3.7.1.1" class="ltx_p" style="width:22.8pt;">no</span>
</span>
</td>
</tr>
<tr id="A1.T8.4.4.4" class="ltx_tr">
<td id="A1.T8.4.4.4.2" class="ltx_td"></td>
<td id="A1.T8.4.4.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.4.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.4.4.4.1.1.1" class="ltx_p" style="width:56.9pt;">
<span id="A1.T8.4.4.4.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:56.9pt;"><img src="/html/2012.02356/assets/images/supp_tables/lady_shirt.png" id="A1.T8.4.4.4.1.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="79" height="118" alt="[Uncaptioned image]">
</span></span>
</span>
</td>
<td id="A1.T8.4.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.4.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.4.4.4.3.1.1" class="ltx_p" style="width:62.6pt;">What is the color of the woman’s shirt?</span>
</span>
</td>
<td id="A1.T8.4.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.4.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.4.4.4.4.1.1" class="ltx_p" style="width:22.8pt;">black</span>
</span>
</td>
<td id="A1.T8.4.4.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.4.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.4.4.4.5.1.1" class="ltx_p" style="width:62.6pt;">What is not the color of the woman’s shirt?</span>
</span>
</td>
<td id="A1.T8.4.4.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.4.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.4.4.4.6.1.1" class="ltx_p" style="width:22.8pt;">white</span>
</span>
</td>
</tr>
<tr id="A1.T8.5.5.5" class="ltx_tr">
<td id="A1.T8.5.5.5.2" class="ltx_td"></td>
<td id="A1.T8.5.5.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.5.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.5.5.5.1.1.1" class="ltx_p" style="width:56.9pt;">
<span id="A1.T8.5.5.5.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:56.9pt;"><img src="/html/2012.02356/assets/images/supp_tables/boy.jpg" id="A1.T8.5.5.5.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="79" height="57" alt="[Uncaptioned image]">
</span></span>
</span>
</td>
<td id="A1.T8.5.5.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.5.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.5.5.5.3.1.1" class="ltx_p" style="width:62.6pt;">Is there a boy?</span>
</span>
</td>
<td id="A1.T8.5.5.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.5.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.5.5.5.4.1.1" class="ltx_p" style="width:22.8pt;">no</span>
</span>
</td>
<td id="A1.T8.5.5.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.5.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.5.5.5.5.1.1" class="ltx_p" style="width:62.6pt;">Is there no boy?</span>
</span>
</td>
<td id="A1.T8.5.5.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.5.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.5.5.5.6.1.1" class="ltx_p" style="width:22.8pt;">yes</span>
</span>
</td>
</tr>
<tr id="A1.T8.6.6.6" class="ltx_tr">
<td id="A1.T8.6.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" rowspan="9">
<span id="A1.T8.6.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.6.6.6.2.1.1" class="ltx_p" style="width:2.8pt;"><span id="A1.T8.6.6.6.2.1.1.1" class="ltx_text">
<span id="A1.T8.6.6.6.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:49.8pt;vertical-align:-21.4pt;"><span class="ltx_transformed_inner" style="width:49.8pt;transform:translate(-21.44pt,0pt) rotate(-90deg) ;">
<span id="A1.T8.6.6.6.2.1.1.1.1.1" class="ltx_p">Adversarial</span>
</span></span></span></span>
</span>
</td>
<td id="A1.T8.6.6.6.3" class="ltx_td ltx_border_t"></td>
<td id="A1.T8.6.6.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.6.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.6.6.6.1.1.1" class="ltx_p" style="width:56.9pt;">
<span id="A1.T8.6.6.6.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:56.9pt;"><img src="/html/2012.02356/assets/x4.jpg" id="A1.T8.6.6.6.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="87" height="69" alt="[Uncaptioned image]">
</span></span>
</span>
</td>
<td id="A1.T8.6.6.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.6.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.6.6.6.4.1.1" class="ltx_p" style="width:62.6pt;">Who is sitting in the boat ?</span>
</span>
</td>
<td id="A1.T8.6.6.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.6.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.6.6.6.5.1.1" class="ltx_p" style="width:22.8pt;">man</span>
</span>
</td>
<td id="A1.T8.6.6.6.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.6.6.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.6.6.6.6.1.1" class="ltx_p" style="width:62.6pt;">Who is sitting in the dining table ?</span>
</span>
</td>
<td id="A1.T8.6.6.6.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.6.6.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.6.6.6.7.1.1" class="ltx_p" style="width:22.8pt;">can’t say</span>
</span>
</td>
</tr>
<tr id="A1.T8.7.7.7" class="ltx_tr">
<td id="A1.T8.7.7.7.2" class="ltx_td"></td>
<td id="A1.T8.7.7.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.7.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.7.7.7.1.1.1" class="ltx_p" style="width:56.9pt;">
<span id="A1.T8.7.7.7.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:56.9pt;"><img src="/html/2012.02356/assets/x5.jpg" id="A1.T8.7.7.7.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="87" height="65" alt="[Uncaptioned image]">
</span></span>
</span>
</td>
<td id="A1.T8.7.7.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.7.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.7.7.7.3.1.1" class="ltx_p" style="width:62.6pt;">How big is the plane ?</span>
</span>
</td>
<td id="A1.T8.7.7.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.7.7.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.7.7.7.4.1.1" class="ltx_p" style="width:22.8pt;">large</span>
</span>
</td>
<td id="A1.T8.7.7.7.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.7.7.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.7.7.7.5.1.1" class="ltx_p" style="width:62.6pt;">How big is the car ?</span>
</span>
</td>
<td id="A1.T8.7.7.7.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T8.7.7.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.7.7.7.6.1.1" class="ltx_p" style="width:22.8pt;">size</span>
</span>
</td>
</tr>
<tr id="A1.T8.8.8.8" class="ltx_tr">
<td id="A1.T8.8.8.8.2" class="ltx_td ltx_border_bb"></td>
<td id="A1.T8.8.8.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A1.T8.8.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.8.8.8.1.1.1" class="ltx_p" style="width:56.9pt;">
<span id="A1.T8.8.8.8.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:56.9pt;"><img src="/html/2012.02356/assets/x6.jpg" id="A1.T8.8.8.8.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="87" height="63" alt="[Uncaptioned image]">
</span></span>
</span>
</td>
<td id="A1.T8.8.8.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A1.T8.8.8.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.8.8.8.3.1.1" class="ltx_p" style="width:62.6pt;">How many puppies are on the bed ?</span>
</span>
</td>
<td id="A1.T8.8.8.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A1.T8.8.8.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.8.8.8.4.1.1" class="ltx_p" style="width:22.8pt;">two</span>
</span>
</td>
<td id="A1.T8.8.8.8.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A1.T8.8.8.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.8.8.8.5.1.1" class="ltx_p" style="width:62.6pt;">How many cats are on the bed?</span>
</span>
</td>
<td id="A1.T8.8.8.8.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A1.T8.8.8.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T8.8.8.8.6.1.1" class="ltx_p" style="width:22.8pt;">none</span>
</span>
</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span>The effect of using transformations (T) to create new Q-A pairs</figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Dataset Analysis</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">In Table <a href="#A2.T9" title="Table 9 ‣ Appendix B Dataset Analysis ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, we compare the distribution per answer-type of our synthetically generated samples with the distribution in the VQA-CP-v2 <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite> dataset.
Since we use our synthetic samples as the pre-training data, and do not use VQA-CP samples for training in our zero-shot setup, this comparison displays the shift between the training (synthetic) and test (human annotated VQA-CP) datasets.</p>
</div>
<figure id="A2.T9" class="ltx_table">
<table id="A2.T9.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A2.T9.1.1" class="ltx_tr">
<td id="A2.T9.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A2.T9.1.1.1.1" class="ltx_text ltx_font_bold">Category</span></td>
<td id="A2.T9.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T9.1.1.2.1" class="ltx_text ltx_font_bold">VQA-CP (%)</span></td>
<td id="A2.T9.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T9.1.1.3.1" class="ltx_text ltx_font_bold">Pretraining (%)</span></td>
</tr>
<tr id="A2.T9.1.2" class="ltx_tr">
<td id="A2.T9.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Yes/No</td>
<td id="A2.T9.1.2.2" class="ltx_td ltx_align_center ltx_border_t">41.86</td>
<td id="A2.T9.1.2.3" class="ltx_td ltx_align_center ltx_border_t">50.18</td>
</tr>
<tr id="A2.T9.1.3" class="ltx_tr">
<td id="A2.T9.1.3.1" class="ltx_td ltx_align_left">Number</td>
<td id="A2.T9.1.3.2" class="ltx_td ltx_align_center">11.91</td>
<td id="A2.T9.1.3.3" class="ltx_td ltx_align_center">8.32</td>
</tr>
<tr id="A2.T9.1.4" class="ltx_tr">
<td id="A2.T9.1.4.1" class="ltx_td ltx_align_left ltx_border_bb">Other</td>
<td id="A2.T9.1.4.2" class="ltx_td ltx_align_center ltx_border_bb">46.23</td>
<td id="A2.T9.1.4.3" class="ltx_td ltx_align_center ltx_border_bb">41.45</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Distribution of samples by answer-type in our pre-training dataset and the VQA-CP evaludation dataset.</figcaption>
</figure>
<figure id="A2.T10" class="ltx_table">
<table id="A2.T10.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A2.T10.2.3" class="ltx_tr">
<td id="A2.T10.2.3.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A2.T10.2.3.1.1" class="ltx_text ltx_font_bold">Hyper-Parameters</span></td>
<td id="A2.T10.2.3.2" class="ltx_td ltx_border_tt"></td>
<td id="A2.T10.2.3.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="A2.T10.2.3.3.1" class="ltx_text ltx_font_bold">Model</span></td>
</tr>
<tr id="A2.T10.2.4" class="ltx_tr">
<td id="A2.T10.2.4.1" class="ltx_td ltx_align_left ltx_border_tt">Batch Size</td>
<td id="A2.T10.2.4.2" class="ltx_td ltx_border_tt"></td>
<td id="A2.T10.2.4.3" class="ltx_td ltx_align_left ltx_border_tt">32-128</td>
</tr>
<tr id="A2.T10.2.2" class="ltx_tr">
<td id="A2.T10.2.2.3" class="ltx_td ltx_align_left">Learning Rate</td>
<td id="A2.T10.2.2.4" class="ltx_td"></td>
<td id="A2.T10.2.2.2" class="ltx_td ltx_align_left">(<math id="A2.T10.1.1.1.m1.1" class="ltx_Math" alttext="1e^{-5}" display="inline"><semantics id="A2.T10.1.1.1.m1.1a"><mrow id="A2.T10.1.1.1.m1.1.1" xref="A2.T10.1.1.1.m1.1.1.cmml"><mn id="A2.T10.1.1.1.m1.1.1.2" xref="A2.T10.1.1.1.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="A2.T10.1.1.1.m1.1.1.1" xref="A2.T10.1.1.1.m1.1.1.1.cmml">​</mo><msup id="A2.T10.1.1.1.m1.1.1.3" xref="A2.T10.1.1.1.m1.1.1.3.cmml"><mi id="A2.T10.1.1.1.m1.1.1.3.2" xref="A2.T10.1.1.1.m1.1.1.3.2.cmml">e</mi><mrow id="A2.T10.1.1.1.m1.1.1.3.3" xref="A2.T10.1.1.1.m1.1.1.3.3.cmml"><mo id="A2.T10.1.1.1.m1.1.1.3.3a" xref="A2.T10.1.1.1.m1.1.1.3.3.cmml">−</mo><mn id="A2.T10.1.1.1.m1.1.1.3.3.2" xref="A2.T10.1.1.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.T10.1.1.1.m1.1b"><apply id="A2.T10.1.1.1.m1.1.1.cmml" xref="A2.T10.1.1.1.m1.1.1"><times id="A2.T10.1.1.1.m1.1.1.1.cmml" xref="A2.T10.1.1.1.m1.1.1.1"></times><cn type="integer" id="A2.T10.1.1.1.m1.1.1.2.cmml" xref="A2.T10.1.1.1.m1.1.1.2">1</cn><apply id="A2.T10.1.1.1.m1.1.1.3.cmml" xref="A2.T10.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="A2.T10.1.1.1.m1.1.1.3.1.cmml" xref="A2.T10.1.1.1.m1.1.1.3">superscript</csymbol><ci id="A2.T10.1.1.1.m1.1.1.3.2.cmml" xref="A2.T10.1.1.1.m1.1.1.3.2">𝑒</ci><apply id="A2.T10.1.1.1.m1.1.1.3.3.cmml" xref="A2.T10.1.1.1.m1.1.1.3.3"><minus id="A2.T10.1.1.1.m1.1.1.3.3.1.cmml" xref="A2.T10.1.1.1.m1.1.1.3.3"></minus><cn type="integer" id="A2.T10.1.1.1.m1.1.1.3.3.2.cmml" xref="A2.T10.1.1.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.1.1.1.m1.1c">1e^{-5}</annotation></semantics></math>, <math id="A2.T10.2.2.2.m2.1" class="ltx_Math" alttext="5e^{-5}" display="inline"><semantics id="A2.T10.2.2.2.m2.1a"><mrow id="A2.T10.2.2.2.m2.1.1" xref="A2.T10.2.2.2.m2.1.1.cmml"><mn id="A2.T10.2.2.2.m2.1.1.2" xref="A2.T10.2.2.2.m2.1.1.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="A2.T10.2.2.2.m2.1.1.1" xref="A2.T10.2.2.2.m2.1.1.1.cmml">​</mo><msup id="A2.T10.2.2.2.m2.1.1.3" xref="A2.T10.2.2.2.m2.1.1.3.cmml"><mi id="A2.T10.2.2.2.m2.1.1.3.2" xref="A2.T10.2.2.2.m2.1.1.3.2.cmml">e</mi><mrow id="A2.T10.2.2.2.m2.1.1.3.3" xref="A2.T10.2.2.2.m2.1.1.3.3.cmml"><mo id="A2.T10.2.2.2.m2.1.1.3.3a" xref="A2.T10.2.2.2.m2.1.1.3.3.cmml">−</mo><mn id="A2.T10.2.2.2.m2.1.1.3.3.2" xref="A2.T10.2.2.2.m2.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.T10.2.2.2.m2.1b"><apply id="A2.T10.2.2.2.m2.1.1.cmml" xref="A2.T10.2.2.2.m2.1.1"><times id="A2.T10.2.2.2.m2.1.1.1.cmml" xref="A2.T10.2.2.2.m2.1.1.1"></times><cn type="integer" id="A2.T10.2.2.2.m2.1.1.2.cmml" xref="A2.T10.2.2.2.m2.1.1.2">5</cn><apply id="A2.T10.2.2.2.m2.1.1.3.cmml" xref="A2.T10.2.2.2.m2.1.1.3"><csymbol cd="ambiguous" id="A2.T10.2.2.2.m2.1.1.3.1.cmml" xref="A2.T10.2.2.2.m2.1.1.3">superscript</csymbol><ci id="A2.T10.2.2.2.m2.1.1.3.2.cmml" xref="A2.T10.2.2.2.m2.1.1.3.2">𝑒</ci><apply id="A2.T10.2.2.2.m2.1.1.3.3.cmml" xref="A2.T10.2.2.2.m2.1.1.3.3"><minus id="A2.T10.2.2.2.m2.1.1.3.3.1.cmml" xref="A2.T10.2.2.2.m2.1.1.3.3"></minus><cn type="integer" id="A2.T10.2.2.2.m2.1.1.3.3.2.cmml" xref="A2.T10.2.2.2.m2.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.2.2.2.m2.1c">5e^{-5}</annotation></semantics></math> )</td>
</tr>
<tr id="A2.T10.2.5" class="ltx_tr">
<td id="A2.T10.2.5.1" class="ltx_td ltx_align_left">Dropout</td>
<td id="A2.T10.2.5.2" class="ltx_td"></td>
<td id="A2.T10.2.5.3" class="ltx_td ltx_align_left">0.1</td>
</tr>
<tr id="A2.T10.2.6" class="ltx_tr">
<td id="A2.T10.2.6.1" class="ltx_td ltx_align_left">Language Layers</td>
<td id="A2.T10.2.6.2" class="ltx_td"></td>
<td id="A2.T10.2.6.3" class="ltx_td ltx_align_left">6</td>
</tr>
<tr id="A2.T10.2.7" class="ltx_tr">
<td id="A2.T10.2.7.1" class="ltx_td ltx_align_left">Cross-Modality Layer</td>
<td id="A2.T10.2.7.2" class="ltx_td"></td>
<td id="A2.T10.2.7.3" class="ltx_td ltx_align_left">4 — 12</td>
</tr>
<tr id="A2.T10.2.8" class="ltx_tr">
<td id="A2.T10.2.8.1" class="ltx_td ltx_align_left">Optimizer</td>
<td id="A2.T10.2.8.2" class="ltx_td"></td>
<td id="A2.T10.2.8.3" class="ltx_td ltx_align_left">BertAdam</td>
</tr>
<tr id="A2.T10.2.9" class="ltx_tr">
<td id="A2.T10.2.9.1" class="ltx_td ltx_align_left">Warmup</td>
<td id="A2.T10.2.9.2" class="ltx_td"></td>
<td id="A2.T10.2.9.3" class="ltx_td ltx_align_left">0.1</td>
</tr>
<tr id="A2.T10.2.10" class="ltx_tr">
<td id="A2.T10.2.10.1" class="ltx_td ltx_align_left">Max Gradient Norm</td>
<td id="A2.T10.2.10.2" class="ltx_td"></td>
<td id="A2.T10.2.10.3" class="ltx_td ltx_align_left">5.0</td>
</tr>
<tr id="A2.T10.2.11" class="ltx_tr">
<td id="A2.T10.2.11.1" class="ltx_td ltx_align_left">Max Text Length</td>
<td id="A2.T10.2.11.2" class="ltx_td"></td>
<td id="A2.T10.2.11.3" class="ltx_td ltx_align_left">30</td>
</tr>
<tr id="A2.T10.2.12" class="ltx_tr">
<td id="A2.T10.2.12.1" class="ltx_td ltx_align_left">ResNet</td>
<td id="A2.T10.2.12.2" class="ltx_td"></td>
<td id="A2.T10.2.12.3" class="ltx_td ltx_align_left">50 / 101 / 152</td>
</tr>
<tr id="A2.T10.2.13" class="ltx_tr">
<td id="A2.T10.2.13.1" class="ltx_td ltx_align_left ltx_border_bb">Epochs</td>
<td id="A2.T10.2.13.2" class="ltx_td ltx_border_bb"></td>
<td id="A2.T10.2.13.3" class="ltx_td ltx_align_left ltx_border_bb">10-40</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Hyper-Parameters for our models</figcaption>
</figure>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.1" class="ltx_p">We further analyze this shift, by computing the t-SNE projections of questions using mean-pooled Glove <cite class="ltx_cite ltx_citemacro_cite">Pennington et al. (<a href="#bib.bib56" title="" class="ltx_ref">2014</a>)</cite> embeddings for our generated questions and observe the overlap with human-authored questions in VQA and GQA <cite class="ltx_cite ltx_citemacro_cite">Hudson and Manning (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite>.
Figure <a href="#A2.F6" title="Figure 6 ‣ Appendix B Dataset Analysis ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
We observe a marked shift between the question clusters for our procedurally generated questions and human annotated questions from VQA and GQA.</p>
</div>
<figure id="A2.T11" class="ltx_table">
<div id="A2.T11.3" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:97.6pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-82.3pt,18.4pt) scale(0.724819985619764,0.724819985619764) ;">
<table id="A2.T11.3.3" class="ltx_tabular ltx_align_middle">
<tr id="A2.T11.2.2.2" class="ltx_tr">
<td id="A2.T11.2.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A2.T11.2.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.2.2.3.1.1" class="ltx_p" style="width:91.0pt;"><span id="A2.T11.2.2.2.3.1.1.1" class="ltx_text ltx_font_bold">Image</span></span>
</span>
</td>
<td id="A2.T11.2.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A2.T11.2.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.2.2.4.1.1" class="ltx_p" style="width:150.8pt;"><span id="A2.T11.2.2.2.4.1.1.1" class="ltx_text ltx_font_bold">Q</span></span>
</span>
</td>
<td id="A2.T11.2.2.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A2.T11.2.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.2.2.5.1.1" class="ltx_p" style="width:71.1pt;"><span id="A2.T11.2.2.2.5.1.1.1" class="ltx_text ltx_font_bold">A</span></span>
</span>
</td>
<td id="A2.T11.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A2.T11.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.1.1.1.1.1.1" class="ltx_p" style="width:150.8pt;"><math id="A2.T11.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{Q_{new}}" display="inline"><semantics id="A2.T11.1.1.1.1.1.1.m1.1a"><msub id="A2.T11.1.1.1.1.1.1.m1.1.1" xref="A2.T11.1.1.1.1.1.1.m1.1.1.cmml"><mi id="A2.T11.1.1.1.1.1.1.m1.1.1.2" xref="A2.T11.1.1.1.1.1.1.m1.1.1.2.cmml">𝐐</mi><mi id="A2.T11.1.1.1.1.1.1.m1.1.1.3" xref="A2.T11.1.1.1.1.1.1.m1.1.1.3.cmml">𝐧𝐞𝐰</mi></msub><annotation-xml encoding="MathML-Content" id="A2.T11.1.1.1.1.1.1.m1.1b"><apply id="A2.T11.1.1.1.1.1.1.m1.1.1.cmml" xref="A2.T11.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.T11.1.1.1.1.1.1.m1.1.1.1.cmml" xref="A2.T11.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="A2.T11.1.1.1.1.1.1.m1.1.1.2.cmml" xref="A2.T11.1.1.1.1.1.1.m1.1.1.2">𝐐</ci><ci id="A2.T11.1.1.1.1.1.1.m1.1.1.3.cmml" xref="A2.T11.1.1.1.1.1.1.m1.1.1.3">𝐧𝐞𝐰</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T11.1.1.1.1.1.1.m1.1c">\mathbf{Q_{new}}</annotation></semantics></math></span>
</span>
</td>
<td id="A2.T11.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A2.T11.2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.2.2.2.1.1" class="ltx_p" style="width:71.1pt;"><math id="A2.T11.2.2.2.2.1.1.m1.1" class="ltx_Math" alttext="\mathbf{A_{new}}" display="inline"><semantics id="A2.T11.2.2.2.2.1.1.m1.1a"><msub id="A2.T11.2.2.2.2.1.1.m1.1.1" xref="A2.T11.2.2.2.2.1.1.m1.1.1.cmml"><mi id="A2.T11.2.2.2.2.1.1.m1.1.1.2" xref="A2.T11.2.2.2.2.1.1.m1.1.1.2.cmml">𝐀</mi><mi id="A2.T11.2.2.2.2.1.1.m1.1.1.3" xref="A2.T11.2.2.2.2.1.1.m1.1.1.3.cmml">𝐧𝐞𝐰</mi></msub><annotation-xml encoding="MathML-Content" id="A2.T11.2.2.2.2.1.1.m1.1b"><apply id="A2.T11.2.2.2.2.1.1.m1.1.1.cmml" xref="A2.T11.2.2.2.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.T11.2.2.2.2.1.1.m1.1.1.1.cmml" xref="A2.T11.2.2.2.2.1.1.m1.1.1">subscript</csymbol><ci id="A2.T11.2.2.2.2.1.1.m1.1.1.2.cmml" xref="A2.T11.2.2.2.2.1.1.m1.1.1.2">𝐀</ci><ci id="A2.T11.2.2.2.2.1.1.m1.1.1.3.cmml" xref="A2.T11.2.2.2.2.1.1.m1.1.1.3">𝐧𝐞𝐰</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T11.2.2.2.2.1.1.m1.1c">\mathbf{A_{new}}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="A2.T11.3.3.3" class="ltx_tr">
<td id="A2.T11.3.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_tt" rowspan="5">
<span id="A2.T11.3.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.3.1.1.1" class="ltx_p" style="width:91.0pt;"><span id="A2.T11.3.3.3.1.1.1.1" class="ltx_text">
<span id="A2.T11.3.3.3.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:99.6pt;"><img src="/html/2012.02356/assets/images/supp_tables/cars.png" id="A2.T11.3.3.3.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="138" height="101" alt="[Uncaptioned image]">
</span></span></span>
</span>
</td>
<td id="A2.T11.3.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A2.T11.3.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.3.2.1.1" class="ltx_p" style="width:150.8pt;">How is something parked ?</span>
</span>
</td>
<td id="A2.T11.3.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A2.T11.3.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.3.3.1.1" class="ltx_p" style="width:71.1pt;">illegally</span>
</span>
</td>
<td id="A2.T11.3.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A2.T11.3.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.3.4.1.1" class="ltx_p" style="width:150.8pt;">How’s-what’s parked?</span>
</span>
</td>
<td id="A2.T11.3.3.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A2.T11.3.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.3.5.1.1" class="ltx_p" style="width:71.1pt;">illegally</span>
</span>
</td>
</tr>
<tr id="A2.T11.3.3.4" class="ltx_tr">
<td id="A2.T11.3.3.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.T11.3.3.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.4.1.1.1" class="ltx_p" style="width:150.8pt;">what does something seem to do ?</span>
</span>
</td>
<td id="A2.T11.3.3.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.T11.3.3.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.4.2.1.1" class="ltx_p" style="width:71.1pt;">park</span>
</span>
</td>
<td id="A2.T11.3.3.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.T11.3.3.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.4.3.1.1" class="ltx_p" style="width:150.8pt;">What do you think something seems to be doing?</span>
</span>
</td>
<td id="A2.T11.3.3.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.T11.3.3.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.4.4.1.1" class="ltx_p" style="width:71.1pt;">park</span>
</span>
</td>
</tr>
<tr id="A2.T11.3.3.5" class="ltx_tr">
<td id="A2.T11.3.3.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.T11.3.3.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.5.1.1.1" class="ltx_p" style="width:150.8pt;">Where was parked something?</span>
</span>
</td>
<td id="A2.T11.3.3.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.T11.3.3.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.5.2.1.1" class="ltx_p" style="width:71.1pt;">behind a legally parked car</span>
</span>
</td>
<td id="A2.T11.3.3.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.T11.3.3.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.5.3.1.1" class="ltx_p" style="width:150.8pt;">Do you know where something was parked?</span>
</span>
</td>
<td id="A2.T11.3.3.5.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.T11.3.3.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.5.4.1.1" class="ltx_p" style="width:71.1pt;">behind a legally parked car</span>
</span>
</td>
</tr>
<tr id="A2.T11.3.3.6" class="ltx_tr">
<td id="A2.T11.3.3.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.T11.3.3.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.6.1.1.1" class="ltx_p" style="width:150.8pt;">How many cars are visible ?</span>
</span>
</td>
<td id="A2.T11.3.3.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.T11.3.3.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.6.2.1.1" class="ltx_p" style="width:71.1pt;">2</span>
</span>
</td>
<td id="A2.T11.3.3.6.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.T11.3.3.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.6.3.1.1" class="ltx_p" style="width:150.8pt;">How many cars are we looking at?</span>
</span>
</td>
<td id="A2.T11.3.3.6.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A2.T11.3.3.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.6.4.1.1" class="ltx_p" style="width:71.1pt;">2</span>
</span>
</td>
</tr>
<tr id="A2.T11.3.3.7" class="ltx_tr">
<td id="A2.T11.3.3.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="A2.T11.3.3.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.7.1.1.1" class="ltx_p" style="width:150.8pt;">Is there two cars parked on the sidewalk on the street ?</span>
</span>
</td>
<td id="A2.T11.3.3.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="A2.T11.3.3.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.7.2.1.1" class="ltx_p" style="width:71.1pt;">Yes</span>
</span>
</td>
<td id="A2.T11.3.3.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="A2.T11.3.3.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.7.3.1.1" class="ltx_p" style="width:150.8pt;">There are two cars parked on the sidewalk, right?</span>
</span>
</td>
<td id="A2.T11.3.3.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="A2.T11.3.3.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.3.3.7.4.1.1" class="ltx_p" style="width:71.1pt;">Yes</span>
</span>
</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 11: </span>Illustration of using paraphrasing to improve the linguistic variation of our questions and answers.</figcaption>
</figure>
<figure id="A2.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F6.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2012.02356/assets/images/tsne-full.png" id="A2.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="126" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F6.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2012.02356/assets/images/tsne-gqa.png" id="A2.F6.2.g1" class="ltx_graphics ltx_img_landscape" width="157" height="126" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F6.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2012.02356/assets/images/tsne-pretrain.png" id="A2.F6.3.g1" class="ltx_graphics ltx_img_landscape" width="157" height="126" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F6.4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2012.02356/assets/images/tsne_vqa.png" id="A2.F6.4.g1" class="ltx_graphics ltx_img_landscape" width="157" height="126" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>t-SNE projections of Glove embedding our generated questions, and human-authored VQA-v2 and GQA questions. Blue: our pretraining dataset, Orange: GQA, Green: VQA. L-R: All, GQA, Pretrain, VQA.</figcaption>
</figure>
<figure id="A2.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2012.02356/assets/images/yn_dist.png" id="A2.F7.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="384" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>“Yes-No”</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2012.02356/assets/images/numeric_ans.png" id="A2.F7.sf2.g1" class="ltx_graphics ltx_img_square" width="598" height="536" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>“Numeric”</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F7.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2012.02356/assets/images/other_gt_500.png" id="A2.F7.sf3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="387" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>”Other”: Highly frequent 
<br class="ltx_break">answers (count <math id="A2.F7.sf3.2.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="A2.F7.sf3.2.m1.1b"><mo id="A2.F7.sf3.2.m1.1.1" xref="A2.F7.sf3.2.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="A2.F7.sf3.2.m1.1c"><gt id="A2.F7.sf3.2.m1.1.1.cmml" xref="A2.F7.sf3.2.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="A2.F7.sf3.2.m1.1d">&gt;</annotation></semantics></math> 500)</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="A2.F7.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2012.02356/assets/images/other_gt_200_lt_500.png" id="A2.F7.sf4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="391" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>“Other”:Answers with 
<br class="ltx_break">count between 200 and 500</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Distribution of most frequent answers in our Pretraining dataset for each answer-type (yes-no, numeric, and other). Please zoom for details.</figcaption>
</figure>
<div id="A2.p3" class="ltx_para">
<p id="A2.p3.3" class="ltx_p">Similarly, we also show the distribution of answers in our dataset in Figure <a href="#A2.F7" title="Figure 7 ‣ Appendix B Dataset Analysis ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
It can be seen that our dataset has a slight imbalance in the proportion of questions with answer “yes” and “no”.
Numeric answers <span id="A2.p3.3.1" class="ltx_text ltx_font_italic">0,1,2,3</span> are most frequent.
Answers about people such as <span id="A2.p3.3.2" class="ltx_text ltx_font_italic">man, woman, people, person, group of people</span> are also more common in the dataset.
The remaining answers have a long-tailed distribution, since there are <math id="A2.p3.1.m1.1" class="ltx_Math" alttext="\sim 90k" display="inline"><semantics id="A2.p3.1.m1.1a"><mrow id="A2.p3.1.m1.1.1" xref="A2.p3.1.m1.1.1.cmml"><mi id="A2.p3.1.m1.1.1.2" xref="A2.p3.1.m1.1.1.2.cmml"></mi><mo id="A2.p3.1.m1.1.1.1" xref="A2.p3.1.m1.1.1.1.cmml">∼</mo><mrow id="A2.p3.1.m1.1.1.3" xref="A2.p3.1.m1.1.1.3.cmml"><mn id="A2.p3.1.m1.1.1.3.2" xref="A2.p3.1.m1.1.1.3.2.cmml">90</mn><mo lspace="0em" rspace="0em" id="A2.p3.1.m1.1.1.3.1" xref="A2.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="A2.p3.1.m1.1.1.3.3" xref="A2.p3.1.m1.1.1.3.3.cmml">k</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p3.1.m1.1b"><apply id="A2.p3.1.m1.1.1.cmml" xref="A2.p3.1.m1.1.1"><csymbol cd="latexml" id="A2.p3.1.m1.1.1.1.cmml" xref="A2.p3.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A2.p3.1.m1.1.1.2.cmml" xref="A2.p3.1.m1.1.1.2">absent</csymbol><apply id="A2.p3.1.m1.1.1.3.cmml" xref="A2.p3.1.m1.1.1.3"><times id="A2.p3.1.m1.1.1.3.1.cmml" xref="A2.p3.1.m1.1.1.3.1"></times><cn type="integer" id="A2.p3.1.m1.1.1.3.2.cmml" xref="A2.p3.1.m1.1.1.3.2">90</cn><ci id="A2.p3.1.m1.1.1.3.3.cmml" xref="A2.p3.1.m1.1.1.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.1.m1.1c">\sim 90k</annotation></semantics></math> unique answers in our dataset compared to <math id="A2.p3.2.m2.1" class="ltx_Math" alttext="\sim 3.5k" display="inline"><semantics id="A2.p3.2.m2.1a"><mrow id="A2.p3.2.m2.1.1" xref="A2.p3.2.m2.1.1.cmml"><mi id="A2.p3.2.m2.1.1.2" xref="A2.p3.2.m2.1.1.2.cmml"></mi><mo id="A2.p3.2.m2.1.1.1" xref="A2.p3.2.m2.1.1.1.cmml">∼</mo><mrow id="A2.p3.2.m2.1.1.3" xref="A2.p3.2.m2.1.1.3.cmml"><mn id="A2.p3.2.m2.1.1.3.2" xref="A2.p3.2.m2.1.1.3.2.cmml">3.5</mn><mo lspace="0em" rspace="0em" id="A2.p3.2.m2.1.1.3.1" xref="A2.p3.2.m2.1.1.3.1.cmml">​</mo><mi id="A2.p3.2.m2.1.1.3.3" xref="A2.p3.2.m2.1.1.3.3.cmml">k</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p3.2.m2.1b"><apply id="A2.p3.2.m2.1.1.cmml" xref="A2.p3.2.m2.1.1"><csymbol cd="latexml" id="A2.p3.2.m2.1.1.1.cmml" xref="A2.p3.2.m2.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A2.p3.2.m2.1.1.2.cmml" xref="A2.p3.2.m2.1.1.2">absent</csymbol><apply id="A2.p3.2.m2.1.1.3.cmml" xref="A2.p3.2.m2.1.1.3"><times id="A2.p3.2.m2.1.1.3.1.cmml" xref="A2.p3.2.m2.1.1.3.1"></times><cn type="float" id="A2.p3.2.m2.1.1.3.2.cmml" xref="A2.p3.2.m2.1.1.3.2">3.5</cn><ci id="A2.p3.2.m2.1.1.3.3.cmml" xref="A2.p3.2.m2.1.1.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.2.m2.1c">\sim 3.5k</annotation></semantics></math> in VQA and <math id="A2.p3.3.m3.1" class="ltx_Math" alttext="\sim 2k" display="inline"><semantics id="A2.p3.3.m3.1a"><mrow id="A2.p3.3.m3.1.1" xref="A2.p3.3.m3.1.1.cmml"><mi id="A2.p3.3.m3.1.1.2" xref="A2.p3.3.m3.1.1.2.cmml"></mi><mo id="A2.p3.3.m3.1.1.1" xref="A2.p3.3.m3.1.1.1.cmml">∼</mo><mrow id="A2.p3.3.m3.1.1.3" xref="A2.p3.3.m3.1.1.3.cmml"><mn id="A2.p3.3.m3.1.1.3.2" xref="A2.p3.3.m3.1.1.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="A2.p3.3.m3.1.1.3.1" xref="A2.p3.3.m3.1.1.3.1.cmml">​</mo><mi id="A2.p3.3.m3.1.1.3.3" xref="A2.p3.3.m3.1.1.3.3.cmml">k</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p3.3.m3.1b"><apply id="A2.p3.3.m3.1.1.cmml" xref="A2.p3.3.m3.1.1"><csymbol cd="latexml" id="A2.p3.3.m3.1.1.1.cmml" xref="A2.p3.3.m3.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A2.p3.3.m3.1.1.2.cmml" xref="A2.p3.3.m3.1.1.2">absent</csymbol><apply id="A2.p3.3.m3.1.1.3.cmml" xref="A2.p3.3.m3.1.1.3"><times id="A2.p3.3.m3.1.1.3.1.cmml" xref="A2.p3.3.m3.1.1.3.1"></times><cn type="integer" id="A2.p3.3.m3.1.1.3.2.cmml" xref="A2.p3.3.m3.1.1.3.2">2</cn><ci id="A2.p3.3.m3.1.1.3.3.cmml" xref="A2.p3.3.m3.1.1.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.3.m3.1c">\sim 2k</annotation></semantics></math> in GQA.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Training Details</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">We use the HuggingFace <cite class="ltx_cite ltx_citemacro_cite">Wolf et al. (<a href="#bib.bib83" title="" class="ltx_ref">2019</a>)</cite> and PyTorch frameworks <cite class="ltx_cite ltx_citemacro_cite">Paszke et al. (<a href="#bib.bib55" title="" class="ltx_ref">2019</a>)</cite>.
Hyperparameters and other training settings are given in Table <a href="#A2.T10" title="Table 10 ‣ Appendix B Dataset Analysis ‣ WeaQA: Weak Supervision via Captions for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2012.02355" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2012.02356" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2012.02356">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2012.02356" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2012.02357" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar  7 23:37:00 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
