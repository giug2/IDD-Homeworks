<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.09545] Seamless Integration: Sampling Strategies in Federated Learning Systems</title><meta property="og:description" content="Federated Learning (FL) represents a paradigm shift in the field of machine learning, offering an approach for a decentralized training of models across a multitude of devices while maintaining the privacy of local dat…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Seamless Integration: Sampling Strategies in Federated Learning Systems">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Seamless Integration: Sampling Strategies in Federated Learning Systems">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.09545">

<!--Generated on Thu Sep  5 14:55:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated Learning,  Decentralized Learning,  Manufacturing Collaboration,  Data Heterogeneity
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">© 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.</p>
</div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p">Accepted to be published in: The 2nd IEEE International Conference on Federated Learning Technologies and Applications (FLTA24)</p>
</div>
<h1 class="ltx_title ltx_title_document">Seamless Integration: Sampling Strategies in Federated Learning Systems
<br class="ltx_break">
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">1<sup id="id1.1.id1" class="ltx_sup">st</sup> Tatjana Legler
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.2.id1" class="ltx_text ltx_font_italic">University of Kaiserslautern-Landau (RPTU)
<br class="ltx_break">German Research Center for Artificial Intelligence (DFKI)
<br class="ltx_break"></span>Kaiserslautern, Germany 
<br class="ltx_break">0000-0002-7297-0845
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">2<sup id="id3.1.id1" class="ltx_sup">nd</sup> Vinit Hegiste


 <span id="id4.2.id2" class="ltx_text"></span><span id="id5.3.id3" class="ltx_text"></span> <span id="id6.4.id4" class="ltx_ERROR undefined">{@IEEEauthorhalign}</span>
3<sup id="id7.5.id5" class="ltx_sup">rd</sup> Martin Ruskowski
<span id="id8.6.id6" class="ltx_text ltx_font_italic">University of Kaiserslautern-Landau (RPTU)
<br class="ltx_break">German Research Center for Artificial Intelligence (DFKI)
<br class="ltx_break"></span>Kaiserslautern, Germany
<br class="ltx_break">0000-0002-6534-9057
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id9.7.id1" class="ltx_text ltx_font_italic">University of Kaiserslautern-Landau (RPTU)
<br class="ltx_break"></span>Kaiserslautern, Germany 
<br class="ltx_break">0000-0001-6944-1988
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id10.id1" class="ltx_p">Federated Learning (FL) represents a paradigm shift in the field of machine learning, offering an approach for a decentralized training of models across a multitude of devices while maintaining the privacy of local data. However, the dynamic nature of FL systems, characterized by the ongoing incorporation of new clients with potentially diverse data distributions and computational capabilities, poses a significant challenge to the stability and efficiency of these distributed learning networks. The seamless integration of new clients is imperative to sustain and enhance the performance and robustness of FL systems. This paper looks into the complexities of integrating new clients into existing FL systems and explores how data heterogeneity and varying data distribution (not independent and identically distributed) among them can affect model training, system efficiency, scalability and stability. Despite these challenges, the integration of new clients into FL systems presents opportunities to enhance data diversity, improve learning performance, and leverage distributed computational power. In contrast to other fields of application such as the distributed optimization of word predictions on Gboard (where federated learning once originated), there are usually only a few clients in the production environment, which is why information from each new client becomes all the more valuable. This paper outlines strategies for effective client selection strategies and solutions for ensuring system scalability and stability. Using the example of images from optical quality inspection, it offers insights into practical approaches. In conclusion, this paper proposes that addressing the challenges presented by new client integration is crucial to the advancement and efficiency of distributed learning networks, thus paving the way for the adoption of Federated Learning in production environments.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated Learning, Decentralized Learning, Manufacturing Collaboration, Data Heterogeneity

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated Learning (FL) has emerged as a transformative approach in the field of machine learning, fundamentally changing how data is utilized and models are trained across distributed networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. In contrast to traditional centralized machine learning paradigms, FL allows numerous devices to collaboratively train a shared model while retaining all training data locally on the devices. This approach addresses significant concerns related to data privacy, security, and access rights <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The decentralized nature of FL not only reduces the risks associated with data centralization but also enables the use of data that was previously inaccessible or unusable due to privacy constraints.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The applications of machine learning in manufacturing span a wide range, from well-established uses such as fault diagnostics, process monitoring, and anomaly detection to more recent innovations like enhancing human-robot collaboration with the aid of Generative Pre-trained Transformers (GPTs) and optimizing construction designs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. The demand for efficient data processing, coupled with the sensitive and proprietary nature of manufacturing data, presents numerous challenges for machine learning applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. These challenges are exacerbated in the dynamic environment of FL, particularly with the continuous integration of new clients. These new participants, characterized by potentially diverse data distributions and varying computational capabilities, introduce complexities that can impact the stability, efficiency, and scalability of FL systems. The heterogeneity in data, often non-independent and identically distributed (non-IID), along with the variance in computational resources among clients, necessitates innovative strategies to seamlessly incorporate these new participants into existing networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The introduction of new clients within a FL system offers the potential to enhance the diversity of the data pool, thereby improving the robustness and performance of the collective learning process. Each new client contributes unique insights and perspectives, enriching the network and making the model more comprehensive and better suited for real-world applications. This is especially valuable in fields like optical quality inspection, where variations in data across different production lines can significantly influence the effectiveness of predictive models.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To fully harness the potential benefits of FL, it is crucial to address the challenges related to the integration of new clients. This requires the use or development of strategies that enable effective onboarding and maintain the stability of the FL system. In this paper, we focus on client selection techniques as a foundation for future research into client integration, examining the complexities introduced by data heterogeneity and proposing strategies for efficient client integration.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">For decades, various approaches, such as zero-defect manufacturing and lean Six Sigma methodologies, have been employed in production environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. These methods have successfully reduced defect rates to levels measurable in ”parts per million” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. However, with the increasing variety of product variants, these traditional methods are reaching their limits <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. To maintain quality assurance amidst this diversity, machine learning methods are increasingly being applied <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Several strategies exist to overcome data scarcity and expand the usable data pool. Generative AI methods, for example, can create synthetic data for multiple product variants and configurations, including training data for various error cases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Additionally, breaking down existing data silos can increase both the volume and diversity of available data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. In the following, we explore FL as a potential approach to achieve this while preserving data privacy. Additionally, synthetic data and real images can be merged with FL, enhancing the robustness and applicability of machine learning models in production, as shown in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">State of the Art</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The manufacturing industry is a data-intensive sector with numerous actors generating data from various sources, including sensors, production processes, and quality control systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. However, the data in manufacturing are often sensitive and protected, complicating the sharing or collaborative training of machine learning models on a shared dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. FL offers a decentralized approach that aims to leverage a broad, heterogeneous database without the need to share raw data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Unlike traditional centralized learning, where models are trained on large datasets housed on a central server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, FL only shares model parameters, such as network weights, rather than raw information like production data or images. This approach allows each participant to benefit from data-sovereign learning while reaping the advantages of collaborative learning, such as improved generalizability and stability. Additionally, by indirectly increasing the sample size, FL can provide access to a greater variety of features.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Beyond the basic concept of FL, McMahan et al. introduced a detailed method called Federated Averaging (FedAvg), in which a central server functions as an aggregator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. The FedAvg process includes the following steps:</p>
</div>
<div id="S2.p3" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Initialization:</span> A server initializes a global model and distributes it to all participating clients, initiating their local training.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Local Training:</span> Each selected client trains the model on its local data.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Aggregation:</span> After local training is completed on the device, the model updates (e.g., weights or weight changes) are sent back to the central server. The server then aggregates these updates to create the global model, typically by calculating an arithmetic average, as done in FedAvg.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p"><span id="S2.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Update:</span> The global server sends the updated model to all clients.</p>
</div>
</li>
</ul>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">This process, known as a communication round, is repeated until the model achieves the desired accuracy or meets a specific convergence or termination criterion (see Figure <a href="#S2.F1" title="Figure 1 ‣ II State of the Art ‣ Seamless Integration: Sampling Strategies in Federated Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2408.09545/assets/FL.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="293" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of the federated learning process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Client Selection Strategies</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">When FL and FedAvg were introduced, initially a <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">random selection</span> of clients was performed to participate in the next communication round <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. The authors observed that incorporating additional clients beyond a certain threshold yields diminishing returns as the additional communication overhead exceeds the performance gains, but did not investigate further into client selection strategies. This simple approach works well when there is a sufficient number of clients with similar data distributions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. However, it often fails to account for device heterogeneity, which can lead to incomplete communication rounds, or the risk of over-sampling frequently provided data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The available computation power, the network connection as well as a possible battery operation which can lead to phases of energy saving during which the clients are inaccessible, can be considered for <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_italic">resource-aware selection</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Using different hardware components can result in different calculation times. For instance, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> focuses on reducing the influence of hardware in the selection process by neither waiting for the slowest clients (known as stragglers) every round nor completely discarding them, which would result in an imbalance in representation. To avoid a prolonged waiting phase in mobile edge computing as devices with varying resources communicate over a cellular network, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> estimates how many clients can reasonably participate.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">For <span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_italic">performance-aware selection</span> metrics such as accuracy or training loss can be used in addition to the consideration of resource constraints as mentioned above <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> demonstrates, that the choice of clients with a higher local loss can improve convergence of the model, as they have a bigger potential for improvement.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">The term <span id="S2.SS1.p4.1.1" class="ltx_text ltx_font_italic">Contribution-Based Selection</span> is used to categorize methods that use the effects of a client on the global model as a selection criterion for the next communication round <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> uses the Shapley value known from cooperative game theory to estimate the contribution of each client to the global model. A further development can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> resulting in a contribution estimation, which looks at the gradient space and data space for each client. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> propose a flexible selection scheme that converges even when communication rounds are interrupted by clients leaving or joining.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> investigate weight selection strategies in federated learning, focusing on scenarios with few clients and repetitive product images in local datasets to mirror real-world industrial conditions. Their comprehensive evaluation across multiple deep learning architectures reveals that Optimal Epoch Weight Selection (OEWS) consistently outperforms other strategies, significantly boosting model performance in industrial applications.</p>
</div>
<div id="S2.SS1.p6" class="ltx_para">
<p id="S2.SS1.p6.1" class="ltx_p"><span id="S2.SS1.p6.1.1" class="ltx_text ltx_font_italic">Clustering</span> approaches can be applied in various contexts within FL. Clustering can be differentiated based on the input parameters used, such as training weights, known information like geolocation data, computed metrics such as accuracy, data distribution details, or resource availability. The objectives of clustering can also vary. Some methods aim to form clusters for further training as separate groups, thereby improving local accuracy. Others use clustering to select a diverse set of clients for the next training round, with the goal of developing a more robust global model.</p>
</div>
<div id="S2.SS1.p7" class="ltx_para">
<p id="S2.SS1.p7.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> proposes a multi-criterial clustering approach, where the task owner can choose resource-based (such as CPU or RAM) or data-oriented (such as data quality or minimal size) requirements. They show that their DBSCAN-based selection of clients performs better than randomly selecting the same number of clients. One drawback with their approach on the one hand is that they rely on the correctness of sent information from clients regarding resources and data and that it has only been tested at MNIST.</p>
</div>
<div id="S2.SS1.p8" class="ltx_para">
<p id="S2.SS1.p8.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> cluster the clients based on the label of the local faults of a computer room and use the Hamming distance to determine similarity. As the Hamming distance describes the number of symbols that distinguish two strings of equal length from each other <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>; this method is only useful if all possible error cases are predefined and displayed in a binary representation. This method is not suitable for recognizing similarities from only the underlying data or weights.</p>
</div>
<div id="S2.SS1.p9" class="ltx_para">
<p id="S2.SS1.p9.1" class="ltx_p">An initial Clustered Federated Learning (CFL) approach was introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, where clients are iteratively bipartitioned based on the cosine similarity of gradient changes. Hierarchical clustering offers the advantage of not requiring the number of clusters to be known in advance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, and it also provides scalability and interpretability of the results. However, since multiple communication rounds are needed to fully separate all clients, recursive bipartitioning clustering demands significant computational and communication resources, which can limit its practical feasibility in scenarios with many participants. Additionally, because clients are randomly selected each round, the algorithm can be computationally inefficient and may even completely fail, further restricting its practical applicability <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
<div id="S2.SS1.p10" class="ltx_para">
<p id="S2.SS1.p10.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> also employs hierarchical clustering to divide clients based on the similarity of gradient changes, but performs the clustering after a predetermined number of communication rounds. The procedure begins by training a global model, which is then fine-tuned on the private datasets of all clients to assess the difference between the global and local model parameters. This difference is used to create clusters and assign clients to them. Subsequent training is conducted independently within each cluster, resulting in the creation of multiple models. This approach is designed to accommodate a wider range of non-IID settings and allows for training on a subset of clients during each round of the FL model training.</p>
</div>
<div id="S2.SS1.p11" class="ltx_para">
<p id="S2.SS1.p11.1" class="ltx_p">To improve the efficiency of CFL, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> proposes the Iterative Federated Clustering Algorithm (IFCA), which randomly generates cluster centroids and assigns clients to clusters in a way that minimizes the loss function. Although model accuracy with IFCA is significantly improved compared to conventional FL, the communication costs remain unchanged, and the computational effort is higher, as all updates must be transmitted and clusters reformed in each round. Additionally, the probability of success is highly dependent on the initialization of the cluster centroids.</p>
</div>
<div id="S2.SS1.p12" class="ltx_para">
<p id="S2.SS1.p12.1" class="ltx_p">Cosine similarity followed by affinity propagation clustering has been shown to effectively cluster clients in image classification settings, as demonstrated by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. Their evaluation, conducted on the MNIST and CIFAR-10 datasets, included a verification procedure to ensure that the new global weights do not reduce accuracy below a predetermined threshold. If the accuracy drops below this threshold, the new weights are rejected, and training continues with the local weights.</p>
</div>
<div id="S2.SS1.p13" class="ltx_para">
<p id="S2.SS1.p13.1" class="ltx_p">Many purely academic approaches often yield effective solutions on benchmark datasets and demonstrate the convergence of their methods to address specific problems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. These problems are typically framed as closed systems, where the primary objective is to optimize specific metrics, such as achieving high accuracy or minimizing the number of communication rounds. However, this perspective frequently overlooks the dynamic nature of real-world systems, which can operate over extended periods. In practice, clients may join or leave the system at any time, or introduce entirely new data, adding layers of complexity that are not fully accounted for in controlled, static environments.</p>
</div>
<div id="S2.SS1.p14" class="ltx_para">
<p id="S2.SS1.p14.1" class="ltx_p">For our further considerations, we will disregard the hardware limitations of the clients, as it is assumed that companies have sufficient computing resources and stable network connections. In this context, optimizing model performance takes precedence over the duration of communication rounds. However, we will ensure that the required computational power remains reasonable for an industrial environment. Additionally, we assume a cross-silo problem definition, characterized by a relatively small number of clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. In this paper, we will focus on examining various client selection techniques to lay the groundwork for improving the integration of new clients in future work. This is particularly important because the careless integration of new clients into a stable system can lead to unwanted performance drops until the system stabilizes again with new clients. Our future work will evaluate how effectively these techniques can incorporate new clients, along with their data and data distributions, into the system.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">First, we examined how clustering approaches, which were primarily demonstrated on MNIST and minimal examples, can be transferred to real-world datasets and state-of-the-art neural networks, such as EfficientNet or DenseNet. Subsequently, we evaluated various clustering methods and selected the most effective one for further analysis. Initial testing was conducted on a somewhat simpler, proprietary dataset (see section <a href="#S3.SS1" title="III-A Dataset ‣ III Methodology ‣ Seamless Integration: Sampling Strategies in Federated Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>) to establish a baseline understanding, which was then validated on a subset of the more complex ImageNet dataset. This approach allowed us to assess the generalizability and robustness of the selected clustering method across different types of data and neural network architectures.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Even relatively small neural networks typically contain millions of trainable parameters. When fine-tuning on custom datasets, it is common practice to adapt only the final layers to the specific data and the required number of output classes. Consequently, analyzing only the final layer’s activations is often sufficient for effective clustering, as it captures the most relevant features learned during fine-tuning. In this case, with a shape of (n, 1280) corresponding to n classes in EfficientNet, this approach results in a small sample size with high dimensionality, which presents unique challenges for clustering.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Dataset</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our Dataset consists of cabins of miniature trucks, where the parts can be 3D printed, milled, or bought separately. See Figure <a href="#S3.F2" title="Figure 2 ‣ III-A Dataset ‣ III Methodology ‣ Seamless Integration: Sampling Strategies in Federated Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for two exemplary images from the quality inspection module. As the product is assembled on demand in lot size one, many combinations are possible. Previous publications proved that one advantage of FL is the higher generalizability <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, therefore allowing us to take many pictures away from the production line, where a higher variability of image date is easier to obtain and the risk of interfering with the production process is avoided. For testing purposes, we split the data set so that each client has only one color and one type of windshield, in addition to the class without windshield for every client. The distribution can be obtained from Table <a href="#S3.T1" title="TABLE I ‣ III-A Dataset ‣ III Methodology ‣ Seamless Integration: Sampling Strategies in Federated Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>: Classes 1 to 4 are evenly distributed at approximately 12%, whereas class 0 is the predominant category, comprising nearly 50% of the total distribution. In contrast, class 5 is underrepresented, accounting for only about 5%.
For some tests, not all clients take part in the federation from the beginning and simulate newcomers that join with new data.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2408.09545/assets/QC_capture.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="293" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Miniature Truck, captured by an inline quality inspection module. Only a cabin with one type of windshield (red) is pictured. In the following step, one trailer is added from a selection of options.</figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Distribution of the training data to the classes</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Class</span></th>
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_left"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">0</span></td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_left"><span id="S3.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">1</span></td>
<td id="S3.T1.1.1.1.4" class="ltx_td ltx_align_left"><span id="S3.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">2</span></td>
<td id="S3.T1.1.1.1.5" class="ltx_td ltx_align_left"><span id="S3.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">3</span></td>
<td id="S3.T1.1.1.1.6" class="ltx_td ltx_align_left"><span id="S3.T1.1.1.1.6.1" class="ltx_text ltx_font_bold">4</span></td>
<td id="S3.T1.1.1.1.7" class="ltx_td ltx_align_left"><span id="S3.T1.1.1.1.7.1" class="ltx_text ltx_font_bold">5</span></td>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<th id="S3.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T1.1.2.2.1.1" class="ltx_text ltx_font_bold">CID</span></th>
<td id="S3.T1.1.2.2.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.1.2.2.3" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.1.2.2.4" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.1.2.2.5" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.1.2.2.6" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.1.2.2.7" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T1.1.3.3" class="ltx_tr">
<th id="S3.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.3.3.1.1" class="ltx_text ltx_font_bold">1</span></th>
<td id="S3.T1.1.3.3.2" class="ltx_td ltx_align_left">565</td>
<td id="S3.T1.1.3.3.3" class="ltx_td ltx_align_left">659</td>
<td id="S3.T1.1.3.3.4" class="ltx_td"></td>
<td id="S3.T1.1.3.3.5" class="ltx_td"></td>
<td id="S3.T1.1.3.3.6" class="ltx_td"></td>
<td id="S3.T1.1.3.3.7" class="ltx_td"></td>
</tr>
<tr id="S3.T1.1.4.4" class="ltx_tr">
<th id="S3.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.4.4.1.1" class="ltx_text ltx_font_bold">2</span></th>
<td id="S3.T1.1.4.4.2" class="ltx_td ltx_align_left">565</td>
<td id="S3.T1.1.4.4.3" class="ltx_td"></td>
<td id="S3.T1.1.4.4.4" class="ltx_td ltx_align_left">574</td>
<td id="S3.T1.1.4.4.5" class="ltx_td"></td>
<td id="S3.T1.1.4.4.6" class="ltx_td"></td>
<td id="S3.T1.1.4.4.7" class="ltx_td"></td>
</tr>
<tr id="S3.T1.1.5.5" class="ltx_tr">
<th id="S3.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.5.5.1.1" class="ltx_text ltx_font_bold">3</span></th>
<td id="S3.T1.1.5.5.2" class="ltx_td ltx_align_left">565</td>
<td id="S3.T1.1.5.5.3" class="ltx_td"></td>
<td id="S3.T1.1.5.5.4" class="ltx_td"></td>
<td id="S3.T1.1.5.5.5" class="ltx_td ltx_align_left">625</td>
<td id="S3.T1.1.5.5.6" class="ltx_td"></td>
<td id="S3.T1.1.5.5.7" class="ltx_td"></td>
</tr>
<tr id="S3.T1.1.6.6" class="ltx_tr">
<th id="S3.T1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.6.6.1.1" class="ltx_text ltx_font_bold">4</span></th>
<td id="S3.T1.1.6.6.2" class="ltx_td ltx_align_left">565</td>
<td id="S3.T1.1.6.6.3" class="ltx_td"></td>
<td id="S3.T1.1.6.6.4" class="ltx_td"></td>
<td id="S3.T1.1.6.6.5" class="ltx_td"></td>
<td id="S3.T1.1.6.6.6" class="ltx_td ltx_align_left">627</td>
<td id="S3.T1.1.6.6.7" class="ltx_td"></td>
</tr>
<tr id="S3.T1.1.7.7" class="ltx_tr">
<th id="S3.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.7.7.1.1" class="ltx_text ltx_font_bold">5</span></th>
<td id="S3.T1.1.7.7.2" class="ltx_td ltx_align_left">499</td>
<td id="S3.T1.1.7.7.3" class="ltx_td ltx_align_left">499</td>
<td id="S3.T1.1.7.7.4" class="ltx_td"></td>
<td id="S3.T1.1.7.7.5" class="ltx_td"></td>
<td id="S3.T1.1.7.7.6" class="ltx_td"></td>
<td id="S3.T1.1.7.7.7" class="ltx_td"></td>
</tr>
<tr id="S3.T1.1.8.8" class="ltx_tr">
<th id="S3.T1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.8.8.1.1" class="ltx_text ltx_font_bold">6</span></th>
<td id="S3.T1.1.8.8.2" class="ltx_td ltx_align_left">499</td>
<td id="S3.T1.1.8.8.3" class="ltx_td"></td>
<td id="S3.T1.1.8.8.4" class="ltx_td ltx_align_left">504</td>
<td id="S3.T1.1.8.8.5" class="ltx_td"></td>
<td id="S3.T1.1.8.8.6" class="ltx_td"></td>
<td id="S3.T1.1.8.8.7" class="ltx_td"></td>
</tr>
<tr id="S3.T1.1.9.9" class="ltx_tr">
<th id="S3.T1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.9.9.1.1" class="ltx_text ltx_font_bold">7</span></th>
<td id="S3.T1.1.9.9.2" class="ltx_td ltx_align_left">499</td>
<td id="S3.T1.1.9.9.3" class="ltx_td"></td>
<td id="S3.T1.1.9.9.4" class="ltx_td"></td>
<td id="S3.T1.1.9.9.5" class="ltx_td ltx_align_left">511</td>
<td id="S3.T1.1.9.9.6" class="ltx_td"></td>
<td id="S3.T1.1.9.9.7" class="ltx_td"></td>
</tr>
<tr id="S3.T1.1.10.10" class="ltx_tr">
<th id="S3.T1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.10.10.1.1" class="ltx_text ltx_font_bold">8</span></th>
<td id="S3.T1.1.10.10.2" class="ltx_td ltx_align_left">499</td>
<td id="S3.T1.1.10.10.3" class="ltx_td"></td>
<td id="S3.T1.1.10.10.4" class="ltx_td"></td>
<td id="S3.T1.1.10.10.5" class="ltx_td"></td>
<td id="S3.T1.1.10.10.6" class="ltx_td ltx_align_left">499</td>
<td id="S3.T1.1.10.10.7" class="ltx_td"></td>
</tr>
<tr id="S3.T1.1.11.11" class="ltx_tr">
<th id="S3.T1.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.11.11.1.1" class="ltx_text ltx_font_bold">9</span></th>
<td id="S3.T1.1.11.11.2" class="ltx_td ltx_align_left">528</td>
<td id="S3.T1.1.11.11.3" class="ltx_td ltx_align_left">565</td>
<td id="S3.T1.1.11.11.4" class="ltx_td"></td>
<td id="S3.T1.1.11.11.5" class="ltx_td"></td>
<td id="S3.T1.1.11.11.6" class="ltx_td"></td>
<td id="S3.T1.1.11.11.7" class="ltx_td"></td>
</tr>
<tr id="S3.T1.1.12.12" class="ltx_tr">
<th id="S3.T1.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.12.12.1.1" class="ltx_text ltx_font_bold">10</span></th>
<td id="S3.T1.1.12.12.2" class="ltx_td ltx_align_left">528</td>
<td id="S3.T1.1.12.12.3" class="ltx_td"></td>
<td id="S3.T1.1.12.12.4" class="ltx_td ltx_align_left">532</td>
<td id="S3.T1.1.12.12.5" class="ltx_td"></td>
<td id="S3.T1.1.12.12.6" class="ltx_td"></td>
<td id="S3.T1.1.12.12.7" class="ltx_td"></td>
</tr>
<tr id="S3.T1.1.13.13" class="ltx_tr">
<th id="S3.T1.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.13.13.1.1" class="ltx_text ltx_font_bold">11</span></th>
<td id="S3.T1.1.13.13.2" class="ltx_td ltx_align_left">528</td>
<td id="S3.T1.1.13.13.3" class="ltx_td"></td>
<td id="S3.T1.1.13.13.4" class="ltx_td"></td>
<td id="S3.T1.1.13.13.5" class="ltx_td ltx_align_left">559</td>
<td id="S3.T1.1.13.13.6" class="ltx_td"></td>
<td id="S3.T1.1.13.13.7" class="ltx_td"></td>
</tr>
<tr id="S3.T1.1.14.14" class="ltx_tr">
<th id="S3.T1.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.14.14.1.1" class="ltx_text ltx_font_bold">12</span></th>
<td id="S3.T1.1.14.14.2" class="ltx_td ltx_align_left">528</td>
<td id="S3.T1.1.14.14.3" class="ltx_td"></td>
<td id="S3.T1.1.14.14.4" class="ltx_td"></td>
<td id="S3.T1.1.14.14.5" class="ltx_td"></td>
<td id="S3.T1.1.14.14.6" class="ltx_td ltx_align_left">506</td>
<td id="S3.T1.1.14.14.7" class="ltx_td"></td>
</tr>
<tr id="S3.T1.1.15.15" class="ltx_tr">
<th id="S3.T1.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.15.15.1.1" class="ltx_text ltx_font_bold">13</span></th>
<td id="S3.T1.1.15.15.2" class="ltx_td ltx_align_left">510</td>
<td id="S3.T1.1.15.15.3" class="ltx_td ltx_align_left">525</td>
<td id="S3.T1.1.15.15.4" class="ltx_td"></td>
<td id="S3.T1.1.15.15.5" class="ltx_td"></td>
<td id="S3.T1.1.15.15.6" class="ltx_td"></td>
<td id="S3.T1.1.15.15.7" class="ltx_td"></td>
</tr>
<tr id="S3.T1.1.16.16" class="ltx_tr">
<th id="S3.T1.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.16.16.1.1" class="ltx_text ltx_font_bold">14</span></th>
<td id="S3.T1.1.16.16.2" class="ltx_td ltx_align_left">510</td>
<td id="S3.T1.1.16.16.3" class="ltx_td"></td>
<td id="S3.T1.1.16.16.4" class="ltx_td ltx_align_left">530</td>
<td id="S3.T1.1.16.16.5" class="ltx_td"></td>
<td id="S3.T1.1.16.16.6" class="ltx_td"></td>
<td id="S3.T1.1.16.16.7" class="ltx_td"></td>
</tr>
<tr id="S3.T1.1.17.17" class="ltx_tr">
<th id="S3.T1.1.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.17.17.1.1" class="ltx_text ltx_font_bold">15</span></th>
<td id="S3.T1.1.17.17.2" class="ltx_td ltx_align_left">510</td>
<td id="S3.T1.1.17.17.3" class="ltx_td"></td>
<td id="S3.T1.1.17.17.4" class="ltx_td"></td>
<td id="S3.T1.1.17.17.5" class="ltx_td ltx_align_left">488</td>
<td id="S3.T1.1.17.17.6" class="ltx_td"></td>
<td id="S3.T1.1.17.17.7" class="ltx_td"></td>
</tr>
<tr id="S3.T1.1.18.18" class="ltx_tr">
<th id="S3.T1.1.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.18.18.1.1" class="ltx_text ltx_font_bold">16</span></th>
<td id="S3.T1.1.18.18.2" class="ltx_td ltx_align_left">510</td>
<td id="S3.T1.1.18.18.3" class="ltx_td"></td>
<td id="S3.T1.1.18.18.4" class="ltx_td"></td>
<td id="S3.T1.1.18.18.5" class="ltx_td"></td>
<td id="S3.T1.1.18.18.6" class="ltx_td ltx_align_left">500</td>
<td id="S3.T1.1.18.18.7" class="ltx_td"></td>
</tr>
<tr id="S3.T1.1.19.19" class="ltx_tr">
<th id="S3.T1.1.19.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.19.19.1.1" class="ltx_text ltx_font_bold">17</span></th>
<td id="S3.T1.1.19.19.2" class="ltx_td ltx_align_left">565</td>
<td id="S3.T1.1.19.19.3" class="ltx_td ltx_align_left">607</td>
<td id="S3.T1.1.19.19.4" class="ltx_td"></td>
<td id="S3.T1.1.19.19.5" class="ltx_td"></td>
<td id="S3.T1.1.19.19.6" class="ltx_td"></td>
<td id="S3.T1.1.19.19.7" class="ltx_td"></td>
</tr>
<tr id="S3.T1.1.20.20" class="ltx_tr">
<th id="S3.T1.1.20.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.20.20.1.1" class="ltx_text ltx_font_bold">18</span></th>
<td id="S3.T1.1.20.20.2" class="ltx_td ltx_align_left">565</td>
<td id="S3.T1.1.20.20.3" class="ltx_td"></td>
<td id="S3.T1.1.20.20.4" class="ltx_td ltx_align_left">633</td>
<td id="S3.T1.1.20.20.5" class="ltx_td"></td>
<td id="S3.T1.1.20.20.6" class="ltx_td"></td>
<td id="S3.T1.1.20.20.7" class="ltx_td"></td>
</tr>
<tr id="S3.T1.1.21.21" class="ltx_tr">
<th id="S3.T1.1.21.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.21.21.1.1" class="ltx_text ltx_font_bold">19</span></th>
<td id="S3.T1.1.21.21.2" class="ltx_td ltx_align_left">565</td>
<td id="S3.T1.1.21.21.3" class="ltx_td"></td>
<td id="S3.T1.1.21.21.4" class="ltx_td"></td>
<td id="S3.T1.1.21.21.5" class="ltx_td ltx_align_left">566</td>
<td id="S3.T1.1.21.21.6" class="ltx_td"></td>
<td id="S3.T1.1.21.21.7" class="ltx_td"></td>
</tr>
<tr id="S3.T1.1.22.22" class="ltx_tr">
<th id="S3.T1.1.22.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.22.22.1.1" class="ltx_text ltx_font_bold">20</span></th>
<td id="S3.T1.1.22.22.2" class="ltx_td ltx_align_left">565</td>
<td id="S3.T1.1.22.22.3" class="ltx_td"></td>
<td id="S3.T1.1.22.22.4" class="ltx_td"></td>
<td id="S3.T1.1.22.22.5" class="ltx_td"></td>
<td id="S3.T1.1.22.22.6" class="ltx_td ltx_align_left">599</td>
<td id="S3.T1.1.22.22.7" class="ltx_td"></td>
</tr>
<tr id="S3.T1.1.23.23" class="ltx_tr">
<th id="S3.T1.1.23.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.23.23.1.1" class="ltx_text ltx_font_bold">21</span></th>
<td id="S3.T1.1.23.23.2" class="ltx_td ltx_align_left">547</td>
<td id="S3.T1.1.23.23.3" class="ltx_td"></td>
<td id="S3.T1.1.23.23.4" class="ltx_td"></td>
<td id="S3.T1.1.23.23.5" class="ltx_td"></td>
<td id="S3.T1.1.23.23.6" class="ltx_td"></td>
<td id="S3.T1.1.23.23.7" class="ltx_td ltx_align_left">580</td>
</tr>
<tr id="S3.T1.1.24.24" class="ltx_tr">
<th id="S3.T1.1.24.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.24.24.1.1" class="ltx_text ltx_font_bold">22</span></th>
<td id="S3.T1.1.24.24.2" class="ltx_td ltx_align_left">544</td>
<td id="S3.T1.1.24.24.3" class="ltx_td"></td>
<td id="S3.T1.1.24.24.4" class="ltx_td"></td>
<td id="S3.T1.1.24.24.5" class="ltx_td"></td>
<td id="S3.T1.1.24.24.6" class="ltx_td"></td>
<td id="S3.T1.1.24.24.7" class="ltx_td ltx_align_left">480</td>
</tr>
<tr id="S3.T1.1.25.25" class="ltx_tr">
<th id="S3.T1.1.25.25.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T1.1.25.25.1.1" class="ltx_text ltx_font_bold">Sum</span></th>
<td id="S3.T1.1.25.25.2" class="ltx_td ltx_align_left ltx_border_t">11759</td>
<td id="S3.T1.1.25.25.3" class="ltx_td ltx_align_left ltx_border_t">2855</td>
<td id="S3.T1.1.25.25.4" class="ltx_td ltx_align_left ltx_border_t">2773</td>
<td id="S3.T1.1.25.25.5" class="ltx_td ltx_align_left ltx_border_t">2749</td>
<td id="S3.T1.1.25.25.6" class="ltx_td ltx_align_left ltx_border_t">2731</td>
<td id="S3.T1.1.25.25.7" class="ltx_td ltx_align_left ltx_border_t">1060</td>
</tr>
<tr id="S3.T1.1.26.26" class="ltx_tr">
<th id="S3.T1.1.26.26.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T1.1.26.26.1.1" class="ltx_text ltx_font_bold">Percent</span></th>
<td id="S3.T1.1.26.26.2" class="ltx_td ltx_align_left">49,15%</td>
<td id="S3.T1.1.26.26.3" class="ltx_td ltx_align_left">11,93%</td>
<td id="S3.T1.1.26.26.4" class="ltx_td ltx_align_left">11,59%</td>
<td id="S3.T1.1.26.26.5" class="ltx_td ltx_align_left">11,49%</td>
<td id="S3.T1.1.26.26.6" class="ltx_td ltx_align_left">11,41%</td>
<td id="S3.T1.1.26.26.7" class="ltx_td ltx_align_left">4,43%</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2408.09545/assets/clustering_100e.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="448" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Comparison of clustering methods on test accuracy across 100 communication rounds.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Usability Study on Clustering</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">A pretrained EfficientNet B4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> was used as the frozen backbone, with one fully connected layer replaced to match the number of classes. The training was done with SGD as the optimizer with a learning rate of 0.001, 200 communication rounds and 5 local epochs.
Using Pytorch, the preliminary examinations for clustering were carried out with our implementation to exclude any other influences (e.g. calculating the global model manually based on client’s weight files).</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">To determine which clustering method is most appropriate, three methods that are well-suited for a large number of data points and clusters were selected: Agglomerative clustering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> and BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. Figure <a href="#S3.F3" title="Figure 3 ‣ III-A Dataset ‣ III Methodology ‣ Seamless Integration: Sampling Strategies in Federated Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents sample results from a larger set of tests that were conducted. The optimal parameters, such as the distance metric, for each method were determined through grid search on the weights from previous runs in which all clients participated. We observe that with agglomerative clustering, accuracy increases rapidly, particularly during the first 10 rounds, and then remains stable while continuing to improve gradually. All other methods exhibit significantly higher fluctuations and, except for BIRCH (n=5), even achieve lower accuracy than agglomerative clustering. It should be noted that BIRCH occasionally even clearly surpasses it. The high fluctuation in BIRCH’s performance could be due to its sensitivity to parameters and the high dimensionality combined with a small sample size. BIRCH is generally designed for larger datasets, and when applied to a small dataset with high dimensional features, it may struggle to form stable clusters, leading to the observed variability in test accuracy. HDBSCAN consistently performed worse in comparison.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Agglomerative clustering performs best in our scenario due to its ability to capture complex hierarchical structures in relatively small, but high-dimensional dataset. Its flexibility in distance metrics, robustness to small sample sizes, and stability in high-dimensional spaces make it well-suited to effectively clustering the final layer activations of your neural network, leading to more accurate and stable results compared to other methods like BIRCH or HDBSCAN.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">A more detailed evaluation was caried out using saved weights from two different scenarios: one in which all clients participated in every communication round, and another where the experiment began with 8 clients (representing all classes but limited to red and blue colors), gradually adding new clients after an initial training phase. Clusters were recalculated after each communication round.
The following observations were made during the experiments: New clients were immediately recognized as newcomers, allowing for precautions to be taken to prevent any adverse effects or using advanced approaches to integrate them effectively. Within a maximum of three communication rounds, each consisting of five local epochs, new clients were correctly assigned to the appropriate cluster. It was also evident that cabin types 1 and 4 were easily distinguishable, while types 2 and 3 were often grouped together into the same cluster or formed mixed clusters.
Agglomerative clustering with complete linkage yielded good results, as did the BIRCH algorithm. However, the resulting clusters sometimes differed; for instance, BIRCH occasionally produced clusters more strongly correlated with cabin colors. In subsequent communication rounds, the clusters realigned with windshield types.
Additionally, one group exhibited a higher loss than the others, with no further improvement. Upon closer inspection of the client IDs, it became clear that the group with the higher loss consisted entirely of clients that trained with type 4 windshields. An analysis of the weights confirmed this clustering. Depending on the use case and objectives, it may be beneficial either to develop multiple models tailored to these clusters or to design the client selection process in a way that disrupts this separation, such as by consistently selecting a client from each cluster.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Evaluation of Client Selection Strategies</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In order to compare different strategies, the clients as listed in Table <a href="#S3.T2" title="TABLE II ‣ III-C Evaluation of Client Selection Strategies ‣ III Methodology ‣ Seamless Integration: Sampling Strategies in Federated Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> were selected for training and the data of the excluded clients were used for testing, whereby random images were selected to create a similar distribution of classes, e.g. to avoid over-representing class 0.
The total number of clients was 22, 16 clients correspond to the setup shown in Table <a href="#S3.T2" title="TABLE II ‣ III-C Evaluation of Client Selection Strategies ‣ III Methodology ‣ Seamless Integration: Sampling Strategies in Federated Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. In order to simulate the complete absence of a class, the last client with class 5 was omitted from the training in some tests, resulting in 15 clients. The configuration for each run was: EfficientNet with SGD as an optimizer with learning rate 0.001, 200 communication rounds with 5 local epochs each.
As a baseline, random sampling of approximately twenty and fifty percent (rounding up to the next integer number) of all participating clients was done. Between a sampling rate of 50% and 20% of all 22 clients, there is no noticeable difference, supporting the findings of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> that saturation occurs at a certain point, and additional client participation no longer results in improvement.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Overview of client selection for sampling tests, indicating the classes present in each client’s dataset. Each ’x’ represents a class the client has, while empty cells indicate missing classes. Missing classes within each color-coded group are highlighted.</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Client ID</span></td>
<td id="S3.T2.1.1.1.2" class="ltx_td ltx_align_left"><span id="S3.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Color</span></td>
<td id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">0</span></td>
<td id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">1</span></td>
<td id="S3.T2.1.1.1.5" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.1.5.1" class="ltx_text ltx_font_bold">2</span></td>
<td id="S3.T2.1.1.1.6" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.1.6.1" class="ltx_text ltx_font_bold">3</span></td>
<td id="S3.T2.1.1.1.7" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.1.7.1" class="ltx_text ltx_font_bold">4</span></td>
<td id="S3.T2.1.1.1.8" class="ltx_td ltx_align_center"><span id="S3.T2.1.1.1.8.1" class="ltx_text ltx_font_bold">5</span></td>
</tr>
<tr id="S3.T2.1.2.2" class="ltx_tr">
<td id="S3.T2.1.2.2.1" class="ltx_td ltx_align_right ltx_border_t">1</td>
<td id="S3.T2.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t" style="background-color:#83CCEB;"><span id="S3.T2.1.2.2.2.1" class="ltx_text" style="background-color:#83CCEB;">blue</span></td>
<td id="S3.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">x</td>
<td id="S3.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">x</td>
<td id="S3.T2.1.2.2.5" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.1.2.2.6" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.1.2.2.7" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.1.2.2.8" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T2.1.3.3" class="ltx_tr">
<td id="S3.T2.1.3.3.1" class="ltx_td ltx_align_right">2</td>
<td id="S3.T2.1.3.3.2" class="ltx_td ltx_align_left" style="background-color:#83CCEB;"><span id="S3.T2.1.3.3.2.1" class="ltx_text" style="background-color:#83CCEB;">blue</span></td>
<td id="S3.T2.1.3.3.3" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.3.3.4" class="ltx_td"></td>
<td id="S3.T2.1.3.3.5" class="ltx_td"></td>
<td id="S3.T2.1.3.3.6" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.3.3.7" class="ltx_td"></td>
<td id="S3.T2.1.3.3.8" class="ltx_td"></td>
</tr>
<tr id="S3.T2.1.4.4" class="ltx_tr">
<td id="S3.T2.1.4.4.1" class="ltx_td ltx_align_right">3</td>
<td id="S3.T2.1.4.4.2" class="ltx_td ltx_align_left" style="background-color:#83CCEB;"><span id="S3.T2.1.4.4.2.1" class="ltx_text" style="background-color:#83CCEB;">blue</span></td>
<td id="S3.T2.1.4.4.3" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.4.4.4" class="ltx_td"></td>
<td id="S3.T2.1.4.4.5" class="ltx_td"></td>
<td id="S3.T2.1.4.4.6" class="ltx_td"></td>
<td id="S3.T2.1.4.4.7" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.4.4.8" class="ltx_td"></td>
</tr>
<tr id="S3.T2.1.5.5" class="ltx_tr">
<td id="S3.T2.1.5.5.1" class="ltx_td ltx_align_right">4</td>
<td id="S3.T2.1.5.5.2" class="ltx_td ltx_align_left" style="background-color:#FF7C80;"><span id="S3.T2.1.5.5.2.1" class="ltx_text" style="background-color:#FF7C80;">red</span></td>
<td id="S3.T2.1.5.5.3" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.5.5.4" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.5.5.5" class="ltx_td"></td>
<td id="S3.T2.1.5.5.6" class="ltx_td"></td>
<td id="S3.T2.1.5.5.7" class="ltx_td"></td>
<td id="S3.T2.1.5.5.8" class="ltx_td"></td>
</tr>
<tr id="S3.T2.1.6.6" class="ltx_tr">
<td id="S3.T2.1.6.6.1" class="ltx_td ltx_align_right">5</td>
<td id="S3.T2.1.6.6.2" class="ltx_td ltx_align_left" style="background-color:#FF7C80;"><span id="S3.T2.1.6.6.2.1" class="ltx_text" style="background-color:#FF7C80;">red</span></td>
<td id="S3.T2.1.6.6.3" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.6.6.4" class="ltx_td"></td>
<td id="S3.T2.1.6.6.5" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.6.6.6" class="ltx_td"></td>
<td id="S3.T2.1.6.6.7" class="ltx_td"></td>
<td id="S3.T2.1.6.6.8" class="ltx_td"></td>
</tr>
<tr id="S3.T2.1.7.7" class="ltx_tr">
<td id="S3.T2.1.7.7.1" class="ltx_td ltx_align_right">6</td>
<td id="S3.T2.1.7.7.2" class="ltx_td ltx_align_left" style="background-color:#FF7C80;"><span id="S3.T2.1.7.7.2.1" class="ltx_text" style="background-color:#FF7C80;">red</span></td>
<td id="S3.T2.1.7.7.3" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.7.7.4" class="ltx_td"></td>
<td id="S3.T2.1.7.7.5" class="ltx_td"></td>
<td id="S3.T2.1.7.7.6" class="ltx_td"></td>
<td id="S3.T2.1.7.7.7" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.7.7.8" class="ltx_td"></td>
</tr>
<tr id="S3.T2.1.8.8" class="ltx_tr">
<td id="S3.T2.1.8.8.1" class="ltx_td ltx_align_right">7</td>
<td id="S3.T2.1.8.8.2" class="ltx_td ltx_align_left" style="background-color:#B5E6A2;"><span id="S3.T2.1.8.8.2.1" class="ltx_text" style="background-color:#B5E6A2;">green</span></td>
<td id="S3.T2.1.8.8.3" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.8.8.4" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.8.8.5" class="ltx_td"></td>
<td id="S3.T2.1.8.8.6" class="ltx_td"></td>
<td id="S3.T2.1.8.8.7" class="ltx_td"></td>
<td id="S3.T2.1.8.8.8" class="ltx_td"></td>
</tr>
<tr id="S3.T2.1.9.9" class="ltx_tr">
<td id="S3.T2.1.9.9.1" class="ltx_td ltx_align_right">8</td>
<td id="S3.T2.1.9.9.2" class="ltx_td ltx_align_left" style="background-color:#B5E6A2;"><span id="S3.T2.1.9.9.2.1" class="ltx_text" style="background-color:#B5E6A2;">green</span></td>
<td id="S3.T2.1.9.9.3" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.9.9.4" class="ltx_td"></td>
<td id="S3.T2.1.9.9.5" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.9.9.6" class="ltx_td"></td>
<td id="S3.T2.1.9.9.7" class="ltx_td"></td>
<td id="S3.T2.1.9.9.8" class="ltx_td"></td>
</tr>
<tr id="S3.T2.1.10.10" class="ltx_tr">
<td id="S3.T2.1.10.10.1" class="ltx_td ltx_align_right">9</td>
<td id="S3.T2.1.10.10.2" class="ltx_td ltx_align_left" style="background-color:#B5E6A2;"><span id="S3.T2.1.10.10.2.1" class="ltx_text" style="background-color:#B5E6A2;">green</span></td>
<td id="S3.T2.1.10.10.3" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.10.10.4" class="ltx_td"></td>
<td id="S3.T2.1.10.10.5" class="ltx_td"></td>
<td id="S3.T2.1.10.10.6" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.10.10.7" class="ltx_td"></td>
<td id="S3.T2.1.10.10.8" class="ltx_td"></td>
</tr>
<tr id="S3.T2.1.11.11" class="ltx_tr">
<td id="S3.T2.1.11.11.1" class="ltx_td ltx_align_right">10</td>
<td id="S3.T2.1.11.11.2" class="ltx_td ltx_align_left" style="background-color:#FCFF2F;"><span id="S3.T2.1.11.11.2.1" class="ltx_text" style="background-color:#FCFF2F;">yellow</span></td>
<td id="S3.T2.1.11.11.3" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.11.11.4" class="ltx_td"></td>
<td id="S3.T2.1.11.11.5" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.11.11.6" class="ltx_td"></td>
<td id="S3.T2.1.11.11.7" class="ltx_td"></td>
<td id="S3.T2.1.11.11.8" class="ltx_td"></td>
</tr>
<tr id="S3.T2.1.12.12" class="ltx_tr">
<td id="S3.T2.1.12.12.1" class="ltx_td ltx_align_right">11</td>
<td id="S3.T2.1.12.12.2" class="ltx_td ltx_align_left" style="background-color:#FCFF2F;"><span id="S3.T2.1.12.12.2.1" class="ltx_text" style="background-color:#FCFF2F;">yellow</span></td>
<td id="S3.T2.1.12.12.3" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.12.12.4" class="ltx_td"></td>
<td id="S3.T2.1.12.12.5" class="ltx_td"></td>
<td id="S3.T2.1.12.12.6" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.12.12.7" class="ltx_td"></td>
<td id="S3.T2.1.12.12.8" class="ltx_td"></td>
</tr>
<tr id="S3.T2.1.13.13" class="ltx_tr">
<td id="S3.T2.1.13.13.1" class="ltx_td ltx_align_right">12</td>
<td id="S3.T2.1.13.13.2" class="ltx_td ltx_align_left" style="background-color:#FCFF2F;"><span id="S3.T2.1.13.13.2.1" class="ltx_text" style="background-color:#FCFF2F;">yellow</span></td>
<td id="S3.T2.1.13.13.3" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.13.13.4" class="ltx_td"></td>
<td id="S3.T2.1.13.13.5" class="ltx_td"></td>
<td id="S3.T2.1.13.13.6" class="ltx_td"></td>
<td id="S3.T2.1.13.13.7" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.13.13.8" class="ltx_td"></td>
</tr>
<tr id="S3.T2.1.14.14" class="ltx_tr">
<td id="S3.T2.1.14.14.1" class="ltx_td ltx_align_right">13</td>
<td id="S3.T2.1.14.14.2" class="ltx_td ltx_align_left" style="background-color:#F8A102;"><span id="S3.T2.1.14.14.2.1" class="ltx_text" style="background-color:#F8A102;">orange</span></td>
<td id="S3.T2.1.14.14.3" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.14.14.4" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.14.14.5" class="ltx_td"></td>
<td id="S3.T2.1.14.14.6" class="ltx_td"></td>
<td id="S3.T2.1.14.14.7" class="ltx_td"></td>
<td id="S3.T2.1.14.14.8" class="ltx_td"></td>
</tr>
<tr id="S3.T2.1.15.15" class="ltx_tr">
<td id="S3.T2.1.15.15.1" class="ltx_td ltx_align_right">14</td>
<td id="S3.T2.1.15.15.2" class="ltx_td ltx_align_left" style="background-color:#F8A102;"><span id="S3.T2.1.15.15.2.1" class="ltx_text" style="background-color:#F8A102;">orange</span></td>
<td id="S3.T2.1.15.15.3" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.15.15.4" class="ltx_td"></td>
<td id="S3.T2.1.15.15.5" class="ltx_td"></td>
<td id="S3.T2.1.15.15.6" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.15.15.7" class="ltx_td"></td>
<td id="S3.T2.1.15.15.8" class="ltx_td"></td>
</tr>
<tr id="S3.T2.1.16.16" class="ltx_tr">
<td id="S3.T2.1.16.16.1" class="ltx_td ltx_align_right">15</td>
<td id="S3.T2.1.16.16.2" class="ltx_td ltx_align_left" style="background-color:#F8A102;"><span id="S3.T2.1.16.16.2.1" class="ltx_text" style="background-color:#F8A102;">orange</span></td>
<td id="S3.T2.1.16.16.3" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.16.16.4" class="ltx_td"></td>
<td id="S3.T2.1.16.16.5" class="ltx_td"></td>
<td id="S3.T2.1.16.16.6" class="ltx_td"></td>
<td id="S3.T2.1.16.16.7" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.16.16.8" class="ltx_td"></td>
</tr>
<tr id="S3.T2.1.17.17" class="ltx_tr">
<td id="S3.T2.1.17.17.1" class="ltx_td ltx_align_right">16</td>
<td id="S3.T2.1.17.17.2" class="ltx_td ltx_align_left" style="background-color:#ADADAD;"><span id="S3.T2.1.17.17.2.1" class="ltx_text" style="background-color:#ADADAD;">silver</span></td>
<td id="S3.T2.1.17.17.3" class="ltx_td ltx_align_center">x</td>
<td id="S3.T2.1.17.17.4" class="ltx_td"></td>
<td id="S3.T2.1.17.17.5" class="ltx_td"></td>
<td id="S3.T2.1.17.17.6" class="ltx_td"></td>
<td id="S3.T2.1.17.17.7" class="ltx_td"></td>
<td id="S3.T2.1.17.17.8" class="ltx_td ltx_align_center">x</td>
</tr>
<tr id="S3.T2.1.18.18" class="ltx_tr">
<td id="S3.T2.1.18.18.1" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T2.1.18.18.1.1" class="ltx_text ltx_font_bold">Sum</span></td>
<td id="S3.T2.1.18.18.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.1.18.18.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.18.18.3.1" class="ltx_text ltx_font_bold">16</span></td>
<td id="S3.T2.1.18.18.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.18.18.4.1" class="ltx_text ltx_font_bold">4</span></td>
<td id="S3.T2.1.18.18.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.18.18.5.1" class="ltx_text ltx_font_bold">3</span></td>
<td id="S3.T2.1.18.18.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.18.18.6.1" class="ltx_text ltx_font_bold">4</span></td>
<td id="S3.T2.1.18.18.7" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.18.18.7.1" class="ltx_text ltx_font_bold">4</span></td>
<td id="S3.T2.1.18.18.8" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.18.18.8.1" class="ltx_text ltx_font_bold">1</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">In comparison to random selection, we evaluate metric-based methods (specifically, highest training loss) and cluster-based methods (focussing on agglomerative clustering). For the initialization of both methods, each client is selected once at the beginning.
Figure <a href="#S3.F4" title="Figure 4 ‣ III-C Evaluation of Client Selection Strategies ‣ III Methodology ‣ Seamless Integration: Sampling Strategies in Federated Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the impact the methods have on client participation. A review of the selection based on the training loss reveals that client 16, with the underrepresented class, is selected with greater frequency. However, client 1, is selected with a similarly high frequency, although class 1 is represented at an average level.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Figure <a href="#S3.F5" title="Figure 5 ‣ III-C Evaluation of Client Selection Strategies ‣ III Methodology ‣ Seamless Integration: Sampling Strategies in Federated Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates the test accuracy over 200 communication rounds for four client selection methods: Random selection with 5 clients participating each round, loss-based selection of the 5 clients with the highest training loss, and agglomerative clustering with k=3 and k=5, corresponding to the number of clusters to be identified. The moving average with windows size 5 is used for clearer visualization. Agglomerative clustering with k=5 consistently outperforms the other methods, showing the highest and most stable accuracy throughout. Initially, it demonstrates a rapid increase in accuracy, maintaining superior performance across all phases. Agglomerative clustering with k=3 and random selection yield similar results, with k=3 exhibiting more fluctuations but eventually aligning with random selection. The training loss method, despite early instability, eventually achieves comparable accuracy, though with more fluctuations. Overall, clustering with k=5 offers the greatest accuracy and stability, while non-randomized methods provide better and more controlled integration of new clients.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2408.09545/assets/Histogramm_selection.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="382" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Comparison of client participation frequencies across different selection methods: The chart displays the number of participations for each client using three selection strategies: Random selection, highest training loss and cluster selection with k=3.</figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2408.09545/assets/test_acc.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The plot shows the moving average of test accuracy across 200 communication rounds for three client selection methods: Random selection, training loss-based selection, and agglomerative clustering, each with their corresponding parameters.</figcaption>
</figure>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">To ensure that cluster-based client selection is effective beyond our dataset, we conducted further experiments using a subset of ImageNet. This subset includes three superclasses: animals, tools, and vehicles, each containing multiple subclasses (for instance, various classes for cats, dogs, and birds under the animals’ superclass). Throughout all runs, the mapping of clients to datasets remained consistent, as did the randomization seed, such as when adding noise to the initially loaded ImageNet weights. The results, shown in Figure <a href="#S3.F6" title="Figure 6 ‣ III-C Evaluation of Client Selection Strategies ‣ III Methodology ‣ Seamless Integration: Sampling Strategies in Federated Learning Systems" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, demonstrate that clustering strategies with any amount of clusters outperform the random and training loss-based strategies, particularly during the first 25 communication rounds. This analysis was conducted using the same hyperparameters as before, highlighting the effectiveness of clustering approaches in improving model accuracy, especially in the early stages of training, but also proofing that ImageNet is more complex and needs more fine-tuning to achieve a higher accuracy. Note, that clustering with k=3 outperforms random selection with n=5, meaning that in each communication round, two fewer clients are required to compute their updates, leading to a more efficient path to achieving high accuracy. It is important to highlight that the images within each superclass were re-clustered based on visual similarity using t-distributed stochastic neighbor embedding, leading to a non-IID distribution.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2408.09545/assets/test_acc_imgnet.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The plot shows the moving average of test accuracy on ImageNet across 100 communication rounds for three client selection methods: Random selection, training loss-based selection, and agglomerative clustering, each with their corresponding parameters. </figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Computational Overhead</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Finally, we measured the client selection time on a local computer, as consistent hardware configurations cannot be guaranteed throughout all tests when using a high-performance cluster. When selecting 10 out of 16 clients for training—already a higher ratio compared to our other setups—the results indicate that agglomerative clustering is only slightly slower than random selection, with selection times of 0.0049 seconds and 0.0027 seconds, respectively. However, HDBSCAN requires significantly more time, taking 0.0444 seconds. Despite this, the communication time for transmitting weights in a network will far exceed these client selection times, especially in a cross-silo setup with a relatively low number of clients making the additional computation time for clustering methods less impactful in the overall process.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Conclusion and Outlook</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this study, we examined various client selection methods within the context of federated learning to address the challenges of integrating new clients with diverse and potentially non-IID data distributions. Our analysis demonstrated that clustering techniques, particularly agglomerative clustering, consistently outperformed random and loss-based client selection methods, offering both higher and more stable accuracy across communication rounds. This improvement is especially pronounced in the early stages of training, where efficient client selection plays a crucial role in accelerating the learning process while maintaining high model performance. The success of clustering-based methods underscores their potential in managing the complexities of data heterogeneity and enhancing the overall efficiency of FL systems.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Future research will focus on further refining clustering-based client selection strategies to better accommodate the dynamic nature of real-world FL environments, where clients may join or leave the system unpredictably. Additionally, the consideration of complex datasets like ImageNet, which has only been touched on in this study, is explored in greater depth in ongoing research and will be crucial to ensure their applicability across various domains. Another promising direction is the development of hybrid approaches that combine the strengths of different selection methods to optimize both accuracy and computational efficiency. As FL continues to evolve, addressing these challenges will be key to its successful adoption in production environments, particularly in industries where data privacy and diversity are paramount.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was funded by the Carl Zeiss Stiftung, Germany under the Sustainable Embedded AI project (P2021-02-009).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas, “Communication-efficient learning of deep networks from decentralized data,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence and Statistics</em>, pp. 1273–1282, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. Nitin Bhagoji, and e. Bonawitz, “Advances and open problems in federated learning,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">FNT in Machine Learning (Foundations and Trends in Machine Learning)</em>, vol. 14, no. 1–2, pp. 1–210, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
I. K. Nti, A. F. Adekoya, B. A. Weyori, and O. Nyarko-Boateng, “Applications of artificial intelligence in engineering and manufacturing: a systematic review,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Journal of Intelligent Manufacturing</em>, vol. 33, no. 6, pp. 1581–1601, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A. Dogan and D. Birant, “Machine learning and data mining in manufacturing,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Expert Systems with Applications</em>, vol. 166, p. 114060, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
M. Arafeh, A. Hammoud, H. Otrok, A. Mourad, C. Talhi, and Z. Dziong, “Independent and identically distributed (iid) data assessment in federated learning,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">GLOBECOM 2022 - 2022 IEEE Global Communications Conference</em>.   IEEE, 2022, pp. 293–298.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith, “Federated optimization in heterogeneous networks,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning and Systems</em>, I. Dhillon, D. Papailiopoulos, and V. Sze, Eds., vol. 2, 2020, pp. 429–450.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
O. Myklebust, “Zero defect manufacturing: A product and plant oriented lifecycle approach,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Procedia CIRP</em>, vol. 12, pp. 246–251, 2013.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
M. K. Hassan, “Applying lean six sigma for waste reduction in a manufacturing environment,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">American Journal of Industrial Engineering</em>, vol. 1, no. 2, pp. 28–35, 2013.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
T. Calvin, “Quality control techniques for zero defects,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Components, Hybrids, and Manufacturing Technology</em>, vol. 6, no. 3, pp. 323–328, 1983.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
W. L. Berry and M. C. Cooper, “Manufacturing flexibility: methods for measuring the impact of product variety on performance in process industries,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Journal of Operations Management</em>, vol. 17, no. 2, pp. 163–178, 1999.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
R. Rai, M. K. Tiwari, D. Ivanov, and A. Dolgui, “Machine learning in manufacturing and industry 4.0 applications,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International Journal of Production Research</em>, vol. 59, no. 16, pp. 4773–4778, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
V. Buggineni, C. Chen, and J. Camelio, “Enhancing manufacturing operations with synthetic data: a systematic framework for data generation, accuracy, and utility,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Frontiers in Manufacturing Technology</em>, vol. 4, 2024.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Z. Adams and A. Nelson, “Breaking down silos: Integrating big data across organizational functions,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">International Journal of Network and Communication Research</em>, vol. 7, no. 1, pp. 81–89, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
V. Hegiste, S. Walunj, J. Antony, T. Legler, and M. Ruskowski, “Enhancing object detection with hybrid dataset in manufacturing environments: Comparing federated learning to conventional techniques,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">1st International Conference on Innovative Engineering Sciences and Technological Research (ICIESTR-2024)</em>, 2024.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
L. Liu, J. Zhang, S. H. Song, and K. B. Letaief, “Client-edge-cloud hierarchical federated learning,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ICC 2020 - 2020 IEEE International Conference on Communications (ICC)</em>.   IEEE, 2020, pp. 1–6.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. A. Ranzato, A. Senior, P. Tucker, K. Yang, Q. Le, and A. Ng, “Large scale distributed deep networks,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger, Eds., vol. 25.   Curran Associates, Inc, 2012.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
F. N. Iandola, M. W. Moskewicz, K. Ashraf, and K. Keutzer, “Firecaffe: Near-linear acceleration of deep neural network training on compute clusters,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>.   IEEE, 2016.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y. Arcas, “Communication-efficient learning of deep networks from decentralized data,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th International Conference on Artificial Intelligence and Statistics</em>, ser. Proceedings of Machine Learning Research, A. Singh and J. Zhu, Eds., vol. 54.   Fort Lauderdale, FL, USA: PMLR, 2017, pp. 1273–1282.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
T. Legler, V. Hegiste, and M. Ruskowski, “Mapping of newcomer clients in federated learning based on activation strength,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">32nd International Conference Flexible Automation and Intelligent Manufacturing</em>, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
X. Li, K. Huang, W. Yang, S. Wang, and Z. Zhang, “On the convergence of fedavg on non-iid data,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Y. Jee Cho, S. Gupta, G. Joshi, and O. Yagan, “Bandit-based communication-efficient client selection strategies for federated learning,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">2020 54th Asilomar Conference on Signals, Systems, and Computers</em>.   Place of publication not identified: IEEE, 2020, pp. 1066–1069.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
F. Maciel, A. M. de Souza, L. F. Bittencourt, and L. A. Villas, “Resource aware client selection for federated learning in iot scenarios,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">19th International Conference on Distributed Computing in Smart Systems and the Internet of Things</em>.   IEEE, 2023, pp. 1–8.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
A. Tahir, Y. Chen, and P. Nilayam, “Fedss: Federated learning with smart selection of clients,” 11.07.2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
T. Nishio and R. Yonetani, “Client selection for federated learning with heterogeneous resources in mobile edge,” in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on Communications (ICC)</em>.   IEEE, 2019, pp. 1–7.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
M. Panigrahi, S. Bharti, and A. Sharma, “A review on client selection models in federated learning,” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">WIREs Data Mining and Knowledge Discovery</em>, vol. 13, no. 6, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Y. J. Cho, J. Wang, and G. Joshi, “Client selection in federated learning: Convergence analysis and power-of-choice selection strategies,” 03.10.2020.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
W. Lin, Y. Xu, B. Liu, D. Li, T. Huang, and F. Shi, “Contribution–based federated learning client selection,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">International Journal of Intelligent Systems</em>, vol. 37, no. 10, pp. 7235–7260, 2022.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Y. Qiao, M. S. Munir, A. Adhikary, A. D. Raha, and C. S. Hong, “Cdfed: Contribution-based dynamic federated learning for managing system and statistical heterogeneity,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">NOMS 2023-2023 IEEE/IFIP Network Operations and Management Symposium</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
M. Jiang, H. R. Roth, W. Li, D. Yang, C. Zhao, V. Nath, D. Xu, Q. Dou, and Z. Xu, “Fair federated medical image segmentation via client contribution estimation,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>.   Piscataway, NJ: IEEE, 2023, pp. 16 302–16 311.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Y. Ruan, X. Zhang, S.-C. Liang, and C. Joe-Wong, “Towards flexible device participation in federated learning,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of The 24th International Conference on Artificial Intelligence and Statistics</em>, ser. Proceedings of Machine Learning Research, A. Banerjee and K. Fukumizu, Eds., vol. 130.   PMLR, 2021, pp. 3403–3411.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
V. Hegiste, T. Legler, and M. Ruskowski, “Towards robust federated image classification: An empirical study of weight selection strategies in manufacturing.” [Online]. Available: http://arxiv.org/pdf/2408.10024v1

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
S. Arisdakessian, O. A. Wahab, A. Mourad, and H. Otrok, “Towards instant clustering approach for federated learning client selection,” in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">2023 International Conference on Computing, Networking and Communications (ICNC)</em>.   IEEE, 2023, pp. 409–413.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
C. Li and H. Wu, “Fedcls:a federated learning client selection algorithm based on cluster label information,” in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">2022 IEEE 96th Vehicular Technology Conference:(VTC 2022-Fall)</em>.   Piscataway, NJ: IEEE, 2022, pp. 1–5.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
R. W. Hamming, <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Coding and information theory</em>.   Prentice-Hall, Inc, 1986.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
F. Sattler, K.-R. Muller, and W. Samek, “Clustered federated learning: Model-agnostic distributed multitask optimization under privacy constraints,” <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on neural networks and learning systems</em>, vol. 32, no. 8, pp. 3710–3722, 2021.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
S. C. Johnson, “Hierarchical clustering schemes,” <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Psychometrika</em>, vol. 32, no. 3, pp. 241–254, 1967.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
M. Duan, D. Liu, X. Ji, R. Liu, L. Liang, X. Chen, and Y. Tan, “Fedgroup: Efficient federated learning via decomposed similarity-based clustering,” in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Intl Conf on Parallel &amp; Distributed Processing with Applications, Big Data &amp; Cloud Computing, Sustainable Computing &amp; Communications, Social Computing &amp; Networking</em>.   IEEE, 2021, pp. 228–237.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
C. Briggs, Z. Fan, and P. Andras, “Federated learning with hierarchical clustering of local updates to improve training on non-iid data,” in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">2020 International Joint Conference on Neural Networks (IJCNN)</em>.   IEEE, 2020, pp. 1–9.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
A. Ghosh, J. Chung, D. Yin, and K. Ramchandran, “An efficient framework for clustered federated learning,” in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, Eds., vol. 33.   Curran Associates, Inc, 2020, pp. 19 586–19 597.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
P. Tian, W. Liao, W. Yu, and E. Blasch, “Wscc: A weight-similarity-based client clustering approach for non-iid federated learning,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, vol. 9, no. 20, pp. 20 243–20 256, 2022.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
D. J. Beutel, T. Topal, A. Mathur, X. Qiu, J. Fernandez-Marques, Y. Gao, L. Sani, K. H. Li, T. Parcollet, P. P. B. d. Gusmão, and N. D. Lane, “Flower: A friendly federated learning research framework.” [Online]. Available: http://arxiv.org/pdf/2007.14390v5

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Y. Huang, L. Chu, Z. Zhou, L. Wang, J. Liu, J. Pei, and Y. Zhang, “Personalized cross-silo federated learning on non-iid data,” <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 35, no. 9, pp. 7865–7873, 2021.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
V. Hegiste, T. Legler, and M. Ruskowski, “Application of federated machine learning in manufacturing,” in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">2022 International Conference on Industry 4.0 Technology (I4Tech)</em>.   IEEE, 2022, pp. 1–8.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
M. Tan and Q. Le, “Efficientnet: Rethinking model scaling for convolutional neural networks,” in <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 36th International Conference on Machine Learning</em>, ser. Proceedings of Machine Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97.   PMLR, 2019, pp. 6105–6114.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
K. Chidananda Gowda and G. Krishna, “Agglomerative clustering using the concept of mutual nearest neighbourhood,” <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Pattern Recognition</em>, vol. 10, no. 2, pp. 105–112, 1978.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
L. McInnes, J. Healy, and S. Astels, “hdbscan: Hierarchical density based clustering,” <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">The Journal of Open Source Software</em>, vol. 2, no. 11, p. 205, 2017.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
T. Zhang, R. Ramakrishnan, and M. Livny, “Birch: A new data clustering algorithm and its applications,” <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Data Mining and Knowledge Discovery</em>, vol. 1, no. 2, pp. 141–182, 1997.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.09544" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.09545" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.09545">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.09545" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.09546" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 14:55:38 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
