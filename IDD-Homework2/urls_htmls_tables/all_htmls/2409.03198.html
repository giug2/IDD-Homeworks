<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.03198] RoomDiffusion: A Specialized Diffusion Model in the Interior Design Industry</title><meta property="og:description" content="Recent advancements in text-to-image diffusion models have significantly transformed visual content generation, yet their application in specialized fields such as interior design remains underexplored.
In this paper, …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RoomDiffusion: A Specialized Diffusion Model in the Interior Design Industry">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="RoomDiffusion: A Specialized Diffusion Model in the Interior Design Industry">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.03198">

<!--Generated on Sat Oct  5 19:07:27 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Beike</span></span></span>
<h1 class="ltx_title ltx_title_document">RoomDiffusion: A Specialized Diffusion Model in the Interior Design Industry</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhaowei Wang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Ying Hao
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Hao Wei
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Qing Xiao
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Lulu Chen
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Yulong Li
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> 
<br class="ltx_break">Yue Yang
</span><span class="ltx_author_notes">Corresponding author: yangyue092@ke.com</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Tianyi Li
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Recent advancements in text-to-image diffusion models have significantly transformed visual content generation, yet their application in specialized fields such as interior design remains underexplored.
In this paper, we present <span id="id1.id1.1" class="ltx_text ltx_font_bold">RoomDiffusion</span>, a pioneering diffusion model meticulously tailored for the interior design industry.
To begin with, we build from scratch a whole data pipeline to update and evaluate data for iterative model optimization.
Subsequently, techniques such as multi-aspect training, multi-stage fine-tune and model fusion are applied to enhance both the visual appeal and precision of the generated results.
Lastly, leveraging the latent consistency Distillation method, we distill and expedite the model for optimal efficiency.
Unlike existing models optimized for general scenarios, RoomDiffusion addresses specific challenges in interior design, such as lack of fashion, high furniture duplication rate, and inaccurate style.
Through our holistic human evaluation protocol with more than 20 professional human evaluators, RoomDiffusion demonstrates industry-leading performance in terms of aesthetics, accuracy, and efficiency, surpassing all existing open source models such as stable diffusion and SDXL.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">For text-to-image diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, the realm of interior design stands as a splendid arena for application.
In traditional interior design processes, people often find themselves in prolonged and costly exchanges with professional designers; another potential challenge is that many individuals often lack clarity about their needs, leading to a disorganized and inefficient design process.
With the growing popularity of diffusion models, these issues appear to be effectively resolved.
People can leverage text-to-image diffusion models to quickly explore a vast array of design ideas, thereby gaining the inspiration they seek.
Additionally, such models can assist professional designers in rapidly generating designs, thus enhancing their efficiency in the workflow.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.03198/assets/img/paper_vis_image3.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="538" height="895" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>RoomDiffusion can generate images following long text prompts.</figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2409.03198/assets/img/paper_img6.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="854" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>RoomDiffusion can generate images in various resolutions.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, existing open-source text-to-image diffusion models like stable diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and SDXL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> are primarily designed to cater to general applications, and their performance in specialized fields is somewhat lacking.
Due to the low signal-to-noise ratio in training data and the limited quantity of indoor scene data, open-source models often exhibit issues such as repetitive furniture, outdated styles, imbalanced furniture proportions, and disjointed compositions, as illustrated in Figure. <a href="#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ RoomDiffusion: A Specialized Diffusion Model in the Interior Design Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this report, we introduce RoomDiffusion to address the aforementioned issues and present the entire process of building RoomDiffusion: (1) we created a massive dataset comprising tens of millions of indoor scene images sourced from various channels. Building upon this foundation, we established a comprehensive system for evaluating the quality of indoor images, assigning 19 labels to each image. Some of these labels were utilized to filter out low-quality images, while others formed the textual components of the dataset. (2) we divide the filtered images into several buckets based on their resolution, and randomly extract data from different buckets during model training. At the same time, we using image resolution as a conditional input to control the training process. (3) we selected high-quality images to create a high-precision dataset, then further fine-tuned the model obtained in the previous step based on this dataset, as done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. (4) we selected several outstanding open-source text-to-image diffusion models for model fusion, such as EpicRealism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and Realistic Vision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. (5) we applied latent consistency Distillation(LCD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> to enhance the model’s inference speed.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To comprehensively evaluate the performance of RoomDiffusion, we combined automated metrics with human assessment.
For automated metrics, we assessed aesthetic quality, CLIP score <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, Frechet Inception Distance(FID) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, and several other indicators.
As for human evaluation, we collaborated with over 20 professional evaluators to establish a rational evaluation framework, focusing on aesthetic appeal, image-text alignment, and spatial coherence through good-same-bad(GSB) assessment.
RoomDiffusion demonstrated a leading advantage across all automated metrics.
In human evaluations, RoomDiffusion surpassed the best open-source models with an 70% win rate across multiple dimensions, demonstrating its superior performance.</p>
</div>
<figure id="S1.F3" class="ltx_figure"><img src="/html/2409.03198/assets/img/fig2.png" id="S1.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="154" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The shortcomings of open source models in interior design</figcaption>
</figure>
<figure id="S1.F4" class="ltx_figure"><img src="/html/2409.03198/assets/img/fig3.png" id="S1.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="164" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The generation results of RoomDiffusion in interior design</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Method</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this chapter, we will provide a comprehensive overview of the entire construction process of RoomDiffusion.
In Section 2.1, we will introduce our data pipeline, with its overall structure illustrated in Figure. <a href="#S2.F5" title="Figure 5 ‣ 2.1 Data Pipeline ‣ 2 Method ‣ RoomDiffusion: A Specialized Diffusion Model in the Interior Design Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Then, in Section 2.2, we will describe all the technologies used in RoomDiffusion, such as Multi-aspect training and LCD, with the complete workflow depicted Figure. <a href="#S2.F7" title="Figure 7 ‣ 2.2 Model Training ‣ 2 Method ‣ RoomDiffusion: A Specialized Diffusion Model in the Interior Design Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data Pipeline</h3>

<figure id="S2.F5" class="ltx_figure"><img src="/html/2409.03198/assets/img/1.png" id="S2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="360" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The architecture diagram of the data pipeline.</figcaption>
</figure>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Raw Data Acquisition</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">In order to build a leading-edge and high-performance text-to-image diffusion model, a comprehensive and well-curated dataset is indispensable.
Leveraging years of experience in the residential sector, we have amassed a substantial collection of high-quality interior design renderings.
Additionally, we have augmented the diversity of our training data through external data procurement and open-source downloads. These efforts have culminated in the creation of a dataset comprising tens of millions of decoration renderings.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Image Quality Assessment</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">After obtaining the raw data, we have identified common issues in the images and developed a set of image quality labeling system to evaluate the quality of the images. Currently, this system consists of 5 primary labels and 19 secondary labels. The specific primary labels are explained below:</p>
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Low Quality</span>: Assessing whether the image is usable, including criteria such as whether the image is not an indoor rendering, stitched, or watermarked.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Basic Attributes</span>: Including image resolution, clarity, brightness and saturation.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Aesthetics</span>:
Evaluating the aesthetic quality of the image, including assessing whether there are issues such as color mismatches, outdated styles, or a lack of realism.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p"><span id="S2.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Composition</span>: Assessing whether the proportions of the main objects in the image are reasonable and identifying issues such as excessive focus on a specific area, occlusion, or incorrect shooting angles.</p>
</div>
</li>
<li id="S2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S2.I1.i5.p1" class="ltx_para">
<p id="S2.I1.i5.p1.1" class="ltx_p"><span id="S2.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Content</span>: Evaluating whether the content of the image is reasonable, such as whether the number of key furniture items is appropriate, or whether it includes people or animals.</p>
</div>
</li>
</ol>
</div>
<div id="S2.SS1.SSS2.p2" class="ltx_para">
<p id="S2.SS1.SSS2.p2.1" class="ltx_p">To obtain accurate image labels as described above, we have developed over ten domain models, including watermark detection, stitching classification, aesthetic scoring, indoor segmentation, and indoor detection. These models have demonstrated superior performance compared to existing open-source models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, as shown in Figure. <a href="#S2.F6" title="Figure 6 ‣ 2.1.2 Image Quality Assessment ‣ 2.1 Data Pipeline ‣ 2 Method ‣ RoomDiffusion: A Specialized Diffusion Model in the Interior Design Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
By applying the models, we can set corresponding discrimination rules for each label. Therefore, we can utilize these models to perform preliminary screening on large-scale datasets, eliminating all low-quality data, and selecting high-quality images for model training.</p>
</div>
<figure id="S2.F6" class="ltx_figure"><img src="/html/2409.03198/assets/img/3.png" id="S2.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="294" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Comparison of performance between our domain model and open-source models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</figcaption>
</figure>
</section>
<section id="S2.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Image Captioning</h4>

<div id="S2.SS1.SSS3.p1" class="ltx_para">
<p id="S2.SS1.SSS3.p1.1" class="ltx_p">We have constructed two forms of image descriptions, one based on label systems and the other based on natural language text.</p>
<ol id="S2.I2" class="ltx_enumerate">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p"><span id="S2.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Labeling system: </span>
We have constructed a labeling system for home decoration scenes, consisting of 5 primary labels, 98 secondary labels, and over three hundred tertiary labels. The primary labels encompass five aspects: room, style, color scheme, soft decoration elements, and hard decoration elements. In order to provide comprehensive labels for each image, we trained a classification model to categorize the room, style, and color scheme of the images. Additionally, we employed a detection model to detect the presence of soft and hard decoration elements within the images. As a result, we are able to accurately interpret the elements depicted in the images.</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p"><span id="S2.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Natural language text</span>

<br class="ltx_break">Since label-based image descriptions lack details such as furniture material, color, and spatial relationships, we further enhance the image descriptions using natural language text. To ensure the accuracy and richness of the image descriptions, we conducted a detailed comparison of the image caption capabilities of different language models. Ultimately, we chose to utilize CogVLM-chat <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and GPT-4V(ision) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> for batch production of text descriptions for images. We designed prompts to guide the models in describing various aspects of the images, including room, style, walls, ceilings, floors, decoration status, furniture, and layout, in order to obtain comprehensive descriptions of the images.</p>
</div>
</li>
</ol>
</div>
<div id="S2.SS1.SSS3.p2" class="ltx_para">
<p id="S2.SS1.SSS3.p2.1" class="ltx_p">In conclusion, we combine the image quality labels, home decoration labels, and natural language text to obtain the most accurate and detailed descriptions of the images. The format of the textual descriptions is as follows: ”[room] + [style] + [quality labels (watermark, clarity, etc.)] + [furniture] + [natural language text].”
Notably, CLIP can typically only accept up to 77 tokens, making it unable to handle most image captions. We split long captions into multiple short captions, encode them separately with CLIP, and then concatenate the results before feeding them into the UNet. This approach enables RoomDiffusion to understand long texts.</p>
</div>
</section>
<section id="S2.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.4 </span>Data Layering</h4>

<div id="S2.SS1.SSS4.p1" class="ltx_para">
<p id="S2.SS1.SSS4.p1.1" class="ltx_p">To fully exploit the value of large-scale datasets, we layer the data based on different qualities and quantities and apply them at different stages of model training.</p>
</div>
<div id="S2.SS1.SSS4.p2" class="ltx_para">
<p id="S2.SS1.SSS4.p2.1" class="ltx_p">For instance, we employ the aforementioned image quality indicators to conduct preliminary screening of the images, and combining them with text generated by the CogVLM-chat model to create a dataset comprising millions of image-text pairs. This dataset is utilized for training the generation model, aiming to enhance the model’s ability to generate outputs with improved aesthetics, semantic control, and coherence. Our objective is to maximize the expansion of the model’s generative boundaries and diversity. Furthermore, we utilize a dataset of hundreds of thousands of images manually selected through screening, combined with text generated by the GPT-4V(ision) model. This combined dataset is employed to fine-tune the generated distribution, thereby narrowing down the model’s generated distribution to a sample space of higher quality.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Model Training</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">We have designed a new training pipeline to enhance the model’s generation performance, improving its aesthetic quality, rationality, and semantic control capability. This pipeline consists of four main components: Multi-aspect training, Multi-stage fine-tune, Model Fusion, and LCD, the entire workflow is shown in the figure. <a href="#S2.F6" title="Figure 6 ‣ 2.1.2 Image Quality Assessment ‣ 2.1 Data Pipeline ‣ 2 Method ‣ RoomDiffusion: A Specialized Diffusion Model in the Interior Design Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="S2.F7" class="ltx_figure"><img src="/html/2409.03198/assets/img/workflow.png" id="S2.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="267" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The complete workflow of RoomDiffusion </figcaption>
</figure>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Multi-aspect training</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">In the context of indoor scene, the resolution plays a crucial role in the results generated by models. Currently, most open-source models generate images at a single resolution, which falls far short of meeting the practical demand for larger and a variety of image sizes. Additionally, in the domain of interior decoration characterized by complexity and a plethora of elements, various unreasonable issues are prone to arise, significantly affecting the reference value of generated images in practical applications.
For instance, if we train the model at a specific resolution, it performs relatively well when inferring at or near that resolution. However, when the inference resolution is larger than the trained resolution, it is prone to issues such as distorted or missing furniture. Conversely, when the inference resolution is smaller, the generated image may be blurry and may exhibit insufficient or distorted furniture details.</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.1" class="ltx_p">To ensure the model performs exceptionally well across various resolutions, we employed a multi-aspect training approach. This method not only enhances the model’s performance at different resolutions but also increases its stability and flexibility in generative tasks.
We partition the data into multiple buckets based on different aspect ratios, assigning each training image to the bucket with the closest aspect ratio. In each iteration, data is sampled from a randomly selected bucket for training. Additionally, the model receives the resolution of the target bucket as a condition to control image generation.
To ensure that the semantic alignment between images and text is maximized, we only perform resizing in the image preprocessing step during training and eliminate all cropping operations. Since the images are placed in buckets with similar aspect ratios, they are not excessively compressed or stretched, and the proportions of objects remain relatively normal.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Multi-stage fine-tune</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">After training on a large interior design dataset, our model has developed strong generative capabilities for indoor scenes. However, its aesthetic quality still requires improvement. To address this, we fine-tuned our model on a smaller, but higher-quality image dataset, as done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">To obtain exceptionally high-quality data, we built a data cleaning pipeline:
(i) the raw data is initially filtered by the powerful image quality system, aiming to automatically filter out high-quality images from massive data and greatly reduce labor costs. Data volume reduced from tens of millions to 100k.
(ii) We will divide the image beauty into 5 levels from multiple perspectives such as decoration colors, furniture selection, hard decoration design, etc., ranging from 1 to 5. The larger the number, the higher the standard beauty. A number of designers with interior design experience are trained to rate the beauty of images. Each picture will pass through multiple professional designers to finally obtain the average beauty value, and the top 10% of the pictures with the highest score will be retained.
(iii) To obtain corresponding high-quality prompts, we use GPT-4V(ision) to produce detailed prompts for top 10% images, and then manually checked the detailed prompts.
Finally, we obtained 5,000 exceptionally high-quality image-text pairs. After training the model as described in the previous section, we continued fine-tuning it on these image-text pairs for 10,000 steps with a learning rate of 1e-6.
Through experimental observation, this method significantly enhances the aesthetic quality and texture of the images generated by the model.</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Model Fusion</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">Although our model has shown significant improvements in aesthetics, semantic control, and coherence, it still has some issues. Due to the high proportion of rendered images in our training data, the results generated by our model lack realism.
To address this issue, we employed model fusion techniques widely used in the open-source community.
By integrating our model with those renowned for realism in the community, we can enhance the authenticity and detail of the generated images.</p>
</div>
<div id="S2.SS2.SSS3.p2" class="ltx_para">
<p id="S2.SS2.SSS3.p2.1" class="ltx_p">However, since most open-source models are primarily designed for single-resolution image inference, the fusion process, while enhancing aesthetics and realism, still introduces issues such as duplicated furniture and image stitching artifacts. To address this, we initially conducted bucket fine-tuning on the open-source models using a small dataset and a low learning rate. This fine-tuning process ensured that the models maintained their realism while adapting to the generation of multi-scale images. Finally, we fused the fine-tuned models to achieve a balance between realism and coherence.</p>
</div>
</section>
<section id="S2.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.4 </span>LCD</h4>

<div id="S2.SS2.SSS4.p1" class="ltx_para">
<p id="S2.SS2.SSS4.p1.1" class="ltx_p">Latent Consistency Distillation (LCD) aims to efficiently distill the pre-trained classifier-free <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> guided diffusion models. LCD directly predicts the solution of such ODE in latent space, requiring only a few iterations, resulting in rapid, high-fidelity sampling. We attempt to apply LCD to accelerate our RoomDiffusion and observe some inspiring properties: (i). The teacher model and data quality jointly affect the performance of student models. (ii). The LoRA-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> distillation scheme performs worse than the model-based scheme. (iii). CFG scale and batch size are the two critical aspects of the LCD distilling process. (iv). The distilled model may lose few high-frequency components during the sampling process.</p>
</div>
<div id="S2.SS2.SSS4.p2" class="ltx_para">
<p id="S2.SS2.SSS4.p2.1" class="ltx_p">To alleviate these properties of the LCD method, we select the best-performing teacher model and the high quality data. We carefully tune the CFG scale, batch size and optimizer in the training process to encourage the student model to accurately imitate the teacher model. Furthermore, in order to solve the problem of missing high-frequency components, we use high-quality data to distill the open source photorealistic stable diffusion models and perform model fusion with our student model in proportion.
Finally, we successfully reduced the model inference time to one-third of the original while maintaining consistent performance across all metrics.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evaluation Protocol</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Traditional evaluation criteria for text-to-image generation generally include aspects such as text-image consistency, aesthetic quality of images, and content coherence. However, these metrics are insufficient for providing a comprehensive assessment of models in decoration scenario.
On the one hand, they fail to identify key issues such as repetitive furniture, mixed styles, and poor fidelity of generated images; on the other hand, certain commonly used metrics may exhibit distortion.
For instance, some existing aesthetic models tend to assign higher scores to images with abundant furniture and intricate patterns, while assigning lower scores to images with minimalist interior design styles.
Hence, it is essential to revamp the evaluation process and develop additional metrics to ensure the credibility of the results.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Evaluation Process</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We have designed a dual evaluation mechanism comprising two steps.The first step is to use automated evaluation metrics to quickly assess the model’s performance during the iterative process. If over 70% of the metrics show improvement, the evaluation moves to the next step, which involves human evaluation using the GSB method. This approach not only conserves human resources but also ensures more reliable results.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Automated Evaluation Metrics</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We measure the performance of our model from multiple dimensions, which can be mainly divided into visual appeal and image-text consistency. Visual appeal includes Fréchet inception distance (FID), and aesthetic score (AS). Image-text consistency include CLIP score (CS) and fine-grained metrics, such as soft-decoration follow rate (SFR), style accuracy (SA), hard-decoration follow rate (HFR) and furniture repetition rate(FRR).</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.ix1.p1" class="ltx_para">
<p id="S3.I1.ix1.p1.1" class="ltx_p"><span id="S3.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">Fréchet inception distance</span>: FID is used to assess the quality of images created by a generative model. It has been used to measure the quality of many SOTA generative models.</p>
</div>
</li>
<li id="S3.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.ix2.p1" class="ltx_para">
<p id="S3.I1.ix2.p1.1" class="ltx_p"><span id="S3.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">Aesthetic score</span>: The common aesthetic score model is inconsistent with human subjective judgment in the decoration scenario. We train an aesthetic scoring model based on images annotated by professional designers to evaluate the image aesthetic scores.</p>
</div>
</li>
<li id="S3.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.ix3.p1" class="ltx_para">
<p id="S3.I1.ix3.p1.1" class="ltx_p"><span id="S3.I1.ix3.p1.1.1" class="ltx_text ltx_font_bold">CLIP Score</span>: is a reference free metric that can be used to evaluate the correlation between a generated caption for an image and the actual content of the image.</p>
</div>
</li>
<li id="S3.I1.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.ix4.p1" class="ltx_para">
<p id="S3.I1.ix4.p1.1" class="ltx_p"><span id="S3.I1.ix4.p1.1.1" class="ltx_text ltx_font_bold">Soft-decoration follow rate</span>: Soft-decoration follow rate statistics the generation accuracy of 49 important furniture.</p>
</div>
</li>
<li id="S3.I1.ix5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.ix5.p1" class="ltx_para">
<p id="S3.I1.ix5.p1.1" class="ltx_p"><span id="S3.I1.ix5.p1.1.1" class="ltx_text ltx_font_bold">Style accuracy</span>: SA statistics the generation accuracy of 8 popular styles.</p>
</div>
</li>
<li id="S3.I1.ix6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.ix6.p1" class="ltx_para">
<p id="S3.I1.ix6.p1.1" class="ltx_p"><span id="S3.I1.ix6.p1.1.1" class="ltx_text ltx_font_bold">Hard-decoration follow rate</span>: Hard-decoration follow rate statistics the generation accuracy of 10 common floors and ceilings.</p>
</div>
</li>
<li id="S3.I1.ix7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.ix7.p1" class="ltx_para">
<p id="S3.I1.ix7.p1.1" class="ltx_p"><span id="S3.I1.ix7.p1.1.1" class="ltx_text ltx_font_bold">Furniture repetition rate</span>: In the results generated by diffusion model, there are often unreasonable repetitions of furniture, such as two toilets in a bathroom or two double beds in a bedroom. Therefore, it is necessary to quantify this phenomenon.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Manual evaluation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To further enhance the confidence in the evaluation process, we conducted a manual assessment.
We used the same 1,000 prompts to generate results from both RoomDiffusion and several open-source models, then had evaluators perform GSB evaluations on the generated results.
Over 20 evaluators participated in the assessment, covering three dimensions: aesthetic evaluation, text-image alignment, and layout rationality.</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.ix1.p1" class="ltx_para">
<p id="S3.I2.ix1.p1.1" class="ltx_p"><span id="S3.I2.ix1.p1.1.1" class="ltx_text ltx_font_bold">Aesthetic Evaluation</span>: Evaluators assessed the images from multiple perspectives, including color coordination, stylistic harmony, and lighting, to select what they considered the best image.</p>
</div>
</li>
<li id="S3.I2.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.ix2.p1" class="ltx_para">
<p id="S3.I2.ix2.p1.1" class="ltx_p"><span id="S3.I2.ix2.p1.1.1" class="ltx_text ltx_font_bold">Text-Image Alignment</span>: Evaluators compared the text with the corresponding generated images and selected the one with the highest suitability.</p>
</div>
</li>
<li id="S3.I2.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.ix3.p1" class="ltx_para">
<p id="S3.I2.ix3.p1.1" class="ltx_p"><span id="S3.I2.ix3.p1.1.1" class="ltx_text ltx_font_bold">Layout Rationality</span>: Evaluators chose the image with the most reasonable spatial and furniture layout.</p>
</div>
</li>
</ul>
<p id="S3.SS3.p1.2" class="ltx_p">Each image was evaluated by at least three evaluators, and the final conclusion was based on the majority opinion.
If a majority opinion could not be reached, the image was excluded from the final statistical results.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Machine Indicator Evaluation</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We compared RoomDiffusion with some of the most excellent and widely-used models in the open-source community, such as EpicRealism, Realistic Vision, and SDXL. The test set consisted of 1,000 randomly selected interior decoration images, with corresponding text descriptions generated by GPT-4V(ision).
We followed the seven metrics mentioned in Section 3.2, which include aesthetics, object generation success rate, wall-ceiling-floor accuracy, style accuracy, furniture repetition rate, FID, and CLIP score, the comparison results are shown in Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Machine Indicator Evaluation ‣ 4 Results ‣ RoomDiffusion: A Specialized Diffusion Model in the Interior Design Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Our model achieved the best performance across all machine evaluation metrics; particularly, it excelled in style accuracy and aesthetics scores, while significantly reducing the furniture repetition rate to the lowest level.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>
The comparison of RoomDiffusion with open-source models. Best results are denoted as <span id="S4.T1.9.1" class="ltx_text ltx_font_bold">bold</span>.</figcaption>
<table id="S4.T1.7" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T1.7.7" class="ltx_tr">
<td id="S4.T1.7.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.7.7.8.1" class="ltx_text" style="font-size:80%;">Methods</span></td>
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.1.1.1.1" class="ltx_text" style="font-size:80%;">AS </span><math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.2.2.2.1" class="ltx_text" style="font-size:80%;">SFR </span><math id="S4.T1.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.2.2.2.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T1.2.2.2.m1.1.1" xref="S4.T1.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.3.3.3.1" class="ltx_text" style="font-size:80%;">SA </span><math id="S4.T1.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.3.3.3.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T1.3.3.3.m1.1.1" xref="S4.T1.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.4.4.4.1" class="ltx_text" style="font-size:80%;">HFR </span><math id="S4.T1.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.4.4.4.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T1.4.4.4.m1.1.1" xref="S4.T1.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.m1.1b"><ci id="S4.T1.4.4.4.m1.1.1.cmml" xref="S4.T1.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.5.5.5.1" class="ltx_text" style="font-size:80%;">FRR </span><math id="S4.T1.5.5.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.5.5.5.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T1.5.5.5.m1.1.1" xref="S4.T1.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.m1.1b"><ci id="S4.T1.5.5.5.m1.1.1.cmml" xref="S4.T1.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.6.6.6" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.6.6.6.1" class="ltx_text" style="font-size:80%;">FID </span><math id="S4.T1.6.6.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.6.6.6.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T1.6.6.6.m1.1.1" xref="S4.T1.6.6.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.m1.1b"><ci id="S4.T1.6.6.6.m1.1.1.cmml" xref="S4.T1.6.6.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T1.7.7.7" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.7.7.7.1" class="ltx_text" style="font-size:80%;">Clip score </span><math id="S4.T1.7.7.7.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.7.7.7.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T1.7.7.7.m1.1.1" xref="S4.T1.7.7.7.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.7.m1.1b"><ci id="S4.T1.7.7.7.m1.1.1.cmml" xref="S4.T1.7.7.7.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.7.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T1.7.8" class="ltx_tr">
<td id="S4.T1.7.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.7.8.1.1" class="ltx_text" style="font-size:80%;">Stable diffusion</span></td>
<td id="S4.T1.7.8.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.7.8.2.1" class="ltx_text" style="font-size:80%;">73.0</span></td>
<td id="S4.T1.7.8.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.7.8.3.1" class="ltx_text" style="font-size:80%;">43.0</span></td>
<td id="S4.T1.7.8.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.7.8.4.1" class="ltx_text" style="font-size:80%;">35.7</span></td>
<td id="S4.T1.7.8.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.7.8.5.1" class="ltx_text" style="font-size:80%;">48.0</span></td>
<td id="S4.T1.7.8.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.7.8.6.1" class="ltx_text" style="font-size:80%;">27.3</span></td>
<td id="S4.T1.7.8.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.7.8.7.1" class="ltx_text" style="font-size:80%;">47.4</span></td>
<td id="S4.T1.7.8.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.7.8.8.1" class="ltx_text" style="font-size:80%;">17.3</span></td>
</tr>
<tr id="S4.T1.7.9" class="ltx_tr">
<td id="S4.T1.7.9.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.7.9.1.1" class="ltx_text" style="font-size:80%;">EpicRealism</span></td>
<td id="S4.T1.7.9.2" class="ltx_td ltx_align_center"><span id="S4.T1.7.9.2.1" class="ltx_text" style="font-size:80%;">65.7</span></td>
<td id="S4.T1.7.9.3" class="ltx_td ltx_align_center"><span id="S4.T1.7.9.3.1" class="ltx_text" style="font-size:80%;">48.6</span></td>
<td id="S4.T1.7.9.4" class="ltx_td ltx_align_center"><span id="S4.T1.7.9.4.1" class="ltx_text" style="font-size:80%;">40.6</span></td>
<td id="S4.T1.7.9.5" class="ltx_td ltx_align_center"><span id="S4.T1.7.9.5.1" class="ltx_text" style="font-size:80%;">48.7</span></td>
<td id="S4.T1.7.9.6" class="ltx_td ltx_align_center"><span id="S4.T1.7.9.6.1" class="ltx_text" style="font-size:80%;">24.5</span></td>
<td id="S4.T1.7.9.7" class="ltx_td ltx_align_center"><span id="S4.T1.7.9.7.1" class="ltx_text" style="font-size:80%;">41.1</span></td>
<td id="S4.T1.7.9.8" class="ltx_td ltx_align_center"><span id="S4.T1.7.9.8.1" class="ltx_text" style="font-size:80%;">17.4</span></td>
</tr>
<tr id="S4.T1.7.10" class="ltx_tr">
<td id="S4.T1.7.10.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.7.10.1.1" class="ltx_text" style="font-size:80%;">Realistic Vision</span></td>
<td id="S4.T1.7.10.2" class="ltx_td ltx_align_center"><span id="S4.T1.7.10.2.1" class="ltx_text" style="font-size:80%;">67.2</span></td>
<td id="S4.T1.7.10.3" class="ltx_td ltx_align_center"><span id="S4.T1.7.10.3.1" class="ltx_text" style="font-size:80%;">48.9</span></td>
<td id="S4.T1.7.10.4" class="ltx_td ltx_align_center"><span id="S4.T1.7.10.4.1" class="ltx_text" style="font-size:80%;">41.1</span></td>
<td id="S4.T1.7.10.5" class="ltx_td ltx_align_center"><span id="S4.T1.7.10.5.1" class="ltx_text" style="font-size:80%;">50.6</span></td>
<td id="S4.T1.7.10.6" class="ltx_td ltx_align_center"><span id="S4.T1.7.10.6.1" class="ltx_text" style="font-size:80%;">29.3</span></td>
<td id="S4.T1.7.10.7" class="ltx_td ltx_align_center"><span id="S4.T1.7.10.7.1" class="ltx_text" style="font-size:80%;">36.9</span></td>
<td id="S4.T1.7.10.8" class="ltx_td ltx_align_center"><span id="S4.T1.7.10.8.1" class="ltx_text" style="font-size:80%;">17.2</span></td>
</tr>
<tr id="S4.T1.7.11" class="ltx_tr">
<td id="S4.T1.7.11.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.7.11.1.1" class="ltx_text" style="font-size:80%;">SDXL</span></td>
<td id="S4.T1.7.11.2" class="ltx_td ltx_align_center"><span id="S4.T1.7.11.2.1" class="ltx_text" style="font-size:80%;">71.7</span></td>
<td id="S4.T1.7.11.3" class="ltx_td ltx_align_center"><span id="S4.T1.7.11.3.1" class="ltx_text" style="font-size:80%;">42.2</span></td>
<td id="S4.T1.7.11.4" class="ltx_td ltx_align_center"><span id="S4.T1.7.11.4.1" class="ltx_text" style="font-size:80%;">42.9</span></td>
<td id="S4.T1.7.11.5" class="ltx_td ltx_align_center"><span id="S4.T1.7.11.5.1" class="ltx_text" style="font-size:80%;">43.0</span></td>
<td id="S4.T1.7.11.6" class="ltx_td ltx_align_center"><span id="S4.T1.7.11.6.1" class="ltx_text" style="font-size:80%;">15.2</span></td>
<td id="S4.T1.7.11.7" class="ltx_td ltx_align_center"><span id="S4.T1.7.11.7.1" class="ltx_text" style="font-size:80%;">33.5</span></td>
<td id="S4.T1.7.11.8" class="ltx_td ltx_align_center"><span id="S4.T1.7.11.8.1" class="ltx_text" style="font-size:80%;">17.4</span></td>
</tr>
<tr id="S4.T1.7.12" class="ltx_tr">
<td id="S4.T1.7.12.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.7.12.1.1" class="ltx_text" style="font-size:80%;">SDXL + refiner</span></td>
<td id="S4.T1.7.12.2" class="ltx_td ltx_align_center"><span id="S4.T1.7.12.2.1" class="ltx_text" style="font-size:80%;">73.0</span></td>
<td id="S4.T1.7.12.3" class="ltx_td ltx_align_center"><span id="S4.T1.7.12.3.1" class="ltx_text" style="font-size:80%;">43.0</span></td>
<td id="S4.T1.7.12.4" class="ltx_td ltx_align_center"><span id="S4.T1.7.12.4.1" class="ltx_text" style="font-size:80%;">50.2</span></td>
<td id="S4.T1.7.12.5" class="ltx_td ltx_align_center"><span id="S4.T1.7.12.5.1" class="ltx_text" style="font-size:80%;">41.0</span></td>
<td id="S4.T1.7.12.6" class="ltx_td ltx_align_center"><span id="S4.T1.7.12.6.1" class="ltx_text" style="font-size:80%;">19.3</span></td>
<td id="S4.T1.7.12.7" class="ltx_td ltx_align_center"><span id="S4.T1.7.12.7.1" class="ltx_text" style="font-size:80%;">33.7</span></td>
<td id="S4.T1.7.12.8" class="ltx_td ltx_align_center"><span id="S4.T1.7.12.8.1" class="ltx_text" style="font-size:80%;">17.0</span></td>
</tr>
<tr id="S4.T1.7.13" class="ltx_tr">
<td id="S4.T1.7.13.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.7.13.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">RoomDiffusion</span></td>
<td id="S4.T1.7.13.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.7.13.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">78.3</span></td>
<td id="S4.T1.7.13.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.7.13.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">54.3</span></td>
<td id="S4.T1.7.13.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.7.13.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">60.1</span></td>
<td id="S4.T1.7.13.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.7.13.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">54.0</span></td>
<td id="S4.T1.7.13.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.7.13.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">14.5</span></td>
<td id="S4.T1.7.13.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.7.13.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">33.4</span></td>
<td id="S4.T1.7.13.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T1.7.13.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">17.8</span></td>
</tr>
</table>
</figure>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2409.03198/assets/img/gsb_result.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="399" height="343" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Manual evaluation results of RoomDiffusion and open source models</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Manual Evaluation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">For manual evaluation, we conducted a comparative assessment based on three dimensions: aesthetics, semantic control, and layout rationality. A total of 20 evaluators participated, comparing the results of RoomDiffusion with those of all the open-source models, and providing their answers from three options: good, same, or bad. After removing the part of ”same”, the evaluation results are shown in Figure. <a href="#S4.F8" title="Figure 8 ‣ 4.1 Machine Indicator Evaluation ‣ 4 Results ‣ RoomDiffusion: A Specialized Diffusion Model in the Interior Design Industry" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, where our model demonstrated significant superiority across all dimensions.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this report, we introduce RoomDiffusion, an industry model applied to interior decoration design scenarios, which outperforms all existing open-source models. Our report details the construction process of the RoomDiffusion model, the evaluation methods used, and the performance comparison with open-source models. We also hope that our technical report can provide a reference for the open-source community and foster more rapid and valuable development in the field of interior decoration design.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Dai, X., Hou, J., Ma, C.Y., Tsai, S., Wang, J., Wang, R., Zhang, P., Vandenhende, S., Wang, X., Dubey, A., et al.: Emu: Enhancing image generation models using photogenic needles in a haystack. arXiv preprint arXiv:2309.15807 (2023)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. Advances in neural information processing systems <span id="bib.bib3.1.1" class="ltx_text ltx_font_bold">34</span>, 8780–8794 (2021)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
epicrealism: https://civitai.com/models/25694/epicrealism. epicrealism (2023)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems <span id="bib.bib5.1.1" class="ltx_text ltx_font_bold">30</span> (2017)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in neural information processing systems <span id="bib.bib6.1.1" class="ltx_text ltx_font_bold">33</span>, 6840–6851 (2020)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 (2022)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 4015–4026 (2023)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C., Yang, J., Su, H., Zhu, J., et al.: Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499 (2023)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Luo, S., Tan, Y., Huang, L., Li, J., Zhao, H.: Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378 (2023)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Müller, J., Penna, J., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952 (2023)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748–8763. PMLR (2021)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684–10695 (2022)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Shi, B., Bai, X., Yao, C.: An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (2017)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 (2020)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Vision, R.: https://civitai.com/models/4201?modelversionid=130072. Realistic Vision (2023)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Wang, J., Chan, K.C., Loy, C.C.: Exploring clip for assessing the look and feel of images. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 2555–2563 (2023)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., et al.: Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079 (2023)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.03197" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.03198" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.03198">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.03198" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.03199" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 19:07:27 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
