<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1712.03316] IQA: Visual Question Answering in Interactive Environments</title><meta property="og:description" content="We introduce Interactive Question Answering (IQA), the task of answering questions that require an autonomous agent to interact with a dynamic visual environment. IQA presents the agent with a scene and a question, lik…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="IQA: Visual Question Answering in Interactive Environments">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="IQA: Visual Question Answering in Interactive Environments">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1712.03316">

<!--Generated on Fri Mar 15 23:55:52 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">IQA: Visual Question Answering in Interactive Environments</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daniel Gordon<sup id="id11.11.id1" class="ltx_sup">1</sup>  Aniruddha Kembhavi<sup id="id12.12.id2" class="ltx_sup">2</sup>  Mohammad Rastegari<sup id="id13.13.id3" class="ltx_sup"><span id="id13.13.id3.1" class="ltx_text ltx_font_italic">2,4</span></sup> 
<br class="ltx_break">Joseph Redmon<sup id="id14.14.id4" class="ltx_sup">1</sup>  Dieter Fox<sup id="id15.15.id5" class="ltx_sup"><span id="id15.15.id5.1" class="ltx_text ltx_font_italic">1,3</span></sup>  Ali Farhadi<sup id="id16.16.id6" class="ltx_sup"><span id="id16.16.id6.1" class="ltx_text ltx_font_italic">1,2</span></sup>
<br class="ltx_break"><sup id="id17.17.id7" class="ltx_sup">1</sup>Paul G. Allen School of Computer Science, University of Washington 
<br class="ltx_break"><sup id="id18.18.id8" class="ltx_sup">2</sup>Allen Institute for Artificial Intelligence 
<br class="ltx_break"><sup id="id19.19.id9" class="ltx_sup">3</sup>Nvidia  <sup id="id20.20.id10" class="ltx_sup">4</sup>Xnor.ai
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id21.id1" class="ltx_p">We introduce <span id="id21.id1.1" class="ltx_text">Interactive Question Answering</span> (<span id="id21.id1.2" class="ltx_text ltx_font_smallcaps">IQA</span>), the task of answering questions that require an autonomous agent to interact with a dynamic visual environment. <span id="id21.id1.3" class="ltx_text ltx_font_smallcaps">IQA</span> presents the agent with a scene and a question, like: “Are there any apples in the fridge?” The agent must navigate around the scene, acquire visual understanding of scene elements, interact with objects (e.g. open refrigerators) and plan for a series of actions conditioned on the question. Popular reinforcement learning approaches with a single controller perform poorly on <span id="id21.id1.4" class="ltx_text ltx_font_smallcaps">IQA</span> owing to the large and diverse state space. We propose the Hierarchical Interactive Memory Network (<span id="id21.id1.5" class="ltx_text ltx_font_smallcaps">himn</span>), consisting of a factorized set of controllers, allowing the system to operate at multiple levels of temporal abstraction.
To evaluate <span id="id21.id1.6" class="ltx_text ltx_font_smallcaps">himn</span>, we introduce <span id="id21.id1.7" class="ltx_text ltx_font_smallcaps">iquad v1</span>, a new dataset built upon AI2-THOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, a simulated photo-realistic environment of configurable indoor scenes with interactive objects.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>For the full dataset and code release, visit <a target="_blank" href="https://github.com/danielgordon10/thor-iqa-cvpr-2018" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/danielgordon10/thor-iqa-cvpr-2018</a>.</span></span></span>
<span id="id21.id1.8" class="ltx_text ltx_font_smallcaps">iquad v1</span> has 75,000 questions, each paired with a unique scene configuration. Our experiments show that our proposed model outperforms popular single controller based methods on <span id="id21.id1.9" class="ltx_text ltx_font_smallcaps">iquad v1</span>. For sample questions and results, please view our video: <a target="_blank" href="https://youtu.be/pXd3C-1jr98" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://youtu.be/pXd3C-1jr98</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">A longstanding goal of the artificial intelligence community has been to create agents that can perform manual tasks in the real world and can communicate with humans via natural language. For instance, a household robot might be posed the following questions: <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">Do we need to buy more milk?</em> which would require it to navigate to the kitchen, open the fridge and check to see if there is sufficient milk in the milk jug, or <em id="S1.p1.1.2" class="ltx_emph ltx_font_italic">How many boxes of cookies do we have?</em> which would require the agent to navigate to the cabinets, open several of them and count the number of cookie boxes. Towards this goal, Visual Question Answering (VQA), the problem of answering questions about visual content, has received significant attention from the computer vision and natural language processing communities. While there has been a lot of progress on VQA, research by and large focuses on answering questions passively about visual content, i.e. without the ability to interact with the environment generating the content. An agent that is only able to answer questions passively is limited in its capacity to aid humans in their tasks.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/1712.03316/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="345" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Samples from <span id="S1.F1.3.1" class="ltx_text ltx_font_smallcaps">iquad v1</span>: Each row shows a question paired with the agent’s initial view and a scene view of the environment (which is not provided to the agent). In the scene view, the agent is shown in black, and the locations of the objects of interest for each question are outlined. Note that none of the questions can be answered accurately given only the initial image.</figcaption>
</figure>
<figure id="S1.F2" class="ltx_figure"><img src="/html/1712.03316/assets/x2.png" id="S1.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="403" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An overview of the Hierarchical Interactive Memory Network (<span id="S1.F2.3.1" class="ltx_text ltx_font_smallcaps">himn</span>)</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">We introduce <span id="S1.p2.1.1" class="ltx_text ltx_font_bold">Interactive Question Answering</span> (<span id="S1.p2.1.2" class="ltx_text ltx_font_smallcaps">IQA</span>), the task of answering questions that require the agent to interact with a dynamic environment. <span id="S1.p2.1.3" class="ltx_text ltx_font_smallcaps">IQA</span> poses several key challenges in addition to the ones posed by VQA. <span id="S1.p2.1.4" class="ltx_text ltx_font_bold">First</span>, the agent must be able to navigate through the environment. <span id="S1.p2.1.5" class="ltx_text ltx_font_bold">Second</span>, it must acquire an understanding of its environment including objects, actions, and affordances. <span id="S1.p2.1.6" class="ltx_text ltx_font_bold">Third</span>, the agent must be able to interact with objects in the environment (such as opening the microwave, picking up books, etc.). <span id="S1.p2.1.7" class="ltx_text ltx_font_bold">Fourth</span>, the agent must be able to plan and execute a series of actions in the environment conditioned on the questions asked of it.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address these challenges, we propose <span id="S1.p3.1.1" class="ltx_text ltx_font_smallcaps">himn</span> (Hierarchical Interactive Memory Network). Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides an overview of <span id="S1.p3.1.2" class="ltx_text ltx_font_smallcaps">himn</span>. Akin to past works on hierarchical reinforcement learning, <span id="S1.p3.1.3" class="ltx_text ltx_font_smallcaps">himn</span> is factorized into a hierarchy of controllers, allowing the system to operate, learn, and reason across multiple time scales while simultaneously reducing the complexity of each individual subtask. A high level controller, referred to as the <em id="S1.p3.1.4" class="ltx_emph ltx_font_italic">Planner</em> chooses the task to be performed (for example, navigation / manipulation / answering / etc.) and generates a command for the chosen task. Tasks specified by the <em id="S1.p3.1.5" class="ltx_emph ltx_font_italic">Planner</em> are executed by a set of low level controllers (<em id="S1.p3.1.6" class="ltx_emph ltx_font_italic">Navigator</em>, <em id="S1.p3.1.7" class="ltx_emph ltx_font_italic">Manipulator</em>, <em id="S1.p3.1.8" class="ltx_emph ltx_font_italic">Detector</em>, <em id="S1.p3.1.9" class="ltx_emph ltx_font_italic">Scanner</em> and <em id="S1.p3.1.10" class="ltx_emph ltx_font_italic">Answerer</em>) which return control to the <em id="S1.p3.1.11" class="ltx_emph ltx_font_italic">Planner</em> when a task termination state is reached.
Since these subtasks are fairly independent, we can pretrain each controller independently, while assuming oracle versions of the remaining controllers. Our experiments show that this factorization enables higher accuracy and generalization to unseen environments.
</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Several question types require the agent to remember where it has been and what it has seen. For example, <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">How many pillows are in this house?</span> requires an agent to navigate around the rooms, open closets and keep track of the number of pillows it encounters. For sufficiently complex spaces, the agent needs to hold this information in memory for a long time. This motivates the need for an explicit external memory representation that is filled by the agent as it interacts with its environment. This memory must be both spatial and semantic so it can represent <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">what</span> is <span id="S1.p4.1.3" class="ltx_text ltx_font_italic">where</span>. We propose a new recurrent layer formulation: Egocentric Spatial GRU (<span id="S1.p4.1.4" class="ltx_text">esGRU</span>) to represent this memory (Sec <a href="#S4.SS1" title="4.1 Spatial Memory ‣ 4 Model ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>).</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Training and evaluating interactive agents in the real world is currently prohibitive from the standpoint of operating costs, scale and research reproducibility. A far more viable alternative is to train and evaluate such agents in realistic simulated environments. Towards this end, we present the <span id="S1.p5.1.1" class="ltx_text">Interactive Question Answering Dataset</span> (<span id="S1.p5.1.2" class="ltx_text ltx_font_smallcaps">iquad v1</span>) built upon AI2-THOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, a photo-realistic customizable simulation environment for indoor scenes integrated with the Unity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> physics engine. <span id="S1.p5.1.3" class="ltx_text ltx_font_smallcaps">iquad v1</span> consists of over 75,000 multiple choice questions, each question accompanied by a unique scene configuration.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We evaluate <span id="S1.p6.1.1" class="ltx_text ltx_font_smallcaps">himn</span> on <span id="S1.p6.1.2" class="ltx_text ltx_font_smallcaps">iquad v1</span> using a question answering accuracy metric and show that it outperforms a baseline based on a common architecture for reinforcement learning used in past work. We evaluate in both familiar and unfamiliar environments to show that our semantic model generalizes well across scenes.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">In summary, our contributions include: (a) proposing <span id="S1.p7.1.1" class="ltx_text">Interactive Question Answering</span>, the task of answering questions that require the agent to interact with a dynamic environment, (b) presenting the Hierarchical Interactive Memory Network, a question answering model factorized into a high level <em id="S1.p7.1.2" class="ltx_emph ltx_font_italic">Planner</em>, a set of low level controllers and a rich semantic spatial memory, (c) the Egocentric Spatial GRU, a new recurrent layer to represent this memory and (d) a new dataset <span id="S1.p7.1.3" class="ltx_text ltx_font_smallcaps">iquad v1</span> towards the task of <span id="S1.p7.1.4" class="ltx_text ltx_font_smallcaps">IQA</span>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Visual Question Answering (VQA):</span>
VQA has seen significant progress over the past few years, owing to the design of deep architectures suited for this task and the creation of large VQA datasets to train these models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>. These include datasets of natural images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, synthetic images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, natural videos <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>, synthetic videos <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> and multimodal contexts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Some of these use questions written by humans <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> and others use questions that are generated automatically <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. <span id="S2.p1.1.2" class="ltx_text ltx_font_smallcaps">iquad v1</span> is set in a photo-realistic simulation environment and uses automatically generated questions. In contrast to the aforementioned datasets that only require the agent to observe the content passively, <span id="S2.p1.1.3" class="ltx_text ltx_font_smallcaps">iquad v1</span> requires the agent to interact with a dynamic environment.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The first deep architectures designed for VQA involved using an RNN to encode the question, using a CNN to encode the image and combining them using fully connected layers to yield the answer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>.
More recently, modular networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> that construct an explicit representation of the reasoning process by exploiting the compositional nature of language have been proposed. Similar architectures have also been applied to the video domain with extensions such as spatiotemporal attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>.
Our proposed approach to question answering allows the agent to interact with its environment and is thus fundamentally different to past QA approaches. However, we note that approaches such as visual attention and modularity can easily be combined with our model to provide further improvements.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Reinforcement Learning (RL):</span>
RL algorithms have been employed in a wide range of problems including locomotion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, obstacle detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and autonomous flight <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. Of particular relevance to our approach is the area of hierarchical reinforcement learning (HRL), which consists of a high level controller and one or more low level controllers. The high-level controller selects a subtask to be executed and invokes one of the low level controllers. The advantage of HRL is that it allows the model to operate at multiple levels of temporal abstraction. Early works proposing HRL algorithms include <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>. More recent approaches include <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> who propose hierarchical-DQN with an intrinsically motivated RL algorithm, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> who use HRL to create a lifelong learning system that has the ability to reuse and transfer knowledge from one task to another, and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> who use HRL to enable zero shot task generalization by learning subtask embeddings that capture correspondences between similar subtasks. Our use of HRL primarily lets us learn at multiple time scales and its integration with the semantic memory lets us divide the complex task of <span id="S2.p3.1.2" class="ltx_text ltx_font_smallcaps">IQA</span> into more concrete tasks of navigation, detection, planning etc. that are easier to train.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">RL techniques have also recently been applied to QA tasks, most notably by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> to train a program generator that constructs an explicit representation of the reasoning process to be performed and an execution engine that executes the program to predict the answer.</p>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">Visual Navigation:</span>
The majority of visual navigation techniques fall into three categories: offline map-based, online map-based, and map-less approaches. Offline map-based techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> require the complete map of the environment to make any decisions about their actions, which limits their use in unseen environments. Online map-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> often construct the map while exploring the environment. The majority of these approaches use the computed map for navigation only, whereas our model constructs a rich semantic map which is used for navigation as well as planning and question answering.
Map-less approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> which use techniques such as obstacle avoidance and feature matching, depend upon implicit representations of the world to perform navigation, and lack long-term memory capabilities.
Recently Gupta <em id="S2.p5.1.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p5.1.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> proposed a joint architecture for a <em id="S2.p5.1.4" class="ltx_emph ltx_font_italic">mapper</em> that produces a spatial memory and a <em id="S2.p5.1.5" class="ltx_emph ltx_font_italic">planner</em> that can plan paths. The similarities between our works lie in the usage of a hierarchical system and a spatial memory. In contrast to their work, navigation is not the end goal of our system, but a subtask towards question answering, and our action space is more diverse as it includes interaction and question answering.</p>
</div>
<div id="S2.p6" class="ltx_para ltx_noindent">
<p id="S2.p6.1" class="ltx_p"><span id="S2.p6.1.1" class="ltx_text ltx_font_bold">Visual Planning:</span>
To answer questions such as <em id="S2.p6.1.2" class="ltx_emph ltx_font_italic">Do I need to buy milk?</em> an agent needs to plan a sequence of actions to explore and interact with the environment. A large body of research on planning algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> use high-level formal languages. These techniques are designed to handle low-dimensional state spaces but do not scale well to high-dimensional state spaces such as natural images.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">Other relevant work includes visual navigation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> and visual semantic planning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> which both use the AI2-THOR environment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. The former tackles navigation, and the latter focuses on high level planning and assumes an ideal low level task executor; in contrast, our model trains low level and high level controllers jointly. Also, both these approaches do not generalize well to unseen scenes, whereas our experiments show that we do not overfit to previously encountered environments. Finally, these methods lack any sort of explicit map, whereas we construct a semantic map which helps us navigate and answer questions.</p>
</div>
<div id="S2.p8" class="ltx_para">
<p id="S2.p8.1" class="ltx_p">Recently Chaplot <em id="S2.p8.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p8.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and Hill <em id="S2.p8.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p8.1.4" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> have proposed models to complete navigation tasks specified via language (e.g. <em id="S2.p8.1.5" class="ltx_emph ltx_font_italic">Go to the red keycard</em>) and trained their systems in simulated 3D environments. These models show the ability to generalize to unseen instructions of seen concepts. In contrast, we tackle several question types that require a variety of navigation behaviours and interaction, and the environment we use is significantly more photo-realistic. In our experiments, we compare our proposed <span id="S2.p8.1.6" class="ltx_text ltx_font_smallcaps">himn</span> model to a baseline system (A3C in Section <a href="#S5" title="5 Experiments ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) that very closely resembles the model architectures proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
<div id="S2.p9" class="ltx_para ltx_noindent">
<p id="S2.p9.1" class="ltx_p"><span id="S2.p9.1.1" class="ltx_text ltx_font_bold">Visual Learning by Simulation:</span>
There has been an increased use of simulated environments and game platforms to train computer vision systems to perform tasks such as learning the dynamics of the world <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>, semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, pedestrian detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> and urban driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>. Several of these are also interactive making them suitable to learn control, including <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>. We choose to use AI2-THOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> in our work since it provides a photo-realistic and interactive environment of real world scenes, making it very suitable to train <span id="S2.p9.1.2" class="ltx_text ltx_font_smallcaps">IQA</span> systems that might be transferable to the real world.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Learning Framework</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Actionable Environment</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Training and evaluating interactive agents in the real world is currently prohibitive from the standpoint of operating costs, scale, time, and research reproducibility. A far more viable alternative is to use simulated environments. However, the framework should be visually realistic, allow interactions with objects, and have a detailed model of the physics of the scene so that agent movements and object interactions are properly represented. Hence, we adopt the AI2-THOR environment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> for our purposes. AI2-THOR is a photo-realistic simulation environment of 120 rooms in indoor settings, tightly integrated with a physics engine. Each scene consists of a variety of objects, from furniture such as couches, appliances such as microwaves and smaller objects such as crockery, cutlery, books, fruit, etc. Many of these objects are actionable such as fridges which can be opened, cups which can be picked up and put down, and stoves which can be turned on and off.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" colspan="3">
<span id="S3.T1.1.1.1.1.1" class="ltx_text">Interactive Question Answering Dataset</span> Statistics</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Train</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Test</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Existence</td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">25,600</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">640</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Counting</td>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r">25,600</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_left ltx_border_r">640</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<td id="S3.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Spatial Relationships</td>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_left ltx_border_r">25,600</td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_left ltx_border_r">640</td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<td id="S3.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Rooms</td>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_align_left ltx_border_r">25</td>
<td id="S3.T1.1.6.5.3" class="ltx_td ltx_align_left ltx_border_r">5</td>
</tr>
<tr id="S3.T1.1.7.6" class="ltx_tr">
<td id="S3.T1.1.7.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Total scene configurations (s.c.)</td>
<td id="S3.T1.1.7.6.2" class="ltx_td ltx_align_left ltx_border_r">76,800</td>
<td id="S3.T1.1.7.6.3" class="ltx_td ltx_align_left ltx_border_r">1,920</td>
</tr>
<tr id="S3.T1.1.8.7" class="ltx_tr">
<td id="S3.T1.1.8.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Avg # objects per (s.c.)</td>
<td id="S3.T1.1.8.7.2" class="ltx_td ltx_align_left ltx_border_r">46</td>
<td id="S3.T1.1.8.7.3" class="ltx_td ltx_align_left ltx_border_r">41</td>
</tr>
<tr id="S3.T1.1.9.8" class="ltx_tr">
<td id="S3.T1.1.9.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Avg # interactable objects (s.c.)</td>
<td id="S3.T1.1.9.8.2" class="ltx_td ltx_align_left ltx_border_r">21</td>
<td id="S3.T1.1.9.8.3" class="ltx_td ltx_align_left ltx_border_r">16</td>
</tr>
<tr id="S3.T1.1.10.9" class="ltx_tr">
<td id="S3.T1.1.10.9.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r">Vocabulary Size</td>
<td id="S3.T1.1.10.9.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">70</td>
<td id="S3.T1.1.10.9.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">70</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>This table shows the statistics of our proposed dataset in a variety of question types, objects and scene configurations.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span id="S3.SS2.1.1" class="ltx_text">Interactive Question Answering Dataset</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">iquad v1</span> is a question answering dataset built upon AI2-THOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. It consists of over 75,000 multiple choice questions for three different question types (table <a href="#S3.T1" title="Table 1 ‣ 3.1 Actionable Environment ‣ 3 Learning Framework ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows more detailed statistics). Each question is accompanied by a scene identifier and a unique arrangement of movable objects in the scene. Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows three such examples. The wide variety of configurations in <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_smallcaps">iquad v1</span> prevent models from memorizing simple rules like “apples are always in the fridge” and render this dataset challenging. <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_smallcaps">iquad v1</span> consists of several question types including: Existence questions (<em id="S3.SS2.p1.1.4" class="ltx_emph ltx_font_italic">Is there an apple in the kitchen?</em>), Counting questions (<em id="S3.SS2.p1.1.5" class="ltx_emph ltx_font_italic">How many forks are present in the scene?</em>),
and Spatial Relationship questions (<em id="S3.SS2.p1.1.6" class="ltx_emph ltx_font_italic">Is there lettuce in the fridge? / Is there a cup on the counter-top?</em>). Questions, ground truth answers, and answer choices are generated automatically. Since natural language understanding is not a focus of this dataset, questions are generated using a set of templates written down a priori. Extending <span id="S3.SS2.p1.1.7" class="ltx_text ltx_font_smallcaps">iquad v1</span> to include more diverse questions generated by humans is future work. <span id="S3.SS2.p1.1.8" class="ltx_text ltx_font_smallcaps">iquad v1</span> is a balanced dataset that prevents models from obtaining high accuracies by simply exploiting trivial language and scene configuration biases. Similar to past balanced VQA datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, each question is associated with multiple scene configurations that result in different answers to the question. We split the 30 kitchen rooms into 25 train and 5 test, and have 1024 unique (question, scene configuration) pairs for each (room, question type) pair in train, and 128 in test. An episode is finished when the <em id="S3.SS2.p1.1.9" class="ltx_emph ltx_font_italic">Answerer</em> is invoked. We evaluate different methods using Top-1 accuracy.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Agent and Objects</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The agent in our environments has a single RGB camera mounted at a fixed height. An agent can perform one of five navigation actions (move ahead 25 cm, rotate 90 degrees left or right, look up or down 30 degrees). We assume a grid-world floor plan that ensures that the agent always moves along the edges of a grid and comes to a stop on a node in this grid. The agent can perform two interaction actions (open and close) to manipulate objects. A wide variety of objects (fridges, cabinets, drawers, microwaves, etc.) can be interacted with. If there are multiple items in the current viewpoint which can be opened or closed, the environment chooses the one nearest to the center of the current image. The success of each action depends on the current state of the environment as well as the agent’s current location. For instance, the agent cannot open a cabinet that is more than 1 meter away or is not in view, or is already open, and it cannot walk through a table or a wall.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Model</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We propose <span id="S4.p1.1.1" class="ltx_text ltx_font_smallcaps">himn</span> (Hierarchical Interactive Memory Network), consisting of a hierarchy of controllers that operate at multiple levels of temporal abstraction and a rich semantic memory that aids in navigation, interaction, and question answering. Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides an overview of <span id="S4.p1.1.2" class="ltx_text ltx_font_smallcaps">himn</span>. We now describe each of <span id="S4.p1.1.3" class="ltx_text ltx_font_smallcaps">himn</span>’s components in greater detail.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/1712.03316/assets/x3.png" id="S4.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="411" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>An overview of the Egocentric Spatial GRU (<span id="S4.F3.4.1" class="ltx_text">esGRU</span>): The <span id="S4.F3.5.2" class="ltx_text">esGRU</span> only allows writing to a local window within the memory, dependent on the agent’s current location and viewpoint.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Spatial Memory</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Several question types require the agent to keep track of objects that it has seen in the past along with their locations. For complex scenes with several locations and interactable objects, the agent needs to hold this information in memory for a long duration. This motivates the need for an explicit external memory representation that is filled by the agent on the fly and can be accessed at any time. To address this, <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">himn</span> uses a rich semantic spatial memory that encodes a semantic representation of each location in the scene. Each location in this memory consists of a feature vector encoding object detection probabilities, free space probability (a 2D occupancy grid), coverage (has the agent inspected this location before), and navigation intent (has the agent attempted to visit this location before). We propose a new recurrent layer formulation: Egocentric Spatial GRU (<span id="S4.SS1.p1.1.2" class="ltx_text">esGRU</span>) to represent this memory, illustrated in Figure <a href="#S4.F3" title="Figure 3 ‣ 4 Model ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The <span id="S4.SS1.p1.1.3" class="ltx_text">esGRU</span> maintains an external global spatial memory represented as a 3D tensor. At each time step, the <span id="S4.SS1.p1.1.4" class="ltx_text">esGRU</span> swaps in local egocentric copies of this memory into the hidden state of the GRU, performs computations using current inputs, and then swaps out the resulting hidden state into the global memory at the predetermined location. This speeds up computations and prevents corrupting the memory at locations far away from the agent’s current viewpoint. When navigating and answering questions, the agent can access the full memory, enabling long-term recall from observations seen hundreds of states prior. Furthermore, only low level controllers have read-write access to this memory. Since the <em id="S4.SS1.p1.1.5" class="ltx_emph ltx_font_italic">Planner</em> only makes high level decisions, without interacting with the world at a lower level, it only has read access to the memory.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/1712.03316/assets/x4.png" id="S4.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="432" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Schematic representation of the <em id="S4.F4.3.1" class="ltx_emph ltx_font_italic">Planner</em></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Planner</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.7" class="ltx_p">The high level <em id="S4.SS2.p1.7.1" class="ltx_emph ltx_font_italic">Planner</em> invokes low level controllers in order to explore the environment, gather knowledge needed to answer the given question, and answer the question. We frame this as a reinforcement learning problem where the agent must issue the fewest possible commands that result in a correct answer. The agent must learn to explore relevant areas of the scene based on learned knowledge (e.g. apples are often in the fridge, cabinets are openable, etc.), the current memory state (e.g. the fridge is to the left), current observations (e.g. the fridge is closed) and the question. At every timestep, the <em id="S4.SS2.p1.7.2" class="ltx_emph ltx_font_italic">Planner</em> chooses to either invoke the <em id="S4.SS2.p1.7.3" class="ltx_emph ltx_font_italic">Navigator</em> providing a relative location in a 5x5 grid in front of the agent, invoke the <em id="S4.SS2.p1.7.4" class="ltx_emph ltx_font_italic">Scanner</em> with a direction such as up or left, invoke the <em id="S4.SS2.p1.7.5" class="ltx_emph ltx_font_italic">Manipulator</em> with open/close commands on a nearby object, or invoke the <em id="S4.SS2.p1.7.6" class="ltx_emph ltx_font_italic">Answerer</em> for a total of 32 discrete actions. It does this by producing a policy <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\pi" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">π</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝜋</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\pi</annotation></semantics></math> consisting of probabilities <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="\pi_{i}" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><msub id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">π</mi><mi id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">𝜋</ci><ci id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\pi_{i}</annotation></semantics></math> for each action, and a value <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mi id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><ci id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">v</annotation></semantics></math> for the current state. <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="\pi" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mi id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml">π</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><ci id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">𝜋</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">\pi</annotation></semantics></math> and <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><mi id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><ci id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">v</annotation></semantics></math> are learned using the A3C algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. Figure <a href="#S4.F4" title="Figure 4 ‣ 4.1 Spatial Memory ‣ 4 Model ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows a schematic of the <em id="S4.SS2.p1.7.7" class="ltx_emph ltx_font_italic">Planner</em>. It consists of a GRU which accepts at each time step the current viewpoint (encoded by a CNN) and the previous action. The <em id="S4.SS2.p1.7.8" class="ltx_emph ltx_font_italic">Planner</em> has read only access to the semantic memory centered around the agent’s current location. The output of this GRU is combined with the question embedding and an embedding of the nearby semantic spatial memory to predict <math id="S4.SS2.p1.6.m6.1" class="ltx_Math" alttext="\pi" display="inline"><semantics id="S4.SS2.p1.6.m6.1a"><mi id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml">π</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><ci id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1">𝜋</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">\pi</annotation></semantics></math> and <math id="S4.SS2.p1.7.m7.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S4.SS2.p1.7.m7.1a"><mi id="S4.SS2.p1.7.m7.1.1" xref="S4.SS2.p1.7.m7.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.7.m7.1b"><ci id="S4.SS2.p1.7.m7.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.7.m7.1c">v</annotation></semantics></math>. The agent receives a fixed reward/penalty based on answering correctly/incorrectly. It is also provided a constant time penalty to encourage efficient explorations of the environment and quick answering, as well as a penalty for attempting to perform invalid actions. The agent is also given intermediate rewards for increasing the “coverage” of the environment, effectively training the network to maximize the amount of the room it has explored as quickly as possible. Finally, at each time step, the <em id="S4.SS2.p1.7.9" class="ltx_emph ltx_font_italic">Planner</em> also predicts which high level actions are viable given the current world state. In many locations in the scenes, certain navigation destinations are unreachable or there are no objects to interact with. Predicting possible/impossible actions at each time step, allows gradients to propagate through all actions rather than just the chosen action. This leads to higher accuracies and faster convergence (see section <a href="#S5.SS2" title="5.2 Invalid Actions ‣ 5 Experiments ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a> for more details).</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Low level controllers</h3>

<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:496.9pt;height:110.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(4.8pt,-1.1pt) scale(1.01983700946765,1.01983700946765) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"></th>
<td id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2">Existence</td>
<td id="S4.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2">Counting</td>
<td id="S4.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2">Spatial Relationships</td>
</tr>
<tr id="S4.T2.1.1.2.2" class="ltx_tr">
<th id="S4.T2.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Model</th>
<td id="S4.T2.1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Accuracy</td>
<td id="S4.T2.1.1.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Length</td>
<td id="S4.T2.1.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Accuracy</td>
<td id="S4.T2.1.1.2.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Length</td>
<td id="S4.T2.1.1.2.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Accuracy</td>
<td id="S4.T2.1.1.2.2.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Length</td>
</tr>
<tr id="S4.T2.1.1.3.3" class="ltx_tr">
<th id="S4.T2.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Most Likely Answer Per Q-type (MLA)</th>
<td id="S4.T2.1.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">50</td>
<td id="S4.T2.1.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.1.1.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">25</td>
<td id="S4.T2.1.1.3.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">-</td>
<td id="S4.T2.1.1.3.3.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">50</td>
<td id="S4.T2.1.1.3.3.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S4.T2.1.1.4.4" class="ltx_tr">
<th id="S4.T2.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr">A3C with ground truth (GT) detections</th>
<td id="S4.T2.1.1.4.4.2" class="ltx_td ltx_align_left ltx_border_r">48.59</td>
<td id="S4.T2.1.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r">332.41</td>
<td id="S4.T2.1.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r">24.53</td>
<td id="S4.T2.1.1.4.4.5" class="ltx_td ltx_align_left ltx_border_r">998.32</td>
<td id="S4.T2.1.1.4.4.6" class="ltx_td ltx_align_left ltx_border_r">49.84</td>
<td id="S4.T2.1.1.4.4.7" class="ltx_td ltx_align_left ltx_border_r">578.71</td>
</tr>
<tr id="S4.T2.1.1.5.5" class="ltx_tr">
<th id="S4.T2.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr">HIMN with YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> detections</th>
<td id="S4.T2.1.1.5.5.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T2.1.1.5.5.2.1" class="ltx_text ltx_font_bold">68.47</span></td>
<td id="S4.T2.1.1.5.5.3" class="ltx_td ltx_align_left ltx_border_r">318.33</td>
<td id="S4.T2.1.1.5.5.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T2.1.1.5.5.4.1" class="ltx_text ltx_font_bold">30.43</span></td>
<td id="S4.T2.1.1.5.5.5" class="ltx_td ltx_align_left ltx_border_r">926.11</td>
<td id="S4.T2.1.1.5.5.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T2.1.1.5.5.6.1" class="ltx_text ltx_font_bold">58.67</span></td>
<td id="S4.T2.1.1.5.5.7" class="ltx_td ltx_align_left ltx_border_r">516.23</td>
</tr>
<tr id="S4.T2.1.1.6.6" class="ltx_tr">
<th id="S4.T2.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr">Human (small sample)</th>
<td id="S4.T2.1.1.6.6.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">90</td>
<td id="S4.T2.1.1.6.6.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">58.40</td>
<td id="S4.T2.1.1.6.6.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">80</td>
<td id="S4.T2.1.1.6.6.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">81.90</td>
<td id="S4.T2.1.1.6.6.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">90</td>
<td id="S4.T2.1.1.6.6.7" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">43.00</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>This tables compares the test accuracy and episode lengths of question answering across different models and question types.</figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Navigator</span> The <em id="S4.SS3.p1.1.2" class="ltx_emph ltx_font_italic">Navigator</em> is invoked by the <em id="S4.SS3.p1.1.3" class="ltx_emph ltx_font_italic">Planner</em> which also provides it with the relative coordinates of the target location. Given a destination specified by the <em id="S4.SS3.p1.1.4" class="ltx_emph ltx_font_italic">Planner</em> and the current estimate of the room’s occupancy grid, the <em id="S4.SS3.p1.1.5" class="ltx_emph ltx_font_italic">Navigator</em> runs A* search to find the shortest path to the goal. As the <em id="S4.SS3.p1.1.6" class="ltx_emph ltx_font_italic">Navigator</em> moves through the environment, it uses the <span id="S4.SS3.p1.1.7" class="ltx_text">esGRU</span> to produce a local (5x5) occupancy grid given the current visual observation. This updates the global occupancy estimate, and prompts a new shortest-path computation. This is a fully supervised problem and can be trained with the standard sigmoid-cross-entropy. The <em id="S4.SS3.p1.1.8" class="ltx_emph ltx_font_italic">Navigator</em> also invokes the <em id="S4.SS3.p1.1.9" class="ltx_emph ltx_font_italic">Scanner</em> to obtain a wide angle view of the environment. Given that the requested destination may be outside the bounds of the room or otherwise impossible (e.g. at a wall or other obstacle), the <em id="S4.SS3.p1.1.10" class="ltx_emph ltx_font_italic">Navigator</em>’s network also predicts a termination signal, and returns control to the <em id="S4.SS3.p1.1.11" class="ltx_emph ltx_font_italic">Planner</em> when the prediction passes a certain threshold.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Scanner</span> The <em id="S4.SS3.p2.1.2" class="ltx_emph ltx_font_italic">Scanner</em> is a simple controller which captures images by rotating the camera up, down, left, or right while maintaining the agent’s current location. The <em id="S4.SS3.p2.1.3" class="ltx_emph ltx_font_italic">Scanner</em> calls the <em id="S4.SS3.p2.1.4" class="ltx_emph ltx_font_italic">Detector</em> on every new image.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">Detector</span> Object detection is a critical component of <span id="S4.SS3.p3.1.2" class="ltx_text ltx_font_smallcaps">himn</span> given that all questions in <span id="S4.SS3.p3.1.3" class="ltx_text ltx_font_smallcaps">iquad v1</span> involve one or more objects in the room. We use YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> fine-tuned on the AI2-THOR training scenes as an object detector. We estimate the depth of an object using the FRCN depth estimation network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and project the probabilities of the detected objects onto the ground plane. Both of these networks operate at real-time speeds, which is necessary since they are invoked on every new image. The detection probabilities are incorporated into the spatial memory using a moving average update rule. We also perform experiments where we substitute the trained detector and depth estimator with oracle detections. Detections provided by the environment still requires the network to learn affordances. For instance, the network must learn that <em id="S4.SS3.p3.1.4" class="ltx_emph ltx_font_italic">microwaves can be opened</em>, <em id="S4.SS3.p3.1.5" class="ltx_emph ltx_font_italic">apples can be in fridges</em>, etc.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para ltx_noindent">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">Manipulator</span> The <em id="S4.SS3.p4.1.2" class="ltx_emph ltx_font_italic">Manipulator</em> is invoked by the <em id="S4.SS3.p4.1.3" class="ltx_emph ltx_font_italic">Planner</em> to manipulate the current state of an object. For example, opening and closing the microwave. This leads to a change in the visual appearance of the scene. If the object is too far away or out of view, the action will fail.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para ltx_noindent">
<p id="S4.SS3.p5.1" class="ltx_p"><span id="S4.SS3.p5.1.1" class="ltx_text ltx_font_bold">Answerer</span> The <em id="S4.SS3.p5.1.2" class="ltx_emph ltx_font_italic">Answerer</em> is invoked by the <em id="S4.SS3.p5.1.3" class="ltx_emph ltx_font_italic">Planner</em> to answer the question. It uses the current image, the full spatial memory, and the question embedding vector to predict answer probabilities <math id="S4.SS3.p5.1.m1.1" class="ltx_Math" alttext="a_{i}" display="inline"><semantics id="S4.SS3.p5.1.m1.1a"><msub id="S4.SS3.p5.1.m1.1.1" xref="S4.SS3.p5.1.m1.1.1.cmml"><mi id="S4.SS3.p5.1.m1.1.1.2" xref="S4.SS3.p5.1.m1.1.1.2.cmml">a</mi><mi id="S4.SS3.p5.1.m1.1.1.3" xref="S4.SS3.p5.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.1.m1.1b"><apply id="S4.SS3.p5.1.m1.1.1.cmml" xref="S4.SS3.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p5.1.m1.1.1.1.cmml" xref="S4.SS3.p5.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p5.1.m1.1.1.2.cmml" xref="S4.SS3.p5.1.m1.1.1.2">𝑎</ci><ci id="S4.SS3.p5.1.m1.1.1.3.cmml" xref="S4.SS3.p5.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.1.m1.1c">a_{i}</annotation></semantics></math> for each possible answer to the question. The question vector is tiled to create a tensor with the same width and height as the spatial memory. These are depthwise concatenated with the spatial memory and passed through 4 convolution and max pool layers followed by a sum over the spatial layers. This output vector is fed through two fully connected layers and a softmax over possible answer choices. After the <em id="S4.SS3.p5.1.4" class="ltx_emph ltx_font_italic">Answerer</em> is invoked, the episode ends, whether the answer was correct or not.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Training</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">The full system is trained jointly. However, since the individual tasks of the controllers are mostly independent, we are able to pretrain them separately. Our initial analysis showed that this leads to faster convergence and better accuracy than training end-to-end from scratch. We outline our training procedures below.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.1" class="ltx_p"><span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_bold">Planner:</span> To pretrain the <em id="S4.SS4.p2.1.2" class="ltx_emph ltx_font_italic">Planner</em>, we assume a perfect <em id="S4.SS4.p2.1.3" class="ltx_emph ltx_font_italic">Navigator</em> and <em id="S4.SS4.p2.1.4" class="ltx_emph ltx_font_italic">Detector</em> by using the ground truth shortest path for the <em id="S4.SS4.p2.1.5" class="ltx_emph ltx_font_italic">Navigator</em> and the ground truth object information for the <em id="S4.SS4.p2.1.6" class="ltx_emph ltx_font_italic">Detector</em>.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para ltx_noindent">
<p id="S4.SS4.p3.1" class="ltx_p"><span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_bold">Navigator:</span> We pretrain the <em id="S4.SS4.p3.1.2" class="ltx_emph ltx_font_italic">Navigator</em> by providing pairs of random starting points and goal locations.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para ltx_noindent">
<p id="S4.SS4.p4.1" class="ltx_p"><span id="S4.SS4.p4.1.1" class="ltx_text ltx_font_bold">Answerer:</span> The <em id="S4.SS4.p4.1.2" class="ltx_emph ltx_font_italic">Answerer</em> is pretrained by using ground-truth partial semantic maps which contain enough information to answer the current question correctly.</p>
</div>
<div id="S4.SS4.p5" class="ltx_para ltx_noindent">
<p id="S4.SS4.p5.1" class="ltx_p"><span id="S4.SS4.p5.1.1" class="ltx_text ltx_font_bold">Detector:</span> The <em id="S4.SS4.p5.1.2" class="ltx_emph ltx_font_italic">Detector</em> is pretrained by fine-tuning YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> on the AI2-THOR training scenes. It is trained to identify small object instances which may repeat in multiple scenes (apples, forks, etc.) as well as large object instances which are unique to each scene (e.g. each fridge model will only exist in one scene).</p>
</div>
<div id="S4.SS4.p6" class="ltx_para ltx_noindent">
<p id="S4.SS4.p6.1" class="ltx_p"><span id="S4.SS4.p6.1.1" class="ltx_text ltx_font_bold">Scanner and Manipulator:</span> There are no trainable parameters for these controllers in our current setup. Their behavior is predefined by the AI2-THOR environment.</p>
</div>
<div id="S4.SS4.p7" class="ltx_para ltx_noindent">
<p id="S4.SS4.p7.1" class="ltx_p"><span id="S4.SS4.p7.1.1" class="ltx_text ltx_font_bold">Joint Training</span> After all trainable controllers are pretrained, we update the model end-to-end.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:496.9pt;height:112pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-33.4pt,7.5pt) scale(0.881600717800474,0.881600717800474) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"></th>
<th id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" colspan="2">Existence</th>
<th id="S4.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" colspan="2">Counting</th>
<th id="S4.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" colspan="2">Spatial Relationships</th>
</tr>
<tr id="S4.T3.1.1.2.2" class="ltx_tr">
<th id="S4.T3.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Model</th>
<th id="S4.T3.1.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Accuracy</th>
<th id="S4.T3.1.1.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Length</th>
<th id="S4.T3.1.1.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Accuracy</th>
<th id="S4.T3.1.1.2.2.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Length</th>
<th id="S4.T3.1.1.2.2.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Accuracy</th>
<th id="S4.T3.1.1.2.2.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Length</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.3.1" class="ltx_tr">
<th id="S4.T3.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">HIMN with YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> detections</th>
<td id="S4.T3.1.1.3.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">68.47</td>
<td id="S4.T3.1.1.3.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">318.33</td>
<td id="S4.T3.1.1.3.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">30.43</td>
<td id="S4.T3.1.1.3.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">926.11</td>
<td id="S4.T3.1.1.3.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">58.67</td>
<td id="S4.T3.1.1.3.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">516.23</td>
</tr>
<tr id="S4.T3.1.1.4.2" class="ltx_tr">
<th id="S4.T3.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr">HIMN with GT detection</th>
<td id="S4.T3.1.1.4.2.2" class="ltx_td ltx_align_left ltx_border_r">86.56</td>
<td id="S4.T3.1.1.4.2.3" class="ltx_td ltx_align_left ltx_border_r">679.70</td>
<td id="S4.T3.1.1.4.2.4" class="ltx_td ltx_align_left ltx_border_r">35.31</td>
<td id="S4.T3.1.1.4.2.5" class="ltx_td ltx_align_left ltx_border_r">604.79</td>
<td id="S4.T3.1.1.4.2.6" class="ltx_td ltx_align_left ltx_border_r">70.94</td>
<td id="S4.T3.1.1.4.2.7" class="ltx_td ltx_align_left ltx_border_r">311.03</td>
</tr>
<tr id="S4.T3.1.1.5.3" class="ltx_tr">
<th id="S4.T3.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr">HIMN with GT detection and oracle navigator (HIMN-GT)</th>
<td id="S4.T3.1.1.5.3.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T3.1.1.5.3.2.1" class="ltx_text ltx_font_bold">88.60</span></td>
<td id="S4.T3.1.1.5.3.3" class="ltx_td ltx_align_left ltx_border_r">618.63</td>
<td id="S4.T3.1.1.5.3.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T3.1.1.5.3.4.1" class="ltx_text ltx_font_bold">48.44</span></td>
<td id="S4.T3.1.1.5.3.5" class="ltx_td ltx_align_left ltx_border_r">871.12</td>
<td id="S4.T3.1.1.5.3.6" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T3.1.1.5.3.6.1" class="ltx_text ltx_font_bold">72.50</span></td>
<td id="S4.T3.1.1.5.3.7" class="ltx_td ltx_align_left ltx_border_r">475.55</td>
</tr>
<tr id="S4.T3.1.1.6.4" class="ltx_tr">
<th id="S4.T3.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr">HIMN-GT Question not given to planner</th>
<td id="S4.T3.1.1.6.4.2" class="ltx_td ltx_align_left ltx_border_r">50.00</td>
<td id="S4.T3.1.1.6.4.3" class="ltx_td ltx_align_left ltx_border_r">150.60</td>
<td id="S4.T3.1.1.6.4.4" class="ltx_td ltx_align_left ltx_border_r">24.50</td>
<td id="S4.T3.1.1.6.4.5" class="ltx_td ltx_align_left ltx_border_r">293.33</td>
<td id="S4.T3.1.1.6.4.6" class="ltx_td ltx_align_left ltx_border_r">50.25</td>
<td id="S4.T3.1.1.6.4.7" class="ltx_td ltx_align_left ltx_border_r">118.09</td>
</tr>
<tr id="S4.T3.1.1.7.5" class="ltx_tr">
<th id="S4.T3.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr">HIMN-GT No loss on invalid actions</th>
<td id="S4.T3.1.1.7.5.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">49.84</td>
<td id="S4.T3.1.1.7.5.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">659.28</td>
<td id="S4.T3.1.1.7.5.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">24.84</td>
<td id="S4.T3.1.1.7.5.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">911.46</td>
<td id="S4.T3.1.1.7.5.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">50.00</td>
<td id="S4.T3.1.1.7.5.7" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">613.50</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Ablation experiments on the <span id="S4.T3.3.1" class="ltx_text ltx_font_smallcaps">himn</span> model.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.2" class="ltx_p">We evaluate <span id="S5.p1.2.1" class="ltx_text ltx_font_smallcaps">himn</span> on the <span id="S5.p1.2.2" class="ltx_text ltx_font_smallcaps">iquad v1</span> dataset, using Top-1 question answering accuracy. An initial baseline of Most likely answer per Question-Type (MLA) shows that the dataset is exactly balanced. Additionally, because we construct the data such that each generated question has a scene configuration for each answer possibility, there is no possible language bias in the dataset. The learned baseline that we compare to (A3C), is based on a common architecture for reinforcement learning used in past works including for visual semantic planning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> and task oriented language grounding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. We extend this for the purpose of question answering. Since <span id="S5.p1.2.3" class="ltx_text ltx_font_smallcaps">himn</span> has access to object detections provided by either the environment or YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, we also provide detections to the baseline. For the baseline model, at each time-step, the raw RGB image observed by the agent is concatenated depth wise with object detections (one channel per object class). This tensor is passed through convolutional layers and fed into a GRU. The question is passed through an LSTM. The output of the LSTM and GRU are concatenated, and passed through two fully connected layers to produce probabilities <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="\pi_{i}" display="inline"><semantics id="S5.p1.1.m1.1a"><msub id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mi id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">π</mi><mi id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1">subscript</csymbol><ci id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">𝜋</ci><ci id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\pi_{i}</annotation></semantics></math> for each action and a value <math id="S5.p1.2.m2.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S5.p1.2.m2.1a"><mi id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><ci id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">v</annotation></semantics></math>. The output of the first fully connected layer is also passed to an answering module that consists of two more fully connected layers with a softmax on the space of all possible answers. The model is trained using the A3C algorithm for action probabilities and a supervised loss on the answers. We also provide a human baseline of random questions on each question type.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Table <a href="#S4.T2" title="Table 2 ‣ 4.3 Low level controllers ‣ 4 Model ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the test accuracies and the average episode lengths for the proposed <span id="S5.p2.1.1" class="ltx_text ltx_font_smallcaps">himn</span> model and baselines for each question type. <span id="S5.p2.1.2" class="ltx_text ltx_font_smallcaps">himn</span> significantly outperforms the baselines on all question types, both with YOLO object detections as well as ground truth object detections. Surprisingly, the A3C baseline performs slightly worse than random chance even with ground truth detections. We conjecture that this is because there is no explicit signal for when to answer, and no persistence of object detections. The A3C model is not able to associate object detections with the question, and thus has no reason to remember detections for long periods of time. Because <span id="S5.p2.1.3" class="ltx_text ltx_font_smallcaps">himn</span> does not overwrite its entire spatial memory at each timestep, object detections persist for much longer, and the <em id="S5.p2.1.4" class="ltx_emph ltx_font_italic">Answerer</em> can better learn the associations between the questions and the objects. <span id="S5.p2.1.5" class="ltx_text ltx_font_smallcaps">himn</span> further benefits from a spatial memory in counting and spatial relationship questions because these require much more spatial reasoning than existence questions. Additionally, because A3C does not learn to answer questions, it also does not learn to efficiently explore the environments, as most of the reward comes from answering questions correctly. <span id="S5.p2.1.6" class="ltx_text ltx_font_smallcaps">himn</span>, on the other hand, traverses much more of the environment, only answering when it is confident that it has sufficiently explored the room. This indicates that <span id="S5.p2.1.7" class="ltx_text ltx_font_smallcaps">himn</span> (which uses an explicit semantic spatial memory with egocentric updates) is more effective than A3C (which uses a standard fully-connected GRU) at (a) Estimating when the environment has been sufficiently explored, given the question (b) Keeping track of past observations for much longer durations, which is important in determining answers for questions that require a thorough search of the environment, and (c) Keeping track of multiple object instances in the scene, which may be observed several time steps apart (which is crucial for answering counting questions).</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Ablation Analysis</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We perform four ablative experiments on our network structure and inputs, shown in table <a href="#S4.T3" title="Table 3 ‣ 4.4 Training ‣ 4 Model ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. First, we use the ground truth object detections and depth instead of YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> and FRCN depth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. This adds a dramatic improvement to our model owing primarily to the fact that without detection mistakes, the <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_italic">Answerer</span> can be more accurate and confident. Secondly, we substitute our learned navigation controller with an oracle <em id="S5.SS1.p1.1.2" class="ltx_emph ltx_font_italic">Navigator</em> that takes the shortest path in the environment. When the optimal <em id="S5.SS1.p1.1.3" class="ltx_emph ltx_font_italic">Navigator</em> is provided, <span id="S5.SS1.p1.1.4" class="ltx_text ltx_font_smallcaps">himn</span> further improves. This is because the <em id="S5.SS1.p1.1.5" class="ltx_emph ltx_font_italic">Planner</em> can more accurately direct the agent through the environment, allowing it to be more efficient and more thorough at exploring the environment. It also takes fewer invalid actions (as seen in table <a href="#S5.T4" title="Table 4 ‣ 5.1 Ablation Analysis ‣ 5 Experiments ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), indicating that it is less likely to get stuck in parts of the room. In our third ablative experiment, we remove the question vector from the input of the <em id="S5.SS1.p1.1.6" class="ltx_emph ltx_font_italic">Planner</em>, only providing it to the <em id="S5.SS1.p1.1.7" class="ltx_emph ltx_font_italic">Answerer</em>, which results in random performance. This shows that the <em id="S5.SS1.p1.1.8" class="ltx_emph ltx_font_italic">Planner</em> utilizes the question to direct the agent towards different parts of the room to gather information required to answer the question. For instance any question about an object in the fridge requires the planner to know the fridge needs to be opened. If the planner is not told the question, it has no reason to open the fridge, and instead will likely choose to continue exploring the room as exploration often gives more reward than opening an object. Also, some questions can be answered soon after an object is observed (<em id="S5.SS1.p1.1.9" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.SS1.p1.1.10" class="ltx_text"></span> Existence), whereas others require longer explorations (<em id="S5.SS1.p1.1.11" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.SS1.p1.1.12" class="ltx_text"></span> Counting). Having access to the questions can clearly help the <em id="S5.SS1.p1.1.13" class="ltx_emph ltx_font_italic">Planner</em> in these scenarios. Tables <a href="#S4.T3" title="Table 3 ‣ 4.4 Training ‣ 4 Model ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that <span id="S5.SS1.p1.1.14" class="ltx_text ltx_font_smallcaps">himn</span> does in fact explore the environment for longer durations for Counting questions than for Existence and Spatial Relationship questions. In our final ablation experiment, we remove the loss on invalid actions. If we do not apply any loss on these actions and only propagate gradients through the chosen action, the agent suffers from the difficulty of exploring a large action space and again performs at random chance.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<div id="S5.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:75.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-97.4pt,34.1pt) scale(0.526631864169522,0.526631864169522) ;">
<table id="S5.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.1.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="4">Percentage of invalid actions</th>
</tr>
<tr id="S5.T4.1.1.2.2" class="ltx_tr">
<th id="S5.T4.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Model</th>
<th id="S5.T4.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Existence</th>
<th id="S5.T4.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Counting</th>
<th id="S5.T4.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Spatial Relationships</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1.3.1" class="ltx_tr">
<th id="S5.T4.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">A3C with GT detections</th>
<td id="S5.T4.1.1.3.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">32.75</td>
<td id="S5.T4.1.1.3.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">34.55</td>
<td id="S5.T4.1.1.3.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">32.63</td>
</tr>
<tr id="S5.T4.1.1.4.2" class="ltx_tr">
<th id="S5.T4.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">HIMN No loss on invalid actions</th>
<td id="S5.T4.1.1.4.2.2" class="ltx_td ltx_align_left ltx_border_r">56.27</td>
<td id="S5.T4.1.1.4.2.3" class="ltx_td ltx_align_left ltx_border_r">53.43</td>
<td id="S5.T4.1.1.4.2.4" class="ltx_td ltx_align_left ltx_border_r">51.93</td>
</tr>
<tr id="S5.T4.1.1.5.3" class="ltx_tr">
<th id="S5.T4.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">HIMN with YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> detections</th>
<td id="S5.T4.1.1.5.3.2" class="ltx_td ltx_align_left ltx_border_r">6.07</td>
<td id="S5.T4.1.1.5.3.3" class="ltx_td ltx_align_left ltx_border_r">5.95</td>
<td id="S5.T4.1.1.5.3.4" class="ltx_td ltx_align_left ltx_border_r">6.68</td>
</tr>
<tr id="S5.T4.1.1.6.4" class="ltx_tr">
<th id="S5.T4.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">HIMN with GT detections</th>
<td id="S5.T4.1.1.6.4.2" class="ltx_td ltx_align_left ltx_border_r">6.49</td>
<td id="S5.T4.1.1.6.4.3" class="ltx_td ltx_align_left ltx_border_r">5.71</td>
<td id="S5.T4.1.1.6.4.4" class="ltx_td ltx_align_left ltx_border_r">5.66</td>
</tr>
<tr id="S5.T4.1.1.7.5" class="ltx_tr">
<th id="S5.T4.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">HIMN-GT</th>
<td id="S5.T4.1.1.7.5.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S5.T4.1.1.7.5.2.1" class="ltx_text ltx_font_bold">1.79</span></td>
<td id="S5.T4.1.1.7.5.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S5.T4.1.1.7.5.3.1" class="ltx_text ltx_font_bold">2.02</span></td>
<td id="S5.T4.1.1.7.5.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S5.T4.1.1.7.5.4.1" class="ltx_text ltx_font_bold">1.27</span></td>
</tr>
<tr id="S5.T4.1.1.8.6" class="ltx_tr">
<th id="S5.T4.1.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">Human</th>
<td id="S5.T4.1.1.8.6.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">5.99</td>
<td id="S5.T4.1.1.8.6.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">6.47</td>
<td id="S5.T4.1.1.8.6.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">3.49</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>This tables compares the percentage of invalid actions across different models on test. Lower is better.</figcaption>
</figure>
<figure id="S5.F5" class="ltx_figure"><img src="/html/1712.03316/assets/x5.png" id="S5.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="187" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>A sample trajectory for answering the existence question: <em id="S5.F5.5.1" class="ltx_emph ltx_font_italic">Is there bread in the room?</em> The purple sections indicate a <em id="S5.F5.6.2" class="ltx_emph ltx_font_italic">Planner</em> step, and the gray sections indicate a lower level controller such as the <em id="S5.F5.7.3" class="ltx_emph ltx_font_italic">Navigator</em> is controlling the agent. For a more detailed explanation, refer to section <a href="#S5.SS4" title="5.4 Qualitative Results ‣ 5 Experiments ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.4</span></a>.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Invalid Actions</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Table <a href="#S5.T4" title="Table 4 ‣ 5.1 Ablation Analysis ‣ 5 Experiments ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the percentage of invalid actions taken by the different methods. Failed actions are due to navigation failures (failing to see an obstacle) or interaction failures (trying to interact with something too far away or otherwise impossible).
There is a clear benefit to including a loss on the invalid actions both in terms of QA accuracy, as can be seen in table <a href="#S4.T3" title="Table 3 ‣ 4.4 Training ‣ 4 Model ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, as well as in terms of percentage of invalid actions performed, shown in table <a href="#S5.T4" title="Table 4 ‣ 5.1 Ablation Analysis ‣ 5 Experiments ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. All models in table <a href="#S5.T4" title="Table 4 ‣ 5.1 Ablation Analysis ‣ 5 Experiments ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> are penalized for every invalid action they attempt, but this only provides feedback on a single action at every timestep. With the addition of a supervised loss on all possible actions, the percentage of invalid actions performed is nearly an order of magnitude lower. By directly training our agent to recognize affordances (valid actions), we are able to mitigate the difficulties posed by a large action space, allowing the <em id="S5.SS2.p1.1.1" class="ltx_emph ltx_font_italic">Planner</em> to learn much more quickly. The validity loss also serves as an auxiliary task which has been shown to aid the convergence of RL algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. By replacing the learned <em id="S5.SS2.p1.1.2" class="ltx_emph ltx_font_italic">Navigator</em> with an oracle, we observe that the majority of failed actions are due to navigation failures. We believe that with a smaller step size, we would further reduce the navigation errors at the expense of longer trajectories.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<div id="S5.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:38.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-96.6pt,17.0pt) scale(0.528772460630737,0.528772460630737) ;">
<table id="S5.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.1.1.1.1" class="ltx_tr">
<th id="S5.T5.1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t"></th>
<th id="S5.T5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" colspan="2">Existence</th>
<th id="S5.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" colspan="2">Counting</th>
<th id="S5.T5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" colspan="2">Spatial Relationships</th>
</tr>
<tr id="S5.T5.1.1.2.2" class="ltx_tr">
<th id="S5.T5.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Model</th>
<th id="S5.T5.1.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">S</th>
<th id="S5.T5.1.1.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">U</th>
<th id="S5.T5.1.1.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">S</th>
<th id="S5.T5.1.1.2.2.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">U</th>
<th id="S5.T5.1.1.2.2.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">S</th>
<th id="S5.T5.1.1.2.2.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">U</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.1.1.3.1" class="ltx_tr">
<th id="S5.T5.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">HIMN with YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> detections</th>
<td id="S5.T5.1.1.3.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">73.68</td>
<td id="S5.T5.1.1.3.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">68.47</td>
<td id="S5.T5.1.1.3.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">36.26</td>
<td id="S5.T5.1.1.3.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">30.43</td>
<td id="S5.T5.1.1.3.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">60.71</td>
<td id="S5.T5.1.1.3.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">58.67</td>
</tr>
<tr id="S5.T5.1.1.4.2" class="ltx_tr">
<th id="S5.T5.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr">HIMN with GT detections</th>
<td id="S5.T5.1.1.4.2.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">94.00</td>
<td id="S5.T5.1.1.4.2.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">86.56</td>
<td id="S5.T5.1.1.4.2.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">42.38</td>
<td id="S5.T5.1.1.4.2.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">35.31</td>
<td id="S5.T5.1.1.4.2.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">73.38</td>
<td id="S5.T5.1.1.4.2.7" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">70.94</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>This tables compares the accuracy of question answering across different models on Seen (S) and Unseen (U) environments.</figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Generalization in Unseen Environments</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">One benefit of <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_smallcaps">himn</span> over other RL architectures is that encoding semantic information into a spatial map should generalize well in both seen and unseen environments. Thus, in table <a href="#S5.T5" title="Table 5 ‣ 5.2 Invalid Actions ‣ 5 Experiments ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we compare <span id="S5.SS3.p1.1.2" class="ltx_text ltx_font_smallcaps">himn</span>’s performance on seen and unseen environments. Unseen environments tests the agent with questions that occur in 5 never-before-seen rooms, whereas the seen environments use the 25 training rooms but never-before-seen object placements and corresponding questions. Despite our relatively small number of training rooms, table <a href="#S5.T5" title="Table 5 ‣ 5.2 Invalid Actions ‣ 5 Experiments ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows that our method only loses up to a few percentage points of accuracy when tested on unseen environments. This contrasts with many other end-to-end RL methods which learn deep features that tend to limit their applicability outside a known domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>. Note that all experiments in previous sections were only performed on unseen environments.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Qualitative Results</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.5" class="ltx_p">Figure <a href="#S5.F5" title="Figure 5 ‣ 5.1 Ablation Analysis ‣ 5 Experiments ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows a sample run of <span id="S5.SS4.p1.5.1" class="ltx_text ltx_font_smallcaps">himn</span> for the question “Is there bread in the room.” Initially, <math id="S5.SS4.p1.1.m1.1" class="ltx_Math" alttext="P(True)" display="inline"><semantics id="S5.SS4.p1.1.m1.1a"><mrow id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml"><mi id="S5.SS4.p1.1.m1.1.1.3" xref="S5.SS4.p1.1.m1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.1.m1.1.1.2" xref="S5.SS4.p1.1.m1.1.1.2.cmml">​</mo><mrow id="S5.SS4.p1.1.m1.1.1.1.1" xref="S5.SS4.p1.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS4.p1.1.m1.1.1.1.1.2" xref="S5.SS4.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS4.p1.1.m1.1.1.1.1.1" xref="S5.SS4.p1.1.m1.1.1.1.1.1.cmml"><mi id="S5.SS4.p1.1.m1.1.1.1.1.1.2" xref="S5.SS4.p1.1.m1.1.1.1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.1.m1.1.1.1.1.1.1" xref="S5.SS4.p1.1.m1.1.1.1.1.1.1.cmml">​</mo><mi id="S5.SS4.p1.1.m1.1.1.1.1.1.3" xref="S5.SS4.p1.1.m1.1.1.1.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.1.m1.1.1.1.1.1.1a" xref="S5.SS4.p1.1.m1.1.1.1.1.1.1.cmml">​</mo><mi id="S5.SS4.p1.1.m1.1.1.1.1.1.4" xref="S5.SS4.p1.1.m1.1.1.1.1.1.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.1.m1.1.1.1.1.1.1b" xref="S5.SS4.p1.1.m1.1.1.1.1.1.1.cmml">​</mo><mi id="S5.SS4.p1.1.m1.1.1.1.1.1.5" xref="S5.SS4.p1.1.m1.1.1.1.1.1.5.cmml">e</mi></mrow><mo stretchy="false" id="S5.SS4.p1.1.m1.1.1.1.1.3" xref="S5.SS4.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><apply id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1"><times id="S5.SS4.p1.1.m1.1.1.2.cmml" xref="S5.SS4.p1.1.m1.1.1.2"></times><ci id="S5.SS4.p1.1.m1.1.1.3.cmml" xref="S5.SS4.p1.1.m1.1.1.3">𝑃</ci><apply id="S5.SS4.p1.1.m1.1.1.1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1.1.1"><times id="S5.SS4.p1.1.m1.1.1.1.1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1.1.1.1.1"></times><ci id="S5.SS4.p1.1.m1.1.1.1.1.1.2.cmml" xref="S5.SS4.p1.1.m1.1.1.1.1.1.2">𝑇</ci><ci id="S5.SS4.p1.1.m1.1.1.1.1.1.3.cmml" xref="S5.SS4.p1.1.m1.1.1.1.1.1.3">𝑟</ci><ci id="S5.SS4.p1.1.m1.1.1.1.1.1.4.cmml" xref="S5.SS4.p1.1.m1.1.1.1.1.1.4">𝑢</ci><ci id="S5.SS4.p1.1.m1.1.1.1.1.1.5.cmml" xref="S5.SS4.p1.1.m1.1.1.1.1.1.5">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">P(True)</annotation></semantics></math> and <math id="S5.SS4.p1.2.m2.1" class="ltx_Math" alttext="P(False)" display="inline"><semantics id="S5.SS4.p1.2.m2.1a"><mrow id="S5.SS4.p1.2.m2.1.1" xref="S5.SS4.p1.2.m2.1.1.cmml"><mi id="S5.SS4.p1.2.m2.1.1.3" xref="S5.SS4.p1.2.m2.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.2.m2.1.1.2" xref="S5.SS4.p1.2.m2.1.1.2.cmml">​</mo><mrow id="S5.SS4.p1.2.m2.1.1.1.1" xref="S5.SS4.p1.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS4.p1.2.m2.1.1.1.1.2" xref="S5.SS4.p1.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS4.p1.2.m2.1.1.1.1.1" xref="S5.SS4.p1.2.m2.1.1.1.1.1.cmml"><mi id="S5.SS4.p1.2.m2.1.1.1.1.1.2" xref="S5.SS4.p1.2.m2.1.1.1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.2.m2.1.1.1.1.1.1" xref="S5.SS4.p1.2.m2.1.1.1.1.1.1.cmml">​</mo><mi id="S5.SS4.p1.2.m2.1.1.1.1.1.3" xref="S5.SS4.p1.2.m2.1.1.1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.2.m2.1.1.1.1.1.1a" xref="S5.SS4.p1.2.m2.1.1.1.1.1.1.cmml">​</mo><mi id="S5.SS4.p1.2.m2.1.1.1.1.1.4" xref="S5.SS4.p1.2.m2.1.1.1.1.1.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.2.m2.1.1.1.1.1.1b" xref="S5.SS4.p1.2.m2.1.1.1.1.1.1.cmml">​</mo><mi id="S5.SS4.p1.2.m2.1.1.1.1.1.5" xref="S5.SS4.p1.2.m2.1.1.1.1.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.2.m2.1.1.1.1.1.1c" xref="S5.SS4.p1.2.m2.1.1.1.1.1.1.cmml">​</mo><mi id="S5.SS4.p1.2.m2.1.1.1.1.1.6" xref="S5.SS4.p1.2.m2.1.1.1.1.1.6.cmml">e</mi></mrow><mo stretchy="false" id="S5.SS4.p1.2.m2.1.1.1.1.3" xref="S5.SS4.p1.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.2.m2.1b"><apply id="S5.SS4.p1.2.m2.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1"><times id="S5.SS4.p1.2.m2.1.1.2.cmml" xref="S5.SS4.p1.2.m2.1.1.2"></times><ci id="S5.SS4.p1.2.m2.1.1.3.cmml" xref="S5.SS4.p1.2.m2.1.1.3">𝑃</ci><apply id="S5.SS4.p1.2.m2.1.1.1.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1.1.1"><times id="S5.SS4.p1.2.m2.1.1.1.1.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1.1.1.1.1"></times><ci id="S5.SS4.p1.2.m2.1.1.1.1.1.2.cmml" xref="S5.SS4.p1.2.m2.1.1.1.1.1.2">𝐹</ci><ci id="S5.SS4.p1.2.m2.1.1.1.1.1.3.cmml" xref="S5.SS4.p1.2.m2.1.1.1.1.1.3">𝑎</ci><ci id="S5.SS4.p1.2.m2.1.1.1.1.1.4.cmml" xref="S5.SS4.p1.2.m2.1.1.1.1.1.4">𝑙</ci><ci id="S5.SS4.p1.2.m2.1.1.1.1.1.5.cmml" xref="S5.SS4.p1.2.m2.1.1.1.1.1.5">𝑠</ci><ci id="S5.SS4.p1.2.m2.1.1.1.1.1.6.cmml" xref="S5.SS4.p1.2.m2.1.1.1.1.1.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.2.m2.1c">P(False)</annotation></semantics></math> both start near 50%. The <em id="S5.SS4.p1.5.2" class="ltx_emph ltx_font_italic">Planner</em> begins searching the room by navigating around the kitchen table. During the initial exploration phase, bread is not detected, and <math id="S5.SS4.p1.3.m3.1" class="ltx_Math" alttext="P(False)" display="inline"><semantics id="S5.SS4.p1.3.m3.1a"><mrow id="S5.SS4.p1.3.m3.1.1" xref="S5.SS4.p1.3.m3.1.1.cmml"><mi id="S5.SS4.p1.3.m3.1.1.3" xref="S5.SS4.p1.3.m3.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.3.m3.1.1.2" xref="S5.SS4.p1.3.m3.1.1.2.cmml">​</mo><mrow id="S5.SS4.p1.3.m3.1.1.1.1" xref="S5.SS4.p1.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS4.p1.3.m3.1.1.1.1.2" xref="S5.SS4.p1.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS4.p1.3.m3.1.1.1.1.1" xref="S5.SS4.p1.3.m3.1.1.1.1.1.cmml"><mi id="S5.SS4.p1.3.m3.1.1.1.1.1.2" xref="S5.SS4.p1.3.m3.1.1.1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.3.m3.1.1.1.1.1.1" xref="S5.SS4.p1.3.m3.1.1.1.1.1.1.cmml">​</mo><mi id="S5.SS4.p1.3.m3.1.1.1.1.1.3" xref="S5.SS4.p1.3.m3.1.1.1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.3.m3.1.1.1.1.1.1a" xref="S5.SS4.p1.3.m3.1.1.1.1.1.1.cmml">​</mo><mi id="S5.SS4.p1.3.m3.1.1.1.1.1.4" xref="S5.SS4.p1.3.m3.1.1.1.1.1.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.3.m3.1.1.1.1.1.1b" xref="S5.SS4.p1.3.m3.1.1.1.1.1.1.cmml">​</mo><mi id="S5.SS4.p1.3.m3.1.1.1.1.1.5" xref="S5.SS4.p1.3.m3.1.1.1.1.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.3.m3.1.1.1.1.1.1c" xref="S5.SS4.p1.3.m3.1.1.1.1.1.1.cmml">​</mo><mi id="S5.SS4.p1.3.m3.1.1.1.1.1.6" xref="S5.SS4.p1.3.m3.1.1.1.1.1.6.cmml">e</mi></mrow><mo stretchy="false" id="S5.SS4.p1.3.m3.1.1.1.1.3" xref="S5.SS4.p1.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.3.m3.1b"><apply id="S5.SS4.p1.3.m3.1.1.cmml" xref="S5.SS4.p1.3.m3.1.1"><times id="S5.SS4.p1.3.m3.1.1.2.cmml" xref="S5.SS4.p1.3.m3.1.1.2"></times><ci id="S5.SS4.p1.3.m3.1.1.3.cmml" xref="S5.SS4.p1.3.m3.1.1.3">𝑃</ci><apply id="S5.SS4.p1.3.m3.1.1.1.1.1.cmml" xref="S5.SS4.p1.3.m3.1.1.1.1"><times id="S5.SS4.p1.3.m3.1.1.1.1.1.1.cmml" xref="S5.SS4.p1.3.m3.1.1.1.1.1.1"></times><ci id="S5.SS4.p1.3.m3.1.1.1.1.1.2.cmml" xref="S5.SS4.p1.3.m3.1.1.1.1.1.2">𝐹</ci><ci id="S5.SS4.p1.3.m3.1.1.1.1.1.3.cmml" xref="S5.SS4.p1.3.m3.1.1.1.1.1.3">𝑎</ci><ci id="S5.SS4.p1.3.m3.1.1.1.1.1.4.cmml" xref="S5.SS4.p1.3.m3.1.1.1.1.1.4">𝑙</ci><ci id="S5.SS4.p1.3.m3.1.1.1.1.1.5.cmml" xref="S5.SS4.p1.3.m3.1.1.1.1.1.5">𝑠</ci><ci id="S5.SS4.p1.3.m3.1.1.1.1.1.6.cmml" xref="S5.SS4.p1.3.m3.1.1.1.1.1.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.3.m3.1c">P(False)</annotation></semantics></math> slowly increases. At timestep 39, the <em id="S5.SS4.p1.5.3" class="ltx_emph ltx_font_italic">Navigator</em> invokes the <em id="S5.SS4.p1.5.4" class="ltx_emph ltx_font_italic">Detector</em>, which sees the bread and incorporates it into the semantic spatial map. However, the <em id="S5.SS4.p1.5.5" class="ltx_emph ltx_font_italic">Navigator</em> does not return control to the <em id="S5.SS4.p1.5.6" class="ltx_emph ltx_font_italic">Planner</em>, as it has not yet reached the desired destination. Upon returning at timestep 45, the <em id="S5.SS4.p1.5.7" class="ltx_emph ltx_font_italic">Planner</em> reads the spatial map, sees the bread, and immediately decides it can answer the question. Thus <math id="S5.SS4.p1.4.m4.1" class="ltx_Math" alttext="\pi_{answer}" display="inline"><semantics id="S5.SS4.p1.4.m4.1a"><msub id="S5.SS4.p1.4.m4.1.1" xref="S5.SS4.p1.4.m4.1.1.cmml"><mi id="S5.SS4.p1.4.m4.1.1.2" xref="S5.SS4.p1.4.m4.1.1.2.cmml">π</mi><mrow id="S5.SS4.p1.4.m4.1.1.3" xref="S5.SS4.p1.4.m4.1.1.3.cmml"><mi id="S5.SS4.p1.4.m4.1.1.3.2" xref="S5.SS4.p1.4.m4.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.4.m4.1.1.3.1" xref="S5.SS4.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S5.SS4.p1.4.m4.1.1.3.3" xref="S5.SS4.p1.4.m4.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.4.m4.1.1.3.1a" xref="S5.SS4.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S5.SS4.p1.4.m4.1.1.3.4" xref="S5.SS4.p1.4.m4.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.4.m4.1.1.3.1b" xref="S5.SS4.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S5.SS4.p1.4.m4.1.1.3.5" xref="S5.SS4.p1.4.m4.1.1.3.5.cmml">w</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.4.m4.1.1.3.1c" xref="S5.SS4.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S5.SS4.p1.4.m4.1.1.3.6" xref="S5.SS4.p1.4.m4.1.1.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.4.m4.1.1.3.1d" xref="S5.SS4.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S5.SS4.p1.4.m4.1.1.3.7" xref="S5.SS4.p1.4.m4.1.1.3.7.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.4.m4.1b"><apply id="S5.SS4.p1.4.m4.1.1.cmml" xref="S5.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS4.p1.4.m4.1.1.1.cmml" xref="S5.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S5.SS4.p1.4.m4.1.1.2.cmml" xref="S5.SS4.p1.4.m4.1.1.2">𝜋</ci><apply id="S5.SS4.p1.4.m4.1.1.3.cmml" xref="S5.SS4.p1.4.m4.1.1.3"><times id="S5.SS4.p1.4.m4.1.1.3.1.cmml" xref="S5.SS4.p1.4.m4.1.1.3.1"></times><ci id="S5.SS4.p1.4.m4.1.1.3.2.cmml" xref="S5.SS4.p1.4.m4.1.1.3.2">𝑎</ci><ci id="S5.SS4.p1.4.m4.1.1.3.3.cmml" xref="S5.SS4.p1.4.m4.1.1.3.3">𝑛</ci><ci id="S5.SS4.p1.4.m4.1.1.3.4.cmml" xref="S5.SS4.p1.4.m4.1.1.3.4">𝑠</ci><ci id="S5.SS4.p1.4.m4.1.1.3.5.cmml" xref="S5.SS4.p1.4.m4.1.1.3.5">𝑤</ci><ci id="S5.SS4.p1.4.m4.1.1.3.6.cmml" xref="S5.SS4.p1.4.m4.1.1.3.6">𝑒</ci><ci id="S5.SS4.p1.4.m4.1.1.3.7.cmml" xref="S5.SS4.p1.4.m4.1.1.3.7">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.4.m4.1c">\pi_{answer}</annotation></semantics></math> and <math id="S5.SS4.p1.5.m5.1" class="ltx_Math" alttext="P(True)" display="inline"><semantics id="S5.SS4.p1.5.m5.1a"><mrow id="S5.SS4.p1.5.m5.1.1" xref="S5.SS4.p1.5.m5.1.1.cmml"><mi id="S5.SS4.p1.5.m5.1.1.3" xref="S5.SS4.p1.5.m5.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.5.m5.1.1.2" xref="S5.SS4.p1.5.m5.1.1.2.cmml">​</mo><mrow id="S5.SS4.p1.5.m5.1.1.1.1" xref="S5.SS4.p1.5.m5.1.1.1.1.1.cmml"><mo stretchy="false" id="S5.SS4.p1.5.m5.1.1.1.1.2" xref="S5.SS4.p1.5.m5.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS4.p1.5.m5.1.1.1.1.1" xref="S5.SS4.p1.5.m5.1.1.1.1.1.cmml"><mi id="S5.SS4.p1.5.m5.1.1.1.1.1.2" xref="S5.SS4.p1.5.m5.1.1.1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.5.m5.1.1.1.1.1.1" xref="S5.SS4.p1.5.m5.1.1.1.1.1.1.cmml">​</mo><mi id="S5.SS4.p1.5.m5.1.1.1.1.1.3" xref="S5.SS4.p1.5.m5.1.1.1.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.5.m5.1.1.1.1.1.1a" xref="S5.SS4.p1.5.m5.1.1.1.1.1.1.cmml">​</mo><mi id="S5.SS4.p1.5.m5.1.1.1.1.1.4" xref="S5.SS4.p1.5.m5.1.1.1.1.1.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S5.SS4.p1.5.m5.1.1.1.1.1.1b" xref="S5.SS4.p1.5.m5.1.1.1.1.1.1.cmml">​</mo><mi id="S5.SS4.p1.5.m5.1.1.1.1.1.5" xref="S5.SS4.p1.5.m5.1.1.1.1.1.5.cmml">e</mi></mrow><mo stretchy="false" id="S5.SS4.p1.5.m5.1.1.1.1.3" xref="S5.SS4.p1.5.m5.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.5.m5.1b"><apply id="S5.SS4.p1.5.m5.1.1.cmml" xref="S5.SS4.p1.5.m5.1.1"><times id="S5.SS4.p1.5.m5.1.1.2.cmml" xref="S5.SS4.p1.5.m5.1.1.2"></times><ci id="S5.SS4.p1.5.m5.1.1.3.cmml" xref="S5.SS4.p1.5.m5.1.1.3">𝑃</ci><apply id="S5.SS4.p1.5.m5.1.1.1.1.1.cmml" xref="S5.SS4.p1.5.m5.1.1.1.1"><times id="S5.SS4.p1.5.m5.1.1.1.1.1.1.cmml" xref="S5.SS4.p1.5.m5.1.1.1.1.1.1"></times><ci id="S5.SS4.p1.5.m5.1.1.1.1.1.2.cmml" xref="S5.SS4.p1.5.m5.1.1.1.1.1.2">𝑇</ci><ci id="S5.SS4.p1.5.m5.1.1.1.1.1.3.cmml" xref="S5.SS4.p1.5.m5.1.1.1.1.1.3">𝑟</ci><ci id="S5.SS4.p1.5.m5.1.1.1.1.1.4.cmml" xref="S5.SS4.p1.5.m5.1.1.1.1.1.4">𝑢</ci><ci id="S5.SS4.p1.5.m5.1.1.1.1.1.5.cmml" xref="S5.SS4.p1.5.m5.1.1.1.1.1.5">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.5.m5.1c">P(True)</annotation></semantics></math> both increase to nearly 100%. For more examples, please see our supplementary video <a target="_blank" href="https://youtu.be/pXd3C-1jr98" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://youtu.be/pXd3C-1jr98</a>.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Limitations</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">Although <span id="S5.SS5.p1.1.1" class="ltx_text ltx_font_smallcaps">himn</span> performs quite well, it still has several obvious limitations. Due to the 2D nature of the semantic spatial map, <span id="S5.SS5.p1.1.2" class="ltx_text ltx_font_smallcaps">himn</span> is unable to differentiate between an object being inside a container and being on top of the container. Two obvious extensions of <span id="S5.SS5.p1.1.3" class="ltx_text ltx_font_smallcaps">himn</span> are storing an explicit height parameter or using multiple 2D slices to construct a 3D map. Secondly, as can be seen in the human experiments in table <a href="#S4.T2" title="Table 2 ‣ 4.3 Low level controllers ‣ 4 Model ‣ IQA: Visual Question Answering in Interactive Environments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, <span id="S5.SS5.p1.1.4" class="ltx_text ltx_font_smallcaps">himn</span> is still fairly inefficient at exploring the environment. We plan on investigating more traditional planning algorithms to reduce the time spent exploring previously searched areas. Finally, our templated language model is quite simple, and would not extend to arbitrary questions. We plan on extending <span id="S5.SS5.p1.1.5" class="ltx_text ltx_font_smallcaps">iquad</span> to include more varied questions, and we will use more expressive language embeddings like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> in future work.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this work, we pose a new problem of Interactive Question Answering for several question types in interactive environments. We propose the Hierarchical Interactive Memory Network, consisting of a factorized set of controllers, allowing the system to learn from long trajectories. We also introduce the Egocentric Spatial GRU for updating spatial memory maps. The effectiveness of our proposed model is demonstrated on a new benchmark dataset built upon a high-quality simulation environment for this task. This dataset still presents several challenges to our model and baselines and warrants future research.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Acknowledgements</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This work was funded in part by the National Science Foundation under contract number NSF-NRI-1637479, NSF-IIS-1338054, NSF-1652052, ONR N00014-13-1-0720, the Allen Distinguished Investigator Award, and the Allen Institute for Artificial Intelligence. We would like to thank Xun Huang for initial discussions and dataset prototypes. We would also like to thank NVIDIA for generously providing a DGX used for this research via the UW NVIDIA AI Lab (NVAIL).</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Unity software.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://unity3d.com" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://unity3d.com</a><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
A. Agrawal, J. Lu, S. Antol, M. Mitchell, C. L. Zitnick, D. Parikh, and
D. Batra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Vqa: Visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
J. L. Alireza Shafaei and M. Schmidt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Play and learn: Using video games to train computer vision models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In E. R. H. Richard C. Wilson and W. A. P. Smith, editors, </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the British Machine Vision Conference (BMVC)</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages
26.1–26.13. BMVA Press, September 2016.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
J. Andreas, M. Rohrbach, T. Darrell, and D. Klein.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Neural module networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">The arcade learning environment: An evaluation platform for general
agents.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">J. Artif. Intell. Res.(JAIR)</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
J. Borenstein and Y. Koren.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Real-time obstacle avoidance for fast mobile robots.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Systems, Man, and Cybernetics</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 1989.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
J. Borenstein and Y. Koren.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">The vector field histogram-fast obstacle avoidance for mobile robots.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on robotics and automation</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 1991.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
D. S. Chaplot, K. M. Sathyendra, R. K. Pasumarthi, D. Rajagopal, and
R. Salakhutdinov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Gated-attention architectures for task-oriented language grounding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, abs/1706.07230, 2017.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
C. Chen, A. Seff, A. Kornhauser, and J. Xiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Deepdriving: Learning affordance for direct perception in autonomous
driving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
A. J. Davison.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Real-time simultaneous localisation and mapping with a single camera.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2003.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
T. G. Dietterich.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Hierarchical reinforcement learning with the maxq value function
decomposition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">J. Artif. Intell. Res.</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 2000.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
C. Dornhege, M. Gissler, M. Teschner, and B. Nebel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Integrating symbolic and geometric planning for mobile manipulation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Safety, Security &amp; Rescue Robotics (SSRR), 2009 IEEE
International Workshop on</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
J. Engel, T. Schöps, and D. Cremers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Lsd-slam: Large-scale direct monocular slam.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
R. E. Fikes and N. J. Nilsson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Strips: A new approach to the application of theorem proving to
problem solving.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Artificial intelligence</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 1971.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Cognitive mapping and planning for visual navigation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 2616–2625, 2017.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
H. Haddad, M. Khatib, S. Lacroix, and R. Chatila.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Reactive navigation in outdoor environments using potential fields.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Robotics and Automation</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 1998.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
A. Handa, V. PǍtrǍucean, V. Badrinarayanan, R. Cipolla, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Understanding realworld indoor sceneswith synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
F. Hill, K. M. Hermann, P. Blunsom, and S. Clark.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Understanding grounded language learning agents.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, abs/1710.09867, 2017.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
R. Hu, J. Andreas, M. Rohrbach, T. Darrell, and K. Saenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Learning to reason: End-to-end module networks for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, abs/1704.05526, 2017.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and
K. Kavukcuoglu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Reinforcement learning with unsupervised auxiliary tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
A. Jaegle, S. Phillips, and K. Daniilidis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Fast, robust, continuous monocular egomotion computation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Robotics and Automation</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Y. Jang, Y. Song, Y. Yu, Y. Kim, and G. Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Tgif-qa: Toward spatio-temporal reasoning in visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, abs/1704.04497, 2017.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
J. Johnson, B. Hariharan, L. van der Maaten, F. fei Li, C. L. Zitnick, and
R. B. Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Clevr: A diagnostic dataset for compositional language and elementary
visual reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, abs/1612.06890, 2016.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
J. Johnson, B. Hariharan, L. van der Maaten, J. Hoffman, F. fei Li, C. L.
Zitnick, and R. B. Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Inferring and executing programs for visual reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">, abs/1705.03633, 2017.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
L. P. Kaelbling and T. Lozano-Pérez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Hierarchical task and motion planning in the now.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Robotics and Automation</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
S. E. Kahou, A. Atkinson, V. Michalski, Á. Kádár, A. Trischler, and
Y. Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Figureqa: An annotated figure dataset for visual reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, abs/1710.07300, 2017.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
A. Kembhavi, M. Salvato, E. Kolve, M. J. Seo, H. Hajishirzi, and A. Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">A diagram is worth a dozen images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
A. Kembhavi, M. Seo, D. Schwenk, J. Choi, A. Farhadi, and H. Hajishirzi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Are you smarter than a sixth grader? textbook question answering for
multimodal machine comprehension.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
M. Kempka, M. Wydmuch, G. Runc, J. Toczek, and W. Jaśkowski.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Vizdoom: A doom-based ai research platform for visual reinforcement
learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computational Intelligence and Games (CIG), 2016 IEEE
Conference on</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
D. Kim and R. Nevatia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Symbolic navigation with a generic map.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Autonomous Robots</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 1999.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
K.-M. Kim, M.-O. Heo, S.-H. Choi, and B.-T. Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Deepstory: Video story qa by deep embedded memory networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCAI</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
N. Kohl and P. Stone.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Policy gradient reinforcement learning for fast quadrupedal
locomotion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Robotics and Automation</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, 2004.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
T. Kollar and N. Roy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Trajectory optimization using reinforcement learning for map
exploration.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">The International Journal of Robotics Research</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, 2008.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">AI2-THOR: An Interactive 3D Environment for Visual AI.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
Y. Kalantidis, L.-J. Li, D. A. Shamma, M. S. Bernstein, and F. fei Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenenbaum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Hierarchical deep reinforcement learning: Integrating temporal
abstraction and intrinsic motivation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and N. Navab.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Deeper depth prediction with fully convolutional residual networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">3D Vision (3DV), 2016 Fourth International Conference on</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">,
2016.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
A. Lerer, S. Gross, and R. Fergus.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Learning physical intuition of block towers by example.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, pages
430–438, 2016.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
C. Linegar, W. Churchill, and P. Newman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Made to measure: Bespoke landmarks for 24-hour, all-weather
localisation with a camera.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Robotics and Automation</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
M. Malinowski and M. Fritz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">A multi-world approach to question answering about real-world scenes
based on uncertain input.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
M. Malinowski, M. Rohrbach, and M. Fritz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Ask your neurons: A neural-based approach to answering questions
about images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
J. Marin, D. Vázquez, D. Gerónimo, and A. M. López.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Learning appearance in virtual scenarios for pedestrian detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, 2010.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
J. Michels, A. Saxena, and A. Y. Ng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">High speed obstacle avoidance using monocular vision and
reinforcement learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, 2005.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
T. Mikolov, K. Chen, G. Corrado, and J. Dean.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Efficient estimation of word representations in vector space.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1301.3781</span><span id="bib.bib45.4.2" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
and K. Kavukcuoglu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Asynchronous methods for deep reinforcement learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
R. Mottaghi, H. Bagherinezhad, M. Rastegari, and A. Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Newtonian scene understanding: Unfolding the dynamics of objects in
static images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
R. Mottaghi, M. Rastegari, A. Gupta, and A. Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">“what happens if…” learning to predict the effect of forces in
images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
J. Mun, P. H. Seo, I. Jung, and B. Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Marioqa: Answering questions by watching gameplay videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib49.4.2" class="ltx_text" style="font-size:90%;">, abs/1612.01669, 2016.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Orb-slam: a versatile and accurate monocular slam system.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Robotics</span><span id="bib.bib50.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
J. Oh, S. Singh, H. Lee, and P. Kohli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Communicating hierarchical neural controllers for learning zero-shot
task generalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">2016.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
G. Oriolo, M. Vendittelli, and G. Ulivi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">On-line map building and navigation for autonomous mobile robots.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Robotics and Automation</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, 1995.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
J. Papon and M. Schoeler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Semantic pose using deep networks trained on synthetic rgb-d.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
R. E. Parr and S. J. Russell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Reinforcement learning with hierarchies of machines.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, 1997.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
J. Pennington, R. Socher, and C. Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Glove: Global vectors for word representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2014 conference on empirical methods in
natural language processing (EMNLP)</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:90%;">, pages 1532–1543, 2014.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
J. Redmon and A. Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Yolov3: An incremental improvement.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text" style="font-size:90%;">2018.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
S. R. Richter, V. Vineet, S. Roth, and V. Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Playing for data: Ground truth from computer games.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib57.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib57.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">The synthia dataset: A large collection of synthetic images for
semantic segmentation of urban scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib58.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib58.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
F. Sadeghi and S. Levine.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">Cad2rl: Real single-image flight without a single real image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib59.4.2" class="ltx_text" style="font-size:90%;">, abs/1611.04201, 2016.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
P. Saeedi, P. D. Lawrence, and D. G. Lowe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">Vision-based 3-d trajectory tracking for unknown environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on robotics</span><span id="bib.bib60.4.2" class="ltx_text" style="font-size:90%;">, 2006.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
R. Sim and J. J. Little.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">Autonomous vision-based exploration and mapping using hybrid maps and
rao-blackwellised particle filters.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib61.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Intelligent Robots and Systems</span><span id="bib.bib61.5.3" class="ltx_text" style="font-size:90%;">.
IEEE, 2006.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:90%;">
S. Srivastava, E. Fang, L. Riano, R. Chitnis, S. Russell, and P. Abbeel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.2.1" class="ltx_text" style="font-size:90%;">Combined task and motion planning through an extensible
planner-independent interface layer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib62.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Robotics and Automation</span><span id="bib.bib62.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text" style="font-size:90%;">
S. Srivastava, L. Riano, S. Russell, and P. Abbeel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.2.1" class="ltx_text" style="font-size:90%;">Using classical planners for tasks with continuous operators in
robotics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib63.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Intl. Conf. on Automated Planning and Scheduling</span><span id="bib.bib63.5.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text" style="font-size:90%;">
R. S. Sutton, D. Precup, and S. P. Singh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.2.1" class="ltx_text" style="font-size:90%;">Between mdps and semi-mdps: A framework for temporal abstraction in
reinforcement learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Artif. Intell.</span><span id="bib.bib64.4.2" class="ltx_text" style="font-size:90%;">, 1999.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text" style="font-size:90%;">
M. Tapaswi, Y. Zhu, R. Stiefelhagen, A. Torralba, R. Urtasun, and S. Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.2.1" class="ltx_text" style="font-size:90%;">Movieqa: Understanding stories in movies through question-answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib65.4.2" class="ltx_text" style="font-size:90%;">, pages 4631–4640, 2016.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text" style="font-size:90%;">
C. Tessler, S. Givony, T. Zahavy, D. J. Mankowitz, and S. Mannor.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.2.1" class="ltx_text" style="font-size:90%;">A deep hierarchical approach to lifelong learning in minecraft.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib66.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib66.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text" style="font-size:90%;">
M. Tomono.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.2.1" class="ltx_text" style="font-size:90%;">3-d object map building using dense object models with sift-based
recognition features.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib67.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Intelligent Robots and Systems</span><span id="bib.bib67.5.3" class="ltx_text" style="font-size:90%;">,
2006.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text" style="font-size:90%;">
P. Wang, Q. Wu, C. Shen, A. van den Hengel, and A. R. Dick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.2.1" class="ltx_text" style="font-size:90%;">Fvqa: Fact-based visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib68.4.2" class="ltx_text" style="font-size:90%;">,
2017.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text" style="font-size:90%;">
D. Wooden.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.2.1" class="ltx_text" style="font-size:90%;">A guide to vision-based map building.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Robotics &amp; Automation Magazine</span><span id="bib.bib69.4.2" class="ltx_text" style="font-size:90%;">, 2006.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text" style="font-size:90%;">
J. Wu, I. Yildirim, J. J. Lim, B. Freeman, and J. Tenenbaum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.2.1" class="ltx_text" style="font-size:90%;">Galileo: Perceiving physical object properties by integrating a
physics engine with deep learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib70.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NIPS</span><span id="bib.bib70.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text" style="font-size:90%;">
Q. Wu, D. Teney, P. Wang, C. Shen, A. R. Dick, and A. van den Hengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.2.1" class="ltx_text" style="font-size:90%;">Visual question answering: A survey of methods and datasets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib71.4.2" class="ltx_text" style="font-size:90%;">, abs/1607.05910, 2016.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text" style="font-size:90%;">
B. Wymann, E. Espié, C. Guionneau, C. Dimitrakakis, R. Coulom, and
A. Sumner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.2.1" class="ltx_text" style="font-size:90%;">Torcs, the open racing car simulator.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Software available at http://torcs. sourceforge. net</span><span id="bib.bib72.4.2" class="ltx_text" style="font-size:90%;">, 2000.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock"><span id="bib.bib73.1.1" class="ltx_text" style="font-size:90%;">
Y. Zhu, D. Gordon, E. Kolve, D. Fox, L. Fei-Fei, A. Gupta, R. Mottaghi, and
A. Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.2.1" class="ltx_text" style="font-size:90%;">Visual semantic planning using deep successor representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib73.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text" style="font-size:90%;">
Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.2.1" class="ltx_text" style="font-size:90%;">Target-driven visual navigation in indoor scenes using deep
reinforcement learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib74.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Robotics and Automation</span><span id="bib.bib74.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1712.03315" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1712.03316" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1712.03316">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1712.03316" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1712.03317" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar 15 23:55:52 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
