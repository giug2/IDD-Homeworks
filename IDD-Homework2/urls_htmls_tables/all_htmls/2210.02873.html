<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2210.02873] Blockchain-based Monitoring for Poison Attack Detection in Decentralized Federated Learning</title><meta property="og:description" content="Federated Learning (FL) is a machine learning technique that addresses the privacy challenges in terms of access rights of local datasets by enabling the training of a model across nodes holding their data samples loca…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Blockchain-based Monitoring for Poison Attack Detection in Decentralized Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Blockchain-based Monitoring for Poison Attack Detection in Decentralized Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2210.02873">

<!--Generated on Thu Mar 14 03:52:28 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
component,  formatting,  style,  styling,  insert
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document"> Blockchain-based Monitoring for Poison Attack Detection in Decentralized Federated Learning
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ranwa Al Mallah
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">Electrical and Computer Engineering</span>
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_italic">Royal Military College of Canada
<br class="ltx_break"></span>Kingston, Canada 
<br class="ltx_break">ranwa.al-mallah@rmc-cmr.ca
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">David López
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_font_italic">Instituto de Ingeniería</span>
<br class="ltx_break"><span id="id4.2.id2" class="ltx_text ltx_font_italic">Universidad Nacional Autónoma de México
<br class="ltx_break"></span>Mexico City, Mexico 
<br class="ltx_break">dlopezfl@iingen.unam.mx
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Federated Learning (FL) is a machine learning technique that addresses the privacy challenges in terms of access rights of local datasets by enabling the training of a model across nodes holding their data samples locally. To achieve decentralized federated learning, blockchain-based FL was proposed as a distributed FL architecture. In decentralized FL, the <span id="id5.id1.1" class="ltx_text ltx_font_italic">chief</span> is eliminated from the learning process as <span id="id5.id1.2" class="ltx_text ltx_font_italic">workers</span> collaborate between each other to train the global model. Decentralized FL applications need to account for the additional delay incurred by blockchain-based FL deployments. Particularly in this setting, to detect targeted/untargeted poisoning attacks, we investigate the end-to-end learning completion latency of a realistic decentralized FL process protected against poisoning attacks. We propose a technique which consists in decoupling the monitoring phase from the detection phase in defenses against poisoning attacks in a decentralized federated learning deployment that aim at monitoring the behavior of the <span id="id5.id1.3" class="ltx_text ltx_font_italic">workers</span>. We demonstrate that our proposed blockchain-based monitoring improved network scalability, robustness and time efficiency. The parallelization of operations results in minimized latency over the end-to-end communication, computation, and consensus delays incurred during the FL and blockchain operations.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
component, formatting, style, styling, insert

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In traditional centralized machine learning, the nodes participating in the training of a model upload their local datasets to a central node <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Distributed machine learning is a system to train independently a model on different nodes. This distributed system accelerates the training for very big amounts of data. However, since data might be sensitive, we need a privacy preserving distributed learning system for collaborative training of distributed machine learning models without any data sharing. Federated learning is a machine learning paradigm that addresses the privacy, security and access rights challenges related to local datasets by training a global model across decentralized nodes and enabling the <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">worker</span> nodes to hold their data samples locally without sharing them <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In centralized federated learning, a central node, the <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">chief</span>, manages the different steps of the learning process that the <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">workers</span> have to go through to train the global model. The <span id="S1.p2.1.3" class="ltx_text ltx_font_italic">chief</span> is mainly responsible for the coordination of the <span id="S1.p2.1.4" class="ltx_text ltx_font_italic">workers</span> and aggregation of the received local model updates. However, in some settings, this strategy may constitute a bottleneck since all the <span id="S1.p2.1.5" class="ltx_text ltx_font_italic">workers</span> have to send their local model updates to the <span id="S1.p2.1.6" class="ltx_text ltx_font_italic">chief</span>, thus affecting the performance of the <span id="S1.p2.1.7" class="ltx_text ltx_font_italic">chief</span>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In decentralized federated learning, the <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">workers</span> collaborate between each other to train the global model. This strategy eliminates the <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">chief</span> from the learning process and prevents the single point of failure that the <span id="S1.p3.1.3" class="ltx_text ltx_font_italic">chief</span> represents in centralized federated learning.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To achieve decentralized federated learning, researchers have developed blockchain-based federated learning as a distributed FL architecture because both the blockchain and federated learning technologies protect the privacy of individuals. The blockchain is a crypto-based secure ledger for data storage and transfer through decentralized, trustless peer-to-peer systems. In our setting, the blockchain network will enable the exchange of the <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">workers</span>’ local model updates while a subset of <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">workers</span>, the miners in the context of a blockchain, are responsible of the aggregation of the received local model updates. The miners can be either randomly selected nodes or separate nodes as in a traditional blockchain network. The <span id="S1.p4.1.3" class="ltx_text ltx_font_italic">workers</span> will send their local model updates to their associated miner in the blockchain network. The miners will exchange between each other all the local model updates in order to proceed with the training of the model. The miners can then complete the first round of the process by running the consensus algorithm. Consensus results in a block to be added to the blockchain and the block stores the global model which is the aggregate of the received local model updates. In the second round of the process, the <span id="S1.p4.1.4" class="ltx_text ltx_font_italic">Workers</span> will then download the global model from the blockchain. The global model serves as an input to the next local model update that the <span id="S1.p4.1.5" class="ltx_text ltx_font_italic">worker</span> will generate in the next iteration of the federated learning.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Decentralized federated learning applications need to account for the additional delay incurred by the blockchain network. Many studies aim at addressing the latency challenges related to decentralized federated learning applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. One application that must consider the end-to-end latency incurred by blockchain-based decentralized federated learning is the detection of targeted/untargeted poisoning attacks. To the best of our knowledge, no previous work has addressed this challenge in this setting. To this aim, we propose a technique to address the latency challenges and the technique consists in the decoupling of the monitoring phase from the detection phase in decentralized FL approaches defenses that protect against poisoning attacks in federated learning.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.2" class="ltx_p">Poisoning attacks in federated learning happen when the attacker is able to inject malicious data into the model during training, and hence alter the learning process. Steinhardt et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> reported that, even under strong defenses, a <math id="S1.p6.1.m1.1" class="ltx_Math" alttext="3\%" display="inline"><semantics id="S1.p6.1.m1.1a"><mrow id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml"><mn id="S1.p6.1.m1.1.1.2" xref="S1.p6.1.m1.1.1.2.cmml">3</mn><mo id="S1.p6.1.m1.1.1.1" xref="S1.p6.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><apply id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1"><csymbol cd="latexml" id="S1.p6.1.m1.1.1.1.cmml" xref="S1.p6.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S1.p6.1.m1.1.1.2.cmml" xref="S1.p6.1.m1.1.1.2">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">3\%</annotation></semantics></math> training dataset poisoning leads to <math id="S1.p6.2.m2.1" class="ltx_Math" alttext="11\%" display="inline"><semantics id="S1.p6.2.m2.1a"><mrow id="S1.p6.2.m2.1.1" xref="S1.p6.2.m2.1.1.cmml"><mn id="S1.p6.2.m2.1.1.2" xref="S1.p6.2.m2.1.1.2.cmml">11</mn><mo id="S1.p6.2.m2.1.1.1" xref="S1.p6.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p6.2.m2.1b"><apply id="S1.p6.2.m2.1.1.cmml" xref="S1.p6.2.m2.1.1"><csymbol cd="latexml" id="S1.p6.2.m2.1.1.1.cmml" xref="S1.p6.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S1.p6.2.m2.1.1.2.cmml" xref="S1.p6.2.m2.1.1.2">11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.2.m2.1c">11\%</annotation></semantics></math> drop in accuracy. Regarding the cybersecurity of FL and protection against this type of attack, many defenses were proposed for the centralized federated learning setting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Lately, detection and behavioral pattern analysis as a defense mechanisms are gaining momentum. The aim of this type of defense is to remove at every iteration of the FL process, the unreliable nodes in the system based on the assessment of their behavior <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. By monitoring the behavior in time of the <span id="S1.p6.2.1" class="ltx_text ltx_font_italic">workers</span> and removing unreliable nodes from the aggregation process, the approaches enable efficiency and security of the centralized FL process.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">No studies have yet addressed the additional delay incurred by the detection of targeted/untargeted poisoning attacks techniques in a decentralized federated learning setting. In fact, network topologies affect the performances of the learning process in distributed FL architectures. We propose a technique where monitoring and detection is done in parallel by the blockchain that returns a filtered reliable set of <span id="S1.p7.1.1" class="ltx_text ltx_font_italic">workers</span> from which miners in the decentralized federated learning environment can randomly pick a subset to continue the FL process. We improved security, scalabilty and time efficiency because the monitoring is decoupled from the FL and distributed. The technique shows great levels of time efficiency because it leverages, on one hand, the underlying blockchain as an immutable security monitoring system for the <span id="S1.p7.1.2" class="ltx_text ltx_font_italic">workers</span>, and on the other hand the latency is minimized in the detection phase because verification is done separately of the decentralized federated learning process.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">The rest of the paper is organised as follows. In Section <a href="#S2" title="II Related Work ‣ Blockchain-based Monitoring for Poison Attack Detection in Decentralized Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> we present related work. In Section <a href="#S3" title="III Blockchain-based Monitoring ‣ Blockchain-based Monitoring for Poison Attack Detection in Decentralized Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> we present the blockchain-based monitoring. We provide experimental results of the poisoning attacks and defense on a mode inference model implemented over the blockchain as a case study in Section <a href="#S4" title="IV Evaluation ‣ Blockchain-based Monitoring for Poison Attack Detection in Decentralized Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>. We conclude the paper and provide future work in Section <a href="#S5" title="V Conclusion ‣ Blockchain-based Monitoring for Poison Attack Detection in Decentralized Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In federated learning, a star network where a central server (<span id="S2.p1.1.1" class="ltx_text ltx_font_italic">chief</span>) is connected to a network of nodes, is the predominant communication topology. However, decentralized topologies (where nodes communicate with their neighbors) are a potential alternative. In data center environments and under a particular setting, decentralized training has been demonstrated to be faster than centralized training when operating on networks with low bandwidth or high latency. Similarly, in federated learning, decentralized algorithms can in theory reduce the high communication cost on the central server. Hierarchical communication patterns have also been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> to further ease the burden on the <span id="S2.p1.1.2" class="ltx_text ltx_font_italic">chief</span>, by first leveraging edge servers to aggregate the updates from edge nodes and then relying on a cloud server to aggregate updates from edge servers. While this is a promising approach to reduce communication, it is not applicable to all networks, as this type of physical hierarchy may not exist or be known a priori.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">A blockchain can be used to replace the <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">chief</span>, the centralized aggregator in the traditional FL system. Miners calculate the averaged model using received update models from <span id="S2.p2.1.2" class="ltx_text ltx_font_italic">workers</span>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">As an example application, López et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> presented a Blockchain for Smart Mobility Data-markets for mobility data transactions designed to solve the privacy, security, and management issues related to the sharing of passively or actively solicited large-scale data. They developed a federated learning environment over that blockchain to create a privacy-aware solution for mode inference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. They show that nodes that collectively, but privately train a Convolutional Neural Network (CNN) can achieve the same accuracy as the conventional centralized training.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Preuveneers et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> proposed a decentralized federated learning environment where model updates are stored on the distributed ledger. This approach exudes large computation overload and latencies. Also, their solution only guarantee the immutability and verify the integrity of the stored data but cannot assure its veracity. There is no defense mechanism implemented as a security measure to protect against poisoning attacks in this setting. Unlike their approach, our technique is applied on decentralized federated learning processes where the defense is integrated in the training of the algorithm in order to eliminate malicious <span id="S2.p4.1.1" class="ltx_text ltx_font_italic">workers</span> that compromise the learning. Qu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> used the blockchain to enable FL without any centralized authority but they provided incentives to the <span id="S2.p4.1.2" class="ltx_text ltx_font_italic">workers</span>, which may result in biased training of the models as only a particular type of <span id="S2.p4.1.3" class="ltx_text ltx_font_italic">worker</span> might be interested in the process. In their work, to guarantee block generation efficiency, pointers of the averaged model are saved on-chain while a distributed hash table is used to save the data and point to an off-chain data storage. Again, they do not consider a defense mechanism implemented to secure the undergoing FL process.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Zhao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> introduced the concept of reputation and reputation status is recorded in the blockchain. However, to verify the validity of a model update, they only check if the signature is invalid, the miner rejects the transaction. They do not study the quality of the uploaded model updates and do not defend against poisoning attacks. Kang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> also use the reputation to reflect how well a <span id="S2.p5.1.1" class="ltx_text ltx_font_italic">worker</span> has performed about model training, which is measured from its training task completion history with the past behaviors of good or unreliable activities. To remove malicious updates, they use the FoolsGold scheme <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. FoolsGold is a defense against targeted poisoning attacks based on inter-client contribution similarity in their model updates.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Some defense mechanisms consider the behavior in time of the <span id="S2.p6.1.1" class="ltx_text ltx_font_italic">workers</span> during training in order to detect an anomaly and reject poisonous model updates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. With a temporal and dynamic monitoring method, the <span id="S2.p6.1.2" class="ltx_text ltx_font_italic">chief</span> can detect and remove malicious or unreliable <span id="S2.p6.1.3" class="ltx_text ltx_font_italic">workers</span> from the system. In this context, the blockchain is used to monitor the training of individual <span id="S2.p6.1.4" class="ltx_text ltx_font_italic">workers</span> and at the same time, detect a malicious <span id="S2.p6.1.5" class="ltx_text ltx_font_italic">worker</span>, thus, ensuring that transactions stored on the blockchain are valid. The inclusion of the blockchain in this setting aims at improving robustness of the system against the attacks. However, this will affect the performance of the learning process in terms of latency. Some studies propose a latency analysis of federated learning via blockchain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. However, the approaches are not applicable in the context of a cybersecurity layer implemented on top of the decentralized federated learning process occurring between the nodes. When a detection mechanism is integrated to protect against poisoning attacks in a decentralized FL process, and particularly when the defense requires monitoring of the behavior of the <span id="S2.p6.1.6" class="ltx_text ltx_font_italic">workers</span>, to account for the additional latency generated, we propose a technique where monitoring is decoupled from detection.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Blockchain-based Monitoring</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We use the blockchain to develop an immutable infrastructure for the decentralized federated learning process of a model and for the monitoring of the <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">workers</span> to detect poisoning attacks. In order to train the shared model efficiently, the security layer added to the blockchain should not constitute a bottleneck by affecting the time taken for model training. In fact, we show in Figure <a href="#S3.F1" title="Figure 1 ‣ III Blockchain-based Monitoring ‣ Blockchain-based Monitoring for Poison Attack Detection in Decentralized Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> the design we put forward that results in high levels of time efficiency. Some miners of the blockchain network are attributed to the FL process and they are called, <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">minersFL</span> and others, <span id="S3.p1.1.3" class="ltx_text ltx_font_italic">minersMON</span>, are attributed to the monitoring. When training starts at <span id="S3.p1.1.4" class="ltx_text ltx_font_italic">t</span>=0, <span id="S3.p1.1.5" class="ltx_text ltx_font_italic">workers</span> in the distributed environment perform the federated learning process as usual. They extract from the green block that was already in the blockchain, the randomly initialized model that they will use to train with their local data. In the initial stage, before detection and defense kicks in, all <span id="S3.p1.1.6" class="ltx_text ltx_font_italic">workers</span> continuously send, iteration after iteration, their local model updates to the subset of miners called <span id="S3.p1.1.7" class="ltx_text ltx_font_italic">minersMON</span>. It’s only after some period of time <span id="S3.p1.1.8" class="ltx_text ltx_font_italic">t ¡ x</span> that the blockchain-based monitoring algorithm is able to return a filtered reliable set of <span id="S3.p1.1.9" class="ltx_text ltx_font_italic">workers</span> from which <span id="S3.p1.1.10" class="ltx_text ltx_font_italic">minersFL</span> nodes can randomly pick a subset to continue the FL process. It is at time <span id="S3.p1.1.11" class="ltx_text ltx_font_italic">t = x</span> precisely that the system becomes protected against attackers although since the beginning, monitoring was performed. Afterwards, iteration after iteration, only reliable nodes will be selected from the pool of nodes to participate in the training. Thus, monitoring and detection are done separately from training, but they must be in synchronisation with each other so as to always provide fresh and accurate evaluation of the nodes.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2210.02873/assets/Parallelprocess.png" id="S3.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="651" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Schema of the blockchain-based monitoring, detection and training process.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">It is important to note that in our design, the model updates are not written on the blockchain, instead, a hash value inside the public ledger points towards them. The model updates remain on the <span id="S3.p2.1.1" class="ltx_text ltx_font_italic">worker</span> side encrypted with keys that no other nodes have access to. Digital signatures are required for information to be stored on the blockchain. Therefore, we hold that attackers are not able to fabricate digital signatures or take control of the majority of the network (over 50%) in order to modify valid blocks on the blockchain. Furthermore, an attacker cannot poison the training data samples because they are stored off-chain on the nodes rather than on the public ledger. However, if the attacker gains access and controls one or some of the nodes, it can send model updates on their behalf that are maliciously fabricated to compromise the system. We detail the blockchain-based monitoring in Algorithm <a href="#alg1" title="In III Blockchain-based Monitoring ‣ Blockchain-based Monitoring for Poison Attack Detection in Decentralized Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="alg1" class="ltx_float ltx_algorithm">
<div id="alg1.8" class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div id="alg1.3.3" class="ltx_listingline">
<span id="alg1.3.3.1" class="ltx_text"><span id="alg1.3.3.1.1" class="ltx_text ltx_font_bold">Result:</span> </span><math id="alg1.1.1.m1.2" class="ltx_Math" alttext="H_{i,z}" display="inline"><semantics id="alg1.1.1.m1.2a"><msub id="alg1.1.1.m1.2.3" xref="alg1.1.1.m1.2.3.cmml"><mi id="alg1.1.1.m1.2.3.2" xref="alg1.1.1.m1.2.3.2.cmml">H</mi><mrow id="alg1.1.1.m1.2.2.2.4" xref="alg1.1.1.m1.2.2.2.3.cmml"><mi id="alg1.1.1.m1.1.1.1.1" xref="alg1.1.1.m1.1.1.1.1.cmml">i</mi><mo id="alg1.1.1.m1.2.2.2.4.1" xref="alg1.1.1.m1.2.2.2.3.cmml">,</mo><mi id="alg1.1.1.m1.2.2.2.2" xref="alg1.1.1.m1.2.2.2.2.cmml">z</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="alg1.1.1.m1.2b"><apply id="alg1.1.1.m1.2.3.cmml" xref="alg1.1.1.m1.2.3"><csymbol cd="ambiguous" id="alg1.1.1.m1.2.3.1.cmml" xref="alg1.1.1.m1.2.3">subscript</csymbol><ci id="alg1.1.1.m1.2.3.2.cmml" xref="alg1.1.1.m1.2.3.2">𝐻</ci><list id="alg1.1.1.m1.2.2.2.3.cmml" xref="alg1.1.1.m1.2.2.2.4"><ci id="alg1.1.1.m1.1.1.1.1.cmml" xref="alg1.1.1.m1.1.1.1.1">𝑖</ci><ci id="alg1.1.1.m1.2.2.2.2.cmml" xref="alg1.1.1.m1.2.2.2.2">𝑧</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.1.1.m1.2c">H_{i,z}</annotation></semantics></math> a subset <span id="alg1.3.3.2" class="ltx_text ltx_font_italic">z</span> of the <span id="alg1.3.3.3" class="ltx_text ltx_font_italic">worker</span> <span id="alg1.3.3.4" class="ltx_text ltx_font_italic">i</span> previously uploaded consecutive Local Model update recorded as a pair of <math id="alg1.2.2.m2.1" class="ltx_Math" alttext="LM" display="inline"><semantics id="alg1.2.2.m2.1a"><mrow id="alg1.2.2.m2.1.1" xref="alg1.2.2.m2.1.1.cmml"><mi id="alg1.2.2.m2.1.1.2" xref="alg1.2.2.m2.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="alg1.2.2.m2.1.1.1" xref="alg1.2.2.m2.1.1.1.cmml">​</mo><mi id="alg1.2.2.m2.1.1.3" xref="alg1.2.2.m2.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.2.2.m2.1b"><apply id="alg1.2.2.m2.1.1.cmml" xref="alg1.2.2.m2.1.1"><times id="alg1.2.2.m2.1.1.1.cmml" xref="alg1.2.2.m2.1.1.1"></times><ci id="alg1.2.2.m2.1.1.2.cmml" xref="alg1.2.2.m2.1.1.2">𝐿</ci><ci id="alg1.2.2.m2.1.1.3.cmml" xref="alg1.2.2.m2.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.2.2.m2.1c">LM</annotation></semantics></math> and <math id="alg1.3.3.m3.1" class="ltx_Math" alttext="GM" display="inline"><semantics id="alg1.3.3.m3.1a"><mrow id="alg1.3.3.m3.1.1" xref="alg1.3.3.m3.1.1.cmml"><mi id="alg1.3.3.m3.1.1.2" xref="alg1.3.3.m3.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="alg1.3.3.m3.1.1.1" xref="alg1.3.3.m3.1.1.1.cmml">​</mo><mi id="alg1.3.3.m3.1.1.3" xref="alg1.3.3.m3.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.3.3.m3.1b"><apply id="alg1.3.3.m3.1.1.cmml" xref="alg1.3.3.m3.1.1"><times id="alg1.3.3.m3.1.1.1.cmml" xref="alg1.3.3.m3.1.1.1"></times><ci id="alg1.3.3.m3.1.1.2.cmml" xref="alg1.3.3.m3.1.1.2">𝐺</ci><ci id="alg1.3.3.m3.1.1.3.cmml" xref="alg1.3.3.m3.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.3.3.m3.1c">GM</annotation></semantics></math> at that time;
</div>
<div id="alg1.4.4" class="ltx_listingline">
<span id="alg1.4.4.1" class="ltx_text ltx_font_bold">Input:</span> Global Model <math id="alg1.4.4.m1.1" class="ltx_Math" alttext="GM^{t}" display="inline"><semantics id="alg1.4.4.m1.1a"><mrow id="alg1.4.4.m1.1.1" xref="alg1.4.4.m1.1.1.cmml"><mi id="alg1.4.4.m1.1.1.2" xref="alg1.4.4.m1.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="alg1.4.4.m1.1.1.1" xref="alg1.4.4.m1.1.1.1.cmml">​</mo><msup id="alg1.4.4.m1.1.1.3" xref="alg1.4.4.m1.1.1.3.cmml"><mi id="alg1.4.4.m1.1.1.3.2" xref="alg1.4.4.m1.1.1.3.2.cmml">M</mi><mi id="alg1.4.4.m1.1.1.3.3" xref="alg1.4.4.m1.1.1.3.3.cmml">t</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="alg1.4.4.m1.1b"><apply id="alg1.4.4.m1.1.1.cmml" xref="alg1.4.4.m1.1.1"><times id="alg1.4.4.m1.1.1.1.cmml" xref="alg1.4.4.m1.1.1.1"></times><ci id="alg1.4.4.m1.1.1.2.cmml" xref="alg1.4.4.m1.1.1.2">𝐺</ci><apply id="alg1.4.4.m1.1.1.3.cmml" xref="alg1.4.4.m1.1.1.3"><csymbol cd="ambiguous" id="alg1.4.4.m1.1.1.3.1.cmml" xref="alg1.4.4.m1.1.1.3">superscript</csymbol><ci id="alg1.4.4.m1.1.1.3.2.cmml" xref="alg1.4.4.m1.1.1.3.2">𝑀</ci><ci id="alg1.4.4.m1.1.1.3.3.cmml" xref="alg1.4.4.m1.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.4.4.m1.1c">GM^{t}</annotation></semantics></math> at iteration <span id="alg1.4.4.2" class="ltx_text ltx_font_italic">t</span>;
</div>
<div id="alg1.8.9" class="ltx_listingline">
<span id="alg1.8.9.1" class="ltx_text ltx_font_bold">for</span> <em id="alg1.8.9.2" class="ltx_emph ltx_font_italic">Iteration t</em> <span id="alg1.8.9.3" class="ltx_text ltx_font_bold">do</span> 
</div>
<div id="alg1.5.5" class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<span id="alg1.5.5.1" class="ltx_text ltx_font_italic">minersFL</span> select a subset of <span id="alg1.5.5.2" class="ltx_text ltx_font_italic">workers</span> and sends them <math id="alg1.5.5.m1.1" class="ltx_Math" alttext="GM^{t}" display="inline"><semantics id="alg1.5.5.m1.1a"><mrow id="alg1.5.5.m1.1.1" xref="alg1.5.5.m1.1.1.cmml"><mi id="alg1.5.5.m1.1.1.2" xref="alg1.5.5.m1.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="alg1.5.5.m1.1.1.1" xref="alg1.5.5.m1.1.1.1.cmml">​</mo><msup id="alg1.5.5.m1.1.1.3" xref="alg1.5.5.m1.1.1.3.cmml"><mi id="alg1.5.5.m1.1.1.3.2" xref="alg1.5.5.m1.1.1.3.2.cmml">M</mi><mi id="alg1.5.5.m1.1.1.3.3" xref="alg1.5.5.m1.1.1.3.3.cmml">t</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="alg1.5.5.m1.1b"><apply id="alg1.5.5.m1.1.1.cmml" xref="alg1.5.5.m1.1.1"><times id="alg1.5.5.m1.1.1.1.cmml" xref="alg1.5.5.m1.1.1.1"></times><ci id="alg1.5.5.m1.1.1.2.cmml" xref="alg1.5.5.m1.1.1.2">𝐺</ci><apply id="alg1.5.5.m1.1.1.3.cmml" xref="alg1.5.5.m1.1.1.3"><csymbol cd="ambiguous" id="alg1.5.5.m1.1.1.3.1.cmml" xref="alg1.5.5.m1.1.1.3">superscript</csymbol><ci id="alg1.5.5.m1.1.1.3.2.cmml" xref="alg1.5.5.m1.1.1.3.2">𝑀</ci><ci id="alg1.5.5.m1.1.1.3.3.cmml" xref="alg1.5.5.m1.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.5.5.m1.1c">GM^{t}</annotation></semantics></math>;
</div>
<div id="alg1.6.6" class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<span id="alg1.6.6.1" class="ltx_text ltx_font_italic">workers</span> train, encrypt their <math id="alg1.6.6.m1.1" class="ltx_Math" alttext="LM^{t+1}" display="inline"><semantics id="alg1.6.6.m1.1a"><mrow id="alg1.6.6.m1.1.1" xref="alg1.6.6.m1.1.1.cmml"><mi id="alg1.6.6.m1.1.1.2" xref="alg1.6.6.m1.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="alg1.6.6.m1.1.1.1" xref="alg1.6.6.m1.1.1.1.cmml">​</mo><msup id="alg1.6.6.m1.1.1.3" xref="alg1.6.6.m1.1.1.3.cmml"><mi id="alg1.6.6.m1.1.1.3.2" xref="alg1.6.6.m1.1.1.3.2.cmml">M</mi><mrow id="alg1.6.6.m1.1.1.3.3" xref="alg1.6.6.m1.1.1.3.3.cmml"><mi id="alg1.6.6.m1.1.1.3.3.2" xref="alg1.6.6.m1.1.1.3.3.2.cmml">t</mi><mo id="alg1.6.6.m1.1.1.3.3.1" xref="alg1.6.6.m1.1.1.3.3.1.cmml">+</mo><mn id="alg1.6.6.m1.1.1.3.3.3" xref="alg1.6.6.m1.1.1.3.3.3.cmml">1</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="alg1.6.6.m1.1b"><apply id="alg1.6.6.m1.1.1.cmml" xref="alg1.6.6.m1.1.1"><times id="alg1.6.6.m1.1.1.1.cmml" xref="alg1.6.6.m1.1.1.1"></times><ci id="alg1.6.6.m1.1.1.2.cmml" xref="alg1.6.6.m1.1.1.2">𝐿</ci><apply id="alg1.6.6.m1.1.1.3.cmml" xref="alg1.6.6.m1.1.1.3"><csymbol cd="ambiguous" id="alg1.6.6.m1.1.1.3.1.cmml" xref="alg1.6.6.m1.1.1.3">superscript</csymbol><ci id="alg1.6.6.m1.1.1.3.2.cmml" xref="alg1.6.6.m1.1.1.3.2">𝑀</ci><apply id="alg1.6.6.m1.1.1.3.3.cmml" xref="alg1.6.6.m1.1.1.3.3"><plus id="alg1.6.6.m1.1.1.3.3.1.cmml" xref="alg1.6.6.m1.1.1.3.3.1"></plus><ci id="alg1.6.6.m1.1.1.3.3.2.cmml" xref="alg1.6.6.m1.1.1.3.3.2">𝑡</ci><cn type="integer" id="alg1.6.6.m1.1.1.3.3.3.cmml" xref="alg1.6.6.m1.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.6.6.m1.1c">LM^{t+1}</annotation></semantics></math>, perform hash and insert it in Merkle tree on the node;
</div>
<div id="alg1.8.10" class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<span id="alg1.8.10.1" class="ltx_text ltx_font_italic">workers</span> sign Merkle root and transmit it to the <span id="alg1.8.10.2" class="ltx_text ltx_font_italic">minersMON</span> nodes of the blockchain;
</div>
<div id="alg1.8.11" class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<span id="alg1.8.11.1" class="ltx_text ltx_font_italic">minersMON</span> verify identities before storing signed Merkle root on the blockchain;
</div>
<div id="alg1.7.7" class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<span id="alg1.7.7.2" class="ltx_text ltx_font_bold">for</span> <em id="alg1.7.7.1" class="ltx_emph ltx_font_italic"><math id="alg1.7.7.1.m1.1" class="ltx_Math" alttext="\forall" display="inline"><semantics id="alg1.7.7.1.m1.1a"><mo id="alg1.7.7.1.m1.1.1" xref="alg1.7.7.1.m1.1.1.cmml">∀</mo><annotation-xml encoding="MathML-Content" id="alg1.7.7.1.m1.1b"><csymbol cd="latexml" id="alg1.7.7.1.m1.1.1.cmml" xref="alg1.7.7.1.m1.1.1">for-all</csymbol></annotation-xml><annotation encoding="application/x-tex" id="alg1.7.7.1.m1.1c">\forall</annotation></semantics></math> workers i</em> <span id="alg1.7.7.3" class="ltx_text ltx_font_bold">do</span> 
</div>
<div id="alg1.8.12" class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<span id="alg1.8.12.1" class="ltx_text ltx_font_italic">minersMON</span> extract Merkle root and select a random time window to examine the behavior of the <span id="alg1.8.12.2" class="ltx_text ltx_font_italic">workers</span>;
</div>
<div id="alg1.8.13" class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<span id="alg1.8.13.1" class="ltx_text ltx_font_italic">workers</span> send to the <span id="alg1.8.13.2" class="ltx_text ltx_font_italic">minersMON</span> the required model updates and their corresponding Merkle path as a proof for examination;
</div>
<div id="alg1.8.14" class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<span id="alg1.8.14.1" class="ltx_text ltx_font_bold">if</span> <em id="alg1.8.14.2" class="ltx_emph ltx_font_italic">Hashes are valid</em> <span id="alg1.8.14.3" class="ltx_text ltx_font_bold">then</span> 
</div>
<div id="alg1.8.8" class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<span id="alg1.8.8.1" class="ltx_text ltx_font_italic">minersMON</span> compute <math id="alg1.8.8.m1.2" class="ltx_Math" alttext="H_{i,z}" display="inline"><semantics id="alg1.8.8.m1.2a"><msub id="alg1.8.8.m1.2.3" xref="alg1.8.8.m1.2.3.cmml"><mi id="alg1.8.8.m1.2.3.2" xref="alg1.8.8.m1.2.3.2.cmml">H</mi><mrow id="alg1.8.8.m1.2.2.2.4" xref="alg1.8.8.m1.2.2.2.3.cmml"><mi id="alg1.8.8.m1.1.1.1.1" xref="alg1.8.8.m1.1.1.1.1.cmml">i</mi><mo id="alg1.8.8.m1.2.2.2.4.1" xref="alg1.8.8.m1.2.2.2.3.cmml">,</mo><mi id="alg1.8.8.m1.2.2.2.2" xref="alg1.8.8.m1.2.2.2.2.cmml">z</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="alg1.8.8.m1.2b"><apply id="alg1.8.8.m1.2.3.cmml" xref="alg1.8.8.m1.2.3"><csymbol cd="ambiguous" id="alg1.8.8.m1.2.3.1.cmml" xref="alg1.8.8.m1.2.3">subscript</csymbol><ci id="alg1.8.8.m1.2.3.2.cmml" xref="alg1.8.8.m1.2.3.2">𝐻</ci><list id="alg1.8.8.m1.2.2.2.3.cmml" xref="alg1.8.8.m1.2.2.2.4"><ci id="alg1.8.8.m1.1.1.1.1.cmml" xref="alg1.8.8.m1.1.1.1.1">𝑖</ci><ci id="alg1.8.8.m1.2.2.2.2.cmml" xref="alg1.8.8.m1.2.2.2.2">𝑧</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.8.8.m1.2c">H_{i,z}</annotation></semantics></math>;
</div>
<div id="alg1.8.15" class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<span id="alg1.8.15.1" class="ltx_text ltx_font_italic">minersMON</span> return to <span id="alg1.8.15.2" class="ltx_text ltx_font_italic">minersFL</span> a filtered reliable set of <span id="alg1.8.15.3" class="ltx_text ltx_font_italic">workers</span>;
</div>
<div id="alg1.8.16" class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<span id="alg1.8.16.1" class="ltx_text ltx_font_italic">minersFL</span> randomly pick a subset of nodes from the filtered reliable set to perform the aggregation step of the FL process;
</div>
<div id="alg1.8.17" class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   

</div>
<div id="alg1.8.18" class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    end if
</div>
<div id="alg1.8.19" class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
</div>
<div id="alg1.8.20" class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    end for
</div>
<div id="alg1.8.21" class="ltx_listingline">  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
</div>
<div id="alg1.8.22" class="ltx_listingline"> end for
</div>
<div id="alg1.8.23" class="ltx_listingline">

</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.10.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span>Blockchain-based monitoring</figcaption>
</figure>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">The aim is to store on the blockchain, a commitment on behalf of the <span id="S3.p3.1.1" class="ltx_text ltx_font_italic">worker</span> on a series of model updates it had worked on. The order in time of the model updates is important. We will use a Merkle Tree because the detection algorithm needs to go back to previous model updates of a <span id="S3.p3.1.2" class="ltx_text ltx_font_italic">worker</span> to validate a benign behavior <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. The algorithm might want to selectively look at a portion in time of the submitted model updates without requiring the <span id="S3.p3.1.3" class="ltx_text ltx_font_italic">worker</span> to send all the models updates it had since the beginning of training. At the same time, this forces the <span id="S3.p3.1.4" class="ltx_text ltx_font_italic">worker</span> to commit to all the model updates it ever did without knowing which portion is going to be evaluated by the miners of the blockchain network. Most importantly, the <span id="S3.p3.1.5" class="ltx_text ltx_font_italic">worker</span> will never be able to go back and modify an entry because the linked timestampting of the hash chain in the blockchain is a commitment to every previous value. Also, the <span id="S3.p3.1.6" class="ltx_text ltx_font_italic">worker</span> will not be able to reorder previous entries because of the binding commitment property to all the messages. Using a Merkle Tree is efficient both because it enables to store on the blockchain, a Merkle root of only 256 bits and enables to selectively reveal a portion of model updates. The Merkle root is a commitment to the Merkle Tree of the entire set of a node’s model updates. For the miners to open the commitment to a single model update, all it requires is its Merkle path.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">We investigated different designs. For example, a hash function, SHA256 takes a variable input size, but always returns a 256-bit hash value. The hash of one model update is 256 bits. However, at each round, storing on the blockchain the hash of every previous model update is linear in input size. Another design would be to store the hash of the concatenation of all previous model updates. This would be constant in size no matter the input. However, in this case, in order to verify the validity of one model update, all other concatenated model updates must be provided. For this reason, using a Merkle Tree is more efficient than the concatenation because it enables to store on the blockchain, a Merkle root of 256 bits and enables to selectively reveal a portion of model updates. We show in Figure <a href="#S3.F2" title="Figure 2 ‣ III Blockchain-based Monitoring ‣ Blockchain-based Monitoring for Poison Attack Detection in Decentralized Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> a prototype of the Merkle Tree stored at the <span id="S3.p4.1.1" class="ltx_text ltx_font_italic">worker</span> node with the Merkle root being the binding commitment to the entire model updates of the node during training. Merkle trees allow efficient and secure verification of the contents of large data structures. This design achieves scalability and most importantly, enables storage of valuable data in contrast to approaches that store reputation metrics. Our design stores the underling temporal and dynamic local models updates of every <span id="S3.p4.1.2" class="ltx_text ltx_font_italic">worker</span> of the system in a transparent and secure way. The monitoring and the training are done in parallel by different miners of the blockchain network.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2210.02873/assets/Merkle_LM.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="329" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Local Model updates stored in a Merkle tree.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Evaluation</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">To evaluate the efficiency of our technique in addressing the latency challenges, we first implement a blockchain-based decentralized FL process consisting in training a Convolutional Neural Network classifier for transportation mode inference as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. We then add the security layer for protection against poisoning attacks under this setting. Any defense that aim at monitoring the behavior of the <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">workers</span> can be implemented. We perform a poisoning attack on the system and compute the time taken by the FL process to converge. We then implement our technique consisting in the decoupling of the monitoring and detection phases. We compare the latency incurred over the end-to-end communication, computation, and consensus delays incurred during the FL and blockchain operations. We demonstrate that the parallelization of operations results in minimized latency.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Case study</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The dataset of this study consists of raw personal data of nodes participating in the task of distributed behavioural choice modelling over the blockchain. As in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, each observation is characterized with trip duration, trip reliability and trip cost. For testing the distributed choice model, we use a subset of 246 observations, only including automobile and train as the two mode choices to explore how choice modelling can be distributed over a blockchain. In this context, the nodes are always in control of their data because they don’t share their raw personal information with anyone. Although this approach ensures the protection of the individual’s privacy, it exposes the system to poisoning attacks. In fact, malicious nodes can inject maliciously fabricated local model updates to sabotage the choice modeling training that is ongoing. This will poison the learning process.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Experimental setup</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The experiments are implemented on four nodes that run on <em id="S4.SS2.p1.1.1" class="ltx_emph ltx_font_italic">Hyperledger Iroha</em>. The nodes are responsible of storing the blockchain and some of them are in charge of different operations as miners participating in different consensus mechanisms. Each node is running on an <em id="S4.SS2.p1.1.2" class="ltx_emph ltx_font_italic">Amazon EC2 t2.medium Virtual Machine</em>. One <em id="S4.SS2.p1.1.3" class="ltx_emph ltx_font_italic">Amazon EC2 t3.2xlarge Virtual Machine</em> is used to run the 10 nodes who are participating in the FL process. Some nodes are designated as <span id="S4.SS2.p1.1.4" class="ltx_text ltx_font_italic">minersFL</span> and others as <span id="S4.SS2.p1.1.5" class="ltx_text ltx_font_italic">minersMON</span>, while all can also behave as <span id="S4.SS2.p1.1.6" class="ltx_text ltx_font_italic">workers</span> in the decentralized setting.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Experimental Results</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We show in Figure <a href="#S4.F3" title="Figure 3 ‣ IV-C Experimental Results ‣ IV Evaluation ‣ Blockchain-based Monitoring for Poison Attack Detection in Decentralized Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> the convergence time taken by the blockchain-based decentralized FL process for the training of a classifier for transportation mode inference under normal conditions (under no attack). We then implement a poisoning attack where one attacker performs a continuous untargeted attack after EPOCH <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mn id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><cn type="integer" id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">30</annotation></semantics></math> by injecting random weight updates aiming at decreasing convergence speed and compromising the system. We compare in the same figure the time taken by the FL process to converge under attack. We notice that as the number of iterations was increasing under attack, the system was never able to converge.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">We then implement our technique consisting in the decoupling of the monitoring and detection phases as a defense aiming at monitoring the behavior of the <span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_italic">workers</span> in the system as per Algorithm 1. In the same figure, we present the results in terms of latency incurred. Compared to the scenario under attack, we notice that the system was able to converge and we see how the end-to-end delay decreased. The parallelization of operations resulted in minimized latency because instead of a single node acting as a <span id="S4.SS3.p2.1.2" class="ltx_text ltx_font_italic">chief</span> having to go through operations one by one: first monitoring the <span id="S4.SS3.p2.1.3" class="ltx_text ltx_font_italic">workers</span>, then detecting the malicious ones and finally selecting the reliable nodes for training, our technique separates the tasks. The separation of the tasks for some nodes to conducts monitoring and others to conduct training ensures that the miners that are in charge of the training will always have, at every iteration, a reliable set of nodes to select from. Those miners are not concerned of monitoring the behaviour of the nodes because other miners were in charge of doing the monitoring. So they won’t waist time and can automatically select reliable nodes to continue training the model. Meanwhile, the other miners in charge of monitoring continue iteration after iteration to assess the behavior of the <span id="S4.SS3.p2.1.4" class="ltx_text ltx_font_italic">workers</span> and build the pool of reliable nodes. This cooperation between the nodes of the blockchain network permits the implementation of a defense mechanism in an efficient way. Otherwise, the defense would have been too computationally costly in terms of latency and bandwidth.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2210.02873/assets/Time_FL_one.png" id="S4.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="354" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Latency in terms of convergence time of a decentralized FL process for the training of a classifier for transportation mode inference: under no attack, under attack: one attacker, parallelization defense technique.</figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Furthermore, in order to demonstrate the impact of more than one attacker on the latency of the system, we present in Figure <a href="#S4.F4" title="Figure 4 ‣ IV-C Experimental Results ‣ IV Evaluation ‣ Blockchain-based Monitoring for Poison Attack Detection in Decentralized Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> the convergence time of the system when two attackers are trying to sabotage the training of the model. We notice that our technique performs equally in both scenarios and is agnostic of the number of attackers. On the other hand, as the number of attackers increase, the performance of the miners in the second scenario is slightly more impacted by the poisoning attack and by the number of operations required to complete the process of FL and its cybersecurity.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2210.02873/assets/Time_FL_two.png" id="S4.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="354" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Latency in terms of convergence time of a decentralized FL process for the training of a classifier for transportation mode inference: two attackers.</figcaption>
</figure>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">In terms of scalability, since our technique consists in the decoupling of the monitoring (with <span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_italic">minersMON</span>) and detection (with <span id="S4.SS3.p4.1.2" class="ltx_text ltx_font_italic">minersFL</span>) phases, we compare in Figure <a href="#S4.F5" title="Figure 5 ‣ IV-C Experimental Results ‣ IV Evaluation ‣ Blockchain-based Monitoring for Poison Attack Detection in Decentralized Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> the time taken by the operations in each phase and that for different network sizes (3, 6 and 9 <span id="S4.SS3.p4.1.3" class="ltx_text ltx_font_italic">workers</span>). When comparing the overall communication, computation, and consensus delays incurred by the nodes <span id="S4.SS3.p4.1.4" class="ltx_text ltx_font_italic">minersMON</span> with those of <span id="S4.SS3.p4.1.5" class="ltx_text ltx_font_italic">minersFL</span>, we notice that even if <span id="S4.SS3.p4.1.6" class="ltx_text ltx_font_italic">minersMON</span> are impacted by the increase in the number of <span id="S4.SS3.p4.1.7" class="ltx_text ltx_font_italic">workers</span> in the system, this remains transparent to <span id="S4.SS3.p4.1.8" class="ltx_text ltx_font_italic">minersFL</span> in the agregation or detection phase since the operations are done in parallel, thus resulting in minimized overall latency.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2210.02873/assets/Time_FL_HISTOGRAM.png" id="S4.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="301" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Delay incurred by the operations of the monitoring and detection phases when the network size increases.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We proposed a blockchain-based monitoring technique for poisoning attack detection in decentralized federated learning. Our approach leverages the blockchain network as an immutable security monitoring for the <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">workers</span> of the system and is used as a record-keeping privacy preserving pattern collection. The parallelization of the monitoring and detection operations results in minimized latency over the end-to-end communication, computation, and consensus delays incurred during the FL and blockchain operations. Our design achieves robustness and scalability and enables the storage of valuable timely and dynamic local models updates of every <span id="S5.p1.1.2" class="ltx_text ltx_font_italic">worker</span> of the system in a transparent and secure manner.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Our proposed technique can be deployed on resource-constrained nodes such as mobiles or Internet of Things (IoT) devices which have low cost and limited energy. However, for more advanced mobility models, the training may consume significant computation power or bandwidth. To satisfy the resource requirements of such a network, a study of the trade-off between the number of miners of the blockchain network that are attributed to the FL process, the <span id="S5.p2.1.1" class="ltx_text ltx_font_italic">minersFL</span> and the <span id="S5.p2.1.2" class="ltx_text ltx_font_italic">minersMON</span> that are attributed to the monitoring should be conducted. Moreover, to further ensure that the machine learning model does not suffer from data leakage, meaning from threats such as black-box attacks where malicious
participants can recover arbitrary inputs fed into their devices, in a future work, we are investigating split learning as a countermeasure to achieve more robust privacy preserving model training in a distributed manner.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Availability</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Data and code of this study are made publicly available by the authors on <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/LiTrans/BSMD/tree/master/use_cases/untargeted_poisoning</span>.
</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al.

</span>
<span class="ltx_bibblock">Tensorflow: A system for large-scale machine learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.4.4" class="ltx_text ltx_font_italic">12th <math id="bib.bib1.1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib1.1.1.m1.1a"><mo stretchy="false" id="bib.bib1.1.1.m1.1.1" xref="bib.bib1.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib1.1.1.m1.1b"><ci id="bib.bib1.1.1.m1.1.1.cmml" xref="bib.bib1.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib1.1.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib1.2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib1.2.2.m2.1a"><mo stretchy="false" id="bib.bib1.2.2.m2.1.1" xref="bib.bib1.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib1.2.2.m2.1b"><ci id="bib.bib1.2.2.m2.1.1.cmml" xref="bib.bib1.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib1.2.2.m2.1c">\}</annotation></semantics></math> Symposium on Operating Systems Design and
Implementation (<math id="bib.bib1.3.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib1.3.3.m3.1a"><mo stretchy="false" id="bib.bib1.3.3.m3.1.1" xref="bib.bib1.3.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib1.3.3.m3.1b"><ci id="bib.bib1.3.3.m3.1.1.cmml" xref="bib.bib1.3.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib1.3.3.m3.1c">\{</annotation></semantics></math>OSDI<math id="bib.bib1.4.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib1.4.4.m4.1a"><mo stretchy="false" id="bib.bib1.4.4.m4.1.1" xref="bib.bib1.4.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib1.4.4.m4.1b"><ci id="bib.bib1.4.4.m4.1.1.cmml" xref="bib.bib1.4.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib1.4.4.m4.1c">\}</annotation></semantics></math> 16)</span>, pages 265–283, 2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Peva Blanchard, Rachid Guerraoui, Julien Stainer, et al.

</span>
<span class="ltx_bibblock">Machine learning with adversaries: Byzantine tolerant gradient
descent.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, pages
119–129, 2017.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Yudong Chen, Lili Su, and Jiaming Xu.

</span>
<span class="ltx_bibblock">Distributed statistical machine learning in adversarial settings:
Byzantine gradient descent.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the ACM on Measurement and Analysis of Computing
Systems</span>, 1(2):1–25, 2017.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Clement Fung, Chris JM Yoon, and Ivan Beschastnikh.

</span>
<span class="ltx_bibblock">Mitigating sybils in federated learning poisoning.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1808.04866</span>, 2018.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Jiawen Kang, Zehui Xiong, Dusit Niyato, Yuze Zou, Yang Zhang, and Mohsen
Guizani.

</span>
<span class="ltx_bibblock">Reliable federated learning for mobile networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">IEEE Wireless Communications</span>, 27(2):72–80, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Hyesung Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim.

</span>
<span class="ltx_bibblock">On-device federated learning via blockchain and its latency analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1808.03949</span>, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Lumin Liu, Jun Zhang, SH Song, and Khaled Ben Letaief.

</span>
<span class="ltx_bibblock">Edge-assisted hierarchical federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1905.06641</span>, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
David López and Bilal Farooq.

</span>
<span class="ltx_bibblock">A multi-layered blockchain framework for smart mobility data-markets.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Transportation Research Part C: Emerging Technologies</span>,
111:588–615, February 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
David López, Ali Yazdizadeh, Bilal Farooq, and Zachary Patterson.

</span>
<span class="ltx_bibblock">Distributed privacy-aware mode inference using federated learning
over blockchain for smart mobility data-market.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">International Choice Modelling Conference 2019</span>, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Artificial Intelligence and Statistics</span>, pages 1273–1282.
PMLR, 2017.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
El Mahdi El Mhamdi, Rachid Guerraoui, and Sébastien Rouault.

</span>
<span class="ltx_bibblock">The hidden vulnerability of distributed learning in byzantium.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1802.07927</span>, 2018.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Xudong Pan, Mi Zhang, Duocai Wu, Qifan Xiao, Shouling Ji, and Zhemin Yang.

</span>
<span class="ltx_bibblock">Justinian’s gaavernor: Robust distributed learning with gradient
aggregation agent.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.4.4" class="ltx_text ltx_font_italic">29th <math id="bib.bib12.1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib12.1.1.m1.1a"><mo stretchy="false" id="bib.bib12.1.1.m1.1.1" xref="bib.bib12.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib12.1.1.m1.1b"><ci id="bib.bib12.1.1.m1.1.1.cmml" xref="bib.bib12.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib12.1.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib12.2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib12.2.2.m2.1a"><mo stretchy="false" id="bib.bib12.2.2.m2.1.1" xref="bib.bib12.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib12.2.2.m2.1b"><ci id="bib.bib12.2.2.m2.1.1.cmml" xref="bib.bib12.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib12.2.2.m2.1c">\}</annotation></semantics></math> Security Symposium (<math id="bib.bib12.3.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib12.3.3.m3.1a"><mo stretchy="false" id="bib.bib12.3.3.m3.1.1" xref="bib.bib12.3.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib12.3.3.m3.1b"><ci id="bib.bib12.3.3.m3.1.1.cmml" xref="bib.bib12.3.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib12.3.3.m3.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib12.4.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib12.4.4.m4.1a"><mo stretchy="false" id="bib.bib12.4.4.m4.1.1" xref="bib.bib12.4.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib12.4.4.m4.1b"><ci id="bib.bib12.4.4.m4.1.1.cmml" xref="bib.bib12.4.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib12.4.4.m4.1c">\}</annotation></semantics></math>
Security 20)</span>, pages 1641–1658, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Davy Preuveneers, Vera Rimmer, Ilias Tsingenopoulos, Jan Spooren, Wouter
Joosen, and Elisabeth Ilie-Zudor.

</span>
<span class="ltx_bibblock">Chained anomaly detection models for federated learning: An intrusion
detection case study.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Applied Sciences</span>, 8(12):2663, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Youyang Qu, Longxiang Gao, Tom H Luan, Yong Xiang, Shui Yu, Bai Li, and Gavin
Zheng.

</span>
<span class="ltx_bibblock">Decentralized privacy using blockchain-enabled federated learning in
fog computing.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Sumudu Samarakoon, Mehdi Bennis, Walid Saad, and Mérouane Debbah.

</span>
<span class="ltx_bibblock">Distributed federated learning for ultra-reliable low-latency
vehicular communications.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Communications</span>, 68(2):1146–1159, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang.

</span>
<span class="ltx_bibblock">Certified defenses for data poisoning attacks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
3517–3529, 2017.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Michael Szydlo.

</span>
<span class="ltx_bibblock">Merkle tree traversal in log space and time.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">International Conference on the Theory and Applications of
Cryptographic Techniques</span>, pages 541–554. Springer, 2004.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Joost Verbraeken, Matthijs Wolting, Jonathan Katzy, Jeroen Kloppenburg, Tim
Verbelen, and Jan S Rellermeyer.

</span>
<span class="ltx_bibblock">A survey on distributed machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Acm computing surveys (csur)</span>, 53(2):1–33, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu, Yong Cheng, Yan Kang, Tianjian Chen, and Han Yu.

</span>
<span class="ltx_bibblock">Federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Synthesis Lectures on Artificial Intelligence and Machine
Learning</span>, 13(3):1–207, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Yang Zhao, Jun Zhao, Linshan Jiang, Rui Tan, and Dusit Niyato.

</span>
<span class="ltx_bibblock">Mobile edge computing, blockchain and reputation-based crowdsourcing
iot federated learning: A secure, decentralized and privacy-preserving
system.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1906.10893</span>, 2019.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2210.02872" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2210.02873" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2210.02873">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2210.02873" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2210.02874" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 03:52:28 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
