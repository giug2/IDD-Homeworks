<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson</title>
<!--Generated on Sat Aug 17 19:09:20 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Continuous Integration and Continuous Delivery (CI/CD),  Chatbot-Enabled Software Engineering,  Large Language Models (LLMs),  Retrieval-Augmented Generation (RAG).
" lang="en" name="keywords"/>
<base href="/html/2408.09277v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S1" title="In Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S2" title="In Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Industrial Context and Motivation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S3" title="In Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Background</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S4" title="In Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5" title="In Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Chatbot Development</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.SS1" title="In V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Corpus Creation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.SS2" title="In V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Chatbot Design</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6" title="In Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Empirical Evaluation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.SS1" title="In VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span> </span><span class="ltx_text ltx_font_italic">Research Questions</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.SS2" title="In VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span> </span><span class="ltx_text ltx_font_italic">Implementation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.SS3" title="In VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-C</span> </span><span class="ltx_text ltx_font_italic">Experimental Setup</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.SS4" title="In VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-D</span> </span><span class="ltx_text ltx_font_italic">Ground Truth</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.SS5" title="In VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-E</span> </span><span class="ltx_text ltx_font_italic">Domain Corpus</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.SS6" title="In VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-F</span> </span><span class="ltx_text ltx_font_italic">Metrics</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.SS6.SSS1" title="In VI-F Metrics ‣ VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-F</span>1 </span>Metrics for RQ1</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.SS6.SSS2" title="In VI-F Metrics ‣ VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-F</span>2 </span>Metric for RQ2</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.SS7" title="In VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-G</span> </span><span class="ltx_text ltx_font_italic">Evaluation Procedure</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.SS8" title="In VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-H</span> </span><span class="ltx_text ltx_font_italic">Answers to RQs</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.SS9" title="In VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-I</span> </span><span class="ltx_text ltx_font_italic">Limitations and Validity Considerations</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S7" title="In Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Lessons Learned</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S8" title="In Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#A0.SS1" title="In VIII Conclusion ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">-A</span> </span><span class="ltx_text ltx_font_italic">Parameters for Microsoft Teams Messages</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#A0.SS2" title="In VIII Conclusion ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">-B</span> </span><span class="ltx_text ltx_font_italic">Parameters for Microsoft Teams Replies</span></span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Daksh Chaudhary12,
Sri Lakshmi Vadlamani2,
Dimple Thomas2,
Shiva Nejati1,
Mehrdad Sabetzadeh1



Email: {dchau012, snejati, m.sabetzadeh}@uottawa.ca; {sri.lakshmi.vadlamani, dimple.thomas}@ericsson.com
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">1University of Ottawa, 800 King Edward Avenue, Ottawa ON K1N 6N5, Canada
</span>
<span class="ltx_contact ltx_role_affiliation">2Ericsson Canada, 349 Terry Fox Dr, Kanata, ON K2K 2V6

</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">This paper presents our experience developing a Llama-based chatbot for question answering about continuous integration and continuous delivery (CI/CD) at Ericsson, a multinational telecommunications company. Our chatbot is designed to handle the specificities of CI/CD documents at Ericsson, employing a retrieval-augmented generation (RAG) model to enhance accuracy and relevance. Our empirical evaluation of the chatbot on industrial CI/CD-related questions indicates that an ensemble retriever, combining BM25 and embedding retrievers, yields the best performance.
When evaluated against a ground truth of 72 CI/CD questions and answers at Ericsson, our most accurate chatbot configuration provides fully correct answers for 61.11% of the questions, partially correct answers for 26.39%, and incorrect answers for 12.50%. Through an error analysis of the partially correct and incorrect answers, we discuss the underlying causes of inaccuracies and provide insights for further refinement. We also reflect on lessons learned and suggest future directions for further improving our chatbot’s accuracy.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Continuous Integration and Continuous Delivery (CI/CD), Chatbot-Enabled Software Engineering, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG).

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">With advances in large language models (LLMs), the high-tech industry is increasingly looking into how chatbots can improve software engineering practices.
There have been existing attempts to employ LLM-based chatbots in the domain of software engineering. Among others, Abdellatif et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib2" title="">2</a>]</cite> and Daniel &amp; Cabot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib9" title="">9</a>]</cite> explore various facets of integrating chatbots into software engineering workflows, highlighting the potential for chatbots to streamline processes, enhance communication, and assist in tasks ranging from bug tracking and documentation to code generation and quality assurance.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">This paper presents our experience developing a Llama-based chatbot for answering questions related to continuous integration and continuous delivery (CI/CD) at Ericsson, a multinational telecommunications company. Ericsson employs agile development and DevOps across various projects, requiring many engineers to work efficiently with CI/CD processes.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">CI/CD enables automated testing, integration, and deployment of code changes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib5" title="">5</a>]</cite>. CI/CD is intrinsically linked to software maintenance and evolution, ensuring that software remains functional and up-to-date as new features and fixes are continuously integrated into the codebase. Our chatbot is designed to handle the contextual factors specific to CI/CD documents at Ericsson. This includes the evolving content of guidelines and team conversations about CI/CD.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To build an accurate chatbot, we opt for a retrieval-augmented generation (RAG) model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib28" title="">28</a>]</cite>. RAG enhances chatbot capabilities by combining retrieval of relevant documents with the generative power of LLMs, thereby providing more accurate and relevant responses. RAG presents two main advantages over fine-tuning a model on domain-specific corpora <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib13" title="">13</a>]</cite>: First, fine-tuning requires a labeled dataset, which can be expensive to build. Second, a fine-tuned model is prone to outdated knowledge; this issue can be mitigated through a RAG model that continuously accesses and retrieves up-to-date information.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We evaluate our chatbot on industrial CI/CD questions. We experiment with alternative retriever models for instantiating a RAG pipeline over Llama 2 and empirically evaluate the accuracy of the resulting pipelines. Our evaluation indicates that an ensemble retriever, combining BM25 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib39" title="">39</a>]</cite> and embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib35" title="">35</a>]</cite>, leads to the best overall outcome. Specifically, by using an ensemble retriever and evaluating the chatbot against a ground truth of 72 CI/CD questions and answers at Ericsson, we obtain fully correct answers for 61.11% of the questions, partially correct answers for 26.39%, and incorrect answers for 12.50%. Following this evaluation, we conduct an error analysis on partially correct and incorrect answers to identify the root causes of the inaccuracies.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1"><span class="ltx_text ltx_font_bold" id="S1.p6.1.1">Novelty.</span> The novelty of our work lies in providing practical insights into the readiness of chatbot technologies for question answering in a complex and specialized yet dynamic setting, where the content from which answers are derived is fluid and changes over time. To the best of our knowledge, we are the first to report on the construction and evaluation of a chatbot for answering CI/CD questions in an industrial context.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1"><span class="ltx_text ltx_font_bold" id="S1.p7.1.1">Significance.</span> Despite recent advances in generative LLMs that have made chatbot construction more accessible, the field remains marked by hype, generally lacks empirical analysis of accuracy, and does not sufficiently elaborate on technical considerations that could have a make-or-break effect on chatbot efficacy. Our work highlights our main technical choices for chatbot design, aiming to assist other researchers and practitioners facing similar challenges. Furthermore, we provide an empirically grounded examination of chatbot accuracy for a software-engineering problem in industry. This contributes to the development of a scientific body of knowledge that facilitates wider deployment of software-engineering chatbots.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1"><span class="ltx_text ltx_font_bold" id="S1.p8.1.1">Structure.</span> The rest of this paper is structured as follows: Section <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S2" title="II Industrial Context and Motivation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">II</span></a> motivates our work and presents our industry context.
Section <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S3" title="III Background ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">III</span></a> provides background information. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S4" title="IV Related Work ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">IV</span></a> surveys related work. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5" title="V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">V</span></a> describes the technical approach that underlies our chatbot. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6" title="VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">VI</span></a> reports on the evaluation of our chatbot. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S7" title="VII Lessons Learned ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">VII</span></a> discusses lessons learned. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S8" title="VIII Conclusion ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">VIII</span></a> concludes the paper.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Industrial Context and Motivation</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Ericsson is a multinational company specializing in providing ICT services and equipment to telecommunications operators and enterprises worldwide. Our chatbot was developed within Ericsson’s Cloud Radio Access Network (CloudRAN) unit. This unit focuses on online and virtualized central system observability and monitoring solutions across cloud-native RAN deployments, including software microservices on containers-as-a-service (CaaS) infrastructure.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">CloudRAN employs CI/CD to automate code integration, testing, and delivery. The CI/CD process at CloudRAN adheres to industry best practices and follows a standard workflow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib11" title="">11</a>]</cite>: All code is stored in a version-control system, with developers working on feature branches. When code is committed, a CI server triggers automated builds and tests to provide instantaneous feedback. Successful builds produce artifacts for deployment. During the CD process, code that passes CI tests is deployed to a staging environment for additional testing and manual review. CloudRAN has an approval process in place before deploying to production. Monitoring tools and centralized logging systems track application performance and detect issues. This iterative process ensures frequent, reliable code integration and delivery, reducing errors and downtime while enabling rapid feedback and improvements.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Our chatbot aims to improve CI/CD at CloudRAN by enabling software engineers, both within the unit and at client sites, to obtain answers to their CI/CD-related questions. A few examples of queries that CloudRAN would like the chatbot to be able to respond to are: <em class="ltx_emph ltx_font_italic" id="S2.p3.1.1">(1) What are the steps to release a microservice?
(2) How can I modify test targets during staging?
(3) How can I migrate from [cluster 1] to [cluster 2]?
(4) How do I add a test channel to a Jenkins pool?</em></p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">We note that, both in the above queries and in the examples provided throughout the rest of the paper, we have altered the content from its original form to preserve confidentiality, while ensuring that the substance remains unchanged. Any redacted text is indicated by square brackets ([]).</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">By handling routine queries, such as our illustrative examples above, the chatbot offers the potential to free up expert engineering resources to address more complex issues. This reduces operational costs, speeds up issue resolution, and allows experts to focus on tasks that require specialized skills.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">The dynamic nature of CloudRAN’s operations, which includes reliance on constantly evolving internal documents and team communications, is an important contextual factor to consider in the design of the chatbot to ensure its longevity. To this end, we employ RAG to incorporate up-to-date information from team workspaces and messaging channels, providing answers based on the most recent knowledge.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Background</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our chatbot falls under the umbrella of retrieval-augmented question-answering (QA) techniques. Retrieval-augmented QA involves integrating a retrieval mechanism to extract pertinent information from a given set of documents, thereby enhancing the accuracy and completeness of answers to queries. Retrieval augmentation has been explored for both extractive QA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib13" title="">13</a>]</cite> and generative QA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib30" title="">30</a>]</cite>, with significant accuracy improvements shown for both types of QA. For generative QA, which is the focus of our work, the associated prompt engineering is one of the most critical steps. OpenAI provides several general guidelines on how to build effective prompts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib37" title="">37</a>]</cite> for RAG tasks. We follow these guidelines for building retrieval augmentation into our chatbot.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">A common approach for implementing retrieval augmentation is through a <em class="ltx_emph ltx_font_italic" id="S3.p2.1.1">retriever and reader architecture</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib13" title="">13</a>]</cite>. The retriever is responsible for efficiently selecting a subset of relevant documents from a larger corpus, acting as an initial filter to narrow the search space. Subsequently, the reader – typically an LLM – is tasked with comprehending and extracting/generating information based on the retrieved documents. The retriever component can be configured to supply the reader with the latest documents pertinent to the user’s query, addressing the challenge posed by the LLMs’ knowledge cut-off. Moreover, providing pertinent context helps reduce LLMs’ tendency to hallucinate <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib42" title="">42</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Several enhancements can be considered to further increase the accuracy of RAG. We explored two such enhancements: end-to-end training and query rewriting. Below, we outline these enhancements and explain our rationale for their inclusion or exclusion.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">End-to-end training involves jointly training the retriever and the reader on domain-specific data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib23" title="">23</a>]</cite>. Previous attempts at end-to-end training have employed LLM readers such as BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib10" title="">10</a>]</cite> and BART <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib27" title="">27</a>]</cite>. However, implementing this approach with readers like Llama 2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib44" title="">44</a>]</cite> and GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib7" title="">7</a>]</cite> remains prohibitively expensive. Since our chatbot is based on Llama 2, we do not pursue end-to-end training. Furthermore, we note that while end-to-end training has been shown to lead to improvements in studies with BERT and BART, these improvements are comparatively modest <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib28" title="">28</a>]</cite>. This suggests that end-to-end training would be worthwhile only if its cost is sufficiently low, which is currently not the case for the newer generation of LLMs.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">A second possible enhancement to consider is query rewriting. In a RAG pipeline, the retriever step fetches documents similar to the user query. A well-written and self-contained query is thus critical for this step. Nonetheless, real-world user queries are not always optimal and may require adjustments to improve the retriever’s accuracy. Ma et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib33" title="">33</a>]</cite> demonstrate the usefulness of adding a query rewriter to RAG. Motivated by their results, we adapt and extend their guidelines to integrate a query rewriter into our chatbot. Although query rewriting inevitably increases QA execution time, our overall pipeline’s execution time remains acceptable, as we show in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6" title="VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">VI</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Related Work</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In software engineering, chatbots are increasingly used to assist with tasks such as code understanding, code generation, and quality assurance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib1" title="">1</a>]</cite>.
This section reviews recent relevant strands of work on chatbot-enabled software engineering and contrasts them with our research.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">A prominent example of a widely used conversational software development tool is GitHub Copilot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib15" title="">15</a>]</cite>. Copilot builds on top of the Codex model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib36" title="">36</a>]</cite>, a fine-tuned version of GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib7" title="">7</a>]</cite>, to provide support in various tasks such as code completion, code generation from natural-language input, code migration, and answering coding questions. Within Copilot, the task most similar to our chatbot function is answering coding questions. Nevertheless, Copilot’s question-answering capabilities are focused on code-related queries <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib16" title="">16</a>]</cite>. In contrast, our chatbot does not have a code-centric focus and is complementary; its primary goal is to assist systems engineers with domain-specific technical questions concerning integration, testing, deployment, and troubleshooting.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">QAssist <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib13" title="">13</a>]</cite> employs a RAG architecture similar to ours to help stakeholders analyze and improve the quality of software requirements specifications. Specifically, this tool uses requirements-relevant content alongside generic domain material sourced from Wikipedia to answer questions about natural-language requirements. While QAssist uses RAG and shares a broad-spectrum question-answering objective similar to ours, it implements extractive QA using BERT variants. As such, QAssist can only highlight passages containing the answer, without the ability to engage with users in a conversational and context-aware manner. In contrast, our chatbot uses Llama 2, a generative LLM, to produce coherent and context-aware answers. These answers are based on knowledge extracted from relevant passages and chat history. Furthermore, in terms of design, our chatbot has a more advanced technology chain than QAssist, aligning with the latest <span class="ltx_text" id="S4.p3.1.1">advances in chatbot development.</span></p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">A recent study by Abedu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib3" title="">3</a>]</cite> employs a RAG-based chatbot to facilitate user access to information within software repositories. This approach allows users to input the URL of the target repository in the chatbot interface and then query the repository’s content. While the ability to dynamically input the desired repository provides flexibility, it also complicates the implementation of tailored preprocessing to enhance retrieval performance. In our chatbot design, we separate the domain-corpus creation process from the chatbot pipeline. This separation enables us to preprocess documents based on their content type. Furthermore, while Abedu et al. only experiment with a fixed choice of retriever (embeddings-based), we conduct a comparative analysis of four different types of retrievers to identify the most suitable one for our problem context, as we discuss in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.SS7" title="VI-G Evaluation Procedure ‣ VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-G</span></span></a>.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">In summary, while chatbots have been deployed to support various software engineering tasks, none are specifically designed for the analytical goals that our chatbot addresses, nor do they have the exact same design considerations as ours.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Chatbot Development</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we describe the approach implemented by our chatbot. The chatbot takes as input a collection of documents – in the context of our industry collaboration, a collection of Ericsson documents related to CI/CD – alongside a natural-language user query. Subsequently, the chatbot uses an LLM to generate a natural-language response to the query. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.SS1" title="V-A Corpus Creation ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-A</span></span></a> describes the process of creating a domain-specific corpus from Ericsson’s CI/CD documents. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.SS2" title="V-B Chatbot Design ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-B</span></span></a> provides an overview of our chatbot’s architecture and discusses the steps involved in generating an answer based on the domain-specific corpus and a given query.</p>
</div>
<figure class="ltx_figure" id="S5.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="412" id="S5.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Steps for Creating a (Domain-specific) CI/CD Corpus</figcaption>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.4.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.5.2">Corpus Creation</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.F1" title="Figure 1 ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">1</span></a> presents the steps we follow for transforming Ericsson’s CI/CD documents into a (domain-specific) corpus.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS1.p2.1.1">Step 1: Data Extraction.</span> We gather CI/CD domain knowledge from two Ericsson-specific sources: (1) messages exchanged among software engineers in CI/CD channels on internal Microsoft Teams, and (2) Confluence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib4" title="">4</a>]</cite> web pages that contain information about common troubleshooting procedures for CI/CD-related tasks within the organization. These data sources enable our chatbot to respond to Ericsson-specific technical questions. Noting that both data sources evolve frequently over time, we first extract the most recent data from these sources. This extraction step is decoupled from the chatbot itself (Section <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.SS2" title="V-B Chatbot Design ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-B</span></span></a>), allowing updates to be performed offline and on a regular basis (e.g., overnight) without disrupting the chatbot’s function.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">The Teams data, comprised of messages and replies, is collected through the Microsoft Graph REST API <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib34" title="">34</a>]</cite>. Using this API, we generate separate tables (in CSV format) for storing messages and replies, with one table dedicated to messages and another to replies. The messages table has 35 parameters (fixed columns) such as message ID, creation time, content, sender’s username, mentions, and reactions. The replies table consists of 43 parameters such as reply ID, reply’s parent message ID, content, mentions, and reactions. A complete list of parameters for Microsoft Teams messages and replies is provided in Appendices <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#A0.SS1" title="-A Parameters for Microsoft Teams Messages ‣ VIII Conclusion ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">-A</span></span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#A0.SS2" title="-B Parameters for Microsoft Teams Replies ‣ VIII Conclusion ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">-B</span></span></a>, respectively.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">The Confluence data consists of a set of HTML pages, with each page containing a troubleshooting topic followed by content outlining the troubleshooting procedure. We extract these pages using a custom REST API developed by Ericsson.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS1.p5.1.1">Step 2: Preprocessing.</span>
We process the data extracted in Step 1 into a suitable format for use by the LLM.</p>
</div>
<div class="ltx_para" id="S5.SS1.p6">
<p class="ltx_p" id="S5.SS1.p6.1">For the Teams data, we strip the HTML formatting from the ‘content’ column of the messages and replies tables discussed earlier and convert the content to plain text. Subsequently, each reply in the replies table is linked to its corresponding parent message in the messages table through the unique parent message ID. This process connects each message to its replies in the same order as they were originally posted, thus preserving the chronological order of messages. To protect privacy, as we reconcile the messages and the responses to them, we remove personal information such as employee names and email addresses from both the content and the associated metadata. Finally, we store every message along with all the responses to it in a plain-text document.</p>
</div>
<div class="ltx_para" id="S5.SS1.p7">
<p class="ltx_p" id="S5.SS1.p7.1">As for the extracted Confluence pages, the preprocessing is straightforward: we store the title and content of each page in a plain-text document.</p>
</div>
<div class="ltx_para" id="S5.SS1.p8">
<p class="ltx_p" id="S5.SS1.p8.1">To facilitate more accurate interpretation of the Teams and Confluence data by the LLM, we augment this data with prefixes. Specifically, each Teams message is prefixed with the phrase “Message:”, while replies to the message are prefixed with “This message has the following responses:”. Similarly, the title of each Confluence page is prefixed with “Page Title:”, and the content following the title is prefixed with “The content of this page is as follows:”.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p9">
<p class="ltx_p" id="S5.SS1.p9.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS1.p9.1.1">Step 3: Indexing.</span>
In this step, we embed and store the preprocessed documents obtained from Step 2 (Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.F1" title="Figure 1 ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">1</span></a>) in a vector database. This vector database serves as the domain-specific corpus for the information retrieval step of our chatbot (Step 2 in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.F2" title="Figure 2 ‣ V-B Chatbot Design ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">2</span></a>, discussed later). Since the preprocessed documents are ultimately supplied to the LLM as relevant context, our objective is to maximize their length. However, two important considerations arise when determining the ideal length. On the one hand, we must ensure that the documents fit within the context length (token limit) accepted by the LLM, as exceeding this limit could result in context loss, runtime errors, or incoherent output. On the other hand, supplying long documents to the LLM can lead to a “needle in the haystack” problem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib17" title="">17</a>]</cite>, where relevant information is lost amongst the noise. Therefore, determining the optimal length of embedded documents becomes an important factor in maximizing chatbot efficacy, as we aim to maximize the amount of relevant information while limiting the noise.</p>
</div>
<div class="ltx_para" id="S5.SS1.p10">
<p class="ltx_p" id="S5.SS1.p10.1">Our choice of how to split the data in a preprocessed document depends on the source from which the document originates. We have custom splitters for each Teams and Confluence. For Teams, based on actual data and the experience of collaborating engineers at Ericsson, the combination of a message and all its replies is anticipated to always be well below the contextual length of modern LLMs. Therefore, we have determined that each individual message, alongside all the replies to it, could be embedded directly as one unit in our vector database. This means that the units fetched by the retriever step of the chatbot will constitute one message and all the associated replies.</p>
</div>
<div class="ltx_para" id="S5.SS1.p11">
<p class="ltx_p" id="S5.SS1.p11.1">For the Confluence data, the length of a preprocessed document could exceed the context length of the LLM. Therefore, we need to apply document chunking before embedding large Confluence documents in the vector database. To determine the optimal chunk size for Confluence data, we conducted exploratory experimentation with values ranging from 200 to 1000 tokens (increasing the chunk size by 100 tokens in each iteration). Based on this experimentation, we observed that a chunk size of 800 tokens led to the best question-answering results. Furthermore, we followed the best practice of making adjacent chunks overlapping <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib13" title="">13</a>]</cite>, maintaining an overlap of 200 tokens (25%) between adjacent chunks. To further preserve context, all chunks belonging to the same Confluence document were prefixed with the document title and the respective chunk number.</p>
</div>
<div class="ltx_para" id="S5.SS1.p12">
<p class="ltx_p" id="S5.SS1.p12.1">Following the splitting of the Teams and Confluence data, we send the resulting chunks to an embedding function to generate text embeddings. To ensure consistent terminology, we refer to these chunks as <em class="ltx_emph ltx_font_italic" id="S5.SS1.p12.1.1">context items</em> rather than “documents”, to avoid ambiguity between the chunks and the original (Confluence) documents. We store the embeddings for each context item in the vector database alongside the item’s original text. The domain-specific corpus depicted in Figures <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.F1" title="Figure 1 ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.F2" title="Figure 2 ‣ V-B Chatbot Design ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">2</span></a> is realized by this vector database.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.4.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.5.2">Chatbot Design</span>
</h3>
<figure class="ltx_figure" id="S5.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="450" id="S5.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of Our Chatbot Design</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.F2" title="Figure 2 ‣ V-B Chatbot Design ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">2</span></a> shows the steps implemented in our chatbot. We will discuss each of these steps below.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS2.p2.1.1">Step 1: Query Rewriting.</span>
Given that user queries may not always be clear or well-structured <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib33" title="">33</a>]</cite>, our query rewriting module prompts an LLM <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Note that the LLM used for query rewriting does not necessarily have to be the same one used for answer generation in Step 4 of Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.F2" title="Figure 2 ‣ V-B Chatbot Design ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">2</span></a>, which will be discussed later. In our current approach, nevertheless, we use Llama 2 as the LLM of choice to implement both Steps 1 and 4 of our chatbot design.</span></span></span> to enhance the query into a more effective search query. To achieve this, we use the query enhancement guidelines of Ma et al.’s <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib33" title="">33</a>]</cite>. Specifically, given a (frozen) LLM and a user query, we prompt the LLM to restate the query in more precise terms before the query is used for question answering. We augment Ma et al.’s prompt template for query rewriting by instructing the model to also analyze the user query in the context of the conversation history and determine if the current query is a follow-up.
In response, the LLM either forms an improved question or returns the user query <span class="ltx_text" id="S5.SS2.p2.1.2">verbatim as the question to pose to the LLM.</span></p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">In addition to attempting to construct a better query based on Ma et el.’s prompt template, we incorporate various prompt engineering techniques to improve the quality of the LLM’s output. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.F3" title="Figure 3 ‣ V-B Chatbot Design ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">3</span></a> presents our query-rewriting prompt and highlights the various considerations involved.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1">We employ the same prompt template that Meta utilized for pre-training Llama 2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib44" title="">44</a>]</cite>. This helps ensure that the model is able to understand the prompt structure and instructions clearly. In the prompt template, the instructions provided between the <code class="ltx_verbatim ltx_font_typewriter" id="S5.SS2.p4.1.1">&lt;&lt;SYS&gt;&gt;</code> tokens convey a system message to the model, instructing it on its task and intended behaviour. To improve results, we explicitly define the task of the LLM (Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.F3" title="Figure 3 ‣ V-B Chatbot Design ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">3</span></a>, <span class="ltx_text" id="S5.SS2.p4.1.2" style="color:#4472C4;">➊<span class="ltx_text" id="S5.SS2.p4.1.2.1" style="color:#000000;">). We then employ zero-shot chain-of-thought prompting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib45" title="">45</a>]</cite> (</span>➋<span class="ltx_text" id="S5.SS2.p4.1.2.2" style="color:#000000;">) to decompose the task into intermediate steps. We mitigate ambiguity in the model’s understanding of the task by explicitly handling possible scenarios (</span>➌<span class="ltx_text" id="S5.SS2.p4.1.2.3" style="color:#000000;">). Further, we instruct the model to retain important key terms from the query (</span>➍<span class="ltx_text" id="S5.SS2.p4.1.2.4" style="color:#000000;">) and to strictly follow the desired output format (</span>➎<span class="ltx_text" id="S5.SS2.p4.1.2.5" style="color:#000000;">).</span></span></p>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1"><span class="ltx_text" id="S5.SS2.p5.1.1" style="color:#000000;">We illustrate Step 1 using the two examples, Example 1 and Example 2, in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.F4" title="Figure 4 ‣ V-B Chatbot Design ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">4</span></a>. Example 1 demonstrates the ability of query rewriting to filter irrelevant information from the user query and generate a better query that focuses on the important aspects. Example 2 highlights how query rewriting can identify the question implied by the user statement. Moreover, in the case of the follow-up question in Example 2, query rewriting uses the conversation history to derive a complete query.</span></p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="793" id="S5.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Prompt Template for Query Rewriting</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="834" id="S5.F4.g1" src="x4.png" width="831"/>
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Examples of Query Rewriting</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p6">
<p class="ltx_p" id="S5.SS2.p6.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS2.p6.1.1" style="color:#000000;">Step 2: Retrieval.</span><span class="ltx_text" id="S5.SS2.p6.1.2" style="color:#000000;">
Following the retrieval-augmented generation approach </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.SS2.p6.1.3.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib28" title="">28</a><span class="ltx_text" id="S5.SS2.p6.1.4.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S5.SS2.p6.1.5" style="color:#000000;">, we employ a retriever component to identify domain-specific knowledge that should be imparted to the LLM for answer generation. In our design, the retriever uses the enhanced query obtained from query rewriting (Step 1) to retrieve relevant context items from the domain-specific corpus, constructed using the process </span><span class="ltx_text" id="S5.SS2.p6.1.6" style="color:#000000;">described in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.SS1" title="V-A Corpus Creation ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-A</span></span></a>.</span><span class="ltx_text" id="S5.SS2.p6.1.7" style="color:#000000;"></span></p>
</div>
<div class="ltx_para" id="S5.SS2.p7">
<p class="ltx_p" id="S5.SS2.p7.8"><span class="ltx_text" id="S5.SS2.p7.8.8" style="color:#000000;">Recognizing that different data sources may necessitate different retrievers for better question-answering accuracy, our approach offers the flexibility to specify the retriever to be used. The selected retriever is responsible for picking the top-<math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p7.1.1.m1.1"><semantics id="S5.SS2.p7.1.1.m1.1a"><mi id="S5.SS2.p7.1.1.m1.1.1" mathcolor="#000000" xref="S5.SS2.p7.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.1.1.m1.1b"><ci id="S5.SS2.p7.1.1.m1.1.1.cmml" xref="S5.SS2.p7.1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p7.1.1.m1.1d">italic_k</annotation></semantics></math> context items to feed to the LLM as relevant information for answer generation. Determining the optimal value of <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p7.2.2.m2.1"><semantics id="S5.SS2.p7.2.2.m2.1a"><mi id="S5.SS2.p7.2.2.m2.1.1" mathcolor="#000000" xref="S5.SS2.p7.2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.2.2.m2.1b"><ci id="S5.SS2.p7.2.2.m2.1.1.cmml" xref="S5.SS2.p7.2.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.2.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p7.2.2.m2.1d">italic_k</annotation></semantics></math> is crucial and should consider the nature and size of the context items. Our exploratory experimentation revealed that setting <math alttext="k=3" class="ltx_Math" display="inline" id="S5.SS2.p7.3.3.m3.1"><semantics id="S5.SS2.p7.3.3.m3.1a"><mrow id="S5.SS2.p7.3.3.m3.1.1" xref="S5.SS2.p7.3.3.m3.1.1.cmml"><mi id="S5.SS2.p7.3.3.m3.1.1.2" mathcolor="#000000" xref="S5.SS2.p7.3.3.m3.1.1.2.cmml">k</mi><mo id="S5.SS2.p7.3.3.m3.1.1.1" mathcolor="#000000" xref="S5.SS2.p7.3.3.m3.1.1.1.cmml">=</mo><mn id="S5.SS2.p7.3.3.m3.1.1.3" mathcolor="#000000" xref="S5.SS2.p7.3.3.m3.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.3.3.m3.1b"><apply id="S5.SS2.p7.3.3.m3.1.1.cmml" xref="S5.SS2.p7.3.3.m3.1.1"><eq id="S5.SS2.p7.3.3.m3.1.1.1.cmml" xref="S5.SS2.p7.3.3.m3.1.1.1"></eq><ci id="S5.SS2.p7.3.3.m3.1.1.2.cmml" xref="S5.SS2.p7.3.3.m3.1.1.2">𝑘</ci><cn id="S5.SS2.p7.3.3.m3.1.1.3.cmml" type="integer" xref="S5.SS2.p7.3.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.3.3.m3.1c">k=3</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p7.3.3.m3.1d">italic_k = 3</annotation></semantics></math> yields the best results in our application context. This choice also happens to be consistent with the <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p7.4.4.m4.1"><semantics id="S5.SS2.p7.4.4.m4.1a"><mi id="S5.SS2.p7.4.4.m4.1.1" mathcolor="#000000" xref="S5.SS2.p7.4.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.4.4.m4.1b"><ci id="S5.SS2.p7.4.4.m4.1.1.cmml" xref="S5.SS2.p7.4.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.4.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p7.4.4.m4.1d">italic_k</annotation></semantics></math> value recommended by Ezzini et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib13" title="">13</a>]</cite> based on systematic experimentation. However, we observe that forcing <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p7.5.5.m5.1"><semantics id="S5.SS2.p7.5.5.m5.1a"><mi id="S5.SS2.p7.5.5.m5.1.1" mathcolor="#000000" xref="S5.SS2.p7.5.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.5.5.m5.1b"><ci id="S5.SS2.p7.5.5.m5.1.1.cmml" xref="S5.SS2.p7.5.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.5.5.m5.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p7.5.5.m5.1d">italic_k</annotation></semantics></math> context items to be considered at all times has the potential to introduce noise in cases where the number of relevant items in the corpus is less than <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p7.6.6.m6.1"><semantics id="S5.SS2.p7.6.6.m6.1a"><mi id="S5.SS2.p7.6.6.m6.1.1" mathcolor="#000000" xref="S5.SS2.p7.6.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.6.6.m6.1b"><ci id="S5.SS2.p7.6.6.m6.1.1.cmml" xref="S5.SS2.p7.6.6.m6.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.6.6.m6.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p7.6.6.m6.1d">italic_k</annotation></semantics></math>. To filter irrelevant information in the top-<math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p7.7.7.m7.1"><semantics id="S5.SS2.p7.7.7.m7.1a"><mi id="S5.SS2.p7.7.7.m7.1.1" mathcolor="#000000" xref="S5.SS2.p7.7.7.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.7.7.m7.1b"><ci id="S5.SS2.p7.7.7.m7.1.1.cmml" xref="S5.SS2.p7.7.7.m7.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.7.7.m7.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p7.7.7.m7.1d">italic_k</annotation></semantics></math> context items, we consider a context item only if it has a higher cosine similarity value to the query than a configurable threshold. This threshold helps prevent the selection of irrelevant context items merely to meet the specified quota of <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p7.8.8.m8.1"><semantics id="S5.SS2.p7.8.8.m8.1a"><mi id="S5.SS2.p7.8.8.m8.1.1" mathcolor="#000000" xref="S5.SS2.p7.8.8.m8.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p7.8.8.m8.1b"><ci id="S5.SS2.p7.8.8.m8.1.1.cmml" xref="S5.SS2.p7.8.8.m8.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p7.8.8.m8.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p7.8.8.m8.1d">italic_k</annotation></semantics></math> items. We set this threshold to 0.7, based on exploratory experimentation and informed by our experience in setting similar thresholds in our previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib32" title="">32</a>]</cite>.</span></p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="981" id="S5.F5.g1" src="x5.png" width="831"/>
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Example of Contextual Compression</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p8">
<p class="ltx_p" id="S5.SS2.p8.1"><span class="ltx_text" id="S5.SS2.p8.1.1" style="color:#000000;">Finally, we employ reordering and contextual compression techniques to further enhance the quality of the retrieved context items. The reordering technique involves placing the most relevant context items at either the beginning or the end when feeding the material to the LLM. This approach was inspired by recent research, which indicates that LLMs utilize context most effectively when it is located at the beginning or end, with a decline in performance when the relevant context is situated in the middle of long contexts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib31" title="">31</a>]</cite>. Moreover, since the information relevant to the user query might be buried within the fetched items, we attempt to compress the items using the query before feeding the items to the LLM. This process, known as contextual compression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib25" title="">25</a>]</cite>, helps reduce the amount of irrelevant information. Similar to the threshold discussed above, contextual compression is a noise-reduction measure; however, whereas the threshold filters out irrelevant items, contextual compression mitigates the noise present within the selected items. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.F5" title="Figure 5 ‣ V-B Chatbot Design ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">5</span></a> presents an example of contextual compression, illustrating that even though the retrieved item remains the same in both cases, contextual compression significantly reduces the noise present within the item with respect to the question.</span></p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="475" id="S5.F6.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Prompt Template for Answer Generation</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p9">
<p class="ltx_p" id="S5.SS2.p9.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS2.p9.1.1" style="color:#000000;">Step 3: Prompt Formation.</span><span class="ltx_text" id="S5.SS2.p9.1.2" style="color:#000000;">
This step takes as input the enhanced query generated from Step 1 and the relevant items retrieved in Step 2, and formulates a question prompt for the LLM. Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.F6" style="color:#000000;" title="Figure 6 ‣ V-B Chatbot Design ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">6</span></a><span class="ltx_text" id="S5.SS2.p9.1.3" style="color:#000000;"> presents our prompt template. In this template, we clearly define the environment in which the model is being deployed, as well as its responsibilities (Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.F6" style="color:#000000;" title="Figure 6 ‣ V-B Chatbot Design ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">6</span></a><span class="ltx_text" id="S5.SS2.p9.1.4" style="color:#000000;">, </span><span class="ltx_text" id="S5.SS2.p9.1.5" style="color:#538135;">➊</span><span class="ltx_text" id="S5.SS2.p9.1.6" style="color:#000000;">). This guides the model to refrain from answering unethical questions. Furthermore, the prompt includes a safeguard to mitigate model hallucinations. Specifically, we instruct the model to answer the query solely based on the retrieved documents (</span><span class="ltx_text" id="S5.SS2.p9.1.7" style="color:#538135;">➋</span><span class="ltx_text" id="S5.SS2.p9.1.8" style="color:#000000;">). In cases where the model is unable to provide an accurate answer, it is instructed to inform the user and either ask for more clarity (if the query is still unclear) or request more information (if sufficient relevant context has not been retrieved). The subsequent portion of the template contains placeholders for the context items and the user question, which are populated at runtime. After these placeholders are populated, the final prompt is passed to the LLM.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p10">
<p class="ltx_p" id="S5.SS2.p10.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS2.p10.1.1" style="color:#000000;">Step 4: Answer Generation.</span><span class="ltx_text" id="S5.SS2.p10.1.2" style="color:#000000;"></span></p>
</div>
<figure class="ltx_figure" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="361" id="S5.F7.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Example of a User Query and Response Interaction</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p11">
<p class="ltx_p" id="S5.SS2.p11.1"><span class="ltx_text" id="S5.SS2.p11.1.1" style="color:#000000;">In this step, we use the LLM, which, in our current implementation, is Llama 2, to generate a response to the prompt obtained in Step 3. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.F7" title="Figure 7 ‣ V-B Chatbot Design ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">7</span></a> illustrates an example response by the chatbot to the following query: “List the prerequisites for adding a release pipeline to microservice”, which is a variant of the query shown earlier in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.F5" title="Figure 5 ‣ V-B Chatbot Design ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">5</span></a>. The chatbot takes this query as input, retrieves relevant context items, and generates <span class="ltx_text" id="S5.SS2.p11.1.1.1">an answer based on them.</span></span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Empirical Evaluation</span>
</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS1.4.1.1">VI-A</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS1.5.2">Research Questions</span>
</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1"><span class="ltx_text" id="S6.SS1.p1.1.1" style="color:#000000;">Our evaluation aims to answer the following two research questions (RQs):</span></p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p2.1.1" style="color:#000000;">RQ1.</span><span class="ltx_text" id="S6.SS1.p2.1.2" style="color:#000000;"> </span><em class="ltx_emph ltx_font_italic" id="S6.SS1.p2.1.3" style="color:#000000;">How accurate is our chatbot?</em><span class="ltx_text" id="S6.SS1.p2.1.4" style="color:#000000;"> RQ1 assesses the accuracy of our chatbot using a combination of automated metrics and a manual analysis of correctness. This manual analysis is followed by a root-cause analysis, which identifies the underlying reasons for the inaccuracies in the chatbot’s responses as observed in our case study.</span></p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p3.1.1" style="color:#000000;">RQ2.</span><span class="ltx_text" id="S6.SS1.p3.1.2" style="color:#000000;"> </span><em class="ltx_emph ltx_font_italic" id="S6.SS1.p3.1.3" style="color:#000000;">What is our chatbot’s response time?</em><span class="ltx_text" id="S6.SS1.p3.1.4" style="color:#000000;"> RQ2 measures the execution time of different chatbot-pipeline instantiations.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS2.4.1.1">VI-B</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS2.5.2">Implementation</span>
</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1"><span class="ltx_text" id="S6.SS2.p1.1.1" style="color:#000000;">We implement our chatbot using Python (version 3.10) along with supporting libraries. Specifically, we utilize the Transformers library (version 4.31.0) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS2.p1.1.2.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib21" title="">21</a><span class="ltx_text" id="S6.SS2.p1.1.3.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS2.p1.1.4" style="color:#000000;"> for loading the model and the language tokenizer. To reduce computational requirements, we load the model in a 4-bit quantized format using the BitsAndBytes library (version 0.41.0) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS2.p1.1.5.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib19" title="">19</a><span class="ltx_text" id="S6.SS2.p1.1.6.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS2.p1.1.7" style="color:#000000;">. HuggingFace serves as the model repository and provides a wrapper for the text-generation pipeline over the model.</span></p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1"><span class="ltx_text" id="S6.SS2.p2.1.1" style="color:#000000;">For our experiments, we use the Llama2-chat 7B parameter model </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS2.p2.1.2.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib44" title="">44</a><span class="ltx_text" id="S6.SS2.p2.1.3.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS2.p2.1.4" style="color:#000000;"> released by Meta. Llama2-chat is an open-source model, and by hosting it locally, we ensure that Ericsson’s confidential data remains secure. Furthermore, this model has been optimized for and has demonstrated strong performance in conversational applications </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS2.p2.1.5.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib44" title="">44</a><span class="ltx_text" id="S6.SS2.p2.1.6.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS2.p2.1.7" style="color:#000000;">. Finally, the model is compliant with Ericsson’s internal LLM usage policies.</span></p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1"><span class="ltx_text" id="S6.SS2.p3.1.1" style="color:#000000;">We employ the BAAI/bge-base-en embedding model </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS2.p3.1.2.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib18" title="">18</a><span class="ltx_text" id="S6.SS2.p3.1.3.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS2.p3.1.4" style="color:#000000;"> for indexing documents as it has shown good performance for the retrieval task and is computationally inexpensive </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS2.p3.1.5.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib20" title="">20</a><span class="ltx_text" id="S6.SS2.p3.1.6.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS2.p3.1.7" style="color:#000000;">. LangChain (0.0.240) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS2.p3.1.8.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib26" title="">26</a><span class="ltx_text" id="S6.SS2.p3.1.9.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS2.p3.1.10" style="color:#000000;"> acts as the primary library, providing support and wrappers for the RAG functions. These include: (a) a wrapper over the Chroma embedding vectorstore </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS2.p3.1.11.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib8" title="">8</a><span class="ltx_text" id="S6.SS2.p3.1.12.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS2.p3.1.13" style="color:#000000;">, (b) support for implementing the chat history window, (c) a wrapper over the retriever, (d) support for various optimizations of the retrieval process, and (e) a wrapper for the question-answer pipeline. We employ the Ragas library (version 0.0.21) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS2.p3.1.14.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib12" title="">12</a><span class="ltx_text" id="S6.SS2.p3.1.15.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS2.p3.1.16" style="color:#000000;"> for our evaluation process.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS3.4.1.1">VI-C</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS3.5.2">Experimental Setup</span>
</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1"><span class="ltx_text" id="S6.SS3.p1.1.1" style="color:#000000;">We deployed our chatbot and performed our experiments on a Kubernetes pod containing an Intel Xeon Gold 6230N CPU with 40 GB of RAM and an Nvidia Tesla T4 GPU with 16 GB of GDDR6 memory. The CUDA version was 12.4, and the OS used was Ubuntu 20.04. Model serving was performed using the RayServe software library </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS3.p1.1.2.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib38" title="">38</a><span class="ltx_text" id="S6.SS3.p1.1.3.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS3.p1.1.4" style="color:#000000;">.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS4.4.1.1">VI-D</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS4.5.2">Ground Truth</span>
</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1"><span class="ltx_text" id="S6.SS4.p1.1.1" style="color:#000000;">The ground truth for our evaluation consists of 72 representative question-answer pairs gleaned over a span of nearly a year from resources and internal communications among members of the CloudRAN team at Ericsson. For each question-answer pair, we recorded the basis (documents and/or messages) upon which the correct answer was articulated.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS5.4.1.1">VI-E</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS5.5.2">Domain Corpus</span>
</h3>
<div class="ltx_para" id="S6.SS5.p1">
<p class="ltx_p" id="S6.SS5.p1.1"><span class="ltx_text" id="S6.SS5.p1.1.1" style="color:#000000;">To build the domain corpus used as input for the retrieval step of our chatbot (Step 2 in Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.F2" style="color:#000000;" title="Figure 2 ‣ V-B Chatbot Design ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S6.SS5.p1.1.2" style="color:#000000;">), we followed the corpus creation process described in Section </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.SS1" style="color:#000000;" title="V-A Corpus Creation ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-A</span></span></a><span class="ltx_text" id="S6.SS5.p1.1.3" style="color:#000000;">. The corpus, which forms the basis for our evaluation in this section, was generated in May 2024. It includes 4,169 Microsoft Teams messages, 18,389 responses to these messages, and 240 Confluence web pages. These CI/CD resources collectively resulted in a total of 4985 context items, which were then stored in our vector database along with their embeddings.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS6">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS6.4.1.1">VI-F</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS6.5.2">Metrics</span>
</h3>
<section class="ltx_subsubsection" id="S6.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS6.SSS1.4.1.1">VI-F</span>1 </span>Metrics for RQ1</h4>
<div class="ltx_para" id="S6.SS6.SSS1.p1">
<p class="ltx_p" id="S6.SS6.SSS1.p1.1"><span class="ltx_text" id="S6.SS6.SSS1.p1.1.1" style="color:#000000;">To address RQ1, we evaluate the performance of the chatbot’s retrieval component through context recall and assess the overall chatbot pipeline using answer similarity, as explained below.</span></p>
</div>
<div class="ltx_para" id="S6.SS6.SSS1.p2">
<p class="ltx_p" id="S6.SS6.SSS1.p2.2"><span class="ltx_text ltx_font_bold" id="S6.SS6.SSS1.p2.2.1" style="color:#000000;">Context Recall@k</span><span class="ltx_text" id="S6.SS6.SSS1.p2.2.2" style="color:#000000;"> measures whether the correct answer to a user question is present within the top </span><math alttext="k" class="ltx_Math" display="inline" id="S6.SS6.SSS1.p2.1.m1.1"><semantics id="S6.SS6.SSS1.p2.1.m1.1a"><mi id="S6.SS6.SSS1.p2.1.m1.1.1" mathcolor="#000000" xref="S6.SS6.SSS1.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS6.SSS1.p2.1.m1.1b"><ci id="S6.SS6.SSS1.p2.1.m1.1.1.cmml" xref="S6.SS6.SSS1.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS6.SSS1.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS6.SSS1.p2.1.m1.1d">italic_k</annotation></semantics></math><span class="ltx_text" id="S6.SS6.SSS1.p2.2.3" style="color:#000000;"> context items fetched by a retriever. In other words, given a question and </span><math alttext="k" class="ltx_Math" display="inline" id="S6.SS6.SSS1.p2.2.m2.1"><semantics id="S6.SS6.SSS1.p2.2.m2.1a"><mi id="S6.SS6.SSS1.p2.2.m2.1.1" mathcolor="#000000" xref="S6.SS6.SSS1.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS6.SSS1.p2.2.m2.1b"><ci id="S6.SS6.SSS1.p2.2.m2.1.1.cmml" xref="S6.SS6.SSS1.p2.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS6.SSS1.p2.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS6.SSS1.p2.2.m2.1d">italic_k</annotation></semantics></math><span class="ltx_text" id="S6.SS6.SSS1.p2.2.4" style="color:#000000;"> retrieved context items, this metric checks if one of these items contains all or part of the answer to the question.</span></p>
</div>
<div class="ltx_para" id="S6.SS6.SSS1.p3">
<p class="ltx_p" id="S6.SS6.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS6.SSS1.p3.1.1" style="color:#000000;">Answer Similarity</span><span class="ltx_text" id="S6.SS6.SSS1.p3.1.2" style="color:#000000;"> evaluates the semantic resemblance between the generated answer and the ground-truth answer. The generated and ground-truth answers are first embedded using an embedding function to create vectors. We employ the same embedding function that was used in Section </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.SS1" style="color:#000000;" title="V-A Corpus Creation ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-A</span></span></a><span class="ltx_text" id="S6.SS6.SSS1.p3.1.3" style="color:#000000;"> (Step 3) to create the document corpus. The semantic association between these vectors is then determined using cosine similarity. The metric value ranges between 0 and 1, with a higher value indicating greater resemblance and, thus, better accuracy.</span></p>
</div>
<div class="ltx_para" id="S6.SS6.SSS1.p4">
<p class="ltx_p" id="S6.SS6.SSS1.p4.1"><span class="ltx_text" id="S6.SS6.SSS1.p4.1.1" style="color:#000000;">In addition to the above automatically computed metrics, we manually evaluate a single iteration of all ground-truth questions answered by our best-performing pipeline (as per Recall@k and answer similarity), classifying the chatbot answers as correct, partially correct, or incorrect:</span></p>
</div>
<div class="ltx_para" id="S6.SS6.SSS1.p5">
<p class="ltx_p" id="S6.SS6.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S6.SS6.SSS1.p5.1.1" style="color:#000000;">Correct.</span><span class="ltx_text" id="S6.SS6.SSS1.p5.1.2" style="color:#000000;"> A (generated) answer is correct if it is semantically equivalent to the ground truth. An answer is deemed equivalent to the ground truth if (a) it does not omit any information present in the ground truth, </span><em class="ltx_emph ltx_font_italic" id="S6.SS6.SSS1.p5.1.3" style="color:#000000;">and</em><span class="ltx_text" id="S6.SS6.SSS1.p5.1.4" style="color:#000000;"> (b) it does not include any information absent from the ground truth.</span></p>
</div>
<div class="ltx_para" id="S6.SS6.SSS1.p6">
<p class="ltx_p" id="S6.SS6.SSS1.p6.1"><span class="ltx_text ltx_font_bold" id="S6.SS6.SSS1.p6.1.1" style="color:#000000;">Partially Correct.</span><span class="ltx_text" id="S6.SS6.SSS1.p6.1.2" style="color:#000000;"> An answer is classified as partially correct if it (a) includes extraneous information not present in the ground truth, (b) is incomplete, i.e. missing information that is in the ground truth, </span><em class="ltx_emph ltx_font_italic" id="S6.SS6.SSS1.p6.1.3" style="color:#000000;">or</em><span class="ltx_text" id="S6.SS6.SSS1.p6.1.4" style="color:#000000;"> (c) is both incomplete and contains extraneous information.</span></p>
</div>
<div class="ltx_para" id="S6.SS6.SSS1.p7">
<p class="ltx_p" id="S6.SS6.SSS1.p7.1"><span class="ltx_text ltx_font_bold" id="S6.SS6.SSS1.p7.1.1" style="color:#000000;">Incorrect.</span><span class="ltx_text" id="S6.SS6.SSS1.p7.1.2" style="color:#000000;"> An answer is incorrect if it has no content intersection with the ground truth.</span></p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S6.SS6.SSS2.4.1.1">VI-F</span>2 </span>Metric for RQ2</h4>
<div class="ltx_para" id="S6.SS6.SSS2.p1">
<p class="ltx_p" id="S6.SS6.SSS2.p1.1"><span class="ltx_text" id="S6.SS6.SSS2.p1.1.1" style="color:#000000;">To address RQ2, we measure the chatbot’s response time in seconds, defined as the duration from when the user submits a question to when the chatbot provides the full answer. The response time was computed over the experimental setup described in Section </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.SS3" style="color:#000000;" title="VI-C Experimental Setup ‣ VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-C</span></span></a><span class="ltx_text" id="S6.SS6.SSS2.p1.1.2" style="color:#000000;">. A basic measure of the usefulness of the chatbot would be for its response time to be less than that of a human expert answering the same question through messaging channels, e.g., on Teams.</span></p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS7">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS7.4.1.1">VI-G</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS7.5.2">Evaluation Procedure</span>
</h3>
<div class="ltx_para" id="S6.SS7.p1">
<p class="ltx_p" id="S6.SS7.p1.1"><span class="ltx_text" id="S6.SS7.p1.1.1" style="color:#000000;">Since our choice of LLM for answer generation is restricted to Llama 2 following Ericsson’s security guidelines, our evaluation procedure is focused on assessing the performance of this particular LLM when combined with various retrievers. We examine the following four retrievers in our study:</span></p>
</div>
<div class="ltx_para" id="S6.SS7.p2">
<p class="ltx_p" id="S6.SS7.p2.1"><span class="ltx_text ltx_font_italic" id="S6.SS7.p2.1.1" style="color:#000000;">(a) TF-IDF-based retriever.</span><span class="ltx_text" id="S6.SS7.p2.1.2" style="color:#000000;"> TF-IDF evaluates the importance of terms within a document relative to a corpus – in our context, the corpus created as described in Section </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.SS1" style="color:#000000;" title="V-A Corpus Creation ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-A</span></span></a><span class="ltx_text" id="S6.SS7.p2.1.3" style="color:#000000;"> – by considering both term frequency (TF) and inverse document frequency (IDF). Specifically, this retriever ranks context items against the user query based on the combined weight of term frequency in the item and rarity across the entire corpus, effectively identifying context items that contain frequently occurring important terms </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS7.p2.1.4.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib43" title="">43</a><span class="ltx_text" id="S6.SS7.p2.1.5.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS7.p2.1.6" style="color:#000000;">.</span></p>
</div>
<div class="ltx_para" id="S6.SS7.p3">
<p class="ltx_p" id="S6.SS7.p3.1"><span class="ltx_text ltx_font_italic" id="S6.SS7.p3.1.1" style="color:#000000;">(b) BM25-based retriever.</span><span class="ltx_text" id="S6.SS7.p3.1.2" style="color:#000000;"> BM25 is a probabilistic information retrieval method that attempts to overcome the drawbacks of TF-IDF by document length normalization </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS7.p3.1.3.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib39" title="">39</a><span class="ltx_text" id="S6.SS7.p3.1.4.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS7.p3.1.5" style="color:#000000;">. This normalization allows BM25 to account for varying document lengths and to prevent longer documents from having an unfair advantage in the retrieval process. This retriever, like the TF-IDF retriever, employs a domain corpus to identify the top-</span><math alttext="k" class="ltx_Math" display="inline" id="S6.SS7.p3.1.m1.1"><semantics id="S6.SS7.p3.1.m1.1a"><mi id="S6.SS7.p3.1.m1.1.1" mathcolor="#000000" xref="S6.SS7.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS7.p3.1.m1.1b"><ci id="S6.SS7.p3.1.m1.1.1.cmml" xref="S6.SS7.p3.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS7.p3.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS7.p3.1.m1.1d">italic_k</annotation></semantics></math><span class="ltx_text" id="S6.SS7.p3.1.6" style="color:#000000;"> relevant items to the user query.</span></p>
</div>
<div class="ltx_para" id="S6.SS7.p4">
<p class="ltx_p" id="S6.SS7.p4.2"><span class="ltx_text ltx_font_italic" id="S6.SS7.p4.2.1" style="color:#000000;">(c) Embeddings-based retriever.</span><span class="ltx_text" id="S6.SS7.p4.2.2" style="color:#000000;"> This retriever uses the embedding of the user query and the embeddings stored in the domain corpus (Section </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.SS1" style="color:#000000;" title="V-A Corpus Creation ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-A</span></span></a><span class="ltx_text" id="S6.SS7.p4.2.3" style="color:#000000;">) to retrieve the top-</span><math alttext="k" class="ltx_Math" display="inline" id="S6.SS7.p4.1.m1.1"><semantics id="S6.SS7.p4.1.m1.1a"><mi id="S6.SS7.p4.1.m1.1.1" mathcolor="#000000" xref="S6.SS7.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS7.p4.1.m1.1b"><ci id="S6.SS7.p4.1.m1.1.1.cmml" xref="S6.SS7.p4.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS7.p4.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS7.p4.1.m1.1d">italic_k</annotation></semantics></math><span class="ltx_text" id="S6.SS7.p4.2.4" style="color:#000000;"> relevant items. The retrieval process has three main steps. First, the retriever embeds the query using the same embedding function as that used for embedding the domain-specific corpus during the indexing process (Step 3 of Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.F1" style="color:#000000;" title="Figure 1 ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" id="S6.SS7.p4.2.5" style="color:#000000;">). Second, the retriever computes cosine similarity scores between the embedded query and the items in the vector database. Finally, the retriever selects and returns the top-</span><math alttext="k" class="ltx_Math" display="inline" id="S6.SS7.p4.2.m2.1"><semantics id="S6.SS7.p4.2.m2.1a"><mi id="S6.SS7.p4.2.m2.1.1" mathcolor="#000000" xref="S6.SS7.p4.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS7.p4.2.m2.1b"><ci id="S6.SS7.p4.2.m2.1.1.cmml" xref="S6.SS7.p4.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS7.p4.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS7.p4.2.m2.1d">italic_k</annotation></semantics></math><span class="ltx_text" id="S6.SS7.p4.2.6" style="color:#000000;"> most semantically similar items.</span></p>
</div>
<div class="ltx_para" id="S6.SS7.p5">
<p class="ltx_p" id="S6.SS7.p5.1"><span class="ltx_text ltx_font_italic" id="S6.SS7.p5.1.1" style="color:#000000;">(d) Ensemble retriever.</span><span class="ltx_text" id="S6.SS7.p5.1.2" style="color:#000000;"> This retriever calculates the simple average of the scores from the BM25- and embeddings-based retrievers and returns the top-</span><math alttext="k" class="ltx_Math" display="inline" id="S6.SS7.p5.1.m1.1"><semantics id="S6.SS7.p5.1.m1.1a"><mi id="S6.SS7.p5.1.m1.1.1" mathcolor="#000000" xref="S6.SS7.p5.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS7.p5.1.m1.1b"><ci id="S6.SS7.p5.1.m1.1.1.cmml" xref="S6.SS7.p5.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS7.p5.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS7.p5.1.m1.1d">italic_k</annotation></semantics></math><span class="ltx_text" id="S6.SS7.p5.1.3" style="color:#000000;"> items according to the averages.</span></p>
</div>
<div class="ltx_para" id="S6.SS7.p6">
<p class="ltx_p" id="S6.SS7.p6.1"><span class="ltx_text" id="S6.SS7.p6.1.1" style="color:#000000;">To prepare the evaluation dataset for each chatbot pipeline instantiated with a different retriever, we iterate over the ground-truth questions and store the chatbot’s answer along with the retrieved context items. The final evaluation dataset includes ground-truth questions and answers, the context item(s) containing the correct answer, the chatbot’s generated answer, and the retrieved context item(s).</span></p>
</div>
<div class="ltx_para" id="S6.SS7.p7">
<p class="ltx_p" id="S6.SS7.p7.1"><span class="ltx_text" id="S6.SS7.p7.1.1" style="color:#000000;">We compute the Recall@k and answer similarity metrics for each retriever using its respective dataset. To account for variations in model output, we conduct three evaluation iterations for each retriever and report averages for answer similarity, Recall@k, and response time.</span></p>
</div>
<div class="ltx_para" id="S6.SS7.p8">
<p class="ltx_p" id="S6.SS7.p8.1"><span class="ltx_text" id="S6.SS7.p8.1.1" style="color:#000000;">Using the Recall@k and answer similarity results, we determine the chatbot pipeline that has the best accuracy. After identifying the most accurate pipeline, we manually analyze the answers generated by one run of this pipeline over all the questions in the ground truth.
Through this analysis, we categorize the generated answers as correct, partially correct, or incorrect, as defined in Section </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.SS6" style="color:#000000;" title="VI-F Metrics ‣ VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-F</span></span></a><span class="ltx_text" id="S6.SS7.p8.1.2" style="color:#000000;">. We then conduct a qualitative error analysis to better understand the inaccuracies in the generated answers.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS8">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS8.4.1.1">VI-H</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS8.5.2">Answers to RQs</span>
</h3>
<figure class="ltx_table" id="S6.T1">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Automatically Computed Accuracy Results</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T1.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T1.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T1.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T1.3.1.1.1.1" style="color:#000000;">Metric</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T1.3.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T1.3.1.1.2.1" style="color:#000000;">TF-IDF</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T1.3.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T1.3.1.1.3.1" style="color:#000000;">BM25</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T1.3.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T1.3.1.1.4.1" style="color:#000000;">Embedding</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T1.3.1.1.5"><span class="ltx_text ltx_font_bold" id="S6.T1.3.1.1.5.1" style="color:#000000;">Ensemble</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T1.3.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S6.T1.3.2.1.1"><span class="ltx_text" id="S6.T1.3.2.1.1.1" style="color:#000000;">Recall@3</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T1.3.2.1.2"><span class="ltx_text" id="S6.T1.3.2.1.2.1" style="color:#000000;">91.60%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T1.3.2.1.3"><span class="ltx_text" id="S6.T1.3.2.1.3.1" style="color:#000000;">91.60%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T1.3.2.1.4"><span class="ltx_text" id="S6.T1.3.2.1.4.1" style="color:#000000;">92.75%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T1.3.2.1.5"><span class="ltx_text" id="S6.T1.3.2.1.5.1" style="color:#000000;">95.10%</span></td>
</tr>
<tr class="ltx_tr" id="S6.T1.3.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S6.T1.3.3.2.1"><span class="ltx_text" id="S6.T1.3.3.2.1.1" style="color:#000000;">Answer Similarity</span></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T1.3.3.2.2"><span class="ltx_text" id="S6.T1.3.3.2.2.1" style="color:#000000;">93.50%</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T1.3.3.2.3"><span class="ltx_text" id="S6.T1.3.3.2.3.1" style="color:#000000;">94.40%</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T1.3.3.2.4"><span class="ltx_text" id="S6.T1.3.3.2.4.1" style="color:#000000;">94.30%</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T1.3.3.2.5"><span class="ltx_text" id="S6.T1.3.3.2.5.1" style="color:#000000;">95.40%</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S6.SS8.p1">
<p class="ltx_p" id="S6.SS8.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS8.p1.1.1" style="color:#000000;">RQ1. How accurate is our chatbot?</span><span class="ltx_text" id="S6.SS8.p1.1.2" style="color:#000000;">
We answer this question using the metrics defined in Section </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.SS6" style="color:#000000;" title="VI-F Metrics ‣ VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-F</span></span></a><span class="ltx_text" id="S6.SS8.p1.1.3" style="color:#000000;">.
Table </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.T1" style="color:#000000;" title="TABLE I ‣ VI-H Answers to RQs ‣ VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">I</span></a><span class="ltx_text" id="S6.SS8.p1.1.4" style="color:#000000;"> presents the Recall@k and answer similarity scores with k </span><math alttext="=3" class="ltx_Math" display="inline" id="S6.SS8.p1.1.m1.1"><semantics id="S6.SS8.p1.1.m1.1a"><mrow id="S6.SS8.p1.1.m1.1.1" xref="S6.SS8.p1.1.m1.1.1.cmml"><mi id="S6.SS8.p1.1.m1.1.1.2" xref="S6.SS8.p1.1.m1.1.1.2.cmml"></mi><mo id="S6.SS8.p1.1.m1.1.1.1" mathcolor="#000000" xref="S6.SS8.p1.1.m1.1.1.1.cmml">=</mo><mn id="S6.SS8.p1.1.m1.1.1.3" mathcolor="#000000" xref="S6.SS8.p1.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS8.p1.1.m1.1b"><apply id="S6.SS8.p1.1.m1.1.1.cmml" xref="S6.SS8.p1.1.m1.1.1"><eq id="S6.SS8.p1.1.m1.1.1.1.cmml" xref="S6.SS8.p1.1.m1.1.1.1"></eq><csymbol cd="latexml" id="S6.SS8.p1.1.m1.1.1.2.cmml" xref="S6.SS8.p1.1.m1.1.1.2">absent</csymbol><cn id="S6.SS8.p1.1.m1.1.1.3.cmml" type="integer" xref="S6.SS8.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS8.p1.1.m1.1c">=3</annotation><annotation encoding="application/x-llamapun" id="S6.SS8.p1.1.m1.1d">= 3</annotation></semantics></math><span class="ltx_text" id="S6.SS8.p1.1.5" style="color:#000000;">. The rationale for selecting this specific value of k was discussed in Section </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.SS2" style="color:#000000;" title="V-B Chatbot Design ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-B</span></span></a><span class="ltx_text" id="S6.SS8.p1.1.6" style="color:#000000;">. As seen from the table, our pipeline performs well when instantiated with any one of the four retrievers. All pipelines fetch the correct context in more than 90% of the cases. The term-based TF-IDF and BM25 retrievers achieve virtually the same Recall@3 results, with an average of 91.60%. The embeddings-based retriever performs slightly better, with an average Recall@3 of 92.75%. </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S6.SS8.p1.1.7" style="color:#000000;">The ensemble retriever yields the best overall results, with Recall@3 averaging at 95.10%, slightly outperforming the TF-IDF, BM25 and embeddings-based retrievers by margins of 3.5%, 3.5%, and 2.35%, respectively.</em><span class="ltx_text" id="S6.SS8.p1.1.8" style="color:#000000;"></span></p>
</div>
<div class="ltx_para" id="S6.SS8.p2">
<p class="ltx_p" id="S6.SS8.p2.1"><span class="ltx_text" id="S6.SS8.p2.1.1" style="color:#000000;">As for answer similarity, we get comparable results for all retrievers with a difference of </span><math alttext="&lt;" class="ltx_Math" display="inline" id="S6.SS8.p2.1.m1.1"><semantics id="S6.SS8.p2.1.m1.1a"><mo id="S6.SS8.p2.1.m1.1.1" mathcolor="#000000" xref="S6.SS8.p2.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S6.SS8.p2.1.m1.1b"><lt id="S6.SS8.p2.1.m1.1.1.cmml" xref="S6.SS8.p2.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S6.SS8.p2.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S6.SS8.p2.1.m1.1d">&lt;</annotation></semantics></math><span class="ltx_text" id="S6.SS8.p2.1.2" style="color:#000000;">2% between the different pipeline instances.
Similar to Recall@3, the ensemble retriever-based pipeline yields the best performance for answer similarity, achieving an average score of 95.10%.</span></p>
</div>
<div class="ltx_para" id="S6.SS8.p3">
<p class="ltx_p" id="S6.SS8.p3.1"><span class="ltx_text" id="S6.SS8.p3.1.1" style="color:#000000;">Since the ensemble-retriever-based pipeline had the best performance across both Recall@3 and answer similarity metrics, we select it for the subsequent manual error analysis.</span></p>
</div>
<div class="ltx_para" id="S6.SS8.p4">
<p class="ltx_p" id="S6.SS8.p4.1"><span class="ltx_text" id="S6.SS8.p4.1.1" style="color:#000000;">Table </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.T2" style="color:#000000;" title="TABLE II ‣ VI-H Answers to RQs ‣ VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">II</span></a><span class="ltx_text" id="S6.SS8.p4.1.2" style="color:#000000;"> summarizes our error analysis results. </span><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S6.SS8.p4.1.3" style="color:#000000;">Out of the 72 answers generated by the chatbot in response to the ground-truth questions, 44 (61.11%) were correct, meaning that they were complete and did not contain any extraneous information. In all cases where the answer was correct, the retriever always retrieved the correct context item(s). A total of 19 (26.39%) answers were partially correct: 11 (15.28%) were missing vital information, 3 (4.17%) contained additional information that could be misleading, and 5 (6.94%) were both incomplete and further contained orthogonal information.
Finally, 9 (12.50%) answers were deemed incorrect as they had no overlap with the <span class="ltx_text" id="S6.SS8.p4.1.3.1">ground-truth answers.</span></em><span class="ltx_text" id="S6.SS8.p4.1.4" style="color:#000000;"></span></p>
</div>
<figure class="ltx_table" id="S6.T2">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Results of Manual Analysis</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle" id="S6.T2.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T2.3.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T2.3.1.1.1.1" style="color:#000000;">Correct</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S6.T2.3.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T2.3.1.1.2.1" style="color:#000000;">Partially Correct</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T2.3.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T2.3.1.1.3.1" style="color:#000000;">Incorrect</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.2.2">
<td class="ltx_td ltx_border_l ltx_border_r" id="S6.T2.3.2.2.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T2.3.2.2.2"><span class="ltx_text ltx_font_bold" id="S6.T2.3.2.2.2.1" style="color:#000000;">(A)</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T2.3.2.2.3"><span class="ltx_text ltx_font_bold" id="S6.T2.3.2.2.3.1" style="color:#000000;">(B)</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T2.3.2.2.4"><span class="ltx_text ltx_font_bold" id="S6.T2.3.2.2.4.1" style="color:#000000;">(C)</span></td>
<td class="ltx_td ltx_border_r" id="S6.T2.3.2.2.5"></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.3.3">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S6.T2.3.3.3.1"><span class="ltx_text" id="S6.T2.3.3.3.1.1" style="color:#000000;">44</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T2.3.3.3.2"><span class="ltx_text" id="S6.T2.3.3.3.2.1" style="color:#000000;">11</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T2.3.3.3.3"><span class="ltx_text" id="S6.T2.3.3.3.3.1" style="color:#000000;">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T2.3.3.3.4"><span class="ltx_text" id="S6.T2.3.3.3.4.1" style="color:#000000;">5</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S6.T2.3.3.3.5"><span class="ltx_text" id="S6.T2.3.3.3.5.1" style="color:#000000;">9</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_parbox ltx_align_center ltx_align_middle" id="S6.T2.4" style="width:260.2pt;"><span class="ltx_text ltx_font_italic" id="S6.T2.4.1" style="color:#000000;">(A) Only Incomplete, (B) Only Extraneous, (C) Both Incomplete and Extraneous</span></p>
</div>
</div>
</figure>
<figure class="ltx_figure" id="S6.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="1010" id="S6.F8.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Results of Error Analysis, including Root Causes of Inaccuracies and Their Prevalence in Our Case Study</figcaption>
</figure>
<div class="ltx_para" id="S6.SS8.p5">
<p class="ltx_p" id="S6.SS8.p5.1"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S6.SS8.p5.1.1" style="color:#000000;">Root causes of errors.</em><span class="ltx_text" id="S6.SS8.p5.1.2" style="color:#000000;"> We analyzed the incorrect and partially correct answers to identify the root causes of errors. Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.F8" style="color:#000000;" title="Figure 8 ‣ VI-H Answers to RQs ‣ VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">8</span></a><span class="ltx_text" id="S6.SS8.p5.1.3" style="color:#000000;"> presents the identified causes with examples and explanations.
Hallucinations were the most prevalent error, affecting 8 answers. The observed hallucinations occurred despite the relevant context items being retrieved. These items were nonetheless ignored by the LLM, which then proceeded to generate its own answer. This indicates that the LLM does not always follow the answer-generation prompt (Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.F6" style="color:#000000;" title="Figure 6 ‣ V-B Chatbot Design ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">6</span></a><span class="ltx_text" id="S6.SS8.p5.1.4" style="color:#000000;">).</span></p>
</div>
<div class="ltx_para" id="S6.SS8.p6">
<p class="ltx_p" id="S6.SS8.p6.1"><span class="ltx_text" id="S6.SS8.p6.1.1" style="color:#000000;">The second and third most major causes (tied in terms of the number of observations) were either that the retriever did not fetch the correct context items or that the LLM focused on the wrong context items. Each of these affected 7 answers. The absence of relevant context items was the most common reason for incomplete answers and occurred particularly for responses that spanned multiple context items. The second category of errors occurred when, despite the retriever having retrieved the correct context item(s), the LLM derived an answer from the incorrect context item(s) that were retrieved alongside the correct one(s) by the retriever.</span></p>
</div>
<div class="ltx_para" id="S6.SS8.p7">
<p class="ltx_p" id="S6.SS8.p7.1"><span class="ltx_text" id="S6.SS8.p7.1.1" style="color:#000000;">The LLM providing a generic answer or refraining from giving an answer were the other root causes identified, respectively affecting 4 and 2 answers. In the former case, the LLM’s tendency to reply with generic answers </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS8.p7.1.2.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib32" title="">32</a><span class="ltx_text" id="S6.SS8.p7.1.3.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS8.p7.1.4" style="color:#000000;"> prevents it from utilizing the context to provide tailored responses. In the latter case, the LLM refrains from answering the question due to one of the following reasons: an incorrect perception that there are ethical considerations, an inability to understand the question, or insufficient context.</span></p>
</div>
<div class="ltx_para" id="S6.SS8.p8">
<p class="ltx_p" id="S6.SS8.p8.1"><span class="ltx_text ltx_font_bold" id="S6.SS8.p8.1.1" style="color:#000000;">RQ2. What is our chatbot’s response time?</span><span class="ltx_text" id="S6.SS8.p8.1.2" style="color:#000000;">
We show in Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.F9" style="color:#000000;" title="Figure 9 ‣ VI-H Answers to RQs ‣ VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">9</span></a><span class="ltx_text" id="S6.SS8.p8.1.3" style="color:#000000;"> the response times (in seconds) for the four chatbot pipelines induced by the four choices of the retriever component as discussed in Section </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.SS7" style="color:#000000;" title="VI-G Evaluation Procedure ‣ VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-G</span></span></a><span class="ltx_text" id="S6.SS8.p8.1.4" style="color:#000000;">. Each boxplot in the figure represents the response times for one specific pipeline, with each data point being the response time for an individual question in the ground truth.
The TF-IDF- and BM25-based pipelines have average response times of 47.83 and 50.93 seconds respectively, with corresponding median values of 46 and 51 seconds. The embedding and ensemble pipelines had average response times of 46.61 and 43.69 seconds, with median values of 46 and 42.50 seconds respectively. Considering the modest hardware resources in our experimental setup (see Section </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.SS3" style="color:#000000;" title="VI-C Experimental Setup ‣ VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-C</span></span></a><span class="ltx_text" id="S6.SS8.p8.1.5" style="color:#000000;">), these response times seem reasonable. Reductions in execution time should be relatively easy to achieve with improved hardware, such as multiple GPUs.</span></p>
</div>
<figure class="ltx_figure" id="S6.F9">
<p class="ltx_p ltx_align_center" id="S6.F9.1"><span class="ltx_text" id="S6.F9.1.1"></span><span class="ltx_text" id="S6.F9.1.2" style="color:#000000;">
</span><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="551" id="S6.F9.1.g1" src="x9.png" width="830"/><span class="ltx_text" id="S6.F9.1.3" style="color:#000000;"></span></p>
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Response Times for Chatbot Instances Using Different Retrievers</figcaption>
</figure>
<div class="ltx_para" id="S6.SS8.p9">
<p class="ltx_p" id="S6.SS8.p9.1"><span class="ltx_text" id="S6.SS8.p9.1.1" style="color:#000000;">In our experiments, we found that the execution time is overwhelmingly dominated by the LLM during the query rewriting and answer generation phases (Steps 1 and 4 in Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S5.F2" style="color:#000000;" title="Figure 2 ‣ V-B Chatbot Design ‣ V Chatbot Development ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S6.SS8.p9.1.2" style="color:#000000;">). On average, these steps account for 99.91% of the total execution time. In contrast, the retrieval and prompt formation steps (Steps 2 and 3) take negligible time, contributing less than one-tenth of a percent to the overall execution. Based on the findings from RQ1, the ensemble-retriever-based pipeline has the highest accuracy. Given that the retrieval step has virtually no impact on execution time, and considering that the overall execution time of the ensemble-retriever-based pipeline is comparable to (or slightly better than) alternatives, as shown in Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.F9" style="color:#000000;" title="Figure 9 ‣ VI-H Answers to RQs ‣ VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">9</span></a><span class="ltx_text" id="S6.SS8.p9.1.3" style="color:#000000;">, we conclude that the ensemble-retriever-based pipeline is the optimal choice for our chatbot.</span></p>
</div>
<div class="ltx_para" id="S6.SS8.p10">
<p class="ltx_p" id="S6.SS8.p10.1"><span class="ltx_text" id="S6.SS8.p10.1.1" style="color:#000000;">To assess our chatbot’s usefulness, we analyzed the response times of experts to user queries in the Teams CI/CD channels at Ericsson, based on message timestamps. Our analysis revealed that the quickest response time from an expert in Teams was approximately 5 minutes.
Thus, while our chatbot could be faster, even on our modest hardware setup, it is sufficiently quick to be valuable to the querying party. That said, to conclusively determine if the response time is practical, a user study is needed, as a human interacting with a chatbot might expect a timely, synchronous conversation, whereas someone asking a question in a Teams channel with colleagues might find loosely asynchronous communication acceptable.
Regardless of the speed of the chatbot’s responses, there are inherent time-saving benefits for whoever has to answer.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS9">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS9.4.1.1">VI-I</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS9.5.2">Limitations and Validity Considerations</span>
</h3>
<div class="ltx_para" id="S6.SS9.p1">
<p class="ltx_p" id="S6.SS9.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S6.SS9.p1.1.1" style="color:#000000;">Limitations.</span><span class="ltx_text" id="S6.SS9.p1.1.2" style="color:#000000;"> Our experiments focused exclusively on Llama 2 as the LLM of choice. This decision was driven by security protocols set by our industry partner, which restrict the use of alternative models on their proprietary data at this time. While further benchmarking with different LLMs remains important, our exclusive use of Llama 2 is unlikely to be a significant limitation, as Llama 2 is a state-of-the-art model that reflects current open-source LLM capabilities well.</span></p>
</div>
<div class="ltx_para" id="S6.SS9.p2">
<p class="ltx_p" id="S6.SS9.p2.1"><span class="ltx_text" id="S6.SS9.p2.1.1" style="color:#000000;">We note that, without a user study, our current empirical results do not provide definitive evidence that our chatbot is ready for wide use at the host company, considering the inaccuracies observed and reported in RQ1. Nevertheless, we can make the following remarks which are likely to increase the likelihood of industrial adoption: First, the state-of-the-art in LLM technologies is evolving rapidly. We present a mature chatbot design that can be instantiated with newer LLMs. We anticipate that error rates will decrease further as more advanced LLMs become available, without our chatbot design being affected. Second, recognizing that chatbots are not infallible, engineers do not rely solely on chatbot responses for decision-making; they further consult with subject matter experts and use a range of other tools to guide their final decisions. These additional steps provide a safeguard against incorrect decisions stemming from inaccurate chatbot answers, thereby mitigating risks associated with using a chatbot that does not have perfect accuracy.</span></p>
</div>
<div class="ltx_para" id="S6.SS9.p3">
<p class="ltx_p" id="S6.SS9.p3.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S6.SS9.p3.1.1" style="color:#000000;">Threats to Validity.</span><span class="ltx_text" id="S6.SS9.p3.1.2" style="color:#000000;"> The validity aspects most pertinent to our evaluation are internal, construct and external validity.
Regarding </span><em class="ltx_emph ltx_font_italic" id="S6.SS9.p3.1.3" style="color:#000000;">internal validity</em><span class="ltx_text" id="S6.SS9.p3.1.4" style="color:#000000;">, we note that data leakage poses a validity threat for LLM-based solutions if the model is exposed to test data during training. In our evaluation, the dataset is proprietary, and we can assert with reasonable confidence that Llama 2 was not exposed to this data during its pre-training. Regarding </span><em class="ltx_emph ltx_font_italic" id="S6.SS9.p3.1.5" style="color:#000000;">construct validity</em><span class="ltx_text" id="S6.SS9.p3.1.6" style="color:#000000;">, we note that our accuracy metric for the retrieval step, Recall@k, aligns with empirical practices in the software engineering community </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS9.p3.1.7.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib13" title="">13</a><span class="ltx_text" id="S6.SS9.p3.1.8.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS9.p3.1.9" style="color:#000000;">. To assess our chatbot’s answer accuracy, we combine semantic similarity with manual human judgment. We opted for semantic similarity because it is more meaning-based than metrics like BLEU and ROUGE, which have been shown to have low correlation with human judgment </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS9.p3.1.10.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib29" title="">29</a><span class="ltx_text" id="S6.SS9.p3.1.11.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS9.p3.1.12" style="color:#000000;">. Developing suitable chatbot-evaluation metrics is an ongoing research topic </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.SS9.p3.1.13.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib14" title="">14</a><span class="ltx_text" id="S6.SS9.p3.1.14.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S6.SS9.p3.1.15" style="color:#000000;">. Given our detailed manual analysis and the close semantic similarity scores for the different chatbot pipelines evaluated, we do not anticipate major construct-validity threats due to our choices about metrics. Regarding </span><em class="ltx_emph ltx_font_italic" id="S6.SS9.p3.1.16" style="color:#000000;">external validity</em><span class="ltx_text" id="S6.SS9.p3.1.17" style="color:#000000;">, we acknowledge that our results are limited to a single case study. Although the industrial context of our work provides valuable insights, we recognize that generalizable conclusions cannot be drawn from a single case. Further case studies are necessary to explore broader applicability.</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Lessons Learned</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1"><span class="ltx_text" id="S7.p1.1.1" style="color:#000000;">Below, we discuss the lessons learned from developing our chatbot. We believe these lessons will be most useful for researchers and practitioners interested in understanding the challenges and limitations of current chatbot technologies.</span></p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1"><span class="ltx_text ltx_font_bold" id="S7.p2.1.1" style="color:#000000;">Beware of hallucinations; balance context carefully.</span><span class="ltx_text" id="S7.p2.1.2" style="color:#000000;"> Based on our error analysis summarized in Table </span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#S6.F8" style="color:#000000;" title="Figure 8 ‣ VI-H Answers to RQs ‣ VI Empirical Evaluation ‣ Developing a Llama-Based Chatbot for CI/CD Question Answering: A Case Study at Ericsson"><span class="ltx_text ltx_ref_tag">8</span></a><span class="ltx_text" id="S7.p2.1.3" style="color:#000000;">, the top three issues accounting for nearly 80% (22/28) of observed inaccuracies are hallucinations, missed context by the retriever, and discarded context by the LLM. Mitigating these issues requires steps to reduce hallucinations, improve retriever results, and ensure the LLM focuses on the correct retrieved items. However, it is important to note that inherent trade-offs exist here: simply increasing the amount of context to avoid missing information can exacerbate hallucinations or worsen the problem of the LLM focusing on incorrect context.</span></p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1"><span class="ltx_text ltx_font_bold" id="S7.p3.1.1" style="color:#000000;">Switching LLMs may require major adaptation effort.</span><span class="ltx_text" id="S7.p3.1.2" style="color:#000000;"> While, theoretically, one should be able to switch LLM models to newer versions or alternative LLM technologies, this was not our experience. Prompting styles and guidelines vary between different models, and even between different versions of the same model family, leading to various issues when these models are updated. For instance, as we explored the possibility of using a different LLM, it became apparent that our query rewriting component might require a major reevaluation. An important takeaway for us was that until further harmonization efforts occur across LLMs for interoperability, significant effort may be necessary to change the underlying LLM or to upgrade the models.</span></p>
</div>
<div class="ltx_para" id="S7.p4">
<p class="ltx_p" id="S7.p4.1"><span class="ltx_text ltx_font_bold" id="S7.p4.1.1" style="color:#000000;">Preprocessing is key for increasing accuracy.</span><span class="ltx_text" id="S7.p4.1.2" style="color:#000000;"> Preprocessing of documents requires careful consideration, as it can significantly impact chatbot accuracy. In our case study, initial testing showed suboptimal performance. Root-cause analysis revealed that despite maintaining an overlap to preserve contextual relationships, as suggested in the literature </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.p4.1.3.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib13" title="">13</a><span class="ltx_text" id="S7.p4.1.4.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S7.p4.1.5" style="color:#000000;">, the retriever failed to fetch all subsequent items necessary to answer the questions. This insight led us to include the document title and chunk number in each context item, which resulted in major accuracy improvements. The lesson learned here is that such examinations and improvements in preprocessing, although often time-consuming, should be prioritized as they can have a drastic impact on the success of a chatbot.</span></p>
</div>
<div class="ltx_para" id="S7.p5">
<p class="ltx_p" id="S7.p5.1"><span class="ltx_text ltx_font_bold" id="S7.p5.1.1" style="color:#000000;">Handling both domain-specific and general queries presents a challenge.</span><span class="ltx_text" id="S7.p5.1.2" style="color:#000000;"> An important tradeoff exists between supporting general and specific queries in a RAG-based chatbot architecture like ours, where instructing the chatbot to respond exclusively based on information retrieved by the retriever component enhances accuracy by reducing hallucinations but, at the same time, limits the chatbot’s ability to use its pre-trained knowledge. One could consider using a classifier to differentiate between domain-specific and general queries and direct different types of queries to different chatbots. However, we opted against this approach due to its potential complexity and error-proneness, as well as the undesirable consequences of misclassifying queries. In particular, we observed that distinguishing between a general query like “What is the process to migrate from one Kubernetes cluster to another?” and a specific one such as “What is the process to migrate from an Incubator to a Production cluster?” requires an understanding of domain terminology that may be tacit or not readily available or usable for query classification. An important lesson learned is that while query classification can enhance the usefulness of a chatbot, achieving the necessary level of accuracy for such classification remains challenging.</span></p>
</div>
<div class="ltx_para" id="S7.p6">
<p class="ltx_p" id="S7.p6.1"><span class="ltx_text ltx_font_bold" id="S7.p6.1.1" style="color:#000000;">Design for scalability.</span><span class="ltx_text" id="S7.p6.1.2" style="color:#000000;"> Many companies inevitably need to deploy chatbots internally due to privacy concerns. This makes scalability provisions a crucial consideration. While our chatbot was built primarily as a proof-of-concept for accuracy and was deployed on only one GPU for testing purposes, we ensured that our chatbot architecture supports horizontal scaling in a Kubernetes environment </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S7.p6.1.3.1" style="color:#000000;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2408.09277v1#bib.bib24" title="">24</a><span class="ltx_text" id="S7.p6.1.4.2" style="color:#000000;">]</span></cite><span class="ltx_text" id="S7.p6.1.5" style="color:#000000;">. If a chatbot is to become an adopted and widely used product, one must consider the possibility that several questions may need to be answered simultaneously. This requires multiple instances of the chatbot running in parallel, with each instance answering questions one at a time. We recommend that scalability needs be addressed early in chatbot design to identify bottlenecks and implement proper scalability mechanisms to support scaling up and down based on demand fluctuations.</span></p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">VIII </span><span class="ltx_text ltx_font_smallcaps" id="S8.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1"><span class="ltx_text" id="S8.p1.1.1" style="color:#000000;">In this paper, we presented our experience developing a question-answering chatbot for continuous integration and continuous delivery (CI/CD) at Ericsson. Through empirical evaluation using real-world CI/CD-related questions, we demonstrated that our chatbot provides useful answers to 87% of queries, with over 60% of the answers being fully correct. In future work, we plan to improve our chatbot’s usability based on feedback from Ericsson. While our prototype shows feasibility in a production environment, improvements are paramount. To this end, we plan to conduct a user study to provide deeper insights into usability. Key areas for improvement include enhancing the chatbot’s ability to handle complex logical queries and developing additional features, such as feedback loops, which are currently missing but are important for future success. In the longer term, we
would like to evolve our current chatbot from a question-answering tool into a smart agent capable of assisting with
the execution of CI/CD tasks based on user prompts.</span></p>
</div>
<section class="ltx_subsection" id="A0.SS1">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="A0.SS1.4.1.1">-A</span> </span><span class="ltx_text ltx_font_italic" id="A0.SS1.5.2">Parameters for Microsoft Teams Messages</span>
</h3>
<div class="ltx_para" id="A0.SS1.p1">
<p class="ltx_p" id="A0.SS1.p1.1"><span class="ltx_text" id="A0.SS1.p1.1.1" style="color:#000000;">As of this writing, Microsoft Teams retains the following 35 parameters (attributes) for each message sent:</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A0.SS1.p1.1.2" style="font-size:80%;color:#000000;">id,
etag,
messageType,
createdDateTime,
lastModifiedDateTime,
lastEditedDateTime,
importance,
locale,
webUrl,
attachments,
mentions,
reactions,
from.user.@odata.type,
userId,
userDisplayName,
userIdentityType,
tenantId,
contentType,
content,
channelIdentity.teamId,
channelIdentity.channelId,
subject,
deletedDateTime,
eventDetail.@odata.type,
eventDetail.channelId,
eventDetail.channelDescription,
eventDetail.initiator.application,
eventDetail.initiator.device,
eventDetail.initiator.user.@odata.type,
eventDetail.initiator.user.id,
eventDetail.initiator.user.displayName,
eventDetail.initiator.user.userIdentityType,
eventDetail.channelDisplayName,
eventDetail.visibleHistoryStartDateTime,
eventDetail.members</span><span class="ltx_text" id="A0.SS1.p1.1.3" style="color:#000000;">.</span></p>
</div>
</section>
<section class="ltx_subsection" id="A0.SS2">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="A0.SS2.4.1.1">-B</span> </span><span class="ltx_text ltx_font_italic" id="A0.SS2.5.2">Parameters for Microsoft Teams Replies</span>
</h3>
<div class="ltx_para" id="A0.SS2.p1">
<p class="ltx_p" id="A0.SS2.p1.1"><span class="ltx_text" id="A0.SS2.p1.1.1" style="color:#000000;">As of this writing, Microsoft Teams retains the following 43 parameters (attributes) for each reply to a message:</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A0.SS2.p1.1.2" style="font-size:80%;color:#000000;">id,
replyToId,
etag,
messageType,
createdDateTime,
lastModifiedDateTime,
lastEditedDateTime,
deletedDateTime,
subject,
summary,
chatId,
importance,
locale,
webUrl,
onBehalfOf,
policyViolation,
eventDetail,
attachments,
mentions,
reactions,
from.application,
from.device,
from.user.@odata.type,
userId,
userDisplayName,
userIdentityType,
tenantId,
contentType,
content,
channelIdentity.teamId,
channelIdentity.channelId,
from,
eventDetail.@odata.type,
eventDetail.callId,
eventDetail.callDuration,
eventDetail.callEventType,
eventDetail.callParticipants,
eventDetail.initiator.application,
eventDetail.initiator.device,
eventDetail.initiator.user.@odata.type,
eventDetail.initiator.user.id,
eventDetail.initiator.user.displayName,
eventDetail.initiator.user.userIdentityType</span><span class="ltx_text" id="A0.SS2.p1.1.3" style="color:#000000;">.</span></p>
</div>
</section>
</section>
<section class="ltx_appendix" id="Ax1">
<h2 class="ltx_title ltx_title_appendix" style="color:#000000;">Acknowledgements</h2>
<div class="ltx_para" id="Ax1.p1">
<p class="ltx_p" id="Ax1.p1.1"><span class="ltx_text" id="Ax1.p1.1.1" style="color:#000000;">We are grateful to the anonymous reviewers of the ICSME 2024 Industry Track for their insightful feedback. We further thank Dorian Gerdes for his valuable suggestions.</span></p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="color:#000000;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="color:#000000;">
Ahmad Abdellatif, Khaled Badran, Diego Elias Costa, and Emad Shihab.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.2.1" style="color:#000000;">A comparison of natural language understanding platforms for
chatbots in software engineering.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.3.1" style="color:#000000;">IEEE Transactions on Software Engineering</span><span class="ltx_text" id="bib.bib1.4.2" style="color:#000000;">, 48(8):3087–3102,
2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="color:#000000;">
Ahmad Abdellatif, Diego Costa, Khaled Badran, Rabe Abdalkareem, and Emad
Shihab.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.2.1" style="color:#000000;">Challenges in chatbot development: A study of Stack Overflow posts.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.3.1" style="color:#000000;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib2.4.2" style="color:#000000;">Proceedings of the 17th International Conference on Mining
Software Repositories (MSR 2020)</span><span class="ltx_text" id="bib.bib2.5.3" style="color:#000000;">, page 174–185. ACM, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="color:#000000;">
Samuel Abedu, Ahmad Abdellatif, and Emad Shihab.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.2.1" style="color:#000000;">LLM-based chatbots for mining software repositories: Challenges and
opportunities.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.3.1" style="color:#000000;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib3.4.2" style="color:#000000;">Proceedings of the 28th International Conference on
Evaluation and Assessment in Software Engineering (EASE 2024)</span><span class="ltx_text" id="bib.bib3.5.3" style="color:#000000;">, page
201–210. ACM, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="color:#000000;">
Atlassian.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.2.1" style="color:#000000;">Confluence.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="color:#000000;">https://www.atlassian.com/software/confluence</span><span class="ltx_text" id="bib.bib4.3.1" style="color:#000000;"> [last accessed:
August. 2024].
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="color:#000000;">
Jean-Marcel Belmont.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.2.1" style="color:#000000;">Hands-on continuous integration and delivery</span><span class="ltx_text" id="bib.bib5.3.2" style="color:#000000;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.4.1" style="color:#000000;">Packt Publishing, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="color:#000000;">
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.2.1" style="color:#000000;">Representation learning: A review and new perspectives.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.3.1" style="color:#000000;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="ltx_text" id="bib.bib6.4.2" style="color:#000000;">,
35(8):1798–1828, 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="color:#000000;">
Tom Brown et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.2.1" style="color:#000000;">Language models are few-shot learners.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.3.1" style="color:#000000;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib7.4.2" style="color:#000000;">Proceedings of the 34th International Conference on Neural
Information Processing Systems (NIPS 2020)</span><span class="ltx_text" id="bib.bib7.5.3" style="color:#000000;">. Curran Associates, Inc., 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="color:#000000;">
Chroma vectorstore.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="color:#000000;">https://www.trychroma.com/</span><span class="ltx_text" id="bib.bib8.2.1" style="color:#000000;"> [last accessed: August. 2024].
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="color:#000000;">
Gwendal Daniel and Jordi Cabot.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.2.1" style="color:#000000;">Applying model-driven engineering to the domain of chatbots: The
Xatkit experience.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.3.1" style="color:#000000;">Science of Computer Programming</span><span class="ltx_text" id="bib.bib9.4.2" style="color:#000000;">, 232, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="color:#000000;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.2.1" style="color:#000000;">BERT: Pre-training of deep bidirectional transformers for language
understanding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.3.1" style="color:#000000;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib10.4.2" style="color:#000000;">Proceedings of the Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language
Technologies, (NAACL-HLT 2019)</span><span class="ltx_text" id="bib.bib10.5.3" style="color:#000000;">. ACL, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="color:#000000;">
Ericsson.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.2.1" style="color:#000000;">CI/CD: Continuous software for continuous change.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="color:#000000;">https://www.ericsson.com/en/ci-cd</span><span class="ltx_text" id="bib.bib11.3.1" style="color:#000000;"> [last accessed: August.
2024].
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="color:#000000;">
ExplodingGradients.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.2.1" style="color:#000000;">Ragas library.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="color:#000000;">https://docs.ragas.io/en/stable/</span><span class="ltx_text" id="bib.bib12.3.1" style="color:#000000;"> [last accessed: August. 2024].
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="color:#000000;">
Saad Ezzini, Sallam Abualhaija, Chetan Arora, and Mehrdad Sabetzadeh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.2.1" style="color:#000000;">AI-based question answering assistance for analyzing
natural-language requirements.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.3.1" style="color:#000000;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib13.4.2" style="color:#000000;">Proceedings of the IEEE/ACM 45th International Conference on
Software Engineering (ICSE 2023)</span><span class="ltx_text" id="bib.bib13.5.3" style="color:#000000;">, pages 1277–1289. IEEE, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="color:#000000;">
Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, and Xiaojun Wan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.2.1" style="color:#000000;">LLM-based NLG evaluation: Current status and challenges.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.3.1" style="color:#000000;">arXiv e-prints (2402.01383)</span><span class="ltx_text" id="bib.bib14.4.2" style="color:#000000;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="color:#000000;">
GitHub.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.2.1" style="color:#000000;">Copilot.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="color:#000000;">https://github.com/features/copilot</span><span class="ltx_text" id="bib.bib15.3.1" style="color:#000000;"> [last accessed: August.
2024].
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="color:#000000;">
GitHub.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.2.1" style="color:#000000;">Copilot Chat.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="color:#000000;">https://docs.github.com/en/copilot/github-copilot-chat/copilot-chat-in-github</span><span class="ltx_text" id="bib.bib16.3.1" style="color:#000000;">
[last accessed: August. 2024].
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="color:#000000;">
Shabnam Hassani, Mehrdad Sabetzadeh, Daniel Amyot, and Jian Liao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.2.1" style="color:#000000;">Rethinking legal compliance automation: Opportunities with large
language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.3.1" style="color:#000000;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib17.4.2" style="color:#000000;">32nd IEEE International Requirements Engineering Conference
(RE 2024)</span><span class="ltx_text" id="bib.bib17.5.3" style="color:#000000;">, pages 432–440. IEEE, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="color:#000000;">
HuggingFace.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.2.1" style="color:#000000;">BAAI/bge-base-en embedding model.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="color:#000000;">https://huggingface.co/BAAI/bge-base-en</span><span class="ltx_text" id="bib.bib18.3.1" style="color:#000000;"> [last accessed: August.
2024].
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="color:#000000;">
HuggingFace.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.2.1" style="color:#000000;">BitsAndBytes library.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="color:#000000;">https://huggingface.co/docs/bitsandbytes/main/en/index</span><span class="ltx_text" id="bib.bib19.3.1" style="color:#000000;"> [last
accessed: August. 2024].
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="color:#000000;">
HuggingFace.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.2.1" style="color:#000000;">MTEB/Leaderboard.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="color:#000000;">https://huggingface.co/spaces/mteb/leaderboard</span><span class="ltx_text" id="bib.bib20.3.1" style="color:#000000;"> [last accessed:
August. 2024].
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="color:#000000;">
HuggingFace.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.2.1" style="color:#000000;">Transformers library.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="color:#000000;">https://huggingface.co/docs/transformers/en/index</span><span class="ltx_text" id="bib.bib21.3.1" style="color:#000000;"> [last
accessed: August. 2024].
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="color:#000000;">
Jez Humble and David Farley.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.2.1" style="color:#000000;">Continuous delivery: Reliable software releases through build,
test, and deployment automation</span><span class="ltx_text" id="bib.bib22.3.2" style="color:#000000;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.4.1" style="color:#000000;">Addison-Wesley, 2010.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="color:#000000;">
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni,
Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard
Grave.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.2.1" style="color:#000000;">Atlas: Few-shot learning with retrieval augmented language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.3.1" style="color:#000000;">The Journal of Machine Learning Research</span><span class="ltx_text" id="bib.bib23.4.2" style="color:#000000;">, 24(1), 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="color:#000000;">
Joao Paulo Karol Santos Nunes, Shiva Nejati, Mehrdad Sabetzadeh, and Elisa Yumi
Nakagawa.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.2.1" style="color:#000000;">Self-adaptive, requirements-driven autoscaling of microservices.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.3.1" style="color:#000000;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib24.4.2" style="color:#000000;">Proceedings of the 19th International Symposium on Software
Engineering for Adaptive and Self-Managing Systems (SEAMS 2024)</span><span class="ltx_text" id="bib.bib24.5.3" style="color:#000000;">, page
168–174. ACM, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="color:#000000;">
LangChain.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.2.1" style="color:#000000;">Contextual compression.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="color:#000000;">https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/contextual_compression/</span><span class="ltx_text" id="bib.bib25.3.1" style="color:#000000;">
[last accessed: August. 2024].
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="color:#000000;">
LangChain.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="color:#000000;">https://www.langchain.com/</span><span class="ltx_text" id="bib.bib26.2.1" style="color:#000000;"> [last accessed: August. 2024].
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="color:#000000;">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.2.1" style="color:#000000;">BART: Denoising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.3.1" style="color:#000000;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib27.4.2" style="color:#000000;">Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics (ACL 2020)</span><span class="ltx_text" id="bib.bib27.5.3" style="color:#000000;">, pages 7871–7880. ACL, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="color:#000000;">
Patrick Lewis et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.2.1" style="color:#000000;">Retrieval-augmented generation for knowledge-intensive NLP tasks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.3.1" style="color:#000000;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib28.4.2" style="color:#000000;">Proceedings of the 34th International Conference on Neural
Information Processing Systems (NIPS 2020)</span><span class="ltx_text" id="bib.bib28.5.3" style="color:#000000;">. Curran Associates Inc., 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="color:#000000;">
Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and
Joelle Pineau.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.2.1" style="color:#000000;">How NOT to evaluate your dialogue system: An empirical study of
unsupervised evaluation metrics for dialogue response generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.3.1" style="color:#000000;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib29.4.2" style="color:#000000;">Proceedings of the Conference on Empirical Methods in Natural
Language Processing, (EMNLP 2016)</span><span class="ltx_text" id="bib.bib29.5.3" style="color:#000000;">. ACL, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="color:#000000;">
Jiongnan Liu, Jiajie Jin, Zihan Wang, Jiehan Cheng, Zhicheng Dou, and Ji-Rong
Wen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.2.1" style="color:#000000;">RETA-LLM: A retrieval-augmented large language model toolkit.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.3.1" style="color:#000000;">arXiv e-prints (2306.05212)</span><span class="ltx_text" id="bib.bib30.4.2" style="color:#000000;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="color:#000000;">
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,
Fabio Petroni, and Percy Liang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.2.1" style="color:#000000;">Lost in the middle: How language models use long contexts.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.3.1" style="color:#000000;">Transactions of the Association for Computational Linguistics</span><span class="ltx_text" id="bib.bib31.4.2" style="color:#000000;">,
12:157–173, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.1.1" style="color:#000000;">
Dipeeka Luitel, Shabnam Hassani, and Mehrdad Sabetzadeh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.2.1" style="color:#000000;">Improving requirements completeness: Automated assistance through
large language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.3.1" style="color:#000000;">Requirements Engineering</span><span class="ltx_text" id="bib.bib32.4.2" style="color:#000000;">, 29(1):73–95, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.1.1" style="color:#000000;">
Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.2.1" style="color:#000000;">Query rewriting in retrieval-augmented large language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.3.1" style="color:#000000;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib33.4.2" style="color:#000000;">Proceedings of the 2023 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2023)</span><span class="ltx_text" id="bib.bib33.5.3" style="color:#000000;">, pages 5303–5315. ACL, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.1.1" style="color:#000000;">
Microsoft.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.2.1" style="color:#000000;">Graph APIs.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="color:#000000;">https://learn.microsoft.com/en-us/graph/use-the-api</span><span class="ltx_text" id="bib.bib34.3.1" style="color:#000000;"> [last
accessed: August. 2024].
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.1.1" style="color:#000000;">
Bhaskar Mitra and Nick Craswell.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.2.1" style="color:#000000;">An introduction to neural information retrieval.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.3.1" style="color:#000000;">Foundations and Trends in Information Retrieval</span><span class="ltx_text" id="bib.bib35.4.2" style="color:#000000;">, 13:1–126,
2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.1.1" style="color:#000000;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.2.1" style="color:#000000;">OpenAI Codex.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="color:#000000;">https://openai.com/index/openai-codex/</span><span class="ltx_text" id="bib.bib36.3.1" style="color:#000000;"> [last accessed: August.
2024].
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.1.1" style="color:#000000;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.2.1" style="color:#000000;">Prompt engineering.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="color:#000000;">https://platform.openai.com/docs/guides/prompt-engineering</span><span class="ltx_text" id="bib.bib37.3.1" style="color:#000000;">
[last accessed: August. 2024].
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.1.1" style="color:#000000;">
RayServe library.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="color:#000000;">https://docs.ray.io/en/latest/serve/index.html</span><span class="ltx_text" id="bib.bib38.2.1" style="color:#000000;"> [last accessed:
August. 2024].
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.1.1" style="color:#000000;">
Stephen Robertson and Hugo Zaragoza.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.2.1" style="color:#000000;">The probabilistic relevance framework: BM25 and beyond.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib39.3.1" style="color:#000000;">Foundations and Trends in Information Retrieval</span><span class="ltx_text" id="bib.bib39.4.2" style="color:#000000;">, 3:333–389,
2009.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.1.1" style="color:#000000;">
Sander Rossel.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.2.1" style="color:#000000;">Continuous integration, delivery, and deployment: Reliable and
faster software releases with automating builds, tests, and deployment</span><span class="ltx_text" id="bib.bib40.3.2" style="color:#000000;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.4.1" style="color:#000000;">Packt Publishing, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.1.1" style="color:#000000;">
Sina Semnani, Violet Yao, Heidi Zhang, and Monica Lam.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.2.1" style="color:#000000;">WikiChat: Stopping the hallucination of large language model
chatbots by few-shot grounding on Wikipedia.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.3.1" style="color:#000000;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib41.4.2" style="color:#000000;">Findings of the Association for Computational Linguistics:
EMNLP 2023</span><span class="ltx_text" id="bib.bib41.5.3" style="color:#000000;">. ACL, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.1.1" style="color:#000000;">
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.2.1" style="color:#000000;">Retrieval augmentation reduces hallucination in conversation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.3.1" style="color:#000000;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib42.4.2" style="color:#000000;">Findings of the Association for Computational Linguistics:
EMNLP 2021</span><span class="ltx_text" id="bib.bib42.5.3" style="color:#000000;">, pages 3784–3803. ACL, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.1.1" style="color:#000000;">
Karen Sparck Jones.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.2.1" style="color:#000000;">A statistical interpretation of term specificity and its application
in retrieval.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.3.1" style="color:#000000;">Journal of Documentation</span><span class="ltx_text" id="bib.bib43.4.2" style="color:#000000;">, 28(1):11–21, 1972.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.1.1" style="color:#000000;">
Hugo Touvron et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.2.1" style="color:#000000;">Llama 2: Open foundation and fine-tuned chat models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.3.1" style="color:#000000;">arXiv e-prints (arXiv:2307.09288)</span><span class="ltx_text" id="bib.bib44.4.2" style="color:#000000;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.1.1" style="color:#000000;">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia,
Ed H. Chi, Quoc V. Le, and Denny Zhou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.2.1" style="color:#000000;">Chain-of-thought prompting elicits reasoning in large language
models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.3.1" style="color:#000000;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib45.4.2" style="color:#000000;">Proceedings of the 36th International Conference on Neural
Information Processing Systems (NIPS 2022)</span><span class="ltx_text" id="bib.bib45.5.3" style="color:#000000;">, volume 35, pages 24824–24837.
Curran Associates, Inc., 2022.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Aug 17 19:09:20 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
