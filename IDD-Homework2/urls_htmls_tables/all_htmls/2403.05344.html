<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.05344] Federated Learning Method for Preserving Privacy in Face Recognition System</title><meta property="og:description" content="The state-of-the-art face recognition systems are typically trained on a single computer, utilizing extensive image datasets collected from various number of users. However, these datasets often contain sensitive perso…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Learning Method for Preserving Privacy in Face Recognition System">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Learning Method for Preserving Privacy in Face Recognition System">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.05344">

<!--Generated on Fri Apr  5 14:43:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
edge computation,  federated learning,  privacy,  secure aggregator,  face recognition
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated Learning Method for Preserving Privacy in Face Recognition System</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Enoch Solomon
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">Department of Computer Science</span>
<br class="ltx_break">Virginia State University 
<br class="ltx_break">Richmond, Virginia 
<br class="ltx_break">esolomon@vsu.edu

</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Abraham Woubie
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.1.id1" class="ltx_text ltx_font_italic">Silo AI</span>
<br class="ltx_break">Helsinki, Finland
<br class="ltx_break">abraham.zewoudie@silo.ai
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">The state-of-the-art face recognition systems are typically trained on a single computer, utilizing extensive image datasets collected from various number of users. However, these datasets often contain sensitive personal information that users may hesitate to disclose. To address potential privacy concerns, we explore the application of federated learning, both with and without secure aggregators, in the context of both supervised and unsupervised face recognition systems. Federated learning facilitates the training of a shared model without necessitating the sharing of individual private data, achieving this by training models on decentralized edge devices housing the data. In our proposed system, each edge device independently trains its own model, which is subsequently transmitted either to a secure aggregator or directly to the central server. To introduce diverse data without the need for data transmission, we employ generative adversarial networks to generate imposter data at the edge. Following this, the secure aggregator or central server combines these individual models to construct a global model, which is then relayed back to the edge devices. Experimental findings based on the CelebA datasets reveal that employing federated learning in both supervised and unsupervised face recognition systems offers dual benefits. Firstly, it safeguards privacy since the original data remains on the edge devices. Secondly, the experimental results demonstrate that the aggregated model yields nearly identical performance compared to the individual models, particularly when the federated model does not utilize a secure aggregator. Hence, our results shed light on the practical challenges associated with privacy-preserving face image training, particularly in terms of the balance between privacy and accuracy.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
edge computation, federated learning, privacy, secure aggregator, face recognition

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Face recognition is the process of automatically identifying or verifying the identity of an individual by analyzing facial patterns <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>. This technology has become an integral component in various security and authentication systems, ranging from smartphone unlocking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> to airport security checks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>. It encompasses two primary subfields: face identification and face verification. Face identification determines the identity of an individual, whereas face verification confirms or denies a claimed identity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>. Ensuring accurate face recognition is integral for granting access to services, as permissions should only be accorded following correct identification or verification.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The rapid advancement of machine learning (ML) and the availability of facial datasets have significantly enhanced the accuracy and performance of face recognition systems. Typically, face recognition systems employ machine learning techniques to train deep neural networks using facial data samples. Data samples are commonly gathered on end-devices like smartphones, while the model training takes place on a computationally robust centralized server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. This setup raises two major concerns. First, since the model is trained on user face data, it is crucial to prevent unauthorized access or data breaches to protect user privacy. Second, such systems involve a heavy data transmission phase, which can place significant stress on the communication infrastructure. Federated learning offers a solution to both issues. Rather than sending the raw sensitive data to the central server for training the centralized model, federated learning advocates for a distributed training approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. In this setup, each device maintains its own instance of the model and trains it using its local data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. After this local training, only the model updates are transmitted to the central server. The server then aggregates these updates and applies them to the global model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. This approach ensures that sensitive facial data remains local, strengthening privacy measures and minimizing data transfers. The most common aggregation strategy is federated averaging, which aggregates the updates using a weighted average<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Mobile phones and smart devices are examples of the modern distributed networks that generate huge amounts of data each day <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. As these devices become more powerful and concerns about data privacy grow, federated learning has emerged as a notable solution to keep data on the device and shift the network’s focus to the edge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
Various companies have adopted federated learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, highlighting its importance in applications that need privacy, especially when training data is spread across devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. The increasing demand for federated learning across various applications has led to the development of numerous tools, including TensorFlow Federated <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, Federated AI Technology Enabler <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, Leaf <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, and PaddleFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. While privacy-preserving data studies have been of interest since the 1970s, it is only in recent times that they are being extensively employed at a large scale <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. For instance, Google uses federated learning in Gboard <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and Android messages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, and Apple has incorporated it in iOS 13 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> for features like “Hey Siri” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">As mentioned previously, privacy concerns are considered as one of the major challenges in face and speaker recognition systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> as these systems usually involve the complete sharing of facial data, which can bring threatening consequences to people’s privacy. Federated learning emerges as a promising approach to address these concerns. Unlike conventional methods that require raw data to be sent to a central server for processing, federated learning enables model training directly on the user’s device, ensuring that sensitive facial data remains local. This decentralized approach not only enhances privacy but also reduces the need for data transmission, thereby saving bandwidth.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Thus, the main contribution of this work centers on the integration of federated learning techniques in the training of deep neural network-based face recognition classifiers, both supervised and unsupervised, with the primary aim of safeguarding user privacy. In the proposed system, each device independently trains its own model and subsequently transmits this local model to either a secure aggregator or directly to a central server. The secure aggregator, in turn, consolidates these local models originating from various devices, assembles a global model, and dispatches it to the central server. Alternatively, the central server may construct the global model directly, without intermediary interaction with the secure aggregator. Ultimately, the central server redistributes the global model to all participating devices.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The proposed system facilitates the training of a face recognition model grounded in a deep neural network. It accomplishes this by utilizing data stored exclusively on the respective devices, guaranteeing that this data never exits the confines of those devices. The cloud-based component of the system employs federated averaging to merge these local models, thereby forming a global model that is subsequently relayed back to the devices for inference. The implementation of secure aggregation ensures that, at a global level, individual updates from the devices remain completely confidential and inscrutable. As the edge devices solely transmit model updates, no raw data ever departs from the edge. Consequently, the aggregator only has access to a model trained for the purpose of identifying a local user, preserving the privacy of all other information pertaining to face image at the edge.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">A second innovation lies in the deployment of a generative adversarial network (GAN) to produce counterfeit data directly on edge devices. Employing a GAN eliminates the necessity of transmitting counterfeit data to the edge or accumulating such data at the edge itself. Transmitting counterfeit data could place a substantial strain on available bandwidth and, more crucially, expose potential vulnerabilities by revealing distinct information about the local user. Conversely, collecting counterfeit data at the edge could prove unfeasible.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">The potential applications of the proposed federated learning systems are diverse and could encompass tasks like smartphone-based learning. By collaboratively learning facial characteristics from a multitude of mobile devices, a shared statistical model can be developed to effectively identify individuals. Nevertheless, users might be hesitant to relinquish their data to a central server, driven by concerns about safeguarding their personal privacy. As a solution, federated learning can be employed to train a centralized, user-independent model without the need to expose or share private data.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">In the context of smartphone-based learning, a collective approach to learning face image characteristics from a substantial pool of mobile and similar devices enables the development of a unified statistical model for user identification. However, users may understandably harbor reservations about transferring their data to a central server, driven by privacy concerns. As a solution, federated learning can be employed to train a central, user-independent model without compromising the confidentiality of their private data.</p>
</div>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">In the context of learning across organizations, entities like universities can be likened to remote devices, each housing a wealth of student data. Nevertheless, universities are typically bound by stringent privacy regulations and practices, and any data leakage could lead to legal, administrative, or ethical complications. Federated learning offers a viable solution, allowing for confidential learning to take place across diverse devices and organizations while safeguarding the sensitive data of these institutions.</p>
</div>
<div id="S1.p11" class="ltx_para">
<p id="S1.p11.1" class="ltx_p">Our experiments conducted on the CelebA datasets reveal that federated learning brings notable advantages to both supervised and unsupervised facial recognition systems. This is achieved by avoiding the transmission of sensitive user data to central servers, while still delivering promising results when compared to individual local models. Consequently, the experimental outcomes provide a quantitative understanding of the challenges associated with the practical application of privacy-preserving training for facial recognition. These challenges are particularly evident in the trade-off between privacy and accuracy.</p>
</div>
<div id="S1.p12" class="ltx_para">
<p id="S1.p12.1" class="ltx_p">The remainder of this paper is structured as follows. Section <a href="#S3" title="III Proposed System ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> provides a detailed description of the proposed system’s architecture. The experimental results are outlined in Section <a href="#S4" title="IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, while Section <a href="#S5" title="V Conclusions ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> presents the conclusions drawn from the work.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Various methodologies have been proposed to enhance the privacy and security of face recognition systems.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Privacy-Preserving Face Recognition</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Various methods have been explored to safeguard facial data. Instead of using real images of individuals’ faces, the authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> propose to generate synthetic images by training a class-conditional GAN. The synthetic data generator was trained on the original face dataset and the identities of the individuals as class labels. The authors then generate the synthetic dataset to train the face recognition model. PriFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> is another method for privacy-preserving face recognition. PriFace uses locality-sensitive hashing to add randomness to facial data, preventing potential misuse or reconstruction of the images. Further, the work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> uses the Householder matrix to protect both model and facial data. This method combines additive and multiplicative perturbations, ensuring efficient user-side computations. For smart home settings, the authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> propose to protect the face feature data of the users using a face recognition approach that combines random matrix and BLS short signature with FaceNet. The work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> proposes to protect the privacy of the faces by encrypting them through affine transformation, which consists of permutation, diffusion and shift transformations. Another scheme presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> performs privacy-preserving face recognition scheme in the frequency domain. This scheme integrates an analysis network that gathers components with the same frequency from different blocks and a fast masking method to further secure the remaining frequency components. We can also highlight the work of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> in which the normalized face feature vectors are encrypted using the CKKS algorithm from the SEAL library. To save computation costs that comes with encryption of query face images, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> proposes to match an encrypted face query against clustered faces in the repository through a novel multi-matching scheme.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Other studies use local differential privacy to ensure that individual data points cannot be reverse-engineered or identified. The work of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> proposes a general privacy protection framework for edge-based face recognition systems. This is done through a local differential privacy algorithm based on the proportion difference of feature information. Furthermore, identity authentication and hash technology are used to ensure the legitimacy of the terminal device and the integrity of the face image in the data acquisition phase.
The authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> introduce a new privacy-preserving face recognition protocol referred to as Privacy using EigEnface
Perturbation (PEEP). This protocol uses local differential privacy to apply perturbation to Eigenfaces. Only the perturbed data is stored in third-party servers, and a standard Eigenface recognition algorithm is run on this data.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Federated Learning for Face Recognition</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Multiple methods use federated learning to ensure privacy-preserving face recognition. The authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> introduce PrivacyFace, which leverages privacy-agnostic clusters during model training. These clusters are indifferent to privacy concerns (i.e., the data in these clusters do not reveal sensitive personal information). PrivacyFace consists of two main components: the Differently Private Local Clustering (DPLC) algorithm, which derives privacy-independent group features, and a consensus-aware face recognition loss that refines the global feature space distribution using these desensitized group features. FedFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> presents a federated learning framework that learns from face images across multiple clients without sharing the images with other clients or a central host. Each client, typically a mobile device, contains face images of only its owner. Face Presentation Attack Detection (FedPAD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> aims to develop generalized fPAD models while ensuring data privacy. Each data owner trains a local fPAD model, and a server aggregates these models without accessing individual private data. Once the global model is refined, it’s used for fPAD inference. FedFR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> is a federated learning-based framework for privacy-aware generic face representation. The framework optimizes personalized models for clients using the Decoupled Feature Customization module, improving both the global model for face representation and the personalized user model.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Proposed System</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The challenge of federated learning revolves around the task of constructing a unified global statistical model using data distributed across a limited number to possibly millions of remote devices. More specifically, the primary objective commonly pursued in federated learning is the minimization of the following objective function:</p>
</div>
<div id="S3.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.4" class="ltx_Math" alttext="\min_{w}F(w),\text{ where }F(w):=\sum_{k=1}^{m}p_{k}F_{k}(w)," display="block"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4.1" xref="S3.E1.m1.4.4.1.1.cmml"><mrow id="S3.E1.m1.4.4.1.1" xref="S3.E1.m1.4.4.1.1.cmml"><mrow id="S3.E1.m1.4.4.1.1.2.2" xref="S3.E1.m1.4.4.1.1.2.3.cmml"><mrow id="S3.E1.m1.4.4.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.4.4.1.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.1.2.cmml"><munder id="S3.E1.m1.4.4.1.1.1.1.1.2.1" xref="S3.E1.m1.4.4.1.1.1.1.1.2.1.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.1.2.1.2" xref="S3.E1.m1.4.4.1.1.1.1.1.2.1.2.cmml">min</mi><mi id="S3.E1.m1.4.4.1.1.1.1.1.2.1.3" xref="S3.E1.m1.4.4.1.1.1.1.1.2.1.3.cmml">w</mi></munder><mo lspace="0.167em" id="S3.E1.m1.4.4.1.1.1.1.1.2a" xref="S3.E1.m1.4.4.1.1.1.1.1.2.cmml">⁡</mo><mi id="S3.E1.m1.4.4.1.1.1.1.1.2.2" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2.cmml">F</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.1.3.2" xref="S3.E1.m1.4.4.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.1.1.3.2.1" xref="S3.E1.m1.4.4.1.1.1.1.1.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">w</mi><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.1.1.3.2.2" xref="S3.E1.m1.4.4.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.4.1.1.2.2.3" xref="S3.E1.m1.4.4.1.1.2.3.cmml">,</mo><mrow id="S3.E1.m1.4.4.1.1.2.2.2" xref="S3.E1.m1.4.4.1.1.2.2.2.cmml"><mtext id="S3.E1.m1.4.4.1.1.2.2.2.2" xref="S3.E1.m1.4.4.1.1.2.2.2.2a.cmml"> where </mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.1.2.2.2.1" xref="S3.E1.m1.4.4.1.1.2.2.2.1.cmml">​</mo><mi id="S3.E1.m1.4.4.1.1.2.2.2.3" xref="S3.E1.m1.4.4.1.1.2.2.2.3.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.1.2.2.2.1a" xref="S3.E1.m1.4.4.1.1.2.2.2.1.cmml">​</mo><mrow id="S3.E1.m1.4.4.1.1.2.2.2.4.2" xref="S3.E1.m1.4.4.1.1.2.2.2.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.1.1.2.2.2.4.2.1" xref="S3.E1.m1.4.4.1.1.2.2.2.cmml">(</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">w</mi><mo rspace="0.278em" stretchy="false" id="S3.E1.m1.4.4.1.1.2.2.2.4.2.2" xref="S3.E1.m1.4.4.1.1.2.2.2.cmml">)</mo></mrow></mrow></mrow><mo rspace="0.111em" id="S3.E1.m1.4.4.1.1.3" xref="S3.E1.m1.4.4.1.1.3.cmml">:=</mo><mrow id="S3.E1.m1.4.4.1.1.4" xref="S3.E1.m1.4.4.1.1.4.cmml"><munderover id="S3.E1.m1.4.4.1.1.4.1" xref="S3.E1.m1.4.4.1.1.4.1.cmml"><mo movablelimits="false" id="S3.E1.m1.4.4.1.1.4.1.2.2" xref="S3.E1.m1.4.4.1.1.4.1.2.2.cmml">∑</mo><mrow id="S3.E1.m1.4.4.1.1.4.1.2.3" xref="S3.E1.m1.4.4.1.1.4.1.2.3.cmml"><mi id="S3.E1.m1.4.4.1.1.4.1.2.3.2" xref="S3.E1.m1.4.4.1.1.4.1.2.3.2.cmml">k</mi><mo id="S3.E1.m1.4.4.1.1.4.1.2.3.1" xref="S3.E1.m1.4.4.1.1.4.1.2.3.1.cmml">=</mo><mn id="S3.E1.m1.4.4.1.1.4.1.2.3.3" xref="S3.E1.m1.4.4.1.1.4.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.4.4.1.1.4.1.3" xref="S3.E1.m1.4.4.1.1.4.1.3.cmml">m</mi></munderover><mrow id="S3.E1.m1.4.4.1.1.4.2" xref="S3.E1.m1.4.4.1.1.4.2.cmml"><msub id="S3.E1.m1.4.4.1.1.4.2.2" xref="S3.E1.m1.4.4.1.1.4.2.2.cmml"><mi id="S3.E1.m1.4.4.1.1.4.2.2.2" xref="S3.E1.m1.4.4.1.1.4.2.2.2.cmml">p</mi><mi id="S3.E1.m1.4.4.1.1.4.2.2.3" xref="S3.E1.m1.4.4.1.1.4.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.1.4.2.1" xref="S3.E1.m1.4.4.1.1.4.2.1.cmml">​</mo><msub id="S3.E1.m1.4.4.1.1.4.2.3" xref="S3.E1.m1.4.4.1.1.4.2.3.cmml"><mi id="S3.E1.m1.4.4.1.1.4.2.3.2" xref="S3.E1.m1.4.4.1.1.4.2.3.2.cmml">F</mi><mi id="S3.E1.m1.4.4.1.1.4.2.3.3" xref="S3.E1.m1.4.4.1.1.4.2.3.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.1.4.2.1a" xref="S3.E1.m1.4.4.1.1.4.2.1.cmml">​</mo><mrow id="S3.E1.m1.4.4.1.1.4.2.4.2" xref="S3.E1.m1.4.4.1.1.4.2.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.1.1.4.2.4.2.1" xref="S3.E1.m1.4.4.1.1.4.2.cmml">(</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">w</mi><mo stretchy="false" id="S3.E1.m1.4.4.1.1.4.2.4.2.2" xref="S3.E1.m1.4.4.1.1.4.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.4.4.1.2" xref="S3.E1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.1.1.cmml" xref="S3.E1.m1.4.4.1"><csymbol cd="latexml" id="S3.E1.m1.4.4.1.1.3.cmml" xref="S3.E1.m1.4.4.1.1.3">assign</csymbol><list id="S3.E1.m1.4.4.1.1.2.3.cmml" xref="S3.E1.m1.4.4.1.1.2.2"><apply id="S3.E1.m1.4.4.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1"><times id="S3.E1.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.1"></times><apply id="S3.E1.m1.4.4.1.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2"><apply id="S3.E1.m1.4.4.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.1.1.1.2.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.1">subscript</csymbol><min id="S3.E1.m1.4.4.1.1.1.1.1.2.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.1.2"></min><ci id="S3.E1.m1.4.4.1.1.1.1.1.2.1.3.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.1.3">𝑤</ci></apply><ci id="S3.E1.m1.4.4.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1.2.2">𝐹</ci></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑤</ci></apply><apply id="S3.E1.m1.4.4.1.1.2.2.2.cmml" xref="S3.E1.m1.4.4.1.1.2.2.2"><times id="S3.E1.m1.4.4.1.1.2.2.2.1.cmml" xref="S3.E1.m1.4.4.1.1.2.2.2.1"></times><ci id="S3.E1.m1.4.4.1.1.2.2.2.2a.cmml" xref="S3.E1.m1.4.4.1.1.2.2.2.2"><mtext id="S3.E1.m1.4.4.1.1.2.2.2.2.cmml" xref="S3.E1.m1.4.4.1.1.2.2.2.2"> where </mtext></ci><ci id="S3.E1.m1.4.4.1.1.2.2.2.3.cmml" xref="S3.E1.m1.4.4.1.1.2.2.2.3">𝐹</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝑤</ci></apply></list><apply id="S3.E1.m1.4.4.1.1.4.cmml" xref="S3.E1.m1.4.4.1.1.4"><apply id="S3.E1.m1.4.4.1.1.4.1.cmml" xref="S3.E1.m1.4.4.1.1.4.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.4.1.1.cmml" xref="S3.E1.m1.4.4.1.1.4.1">superscript</csymbol><apply id="S3.E1.m1.4.4.1.1.4.1.2.cmml" xref="S3.E1.m1.4.4.1.1.4.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.4.1.2.1.cmml" xref="S3.E1.m1.4.4.1.1.4.1">subscript</csymbol><sum id="S3.E1.m1.4.4.1.1.4.1.2.2.cmml" xref="S3.E1.m1.4.4.1.1.4.1.2.2"></sum><apply id="S3.E1.m1.4.4.1.1.4.1.2.3.cmml" xref="S3.E1.m1.4.4.1.1.4.1.2.3"><eq id="S3.E1.m1.4.4.1.1.4.1.2.3.1.cmml" xref="S3.E1.m1.4.4.1.1.4.1.2.3.1"></eq><ci id="S3.E1.m1.4.4.1.1.4.1.2.3.2.cmml" xref="S3.E1.m1.4.4.1.1.4.1.2.3.2">𝑘</ci><cn type="integer" id="S3.E1.m1.4.4.1.1.4.1.2.3.3.cmml" xref="S3.E1.m1.4.4.1.1.4.1.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.4.4.1.1.4.1.3.cmml" xref="S3.E1.m1.4.4.1.1.4.1.3">𝑚</ci></apply><apply id="S3.E1.m1.4.4.1.1.4.2.cmml" xref="S3.E1.m1.4.4.1.1.4.2"><times id="S3.E1.m1.4.4.1.1.4.2.1.cmml" xref="S3.E1.m1.4.4.1.1.4.2.1"></times><apply id="S3.E1.m1.4.4.1.1.4.2.2.cmml" xref="S3.E1.m1.4.4.1.1.4.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.4.2.2.1.cmml" xref="S3.E1.m1.4.4.1.1.4.2.2">subscript</csymbol><ci id="S3.E1.m1.4.4.1.1.4.2.2.2.cmml" xref="S3.E1.m1.4.4.1.1.4.2.2.2">𝑝</ci><ci id="S3.E1.m1.4.4.1.1.4.2.2.3.cmml" xref="S3.E1.m1.4.4.1.1.4.2.2.3">𝑘</ci></apply><apply id="S3.E1.m1.4.4.1.1.4.2.3.cmml" xref="S3.E1.m1.4.4.1.1.4.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.1.4.2.3.1.cmml" xref="S3.E1.m1.4.4.1.1.4.2.3">subscript</csymbol><ci id="S3.E1.m1.4.4.1.1.4.2.3.2.cmml" xref="S3.E1.m1.4.4.1.1.4.2.3.2">𝐹</ci><ci id="S3.E1.m1.4.4.1.1.4.2.3.3.cmml" xref="S3.E1.m1.4.4.1.1.4.2.3.3">𝑘</ci></apply><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝑤</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">\min_{w}F(w),\text{ where }F(w):=\sum_{k=1}^{m}p_{k}F_{k}(w),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.p2.6" class="ltx_p">where <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">m</annotation></semantics></math> represents the total number of devices, <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="F_{k}" display="inline"><semantics id="S3.p2.2.m2.1a"><msub id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml"><mi id="S3.p2.2.m2.1.1.2" xref="S3.p2.2.m2.1.1.2.cmml">F</mi><mi id="S3.p2.2.m2.1.1.3" xref="S3.p2.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.1.cmml" xref="S3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.p2.2.m2.1.1.2.cmml" xref="S3.p2.2.m2.1.1.2">𝐹</ci><ci id="S3.p2.2.m2.1.1.3.cmml" xref="S3.p2.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">F_{k}</annotation></semantics></math> denotes the local objective function for the <math id="S3.p2.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.p2.3.m3.1a"><mi id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><ci id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">k</annotation></semantics></math>th device, and <math id="S3.p2.4.m4.1" class="ltx_Math" alttext="p_{k}" display="inline"><semantics id="S3.p2.4.m4.1a"><msub id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml"><mi id="S3.p2.4.m4.1.1.2" xref="S3.p2.4.m4.1.1.2.cmml">p</mi><mi id="S3.p2.4.m4.1.1.3" xref="S3.p2.4.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><apply id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p2.4.m4.1.1.1.cmml" xref="S3.p2.4.m4.1.1">subscript</csymbol><ci id="S3.p2.4.m4.1.1.2.cmml" xref="S3.p2.4.m4.1.1.2">𝑝</ci><ci id="S3.p2.4.m4.1.1.3.cmml" xref="S3.p2.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">p_{k}</annotation></semantics></math> signifies the relative impact of each device with <math id="S3.p2.5.m5.1" class="ltx_Math" alttext="p_{k}\geq 0" display="inline"><semantics id="S3.p2.5.m5.1a"><mrow id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml"><msub id="S3.p2.5.m5.1.1.2" xref="S3.p2.5.m5.1.1.2.cmml"><mi id="S3.p2.5.m5.1.1.2.2" xref="S3.p2.5.m5.1.1.2.2.cmml">p</mi><mi id="S3.p2.5.m5.1.1.2.3" xref="S3.p2.5.m5.1.1.2.3.cmml">k</mi></msub><mo id="S3.p2.5.m5.1.1.1" xref="S3.p2.5.m5.1.1.1.cmml">≥</mo><mn id="S3.p2.5.m5.1.1.3" xref="S3.p2.5.m5.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><apply id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1"><geq id="S3.p2.5.m5.1.1.1.cmml" xref="S3.p2.5.m5.1.1.1"></geq><apply id="S3.p2.5.m5.1.1.2.cmml" xref="S3.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.p2.5.m5.1.1.2.1.cmml" xref="S3.p2.5.m5.1.1.2">subscript</csymbol><ci id="S3.p2.5.m5.1.1.2.2.cmml" xref="S3.p2.5.m5.1.1.2.2">𝑝</ci><ci id="S3.p2.5.m5.1.1.2.3.cmml" xref="S3.p2.5.m5.1.1.2.3">𝑘</ci></apply><cn type="integer" id="S3.p2.5.m5.1.1.3.cmml" xref="S3.p2.5.m5.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">p_{k}\geq 0</annotation></semantics></math> and <math id="S3.p2.6.m6.1" class="ltx_Math" alttext="\sum_{k=1}^{m}p_{k}=1" display="inline"><semantics id="S3.p2.6.m6.1a"><mrow id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml"><mrow id="S3.p2.6.m6.1.1.2" xref="S3.p2.6.m6.1.1.2.cmml"><msubsup id="S3.p2.6.m6.1.1.2.1" xref="S3.p2.6.m6.1.1.2.1.cmml"><mo id="S3.p2.6.m6.1.1.2.1.2.2" xref="S3.p2.6.m6.1.1.2.1.2.2.cmml">∑</mo><mrow id="S3.p2.6.m6.1.1.2.1.2.3" xref="S3.p2.6.m6.1.1.2.1.2.3.cmml"><mi id="S3.p2.6.m6.1.1.2.1.2.3.2" xref="S3.p2.6.m6.1.1.2.1.2.3.2.cmml">k</mi><mo id="S3.p2.6.m6.1.1.2.1.2.3.1" xref="S3.p2.6.m6.1.1.2.1.2.3.1.cmml">=</mo><mn id="S3.p2.6.m6.1.1.2.1.2.3.3" xref="S3.p2.6.m6.1.1.2.1.2.3.3.cmml">1</mn></mrow><mi id="S3.p2.6.m6.1.1.2.1.3" xref="S3.p2.6.m6.1.1.2.1.3.cmml">m</mi></msubsup><msub id="S3.p2.6.m6.1.1.2.2" xref="S3.p2.6.m6.1.1.2.2.cmml"><mi id="S3.p2.6.m6.1.1.2.2.2" xref="S3.p2.6.m6.1.1.2.2.2.cmml">p</mi><mi id="S3.p2.6.m6.1.1.2.2.3" xref="S3.p2.6.m6.1.1.2.2.3.cmml">k</mi></msub></mrow><mo id="S3.p2.6.m6.1.1.1" xref="S3.p2.6.m6.1.1.1.cmml">=</mo><mn id="S3.p2.6.m6.1.1.3" xref="S3.p2.6.m6.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><apply id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1"><eq id="S3.p2.6.m6.1.1.1.cmml" xref="S3.p2.6.m6.1.1.1"></eq><apply id="S3.p2.6.m6.1.1.2.cmml" xref="S3.p2.6.m6.1.1.2"><apply id="S3.p2.6.m6.1.1.2.1.cmml" xref="S3.p2.6.m6.1.1.2.1"><csymbol cd="ambiguous" id="S3.p2.6.m6.1.1.2.1.1.cmml" xref="S3.p2.6.m6.1.1.2.1">superscript</csymbol><apply id="S3.p2.6.m6.1.1.2.1.2.cmml" xref="S3.p2.6.m6.1.1.2.1"><csymbol cd="ambiguous" id="S3.p2.6.m6.1.1.2.1.2.1.cmml" xref="S3.p2.6.m6.1.1.2.1">subscript</csymbol><sum id="S3.p2.6.m6.1.1.2.1.2.2.cmml" xref="S3.p2.6.m6.1.1.2.1.2.2"></sum><apply id="S3.p2.6.m6.1.1.2.1.2.3.cmml" xref="S3.p2.6.m6.1.1.2.1.2.3"><eq id="S3.p2.6.m6.1.1.2.1.2.3.1.cmml" xref="S3.p2.6.m6.1.1.2.1.2.3.1"></eq><ci id="S3.p2.6.m6.1.1.2.1.2.3.2.cmml" xref="S3.p2.6.m6.1.1.2.1.2.3.2">𝑘</ci><cn type="integer" id="S3.p2.6.m6.1.1.2.1.2.3.3.cmml" xref="S3.p2.6.m6.1.1.2.1.2.3.3">1</cn></apply></apply><ci id="S3.p2.6.m6.1.1.2.1.3.cmml" xref="S3.p2.6.m6.1.1.2.1.3">𝑚</ci></apply><apply id="S3.p2.6.m6.1.1.2.2.cmml" xref="S3.p2.6.m6.1.1.2.2"><csymbol cd="ambiguous" id="S3.p2.6.m6.1.1.2.2.1.cmml" xref="S3.p2.6.m6.1.1.2.2">subscript</csymbol><ci id="S3.p2.6.m6.1.1.2.2.2.cmml" xref="S3.p2.6.m6.1.1.2.2.2">𝑝</ci><ci id="S3.p2.6.m6.1.1.2.2.3.cmml" xref="S3.p2.6.m6.1.1.2.2.3">𝑘</ci></apply></apply><cn type="integer" id="S3.p2.6.m6.1.1.3.cmml" xref="S3.p2.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">\sum_{k=1}^{m}p_{k}=1</annotation></semantics></math>.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Federated learning empowers the distributed training of face recognition models, accommodating a diverse range of client devices. As illustrated in Fig. 1, the envisioned federated learning system for face recognition functions across three key locations: edge devices, a secure aggregator, and a central main server. These edge devices encompass a variety of hardware, including mobile phones, laptops, and similar devices. In contrast, the aggregator and main server typically operate as cloud-based services.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Fig. 1 depicts the training of a central model using a distributed dataset. Here, a multitude of nodes, which could represent user devices, possess subsets of data with varying sizes. At the device level, each node computes a model update, which is subsequently conveyed to a central server. During each training iteration, a substantial volume of these updates or gradients is amalgamated at the central server. The central server then derives a global update for the central model by computing the average of these individual local updates.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">Note that the architecture of the proposed system remains consistent for both supervised and unsupervised face recognition systems. The key distinction lies in the utilization of labels for training in supervised systems, while unsupervised systems do not rely on labels for training individual face recognition models.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">While it is feasible to train individual face recognition models in the supervised system using only client images of a given person on a specific device, our preference is to enhance model robustness and improve the ability to distinguish impostor images. To achieve this, as depicted in Fig. 1, we employ two distinct methods for generating impostor image data for each individual on the edge device:</p>
</div>
<div id="S3.p7" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">In the first method, we randomly select the image of other persons from the CelebA dataset as impostor image data for a given person.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">In the second method, we train a GAN model to generate impostor image data as it is not always easy to find image data of different persons in edge devices. Thus, we use the work of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> to train a GAN model on the CelebA dataset. Once the impostor images are generated using the trained GAN model, they are combined with client image data to train an individual face recognition model on a specific edge device.</p>
</div>
</li>
</ul>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2403.05344/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="417" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The proposed face recognition system incorporates federated learning. Through the implementation of a secure aggregator, we empower a collective of inherently untrusting devices to collaborate and calculate an aggregate value without disclosing their individual private data.</figcaption>
</figure>
<div id="S3.p8" class="ltx_para">
<p id="S3.p8.1" class="ltx_p">The proposed system employs distributed gradient descent to train a deep neural network across training data residing on user-held devices, with the aim of analyzing the impact of a secure aggregator. In the system that incorporates a secure aggregator, the process unfolds as follows:</p>
</div>
<div id="S3.p9" class="ltx_para">
<ol id="S3.I2" class="ltx_enumerate">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">Local Training: Initially, an individual model is trained locally on each user’s device.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">Model Transmission to Secure Aggregator: Subsequently, each user’s device transmits its locally trained model to the secure aggregator.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p">Global Model Creation: The secure aggregator aggregates these individual models to construct a global model.</p>
</div>
</li>
<li id="S3.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I2.i4.p1" class="ltx_para">
<p id="S3.I2.i4.p1.1" class="ltx_p">Aggregated Model Transmission: The aggregated model is then sent to the central main server.</p>
</div>
</li>
<li id="S3.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S3.I2.i5.p1" class="ltx_para">
<p id="S3.I2.i5.p1.1" class="ltx_p">Distribution to Devices: Finally, the main server redistributes the global model to each individual device.</p>
</div>
</li>
</ol>
</div>
<div id="S3.p10" class="ltx_para">
<p id="S3.p10.1" class="ltx_p">In contrast, in the system where a secure aggregator is not utilized, the workflow proceeds as follows:</p>
</div>
<div id="S3.p11" class="ltx_para">
<ol id="S3.I3" class="ltx_enumerate">
<li id="S3.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I3.i1.p1" class="ltx_para">
<p id="S3.I3.i1.p1.1" class="ltx_p">Local Training: Each device independently conducts local training to create an individual model.</p>
</div>
</li>
<li id="S3.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I3.i2.p1" class="ltx_para">
<p id="S3.I3.i2.p1.1" class="ltx_p">Model Transmission to Main Server: These individual models are directly transmitted to the central main server.</p>
</div>
</li>
<li id="S3.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I3.i3.p1" class="ltx_para">
<p id="S3.I3.i3.p1.1" class="ltx_p">Global Model Creation: The main server combines these individual models to form a global model.</p>
</div>
</li>
<li id="S3.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I3.i4.p1" class="ltx_para">
<p id="S3.I3.i4.p1.1" class="ltx_p">Aggregated Model Transmission: The global model is sent back to each individual device for further use and updates.</p>
</div>
</li>
</ol>
</div>
<div id="S3.p12" class="ltx_para">
<p id="S3.p12.1" class="ltx_p">This dual approach allows for a comparative analysis of the system’s performance with and without the incorporation of a secure aggregator.</p>
</div>
<div id="S3.p13" class="ltx_para">
<p id="S3.p13.3" class="ltx_p">Privacy serves as a significant driving force behind the adoption of federated learning applications. These systems are designed to safeguard user data by prioritizing the sharing of model updates, such as gradient information, rather than the raw and potentially sensitive data itself. This innovative approach to collaborative machine learning not only enhances data privacy but also enables the collective training of robust and accurate models without exposing individual user information to undue risks or breaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. While federated learning mitigates some privacy risks by not directly sharing raw data, it’s important to recognize that sending model updates during the training process can still pose potential privacy challenges <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. While recent advancements in federated learning have made strides in enhancing privacy through tools like secure multiparty computation (SMC) or differential privacy (DP), these approaches have trade-offs between privacy and model performance. The secure aggregator belongs to the class of secure multi-party computation algorithms, where a set of inherently distrustful devices denoted as <math id="S3.p13.1.m1.1" class="ltx_Math" alttext="d\in U" display="inline"><semantics id="S3.p13.1.m1.1a"><mrow id="S3.p13.1.m1.1.1" xref="S3.p13.1.m1.1.1.cmml"><mi id="S3.p13.1.m1.1.1.2" xref="S3.p13.1.m1.1.1.2.cmml">d</mi><mo id="S3.p13.1.m1.1.1.1" xref="S3.p13.1.m1.1.1.1.cmml">∈</mo><mi id="S3.p13.1.m1.1.1.3" xref="S3.p13.1.m1.1.1.3.cmml">U</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p13.1.m1.1b"><apply id="S3.p13.1.m1.1.1.cmml" xref="S3.p13.1.m1.1.1"><in id="S3.p13.1.m1.1.1.1.cmml" xref="S3.p13.1.m1.1.1.1"></in><ci id="S3.p13.1.m1.1.1.2.cmml" xref="S3.p13.1.m1.1.1.2">𝑑</ci><ci id="S3.p13.1.m1.1.1.3.cmml" xref="S3.p13.1.m1.1.1.3">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p13.1.m1.1c">d\in U</annotation></semantics></math> individually possess private values <math id="S3.p13.2.m2.1" class="ltx_Math" alttext="x_{u}" display="inline"><semantics id="S3.p13.2.m2.1a"><msub id="S3.p13.2.m2.1.1" xref="S3.p13.2.m2.1.1.cmml"><mi id="S3.p13.2.m2.1.1.2" xref="S3.p13.2.m2.1.1.2.cmml">x</mi><mi id="S3.p13.2.m2.1.1.3" xref="S3.p13.2.m2.1.1.3.cmml">u</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p13.2.m2.1b"><apply id="S3.p13.2.m2.1.1.cmml" xref="S3.p13.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p13.2.m2.1.1.1.cmml" xref="S3.p13.2.m2.1.1">subscript</csymbol><ci id="S3.p13.2.m2.1.1.2.cmml" xref="S3.p13.2.m2.1.1.2">𝑥</ci><ci id="S3.p13.2.m2.1.1.3.cmml" xref="S3.p13.2.m2.1.1.3">𝑢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p13.2.m2.1c">x_{u}</annotation></semantics></math>. These devices collaborate to calculate an aggregate value, such as the sum <math id="S3.p13.3.m3.1" class="ltx_Math" alttext="\sum_{u\in U}x_{u}" display="inline"><semantics id="S3.p13.3.m3.1a"><mrow id="S3.p13.3.m3.1.1" xref="S3.p13.3.m3.1.1.cmml"><msub id="S3.p13.3.m3.1.1.1" xref="S3.p13.3.m3.1.1.1.cmml"><mo id="S3.p13.3.m3.1.1.1.2" xref="S3.p13.3.m3.1.1.1.2.cmml">∑</mo><mrow id="S3.p13.3.m3.1.1.1.3" xref="S3.p13.3.m3.1.1.1.3.cmml"><mi id="S3.p13.3.m3.1.1.1.3.2" xref="S3.p13.3.m3.1.1.1.3.2.cmml">u</mi><mo id="S3.p13.3.m3.1.1.1.3.1" xref="S3.p13.3.m3.1.1.1.3.1.cmml">∈</mo><mi id="S3.p13.3.m3.1.1.1.3.3" xref="S3.p13.3.m3.1.1.1.3.3.cmml">U</mi></mrow></msub><msub id="S3.p13.3.m3.1.1.2" xref="S3.p13.3.m3.1.1.2.cmml"><mi id="S3.p13.3.m3.1.1.2.2" xref="S3.p13.3.m3.1.1.2.2.cmml">x</mi><mi id="S3.p13.3.m3.1.1.2.3" xref="S3.p13.3.m3.1.1.2.3.cmml">u</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p13.3.m3.1b"><apply id="S3.p13.3.m3.1.1.cmml" xref="S3.p13.3.m3.1.1"><apply id="S3.p13.3.m3.1.1.1.cmml" xref="S3.p13.3.m3.1.1.1"><csymbol cd="ambiguous" id="S3.p13.3.m3.1.1.1.1.cmml" xref="S3.p13.3.m3.1.1.1">subscript</csymbol><sum id="S3.p13.3.m3.1.1.1.2.cmml" xref="S3.p13.3.m3.1.1.1.2"></sum><apply id="S3.p13.3.m3.1.1.1.3.cmml" xref="S3.p13.3.m3.1.1.1.3"><in id="S3.p13.3.m3.1.1.1.3.1.cmml" xref="S3.p13.3.m3.1.1.1.3.1"></in><ci id="S3.p13.3.m3.1.1.1.3.2.cmml" xref="S3.p13.3.m3.1.1.1.3.2">𝑢</ci><ci id="S3.p13.3.m3.1.1.1.3.3.cmml" xref="S3.p13.3.m3.1.1.1.3.3">𝑈</ci></apply></apply><apply id="S3.p13.3.m3.1.1.2.cmml" xref="S3.p13.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.p13.3.m3.1.1.2.1.cmml" xref="S3.p13.3.m3.1.1.2">subscript</csymbol><ci id="S3.p13.3.m3.1.1.2.2.cmml" xref="S3.p13.3.m3.1.1.2.2">𝑥</ci><ci id="S3.p13.3.m3.1.1.2.3.cmml" xref="S3.p13.3.m3.1.1.2.3">𝑢</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p13.3.m3.1c">\sum_{u\in U}x_{u}</annotation></semantics></math>, while ensuring that no device discloses any information about its private value to others, except what can be inferred from the resulting aggregate value.</p>
</div>
<div id="S3.p14" class="ltx_para">
<p id="S3.p14.1" class="ltx_p">The proposed system aims to uphold the privacy of federated learning by incorporating the use of secure multiparty computation (SMC) techniques. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. The adoption of secure multiparty computation serves to safeguard individual model updates, ensuring their privacy and confidentiality. The central server is unable to observe individual local updates; it can only access the aggregated results at each round.</p>
</div>
<div id="S3.p15" class="ltx_para">
<p id="S3.p15.1" class="ltx_p">The proposed work employs the classical federated learning average (FedAvg)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. The process involves local optimization executed on participating clients and a subsequent server step to update the global model. Notably, Algorithm 1 illustrates that devices communicate only the updated weights rather than face image data, preserving the security and privacy of the user’s facial information locally.</p>
</div>
<div id="S3.p16" class="ltx_para">
<p id="S3.p16.1" class="ltx_p">Addressing the transfer of a substantial volume of updated model parameters from users to a server, which is often restricted in throughput  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, poses a significant obstacle in federated learning. This difficulty can be addressed through strategies such as minimizing the number of participating users, achieved through the implementation of scheduling policies  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Dataset</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">CelebA (Celebrities Attributes Dataset) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, is a popular dataset in the field of computer vision and machine learning. It was created by researchers at the Chinese University of Hong Kong and is often used for various facial recognition and image analysis tasks. CelebA is known for its large collection of celebrity images and the annotations associated with them. CelebA contains more than 200,000 celebrity images. These images cover a wide range of celebrities from different backgrounds and professions. Each image in the CelebA dataset is annotated with a set of 40 binary attributes. These attributes include characteristics like ”smiling,” ”wearing glasses,” ”wearing a hat,” and so on. These annotations are valuable for tasks like facial attribute prediction and facial attribute manipulation. In addition to attribute annotations, CelebA also provides identity labels for the celebrities in the dataset. This can be useful for tasks involving face recognition. The images in CelebA showcase a wide variety of poses, expressions, lighting conditions, and backgrounds, making it suitable for a broad range of computer vision tasks. The dataset is typically split into training, validation, and test sets to facilitate model training and evaluation.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.6.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.7.2" class="ltx_text ltx_font_italic">Experimental Setup</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The system architectures are:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">The CNN architecture utilized in our work closely mirrors VGG-M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, a widely adopted architecture for image classification and speech technology applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Furthermore, we incorporate a max-pooling layer with dimensions of 2 by 2, along with batch normalization and dropout layers.</p>
</div>
<div id="S4.I1.i1.p2" class="ltx_para">
<p id="S4.I1.i1.p2.3" class="ltx_p">The supervised system has been implemented using the Keras deep learning library <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> to train the model. The network is trained on Titan X GPUs for 100 epochs or until the validation error stops decreasing, whichever is sooner, using a batch-size of 64. We use SGD with momentum (0.9), weight decay (<math id="S4.I1.i1.p2.1.m1.3" class="ltx_Math" alttext="5\text{\times}{10}^{-4}" display="inline"><semantics id="S4.I1.i1.p2.1.m1.3a"><mrow id="S4.I1.i1.p2.1.m1.3.3.3" xref="S4.I1.i1.p2.1.m1.3.3.3.cmml"><mn id="S4.I1.i1.p2.1.m1.1.1.1.1.1.1.1" xref="S4.I1.i1.p2.1.m1.3.3.3.cmml">5</mn><mtext id="S4.I1.i1.p2.1.m1.2.2.2.2.2.2.2" xref="S4.I1.i1.p2.1.m1.3.3.3.cmml">×</mtext><msup id="S4.I1.i1.p2.1.m1.3.3.3.3.3.3.3" xref="S4.I1.i1.p2.1.m1.3.3.3.cmml"><mn id="S4.I1.i1.p2.1.m1.3.3.3.3.3.3.3.2" xref="S4.I1.i1.p2.1.m1.3.3.3.cmml">10</mn><mrow id="S4.I1.i1.p2.1.m1.3.3.3.3.3.3.3.3.2" xref="S4.I1.i1.p2.1.m1.3.3.3.cmml"><mo id="S4.I1.i1.p2.1.m1.3.3.3.3.3.3.3.3.2a" xref="S4.I1.i1.p2.1.m1.3.3.3.cmml">−</mo><mn id="S4.I1.i1.p2.1.m1.3.3.3.3.3.3.3.3.2.2" xref="S4.I1.i1.p2.1.m1.3.3.3.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p2.1.m1.3b"><csymbol cd="latexml" id="S4.I1.i1.p2.1.m1.3.3.3.cmml" xref="S4.I1.i1.p2.1.m1.3.3.3">5E-4</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p2.1.m1.3c">5\text{\times}{10}^{-4}</annotation></semantics></math>) and a logarithmically decaying learning rate (initialised to <math id="S4.I1.i1.p2.2.m2.1" class="ltx_Math" alttext="10^{-2}" display="inline"><semantics id="S4.I1.i1.p2.2.m2.1a"><msup id="S4.I1.i1.p2.2.m2.1.1" xref="S4.I1.i1.p2.2.m2.1.1.cmml"><mn id="S4.I1.i1.p2.2.m2.1.1.2" xref="S4.I1.i1.p2.2.m2.1.1.2.cmml">10</mn><mrow id="S4.I1.i1.p2.2.m2.1.1.3" xref="S4.I1.i1.p2.2.m2.1.1.3.cmml"><mo id="S4.I1.i1.p2.2.m2.1.1.3a" xref="S4.I1.i1.p2.2.m2.1.1.3.cmml">−</mo><mn id="S4.I1.i1.p2.2.m2.1.1.3.2" xref="S4.I1.i1.p2.2.m2.1.1.3.2.cmml">2</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p2.2.m2.1b"><apply id="S4.I1.i1.p2.2.m2.1.1.cmml" xref="S4.I1.i1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.I1.i1.p2.2.m2.1.1.1.cmml" xref="S4.I1.i1.p2.2.m2.1.1">superscript</csymbol><cn type="integer" id="S4.I1.i1.p2.2.m2.1.1.2.cmml" xref="S4.I1.i1.p2.2.m2.1.1.2">10</cn><apply id="S4.I1.i1.p2.2.m2.1.1.3.cmml" xref="S4.I1.i1.p2.2.m2.1.1.3"><minus id="S4.I1.i1.p2.2.m2.1.1.3.1.cmml" xref="S4.I1.i1.p2.2.m2.1.1.3"></minus><cn type="integer" id="S4.I1.i1.p2.2.m2.1.1.3.2.cmml" xref="S4.I1.i1.p2.2.m2.1.1.3.2">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p2.2.m2.1c">10^{-2}</annotation></semantics></math> and decaying to <math id="S4.I1.i1.p2.3.m3.1" class="ltx_Math" alttext="10^{-8}" display="inline"><semantics id="S4.I1.i1.p2.3.m3.1a"><msup id="S4.I1.i1.p2.3.m3.1.1" xref="S4.I1.i1.p2.3.m3.1.1.cmml"><mn id="S4.I1.i1.p2.3.m3.1.1.2" xref="S4.I1.i1.p2.3.m3.1.1.2.cmml">10</mn><mrow id="S4.I1.i1.p2.3.m3.1.1.3" xref="S4.I1.i1.p2.3.m3.1.1.3.cmml"><mo id="S4.I1.i1.p2.3.m3.1.1.3a" xref="S4.I1.i1.p2.3.m3.1.1.3.cmml">−</mo><mn id="S4.I1.i1.p2.3.m3.1.1.3.2" xref="S4.I1.i1.p2.3.m3.1.1.3.2.cmml">8</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p2.3.m3.1b"><apply id="S4.I1.i1.p2.3.m3.1.1.cmml" xref="S4.I1.i1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.I1.i1.p2.3.m3.1.1.1.cmml" xref="S4.I1.i1.p2.3.m3.1.1">superscript</csymbol><cn type="integer" id="S4.I1.i1.p2.3.m3.1.1.2.cmml" xref="S4.I1.i1.p2.3.m3.1.1.2">10</cn><apply id="S4.I1.i1.p2.3.m3.1.1.3.cmml" xref="S4.I1.i1.p2.3.m3.1.1.3"><minus id="S4.I1.i1.p2.3.m3.1.1.3.1.cmml" xref="S4.I1.i1.p2.3.m3.1.1.3"></minus><cn type="integer" id="S4.I1.i1.p2.3.m3.1.1.3.2.cmml" xref="S4.I1.i1.p2.3.m3.1.1.3.2">8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p2.3.m3.1c">10^{-8}</annotation></semantics></math>).</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>The architecture employed for the supervised face verification system..</figcaption>
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:2.15277pt;"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Layer</span></th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:2.15277pt;"><span id="S4.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Kernel</span></th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:2.15277pt;"><span id="S4.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Filters</span></th>
<th id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:2.15277pt;"><span id="S4.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Output size</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<td id="S4.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">Conv-1</td>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">3 X 3</td>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">64</td>
<td id="S4.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">350 X 80 X 64</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<td id="S4.T1.1.3.2.1" class="ltx_td ltx_align_center">Conv-2</td>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_center">3 X 3</td>
<td id="S4.T1.1.3.2.3" class="ltx_td ltx_align_center">128</td>
<td id="S4.T1.1.3.2.4" class="ltx_td ltx_align_center">175 X 40 X 128</td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<td id="S4.T1.1.4.3.1" class="ltx_td ltx_align_center">Conv-3</td>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_center">3 X 3</td>
<td id="S4.T1.1.4.3.3" class="ltx_td ltx_align_center">256</td>
<td id="S4.T1.1.4.3.4" class="ltx_td ltx_align_center">87 X 20 X 256</td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<td id="S4.T1.1.5.4.1" class="ltx_td ltx_align_center">fc-1</td>
<td id="S4.T1.1.5.4.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.5.4.3" class="ltx_td ltx_align_center">1000</td>
<td id="S4.T1.1.5.4.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T1.1.6.5" class="ltx_tr">
<td id="S4.T1.1.6.5.1" class="ltx_td ltx_align_center">fc-2</td>
<td id="S4.T1.1.6.5.2" class="ltx_td ltx_align_center">-</td>
<td id="S4.T1.1.6.5.3" class="ltx_td ltx_align_center">400</td>
<td id="S4.T1.1.6.5.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S4.T1.1.7.6" class="ltx_tr">
<td id="S4.T1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_b">fc-3</td>
<td id="S4.T1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b">-</td>
<td id="S4.T1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b">1</td>
<td id="S4.T1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_b">-</td>
</tr>
</tbody>
</table>
</figure>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">An autoencoder is employed to train the unsupervised system with the primary objective of enabling the network to acquire a representation of person-specific facial data. The CNN component, identified as the encoder, is optimized to learn a sophisticated representation of the provided facial image, while the decoder component is fine-tuned to reconstruct the encoder’s output into the corresponding facial image. Following the training phase, the decoder component is discarded, and the already learned encoding representation is repurposed for the face verification task. The unsupervised system does not utilize impostor data since its primary focus is on acquiring a compact vector representation of distinct individual faces.</p>
</div>
</li>
</ul>
</div>
<figure id="S4.SS2.1" class="ltx_figure"><img src="/html/2403.05344/assets/Figures/alg1.png" id="S4.SS2.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="413" alt="[Uncaptioned image]">
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The proposed face verification system has been carried out on the following databases namely CelebA  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. We randomly selected 1000 persons’ face images. We allocated 90% of each person’s face images for training an individual, face-dependent model, while the remaining 10% was reserved for evaluation. For instance, if a given person had 100 face images in the development set from the database, 90 images were utilized for training the individual face model, and the remaining 10 images were used for evaluation. Additionally, impostor data was introduced into the test set for comprehensive assessment.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Initially, our intention was to train individual face models exclusively using the authentic client face data for each person on every device. However, due to the limited number of files for each individual in the dataset—most individuals having fewer than 100 face images—this approach resulted in an overfitting problem. To address this, we modified our strategy and trained individual face models by incorporating both the true face of the individual and the face images of other individuals as impostor face data.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">We adopted two distinct methods to generate impostor face images for each individual device. In the first method, we selected face images of other individuals from each dataset as impostor face images, with 100 samples chosen for each individual on a given device. For the second method, impostor data was created using a GAN model, leveraging the approach outlined in  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> to train the GAN model on each dataset. Similar to the first method, we generated 100 impostor face images for each individual device.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">The primary challenge in training the GAN model to generate impostor face images lies in its time-consuming training phase. The computational cost of training the GAN model for 50 hours on the CelebA dataset with a Quadro P2000 GPU amounts to 3.5 hours. However, once the GAN model is trained, the extraction of impostor face image samples on edge devices becomes significantly faster. It’s important to note that the training of the GAN model is a one-time task.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p">The proposed system’s performance is assessed using the Equal Error Rate (EER), a metric that measures the point at which the rates of acceptance and rejection errors are equal.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Experimental Results</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">As it is mentioned in Section <a href="#S3" title="III Proposed System ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, we have analyzed the impact of federated learning both for supervised and unsupervised face verification systems with and without using the secure aggregator. Thus, the experimental results of the supervised and unsupervised systems with and without using the secure aggregator are described below.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS1.5.1.1" class="ltx_text">IV-C</span>1 </span>Supervised Systems without Secure Aggregator</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">Fig. <a href="#S4.F3" title="Figure 3 ‣ IV-C2 Supervised Systems using Secure Aggregator ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the comparative performance of both individual and aggregated models within the supervised system, with and without the utilization of GAN. Notably, the distinction between Fig. <a href="#S4.F3" title="Figure 3 ‣ IV-C2 Supervised Systems using Secure Aggregator ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a) and Fig. <a href="#S4.F3" title="Figure 3 ‣ IV-C2 Supervised Systems using Secure Aggregator ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (b) lies in the method of generating impostor face image samples. In Fig. <a href="#S4.F3" title="Figure 3 ‣ IV-C2 Supervised Systems using Secure Aggregator ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a), impostor face image samples are created by selecting face images of different individuals (i.e., extracting face images from CelebA to serve as impostors for a given face image). Conversely, in Fig. <a href="#S4.F3" title="Figure 3 ‣ IV-C2 Supervised Systems using Secure Aggregator ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (b), the GAN model is employed to generate the impostor face images.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p">The primary distinction between the individual and aggregated face image models lies in their training approach. For the individual model, a dedicated face image model is initially trained for each specific face image, utilizing the corresponding individual’s face image data. Subsequently, the face image samples are assessed using this personalized face image model. In this work, the individual model serves as the baseline system, wherein 1000 individual face image models are trained using face image samples from 1000 devices.</p>
</div>
<div id="S4.SS3.SSS1.p3" class="ltx_para">
<p id="S4.SS3.SSS1.p3.1" class="ltx_p">In contrast, the aggregated model employs a collaborative approach. Each of the 1000 devices transmits their parameters to a secure aggregator. The aggregator computes the average of these parameters, establishing them as its updated weight parameters, and then redistributes them to the 1000 devices. Consequently, this collaborative model is referred to as the aggregated (federated) face image model.</p>
</div>
<figure id="S4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2403.05344/assets/Figures/Supervided_Without_SA_Without_GAN.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="314" height="236" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F2.1" class="ltx_p ltx_figure_panel ltx_align_center">(a) Impostors selected from CelebA dataset.
  <img src="/html/2403.05344/assets/Figures/Supervided_Without_SA_With_GAN.png" id="S4.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="314" height="236" alt="Refer to caption"> (b) Impostors generated using GAN.</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Histograms depicting the Equal Error Rate (EER) across 1000 devices are presented for the comparison between individual and federated models in the supervised systems. Notably, this evaluation focuses on models that do not utilize a secure aggregator (SA).</figcaption>
</figure>
<div id="S4.SS3.SSS1.p4" class="ltx_para">
<p id="S4.SS3.SSS1.p4.1" class="ltx_p">In Fig. <a href="#S4.F3" title="Figure 3 ‣ IV-C2 Supervised Systems using Secure Aggregator ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, it is evident that when each of the 1000 devices employs its own individual model, the Equal Error Rate (EER) surpasses 1.98. However, with the utilization of federated/aggregated models, irrespective of the impostor generation method, a majority of the devices exhibit EER values below 1.98. Specifically, employing the first method, which uses the face images of other individuals as impostor data, around 324 devices yield an EER below 1.98. Meanwhile, in the second aggregation method involving GAN-generated impostor face images, a similar number of devices achieve an EER below 1.98.</p>
</div>
<div id="S4.SS3.SSS1.p5" class="ltx_para">
<p id="S4.SS3.SSS1.p5.1" class="ltx_p">In both Fig.<a href="#S4.F3" title="Figure 3 ‣ IV-C2 Supervised Systems using Secure Aggregator ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a) and Fig.<a href="#S4.F3" title="Figure 3 ‣ IV-C2 Supervised Systems using Secure Aggregator ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (b), it is noticeable that the aggregated face image model consistently outperforms the individual model in terms of average EER, regardless of whether GAN or other persons’ face images are used as impostor data. These figures also indicate that the two aggregated methods yield nearly identical average EER values. This suggests the feasibility of employing GAN for on-device generation of impostor face images, eliminating the need to transfer impostor data from external sources to edge devices.</p>
</div>
<div id="S4.SS3.SSS1.p6" class="ltx_para">
<p id="S4.SS3.SSS1.p6.1" class="ltx_p">Table <a href="#S4.T2" title="TABLE II ‣ IV-C2 Supervised Systems using Secure Aggregator ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> presents a comprehensive overview of the results. The average EER of individual models across the 1000 devices/face images in the supervised face image verification system stands at 2.11. This average EER serves as the baseline system, calculated by utilizing data trained specifically for each face image/device.</p>
</div>
<div id="S4.SS3.SSS1.p7" class="ltx_para">
<p id="S4.SS3.SSS1.p7.1" class="ltx_p">Additionally, Table <a href="#S4.T2" title="TABLE II ‣ IV-C2 Supervised Systems using Secure Aggregator ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> highlights that the average EER for the 1000 devices under the federated model, without employing a secure aggregator and GAN, is 1.55. This represents a noteworthy 26.5% relative improvement compared to the baseline systems. Similarly, leveraging GAN to generate impostor data during the training of face image models yields superior EER results compared to the baseline system. The table illustrates that the federated model utilizing GAN-generated data achieves an average EER of 1.98, reflecting a 6</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS2.5.1.1" class="ltx_text">IV-C</span>2 </span>Supervised Systems using Secure Aggregator</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">Table <a href="#S4.T2" title="TABLE II ‣ IV-C2 Supervised Systems using Secure Aggregator ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> reveals that the average Equal Error Rate (EER) of the 1000 devices within the federated model of the supervised system, incorporating both a secure aggregator and impostor face images from the CelebA dataset, is 2.61. Similarly, the average EER for the 1000 devices in the federated model of the same system, employing the GAN technique to generate impostor face images, is 2.73. These findings suggest that, regardless of the impostor generation method, the inclusion of a secure aggregator leads to inferior results compared to both individual systems and federated systems that do not involve a secure aggregator.</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.p2.1" class="ltx_p">The decline in EER when employing a secure aggregator in the federated system can be attributed to the trade-off between privacy enhancement and model performance or system efficiency. While recent approaches aim to bolster the privacy of federated learning through secure aggregation, this often comes at the expense of reduced model performance or overall system efficiency. Consequently, it becomes essential to weigh the privacy aspect alongside the EER values. Nevertheless, the results from systems incorporating secure aggregators remain acceptable despite the observed trade-offs.</p>
</div>
<div id="S4.SS3.SSS2.p3" class="ltx_para">
<p id="S4.SS3.SSS2.p3.1" class="ltx_p">In Figure <a href="#S4.F4" title="Figure 4 ‣ IV-C2 Supervised Systems using Secure Aggregator ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the distribution of Equal Error Rate (EER) among the 1000 devices is presented for both individual and aggregated models within the supervised system, with and without the inclusion of a secure aggregator. The figure also highlights the influence of using Generative Adversarial Networks (GAN) for impostor face image generation. The depicted elements include the minimum, lower quartile, median, upper quartile, and maximum EER values.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2403.05344/assets/Figures/Supervided_With_SA_Without_GAN.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="314" height="236" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F3.1" class="ltx_p ltx_figure_panel ltx_align_center">(a) Impostors selected from CelebA dataset.
  <img src="/html/2403.05344/assets/Figures/Supervided_With_SA_With_GAN.png" id="S4.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="314" height="236" alt="Refer to caption"> (b) Impostors generated using GAN.</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Histograms illustrating the Equal Error Rate (EER) distribution across 1000 devices are provided for a comparison between individual and federated models in the supervised system. This analysis specifically considers models that incorporate a secure aggregator (SA).</figcaption>
</figure>
<div id="S4.SS3.SSS2.p4" class="ltx_para">
<p id="S4.SS3.SSS2.p4.1" class="ltx_p">As evident from the figure, aggregated models, particularly those not incorporating a secure aggregator, consistently outperform individual models in terms of average EER, regardless of the impostor generation method employed. The visual representation of EER distribution provides a clear indication of the superior performance of aggregated models, reinforcing the efficacy of collaborative approaches in contrast to individual models within the supervised system.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2403.05344/assets/Figures/BoxPlot_Supervised_Image.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The box plot depicts the distribution of Equal Error Rates (EER) for both supervised individual and federated models across 1000 devices. The analysis considers scenarios both with and without using a Secure Aggregator (SA). Additionally, the influence of impostor selections, with and without the incorporation of Generative Adversarial Network (GAN), is highlighted.</figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>The table provides a comparison of Equal Error Rates (EER) for supervised face verification systems, considering both individual and federated approaches, with and without the use of a Secure Aggregator (SA). The inclusion of GAN-generated data for impostor face images is also accounted for in the comparison.</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:2.15277pt;"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">System</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:2.15277pt;"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">With SA</span></th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:2.15277pt;"><span id="S4.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Without SA</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">Individual Model</td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" colspan="2">2.11</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_center">Federated Model without GAN</td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center">2.61</td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_center">1.55</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_b">Federated Model with GAN</td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b">2.73</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b">1.98</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS3.5.1.1" class="ltx_text">IV-C</span>3 </span>Unsupervised Systems</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">In Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-C3 Unsupervised Systems ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the Equal Error Rates (EERs) are presented for both individual and aggregated face image models within the unsupervised system, with and without the implementation of a secure aggregator. As depicted, the EER for all 1000 devices exceeds 2.57 when each device utilizes its individual model. However, when the federated model does not employ a secure aggregator, as shown in Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-C3 Unsupervised Systems ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (a), approximately 680 devices achieve an EER below 2.35.</p>
</div>
<div id="S4.SS3.SSS3.p2" class="ltx_para">
<p id="S4.SS3.SSS3.p2.1" class="ltx_p">This visual representation underscores a notable improvement in EER when transitioning from individual models to federated models without a secure aggregator in the unsupervised system. The collaborative approach appears to enhance the performance of the face image models, contributing to lower EER values for a significant portion of the devices.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2403.05344/assets/Figures/Unsupervided_Without_SA.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="314" height="236" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F5.1" class="ltx_p ltx_figure_panel ltx_align_center">(a) Without using a secure aggregator (SA).
  <img src="/html/2403.05344/assets/Figures/Unsupervided_With_SA.png" id="S4.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="314" height="236" alt="Refer to caption"> (b) Using a secure aggregator.</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The Equal Error Rate (EER) across 1000 devices is reported for the comparison between the individual and federated models in the unsupervised system.</figcaption>
</figure>
<div id="S4.SS3.SSS3.p3" class="ltx_para">
<p id="S4.SS3.SSS3.p3.1" class="ltx_p">In contrast to the results depicted in Fig.<a href="#S4.F5" title="Figure 5 ‣ IV-C3 Unsupervised Systems ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (b), the use of a secure aggregator results in a deterioration of Equal Error Rates (EERs), leading to inferior results when compared to the individual models. This observation highlights a significant discrepancy in performance when incorporating a secure aggregator within the unsupervised system. The visual representation in Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-C3 Unsupervised Systems ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (b) underscores the importance of carefully evaluating the influence of a secure aggregator on EER results, revealing a potential trade-off between privacy-enhancing measures and model performance in the context of the unsupervised system.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>The Equal Error Rate (EER) is compared between the unsupervised face verification systems, considering both individual and federated approaches, with and without the use of a Secure Aggregator (SA).</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:2.15277pt;"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">System</span></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:2.15277pt;"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">With SA</span></th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:2.15277pt;"><span id="S4.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">Without SA</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<td id="S4.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">Individual Model</td>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" colspan="2">2.57</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<td id="S4.T3.1.3.2.1" class="ltx_td ltx_align_center ltx_border_b">Federated Model</td>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_b">3.07</td>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_b">2.35</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS3.SSS3.p4" class="ltx_para">
<p id="S4.SS3.SSS3.p4.1" class="ltx_p">Table <a href="#S4.T3" title="TABLE III ‣ IV-C3 Unsupervised Systems ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> displays the average Equal Error Rate (EER) of individual models across the 1000 persons within the unsupervised face verification system, amounting to 2.57. In contrast, the table reveals that the average EER for the 1000 devices in the federated model, under the same system but without a secure aggregator, is 2.35. This signifies an 8.56% relative improvement in EER compared to the baseline unsupervised system. However, it is noteworthy that the inclusion of a secure aggregator in the federated model results in a worse outcome compared to the baseline system, as indicated in the table.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2403.05344/assets/Figures/BoxPlot_Unsupervised_Image.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>A box plot is presented to illustrate the distribution of Equal Error Rates (EER) for unsupervised individual and federated models across 1000 devices. The analysis encompasses scenarios both with and without the utilization of a Secure Aggregator (SA).</figcaption>
</figure>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">Discussions</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">The results presented in Table <a href="#S4.T2" title="TABLE II ‣ IV-C2 Supervised Systems using Secure Aggregator ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> and <a href="#S4.T3" title="TABLE III ‣ IV-C3 Unsupervised Systems ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> consistently highlight that, regardless of the face verification system (supervised or unsupervised), the introduction of a secure aggregator tends to reduce the performance of the federated model. Conversely, when the federated model operates without a secure aggregator, EER results improve in comparison to the individual models. Despite the slight deterioration caused by the secure aggregator, it’s essential to consider that its inclusion ensures the privacy of the data.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">It’s worth noting that, while the EER results experience a slight decline with the secure aggregator, the overall performance remains satisfactory. The compromise in EER is balanced by the privacy-preserving benefits offered by the secure aggregator. The EER results, even with the use of a secure aggregator, are acceptable, emphasizing the trade-off between privacy protection and model performance.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">In addition to the individual and federated models, another experiment was conducted by pooling all face image samples from the 1000 persons and training a single generic face image model on a single computer. The results demonstrate that the average EER of the global model on the supervised and unsupervised face verification systems is 1.2% and 2.2%, respectively. These results are comparable to the federated model’s performance (see Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-C2 Supervised Systems using Secure Aggregator ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#S4.F6" title="Figure 6 ‣ IV-C3 Unsupervised Systems ‣ IV-C Experimental Results ‣ IV Experiments ‣ Federated Learning Method for Preserving Privacy in Face Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). The federated model achieves similar EER values as the global model while preserving the privacy of face image data. This underscores the advantage of using federated learning for face recognition systems.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">The work employs 1000 devices to compare the performance of individual versus federated models. Statistical analysis using Student’s t-test supports the significance of the observed differences. The computed P-values for both comparisons, where the federated model selects impostor face images from CelebA (federated model 1) and where GAN is used for impostor face image generation (federated model 2), are both less than the standard significance level of 0.05. Thus, we reject the null hypothesis, affirming that the mean EER differences between individual and federated models are statistically significant.</p>
</div>
<div id="S4.SS4.p5" class="ltx_para">
<p id="S4.SS4.p5.1" class="ltx_p">Finally, the experiment considered the impact of updating local models more than once. The results indicate that updating local models more than once does not lead to an improvement in EER. This could be attributed to the similarity in training data among devices during each training phase. Although updating more frequently did not yield enhanced performance, this decision was driven by the need to maintain data privacy.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusions</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we propose the adoption of federated learning as a safeguard for the privacy of facial image data residing on edge devices, applicable to both supervised and unsupervised face recognition systems. Our approach centers on decentralized training, eliminating the need for devices to transmit their raw image data to centralized servers. Instead, each user’s data remains securely stored and processed solely on their respective edge device. Consequently, training occurs exclusively at the local level, with each device contributing updates to a central model. Subsequently, a secure aggregator consolidates these local models into a single federated model, which is then distributed via the main server back to the individual devices. Furthermore, our research delves into an analysis of the influence of the secure aggregator on the performance of face recognition systems.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Our proposed system offers two primary advantages. Firstly, as raw data remains confined to individual devices, the privacy of facial images is preserved. Secondly, experimental findings reveal that the federated model, devoid of a secure aggregator, achieves a superior average Equal Error Rate (EER) compared to individual models. However, when the federated model incorporates the secure aggregator, the aggregated model yields EER results that are slightly less favorable than those of individual models. Nonetheless, the EER results remain commendable, emphasizing the importance of weighing the trade-offs between privacy and performance.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Future research works should delve into refining aggregation techniques beyond simplistic averaging methods. Additionally, exploring the effects of scaling up the number of devices beyond the 1000 devices employed in this work holds promise for further enhancing the effectiveness of privacy-preserving face recognition systems.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">McFee, B., McVicar, M., Raffel, C., Liang, D., Nieto, O., Moore, J., Ellis, D., Repetto, D., Viktorin, P., Santos, J. and Others Librosa: v0. 4.0. <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Zenodo 2015</span>. (2015)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">Nautsch, A., Jiménez, A., Treiber, A., Kolberg, J., Jasserand, C., Kindt, E., Delgado, H., Todisco, M., Hmani, M., Mtibaa, A. and Others Preserving privacy in speaker and speech characterisation. <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Computer Speech And Language</span>. <span id="bib.bib2.2.2" class="ltx_text ltx_font_bold">58</span> pp. 441-480 (2019)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">Chollet, F. and Others Keras (2015). (2017)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">Nagrani, A., Chung, J. and Zisserman, A. Voxceleb: a large-scale speaker identification dataset. <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">ArXiv Preprint ArXiv:1706.08612</span>. (2017)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">LeCun, Y., Bengio, Y. and Hinton, G. Deep learning. <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Nature</span>. <span id="bib.bib5.2.2" class="ltx_text ltx_font_bold">521</span>, 436-444 (2015)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">Chen, J. and Ran, X. Deep Learning With Edge Computing: A Review.. <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proceedings Of The IEEE</span>. <span id="bib.bib6.2.2" class="ltx_text ltx_font_bold">107</span>, 1655-1674 (2019)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Le, Q., Mao, M., Ranzato, M., Senior, A., Tucker, P. and Others Large scale distributed deep networks. (2012)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">McMahan, B., Moore, E., Ramage, D., Hampson, S. and Arcas, B. Communication-efficient learning of deep networks from decentralized data. <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Artificial Intelligence And Statistics</span>. pp. 1273-1282 (2017)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">Bonawitz, K., Eichner, H., Grieskamp, W., Huba, D., Ingerman, A., Ivanov, V., Kiddon, C., Konečnỳ, J., Mazzocchi, S., McMahan, H. and Others Towards federated learning at scale: System design. <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">ArXiv Preprint ArXiv:1902.01046</span>. (2019)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">Reynolds, D. Automatic speaker recognition using Gaussian mixture speaker models. <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">The Lincoln Laboratory Journal</span>. (1995)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">Furui, S. An overview of speaker recognition technology. <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Automatic Speech And Speaker Recognition</span>. pp. 31-56 (1996)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">Bonawitz, K., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H., Patel, S., Ramage, D., Segal, A. and Seth, K. Practical secure aggregation for privacy-preserving machine learning. <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings Of The 2017 ACM SIGSAC Conference On Computer And Communications Security</span>. pp. 1175-1191 (2017)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">Donahue, C., McAuley, J. and Puckette, M. Adversarial Audio Synthesis. <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">ICLR</span>. (2019)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">Brendan McMahan, H., Moore, E., Ramage, D., Hampson, S. and Arcas, B. Communication-efficient learning of deep networks from decentralized data. <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">ArXiv E-prints</span>. pp. arXiv-1602 (2016)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">Kairouz, P., McMahan, H., Avent, B., Bellet, A., Bennis, M., Bhagoji, A., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R. and Others Advances and open problems in federated learning. <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">ArXiv Preprint ArXiv:1912.04977</span>. (2019)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">Li, T., Sahu, A., Talwalkar, A. and Smith, V. Federated learning: Challenges, methods, and future directions. <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">IEEE Signal Processing Magazine</span>. <span id="bib.bib16.2.2" class="ltx_text ltx_font_bold">37</span>, 50-60 (2020)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">Chen, M., Yang, Z., Saad, W., Yin, C., Poor, H. and Cui, S. A joint learning and communications framework for federated learning over wireless networks. <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">IEEE Transactions On Wireless Communications</span>. (2020)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">Yang, H., Liu, Z., Quek, T. and Poor, H. Scheduling policies for federated learning in wireless networks. <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">IEEE Transactions On Communications</span>. <span id="bib.bib18.2.2" class="ltx_text ltx_font_bold">68</span>, 317-333 (2019)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">Amiri, M., Gündüz, D., Kulkarni, S. and Poor, H. Update aware device scheduling for federated learning at the wireless edge. <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">2020 IEEE International Symposium On Information Theory (ISIT)</span>. pp. 2598-2603 (2020)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">Chatfield, K., Simonyan, K., Vedaldi, A. and Zisserman, A. Return of the devil in the details: Delving deep into convolutional nets. <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">ArXiv Preprint ArXiv:1405.3531</span>. (2014)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">Chung, J. and Zisserman, A. Out of time: automated lip sync in the wild. <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Asian Conference On Computer Vision</span>. pp. 251-263 (2016)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">Rahulamathavan, Y., Sutharsini, K., Ray, I., Lu, R. and Rajarajan, M. Privacy-preserving iVector-based speaker verification. <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">IEEE/ACM Transactions On Audio, Speech, And Language Processing</span>. <span id="bib.bib22.2.2" class="ltx_text ltx_font_bold">27</span>, 496-506 (2018)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">Carlini, N., Liu, C., Kos, J., Erlingsson, Ú. and Song, D. The secret sharer: Measuring unintended neural network memorization and extracting secrets. <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">ArXiv Preprint ArXiv:1802.08232</span>. <span id="bib.bib23.2.2" class="ltx_text ltx_font_bold">5</span> (2018)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">Wainwright, M., Jordan, M. and Duchi, J. Privacy aware learning. <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Advances In Neural Information Processing Systems</span>. <span id="bib.bib24.2.2" class="ltx_text ltx_font_bold">25</span> (2012)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">Dwork, C., Roth, A. and Others The algorithmic foundations of differential privacy.. <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Found. Trends Theor. Comput. Sci.</span>. <span id="bib.bib25.2.2" class="ltx_text ltx_font_bold">9</span>, 211-407 (2014)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">McMahan, H., Ramage, D., Talwar, K. and Zhang, L. Learning differentially private recurrent language models. <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">ArXiv Preprint ArXiv:1710.06963</span>. (2017)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">Ghazi, B., Pagh, R. and Velingker, A. Scalable and differentially private distributed aggregation in the shuffled model. <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">ArXiv Preprint ArXiv:1906.08320</span>. (2019)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">Group, W. and Others Federated learning white paper v1. 0. (2020)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">Huang, M., Li, H., Bai, B., Wang, C., Bai, K. and Wang, F. A Federated Multi-View Deep Learning Framework for Privacy-Preserving Recommendations. <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">ArXiv Preprint ArXiv:2008.10808</span>. (2020)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">Hard, A., Rao, K., Mathews, R., Ramaswamy, S., Beaufays, F., Augenstein, S., Eichner, H., Kiddon, C. and Ramage, D. Federated learning for mobile keyboard prediction. <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">ArXiv Preprint ArXiv:1811.03604</span>. (2018)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">Yang, Q., Liu, Y., Chen, T. and Tong, Y. Federated machine learning: Concept and applications. <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">ACM Transactions On Intelligent Systems And Technology (TIST)</span>. <span id="bib.bib31.2.2" class="ltx_text ltx_font_bold">10</span>, 1-19 (2019)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">Zhao, Y., Zhao, J., Jiang, L., Tan, R. and Niyato, D. Mobile edge computing, blockchain and reputation-based crowdsourcing iot federated learning: A secure, decentralized and privacy-preserving system. (2020)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">The TFF Authors TensorFlow Federated. (https://www.tensorflow.org/federated/,2019), Accessed: 2021-09-20

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">The FATE Authors Federated AI Technology Enabler. (URL https://www.fedai.org/,2019), Accessed: 2021-09-20

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">The Leaf Authors Leaf. (https://leaf.cmu.edu/,2019), Accessed: 2021-09-20

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">The PaddleFL Authors PaddleFL.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">Support.google Your chats stay private while Messages improves suggestions. (https://support.google.com/messages/answer/9327902/,2019), Accessed: 2021-09-20

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">Erlingsson, Ú., Pihur, V. and Korolova, A. Rappor: Randomized aggregatable privacy-preserving ordinal response. <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Proceedings Of The 2014 ACM SIGSAC Conference On Computer And Communications Security</span>. pp. 1054-1067 (2014)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">Apple Private Federated Learning (NeurIPS 2019 Expo Talk Abstract). (https://nips.cc/ExpoConferences/2019/schedule?talkid=40,2019), Accessed: 2021-09-20

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">Apple Designing for privacy.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">Boutros, F., Huber, M., Siebke, P., Rieber, T. and Damer, N. SFace: Privacy-friendly and Accurate Face Recognition using Synthetic Data. <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">2022 IEEE International Joint Conference On Biometrics (IJCB)</span>. pp. 1-11 (2022)

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">Liu, Z., Luo, P., Wang, X. and Tang, X. Deep Learning Face Attributes in the Wild. (2015)

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">Huang, G., Mattar, M., Berg, T. and Learned-Miller, E. Labeled Faces in the Wild: A Database Forstudying Face Recognition in Unconstrained Environments. (2008), https://hal.inria.fr/inria-00321923

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">Wolf, L., Hassner, T. and Maoz, I. Face recognition in unconstrained videos with matched background similarity. <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">Proceedings Of The CVPR</span>. pp. 529-534 (2011,6,20)

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">Zheng, T., Deng, W. and Hu, J. Cross-age lfw: A database for studying cross-age face recognition in unconstrained environments. <span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">ArXiv Preprint</span>. <span id="bib.bib45.2.2" class="ltx_text ltx_font_bold">arXiv:1708.08197</span> (2017)

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">Sengupta, S. Frontal to Profile Face Verification in the Wild. <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">IEEE Conference On Applications Of Computer Vision</span>. (2016,3,7)

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">Zhao, S., Zhang, L. and Xiong, P. PriFace: a privacy-preserving face recognition framework under untrusted server. <span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">J Ambient Intell Human Comput</span>. <span id="bib.bib47.2.2" class="ltx_text ltx_font_bold">14</span> pp. 2967-2979 (2023)

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">Gao, W., Yu, J., Hao, R., Kong, F. and Liu, X. Privacy-Preserving Face Recognition With Multi-Edge Assistance for Intelligent Security Systems. <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">IEEE Internet Of Things Journal</span>. <span id="bib.bib48.2.2" class="ltx_text ltx_font_bold">10</span>, 10948-10958 (2023)

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">Kou, X., Zhang, Z., Zhang, Y. and Li, L. Efficient and Privacy-Preserving Distributed Face Recognition Scheme via FaceNet. <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">Proceedings Of The ACM Turing Award Celebration Conference - China</span>. pp. 110-115 (2021), https://doi.org/10.1145/3472634.3472661

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">Guo, S., Xiang, T. and Li, X. Towards efficient privacy-preserving face recognition in the cloud. <span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">Signal Processing</span>. <span id="bib.bib50.2.2" class="ltx_text ltx_font_bold">164</span> pp. 320-328 (2019), https://www.sciencedirect.com/science/article/pii/S0165168419302324

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">Wang, Y., Liu, J., Luo, M., Yang, L. and Wang, L. Privacy-Preserving Face Recognition in the Frequency Domain. <span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">Proceedings Of The AAAI Conference On Artificial Intelligence</span>. <span id="bib.bib51.2.2" class="ltx_text ltx_font_bold">36</span>, 2558-2566 (2022), https://ojs.aaai.org/index.php/AAAI/article/view/20157

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">Yang, Y., Zhang, Q., Gao, W. and Others Design on Face Recognition System with Privacy Preservation Based on Homomorphic Encryption. <span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">Wireless Pers Commun</span>. <span id="bib.bib52.2.2" class="ltx_text ltx_font_bold">123</span> pp. 3737-3754 (2022)

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">Yang, X., Zhu, H., Lu, R., Liu, X. and Li, H. Efficient and Privacy-Preserving Online Face Recognition Over Encrypted Outsourced Data. <span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">2018 IEEE International Conference On Internet Of Things (iThings) And IEEE Green Computing And Communications (GreenCom) And IEEE Cyber, Physical And Social Computing (CPSCom) And IEEE Smart Data (SmartData)</span>. pp. 366-373 (2018)

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">Liu, M., Hu, H., Xiang, H., Yang, C., Lyu, L. and Zhang, X. Clustering-Based Efficient Privacy-Preserving Face Recognition Scheme without Compromising Accuracy. <span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">ACM Trans. Sen. Netw.</span>. <span id="bib.bib54.2.2" class="ltx_text ltx_font_bold">17</span> (2021,6), https://doi.org/10.1145/3448414

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">Mi, Y., Huang, Y., Ji, J., Liu, H., Xu, X., Ding, S. and Zhou, S. DuetFace: Collaborative Privacy-Preserving Face Recognition via Channel Splitting in the Frequency Domain. <span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">Proceedings Of The 30th ACM International Conference On Multimedia</span>. pp. 6755-6764 (2022), https://doi.org/10.1145/3503161.3548303

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">Xie, Y., Li, P., Nedjah, N. and Others Privacy protection framework for face recognition in edge-based Internet of Things. <span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">Cluster Comput</span>. <span id="bib.bib56.2.2" class="ltx_text ltx_font_bold">26</span> pp. 3017-3035 (2023)

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">Chamikara, M., Bertok, P., Khalil, I., Liu, D. and Camtepe, S. Privacy Preserving Face Recognition Utilizing Differential Privacy. <span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">Computers And Security</span>. <span id="bib.bib57.2.2" class="ltx_text ltx_font_bold">97</span> pp. 101951 (2020), https://www.sciencedirect.com/science/article/pii/S0167404820302273

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">Meng, Q., Zhou, F., Ren, H., Feng, T., Liu, G. and Lin, Y. Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters. (2022)

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">Aggarwal, D., Zhou, J. and Jain, A. FedFace: Collaborative Learning of Face Recognition Model. (2021)

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">Shao, R., Perera, P., Yuen, P. and Patel, V. Federated Face Presentation Attack Detection. (2020)

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">Liu, C., Wang, C., Chien, S. and Lai, S. FedFR: Joint Optimization Federated Framework for Generic and Personalized Face Recognition. <span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">Proceedings Of The AAAI Conference On Artificial Intelligence</span>. <span id="bib.bib61.2.2" class="ltx_text ltx_font_bold">36</span>, 1656-1664 (2022), https://ojs.aaai.org/index.php/AAAI/article/view/20057

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">Wang, X., Peng, J., Zhang, S., Chen, B., Wang, Y. and Guo, Y. A Survey of Face Recognition. (2022)

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">Dave, G., Chao, X. and Sriadibhatla, K. Face Recognition in Mobile Phones. <span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">Department Of Electrical Engineering, Stanford University</span>. (2010,1)

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">Rajamäki, J., Turunen, T., Harju, A., Heikkilä, M., Hilakivi, M., Rusanen, S. and Leppävaara, L. Face recognition as an airport and seaport security tool. (2009,7)

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">Woubie, A. and Bäckström, T. Federated learning for privacy-preserving speaker recognition. <span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>. <span id="bib.bib65.2.2" class="ltx_text ltx_font_bold">9</span> pp. 149477-149485 (2021)

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">Woubie, A. and Bäckström, T. Federated Learning for Privacy Preserving On-Device Speaker Recognition. <span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">Proc. 2021 ISCA Symposium On Security And Privacy In Speech Communication</span>. pp. 1-5 (2021)

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">Solomon, E., Woubie, A. and Cios, K. UFace: An Unsupervised Deep Learning Face Verification System. <span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">Electronics</span>. <span id="bib.bib67.2.2" class="ltx_text ltx_font_bold">11</span>, 3909 (2022)

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">Solomon, E. and Cios, K. FASS: Face Anti-Spoofing System Using Image Quality Features and Deep Learning. <span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">Electronics</span>. <span id="bib.bib68.2.2" class="ltx_text ltx_font_bold">12</span>, 2199 (2023)

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">Solomon, E. and Cios, K. HDLHC: Hybrid Face Anti-Spoofing Method Concatenating Deep Learning and Hand-Crafted Features. <span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">2023 IEEE 6th International Conference On Electronic Information And Communication Technology (ICEICT)</span>. pp. 470-474 (2023)

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">Solomon, E., Woubie, A. and Emiru, E. Unsupervised Deep Learning Image Verification Method. <span id="bib.bib70.1.1" class="ltx_text ltx_font_italic">ArXiv Preprint ArXiv:2312.14395</span>. (2023)

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">Solomon, E., Woubie, A. and Emiru, E. Autoencoder Based Face Verification System. <span id="bib.bib71.1.1" class="ltx_text ltx_font_italic">ArXiv Preprint ArXiv:2312.14301</span>. (2023)

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">Woubie, A., Solomon, E. and Emiru, E. Image Clustering using Restricted Boltzman Machine. <span id="bib.bib72.1.1" class="ltx_text ltx_font_italic">ArXiv Preprint ArXiv:2312.13845</span>. (2023)

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">Solomon, E., Woubie, A. and Emiru, E. Deep Learning Based Face Recognition Method using Siamese Network. <span id="bib.bib73.1.1" class="ltx_text ltx_font_italic">ArXiv Preprint ArXiv:2312.14001</span>. (2023)

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">Solomon, E. Face Anti-Spoofing and Deep Learning Based Unsupervised Image Recognition Systems. (2023)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.05343" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.05344" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.05344">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.05344" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.05345" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 14:43:46 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
