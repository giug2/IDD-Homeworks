<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2309.11549] Large Synthetic Data from the arùúíiv for OCR Post Correction of Historic Scientific Articles</title><meta property="og:description" content="Historical scientific articles often require Optical Character Recognition (OCR) to transform scanned documents into machine-readable text, a process that often produces errors.
We present a pipeline for the generation‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Large Synthetic Data from the arùúíiv for OCR Post Correction of Historic Scientific Articles">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Large Synthetic Data from the arùúíiv for OCR Post Correction of Historic Scientific Articles">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2309.11549">

<!--Generated on Wed Feb 28 05:14:20 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="scholarly document processing optical character recognition astronomy.">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\DeclareNewFootnote</span>
<p id="p1.2" class="ltx_p">[para]default

</p>
</div>
<span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>School of Information Sciences, University of Illinois, Urbana-Champaign, 61820, USA
<span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{jnaiman,mcosil2}@illinois.edu</span></span></span>
<br class="ltx_break"></span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Harvard-Smithsonian Center for Astrophysics, Cambridge, 02138, USA
<br class="ltx_break"><span id="id2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>{pwilliams,agoodman}@cfa.harvard.edu</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Large Synthetic Data from the ar<math id="id1.m1.1" class="ltx_Math" alttext="\rm{\chi}" display="inline"><semantics id="id1.m1.1b"><mi id="id1.m1.1.1" xref="id1.m1.1.1.cmml">œá</mi><annotation-xml encoding="MathML-Content" id="id1.m1.1c"><ci id="id1.m1.1.1.cmml" xref="id1.m1.1.1">ùúí</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.m1.1d">\rm{\chi}</annotation></semantics></math>iv for OCR Post Correction of Historic Scientific Articles </h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">J. P. Naiman 
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-9397-6189" title="ORCID identifier" class="ltx_ref">0000-0002-9397-6189</a></span>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Morgan G. Cosillo
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0009-0008-4755-7531" title="ORCID identifier" class="ltx_ref">0009-0008-4755-7531</a></span>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Peter K. G. Williams
</span><span class="ltx_author_notes">22
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-3734-3587" title="ORCID identifier" class="ltx_ref">0000-0003-3734-3587</a></span>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alyssa Goodman
</span><span class="ltx_author_notes">22
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-1312-0477" title="ORCID identifier" class="ltx_ref">0000-0003-1312-0477</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.1a" class="ltx_p">Historical scientific articles often require Optical Character Recognition (OCR) to transform scanned documents into machine-readable text, a process that often produces errors.
We present a pipeline for the generation of a synthetic ground truth/OCR dataset to correct the OCR results of the astrophysics literature holdings of the NASA Astrophysics Data System (ADS). By mining the ar<math id="id2.1.m1.1" class="ltx_Math" alttext="\rm{\chi}" display="inline"><semantics id="id2.1.m1.1a"><mi id="id2.1.m1.1.1" xref="id2.1.m1.1.1.cmml">œá</mi><annotation-xml encoding="MathML-Content" id="id2.1.m1.1b"><ci id="id2.1.m1.1.1.cmml" xref="id2.1.m1.1.1">ùúí</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.1.m1.1c">\rm{\chi}</annotation></semantics></math>iv we create, to the authors‚Äô knowledge, the largest scientific synthetic ground truth/OCR post correction dataset of 203,354,393
 character pairs. Baseline models trained with this dataset find the mean improvement in character and word error rates of <span id="id2.1a.1" class="ltx_text" style="color:#000000;">7.71
% and 18.82
%</span> for historical OCR text, respectively.
Interactive dashboards to explore the dataset are available online: <a target="_blank" href="https://readingtimemachine.github.io/projects/1-ocr-groundtruth-may2023" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://readingtimemachine.github.io/projects/1-ocr-groundtruth-may2023</a>, and data and code, are hosted on GitHub: <a target="_blank" href="https://github.com/ReadingTimeMachine/ocr_post_correction" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ReadingTimeMachine/ocr_post_correction</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>scholarly document processing optical character recognition astronomy.
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The ability to digitally store and parse scientific literature is vital to ensure access and proliferation of scientific ideas <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>.
While digital storage is supported for much of contemporary scientific literature, the text of many historical documents is ‚Äútrapped‚Äù within scanned pages of paper journals and theses.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recently, various deep learning methods have been employed to extract page objects (e.g., figures) from scans <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
An obstacle to the extraction of information from historical articles is the accuracy of these extracted materials.
This is especially of concern for any text objects which contain the bulk of the information in an article.
A typical solution is to extract text with Optical Character Recognition (OCR) engines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>.
However, the generated text is often noisy which is not only an issue for comprehension by humans and screen readers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, but also can affect ‚Äúdownstream‚Äù natural language processing tasks such as topic modeling, sentence segmentation and named entity recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, often times causing significant errors in these processes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.3" class="ltx_p">Here, we discuss a new method for addressing OCR noise in the context of the extraction of text from a subset of <math id="S1.p3.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S1.p3.1.m1.1a"><mo id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><csymbol cd="latexml" id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">\sim</annotation></semantics></math>56k articles from the pre-digital holdings of the the Astrophysics Data System (ADS)<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://ui.adsabs.harvard.edu/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ui.adsabs.harvard.edu/</a></span></span></span> from <math id="S1.p3.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S1.p3.2.m2.1a"><mo id="S1.p3.2.m2.1.1" xref="S1.p3.2.m2.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S1.p3.2.m2.1b"><csymbol cd="latexml" id="S1.p3.2.m2.1.1.cmml" xref="S1.p3.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.2.m2.1c">\sim</annotation></semantics></math>1850-1997 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.
While our ultimate goal is to correct all historical text within the ADS holdings, our initial focus is on the correction of ‚Äúplain text‚Äù in the main portions of articles (i.e., not text within tables or captions).
Our method relies on generating synthetic data from mining the ar<math id="S1.p3.3.m3.1" class="ltx_Math" alttext="\rm{\chi}" display="inline"><semantics id="S1.p3.3.m3.1a"><mi id="S1.p3.3.m3.1.1" xref="S1.p3.3.m3.1.1.cmml">œá</mi><annotation-xml encoding="MathML-Content" id="S1.p3.3.m3.1b"><ci id="S1.p3.3.m3.1.1.cmml" xref="S1.p3.3.m3.1.1">ùúí</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.3.m3.1c">\rm{\chi}</annotation></semantics></math>iv source files (LaTeX/TeX files which compile to PDFs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>) for ‚Äúpost correction‚Äù models which are applied to previously extracted OCR text.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Post correction methods are vital to the extraction of text from the historical holdings of ADS as only a small portion of the articles can be mined with PDF-parsing software <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Additionally, in many large historical corpora it is not computationally feasible to re-OCR holdings each time an OCR engine is upgraded <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, making post correction the only option to reduce errors.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">While the work presented here focuses on the literature of the ‚Äúbig-data‚Äù science of astronomy and astrophysics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, our methods of synthetic data generation can be generalized to other scientific fields.
To aid in future generalizability, we use the open-source OCR engine <span id="S1.p5.1.1" class="ltx_text ltx_font_sansserif">Tesseract</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> and provide all code in <span id="S1.p5.1.2" class="ltx_text ltx_font_sansserif">Python</span>.
Because the dataset is large we provide interactive visualizations to assist any user of our resource in their investigation of the dataset.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>OCR Noise Reduction Techniques &amp; Mining the ar<math id="S2.1.m1.1" class="ltx_Math" alttext="\rm{\chi}" display="inline"><semantics id="S2.1.m1.1b"><mi id="S2.1.m1.1.1" xref="S2.1.m1.1.1.cmml">œá</mi><annotation-xml encoding="MathML-Content" id="S2.1.m1.1c"><ci id="S2.1.m1.1.1.cmml" xref="S2.1.m1.1.1">ùúí</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.1.m1.1d">\rm{\chi}</annotation></semantics></math>iv </h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">OCR noise is prevalent in the majority of OCR datasets used in the fields of digital humanities and cultural analytics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
OCR errors do not follow patterns of typical misspellings, thus their correction generally relies on different tools than spell-checking software <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
OCR post correction, a method of error mitigation, in which OCR‚Äôd text is de-noised, is a field covering a wide range of digitization applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> and models have historically taken several forms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>.
More recently, deep learning models have been developed to tackle post correction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> which typically make use of sequence-to-sequence models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.2" class="ltx_p">These deep learning methods require large training datasets, making their testing predominately completed with well known OCR post correction datasets from the community <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
As manual annotations can be time consuming at scale <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, synthetic datasets are often used <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>.
In particular, mining the ar<math id="S2.p2.1.m1.1" class="ltx_Math" alttext="\rm{\chi}" display="inline"><semantics id="S2.p2.1.m1.1a"><mi id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">œá</mi><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><ci id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">ùúí</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">\rm{\chi}</annotation></semantics></math>iv is a popular method to generate synthetic machine learning training datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Given the variety of journals represented in the ar<math id="S2.p2.2.m2.1" class="ltx_Math" alttext="\rm{\chi}" display="inline"><semantics id="S2.p2.2.m2.1a"><mi id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">œá</mi><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><ci id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">ùúí</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">\rm{\chi}</annotation></semantics></math>iv database, its mining represents a vital opportunity to create domain-specific synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, which is necessary as models trained on one type of document will often fail on documents dissimilar to the training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In what follows, we make use of two decades of the oldest articles available through the ar<math id="S3.p1.1.m1.1" class="ltx_Math" alttext="\rm{\chi}" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">œá</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ùúí</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\rm{\chi}</annotation></semantics></math>iv Bulk Downloads <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> (1991-2011) for a total of 712,975
 articles.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Compiling the Astrophysics ar<math id="S3.SS1.1.m1.1" class="ltx_Math" alttext="\rm{\chi}" display="inline"><semantics id="S3.SS1.1.m1.1b"><mi id="S3.SS1.1.m1.1.1" xref="S3.SS1.1.m1.1.1.cmml">œá</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.1.m1.1c"><ci id="S3.SS1.1.m1.1.1.cmml" xref="S3.SS1.1.m1.1.1">ùúí</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.1.m1.1d">\rm{\chi}</annotation></semantics></math>iv Bulk Downloads</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Once downloaded, all article files are checked for corrupt decompression formats and a main TeX file (those containing <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_sansserif">\documentclass</span> or <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_sansserif">\documentstyle</span>) for a total of 318,033
 articles.
To construct an ‚Äúastronomy article‚Äù list, class/style commands are parsed with <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_sansserif">regex</span> and those which denote typical astrophysical journal names (e.g., ‚Äúaastex‚Äù, ‚Äúapj‚Äù, ‚Äúmn‚Äù) are kept. These names correspond to the three journals which have the most complete scanned historical corpus (The Astrophysical Journal, Astronomy &amp; Astrophysics, and Monthly Notices of the Royal Astronomical Society) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
This results in a total of 65,132
 articles.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">This set of <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mo id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><csymbol cd="latexml" id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\sim</annotation></semantics></math>65k files are tested for PDF-compilation errors for a total of 26,578
 successfully compiled astronomy articles.
The main sources of error are missing files (e.g., missing figure files) and an inability to distinguish which TeX file in a directory is the main article document.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Segmentation of TeX Documents</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Many parsers exist for TeX files with output formats such as plain text (e.g., <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_sansserif">opendetex</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>), XML (e.g., <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_sansserif">LaTeXML</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_sansserif">unarXive</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>) or document trees (e.g., <span id="S3.SS2.p1.1.4" class="ltx_text ltx_font_sansserif">TexSoup</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>).
With all methods, this parsing tends to be non-trivial <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.
As the documents are compiled once marking modifications are applied to the TeX to track synthetic ground truth (SGT) locations, any parser must account for errors that could occur in the compilation process.
Additionally, checks for incorrect splitting of TeX source into trees are required.
This excludes ‚Äúoff the shelf‚Äù parsers which only run a subset of these checks<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>For example, following the process in <a href="#S3.SS3" title="3.3 Marking the ‚Äúground truth‚Äù words in LaTeX &amp; OCR‚Äôing Pages ‚Ä£ 3 Methods ‚Ä£ Large Synthetic Data from the arùúíiv for OCR Post Correction of Historic Scientific Articles" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section¬†3.3</span></a>, TeXSoup finds errors in only 46.2
% of files, while our method finds errors in 70.4
%.</span></span></span>. Thus this work makes use of a custom-built TeX parser.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><a href="#S3.F1" title="Figure 1 ‚Ä£ 3.3 Marking the ‚Äúground truth‚Äù words in LaTeX &amp; OCR‚Äôing Pages ‚Ä£ 3 Methods ‚Ä£ Large Synthetic Data from the arùúíiv for OCR Post Correction of Historic Scientific Articles" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure¬†1</span></a> diagrams the segmentation process which uses <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_sansserif">regex</span> to break TeX files into document trees. A raw TeX document (‚ÄúRaw LaTeX‚Äù snippet shown in upper left gray panel) is parsed to find the locations of special characters denoting commands, variables, and environments (‚ÄúSplits with regex‚Äù blue upper middle panel).
A hierarchy is then constructed with checks for closing and opening statements of commands (closing {}) inline math formulas (paired $‚Äôs) and environments (\begin, \end) and stored in a tree (‚ÄúTree‚Äù purple upper right panel).
Commands which reside within plain text sentences such as inline math, citations, and references (‚Äú\ref{}‚Äù commands) are stored with special tags.
</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Marking the ‚Äúground truth‚Äù words in LaTeX &amp; OCR‚Äôing Pages</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Many methods for marking TeX documents to generate synthetic data for page objects (e.g., figures) modify the LaTeX to add bounding boxes in specific colors around objects and use image processing techniques to extract object locations after the PDF is rendered <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
Rendered PDFs can potentially be mined for SGT text, however, this can lead to errors in the extracted SGT text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">To avoid SGT-text parsing errors, this work adopts a different approach by modifying the TeX source documents with markers denoting every word, inline equation, citation, and reference using the <span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_sansserif">tikzmark</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> package as shown within the green outlined ‚ÄúMarked LaTeX‚Äù box of <a href="#S3.F1" title="Figure 1 ‚Ä£ 3.3 Marking the ‚Äúground truth‚Äù words in LaTeX &amp; OCR‚Äôing Pages ‚Ä£ 3 Methods ‚Ä£ Large Synthetic Data from the arùúíiv for OCR Post Correction of Historic Scientific Articles" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure¬†1</span></a>.
Inline math, citations, and references are included as they are frequently interspersed with the plain text.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">After storing the locations of each SGT object (‚ÄúTree‚Äù purple box in <a href="#S3.F1" title="Figure 1 ‚Ä£ 3.3 Marking the ‚Äúground truth‚Äù words in LaTeX &amp; OCR‚Äôing Pages ‚Ä£ 3 Methods ‚Ä£ Large Synthetic Data from the arùúíiv for OCR Post Correction of Historic Scientific Articles" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure¬†1</span></a>), all text within the ‚Äúplain text‚Äù sections are split into words using white space and starting (ending) <span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_sansserif">\tikzmark</span> commands are placed at the word/citation/reference/inline math start (end).
Once the TeX document is compiled, the marks are stored in the auxilary (.aux) file produced during compilation which is then parsed to match each word to its location on the final, rendered PDF page.
At this stage, documents which contain the <span id="S3.SS3.p3.1.2" class="ltx_text ltx_font_sansserif">\input</span> command are ignored as these can include text external to the document being parsed.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">Once the marked files are compiled, each page of each article is OCR‚Äôd with <span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_sansserif">Tesseract</span>, following methods used with articles from the historical holdings of the ADS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Examples of these bounding boxes and words are shown in the orange ‚ÄúOCR with boxes‚Äù panel of <a href="#S3.F1" title="Figure 1 ‚Ä£ 3.3 Marking the ‚Äúground truth‚Äù words in LaTeX &amp; OCR‚Äôing Pages ‚Ä£ 3 Methods ‚Ä£ Large Synthetic Data from the arùúíiv for OCR Post Correction of Historic Scientific Articles" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure¬†1</span></a>.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2309.11549/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="156" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Diagram of TeX parsed into its attributes (‚ÄúRaw LaTeX‚Äù, ‚ÄúSplits with regex‚Äù), and the tree structure built from the positions of these splits within the document (‚ÄúTree‚Äù), as outlined in <a href="#S3.SS2" title="3.2 Segmentation of TeX Documents ‚Ä£ 3 Methods ‚Ä£ Large Synthetic Data from the arùúíiv for OCR Post Correction of Historic Scientific Articles" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section¬†3.2</span></a>. TeX is then marked with the <span id="S3.F1.2.1" class="ltx_text ltx_font_sansserif">tikzmark</span> package and OCR‚Äôd (section from three top lines in ‚ÄúTree‚Äù shown in ‚ÄúMarked LaTeX‚Äù, <a href="#S3.SS3" title="3.3 Marking the ‚Äúground truth‚Äù words in LaTeX &amp; OCR‚Äôing Pages ‚Ä£ 3 Methods ‚Ä£ Large Synthetic Data from the arùúíiv for OCR Post Correction of Historic Scientific Articles" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section¬†3.3</span></a>). Once the TeX is compiled into a PDF, the auxiliary files are parsed to locate the SGT word locations on the rendered PDF page (‚ÄúMarked PDF‚Äù, <a href="#S3.SS4" title="3.4 The OCR-SGT Alignment Algorithm &amp; Dataset Characteristics ‚Ä£ 3 Methods ‚Ä£ Large Synthetic Data from the arùúíiv for OCR Post Correction of Historic Scientific Articles" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section¬†3.4</span></a>), OCR words are collected (‚ÄúOCR with boxes‚Äù), and SGT-OCR boxes are aligned (‚ÄúOutput data SGT: OCR-word(s)‚Äù, <a href="#S3.SS4" title="3.4 The OCR-SGT Alignment Algorithm &amp; Dataset Characteristics ‚Ä£ 3 Methods ‚Ä£ Large Synthetic Data from the arùúíiv for OCR Post Correction of Historic Scientific Articles" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section¬†3.4</span></a>). See text for more details.</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>The OCR-SGT Alignment Algorithm &amp; Dataset Characteristics</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The final step in creating our SGT - OCR dataset is to align the OCR and SGT words. In what follows, ‚Äúelement‚Äù is defined as a plain text word, inline math formula, citation, or reference. Our alignment routine is as follows:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Step 1: Locations of the bottom left and right bounds of each marked element are found from the .aux files. These locations are shown as solid magenta lines in the magenta ‚ÄúMarked PDF‚Äù panel of <a href="#S3.F1" title="Figure 1 ‚Ä£ 3.3 Marking the ‚Äúground truth‚Äù words in LaTeX &amp; OCR‚Äôing Pages ‚Ä£ 3 Methods ‚Ä£ Large Synthetic Data from the arùúíiv for OCR Post Correction of Historic Scientific Articles" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure¬†1</span></a>.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Step 2: As <span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_sansserif">tikzmark</span> gives only the lower y-position of each element, a bounding box is created by assuming 11pt font for each element (11pt font is an average value, font size is not always specified explicitly in the TeX file), shown by the dashed magenta lines in the ‚ÄúMarked PDF‚Äù panel of <a href="#S3.F1" title="Figure 1 ‚Ä£ 3.3 Marking the ‚Äúground truth‚Äù words in LaTeX &amp; OCR‚Äôing Pages ‚Ä£ 3 Methods ‚Ä£ Large Synthetic Data from the arùúíiv for OCR Post Correction of Historic Scientific Articles" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure¬†1</span></a>.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Step 3: If the bounding box is found to span more than one line, the SGT element is assumed to be hyphenated and each part is marked as a separate word. Alignment operates page-by-page, therefore hyphenated elements which span multiple pages are ignored.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Step 4: The ‚Äúraw‚Äù SGT element is extracted from the source TeX.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">Step 5: All OCR bounding boxes which overlap with a SGT box are associated with that SGT element. If an OCR bounding box is associated with more than one SGT element, the OCR element is associated with the SGT element with which it has the largest intersection-over-union (IOU).</p>
</div>
</li>
<li id="S3.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i6.p1" class="ltx_para">
<p id="S3.I1.i6.p1.1" class="ltx_p">Step 6: All OCR elements associated with a SGT element are ordered by increasing horizontal position and combined into a single OCR element for that SGT element. This is shown by the data structure in the yellow ‚ÄúOutput in SGT: OCR-words‚Äù box of <a href="#S3.F1" title="Figure 1 ‚Ä£ 3.3 Marking the ‚Äúground truth‚Äù words in LaTeX &amp; OCR‚Äôing Pages ‚Ä£ 3 Methods ‚Ä£ Large Synthetic Data from the arùúíiv for OCR Post Correction of Historic Scientific Articles" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure¬†1</span></a>.</p>
</div>
</li>
<li id="S3.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i7.p1" class="ltx_para">
<p id="S3.I1.i7.p1.1" class="ltx_p">Step 7: SGT word ‚Äútype‚Äù is stored along with SGT word (plain text, inline math, citation, reference and whether the word is hyphenated).</p>
</div>
</li>
<li id="S3.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i8.p1" class="ltx_para">
<p id="S3.I1.i8.p1.1" class="ltx_p">Step 8: Elements are ordered by <span id="S3.I1.i8.p1.1.1" class="ltx_text ltx_font_sansserif">tikzmark</span>s and aligned with edit distance operations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. <span id="S3.I1.i8.p1.1.2" class="ltx_text ltx_font_sansserif">spaCY</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> is used to tokenize aligned pages as sentences <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">While the majority of articles are aligned without error, <span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_sansserif">Tesseract</span> errors are possible on single pages.
From this corpus of 7,850
 articles which contain successfully aligned pages, our algorithm produces a total of
71,735
 pages of 1,527,118
 SGT/OCR sentence pairs which contain a total of 203,354,393
 character pairs.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.6" class="ltx_p">The relationships between SGT and OCR aligned characters closely follow other popular datasets with the majority of Levenshtein edit distance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> operations in our dataset (other datasets) being replacements <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><mo id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><csymbol cd="latexml" id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">\sim</annotation></semantics></math>61.5% (<math id="S3.SS4.p3.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.SS4.p3.2.m2.1a"><mo id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><csymbol cd="latexml" id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">\sim</annotation></semantics></math>40-60%), followed by deletions <math id="S3.SS4.p3.3.m3.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.SS4.p3.3.m3.1a"><mo id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><csymbol cd="latexml" id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">\sim</annotation></semantics></math>19.6% (<math id="S3.SS4.p3.4.m4.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.SS4.p3.4.m4.1a"><mo id="S3.SS4.p3.4.m4.1.1" xref="S3.SS4.p3.4.m4.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.1b"><csymbol cd="latexml" id="S3.SS4.p3.4.m4.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.1c">\sim</annotation></semantics></math>10-18%) and insertions <math id="S3.SS4.p3.5.m5.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.SS4.p3.5.m5.1a"><mo id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.1b"><csymbol cd="latexml" id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.1c">\sim</annotation></semantics></math>18.9% (<math id="S3.SS4.p3.6.m6.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S3.SS4.p3.6.m6.1a"><mo id="S3.SS4.p3.6.m6.1.1" xref="S3.SS4.p3.6.m6.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.6.m6.1b"><csymbol cd="latexml" id="S3.SS4.p3.6.m6.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.6.m6.1c">\sim</annotation></semantics></math>5-24%) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">Interactive versions of large confusion matrices for alphabetic characters, digits, punctuation marks and frequent words are hosted on this project‚Äôs webpage<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://readingtimemachine.github.io/projects/1-ocr-groundtruth-may2023" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://readingtimemachine.github.io/projects/1-ocr-groundtruth-may2023</a></span></span></span>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Post Correction Model Baseline Tests</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.2" class="ltx_p">To test the post correction effectiveness of our dataset we train a baseline transformer model ‚Äì <span id="S4.p1.2.1" class="ltx_text ltx_font_sansserif">byt5</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> ‚Äì with the dataset.
This model is effective for datasets such as ours which contain many out-of-vocabulary OCR words <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.
The model‚Äôs initial training uses 100k aligned sentences for training, and 5k in the validation and test datasets.
Here, transfer learning from the <span id="S4.p1.2.2" class="ltx_text" style="color:#000000;">byt5/google-small</span> model on HuggingFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> is used, and, for all models, training occurs on a NVIDIA V100 for <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.p1.1.m1.1a"><mo id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><csymbol cd="latexml" id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\sim</annotation></semantics></math>87000 iterations over <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.p1.2.m2.1a"><mo id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><csymbol cd="latexml" id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">\sim</annotation></semantics></math>24 hours, in which the model converges.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The entry above the first thick line of <a href="#S4" title="4 Post Correction Model Baseline Tests ‚Ä£ Large Synthetic Data from the arùúíiv for OCR Post Correction of Historic Scientific Articles" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section¬†4</span></a> (‚Äú<span id="S4.p2.1.1" class="ltx_text ltx_font_sansserif">byt5</span>,words‚Äù) shows the ability of the model to correct only the parts of each aligned SGT-OCR text which have been tagged as plain text in the test datasets.
Here, <span id="S4.p2.1.2" class="ltx_text ltx_font_sansserif">byt5</span> improves the character error rate (CER) by 67.35
% and the word error rate (WER) by 60.18
%.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">While the focus of this work is on correcting the plain text within our corpus, historical ADS articles also contain inline math and citations.
Here, we simplify the problem by testing the accuracy of the model on <span id="S4.p3.1.1" class="ltx_text ltx_font_italic">detecting</span> these elements in the text.
To proceed, we modify the input and output text by marking these environments with characters that do not appear in the plain text corpus.
For example, we replace each instance of a SGT or post corrected OCR inline math formula with a single character ($) and determine how often these characters align in the SGT and predicted OCR.
The ‚Äú<span id="S4.p3.1.2" class="ltx_text ltx_font_sansserif">byt5</span>,full,fixed‚Äù row in <a href="#S4" title="4 Post Correction Model Baseline Tests ‚Ä£ Large Synthetic Data from the arùúíiv for OCR Post Correction of Historic Scientific Articles" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section¬†4</span></a> lists the results of this ‚Äúfixed‚Äù model, trained on 500k ‚Äúfixed‚Äù sentences (10k in the validation and test sets).
Here, the CER and WER improvements have both increased to their highest rates of 85.51
% and 84.44
%, respectively.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.12" class="ltx_p">To test the model‚Äôs accuracy on pre-digital OCR, we apply the ‚Äú<span id="S4.p4.12.1" class="ltx_text ltx_font_sansserif">byt5</span>,full,fixed‚Äù model to <span id="S4.p4.12.2" class="ltx_text" style="color:#000000;">202
</span> hand-annotated sentences from the main text of articles in the historical ADS corpus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.
When applied to this dataset, the mean improvement, <math id="S4.p4.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.p4.1.m1.1a"><mo stretchy="false" id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><ci id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">\langle</annotation></semantics></math>I<math id="S4.p4.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.p4.2.m2.1a"><mo stretchy="false" id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><ci id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">\rangle</annotation></semantics></math>, in CER and WER from correction with the fixed-<span id="S4.p4.12.3" class="ltx_text ltx_font_sansserif">byt5</span> model (i.e.¬†‚Äú<span id="S4.p4.12.4" class="ltx_text ltx_font_sansserif">byt5</span>,full,fixed‚Äù for the ar<math id="S4.p4.3.m3.1" class="ltx_Math" alttext="\rm{\chi}" display="inline"><semantics id="S4.p4.3.m3.1a"><mi id="S4.p4.3.m3.1.1" xref="S4.p4.3.m3.1.1.cmml">œá</mi><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><ci id="S4.p4.3.m3.1.1.cmml" xref="S4.p4.3.m3.1.1">ùúí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">\rm{\chi}</annotation></semantics></math>iv data) are 7.71
% and 18.82
%, respectively, as shown in the ‚Äúhistorical,full,fixed‚Äù row of <a href="#S4" title="4 Post Correction Model Baseline Tests ‚Ä£ Large Synthetic Data from the arùúíiv for OCR Post Correction of Historic Scientific Articles" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section¬†4</span></a>.
While the improvements in CER and WER are more modest than the improvement in the ar<math id="S4.p4.4.m4.1" class="ltx_Math" alttext="\rm{\chi}" display="inline"><semantics id="S4.p4.4.m4.1a"><mi id="S4.p4.4.m4.1.1" xref="S4.p4.4.m4.1.1.cmml">œá</mi><annotation-xml encoding="MathML-Content" id="S4.p4.4.m4.1b"><ci id="S4.p4.4.m4.1.1.cmml" xref="S4.p4.4.m4.1.1">ùúí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.4.m4.1c">\rm{\chi}</annotation></semantics></math>iv dataset, they are nonetheless significantly larger than those from a generic post correction model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> (<math id="S4.p4.5.m5.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.p4.5.m5.1a"><mo stretchy="false" id="S4.p4.5.m5.1.1" xref="S4.p4.5.m5.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.p4.5.m5.1b"><ci id="S4.p4.5.m5.1.1.cmml" xref="S4.p4.5.m5.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.5.m5.1c">\langle</annotation></semantics></math>I<math id="S4.p4.6.m6.1" class="ltx_math_unparsed" alttext="\rangle_{\rm CER}" display="inline"><semantics id="S4.p4.6.m6.1a"><mrow id="S4.p4.6.m6.1b"><mo stretchy="false" id="S4.p4.6.m6.1.1">‚ü©</mo><msub id="S4.p4.6.m6.1.2"><mi id="S4.p4.6.m6.1.2a"></mi><mi id="S4.p4.6.m6.1.2.1">CER</mi></msub></mrow><annotation encoding="application/x-tex" id="S4.p4.6.m6.1c">\rangle_{\rm CER}</annotation></semantics></math>=-2499.35
%, <math id="S4.p4.7.m7.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.p4.7.m7.1a"><mo stretchy="false" id="S4.p4.7.m7.1.1" xref="S4.p4.7.m7.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.p4.7.m7.1b"><ci id="S4.p4.7.m7.1.1.cmml" xref="S4.p4.7.m7.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.7.m7.1c">\langle</annotation></semantics></math>I<math id="S4.p4.8.m8.1" class="ltx_math_unparsed" alttext="\rangle_{\rm WER}" display="inline"><semantics id="S4.p4.8.m8.1a"><mrow id="S4.p4.8.m8.1b"><mo stretchy="false" id="S4.p4.8.m8.1.1">‚ü©</mo><msub id="S4.p4.8.m8.1.2"><mi id="S4.p4.8.m8.1.2a"></mi><mi id="S4.p4.8.m8.1.2.1">WER</mi></msub></mrow><annotation encoding="application/x-tex" id="S4.p4.8.m8.1c">\rangle_{\rm WER}</annotation></semantics></math>=-499.26
%) or from when <span id="S4.p4.12.5" class="ltx_text ltx_font_sansserif">byt5</span> is trained on the words from the historical dataset alone (<math id="S4.p4.9.m9.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.p4.9.m9.1a"><mo stretchy="false" id="S4.p4.9.m9.1.1" xref="S4.p4.9.m9.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.p4.9.m9.1b"><ci id="S4.p4.9.m9.1.1.cmml" xref="S4.p4.9.m9.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.9.m9.1c">\langle</annotation></semantics></math>I<math id="S4.p4.10.m10.1" class="ltx_math_unparsed" alttext="\rangle_{\rm CER}" display="inline"><semantics id="S4.p4.10.m10.1a"><mrow id="S4.p4.10.m10.1b"><mo stretchy="false" id="S4.p4.10.m10.1.1">‚ü©</mo><msub id="S4.p4.10.m10.1.2"><mi id="S4.p4.10.m10.1.2a"></mi><mi id="S4.p4.10.m10.1.2.1">CER</mi></msub></mrow><annotation encoding="application/x-tex" id="S4.p4.10.m10.1c">\rangle_{\rm CER}</annotation></semantics></math>=-443.18
%, <math id="S4.p4.11.m11.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.p4.11.m11.1a"><mo stretchy="false" id="S4.p4.11.m11.1.1" xref="S4.p4.11.m11.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.p4.11.m11.1b"><ci id="S4.p4.11.m11.1.1.cmml" xref="S4.p4.11.m11.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.11.m11.1c">\langle</annotation></semantics></math>I<math id="S4.p4.12.m12.1" class="ltx_math_unparsed" alttext="\rangle_{\rm WER}" display="inline"><semantics id="S4.p4.12.m12.1a"><mrow id="S4.p4.12.m12.1b"><mo stretchy="false" id="S4.p4.12.m12.1.1">‚ü©</mo><msub id="S4.p4.12.m12.1.2"><mi id="S4.p4.12.m12.1.2a"></mi><mi id="S4.p4.12.m12.1.2.1">WER</mi></msub></mrow><annotation encoding="application/x-tex" id="S4.p4.12.m12.1c">\rangle_{\rm WER}</annotation></semantics></math>=-209.74
%), both of which result in a large <span id="S4.p4.12.6" class="ltx_text ltx_font_italic">negative</span> improvement.</p>
</div>
<figure id="S4.36" class="ltx_table">
<div id="S4.36.36" class="ltx_block">
<table id="S4.12.12.12" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.12.12.12.13.1" class="ltx_tr">
<td id="S4.12.12.12.13.1.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_t">¬†¬†<span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;">¬†</span>
</td>
<td id="S4.12.12.12.13.1.2" class="ltx_td ltx_align_right ltx_border_t" colspan="4">CER in % ¬†¬†<span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;">¬†</span>
</td>
<td id="S4.12.12.12.13.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="4">WER in %</td>
</tr>
<tr id="S4.12.12.12.12" class="ltx_tr">
<td id="S4.12.12.12.12.13" class="ltx_td ltx_align_right ltx_border_l">Model ¬†¬†<span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;">¬†</span>
</td>
<td id="S4.2.2.2.2.2" class="ltx_td ltx_align_center">
<math id="S4.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.1.1.1.1.1.m1.1.1" xref="S4.1.1.1.1.1.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.1.1.1.1.1.m1.1b"><ci id="S4.1.1.1.1.1.m1.1.1.cmml" xref="S4.1.1.1.1.1.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.1.1.1.1.1.m1.1c">\langle</annotation></semantics></math>B<math id="S4.2.2.2.2.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.2.2.2.2.2.m2.1a"><mo stretchy="false" id="S4.2.2.2.2.2.m2.1.1" xref="S4.2.2.2.2.2.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.2.2.2.2.2.m2.1b"><ci id="S4.2.2.2.2.2.m2.1.1.cmml" xref="S4.2.2.2.2.2.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.2.2.2.2.2.m2.1c">\rangle</annotation></semantics></math>
</td>
<td id="S4.4.4.4.4.4" class="ltx_td ltx_align_center">
<math id="S4.3.3.3.3.3.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.3.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.3.3.3.3.3.m1.1.1" xref="S4.3.3.3.3.3.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.3.3.3.3.3.m1.1b"><ci id="S4.3.3.3.3.3.m1.1.1.cmml" xref="S4.3.3.3.3.3.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.3.3.3.3.3.m1.1c">\langle</annotation></semantics></math>A<math id="S4.4.4.4.4.4.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.4.4.4.4.4.m2.1a"><mo stretchy="false" id="S4.4.4.4.4.4.m2.1.1" xref="S4.4.4.4.4.4.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.4.4.4.4.4.m2.1b"><ci id="S4.4.4.4.4.4.m2.1.1.cmml" xref="S4.4.4.4.4.4.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.4.4.4.4.4.m2.1c">\rangle</annotation></semantics></math>
</td>
<td id="S4.6.6.6.6.6" class="ltx_td ltx_align_center">
<math id="S4.5.5.5.5.5.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.5.5.5.5.5.m1.1a"><mo stretchy="false" id="S4.5.5.5.5.5.m1.1.1" xref="S4.5.5.5.5.5.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.5.5.5.5.5.m1.1b"><ci id="S4.5.5.5.5.5.m1.1.1.cmml" xref="S4.5.5.5.5.5.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.5.5.5.5.5.m1.1c">\langle</annotation></semantics></math>I<math id="S4.6.6.6.6.6.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.6.6.6.6.6.m2.1a"><mo stretchy="false" id="S4.6.6.6.6.6.m2.1.1" xref="S4.6.6.6.6.6.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.6.6.6.6.6.m2.1b"><ci id="S4.6.6.6.6.6.m2.1.1.cmml" xref="S4.6.6.6.6.6.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.6.6.6.6.6.m2.1c">\rangle</annotation></semantics></math>
</td>
<td id="S4.12.12.12.12.14" class="ltx_td ltx_align_right">% Improved ¬†¬†<span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;">¬†</span>
</td>
<td id="S4.8.8.8.8.8" class="ltx_td ltx_align_center">
<math id="S4.7.7.7.7.7.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.7.7.7.7.7.m1.1a"><mo stretchy="false" id="S4.7.7.7.7.7.m1.1.1" xref="S4.7.7.7.7.7.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.7.7.7.7.7.m1.1b"><ci id="S4.7.7.7.7.7.m1.1.1.cmml" xref="S4.7.7.7.7.7.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.7.7.7.7.7.m1.1c">\langle</annotation></semantics></math>B<math id="S4.8.8.8.8.8.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.8.8.8.8.8.m2.1a"><mo stretchy="false" id="S4.8.8.8.8.8.m2.1.1" xref="S4.8.8.8.8.8.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.8.8.8.8.8.m2.1b"><ci id="S4.8.8.8.8.8.m2.1.1.cmml" xref="S4.8.8.8.8.8.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.8.8.8.8.8.m2.1c">\rangle</annotation></semantics></math>
</td>
<td id="S4.10.10.10.10.10" class="ltx_td ltx_align_center">
<math id="S4.9.9.9.9.9.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.9.9.9.9.9.m1.1a"><mo stretchy="false" id="S4.9.9.9.9.9.m1.1.1" xref="S4.9.9.9.9.9.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.9.9.9.9.9.m1.1b"><ci id="S4.9.9.9.9.9.m1.1.1.cmml" xref="S4.9.9.9.9.9.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.9.9.9.9.9.m1.1c">\langle</annotation></semantics></math>A<math id="S4.10.10.10.10.10.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.10.10.10.10.10.m2.1a"><mo stretchy="false" id="S4.10.10.10.10.10.m2.1.1" xref="S4.10.10.10.10.10.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.10.10.10.10.10.m2.1b"><ci id="S4.10.10.10.10.10.m2.1.1.cmml" xref="S4.10.10.10.10.10.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.10.10.10.10.10.m2.1c">\rangle</annotation></semantics></math>
</td>
<td id="S4.12.12.12.12.12" class="ltx_td ltx_align_center">
<math id="S4.11.11.11.11.11.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.11.11.11.11.11.m1.1a"><mo stretchy="false" id="S4.11.11.11.11.11.m1.1.1" xref="S4.11.11.11.11.11.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.11.11.11.11.11.m1.1b"><ci id="S4.11.11.11.11.11.m1.1.1.cmml" xref="S4.11.11.11.11.11.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.11.11.11.11.11.m1.1c">\langle</annotation></semantics></math>I<math id="S4.12.12.12.12.12.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.12.12.12.12.12.m2.1a"><mo stretchy="false" id="S4.12.12.12.12.12.m2.1.1" xref="S4.12.12.12.12.12.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.12.12.12.12.12.m2.1b"><ci id="S4.12.12.12.12.12.m2.1.1.cmml" xref="S4.12.12.12.12.12.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.12.12.12.12.12.m2.1c">\rangle</annotation></semantics></math>
</td>
<td id="S4.12.12.12.12.15" class="ltx_td ltx_align_center ltx_border_r">% Improved</td>
</tr>
<tr id="S4.12.12.12.14.2" class="ltx_tr">
<td id="S4.12.12.12.14.2.1" class="ltx_td ltx_align_right ltx_border_l ltx_border_tt">
<span id="S4.12.12.12.14.2.1.1" class="ltx_text ltx_font_sansserif">byt5</span>,words ¬†¬†<span class="ltx_rule" style="width:1.0pt;background:black;display:inline-block;">¬†</span>
</td>
<td id="S4.12.12.12.14.2.2" class="ltx_td ltx_border_tt"></td>
<td id="S4.12.12.12.14.2.3" class="ltx_td ltx_border_tt"></td>
<td id="S4.12.12.12.14.2.4" class="ltx_td ltx_border_tt"></td>
<td id="S4.12.12.12.14.2.5" class="ltx_td ltx_border_tt"></td>
<td id="S4.12.12.12.14.2.6" class="ltx_td ltx_border_tt"></td>
<td id="S4.12.12.12.14.2.7" class="ltx_td ltx_border_tt"></td>
<td id="S4.12.12.12.14.2.8" class="ltx_td ltx_border_tt"></td>
<td id="S4.12.12.12.14.2.9" class="ltx_td ltx_border_tt"></td>
</tr>
</tbody>
</table>
<p id="S4.36.36.37" class="ltx_p ltx_align_center">&amp; 2.37
  67.35
  93.00
  15.34
  6.46
  60.18
  90.38

<span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;">¬†</span></p>
<p id="S4.36.36.38" class="ltx_p ltx_align_center"><span id="S4.36.36.38.1" class="ltx_text ltx_font_sansserif">byt5</span>,full,fixed  12.53
  2.47
  85.51
  98.22
  19.81
  3.84
  84.44
  99.24</p>
<p id="S4.36.36.39" class="ltx_p ltx_align_center"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;">¬†</span><span id="S4.36.36.39.1" class="ltx_text" style="color:#000000;">historical,full,fixed</span>  5.53
  3.94
  7.71
  82.67
  8.98
  8.20
  18.82
  82.67</p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_block">Table 1: </span>Mean CER and WER in percent for original datasets, <math id="S4.25.25.25.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.25.25.25.m1.1a"><mo stretchy="false" id="S4.25.25.25.m1.1.1" xref="S4.25.25.25.m1.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.25.25.25.m1.1b"><ci id="S4.25.25.25.m1.1.1.cmml" xref="S4.25.25.25.m1.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.25.25.25.m1.1c">\langle</annotation></semantics></math>B<math id="S4.26.26.26.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.26.26.26.m2.1a"><mo stretchy="false" id="S4.26.26.26.m2.1.1" xref="S4.26.26.26.m2.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.26.26.26.m2.1b"><ci id="S4.26.26.26.m2.1.1.cmml" xref="S4.26.26.26.m2.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.26.26.26.m2.1c">\rangle</annotation></semantics></math>, after post correction with listed models, <math id="S4.27.27.27.m3.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.27.27.27.m3.1a"><mo stretchy="false" id="S4.27.27.27.m3.1.1" xref="S4.27.27.27.m3.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.27.27.27.m3.1b"><ci id="S4.27.27.27.m3.1.1.cmml" xref="S4.27.27.27.m3.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.27.27.27.m3.1c">\langle</annotation></semantics></math>A<math id="S4.28.28.28.m4.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.28.28.28.m4.1a"><mo stretchy="false" id="S4.28.28.28.m4.1.1" xref="S4.28.28.28.m4.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.28.28.28.m4.1b"><ci id="S4.28.28.28.m4.1.1.cmml" xref="S4.28.28.28.m4.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.28.28.28.m4.1c">\rangle</annotation></semantics></math>, and the improvement percent, <math id="S4.29.29.29.m5.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.29.29.29.m5.1a"><mo stretchy="false" id="S4.29.29.29.m5.1.1" xref="S4.29.29.29.m5.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.29.29.29.m5.1b"><ci id="S4.29.29.29.m5.1.1.cmml" xref="S4.29.29.29.m5.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.29.29.29.m5.1c">\langle</annotation></semantics></math>I<math id="S4.30.30.30.m6.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.30.30.30.m6.1a"><mo stretchy="false" id="S4.30.30.30.m6.1.1" xref="S4.30.30.30.m6.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.30.30.30.m6.1b"><ci id="S4.30.30.30.m6.1.1.cmml" xref="S4.30.30.30.m6.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.30.30.30.m6.1c">\rangle</annotation></semantics></math>. Also shown are the percent of test instances with improvement (<math id="S4.31.31.31.m7.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.31.31.31.m7.1a"><mo stretchy="false" id="S4.31.31.31.m7.1.1" xref="S4.31.31.31.m7.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.31.31.31.m7.1b"><ci id="S4.31.31.31.m7.1.1.cmml" xref="S4.31.31.31.m7.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.31.31.31.m7.1c">\langle</annotation></semantics></math>A<math id="S4.32.32.32.m8.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.32.32.32.m8.1a"><mo stretchy="false" id="S4.32.32.32.m8.1.1" xref="S4.32.32.32.m8.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.32.32.32.m8.1b"><ci id="S4.32.32.32.m8.1.1.cmml" xref="S4.32.32.32.m8.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.32.32.32.m8.1c">\rangle</annotation></semantics></math><math id="S4.33.33.33.m9.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.33.33.33.m9.1a"><mo id="S4.33.33.33.m9.1.1" xref="S4.33.33.33.m9.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.33.33.33.m9.1b"><lt id="S4.33.33.33.m9.1.1.cmml" xref="S4.33.33.33.m9.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.33.33.33.m9.1c">&lt;</annotation></semantics></math><math id="S4.34.34.34.m10.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S4.34.34.34.m10.1a"><mo stretchy="false" id="S4.34.34.34.m10.1.1" xref="S4.34.34.34.m10.1.1.cmml">‚ü®</mo><annotation-xml encoding="MathML-Content" id="S4.34.34.34.m10.1b"><ci id="S4.34.34.34.m10.1.1.cmml" xref="S4.34.34.34.m10.1.1">‚ü®</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.34.34.34.m10.1c">\langle</annotation></semantics></math>B<math id="S4.35.35.35.m11.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S4.35.35.35.m11.1a"><mo stretchy="false" id="S4.35.35.35.m11.1.1" xref="S4.35.35.35.m11.1.1.cmml">‚ü©</mo><annotation-xml encoding="MathML-Content" id="S4.35.35.35.m11.1b"><ci id="S4.35.35.35.m11.1.1.cmml" xref="S4.35.35.35.m11.1.1">‚ü©</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.35.35.35.m11.1c">\rangle</annotation></semantics></math>) as ‚Äú% Improved‚Äù. All calculations use the ar<math id="S4.36.36.36.m12.1" class="ltx_Math" alttext="\rm{\chi}" display="inline"><semantics id="S4.36.36.36.m12.1a"><mi id="S4.36.36.36.m12.1.1" xref="S4.36.36.36.m12.1.1.cmml">œá</mi><annotation-xml encoding="MathML-Content" id="S4.36.36.36.m12.1b"><ci id="S4.36.36.36.m12.1.1.cmml" xref="S4.36.36.36.m12.1.1">ùúí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.36.36.36.m12.1c">\rm{\chi}</annotation></semantics></math>iv dataset except for the last row which uses the historical dataset.</figcaption>
<section id="S5" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Current Limitations &amp; Future Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.2" class="ltx_p">While the full dataset cannot be shared directly (ar<math id="S5.p1.1.m1.1" class="ltx_Math" alttext="\rm{\chi}" display="inline"><semantics id="S5.p1.1.m1.1a"><mi id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">œá</mi><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><ci id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">ùúí</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\rm{\chi}</annotation></semantics></math>iv administrators, Private communication), we share a subset of our aligned sentences along with analysis notebooks in GitHub<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://github.com/ReadingTimeMachine/ocr_post_correction" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ReadingTimeMachine/ocr_post_correction</a></span></span></span>.
We are currently working with the ar<math id="S5.p1.2.m2.1" class="ltx_Math" alttext="\rm{\chi}" display="inline"><semantics id="S5.p1.2.m2.1a"><mi id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml">œá</mi><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><ci id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1">ùúí</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">\rm{\chi}</annotation></semantics></math>iv to make a larger portion of the dataset available to the public.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.4" class="ltx_p">LaTeX source from <math id="S5.p2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.p2.1.m1.1a"><mo id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><csymbol cd="latexml" id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">\sim</annotation></semantics></math>1990-2010 is known to be difficult to compile due to updates in TeX compilation software <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> which, in part, lead to the drop of the initial <math id="S5.p2.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.p2.2.m2.1a"><mo id="S5.p2.2.m2.1.1" xref="S5.p2.2.m2.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S5.p2.2.m2.1b"><csymbol cd="latexml" id="S5.p2.2.m2.1.1.cmml" xref="S5.p2.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.2.m2.1c">\sim</annotation></semantics></math>65k astronomy articles to <math id="S5.p2.3.m3.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.p2.3.m3.1a"><mo id="S5.p2.3.m3.1.1" xref="S5.p2.3.m3.1.1.cmml">‚àº</mo><annotation-xml encoding="MathML-Content" id="S5.p2.3.m3.1b"><csymbol cd="latexml" id="S5.p2.3.m3.1.1.cmml" xref="S5.p2.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.3.m3.1c">\sim</annotation></semantics></math>7k. Partnership with the ar<math id="S5.p2.4.m4.1" class="ltx_Math" alttext="\rm{\chi}" display="inline"><semantics id="S5.p2.4.m4.1a"><mi id="S5.p2.4.m4.1.1" xref="S5.p2.4.m4.1.1.cmml">œá</mi><annotation-xml encoding="MathML-Content" id="S5.p2.4.m4.1b"><ci id="S5.p2.4.m4.1.1.cmml" xref="S5.p2.4.m4.1.1">ùúí</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.4.m4.1c">\rm{\chi}</annotation></semantics></math>iv to support more documents, along with adding support for a wider range of documents (e.g., those with the \input command) will increase the dataset size.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">While the accuracy of the ‚Äú<span id="S5.p3.1.1" class="ltx_text ltx_font_sansserif">byt5</span>,full,fixed‚Äù model applied to the historical dataset (‚Äúhistorical,full,fixed‚Äù) is lower overall, because there is no associated TeX with these historical documents, some ambiguity in the ‚Äúground truth‚Äù is expected (e.g., the phrase ‚Äú<math id="S5.p3.1.m1.1" class="ltx_Math" alttext="\leq" display="inline"><semantics id="S5.p3.1.m1.1a"><mo id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml">‚â§</mo><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><leq id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">\leq</annotation></semantics></math>90%‚Äù can be written as <code id="S5.p3.1.2" class="ltx_verbatim ltx_font_typewriter">$\le$90\%</code>, <code id="S5.p3.1.3" class="ltx_verbatim ltx_font_typewriter">$\le 90$\%</code> or <code id="S5.p3.1.4" class="ltx_verbatim ltx_font_typewriter">$\le 90 \%$</code> and the meaning of the phrase is unchanged).
Post correction with consideration for these nuances is relegated to future work.
</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">Finally, a larger historical dataset would undeniably enhance our post correction accuracy. A discussion of the methods used to generate a larger manual dataset is relegated to future work.</p>
</div>
<section id="S5.SS0.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Acknowledgments</h4>

<div id="S5.SS0.SSSx1.p1" class="ltx_para">
<p id="S5.SS0.SSSx1.p1.1" class="ltx_p">This work is supported by a NASA Astrophysics Data Analysis Program Grant (20-ADAP20-0225).</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="bib" class="ltx_bibliography ltx_centering">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
ar<math id="bib.bib1.1.m1.1" class="ltx_Math" alttext="\rm{\chi}" display="inline"><semantics id="bib.bib1.1.m1.1a"><mi id="bib.bib1.1.m1.1.1" xref="bib.bib1.1.m1.1.1.cmml">œá</mi><annotation-xml encoding="MathML-Content" id="bib.bib1.1.m1.1b"><ci id="bib.bib1.1.m1.1.1.cmml" xref="bib.bib1.1.m1.1.1">ùúí</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib1.1.m1.1c">\rm{\chi}</annotation></semantics></math>iv bulk downloads. <a target="_blank" href="https://info.arxiv.org/help/bulk_data_s3.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://info.arxiv.org/help/bulk_data_s3.html</a>,
accessed: 2022-03-05

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
ar<math id="bib.bib2.1.m1.1" class="ltx_Math" alttext="\rm{\chi}" display="inline"><semantics id="bib.bib2.1.m1.1a"><mi id="bib.bib2.1.m1.1.1" xref="bib.bib2.1.m1.1.1.cmml">œá</mi><annotation-xml encoding="MathML-Content" id="bib.bib2.1.m1.1b"><ci id="bib.bib2.1.m1.1.1.cmml" xref="bib.bib2.1.m1.1.1">ùúí</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib2.1.m1.1c">\rm{\chi}</annotation></semantics></math>iv hiring and needs. <a target="_blank" href="https://info.arxiv.org/hiring/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://info.arxiv.org/hiring/</a>, accessed:
2023-07-17

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Huggingface byt5-small. <a target="_blank" href="https://huggingface.co/google/byt5-small" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/google/byt5-small</a>,
accessed: 2023-03-25

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Huggingface yelpfeast/byt5-base-english-ocr-correction.
<a target="_blank" href="https://huggingface.co/yelpfeast/byt5-base-english-ocr-correction" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/yelpfeast/byt5-base-english-ocr-correction</a>,
accessed: 2023-07-20

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
The levenshtein package. <a target="_blank" href="https://github.com/maxbachmann/Levenshtein" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/maxbachmann/Levenshtein</a>,
accessed: 2023-05-29

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Opendetex. <a target="_blank" href="https://github.com/pkubowicz/opendetex" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/pkubowicz/opendetex</a>, accessed: 2023-05-29

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
The spacy sentence tokenizer. <a target="_blank" href="https://spacy.io/api/sentencizer" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://spacy.io/api/sentencizer</a>, accessed:
2023-05-29

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Texsoup. <a target="_blank" href="https://github.com/alvinwan/TexSoup" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/alvinwan/TexSoup</a>, accessed: 2022-10-30

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
The tikzmark package. <a target="_blank" href="https://texdoc.org/serve/tikzmark/0" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://texdoc.org/serve/tikzmark/0</a>, accessed:
2023-05-29

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Accomazzi, A., Kurtz, M.J., Henneken, E.A., Grant, C.S., Thompson,
D., Chyla, R., Holachek, A., Sudilovsky, V., Murray, S.S.: Improved
Functionality and Curation Support in the ADS. In: American Astronomical
Society Meeting Abstracts #225. American Astronomical Society Meeting
Abstracts, vol.¬†225, p. 336.55 (Jan 2015)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Ahuja, A., Devera, A., Fox, E.A.: Parsing electronic theses and dissertations
using object detection. In: Proceedings of the first Workshop on Information
Extraction from Scientific Publications. pp. 121‚Äì130. Association for
Computational Linguistics, Online (Nov 2022),
<a target="_blank" href="https://aclanthology.org/2022.wiesp-1.14" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.wiesp-1.14</a>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Boros, E., Nguyen, N.K., Lejeune, G., Doucet, A.: Assessing the impact of OCR
noise on multilingual event detection over digitised documents. International
Journal on Digital Libraries <span id="bib.bib12.1.1" class="ltx_text ltx_font_bold">23</span>(3), 241‚Äì266 (Sep 2022).
https://doi.org/10.1007/s00799-022-00325-2

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Chiron, G., Doucet, A., Coustaty, M., Moreux, J.P.: Icdar2017 competition on
post-ocr text correction. In: 2017 14th IAPR International Conference on
Document Analysis and Recognition (ICDAR). vol.¬†01, pp. 1423‚Äì1428 (2017).
https://doi.org/10.1109/ICDAR.2017.232

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Eichhorn, G., Accomazzi, A., Grant, C.S., Kurtz, M.J., Rey¬†Bacaicoa, V.,
Murray, S.S.: New Data and Search Features in the NASA ADS
Abstract Service p.¬†1298 (Mar 2002),
<a target="_blank" href="https://ui.adsabs.harvard.edu/abs/2002LPI....33.1298E" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ui.adsabs.harvard.edu/abs/2002LPI....33.1298E</a>, conference Name:
Lunar and Planetary Science Conference ADS Bibcode: 2002LPI‚Ä¶.33.1298E

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Etter, D., Rawls, S., Carpenter, C., Sell, G.: A Synthetic Recipe for
OCR. In: 2019 International Conference on Document Analysis and
Recognition (ICDAR). pp. 864‚Äì869. IEEE, Sydney, Australia (Sep 2019).
https://doi.org/10.1109/ICDAR.2019.00143

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Evershed, J., Fitch, K.: Correcting noisy ocr: Context beats confusion. In:
Proceedings of the First International Conference on Digital Access to
Textual Cultural Heritage. p. 45‚Äì51. DATeCH ‚Äô14, Association for Computing
Machinery, New York, NY, USA (2014). https://doi.org/10.1145/2595188.2595200

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Ginev, D., Miller, B.R.: Latexml 2012-a year of latexml. In: Intelligent
Computer Mathematics: MKM, Calculemus, DML, and Systems and Projects 2013,
Held as Part of CICM 2013, Bath, UK, July 8-12, 2013. Proceedings 6. pp.
335‚Äì338. Springer (2013)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Honnibal, M., Montani, I.: spaCy 2: Natural language understanding with
Bloom embeddings, convolutional neural networks and incremental parsing
(2017), to appear

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Jiang, M., Hu, Y., Worthey, G., Dubnicek, R.C., Capitanu, B., Kudeki, D.,
Downie, J.S., et¬†al.: The gutenberg-hathitrust parallel corpus: A real-world
dataset for noise investigation in uncorrected ocr texts. In: iConference
2021 (Poster) (2021)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Kahu, S.Y.: Figure Extraction from Scanned Electronic Theses and Dissertations.
Master‚Äôs thesis, Virginia Tech (2020)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Krishnan, P., Jawahar, C.: Generating synthetic data for text recognition.
arXiv preprint arXiv:1608.04224 (2016)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Le, T.A., Baydin, A.G., Zinkov, R., Wood, F.: Using synthetic data to train
neural networks is model-based reasoning. In: 2017 international joint
conference on neural networks (IJCNN). pp. 3514‚Äì3521. IEEE (2017)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Levenshtein, V.I.: Binary Codes Capable of Correcting Deletions, Insertions
and Reversals. Soviet Physics Doklady <span id="bib.bib23.1.1" class="ltx_text ltx_font_bold">10</span>, ¬†707 (Feb 1966)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Li, M., Cui, L., Huang, S., Wei, F., Zhou, M., Li, Z.: TableBank: A
Benchmark Dataset for Table Detection and Recognition (Jul 2020),
<a target="_blank" href="http://arxiv.org/abs/1903.01949" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1903.01949</a>, arXiv:1903.01949 [cs]

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Li, M., Xu, Y., Cui, L., Huang, S., Wei, F., Li, Z., Zhou, M.: Docbank: A
benchmark dataset for document layout analysis. In: Proceedings of the 28th
International Conference on Computational Linguistics. pp. 949‚Äì960 (2020)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Liu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininejad, M., Lewis, M.,
Zettlemoyer, L.: Multilingual Denoising Pre-training for Neural Machine
Translation. Transactions of the Association for Computational Linguistics
<span id="bib.bib26.1.1" class="ltx_text ltx_font_bold">8</span>, 726‚Äì742 (11 2020). https://doi.org/10.1162/tacl_a_00343

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Maheshwari, A., Singh, N., Krishna, A., Ramakrishnan, G.: A Benchmark and
Dataset for Post-OCR text correction in Sanskrit (Nov 2022).
https://doi.org/10.48550/arXiv.2211.07980, arXiv:2211.07980 [cs]

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Mayernik, M.S., Hart, D.L., Maull, K.E., Weber, N.M.: Assessing and tracing the
outcomes and impact of research infrastructures. Journal of the Association
for Information Science and Technology <span id="bib.bib28.1.1" class="ltx_text ltx_font_bold">68</span>(6), 1341‚Äì1359 (2017).
https://doi.org/https://doi.org/10.1002/asi.23721

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Naiman, J.P., Williams, P.K., Goodman, A.: The digitization of historical
astrophysical literature with highly localized figures and figure captions.
International Journal on Digital Libraries pp. 1‚Äì21 (2023)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Naiman, J., Williams, P.K., Goodman, A.: Figure and figure caption extraction
for mixed raster and vector pdfs: Digitization of astronomical literature
with ocr features. In: Linking Theory and Practice of Digital Libraries: 26th
International Conference on Theory and Practice of Digital Libraries, TPDL
2022, Padua, Italy, September 20‚Äì23, 2022, Proceedings. pp. 52‚Äì67. Springer
(2022)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Nguyen, T.T.H., Jatowt, A., Coustaty, M., Nguyen, N.V., Doucet, A.: Deep
Statistical Analysis of OCR Errors for Effective Post-OCR
Processing. In: 2019 ACM/IEEE Joint Conference on Digital
Libraries (JCDL). pp. 29‚Äì38 (Jun 2019). https://doi.org/10.1109/JCDL.2019.00015

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Pepe, A., Goodman, A., Muench, A.: The ADS All-Sky Survey. In:
Ballester, P., Egret, D., Lorente, N.P.F. (eds.) Astronomical Data
Analysis Software and Systems XXI. Astronomical Society of the Pacific
Conference Series, vol.¬†461, p.¬†275 (Sep 2012)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Pfahler, L., Morik, K.: Self-Supervised Pretraining of Graph Neural
Network for the Retrieval of Related Mathematical Expressions in
Scientific Articles (Aug 2022), <a target="_blank" href="http://arxiv.org/abs/2209.00446" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2209.00446</a>,
arXiv:2209.00446 [cs]

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Ramirez-Orta, J.A., Xamena, E., Maguitman, A., Milios, E., Soto, A.J.: Post-ocr
document correction with large ensembles of character sequence-to-sequence
models. In: Proceedings of the AAAI Conference on Artificial Intelligence.
vol.¬†36, pp. 11192‚Äì11199 (2022)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Ren, X., Chen, K., Sun, J.: A CNN Based Scene Chinese Text Recognition
Algorithm With Synthetic Data Engine. arXiv e-prints arXiv:1604.01891 (Apr
2016). https://doi.org/10.48550/arXiv.1604.01891

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Rigaud, C., Doucet, A., Coustaty, M., Moreux, J.P.: ICDAR 2019 Competition
on Post-OCR Text Correction

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Rigaud, C., Doucet, A., Coustaty, M., Moreux, J.P.: Icdar 2019 competition on
post-ocr text correction. In: 2019 International Conference on Document
Analysis and Recognition (ICDAR). pp. 1588‚Äì1593 (2019).
https://doi.org/10.1109/ICDAR.2019.00255

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Saier, T., F√§rber, M.: Bibliometric-enhanced arxiv: A data set for
paper-based and citation-based tasks. In: BIR@ ECIR. pp. 14‚Äì26 (2019)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Saier, T., Krause, J., F√§rber, M.: unarXive 2022: All arXiv
Publications Pre-Processed for NLP, Including Structured Full-Text and
Citation Network. arXiv e-prints arXiv:2303.14957 (Mar 2023).
https://doi.org/10.48550/arXiv.2303.14957

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Sandy, H.M., Mitchell, E., Corrado, E.M., Budd, J., West, J.D., Bossaller, J.,
VanScoy, A.: Making a case for open research: Implications for
reproducibility and transparency. Proceedings of the Association for
Information Science and Technology <span id="bib.bib40.1.1" class="ltx_text ltx_font_bold">54</span>(1), 583‚Äì586 (2017).
https://doi.org/https://doi.org/10.1002/pra2.2017.14505401079

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Schmitt-Koopmann, F.M., Huang, E.M., Darvishy, A.: Accessible PDFs:
Applying Artificial Intelligence for Automated Remediation of
STEM PDFs. In: Proceedings of the 24th International ACM SIGACCESS
Conference on Computers and Accessibility. pp.¬†1‚Äì6. ASSETS ‚Äô22,
Association for Computing Machinery, New York, NY, USA (Oct 2022).
https://doi.org/10.1145/3517428.3550407

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Smith, L., Arcand, K., Smith, R., Bookbinder, J., Smith, J.:
Capturing the many faces of an exploded star: communicating complex and
evolving astronomical data. JCOM Journal of Science Communication
<span id="bib.bib42.1.1" class="ltx_text ltx_font_bold">16</span>, 16050202 (Nov 2017). https://doi.org/10.22323/2.16050202

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Smith, R.: An overview of the tesseract ocr engine. In: Proceedings of the
Ninth International Conference on Document Analysis and Recognition - Volume
02. p. 629‚Äì633. ICDAR ‚Äô07, IEEE Computer Society, USA (2007)

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Sohmen, L., Charbonnier, J., Bl√ºmel, I., Wartena, C., Heller, L.: Figures
in scientific open access publications. In: International Conference on
Theory and Practice of Digital Libraries. pp. 220‚Äì226. Springer (2018)

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Springmann, U., Reul, C., Dipper, S., Baiter, J.: Ground truth for training ocr
engines on historical documents in german fraktur and early modern latin.
Journal for Language Technology and Computational Linguistics
<span id="bib.bib45.1.1" class="ltx_text ltx_font_bold">33</span>(1), 97‚Äì114 (2018)

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Stephens, Z.D., Lee, S.Y., Faghri, F., Campbell, R.H., Zhai, C., Efron, M.J.,
Iyer, R., Schatz, M.C., Sinha, S., Robinson, G.E.: Big data: Astronomical or
genomical? PLOS Biology <span id="bib.bib46.1.1" class="ltx_text ltx_font_bold">13</span>(7), 1‚Äì11 (07 2015).
https://doi.org/10.1371/journal.pbio.1002195

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Strien, D., Beelen, K., Coll¬†Ardanuy, M., Hosseini, K., Mcgillivray, B.,
Colavizza, G.: Assessing the impact of ocr quality on downstream nlp tasks.
SCITEPRESS-Science and Technology Publications (02 2020).
https://doi.org/10.5220/0009169004840496

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Tafti, A.P., Baghaie, A., Assefi, M., Arabnia, H.R., Yu, Z., Peissig, P.: Ocr
as a service: an experimental evaluation of google docs ocr, tesseract, abbyy
finereader, and transym. In: Advances in Visual Computing: 12th International
Symposium, ISVC 2016, Las Vegas, NV, USA, December 12-14, 2016, Proceedings,
Part I 12. pp. 735‚Äì746. Springer (2016)

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Urban, M.: An introduction to LATEX. TEX users group (1986)

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Xue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S., Kale, M., Roberts,
A., Raffel, C.: ByT5: Towards a Token-Free Future with
Pre-trained Byte-to-Byte Models. Transactions of the Association for
Computational Linguistics <span id="bib.bib50.1.1" class="ltx_text ltx_font_bold">10</span>, 291‚Äì306 (2022).
https://doi.org/10.1162/tacl_a_00461, place: Cambridge, MA Publisher: MIT Press

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Zaytsev, A.: Hathitrust and a mission for accessibility. Journal of Electronic
Publishing <span id="bib.bib51.1.1" class="ltx_text ltx_font_bold">18</span>(3) (2015)

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Zharikov, I., Nikitin, F., Vasiliev, I., Dokholyan, V.: DDI-100: Dataset
for Text Detection and Recognition. In: Proceedings of the 2020 4th
International Symposium on Computer Science and Intelligent
Control. pp.¬†1‚Äì5 (Nov 2020). https://doi.org/10.1145/3440084.3441192,
arXiv:1912.11658 [cs]

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Zhu, W., Liu, Y., Hao, L.: A Novel OCR Approach Based on Document
Layout Analysis and Text Block Classification. In: 2016 12th
International Conference on Computational Intelligence and Security
(CIS). pp. 91‚Äì94 (Dec 2016). https://doi.org/10.1109/CIS.2016.0029

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_centering ltx_role_newpage"></div>
</div>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2309.11548" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2309.11549" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2309.11549">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2309.11549" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2309.11551" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 05:14:20 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
