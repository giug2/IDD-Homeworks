<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models</title>
<!--Generated on Fri Nov 24 20:21:32 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2311.14838v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S1" title="1 Introduction ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S1.SS1" title="1.1 Data Sources ‣ 1 Introduction ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Data Sources</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S1.SS2" title="1.2 Training schedule ‣ 1 Introduction ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Training schedule</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S1.SS3" title="1.3 Data Mixing ‣ 1 Introduction ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3 </span>Data Mixing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S1.SS4" title="1.4 Data Augmentation ‣ 1 Introduction ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4 </span>Data Augmentation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S2" title="2 OpusCleaner ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>OpusCleaner</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S2.SS1" title="2.1 Data Download ‣ 2 OpusCleaner ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Data Download</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S2.SS2" title="2.2 Data Cleaning ‣ 2 OpusCleaner ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Data Cleaning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S2.SS2.SSS1" title="2.2.1 Filter and preview ‣ 2.2 Data Cleaning ‣ 2 OpusCleaner ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Filter and preview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S2.SS2.SSS2" title="2.2.2 Filters and pipelines ‣ 2.2 Data Cleaning ‣ 2 OpusCleaner ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Filters and pipelines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S2.SS2.SSS3" title="2.2.3 Processsing all data ‣ 2.2 Data Cleaning ‣ 2 OpusCleaner ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>Processsing all data</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S3" title="3 OpusTrainer ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>OpusTrainer</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="#S3.SS0.SSS0.Px1" title="Multilingual model training ‣ 3 OpusTrainer ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title">Multilingual model training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS1" title="3.1 Data Scheduling ‣ 3 OpusTrainer ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Data Scheduling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS2" title="3.2 Data Augmentation ‣ 3 OpusTrainer ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Data Augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S3.SS3" title="3.3 Terminology ‣ 3 OpusTrainer ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Terminology</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="#S4" title="4 Case study: A Robust French-English system ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Case study: A Robust French-English system</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS1" title="4.1 Test set design ‣ 4 Case study: A Robust French-English system ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Test set design</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="#S4.SS2" title="4.2 Model ‣ 4 Case study: A Robust French-English system ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="#S4.SS3" title="4.3 Results ‣ 4 Case study: A Robust French-English system ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="#S4.SS3.SSS1" title="4.3.1 Caveats ‣ 4.3 Results ‣ 4 Case study: A Robust French-English system ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Caveats</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="#S5" title="5 Conclusion ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nikolay Bogoychev 
<br class="ltx_break"/>School of Informatics 
<br class="ltx_break"/>University of Edinburgh
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">n.boogoych@ed.ac.uk</span>
<br class="ltx_break"/>Jelmer van der Linde 
<br class="ltx_break"/>School of Informatics 
<br class="ltx_break"/>University of Edinburgh 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">jelmervdl@ed.ac.uk</span>
<br class="ltx_break"/>Graeme Nail 
<br class="ltx_break"/>School of Informatics 
<br class="ltx_break"/>University of Edinburgh
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id3.3.id3">gnail@ed.ac.uk</span>
<br class="ltx_break"/>Barry Haddow 
<br class="ltx_break"/>School of Informatics 
<br class="ltx_break"/>University of Edinburgh
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id4.4.id4">bhaddow@ed.ac.uk</span>
<br class="ltx_break"/>Jaume Zaragoza-Bernabeu 
<br class="ltx_break"/>Prompsit Language Engineering 
<br class="ltx_break"/>SL (PLE), Spain 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id5.5.id5">jzaragoza@prompsit.com</span>
<br class="ltx_break"/>Gema Ramírez-Sánchez 
<br class="ltx_break"/>Prompsit Language Engineering 
<br class="ltx_break"/>SL (PLE), Spain 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id6.6.id6">gramirez@prompsit.com</span>
<br class="ltx_break"/>Lukas Weymann 
<br class="ltx_break"/>Prompsit Language Engineering 
<br class="ltx_break"/>SL (PLE), Spain 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id7.7.id7">lweymann@prompsit.com</span>
<br class="ltx_break"/>Tudor Nicolae Mateiu 
<br class="ltx_break"/>Prompsit Language Engineering 
<br class="ltx_break"/>SL (PLE), Spain 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id8.8.id8">tudornm@prompsit.com </span>
<br class="ltx_break"/>Jindřich Helcl
<br class="ltx_break"/>Faculty of Mathematics and Physics 
<br class="ltx_break"/>Charles University, Czechia 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id9.9.id9">helcl@ufal.mff.cuni.cz </span>
<br class="ltx_break"/>Mikko Aulamo
<br class="ltx_break"/>Department of Digital Humanities
<br class="ltx_break"/>Language Technology 
<br class="ltx_break"/>University of Helsinki, Finland 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id10.10.id10">mikko.aulamo@helsinki.fi</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id11.id1">Developing high quality machine translation systems is a labour intensive, challenging and confusing process for newcomers to the field. We present a pair of tools <span class="ltx_text ltx_font_italic" id="id11.id1.1">OpusCleaner</span> and <span class="ltx_text ltx_font_italic" id="id11.id1.2">OpusTrainer</span> that aim to simplify the process, reduce the amount of work and lower the entry barrier for newcomers.</p>
<p class="ltx_p" id="id12.id2"><span class="ltx_text ltx_font_italic" id="id12.id2.1">OpusCleaner</span> is a data downloading, cleaning, and proprocessing toolkit. It is designed to allow researchers to quickly download, visualise and preprocess bilingual (or monolingual) data that comes from many different sources, each of them with different quality, issues, and unique filtering/preprocessing requirements.</p>
<p class="ltx_p" id="id13.id3"><span class="ltx_text ltx_font_italic" id="id13.id3.1">OpusTrainer</span> is a data scheduling and data augmenting tool aimed at building large scale, robust machine translation systems and large language models. It features deterministic data mixing from many different sources, on-the-fly data augmentation and more.</p>
<p class="ltx_p" id="id14.id4">Using these tools, we showcase how we can use it to create high quality machine translation model robust to noisy user input; multilingual models and terminology aware models.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Machine translation is ubiquitous in modern society, however training high quality machine translation systems is not trivial. A lot of the knowledge about how to build high quality systems is not well defined, comes from experience and at times may seem counter intuitive. With <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">OpusTrainer</span> and <span class="ltx_text ltx_font_italic" id="S1.p1.1.2">OpusCleaner</span> we aim to explicitly address the main challenges in a user friendly manner and simplify the workload for machine translation researchers.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">There are several challenges when it comes to building high quality MT systems:</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Data Sources</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">Parallel data for machine translation systems comes from many different sources, that have widely varying quality. As an example, using Opus’s website<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://opus.nlpl.eu/" title="">https://opus.nlpl.eu/</a></span></span></span>, and filtering parallel data sources for Chinese to English, we are presented with a dozen different corpora. Here we find that some are in traditional script, others are in simplified script, and these may or may not have been tokenized. This is before noting any language identification issues. In order to build a high quality translation system, we need to first have quality data, which necessarily means auditing each corpus manually and then deciding how to preprocess it.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Training schedule</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.1">High quality machine translation systems require the use of backtranslation <cite class="ltx_cite ltx_citemacro_citep">(Sennrich et al., <a class="ltx_ref" href="#bib.bib8" title="">2016</a>)</cite>, usually included in the form of pretraining. Often at the end of training models are fine tuned to in-domain data. Without a training scheduler that supports different training stages, start-and-stop training approach is necessary which presents challenge for automation and increases the burden on the researcher.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Data Mixing</h3>
<div class="ltx_para" id="S1.SS3.p1">
<p class="ltx_p" id="S1.SS3.p1.1">Noisy web-crawled data is useful for translation quality, but including it too early in the training may lead to model divergence. Furthermore dirty data is orders of magnitude more available than clean manually curated parallel data. Without any upsampling, clean data might be overshadowed by dirty data, but upsampling is wasteful in terms of disk space. Finally, multilingual models require careful data mixing such that low resource language languages are not overwhelmed by high resource ones, without a training scheduler that supports data source mixing, this is achieved by upsampling low resource data and carefully mixing and shuffling it in the training data.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.4 </span>Data Augmentation</h3>
<div class="ltx_para" id="S1.SS4.p1">
<p class="ltx_p" id="S1.SS4.p1.1">Machine translation models are training on sanitised parallel data that is usually not representative of noisy user input:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Typos are quite rare in clean data, and spellchecker is often used on web-crawled data.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">All caps and title-case text are often missing.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Emoji are basically non existent in parallel data.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Models are not trained to cope with untranslatable tokens, which should be copied between the source and the target language.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.SS4.p2">
<p class="ltx_p" id="S1.SS4.p2.1">OpusTrainer and OpusCleaner are designed to resolve the above issues, and make it easy for a novice user to build high quality translation systems, by explicitly setting the expectations that training data must be carefully audited, and training data must be scheduled.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>OpusCleaner</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In order to address the daunting task of data cleaning, we developed OpusCleaner, a single graphical frontend that does data downloading and cleaning, while being modular to allow for custom modifications depending on the language pair in question. We show screenshots of the welcome screen on Figure  <a class="ltx_ref" href="#S2.F1" title="Figure 1 ‣ 2 OpusCleaner ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="531" id="S2.F1.g1" src="extracted/5254613/screenshots/OpusCleaner/initial_nodatasets.png" width="419"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">Initial screen of OpusCleaner</span></figcaption>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data Download</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">OpusCleaner provides seamless integration with MTData <cite class="ltx_cite ltx_citemacro_citep">(Gowda et al., <a class="ltx_ref" href="#bib.bib3" title="">2021</a>)</cite> as shown on Figures  <a class="ltx_ref" href="#S2.F2" title="Figure 2 ‣ 2.1 Data Download ‣ 2 OpusCleaner ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_tag">2</span></a> and  <a class="ltx_ref" href="#S2.F3" title="Figure 3 ‣ 2.1 Data Download ‣ 2 OpusCleaner ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_tag">3</span></a>. Datasets can be searched by languages and then downloaded individually, or in bulk. Basic information about each dataset (number of lines, version, size) are shown, as well as link to the dataset description page in Opus.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Additionally, adding one’s own custom datasets is possible.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="187" id="S2.F2.g1" src="extracted/5254613/screenshots/OpusCleaner/initial.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">Search dataset pane</span></figcaption>
</figure>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="300" id="S2.F3.g1" src="extracted/5254613/screenshots/OpusCleaner/dataset_search.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S2.F3.3.2" style="font-size:90%;">Search and download dataset with links to dataset and basic information.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data Cleaning</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Once all datasets are acquired, we can navigate to the Data Tailor screen (Figure <a class="ltx_ref" href="#S2.F4" title="Figure 4 ‣ 2.2 Data Cleaning ‣ 2 OpusCleaner ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_tag">4</span></a>) where we can label every dataset with an arbitrary label (Such as <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">medium</span> or <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.2">dirty</span>) so that we can keep track of the overall quality of each dataset.</p>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="205" id="S2.F4.g1" src="extracted/5254613/screenshots/OpusCleaner/data_tayloring.png" width="419"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S2.F4.3.2" style="font-size:90%;">Initial screen of data tailoring, as well as dataset labelling.</span></figcaption>
</figure>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Filter and preview</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">For each dataset, we visualise a sample of 3000 sentences that includes the first 100, the last 100 and random lines in between. From this window we can identidy the idiosyncraticies of that dataset and add the appropriate filters to fix them. For example, if we spot that some lines are in the wrong language (Figure <a class="ltx_ref" href="#S2.F5" title="Figure 5 ‣ 2.2.1 Filter and preview ‣ 2.2 Data Cleaning ‣ 2 OpusCleaner ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_tag">5</span></a>) we can add language identifier filter and see the result of it in the preview window (Figure <a class="ltx_ref" href="#S2.F6" title="Figure 6 ‣ 2.2.1 Filter and preview ‣ 2.2 Data Cleaning ‣ 2 OpusCleaner ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
<figure class="ltx_figure" id="S2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="293" id="S2.F5.g1" src="extracted/5254613/screenshots/OpusCleaner/data_cleaning_screen.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S2.F5.3.2" style="font-size:90%;">Initial view of dataset cleaning with some sentences obviously in the wrong language.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S2.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="295" id="S2.F6.g1" src="extracted/5254613/screenshots/OpusCleaner/post_fasttext.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S2.F6.3.2" style="font-size:90%;">Fasttext langid filter removes lines in wrong language.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1">Another example is finding mismatched punctuation on the source and the target (Figure <a class="ltx_ref" href="#S2.F7" title="Figure 7 ‣ 2.2.1 Filter and preview ‣ 2.2 Data Cleaning ‣ 2 OpusCleaner ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_tag">7</span></a>). We can then create a simple filter that fixes the issue and apply it, see the result (Figure <a class="ltx_ref" href="#S2.F8" title="Figure 8 ‣ 2.2.1 Filter and preview ‣ 2.2 Data Cleaning ‣ 2 OpusCleaner ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_tag">8</span></a>).</p>
</div>
<figure class="ltx_figure" id="S2.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="295" id="S2.F7.g1" src="extracted/5254613/screenshots/OpusCleaner/mismatch_punkt.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S2.F7.3.2" style="font-size:90%;">Mismatched punctuation on the source and the target.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S2.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="295" id="S2.F8.g1" src="extracted/5254613/screenshots/OpusCleaner/mismatch_punkt_fix.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S2.F8.3.2" style="font-size:90%;">Fixing mismatched punctuation.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Filters and pipelines</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">OpusCleaner is designed to clean data in a pipelined manner. Multiple filters are chained where every filter receives data on <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS2.p1.1.1">stdin</span> and outputs it on <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS2.p1.1.2">stdout</span>. OpusCleaner itself takes care of managing the pipeline. A typical pipeline would have a number of filters chained up as shown on Figure <a class="ltx_ref" href="#S2.F9" title="Figure 9 ‣ 2.2.2 Filters and pipelines ‣ 2.2 Data Cleaning ‣ 2 OpusCleaner ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="310" id="S2.F9.g1" src="extracted/5254613/screenshots/OpusCleaner/filter_view.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="S2.F9.3.2" style="font-size:90%;">Adding multiple filters and visualising the difference.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.SSS2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.p2.1">We support 28 built in filters with custom user filters supported by simply providing a json configuration file that specifies path to filter executable and optionally what arguments it should have.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Processsing all data</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">Once we have determined filters for every single downloaded dataset, we run a command line utility that does batch processing of all datasets, taking care of also cutting up files and parallelising processing. Once all processing is done, we provide an utility to deduplicate the data but preserving the split of datasets and then the user can proceed with training the machine translation system.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS3.p2">
<p class="ltx_p" id="S2.SS2.SSS3.p2.1">OpusCleaner<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/hplt-project/OpusCleaner" title="">https://github.com/hplt-project/OpusCleaner</a></span></span></span> is open source, under active development and available for free for anyone to use.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>OpusTrainer</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">As discussed in section  <a class="ltx_ref" href="#S1" title="1 Introduction ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_tag">1</span></a>, training high quality machine translation systems requires carefully combining parallel data from different sources and quality levels; applying on the fly modifications to it and more.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">This is challenging to achieve with neural network toolkits that make use of static training data, because ideally we want to modify the data mixture and potentially augment it on the fly, without having to <span class="ltx_text ltx_font_italic" id="S3.p2.1.1">prepare</span> the data first and write it to disk which is wasteful.</p>
</div>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Multilingual model training</h5>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.1">The problem is exacerbated when training many-to-many or English-to-many multilingual models where high resource languages would often have orders of magnitude more data than low resource languages. In order for a multilingual model to train well in this setting, it needs to see balanced data from all languages <cite class="ltx_cite ltx_citemacro_citep">(Freitag and Firat, <a class="ltx_ref" href="#bib.bib2" title="">2020</a>)</cite>. Doing this by concatenating and upsampling data (in order to get equal amounts of data seen for all languages), would waste multiple terabytes of disk space.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Scheduling</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">OpusTrainer solves this problem by streaming and mixing data from multiple sources. OpusTrainer uses a simple yaml configuration file where the user can declare all of their data sources and a desired mix of them for different stages of training. OpusTrainer then reads in the data from different sources and then outputs the desired mix to <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">stdout</span>. OpusTrainer is meant to be used with neural network toolkits that support reading data from <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.2">stdin</span> such as Marian <cite class="ltx_cite ltx_citemacro_citep">(Junczys-Dowmunt et al., <a class="ltx_ref" href="#bib.bib4" title="">2018</a>)</cite>, but it can also output the desired data mix to a file, making it usable with all toolkits. An example configuration that describes a full training run with various data mixings for different stages of training can be seen on Figure <a class="ltx_ref" href="#S3.F10" title="Figure 10 ‣ 3.1 Data Scheduling ‣ 3 OpusTrainer ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="632" id="S3.F10.g1" src="extracted/5254613/screenshots/OpusTrainer/opustrainer_basic.png" width="419"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F10.2.1.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" id="S3.F10.3.2" style="font-size:90%;">OpusTrainer basic configuration defining the data scheduling for training a model.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data Augmentation</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Humans are very robust to decoding noisy texts,
but this can pose a major challenge to machine translation systems due to the way we collect our training data:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Title Case and Upper Case parallel data is quite rare in training data, and is sometimes regularised during acquisition.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Typos are also comparatively rare in training data, because either we use clean sources or we perform spellchecking on web crawled sources.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">Emojis, which human readers expect to be copied over from the source to the target, are not seen during training, because typically lines containing emojis are removed from the training data at preprocessing steps.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.SS2.p1.2">In order to alleviate these issues, OpusTrainer provides multiple data modifiers which can be applied on the fly, at random on the training data:</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1">UpperCaser and TitleCaser</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">Typo modifier, which inserts typos in words during training</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1">Merge modifier, which randomly merges several input sentences together to help the model be more robust to longer sentences.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1">Noise modifier, that generates random sentences consisting of unicode noise, identical on both the source and the target side. This modifier teaches the model to copy unknown strings to the target side.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i5.p1">
<p class="ltx_p" id="S3.I2.i5.p1.1">Inline Noise modifier: A more complicated version of the above that uses word alignments in order to <span class="ltx_text ltx_font_italic" id="S3.I2.i5.p1.1.1">inject</span> noisy unicode characters (including Emoji) in approximately the same logical place on both the source and the target side. This modifier teaches the model that unknown sequences of <span class="ltx_text ltx_font_italic" id="S3.I2.i5.p1.1.2">&lt;unk&gt;</span> characters should be just copied on the target side.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">All of those modifiers are applied to each sentence in the training data with a user defined probability as shown on Figure <a class="ltx_ref" href="#S3.F11" title="Figure 11 ‣ 3.2 Data Augmentation ‣ 3 OpusTrainer ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="214" id="S3.F11.g1" src="extracted/5254613/screenshots/OpusTrainer/modifiers.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F11.2.1.1" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text" id="S3.F11.3.2" style="font-size:90%;">Different modifiers specified in YAML format to be used during training.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Terminology</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">OpusTrainer is able to leverage word alignment information to produce terminology augmented systems, precisely as the one described in <cite class="ltx_cite ltx_citemacro_citet">Bogoychev and Chen (<a class="ltx_ref" href="#bib.bib1" title="">2023</a>)</cite>. This is achieved by finding bijective word alignment mappings between the source and the target sentences and at randomly injecting terminology hints in the source, precisely like the one show on  <a class="ltx_ref" href="#S3.F12" title="Figure 12 ‣ 3.3 Terminology ‣ 3 OpusTrainer ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_tag">12</span></a>.</p>
</div>
<figure class="ltx_figure ltx_align_center" id="S3.F12">
<p class="ltx_p ltx_align_center" id="S3.F12.2.2">Where is the airport? <math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S3.F12.1.1.m1.1"><semantics id="S3.F12.1.1.m1.1a"><mo id="S3.F12.1.1.m1.1.1" stretchy="false" xref="S3.F12.1.1.m1.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S3.F12.1.1.m1.1b"><ci id="S3.F12.1.1.m1.1.1.cmml" xref="S3.F12.1.1.m1.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F12.1.1.m1.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.F12.1.1.m1.1d">↔</annotation></semantics></math> Wo ist der Flughafen? 
<br class="ltx_break"/>Where is the airport <span class="ltx_text ltx_font_italic" id="S3.F12.2.2.1">__target__ Flughafen __done__</span>? <math alttext="\leftrightarrow" class="ltx_Math" display="inline" id="S3.F12.2.2.m2.1"><semantics id="S3.F12.2.2.m2.1a"><mo id="S3.F12.2.2.m2.1.1" stretchy="false" xref="S3.F12.2.2.m2.1.1.cmml">↔</mo><annotation-xml encoding="MathML-Content" id="S3.F12.2.2.m2.1b"><ci id="S3.F12.2.2.m2.1.1.cmml" xref="S3.F12.2.2.m2.1.1">↔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F12.2.2.m2.1c">\leftrightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.F12.2.2.m2.1d">↔</annotation></semantics></math> Wo ist der Flughafen? 
<br class="ltx_break"/></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F12.6.1.1" style="font-size:90%;">Figure 12</span>: </span><span class="ltx_text" id="S3.F12.7.2" style="font-size:90%;">Terminology augmentation in practise. During training it is hinted that the target word <span class="ltx_text ltx_font_italic" id="S3.F12.7.2.1">Flughafen</span> corresponds to <span class="ltx_text ltx_font_italic" id="S3.F12.7.2.2">Airport</span>, so that at inference when providing the model with terminology hints it will know how to incorporate them at the output.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">These terminology hints can then be used at inference time, and the model will know how to incorporate the desired terminology hint at the target side. The relevant training options are shown on figure  <a class="ltx_ref" href="#S3.F13" title="Figure 13 ‣ 3.3 Terminology ‣ 3 OpusTrainer ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_tag">13</span></a></p>
</div>
<figure class="ltx_figure" id="S3.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="150" id="S3.F13.g1" src="extracted/5254613/screenshots/OpusTrainer/tags_modifier.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F13.2.1.1" style="font-size:90%;">Figure 13</span>: </span><span class="ltx_text" id="S3.F13.3.2" style="font-size:90%;">Tag modifier is used to add terminology hints to the source during training. Values of 3% to 7% seem to work well in practise.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">OpusTrainer is open source and available on GitHub,<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/hplt-project/OpusTrainer" title="">https://github.com/hplt-project/OpusTrainer</a></span></span></span> with ample documentation and examples. OpusTrainer is designed to be used mainly with neural network toolkits that read in training input on <span class="ltx_text ltx_font_italic" id="S3.SS3.p3.1.1">stdin</span>, as it takes care of shuffling between epochs, resuming training and all other functions normally done by the data module of a neural network toolkit. It can, however, also be used to write a preprocessed training corpus on disk so toolkits that do not support reading <span class="ltx_text ltx_font_italic" id="S3.SS3.p3.1.2">stdin</span> can also make use of it.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Case study: A Robust French-English system</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We highlight the use cases of data augmentation by using OpusCleaner and OpusTrainer to train a French-English machine translation system. We define robustness as the following criteria, which are all common concerns for real world web text.</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Accurate translation of URLs (URLs need to be copied to the target side without any modification).</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">Accurate copy behaviour on OOV tokens such as emoji or snippets of foreign language texts. The latter often occur in wikipedia, where foreign language terms such as named entities appear alongside their local language transliteration.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">No quality loss when translating Upper Case and Title case texts compared to normal cased text (All caps and tittle case often appear in tittles of newspapers).</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1">Robustness to typos (social media users).</p>
</div>
</li>
</ul>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Test set design</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">As a baseline test set we use <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">newstest15</span> and we make several version of it to more accurately measure robustness.</p>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1">Title Case version of the test set</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1">All caps version of the test set</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1">Typo-ed version of the test set, where we insert 4 typos in each line using the python’s typo library.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pypi.org/project/typo/" title="">https://pypi.org/project/typo/</a></span></span></span></p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i4.p1">
<p class="ltx_p" id="S4.I2.i4.p1.1">Emoji augmented test set where we insert random emoji in corresponding places on the source and the target, by using precomputed word alignments in order to place the emoji in both texts in the correct corresponding location. Example on figure  <a class="ltx_ref" href="#S4.F14" title="Figure 14 ‣ 4.1 Test set design ‣ 4 Case study: A Robust French-English system ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_tag">14</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i5.p1">
<p class="ltx_p" id="S4.I2.i5.p1.1">Random unicode sequence augmented test set where the random unicode sequences are inserted in the same manner as the emoji. Example on figure  <a class="ltx_ref" href="#S4.F14" title="Figure 14 ‣ 4.1 Test set design ‣ 4 Case study: A Robust French-English system ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_tag">14</span></a>.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S4.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="156" id="S4.F14.g1" src="extracted/5254613/screenshots/fr_aug.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F14.2.1.1" style="font-size:90%;">Figure 14</span>: </span><span class="ltx_text" id="S4.F14.3.2" style="font-size:90%;">Example cases of noise/emoji inside the source and the corresponding target translation. We aim for our model to be able to reproduce those at decode time.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">On top of that we prepare a dataset of sentences containing URLs from the paracrawl project. We take sentences containing exactly the same URLs on both the source and the target, then we remove the URLs and take the top 1500 sentences according to their bicleaner-ai <cite class="ltx_cite ltx_citemacro_citep">("Zaragoza-Bernabeu et al., <a class="ltx_ref" href="#bib.bib10" title="">"2022"</a>)</cite> score and reinsert the URLs.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">For quality we report BLEU, but we also use several specific metrics. For the URL test set we measure the percentage of exact matches of URLs. For datasets with tittle case and all caps we measure as well BLEU-uncased to see how good translation quality is, regardless of the case outputted. Finally, for datasets with emoji and unicode sequences, we extract all of the OOV characters and measure ChrF <cite class="ltx_cite ltx_citemacro_citep">(Popović, <a class="ltx_ref" href="#bib.bib6" title="">2015</a>)</cite> on them only, so that we can see how effective our system is at copying them to the target side.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Model</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">For training data we use all of the available French-English data accessible through MTData <cite class="ltx_cite ltx_citemacro_citep">(Gowda et al., <a class="ltx_ref" href="#bib.bib3" title="">2021</a>)</cite> and we clean it using OpusCleaner.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">We split the data into four categories based on its providence and subjective perceived quality through manual inspection:</p>
<ul class="ltx_itemize" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1">Canonically clean datasets such as Europarl, Un are designated as clean (22M parallel sentences).</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.1">Slightly less clean data (9M), designated as clean<span class="ltx_text ltx_font_italic" id="S4.I3.i2.p1.1.1">ish</span>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i3.p1">
<p class="ltx_p" id="S4.I3.i3.p1.1">Not clean data, but not generated from crawled sources (16M), designated as medium.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i4.p1">
<p class="ltx_p" id="S4.I3.i4.p1.1">Web crawled data is designated as dirty (363M)</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">We use Marian <cite class="ltx_cite ltx_citemacro_citep">(Junczys-Dowmunt et al., <a class="ltx_ref" href="#bib.bib4" title="">2018</a>)</cite> to train transformer-big <cite class="ltx_cite ltx_citemacro_citet">Vaswani et al. (<a class="ltx_ref" href="#bib.bib9" title="">2017</a>)</cite> models on the training data with varying degree of data augmentation. We train 7 different models with various additional <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.1">perks</span>, some related to data augmentation, some not in order to show how we progressively achieve a more robust model.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<ol class="ltx_enumerate" id="S4.I4">
<li class="ltx_item" id="S4.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I4.i1.p1">
<p class="ltx_p" id="S4.I4.i1.p1.1">Pure model</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I4.i2.p1">
<p class="ltx_p" id="S4.I4.i2.p1.1">+ Sentencepiece sampling <cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson, <a class="ltx_ref" href="#bib.bib5" title="">2018</a>)</cite>. Sentencepiece sampling makes splits of words non-deterministic, potentially making unseen words handling more robust.</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I4.i3.p1">
<p class="ltx_p" id="S4.I4.i3.p1.1">+ UpperCase and LowerCase</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S4.I4.i4.p1">
<p class="ltx_p" id="S4.I4.i4.p1.1">+ typos</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S4.I4.i5.p1">
<p class="ltx_p" id="S4.I4.i5.p1.1">+ Unicode Vocabulary Fallback. Sentencepiece models can’t split OOV tokens such as Chinese characters into subwords, but if we consider that every character is represented by unicode bytes, we can split unseen characters such as emoji and hanzi
</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S4.I4.i6.p1">
<p class="ltx_p" id="S4.I4.i6.p1.1">+ noisy sentences</p>
</div>
</li>
<li class="ltx_item" id="S4.I4.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="S4.I4.i7.p1">
<p class="ltx_p" id="S4.I4.i7.p1.1">+ inline noise</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_phantom" id="S4.SS2.p4.1.1"><span style="visibility:hidden">Hex</span></span>
<br class="ltx_break"/></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We present our results on table  <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.3 Results ‣ 4 Case study: A Robust French-English system ‣ OpusCleaner and OpusTrainer, open source toolkits for training Machine Translation and Large language models"><span class="ltx_text ltx_ref_tag">1</span></a>. We train 7 different systems with different degrees of augmentation. We can see that progressively, as we add more modifiers to the training set up, the model becomes more robust to various sources of noisy user input. System 3 onwards have capture TitleCase and UpperCase with relatively small performance loss compared to plain sentences. System 5 that uses UTF fallback for OOV tokens starts capturing emoji and other OOV tokens. Systems 6 and 7 enhance the training data with lots of noisy examples and that leads to really good copy rate of OOV tokens to the target side, as shown in the two ChrF columns.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.2.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T1.2.1.1.1"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="10" id="S4.T1.2.1.1.2"><span class="ltx_text" id="S4.T1.2.1.1.2.1" style="font-size:90%;">newstest15 BLEU</span></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.3"></th>
<td class="ltx_td ltx_border_tt" id="S4.T1.2.1.1.4"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.2">
<td class="ltx_td ltx_border_t" id="S4.T1.2.2.2.1"></td>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.2.2"><span class="ltx_text" id="S4.T1.2.2.2.2.1" style="font-size:90%;">plain</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.2.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.2.2.2.3.1">
<tr class="ltx_tr" id="S4.T1.2.2.2.3.1.1">
<td class="ltx_td ltx_align_center" id="S4.T1.2.2.2.3.1.1.1"><span class="ltx_text" id="S4.T1.2.2.2.3.1.1.1.1" style="font-size:90%;">TC</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.2.3.1.2">
<td class="ltx_td ltx_align_center" id="S4.T1.2.2.2.3.1.2.1"><span class="ltx_text" id="S4.T1.2.2.2.3.1.2.1.1" style="font-size:90%;">uncased</span></td>
</tr>
</table>
<span class="ltx_text" id="S4.T1.2.2.2.3.2" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.2.4"><span class="ltx_text" id="S4.T1.2.2.2.4.1" style="font-size:90%;">TC</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.2.5">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.2.2.2.5.1">
<tr class="ltx_tr" id="S4.T1.2.2.2.5.1.1">
<td class="ltx_td ltx_align_left" id="S4.T1.2.2.2.5.1.1.1"><span class="ltx_text" id="S4.T1.2.2.2.5.1.1.1.1" style="font-size:90%;">CAPS</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.2.5.1.2">
<td class="ltx_td ltx_align_left" id="S4.T1.2.2.2.5.1.2.1"><span class="ltx_text" id="S4.T1.2.2.2.5.1.2.1.1" style="font-size:90%;">uncased</span></td>
</tr>
</table>
<span class="ltx_text" id="S4.T1.2.2.2.5.2" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.2.6"><span class="ltx_text" id="S4.T1.2.2.2.6.1" style="font-size:90%;">CAPS</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.2.7"><span class="ltx_text" id="S4.T1.2.2.2.7.1" style="font-size:90%;">typo</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.2.8"><span class="ltx_text" id="S4.T1.2.2.2.8.1" style="font-size:90%;">noise</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.2.9">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.2.2.2.9.1">
<tr class="ltx_tr" id="S4.T1.2.2.2.9.1.1">
<td class="ltx_td ltx_align_center" id="S4.T1.2.2.2.9.1.1.1">
<span class="ltx_text" id="S4.T1.2.2.2.9.1.1.1.1" style="font-size:90%;">noise</span><sup class="ltx_sup" id="S4.T1.2.2.2.9.1.1.1.2"><span class="ltx_text" id="S4.T1.2.2.2.9.1.1.1.2.1" style="font-size:90%;">1</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.2.9.1.2">
<td class="ltx_td ltx_align_center" id="S4.T1.2.2.2.9.1.2.1"><span class="ltx_text" id="S4.T1.2.2.2.9.1.2.1.1" style="font-size:90%;">chrf</span></td>
</tr>
</table>
<span class="ltx_text" id="S4.T1.2.2.2.9.2" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.2.10"><span class="ltx_text" id="S4.T1.2.2.2.10.1" style="font-size:90%;">emoji</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.2.11">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.2.2.2.11.1">
<tr class="ltx_tr" id="S4.T1.2.2.2.11.1.1">
<td class="ltx_td ltx_align_center" id="S4.T1.2.2.2.11.1.1.1">
<span class="ltx_text" id="S4.T1.2.2.2.11.1.1.1.1" style="font-size:90%;">emoji</span><sup class="ltx_sup" id="S4.T1.2.2.2.11.1.1.1.2"><span class="ltx_text" id="S4.T1.2.2.2.11.1.1.1.2.1" style="font-size:90%;">1</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.2.11.1.2">
<td class="ltx_td ltx_align_center" id="S4.T1.2.2.2.11.1.2.1"><span class="ltx_text" id="S4.T1.2.2.2.11.1.2.1.1" style="font-size:90%;">chrf</span></td>
</tr>
</table>
<span class="ltx_text" id="S4.T1.2.2.2.11.2" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.2.12">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.2.2.2.12.1">
<tr class="ltx_tr" id="S4.T1.2.2.2.12.1.1">
<td class="ltx_td ltx_align_center" id="S4.T1.2.2.2.12.1.1.1"><span class="ltx_text" id="S4.T1.2.2.2.12.1.1.1.1" style="font-size:90%;">url</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.2.12.1.2">
<td class="ltx_td ltx_align_center" id="S4.T1.2.2.2.12.1.2.1"><span class="ltx_text" id="S4.T1.2.2.2.12.1.2.1.1" style="font-size:90%;">BLEU</span></td>
</tr>
</table>
<span class="ltx_text" id="S4.T1.2.2.2.12.2" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.2.13">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.2.2.2.13.1">
<tr class="ltx_tr" id="S4.T1.2.2.2.13.1.1">
<td class="ltx_td ltx_align_center" id="S4.T1.2.2.2.13.1.1.1"><span class="ltx_text" id="S4.T1.2.2.2.13.1.1.1.1" style="font-size:90%;">URL only</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.2.2.13.1.2">
<td class="ltx_td ltx_align_center" id="S4.T1.2.2.2.13.1.2.1"><span class="ltx_text" id="S4.T1.2.2.2.13.1.2.1.1" style="font-size:90%;">precision</span></td>
</tr>
</table>
<span class="ltx_text" id="S4.T1.2.2.2.13.2" style="font-size:90%;"></span>
</th>
</tr>
<tr class="ltx_tr" id="S4.T1.2.3.3">
<td class="ltx_td ltx_align_right" id="S4.T1.2.3.3.1"><span class="ltx_text" id="S4.T1.2.3.3.1.1" style="font-size:90%;">baseline (1)</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.3.3.2"><span class="ltx_text" id="S4.T1.2.3.3.2.1" style="font-size:90%;">40</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.3.3.3"><span class="ltx_text" id="S4.T1.2.3.3.3.1" style="font-size:90%;">34.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.3.3.4"><span class="ltx_text" id="S4.T1.2.3.3.4.1" style="font-size:90%;">8.6</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.3.3.5"><span class="ltx_text" id="S4.T1.2.3.3.5.1" style="font-size:90%;">21.5</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.3.3.6"><span class="ltx_text" id="S4.T1.2.3.3.6.1" style="font-size:90%;">20.5</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.3.3.7"><span class="ltx_text" id="S4.T1.2.3.3.7.1" style="font-size:90%;">29.6</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.3.3.8"><span class="ltx_text" id="S4.T1.2.3.3.8.1" style="font-size:90%;">34.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.3.3.9"><span class="ltx_text" id="S4.T1.2.3.3.9.1" style="font-size:90%;">0</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.3.3.10"><span class="ltx_text" id="S4.T1.2.3.3.10.1" style="font-size:90%;">35.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.3.3.11"><span class="ltx_text" id="S4.T1.2.3.3.11.1" style="font-size:90%;">0.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.3.3.12"><span class="ltx_text" id="S4.T1.2.3.3.12.1" style="font-size:90%;">62.7</span></td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.3.3.13"><span class="ltx_text" id="S4.T1.2.3.3.13.1" style="font-size:90%;">90%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.4.4">
<td class="ltx_td ltx_align_right" id="S4.T1.2.4.4.1"><span class="ltx_text" id="S4.T1.2.4.4.1.1" style="font-size:90%;">+ spm sample (2)</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.4.4.2"><span class="ltx_text" id="S4.T1.2.4.4.2.1" style="font-size:90%;">39</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.4.4.3"><span class="ltx_text" id="S4.T1.2.4.4.3.1" style="font-size:90%;">36.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.4.4.4"><span class="ltx_text" id="S4.T1.2.4.4.4.1" style="font-size:90%;">9.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.4.4.5"><span class="ltx_text" id="S4.T1.2.4.4.5.1" style="font-size:90%;">29.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.4.4.6"><span class="ltx_text" id="S4.T1.2.4.4.6.1" style="font-size:90%;">21.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.4.4.7"><span class="ltx_text" id="S4.T1.2.4.4.7.1" style="font-size:90%;">30.5</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.4.4.8"><span class="ltx_text" id="S4.T1.2.4.4.8.1" style="font-size:90%;">33.4</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.4.4.9"><span class="ltx_text" id="S4.T1.2.4.4.9.1" style="font-size:90%;">0.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.4.4.10"><span class="ltx_text" id="S4.T1.2.4.4.10.1" style="font-size:90%;">34.7</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.4.4.11"><span class="ltx_text" id="S4.T1.2.4.4.11.1" style="font-size:90%;">0.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.4.4.12"><span class="ltx_text" id="S4.T1.2.4.4.12.1" style="font-size:90%;">61.4</span></td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.4.4.13"><span class="ltx_text" id="S4.T1.2.4.4.13.1" style="font-size:90%;">87%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.5.5">
<td class="ltx_td ltx_align_right" id="S4.T1.2.5.5.1"><span class="ltx_text" id="S4.T1.2.5.5.1.1" style="font-size:90%;">+ UC/LC noise (3)</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.5.5.2"><span class="ltx_text" id="S4.T1.2.5.5.2.1" style="font-size:90%;">38.4</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.5.5.3"><span class="ltx_text" id="S4.T1.2.5.5.3.1" style="font-size:90%;">37.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.5.5.4"><span class="ltx_text" id="S4.T1.2.5.5.4.1" style="font-size:90%;">36.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.5.5.5"><span class="ltx_text" id="S4.T1.2.5.5.5.1" style="font-size:90%;">34.5</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.5.5.6"><span class="ltx_text" id="S4.T1.2.5.5.6.1" style="font-size:90%;">34.5</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.5.5.7"><span class="ltx_text" id="S4.T1.2.5.5.7.1" style="font-size:90%;">29.7</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.5.5.8"><span class="ltx_text" id="S4.T1.2.5.5.8.1" style="font-size:90%;">32.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.5.5.9"><span class="ltx_text" id="S4.T1.2.5.5.9.1" style="font-size:90%;">0.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.5.5.10"><span class="ltx_text" id="S4.T1.2.5.5.10.1" style="font-size:90%;">34.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.5.5.11"><span class="ltx_text" id="S4.T1.2.5.5.11.1" style="font-size:90%;">0.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.5.5.12"><span class="ltx_text" id="S4.T1.2.5.5.12.1" style="font-size:90%;">60.9</span></td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.5.5.13"><span class="ltx_text" id="S4.T1.2.5.5.13.1" style="font-size:90%;">87%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.6.6">
<td class="ltx_td ltx_align_right" id="S4.T1.2.6.6.1"><span class="ltx_text" id="S4.T1.2.6.6.1.1" style="font-size:90%;">+ typos (4)</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.6.6.2"><span class="ltx_text" id="S4.T1.2.6.6.2.1" style="font-size:90%;">38.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.6.6.3"><span class="ltx_text" id="S4.T1.2.6.6.3.1" style="font-size:90%;">38</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.6.6.4"><span class="ltx_text" id="S4.T1.2.6.6.4.1" style="font-size:90%;">36.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.6.6.5"><span class="ltx_text" id="S4.T1.2.6.6.5.1" style="font-size:90%;">35.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.6.6.6"><span class="ltx_text" id="S4.T1.2.6.6.6.1" style="font-size:90%;">35.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.6.6.7"><span class="ltx_text" id="S4.T1.2.6.6.7.1" style="font-size:90%;">36.7</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.6.6.8"><span class="ltx_text" id="S4.T1.2.6.6.8.1" style="font-size:90%;">33.5</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.6.6.9"><span class="ltx_text" id="S4.T1.2.6.6.9.1" style="font-size:90%;">0.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.6.6.10"><span class="ltx_text" id="S4.T1.2.6.6.10.1" style="font-size:90%;">34.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.6.6.11"><span class="ltx_text" id="S4.T1.2.6.6.11.1" style="font-size:90%;">5.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.6.6.12"><span class="ltx_text" id="S4.T1.2.6.6.12.1" style="font-size:90%;">61.2</span></td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.6.6.13"><span class="ltx_text" id="S4.T1.2.6.6.13.1" style="font-size:90%;">86%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.7.7">
<td class="ltx_td ltx_align_right" id="S4.T1.2.7.7.1"><span class="ltx_text" id="S4.T1.2.7.7.1.1" style="font-size:90%;">+ UTF-8 fallback (5)</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.7.7.2"><span class="ltx_text" id="S4.T1.2.7.7.2.1" style="font-size:90%;">38.5</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.7.7.3"><span class="ltx_text" id="S4.T1.2.7.7.3.1" style="font-size:90%;">38</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.7.7.4"><span class="ltx_text" id="S4.T1.2.7.7.4.1" style="font-size:90%;">36.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.7.7.5"><span class="ltx_text" id="S4.T1.2.7.7.5.1" style="font-size:90%;">34.7</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.7.7.6"><span class="ltx_text" id="S4.T1.2.7.7.6.1" style="font-size:90%;">34.7</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.7.7.7"><span class="ltx_text" id="S4.T1.2.7.7.7.1" style="font-size:90%;">36.8</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.7.7.8"><span class="ltx_text" id="S4.T1.2.7.7.8.1" style="font-size:90%;">35.2</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.7.7.9"><span class="ltx_text" id="S4.T1.2.7.7.9.1" style="font-size:90%;">55.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.7.7.10"><span class="ltx_text" id="S4.T1.2.7.7.10.1" style="font-size:90%;">37</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.7.7.11"><span class="ltx_text" id="S4.T1.2.7.7.11.1" style="font-size:90%;">64.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.7.7.12"><span class="ltx_text" id="S4.T1.2.7.7.12.1" style="font-size:90%;">61</span></td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.7.7.13"><span class="ltx_text" id="S4.T1.2.7.7.13.1" style="font-size:90%;">85%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.8.8">
<td class="ltx_td ltx_align_right" id="S4.T1.2.8.8.1"><span class="ltx_text" id="S4.T1.2.8.8.1.1" style="font-size:90%;">+ noise (6)</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.8.8.2"><span class="ltx_text" id="S4.T1.2.8.8.2.1" style="font-size:90%;">39.6</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.8.8.3"><span class="ltx_text" id="S4.T1.2.8.8.3.1" style="font-size:90%;">39.1</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.8.8.4"><span class="ltx_text" id="S4.T1.2.8.8.4.1" style="font-size:90%;">37.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.8.8.5"><span class="ltx_text" id="S4.T1.2.8.8.5.1" style="font-size:90%;">35.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.8.8.6"><span class="ltx_text" id="S4.T1.2.8.8.6.1" style="font-size:90%;">35.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.8.8.7"><span class="ltx_text" id="S4.T1.2.8.8.7.1" style="font-size:90%;">37.6</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.8.8.8"><span class="ltx_text" id="S4.T1.2.8.8.8.1" style="font-size:90%;">38.9</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.8.8.9"><span class="ltx_text" id="S4.T1.2.8.8.9.1" style="font-size:90%;">87</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.8.8.10"><span class="ltx_text" id="S4.T1.2.8.8.10.1" style="font-size:90%;">38.7</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.8.8.11"><span class="ltx_text" id="S4.T1.2.8.8.11.1" style="font-size:90%;">72.3</span></td>
<td class="ltx_td ltx_align_right" id="S4.T1.2.8.8.12"><span class="ltx_text" id="S4.T1.2.8.8.12.1" style="font-size:90%;">61.3</span></td>
<td class="ltx_td ltx_align_left" id="S4.T1.2.8.8.13"><span class="ltx_text" id="S4.T1.2.8.8.13.1" style="font-size:90%;">86%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.9.9">
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.2.9.9.1"><span class="ltx_text" id="S4.T1.2.9.9.1.1" style="font-size:90%;">+ inline noise (7)</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.2.9.9.2"><span class="ltx_text" id="S4.T1.2.9.9.2.1" style="font-size:90%;">39.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.2.9.9.3"><span class="ltx_text" id="S4.T1.2.9.9.3.1" style="font-size:90%;">38.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.2.9.9.4"><span class="ltx_text" id="S4.T1.2.9.9.4.1" style="font-size:90%;">37.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.2.9.9.5"><span class="ltx_text" id="S4.T1.2.9.9.5.1" style="font-size:90%;">35.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.2.9.9.6"><span class="ltx_text" id="S4.T1.2.9.9.6.1" style="font-size:90%;">35.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.2.9.9.7"><span class="ltx_text" id="S4.T1.2.9.9.7.1" style="font-size:90%;">37.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.2.9.9.8"><span class="ltx_text" id="S4.T1.2.9.9.8.1" style="font-size:90%;">41.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.2.9.9.9"><span class="ltx_text" id="S4.T1.2.9.9.9.1" style="font-size:90%;">92</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.2.9.9.10"><span class="ltx_text" id="S4.T1.2.9.9.10.1" style="font-size:90%;">39.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.2.9.9.11"><span class="ltx_text" id="S4.T1.2.9.9.11.1" style="font-size:90%;">80.7</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.2.9.9.12"><span class="ltx_text" id="S4.T1.2.9.9.12.1" style="font-size:90%;">61.2</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.2.9.9.13"><span class="ltx_text" id="S4.T1.2.9.9.13.1" style="font-size:90%;">86%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Results table 
<br class="ltx_break"/><sup class="ltx_sup" id="S4.T1.7.1">1</sup> ChrF score was calculated on the noise/emoji only, meaning we only measure how well our model copies just OOV tokens without considering translation quality.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Caveats</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">There are some caveats that come with our test results. The more modifiers are used, the more <span class="ltx_text ltx_font_italic" id="S4.SS3.SSS1.p1.1.1">difficult</span> the training data seems to be to model, and therefore it takes more iterations through the training data to achieve convergence. Therefore all models presented have seen different amounts of training data. We will control for this setting in future work.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p2">
<p class="ltx_p" id="S4.SS3.SSS1.p2.1">Furthermore we see slight degradation in terms of translation quality when we add modifications to the training data on the plain test set. This suggests that the gains we have are not entirely for free. Finally, we observe slight deterioration on URLs. We measure only exact matches on URLs because an almost correct URL is not useful. This regression bodes for further investigation.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We present a feature complete data preprocessing and data scheduling toolkit for training machine translation systems (but also just as useful for Large Language Models). Our tools are designed with novice and experts in mind so that they lower the entry barrier to the field of machine translation, while still allowing for state of the art results. Our data augmentation utilities are crucial for producing robust machine translation systems, as well as terminology systems <cite class="ltx_cite ltx_citemacro_citep">(Bogoychev and Chen, <a class="ltx_ref" href="#bib.bib1" title="">2023</a>)</cite>. Our toolkit was developed concurrently and independently to Sotastream <cite class="ltx_cite ltx_citemacro_citep">(Post et al., <a class="ltx_ref" href="#bib.bib7" title="">2023</a>)</cite> and provides similar functionality.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bogoychev and Chen [2023]</span>
<span class="ltx_bibblock">
Nikolay Bogoychev and Pinzhen Chen.

</span>
<span class="ltx_bibblock">Terminology-aware translation with constrained decoding and large language model prompting, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag and Firat [2020]</span>
<span class="ltx_bibblock">
Markus Freitag and Orhan Firat.

</span>
<span class="ltx_bibblock">Complete multilingual neural machine translation.

</span>
<span class="ltx_bibblock">In Loïc Barrault, Ondřej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Alexander Fraser, Yvette Graham, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, André Martins, Makoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, and Matteo Negri, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the Fifth Conference on Machine Translation</em>, pages 550–560, Online, November 2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.wmt-1.66" title="">https://aclanthology.org/2020.wmt-1.66</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gowda et al. [2021]</span>
<span class="ltx_bibblock">
Thamme Gowda, Zhao Zhang, Chris Mattmann, and Jonathan May.

</span>
<span class="ltx_bibblock">Many-to-English machine translation tools, data, and pretrained models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</em>, pages 306–316, Online, August 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/2021.acl-demo.37" title="">10.18653/v1/2021.acl-demo.37</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.acl-demo.37" title="">https://aclanthology.org/2021.acl-demo.37</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Junczys-Dowmunt et al. [2018]</span>
<span class="ltx_bibblock">
Marcin Junczys-Dowmunt, Roman Grundkiewicz, Tomasz Dwojak, Hieu Hoang, Kenneth Heafield, Tom Neckermann, Frank Seide, Ulrich Germann, Alham Fikri Aji, Nikolay Bogoychev, André F. T. Martins, and Alexandra Birch.

</span>
<span class="ltx_bibblock">Marian: Fast neural machine translation in C++.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of ACL 2018, System Demonstrations</em>, pages 116–121, Melbourne, Australia, July 2018. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.aclweb.org/anthology/P18-4020" title="">http://www.aclweb.org/anthology/P18-4020</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson [2018]</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson.

</span>
<span class="ltx_bibblock">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, pages 66–71, Brussels, Belgium, November 2018. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/D18-2012" title="">10.18653/v1/D18-2012</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D18-2012" title="">https://aclanthology.org/D18-2012</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popović [2015]</span>
<span class="ltx_bibblock">
Maja Popović.

</span>
<span class="ltx_bibblock">chrF: character n-gram F-score for automatic MT evaluation.

</span>
<span class="ltx_bibblock">In Ondřej Bojar, Rajan Chatterjee, Christian Federmann, Barry Haddow, Chris Hokamp, Matthias Huck, Varvara Logacheva, and Pavel Pecina, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the Tenth Workshop on Statistical Machine Translation</em>, pages 392–395, Lisbon, Portugal, September 2015. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/W15-3049" title="">10.18653/v1/W15-3049</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W15-3049" title="">https://aclanthology.org/W15-3049</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post et al. [2023]</span>
<span class="ltx_bibblock">
Matt Post, Thamme Gowda, Roman Grundkiewicz, Huda Khayrallah, Rohit Jain, and Marcin Junczys-Dowmunt.

</span>
<span class="ltx_bibblock">Sotastream: A streaming approach to machine translation training, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et al. [2016]</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch.

</span>
<span class="ltx_bibblock">Improving neural machine translation models with monolingual data.

</span>
<span class="ltx_bibblock">In Katrin Erk and Noah A. Smith, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 86–96, Berlin, Germany, August 2016. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <a class="ltx_ref ltx_Url" href="10.18653/v1/P16-1009" title="">10.18653/v1/P16-1009</a>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P16-1009" title="">https://aclanthology.org/P16-1009</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. [2017]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 31st International Conference on Neural Information Processing Systems</em>, NIPS’17, page 6000–6010, Red Hook, NY, USA, 2017. Curran Associates Inc.

</span>
<span class="ltx_bibblock">ISBN 9781510860964.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">"Zaragoza-Bernabeu et al. ["2022"]</span>
<span class="ltx_bibblock">
Jaume "Zaragoza-Bernabeu, Gema Ramírez-Sánchez, Marta Bañón, and Sergio" Ortiz Rojas.

</span>
<span class="ltx_bibblock">"bicleaner AI: Bicleaner goes neural".

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">"Proceedings of the Thirteenth Language Resources and Evaluation Conference"</em>, pages "824–831", "Marseille, France", June "2022". "European Language Resources Association".

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="%22https://aclanthology.org/2022.lrec-1.87%22" title="">"https://aclanthology.org/2022.lrec-1.87"</a>.

</span>
</li>
</ul>
</section>
</article><div class="section" id="target-section"><div id="license-tr">License: CC BY 4</div><div id="watermark-tr">arXiv:2311.14838v1 [cs.CL] 24 Nov 2023</div></div>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Nov 24 20:21:32 2023 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
