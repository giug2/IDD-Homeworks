<article class="ltx_document ltx_authors_1line" lang="en">
 <h1 class="ltx_title ltx_title_document">
  Embodied Task Planning with Large Language Models
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <span class="ltx_text ltx_font_bold" id="id1.1.id1">
     Zhenyu Wu
     <sup class="ltx_sup" id="id1.1.id1.1">
      <span class="ltx_text ltx_font_medium" id="id1.1.id1.1.1">
       1
      </span>
     </sup>
     ,
    </span>
    <span class="ltx_text ltx_font_bold" id="id2.2.id2">
     Ziwei Wang
     <sup class="ltx_sup" id="id2.2.id2.1">
      <span class="ltx_text ltx_font_medium" id="id2.2.id2.1.1">
       2,3
      </span>
     </sup>
     ,
    </span>
    <span class="ltx_text ltx_font_bold" id="id3.3.id3">
     Xiuwei Xu
     <sup class="ltx_sup" id="id3.3.id3.1">
      <span class="ltx_text ltx_font_medium" id="id3.3.id3.1.1">
       2,3
      </span>
     </sup>
     ,
    </span>
    <span class="ltx_text ltx_font_bold" id="id4.4.id4">
     Jiwen Lu
     <sup class="ltx_sup" id="id4.4.id4.1">
      <span class="ltx_text ltx_font_medium" id="id4.4.id4.1.1">
       2,3
      </span>
     </sup>
     ,
    </span>
    <span class="ltx_text ltx_font_bold" id="id5.5.id5">
     Haibin Yan
     <sup class="ltx_sup" id="id5.5.id5.1">
      <span class="ltx_text ltx_font_medium" id="id5.5.id5.1.1">
       1
      </span>
     </sup>
     <br class="ltx_break"/>
    </span>
    <sup class="ltx_sup" id="id6.6.id6">
     1
    </sup>
    School of Automation
   </span>
   <span class="ltx_author_notes">
    Corresponding author.
    <span class="ltx_contact ltx_role_affiliation">
    </span>
   </span>
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Beijing University of Posts and Telecommunications
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
    </span>
   </span>
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    China
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id7.1.id1">
     2
    </sup>
    Department of Automation
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
    </span>
   </span>
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Tsinghua University
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
    </span>
   </span>
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    China
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id8.1.id1">
     3
    </sup>
    Beijing National Research Center for Information Science and Technology
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
    </span>
   </span>
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    China
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id9.1.id1">
     {wuzhenyu, eyanhaibin}@bupt.edu.cn; yali110136@gmail.com;
     <br class="ltx_break"/>
     xxw21@mails.tsinghua.edu.cn; lujiwen@tsinghua.edu.cn
     <br class="ltx_break"/>
     <a class="ltx_ref ltx_href ltx_font_serif" href="https://gary3410.github.io/TaPA" target="_blank" title="">
      https://gary3410.github.io/TaPA
     </a>
    </span>
    <br class="ltx_break"/>
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
    </span>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id10.id1">
   <span class="ltx_text" id="id10.id1.1">
    Equipping embodied agents with commonsense is important for robots to successfully complete complex human instructions in general environments. Recent large language models (LLM) can embed rich semantic knowledge for agents in plan generation of complex tasks, while they lack the information about the realistic world and usually yield infeasible action sequences. In this paper, we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical scene constraint, where the agent generates executable plans according to the existed objects in the scene by aligning LLMs with the visual perception models. Specifically, we first construct a multimodal dataset containing triplets of indoor scenes, instructions and action plans, where we provide the designed prompts and the list of existing objects in the scene for GPT-3.5 to generate a large number of instructions and corresponding planned actions. The generated data is leveraged for grounded plan tuning of pre-trained LLMs. During inference, we discover the objects in the scene by extending open-vocabulary object detectors to multi-view RGB images collected in different achievable locations. Experimental results show that the generated plan from our TaPA framework can achieve higher success rate than LLaVA and GPT-3.5 by a sizable margin, which indicates the practicality of embodied task planning in general and complex environments.
   </span>
  </p>
 </div>
 <div class="ltx_para ltx_noindent" id="p1">
  <blockquote class="ltx_quote" id="p1.1">
   <p class="ltx_p" id="p1.1.1">
    <span class="ltx_text ltx_font_bold" id="p1.1.1.1">
     Keywords:
    </span>
    Embodied task planning, large language models, open-vocabulary detection
   </p>
  </blockquote>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para ltx_noindent" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Equipping embodied agents with general commonsense knowledge to accomplish complex tasks based on the natural language commands is desirable in many applications such as domestic service
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib1" title="">
      1
     </a>
     ]
    </cite>
    , medical treatment
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib2" title="">
      2
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib3" title="">
      3
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib4" title="">
      4
     </a>
     ]
    </cite>
    and agricultural picking
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib5" title="">
      5
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib6" title="">
      6
     </a>
     ]
    </cite>
    .
Due to the limited training samples and diverse tasks in downstream applications, directly training an embodied agent across different deployment scenarios is infeasible. Recent progress in large language models (LLMs)
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib7" title="">
      7
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib8" title="">
      8
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib9" title="">
      9
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib10" title="">
      10
     </a>
     ]
    </cite>
    acquires rich commonsense knowledge from the vast web data, whose knowledge can be potentially leveraged by embodied agents to generate action plans for human requirements represented in natural language.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    However, LLMs cannot perceive the surrounding scenes and may generate inexecutable actions due to the requirement of interacting with non-existed objects.
For example, given the human command ”Give me some wine”, the generated action steps from GPT-3.5 are ”pouring wine from the bottle to the glass”. There may be only mugs instead of glasses in the realistic scenes, and the executable actions should be ”pouring wine from the bottle to the mug”. Therefore, grounding the task plan generated by LLMs to the physical world is necessary to construct embodied agents for complex task accomplishment.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    To acquire executable task plans in the given physical scenes, many previous works filter or align the generated actions by considering the visual clues in the scene for the task of general manipulation of tabletop objects
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib11" title="">
      11
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib12" title="">
      12
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib13" title="">
      13
     </a>
     ]
    </cite>
    . In order to further diversify tasks in house-level environments, SayCan
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib14" title="">
      14
     </a>
     ]
    </cite>
    and LLM-Planner
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib15" title="">
      15
     </a>
     ]
    </cite>
    employ visual navigation to collect information in the house for the challenging grounded plan generation. Nevertheless, SayCan can only accomplish tasks in the kitchen scenarios and LLM-Planner performs planning in the ALFRED simulator
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib16" title="">
      16
     </a>
     ]
    </cite>
    where most tasks are simple such as putting and placing. They both fail to satisfy the requirement of numerous complex tasks and diverse deployment scenarios in our daily life.
   </p>
  </div>
  <figure class="ltx_figure" id="S1.F1">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="187" id="S1.F1.g1" src="/html/2307.01848/assets/figs/figure_1_v6.png" width="598"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 1:
    </span>
    Our embodied task planning framework collects multiple RGB images from various standing points and viewpoints. Utilizing an open vocabulary detector generates a list of objects existed in the scene. Combining human instructions and the predicted object list, our TaPA generates executable action plans for navigation or manipulation tasks.
   </figcaption>
  </figure>
  <div class="ltx_para ltx_noindent" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    In this paper, we present a task planning agent called TaPA for embodied task plan grounding in physical scenes. The unreleased SayCan cannot be applied in diverse indoor scenarios, and the LLM-Planner in the ALFRED benchmark fails to generate plans for complex tasks due to the pre-defined simple instructions in the simulator.
On the contrary, our agent can generate grounded plans without constraining task types and target objects.
Therefore, Our agent acquires general commonsense knowledge to yield action steps for complex household tasks such as making sandwiches and setting tables, which provides the foundational instructions for the downstream navigation and manipulation process to deal with high-level requirements from humans. Figure
    <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Embodied Task Planning with Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    demonstrates the overall pipeline of our TaPA that generates the executable action steps by considering the scene information and the human instructions.
Figure
    <a class="ltx_ref" href="#S2.F2" title="Figure 2 ‣ 2 Related Work ‣ Embodied Task Planning with Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    shows the statistical difference between our TaPA and conventional ALFRED benchmark, where our tasks are much more complex with longer steps for accomplishment. More specifically, we first construct a multimodal dataset where each sample is a triplet of visual scenes, instructions, and corresponding plans.
By leveraging the generated dataset, we finetune the pre-trained LLaMA
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib7" title="">
      7
     </a>
     ]
    </cite>
    network by predicting the action steps based on the object list of the scene, which is employed as our task planner. For the acquisition of the object list during inference, the embodied agent effectively visits standing points to collect RGB images providing sufficient information in different views, and generalizes the open-vocabulary detector for multi-view images to acquire the list of existed objects. Our TaPA agent achieves higher success rate of the generated action plans compared with the state-of-the-art LLMs including LLaMA and GPT-3.5 and large multimodal models (LMMs) such as LLaVA
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib17" title="">
      17
     </a>
     ]
    </cite>
    . Our contributions can be summarized as follows:
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p5">
   <ul class="ltx_itemize" id="S1.I1">
    <li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i1.p1">
      <p class="ltx_p" id="S1.I1.i1.p1.1">
       To the best of our knowledge, we propose the first benchmark for complex embodied task planning that is practical in realistic indoor deployment scenarios.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i2.p1">
      <p class="ltx_p" id="S1.I1.i2.p1.1">
       We design a framework for large-scale multimodal dataset generation in order to train the task planner from pre-trained LLMs and construct a multimodal dataset containing 80 indoor scenes with 15K instructions and corresponding action plans.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para ltx_noindent" id="S1.I1.i3.p1">
      <p class="ltx_p" id="S1.I1.i3.p1.1">
       We evaluate different LLMs and LMMs for complex embodied task planning in our benchmark, and conduct the ablation study to select the optimal representation of visual scenes for executable action generation.
      </p>
     </div>
    </li>
   </ul>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Related Work
  </h2>
  <div class="ltx_para ltx_noindent" id="S2.p1">
   <p class="ltx_p" id="S2.p1.1">
    <span class="ltx_text ltx_font_bold" id="S2.p1.1.1">
     Large pre-trained models:
    </span>
    Large-scale pre-trained models have revolutionized the natural language processing (NLP)
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib18" title="">
      18
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib19" title="">
      19
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib20" title="">
      20
     </a>
     ]
    </cite>
    and the computer vision
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib21" title="">
      21
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib22" title="">
      22
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib23" title="">
      23
     </a>
     ]
    </cite>
    communities in recent years. Benefiting from the vast training data and numerous parameters, the large pre-trained models acquire strong generalization ability across different deployment scenarios. For large language models, recent studies show that they not only perform well in NLP tasks, but also emerge the ability to master the rich knowledge about the realistic world with factual answers. Therefore, LLMs such as LLaMA
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib7" title="">
      7
     </a>
     ]
    </cite>
    , GPT-3
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib24" title="">
      24
     </a>
     ]
    </cite>
    are widely adopted to diverse tasks by interacting with input from other modalities such as visual feature learning
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib25" title="">
      25
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib26" title="">
      26
     </a>
     ]
    </cite>
    , pseudo-code generation
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib27" title="">
      27
     </a>
     ]
    </cite>
    , tool usage
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib28" title="">
      28
     </a>
     ]
    </cite>
    and math problem solving
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib29" title="">
      29
     </a>
     ]
    </cite>
    . For large vision models, objects in the open environments can be detected
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib23" title="">
      23
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib30" title="">
      30
     </a>
     ]
    </cite>
    or segmented
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib31" title="">
      31
     </a>
     ]
    </cite>
    for scene understanding, where bounding boxes and masks are generated for all scenarios and visual features are aligned with text embedding for category assignment. To learn the joint embedding space of language and vision for multimodal tasks, CLIP
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib32" title="">
      32
     </a>
     ]
    </cite>
    leverages contrastive learning to minimize the distance between similar image-text pairs. LLaVA
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib17" title="">
      17
     </a>
     ]
    </cite>
    synthesized a multimodal dataset with images, captions and bounding boxes in the tasks of conversation, detailed description and complex reasoning, so that the instructional tuning of LLMs acquires general-purpose instruction-following visual agent.
In this paper, we leverage LLMs to generate executable plans for embodied tasks with the visual information acquired from the open-vocabulary detection models.
   </p>
  </div>
  <figure class="ltx_figure" id="S2.F2">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="217" id="S2.F2.g1" src="/html/2307.01848/assets/figs/figure_2_v7.png" width="598"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 2:
    </span>
    Statistical comparison of TaPA and ALFRED dataset.
The pie chart shows the top 20 frequently appearing verbs (inner circle) and the corresponding top 4 nouns (outer circle) for each verb. The bar chart shows the percentage of instructions with different numbers of implementation actions, where TaPA contains more complex instructions compared to ALFRED.
   </figcaption>
  </figure>
  <div class="ltx_para ltx_noindent" id="S2.p2">
   <p class="ltx_p" id="S2.p2.1">
    <span class="ltx_text ltx_font_bold" id="S2.p2.1.1">
     Language model grounding for embodied tasks:
    </span>
    An embodied agent not only requires active exploration
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib33" title="">
      33
     </a>
     ]
    </cite>
    , manipulation
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib34" title="">
      34
     </a>
     ]
    </cite>
    , and scene perception
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib35" title="">
      35
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib36" title="">
      36
     </a>
     ]
    </cite>
    as well as embodied task planning ability.
Embodied task planning aims to generate executable action steps in the given environments, where action plans are generated from grounded LLMs by receiving information from the surrounding environments
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib37" title="">
      37
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib38" title="">
      38
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib39" title="">
      39
     </a>
     ]
    </cite>
    or prompt engineering
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib40" title="">
      40
     </a>
     ]
    </cite>
    . For the former, agents acquire the feedback from environments by interacting with the objects to ground the task plan. Li
    <em class="ltx_emph ltx_font_italic" id="S2.p2.1.2">
     et al.
    </em>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib41" title="">
      41
     </a>
     ]
    </cite>
    employed LLMs as a general scaffold for interactive decision-making in complex tasks, where the generated policies were grounded to the given environments for executable implementation according to the action feedback. For prompt engineering, researchers carefully design the language prompts for LLMs to guide them to ground the generated content. Huang
    <em class="ltx_emph ltx_font_italic" id="S2.p2.1.3">
     et al.
    </em>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib40" title="">
      40
     </a>
     ]
    </cite>
    prompted simple examples of task instructions and corresponding actions for LLMs to produce plausible task plans, and filtered out executable subsets by constructing mapping with semantic similarity. To enable the LLMs to be aware of the surrounding scenes with boosted plan plausibility, Brohan
    <em class="ltx_emph ltx_font_italic" id="S2.p2.1.4">
     et al.
    </em>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib14" title="">
      14
     </a>
     ]
    </cite>
    and Song
    <em class="ltx_emph ltx_font_italic" id="S2.p2.1.5">
     et al.
    </em>
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib15" title="">
      15
     </a>
     ]
    </cite>
    extracted the visual information of the scene by latent features or object names for LLMs, where the generated plans were limited to the one with the highest success rate for task completion. However, these works can only accomplish simple tasks such as placing and putting in the VirtualHome
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib42" title="">
      42
     </a>
     ]
    </cite>
    or ALFRED simulators, which fail to be applied to practical deployment scenarios with diverse complex tasks.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Approach
  </h2>
  <div class="ltx_para ltx_noindent" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    In this section, we first describe the construction of the multimodal instruction dataset that is leveraged to tune our TaPA task planner, and then describe the details of grounding embodied task plans to the visual scene with image collection and open-vocabulary detection.
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Data Generation of Embodied Task Planning
   </h3>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     Although large vision-language models (VLM)
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib17" title="">
       17
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib43" title="">
       43
      </a>
      ]
     </cite>
     and large multimodal models
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib44" title="">
       44
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib45" title="">
       45
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib46" title="">
       46
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib47" title="">
       47
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib48" title="">
       48
      </a>
      ]
     </cite>
     have achieved surprising performance on a wide range of complex perception tasks, embodied task planning that is grounded to the realistic indoor scenes still remains challenging due to the lack of the large-scale multimodal dataset to train the planning agent. Considering the recent success of GPT models on high-level human-like reasoning, we leverage GPT-3.5 with the presented scene representation and designed prompt to generate the large-scale multimodal dataset for our planning agent tuning.
    </p>
   </div>
   <figure class="ltx_figure ltx_align_floatright" id="S3.F3">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="176" id="S3.F3.g1" src="/html/2307.01848/assets/figs/figure_3_v6.png" width="281"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 3:
     </span>
     The pipeline of embedding the scene information for the LLM to generate executable actions. The image collection strategies to acquire the list of all existed objects in the scene include random sampling, traversal sampling, the overall center point and block-wise center points, where the object list is leveraged as the condition for action planning. The dashed circles in different colors represent grids in various blocks for block-wise center point selection.
    </figcaption>
   </figure>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.8">
     Given an embodied 3D scene
     <math alttext="X_{s}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1">
      <semantics id="S3.SS1.p2.1.m1.1a">
       <msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">
        <mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">
         X
        </mi>
        <mi id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">
         s
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b">
        <apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">
         <csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">
          𝑋
         </ci>
         <ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">
          𝑠
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">
        X_{s}
       </annotation>
      </semantics>
     </math>
     , we directly utilize the class names of all objects as the representation of the scene which is denoted as
     <math alttext="X_{l}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1">
      <semantics id="S3.SS1.p2.2.m2.1a">
       <msub id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">
        <mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">
         X
        </mi>
        <mi id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">
         l
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b">
        <apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">
         <csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">
          𝑋
         </ci>
         <ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">
          𝑙
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">
        X_{l}
       </annotation>
      </semantics>
     </math>
     . All duplicate names are removed to provide scene information for the LLM such as
     <math alttext="X_{l}=[table,chair,keyboard,...]" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.4">
      <semantics id="S3.SS1.p2.3.m3.4a">
       <mrow id="S3.SS1.p2.3.m3.4.4" xref="S3.SS1.p2.3.m3.4.4.cmml">
        <msub id="S3.SS1.p2.3.m3.4.4.5" xref="S3.SS1.p2.3.m3.4.4.5.cmml">
         <mi id="S3.SS1.p2.3.m3.4.4.5.2" xref="S3.SS1.p2.3.m3.4.4.5.2.cmml">
          X
         </mi>
         <mi id="S3.SS1.p2.3.m3.4.4.5.3" xref="S3.SS1.p2.3.m3.4.4.5.3.cmml">
          l
         </mi>
        </msub>
        <mo id="S3.SS1.p2.3.m3.4.4.4" xref="S3.SS1.p2.3.m3.4.4.4.cmml">
         =
        </mo>
        <mrow id="S3.SS1.p2.3.m3.4.4.3.3" xref="S3.SS1.p2.3.m3.4.4.3.4.cmml">
         <mo id="S3.SS1.p2.3.m3.4.4.3.3.4" stretchy="false" xref="S3.SS1.p2.3.m3.4.4.3.4.cmml">
          [
         </mo>
         <mrow id="S3.SS1.p2.3.m3.2.2.1.1.1" xref="S3.SS1.p2.3.m3.2.2.1.1.1.cmml">
          <mi id="S3.SS1.p2.3.m3.2.2.1.1.1.2" xref="S3.SS1.p2.3.m3.2.2.1.1.1.2.cmml">
           t
          </mi>
          <mo id="S3.SS1.p2.3.m3.2.2.1.1.1.1" lspace="0em" rspace="0em" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.cmml">
           ​
          </mo>
          <mi id="S3.SS1.p2.3.m3.2.2.1.1.1.3" xref="S3.SS1.p2.3.m3.2.2.1.1.1.3.cmml">
           a
          </mi>
          <mo id="S3.SS1.p2.3.m3.2.2.1.1.1.1a" lspace="0em" rspace="0em" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.cmml">
           ​
          </mo>
          <mi id="S3.SS1.p2.3.m3.2.2.1.1.1.4" xref="S3.SS1.p2.3.m3.2.2.1.1.1.4.cmml">
           b
          </mi>
          <mo id="S3.SS1.p2.3.m3.2.2.1.1.1.1b" lspace="0em" rspace="0em" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.cmml">
           ​
          </mo>
          <mi id="S3.SS1.p2.3.m3.2.2.1.1.1.5" xref="S3.SS1.p2.3.m3.2.2.1.1.1.5.cmml">
           l
          </mi>
          <mo id="S3.SS1.p2.3.m3.2.2.1.1.1.1c" lspace="0em" rspace="0em" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1.cmml">
           ​
          </mo>
          <mi id="S3.SS1.p2.3.m3.2.2.1.1.1.6" xref="S3.SS1.p2.3.m3.2.2.1.1.1.6.cmml">
           e
          </mi>
         </mrow>
         <mo id="S3.SS1.p2.3.m3.4.4.3.3.5" xref="S3.SS1.p2.3.m3.4.4.3.4.cmml">
          ,
         </mo>
         <mrow id="S3.SS1.p2.3.m3.3.3.2.2.2" xref="S3.SS1.p2.3.m3.3.3.2.2.2.cmml">
          <mi id="S3.SS1.p2.3.m3.3.3.2.2.2.2" xref="S3.SS1.p2.3.m3.3.3.2.2.2.2.cmml">
           c
          </mi>
          <mo id="S3.SS1.p2.3.m3.3.3.2.2.2.1" lspace="0em" rspace="0em" xref="S3.SS1.p2.3.m3.3.3.2.2.2.1.cmml">
           ​
          </mo>
          <mi id="S3.SS1.p2.3.m3.3.3.2.2.2.3" xref="S3.SS1.p2.3.m3.3.3.2.2.2.3.cmml">
           h
          </mi>
          <mo id="S3.SS1.p2.3.m3.3.3.2.2.2.1a" lspace="0em" rspace="0em" xref="S3.SS1.p2.3.m3.3.3.2.2.2.1.cmml">
           ​
          </mo>
          <mi id="S3.SS1.p2.3.m3.3.3.2.2.2.4" xref="S3.SS1.p2.3.m3.3.3.2.2.2.4.cmml">
           a
          </mi>
          <mo id="S3.SS1.p2.3.m3.3.3.2.2.2.1b" lspace="0em" rspace="0em" xref="S3.SS1.p2.3.m3.3.3.2.2.2.1.cmml">
           ​
          </mo>
          <mi id="S3.SS1.p2.3.m3.3.3.2.2.2.5" xref="S3.SS1.p2.3.m3.3.3.2.2.2.5.cmml">
           i
          </mi>
          <mo id="S3.SS1.p2.3.m3.3.3.2.2.2.1c" lspace="0em" rspace="0em" xref="S3.SS1.p2.3.m3.3.3.2.2.2.1.cmml">
           ​
          </mo>
          <mi id="S3.SS1.p2.3.m3.3.3.2.2.2.6" xref="S3.SS1.p2.3.m3.3.3.2.2.2.6.cmml">
           r
          </mi>
         </mrow>
         <mo id="S3.SS1.p2.3.m3.4.4.3.3.6" xref="S3.SS1.p2.3.m3.4.4.3.4.cmml">
          ,
         </mo>
         <mrow id="S3.SS1.p2.3.m3.4.4.3.3.3" xref="S3.SS1.p2.3.m3.4.4.3.3.3.cmml">
          <mi id="S3.SS1.p2.3.m3.4.4.3.3.3.2" xref="S3.SS1.p2.3.m3.4.4.3.3.3.2.cmml">
           k
          </mi>
          <mo id="S3.SS1.p2.3.m3.4.4.3.3.3.1" lspace="0em" rspace="0em" xref="S3.SS1.p2.3.m3.4.4.3.3.3.1.cmml">
           ​
          </mo>
          <mi id="S3.SS1.p2.3.m3.4.4.3.3.3.3" xref="S3.SS1.p2.3.m3.4.4.3.3.3.3.cmml">
           e
          </mi>
          <mo id="S3.SS1.p2.3.m3.4.4.3.3.3.1a" lspace="0em" rspace="0em" xref="S3.SS1.p2.3.m3.4.4.3.3.3.1.cmml">
           ​
          </mo>
          <mi id="S3.SS1.p2.3.m3.4.4.3.3.3.4" xref="S3.SS1.p2.3.m3.4.4.3.3.3.4.cmml">
           y
          </mi>
          <mo id="S3.SS1.p2.3.m3.4.4.3.3.3.1b" lspace="0em" rspace="0em" xref="S3.SS1.p2.3.m3.4.4.3.3.3.1.cmml">
           ​
          </mo>
          <mi id="S3.SS1.p2.3.m3.4.4.3.3.3.5" xref="S3.SS1.p2.3.m3.4.4.3.3.3.5.cmml">
           b
          </mi>
          <mo id="S3.SS1.p2.3.m3.4.4.3.3.3.1c" lspace="0em" rspace="0em" xref="S3.SS1.p2.3.m3.4.4.3.3.3.1.cmml">
           ​
          </mo>
          <mi id="S3.SS1.p2.3.m3.4.4.3.3.3.6" xref="S3.SS1.p2.3.m3.4.4.3.3.3.6.cmml">
           o
          </mi>
          <mo id="S3.SS1.p2.3.m3.4.4.3.3.3.1d" lspace="0em" rspace="0em" xref="S3.SS1.p2.3.m3.4.4.3.3.3.1.cmml">
           ​
          </mo>
          <mi id="S3.SS1.p2.3.m3.4.4.3.3.3.7" xref="S3.SS1.p2.3.m3.4.4.3.3.3.7.cmml">
           a
          </mi>
          <mo id="S3.SS1.p2.3.m3.4.4.3.3.3.1e" lspace="0em" rspace="0em" xref="S3.SS1.p2.3.m3.4.4.3.3.3.1.cmml">
           ​
          </mo>
          <mi id="S3.SS1.p2.3.m3.4.4.3.3.3.8" xref="S3.SS1.p2.3.m3.4.4.3.3.3.8.cmml">
           r
          </mi>
          <mo id="S3.SS1.p2.3.m3.4.4.3.3.3.1f" lspace="0em" rspace="0em" xref="S3.SS1.p2.3.m3.4.4.3.3.3.1.cmml">
           ​
          </mo>
          <mi id="S3.SS1.p2.3.m3.4.4.3.3.3.9" xref="S3.SS1.p2.3.m3.4.4.3.3.3.9.cmml">
           d
          </mi>
         </mrow>
         <mo id="S3.SS1.p2.3.m3.4.4.3.3.7" xref="S3.SS1.p2.3.m3.4.4.3.4.cmml">
          ,
         </mo>
         <mi id="S3.SS1.p2.3.m3.1.1" mathvariant="normal" xref="S3.SS1.p2.3.m3.1.1.cmml">
          …
         </mi>
         <mo id="S3.SS1.p2.3.m3.4.4.3.3.8" stretchy="false" xref="S3.SS1.p2.3.m3.4.4.3.4.cmml">
          ]
         </mo>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.4b">
        <apply id="S3.SS1.p2.3.m3.4.4.cmml" xref="S3.SS1.p2.3.m3.4.4">
         <eq id="S3.SS1.p2.3.m3.4.4.4.cmml" xref="S3.SS1.p2.3.m3.4.4.4">
         </eq>
         <apply id="S3.SS1.p2.3.m3.4.4.5.cmml" xref="S3.SS1.p2.3.m3.4.4.5">
          <csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.4.4.5.1.cmml" xref="S3.SS1.p2.3.m3.4.4.5">
           subscript
          </csymbol>
          <ci id="S3.SS1.p2.3.m3.4.4.5.2.cmml" xref="S3.SS1.p2.3.m3.4.4.5.2">
           𝑋
          </ci>
          <ci id="S3.SS1.p2.3.m3.4.4.5.3.cmml" xref="S3.SS1.p2.3.m3.4.4.5.3">
           𝑙
          </ci>
         </apply>
         <list id="S3.SS1.p2.3.m3.4.4.3.4.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3">
          <apply id="S3.SS1.p2.3.m3.2.2.1.1.1.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1">
           <times id="S3.SS1.p2.3.m3.2.2.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.1">
           </times>
           <ci id="S3.SS1.p2.3.m3.2.2.1.1.1.2.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.2">
            𝑡
           </ci>
           <ci id="S3.SS1.p2.3.m3.2.2.1.1.1.3.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.3">
            𝑎
           </ci>
           <ci id="S3.SS1.p2.3.m3.2.2.1.1.1.4.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.4">
            𝑏
           </ci>
           <ci id="S3.SS1.p2.3.m3.2.2.1.1.1.5.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.5">
            𝑙
           </ci>
           <ci id="S3.SS1.p2.3.m3.2.2.1.1.1.6.cmml" xref="S3.SS1.p2.3.m3.2.2.1.1.1.6">
            𝑒
           </ci>
          </apply>
          <apply id="S3.SS1.p2.3.m3.3.3.2.2.2.cmml" xref="S3.SS1.p2.3.m3.3.3.2.2.2">
           <times id="S3.SS1.p2.3.m3.3.3.2.2.2.1.cmml" xref="S3.SS1.p2.3.m3.3.3.2.2.2.1">
           </times>
           <ci id="S3.SS1.p2.3.m3.3.3.2.2.2.2.cmml" xref="S3.SS1.p2.3.m3.3.3.2.2.2.2">
            𝑐
           </ci>
           <ci id="S3.SS1.p2.3.m3.3.3.2.2.2.3.cmml" xref="S3.SS1.p2.3.m3.3.3.2.2.2.3">
            ℎ
           </ci>
           <ci id="S3.SS1.p2.3.m3.3.3.2.2.2.4.cmml" xref="S3.SS1.p2.3.m3.3.3.2.2.2.4">
            𝑎
           </ci>
           <ci id="S3.SS1.p2.3.m3.3.3.2.2.2.5.cmml" xref="S3.SS1.p2.3.m3.3.3.2.2.2.5">
            𝑖
           </ci>
           <ci id="S3.SS1.p2.3.m3.3.3.2.2.2.6.cmml" xref="S3.SS1.p2.3.m3.3.3.2.2.2.6">
            𝑟
           </ci>
          </apply>
          <apply id="S3.SS1.p2.3.m3.4.4.3.3.3.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3.3">
           <times id="S3.SS1.p2.3.m3.4.4.3.3.3.1.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3.3.1">
           </times>
           <ci id="S3.SS1.p2.3.m3.4.4.3.3.3.2.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3.3.2">
            𝑘
           </ci>
           <ci id="S3.SS1.p2.3.m3.4.4.3.3.3.3.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3.3.3">
            𝑒
           </ci>
           <ci id="S3.SS1.p2.3.m3.4.4.3.3.3.4.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3.3.4">
            𝑦
           </ci>
           <ci id="S3.SS1.p2.3.m3.4.4.3.3.3.5.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3.3.5">
            𝑏
           </ci>
           <ci id="S3.SS1.p2.3.m3.4.4.3.3.3.6.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3.3.6">
            𝑜
           </ci>
           <ci id="S3.SS1.p2.3.m3.4.4.3.3.3.7.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3.3.7">
            𝑎
           </ci>
           <ci id="S3.SS1.p2.3.m3.4.4.3.3.3.8.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3.3.8">
            𝑟
           </ci>
           <ci id="S3.SS1.p2.3.m3.4.4.3.3.3.9.cmml" xref="S3.SS1.p2.3.m3.4.4.3.3.3.9">
            𝑑
           </ci>
          </apply>
          <ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">
           …
          </ci>
         </list>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.4c">
        X_{l}=[table,chair,keyboard,...]
       </annotation>
      </semantics>
     </math>
     . Based on the above scene information, a simple approach used in ALFRED benchmark
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib16" title="">
       16
      </a>
      ]
     </cite>
     to generate the multimodal instruction following the dataset for embodied task plans is to artificially design a series of instructions with corresponding step-by-step actions. However, the hand-crafted design requires extremely high annotation cost to generate complex task plans that are practical for realistic service robots such as tidying up the bathroom and making sandwiches. To efficiently generate the large-scale complex instructions
     <math alttext="X_{q}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1">
      <semantics id="S3.SS1.p2.4.m4.1a">
       <msub id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">
        <mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">
         X
        </mi>
        <mi id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">
         q
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b">
        <apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">
         <csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">
          𝑋
         </ci>
         <ci id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">
          𝑞
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">
        X_{q}
       </annotation>
      </semantics>
     </math>
     and executable corresponding plans
     <math alttext="X_{a}" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1">
      <semantics id="S3.SS1.p2.5.m5.1a">
       <msub id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">
        <mi id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">
         X
        </mi>
        <mi id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml">
         a
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b">
        <apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">
         <csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">
          𝑋
         </ci>
         <ci id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3">
          𝑎
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">
        X_{a}
       </annotation>
      </semantics>
     </math>
     for the given 3D scene, we design a prompt to simulate the scenarios of embodied task planning for GPT-3.5 to automatically synthesize data based on the object name list
     <math alttext="X_{l}" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m6.1">
      <semantics id="S3.SS1.p2.6.m6.1a">
       <msub id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml">
        <mi id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml">
         X
        </mi>
        <mi id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml">
         l
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b">
        <apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">
         <csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">
          𝑋
         </ci>
         <ci id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3">
          𝑙
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">
        X_{l}
       </annotation>
      </semantics>
     </math>
     . As shown in Table
     <a class="ltx_ref" href="#Ax1.T5" title="Table 5 ‣ Supplementary Material ‣ Embodied Task Planning with Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     of the supplementary materials, our prompt describes the definition of embodied task planning, the requirements and several examples of generated instructions and corresponding action plans. Specifically, the prompt designs a conversation between the service robot and humans to generate executable instructions and actions, which simulates the exploration of robots in the embodied environments and provides the requirements from humans. The generated instructions are diverse including requests, commands and queries, where only instructions with explicitly executable actions are added to our dataset. Meanwhile, we emphasize that the target object of the generated action should be constrained within the object list
     <math alttext="X_{l}" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m7.1">
      <semantics id="S3.SS1.p2.7.m7.1a">
       <msub id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml">
        <mi id="S3.SS1.p2.7.m7.1.1.2" xref="S3.SS1.p2.7.m7.1.1.2.cmml">
         X
        </mi>
        <mi id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml">
         l
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b">
        <apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">
         <csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2">
          𝑋
         </ci>
         <ci id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3">
          𝑙
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">
        X_{l}
       </annotation>
      </semantics>
     </math>
     to mitigate the object hallucination that leads to inexecutable plans. For the object list leveraged in the prompt for dataset generation, we directly utilize the groundtruth label of existed instances in the scene. In Table
     <a class="ltx_ref" href="#S3.T1" title="Table 1 ‣ 3.1 Data Generation of Embodied Task Planning ‣ 3 Approach ‣ Embodied Task Planning with Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     , we show examples of the generated sample containing the object name list of the scene, the instruction and the executable action steps. In embodied task planning, the agent can only get access to the visual scene containing all interactive objects without the groundtruth object list. Therefore, we construct the multimodal dataset by defining triplets for each sample as
     <math alttext="\bm{X}=(X_{v},X_{q},X_{a})" class="ltx_Math" display="inline" id="S3.SS1.p2.8.m8.3">
      <semantics id="S3.SS1.p2.8.m8.3a">
       <mrow id="S3.SS1.p2.8.m8.3.3" xref="S3.SS1.p2.8.m8.3.3.cmml">
        <mi id="S3.SS1.p2.8.m8.3.3.5" xref="S3.SS1.p2.8.m8.3.3.5.cmml">
         𝑿
        </mi>
        <mo id="S3.SS1.p2.8.m8.3.3.4" xref="S3.SS1.p2.8.m8.3.3.4.cmml">
         =
        </mo>
        <mrow id="S3.SS1.p2.8.m8.3.3.3.3" xref="S3.SS1.p2.8.m8.3.3.3.4.cmml">
         <mo id="S3.SS1.p2.8.m8.3.3.3.3.4" stretchy="false" xref="S3.SS1.p2.8.m8.3.3.3.4.cmml">
          (
         </mo>
         <msub id="S3.SS1.p2.8.m8.1.1.1.1.1" xref="S3.SS1.p2.8.m8.1.1.1.1.1.cmml">
          <mi id="S3.SS1.p2.8.m8.1.1.1.1.1.2" xref="S3.SS1.p2.8.m8.1.1.1.1.1.2.cmml">
           X
          </mi>
          <mi id="S3.SS1.p2.8.m8.1.1.1.1.1.3" xref="S3.SS1.p2.8.m8.1.1.1.1.1.3.cmml">
           v
          </mi>
         </msub>
         <mo id="S3.SS1.p2.8.m8.3.3.3.3.5" xref="S3.SS1.p2.8.m8.3.3.3.4.cmml">
          ,
         </mo>
         <msub id="S3.SS1.p2.8.m8.2.2.2.2.2" xref="S3.SS1.p2.8.m8.2.2.2.2.2.cmml">
          <mi id="S3.SS1.p2.8.m8.2.2.2.2.2.2" xref="S3.SS1.p2.8.m8.2.2.2.2.2.2.cmml">
           X
          </mi>
          <mi id="S3.SS1.p2.8.m8.2.2.2.2.2.3" xref="S3.SS1.p2.8.m8.2.2.2.2.2.3.cmml">
           q
          </mi>
         </msub>
         <mo id="S3.SS1.p2.8.m8.3.3.3.3.6" xref="S3.SS1.p2.8.m8.3.3.3.4.cmml">
          ,
         </mo>
         <msub id="S3.SS1.p2.8.m8.3.3.3.3.3" xref="S3.SS1.p2.8.m8.3.3.3.3.3.cmml">
          <mi id="S3.SS1.p2.8.m8.3.3.3.3.3.2" xref="S3.SS1.p2.8.m8.3.3.3.3.3.2.cmml">
           X
          </mi>
          <mi id="S3.SS1.p2.8.m8.3.3.3.3.3.3" xref="S3.SS1.p2.8.m8.3.3.3.3.3.3.cmml">
           a
          </mi>
         </msub>
         <mo id="S3.SS1.p2.8.m8.3.3.3.3.7" stretchy="false" xref="S3.SS1.p2.8.m8.3.3.3.4.cmml">
          )
         </mo>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.3b">
        <apply id="S3.SS1.p2.8.m8.3.3.cmml" xref="S3.SS1.p2.8.m8.3.3">
         <eq id="S3.SS1.p2.8.m8.3.3.4.cmml" xref="S3.SS1.p2.8.m8.3.3.4">
         </eq>
         <ci id="S3.SS1.p2.8.m8.3.3.5.cmml" xref="S3.SS1.p2.8.m8.3.3.5">
          𝑿
         </ci>
         <vector id="S3.SS1.p2.8.m8.3.3.3.4.cmml" xref="S3.SS1.p2.8.m8.3.3.3.3">
          <apply id="S3.SS1.p2.8.m8.1.1.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.1">
           <csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.1">
            subscript
           </csymbol>
           <ci id="S3.SS1.p2.8.m8.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.1.2">
            𝑋
           </ci>
           <ci id="S3.SS1.p2.8.m8.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.8.m8.1.1.1.1.1.3">
            𝑣
           </ci>
          </apply>
          <apply id="S3.SS1.p2.8.m8.2.2.2.2.2.cmml" xref="S3.SS1.p2.8.m8.2.2.2.2.2">
           <csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.2.2.2.2.2.1.cmml" xref="S3.SS1.p2.8.m8.2.2.2.2.2">
            subscript
           </csymbol>
           <ci id="S3.SS1.p2.8.m8.2.2.2.2.2.2.cmml" xref="S3.SS1.p2.8.m8.2.2.2.2.2.2">
            𝑋
           </ci>
           <ci id="S3.SS1.p2.8.m8.2.2.2.2.2.3.cmml" xref="S3.SS1.p2.8.m8.2.2.2.2.2.3">
            𝑞
           </ci>
          </apply>
          <apply id="S3.SS1.p2.8.m8.3.3.3.3.3.cmml" xref="S3.SS1.p2.8.m8.3.3.3.3.3">
           <csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.3.3.3.3.3.1.cmml" xref="S3.SS1.p2.8.m8.3.3.3.3.3">
            subscript
           </csymbol>
           <ci id="S3.SS1.p2.8.m8.3.3.3.3.3.2.cmml" xref="S3.SS1.p2.8.m8.3.3.3.3.3.2">
            𝑋
           </ci>
           <ci id="S3.SS1.p2.8.m8.3.3.3.3.3.3.cmml" xref="S3.SS1.p2.8.m8.3.3.3.3.3.3">
            𝑎
           </ci>
          </apply>
         </vector>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.3c">
        \bm{X}=(X_{v},X_{q},X_{a})
       </annotation>
      </semantics>
     </math>
     . For the training stage of the task planner, we directly leverage the groundtruth object list for each scene to avoid the influence of inaccurate visual perception. For the inference phase, the extended open-vocabulary object detector predicts the list of all existed objects in the scene.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p3">
    <p class="ltx_p" id="S3.SS1.p3.1">
     We employ the AI2-THOR simulator
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib49" title="">
       49
      </a>
      ]
     </cite>
     as the embodied environment for our agent, where we split the scenes with 80 for training and 20 for evaluation. To enlarge the scale and diversity of instructions and action steps in training samples for effective task planner finetuning, we expand the original 80 training scenes to 6400 training scenes by directly modifying the groundtruth object list. For each scene type, we initially acquire the list of objects that possibly appear in this type of scene by enumerating all rooms in the same room type. Then we randomly substitute existed objects with other ones that possibly exist in the same room type and are not observed. The plausibility constraint aims to prevent generating counterintuitive objects for given scene types. We collected 15K samples for training and leverages another 60 triplets for evaluation with our multimodal data generation framework.
    </p>
   </div>
   <figure class="ltx_table" id="S3.T1">
    <div class="ltx_block ltx_minipage ltx_align_center ltx_align_middle" id="S3.T1.1" style="width:433.6pt;">
     <div class="ltx_para ltx_noindent" id="S3.T1.1.p1">
      <svg class="ltx_picture" height="850.83" id="S3.T1.1.p1.pic1" overflow="visible" version="1.1" width="600">
       <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,850.83) matrix(1 0 0 -1 0 0)">
        <g fill="#404040" fill-opacity="1.0">
         <path d="M 0 5.91 L 0 844.93 C 0 848.19 2.64 850.83 5.91 850.83 L 594.09 850.83 C 597.36 850.83 600 848.19 600 844.93 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none">
         </path>
        </g>
        <g fill="#F2F2F2" fill-opacity="1.0">
         <path d="M 1.97 5.91 L 1.97 844.93 C 1.97 847.1 3.73 848.87 5.91 848.87 L 594.09 848.87 C 596.27 848.87 598.03 847.1 598.03 844.93 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none">
         </path>
        </g>
        <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)">
         <foreignobject color="#000000" height="823.28" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
          <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" style="width:402.3pt;">
           <span class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2">
            <span class="ltx_tr" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2">
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" style="width:199.5pt;">
                <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="236" id="S3.T1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" src="/html/2307.01848/assets/figs/top_down_5.png" width="236"/>
               </span>
              </span>
             </span>
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1" style="width:216.8pt;">
                <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="236" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.g1" src="/html/2307.01848/assets/figs/top_down.png" width="236"/>
               </span>
              </span>
             </span>
            </span>
            <span class="ltx_tr" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3">
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.1" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.1.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.1.1.1" style="width:199.5pt;">
                <span class="ltx_text ltx_font_typewriter" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.1.1.1.1">
                </span>
                <span class="ltx_text ltx_font_bold" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.1.1.1.2">
                 Instruction:
                </span>
               </span>
              </span>
             </span>
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.2" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.2.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.2.1.1" style="width:216.8pt;">
                <span class="ltx_text ltx_font_typewriter" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.2.1.1.1">
                </span>
                <span class="ltx_text ltx_font_bold" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.2.1.1.2">
                 Instruction:
                </span>
               </span>
              </span>
             </span>
            </span>
            <span class="ltx_tr" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.4">
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.4.1" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.4.1.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.4.1.1.1" style="width:199.5pt;">
                Can you clean the sink and the toilet, please?
               </span>
              </span>
             </span>
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.4.2" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.4.2.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.4.2.1.1" style="width:216.8pt;">
                Can you make me a sandwich?
               </span>
              </span>
             </span>
            </span>
            <span class="ltx_tr" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5">
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.1.1" style="width:199.5pt;">
                <span class="ltx_text ltx_font_typewriter" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.1.1.1">
                </span>
                <span class="ltx_text ltx_font_bold" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.1.1.2">
                 Task planning:
                </span>
               </span>
              </span>
             </span>
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.2" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.2.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.2.1.1" style="width:216.8pt;">
                <span class="ltx_text ltx_font_typewriter" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.2.1.1.1">
                </span>
                <span class="ltx_text ltx_font_bold" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.2.1.1.2">
                 Task planning:
                </span>
               </span>
              </span>
             </span>
            </span>
            <span class="ltx_tr" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.6">
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.6.1" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.6.1.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.6.1.1.1" style="width:199.5pt;">
                Step 1: Grasp a sponge
               </span>
              </span>
             </span>
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.6.2" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.6.2.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.6.2.1.1" style="width:216.8pt;">
                Step 1. Grasp a plate
               </span>
              </span>
             </span>
            </span>
            <span class="ltx_tr" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.7">
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.7.1" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.7.1.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.7.1.1.1" style="width:199.5pt;">
                Step 2: Move to the sink
               </span>
              </span>
             </span>
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.7.2" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.7.2.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.7.2.1.1" style="width:216.8pt;">
                Step 2. Grasp the knife
               </span>
              </span>
             </span>
            </span>
            <span class="ltx_tr" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.8">
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.8.1" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.8.1.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.8.1.1.1" style="width:199.5pt;">
                Step 3: Wet the sponge
               </span>
              </span>
             </span>
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.8.2" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.8.2.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.8.2.1.1" style="width:216.8pt;">
                Step 3. Grasp a piece of bread
               </span>
              </span>
             </span>
            </span>
            <span class="ltx_tr" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.9">
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.9.1" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.9.1.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.9.1.1.1" style="width:199.5pt;">
                Step 4: Scrub the sink
               </span>
              </span>
             </span>
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.9.2" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.9.2.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.9.2.1.1" style="width:216.8pt;">
                Step 4. Move the knife to the bread and slice it
               </span>
              </span>
             </span>
            </span>
            <span class="ltx_tr" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.10">
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.10.1" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.10.1.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.10.1.1.1" style="width:199.5pt;">
                Step 5: Rinse the sponge
               </span>
              </span>
             </span>
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.10.2" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.10.2.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.10.2.1.1" style="width:216.8pt;">
                Step 5. Grasp another piece of bread
               </span>
              </span>
             </span>
            </span>
            <span class="ltx_tr" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.11">
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.11.1" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.11.1.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.11.1.1.1" style="width:199.5pt;">
                Step 6: Grasp a towel
               </span>
              </span>
             </span>
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.11.2" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.11.2.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.11.2.1.1" style="width:216.8pt;">
                Step 6. Move the knife to the bread and slice it
               </span>
              </span>
             </span>
            </span>
            <span class="ltx_tr" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.12">
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.12.1" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.12.1.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.12.1.1.1" style="width:199.5pt;">
                Step 7: Dry the sink
               </span>
              </span>
             </span>
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.12.2" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.12.2.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.12.2.1.1" style="width:216.8pt;">
                Step 7. Grasp a lettuce
               </span>
              </span>
             </span>
            </span>
            <span class="ltx_tr" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.13">
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.13.1" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.13.1.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.13.1.1.1" style="width:199.5pt;">
                Step 8: Move to the toilet
               </span>
              </span>
             </span>
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.13.2" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.13.2.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.13.2.1.1" style="width:216.8pt;">
                Step 8. Tear the lettuce and place it on the plate
               </span>
              </span>
             </span>
            </span>
            <span class="ltx_tr" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.14">
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.14.1" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.14.1.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.14.1.1.1" style="width:199.5pt;">
                Step 9. Grasp a scrub brush
               </span>
              </span>
             </span>
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.14.2" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.14.2.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.14.2.1.1" style="width:216.8pt;">
                Step 9. Grasp a tomato
               </span>
              </span>
             </span>
            </span>
            <span class="ltx_tr" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.15">
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.15.1" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.15.1.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.15.1.1.1" style="width:199.5pt;">
                Step 10. Scrub the toilet bowl
               </span>
              </span>
             </span>
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.15.2" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.15.2.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.15.2.1.1" style="width:216.8pt;">
                Step 10. Slice the tomato and place it on the plate
               </span>
              </span>
             </span>
            </span>
            <span class="ltx_tr" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.16">
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.16.1" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.16.1.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.16.1.1.1" style="width:199.5pt;">
                Step 11. Place the scrub brush back in its place
               </span>
              </span>
             </span>
             <span class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.16.2" style="padding-top:1pt;padding-bottom:1pt;">
              <span class="ltx_inline-block ltx_align_top" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.16.2.1">
               <span class="ltx_p" id="S3.T1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.16.2.1.1" style="width:216.8pt;">
                Step 11. Move the two slices of bread to the plate
               </span>
              </span>
             </span>
            </span>
           </span>
          </span>
         </foreignobject>
        </g>
       </g>
      </svg>
     </div>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_block">
       Table 1:
      </span>
      Examples of the generated multimodal triplet data including visual scenes, instructions and the corresponding plans.
     </figcaption>
    </div>
   </figure>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Grounding Task Plans to Surrounding Scenes
   </h3>
   <div class="ltx_para ltx_noindent" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     In order to ground the embodied task plan to the physical world with feasibility constraints, it is necessary to accurately obtain the object list in the scene without instance missing or false positives. We generalize the open-vocabulary object detector for object list acquisition since novel objects unseen in detector training may appear in the deployment scenarios. As shown in Figure
     <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Embodied Task Planning with Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     , the agent collects RGB images in different locations to perceive the visual scenes to discover existed objects. We design several image collection strategies to explore the surrounding 3D scenes. The location selection criteria contains traversal positions, random positions, the overall center point and block-wise center points, and the agent rotates the camera to obtain multi-view images for each location selection criteria. Therefore, we formally write the image collection strategies
     <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1">
      <semantics id="S3.SS2.p1.1.m1.1a">
       <mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">
        𝒮
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b">
        <ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">
         𝒮
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">
        \mathcal{S}
       </annotation>
      </semantics>
     </math>
     in the following:
    </p>
    <table class="ltx_equation ltx_eqn_table" id="S3.E1">
     <tbody>
      <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
       <td class="ltx_eqn_cell ltx_eqn_center_padleft">
       </td>
       <td class="ltx_eqn_cell ltx_align_center">
        <math alttext="\mathcal{S}=\{(x,y,\theta)|(x,y)\in L(\bm{\lambda},\mathcal{A}),\theta=k\theta_{0}\}" class="ltx_Math" display="block" id="S3.E1.m1.9">
         <semantics id="S3.E1.m1.9a">
          <mrow id="S3.E1.m1.9.9" xref="S3.E1.m1.9.9.cmml">
           <mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.9.9.4" xref="S3.E1.m1.9.9.4.cmml">
            𝒮
           </mi>
           <mo id="S3.E1.m1.9.9.3" xref="S3.E1.m1.9.9.3.cmml">
            =
           </mo>
           <mrow id="S3.E1.m1.9.9.2.2" xref="S3.E1.m1.9.9.2.3.cmml">
            <mo id="S3.E1.m1.9.9.2.2.3" stretchy="false" xref="S3.E1.m1.9.9.2.3.1.cmml">
             {
            </mo>
            <mrow id="S3.E1.m1.8.8.1.1.1.2" xref="S3.E1.m1.8.8.1.1.1.1.cmml">
             <mo id="S3.E1.m1.8.8.1.1.1.2.1" stretchy="false" xref="S3.E1.m1.8.8.1.1.1.1.cmml">
              (
             </mo>
             <mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">
              x
             </mi>
             <mo id="S3.E1.m1.8.8.1.1.1.2.2" xref="S3.E1.m1.8.8.1.1.1.1.cmml">
              ,
             </mo>
             <mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">
              y
             </mi>
             <mo id="S3.E1.m1.8.8.1.1.1.2.3" xref="S3.E1.m1.8.8.1.1.1.1.cmml">
              ,
             </mo>
             <mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">
              θ
             </mi>
             <mo id="S3.E1.m1.8.8.1.1.1.2.4" stretchy="false" xref="S3.E1.m1.8.8.1.1.1.1.cmml">
              )
             </mo>
            </mrow>
            <mo id="S3.E1.m1.9.9.2.2.4" lspace="0em" rspace="0em" xref="S3.E1.m1.9.9.2.3.1.cmml">
             |
            </mo>
            <mrow id="S3.E1.m1.9.9.2.2.2.2" xref="S3.E1.m1.9.9.2.2.2.3.cmml">
             <mrow id="S3.E1.m1.9.9.2.2.2.1.1" xref="S3.E1.m1.9.9.2.2.2.1.1.cmml">
              <mrow id="S3.E1.m1.9.9.2.2.2.1.1.2.2" xref="S3.E1.m1.9.9.2.2.2.1.1.2.1.cmml">
               <mo id="S3.E1.m1.9.9.2.2.2.1.1.2.2.1" stretchy="false" xref="S3.E1.m1.9.9.2.2.2.1.1.2.1.cmml">
                (
               </mo>
               <mi id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">
                x
               </mi>
               <mo id="S3.E1.m1.9.9.2.2.2.1.1.2.2.2" xref="S3.E1.m1.9.9.2.2.2.1.1.2.1.cmml">
                ,
               </mo>
               <mi id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml">
                y
               </mi>
               <mo id="S3.E1.m1.9.9.2.2.2.1.1.2.2.3" stretchy="false" xref="S3.E1.m1.9.9.2.2.2.1.1.2.1.cmml">
                )
               </mo>
              </mrow>
              <mo id="S3.E1.m1.9.9.2.2.2.1.1.1" xref="S3.E1.m1.9.9.2.2.2.1.1.1.cmml">
               ∈
              </mo>
              <mrow id="S3.E1.m1.9.9.2.2.2.1.1.3" xref="S3.E1.m1.9.9.2.2.2.1.1.3.cmml">
               <mi id="S3.E1.m1.9.9.2.2.2.1.1.3.2" xref="S3.E1.m1.9.9.2.2.2.1.1.3.2.cmml">
                L
               </mi>
               <mo id="S3.E1.m1.9.9.2.2.2.1.1.3.1" lspace="0em" rspace="0em" xref="S3.E1.m1.9.9.2.2.2.1.1.3.1.cmml">
                ​
               </mo>
               <mrow id="S3.E1.m1.9.9.2.2.2.1.1.3.3.2" xref="S3.E1.m1.9.9.2.2.2.1.1.3.3.1.cmml">
                <mo id="S3.E1.m1.9.9.2.2.2.1.1.3.3.2.1" stretchy="false" xref="S3.E1.m1.9.9.2.2.2.1.1.3.3.1.cmml">
                 (
                </mo>
                <mi id="S3.E1.m1.6.6" xref="S3.E1.m1.6.6.cmml">
                 𝝀
                </mi>
                <mo id="S3.E1.m1.9.9.2.2.2.1.1.3.3.2.2" xref="S3.E1.m1.9.9.2.2.2.1.1.3.3.1.cmml">
                 ,
                </mo>
                <mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.7.7" xref="S3.E1.m1.7.7.cmml">
                 𝒜
                </mi>
                <mo id="S3.E1.m1.9.9.2.2.2.1.1.3.3.2.3" stretchy="false" xref="S3.E1.m1.9.9.2.2.2.1.1.3.3.1.cmml">
                 )
                </mo>
               </mrow>
              </mrow>
             </mrow>
             <mo id="S3.E1.m1.9.9.2.2.2.2.3" xref="S3.E1.m1.9.9.2.2.2.3a.cmml">
              ,
             </mo>
             <mrow id="S3.E1.m1.9.9.2.2.2.2.2" xref="S3.E1.m1.9.9.2.2.2.2.2.cmml">
              <mi id="S3.E1.m1.9.9.2.2.2.2.2.2" xref="S3.E1.m1.9.9.2.2.2.2.2.2.cmml">
               θ
              </mi>
              <mo id="S3.E1.m1.9.9.2.2.2.2.2.1" xref="S3.E1.m1.9.9.2.2.2.2.2.1.cmml">
               =
              </mo>
              <mrow id="S3.E1.m1.9.9.2.2.2.2.2.3" xref="S3.E1.m1.9.9.2.2.2.2.2.3.cmml">
               <mi id="S3.E1.m1.9.9.2.2.2.2.2.3.2" xref="S3.E1.m1.9.9.2.2.2.2.2.3.2.cmml">
                k
               </mi>
               <mo id="S3.E1.m1.9.9.2.2.2.2.2.3.1" lspace="0em" rspace="0em" xref="S3.E1.m1.9.9.2.2.2.2.2.3.1.cmml">
                ​
               </mo>
               <msub id="S3.E1.m1.9.9.2.2.2.2.2.3.3" xref="S3.E1.m1.9.9.2.2.2.2.2.3.3.cmml">
                <mi id="S3.E1.m1.9.9.2.2.2.2.2.3.3.2" xref="S3.E1.m1.9.9.2.2.2.2.2.3.3.2.cmml">
                 θ
                </mi>
                <mn id="S3.E1.m1.9.9.2.2.2.2.2.3.3.3" xref="S3.E1.m1.9.9.2.2.2.2.2.3.3.3.cmml">
                 0
                </mn>
               </msub>
              </mrow>
             </mrow>
            </mrow>
            <mo id="S3.E1.m1.9.9.2.2.5" stretchy="false" xref="S3.E1.m1.9.9.2.3.1.cmml">
             }
            </mo>
           </mrow>
          </mrow>
          <annotation-xml encoding="MathML-Content" id="S3.E1.m1.9b">
           <apply id="S3.E1.m1.9.9.cmml" xref="S3.E1.m1.9.9">
            <eq id="S3.E1.m1.9.9.3.cmml" xref="S3.E1.m1.9.9.3">
            </eq>
            <ci id="S3.E1.m1.9.9.4.cmml" xref="S3.E1.m1.9.9.4">
             𝒮
            </ci>
            <apply id="S3.E1.m1.9.9.2.3.cmml" xref="S3.E1.m1.9.9.2.2">
             <csymbol cd="latexml" id="S3.E1.m1.9.9.2.3.1.cmml" xref="S3.E1.m1.9.9.2.2.3">
              conditional-set
             </csymbol>
             <vector id="S3.E1.m1.8.8.1.1.1.1.cmml" xref="S3.E1.m1.8.8.1.1.1.2">
              <ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">
               𝑥
              </ci>
              <ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">
               𝑦
              </ci>
              <ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">
               𝜃
              </ci>
             </vector>
             <apply id="S3.E1.m1.9.9.2.2.2.3.cmml" xref="S3.E1.m1.9.9.2.2.2.2">
              <csymbol cd="ambiguous" id="S3.E1.m1.9.9.2.2.2.3a.cmml" xref="S3.E1.m1.9.9.2.2.2.2.3">
               formulae-sequence
              </csymbol>
              <apply id="S3.E1.m1.9.9.2.2.2.1.1.cmml" xref="S3.E1.m1.9.9.2.2.2.1.1">
               <in id="S3.E1.m1.9.9.2.2.2.1.1.1.cmml" xref="S3.E1.m1.9.9.2.2.2.1.1.1">
               </in>
               <interval closure="open" id="S3.E1.m1.9.9.2.2.2.1.1.2.1.cmml" xref="S3.E1.m1.9.9.2.2.2.1.1.2.2">
                <ci id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4">
                 𝑥
                </ci>
                <ci id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5">
                 𝑦
                </ci>
               </interval>
               <apply id="S3.E1.m1.9.9.2.2.2.1.1.3.cmml" xref="S3.E1.m1.9.9.2.2.2.1.1.3">
                <times id="S3.E1.m1.9.9.2.2.2.1.1.3.1.cmml" xref="S3.E1.m1.9.9.2.2.2.1.1.3.1">
                </times>
                <ci id="S3.E1.m1.9.9.2.2.2.1.1.3.2.cmml" xref="S3.E1.m1.9.9.2.2.2.1.1.3.2">
                 𝐿
                </ci>
                <interval closure="open" id="S3.E1.m1.9.9.2.2.2.1.1.3.3.1.cmml" xref="S3.E1.m1.9.9.2.2.2.1.1.3.3.2">
                 <ci id="S3.E1.m1.6.6.cmml" xref="S3.E1.m1.6.6">
                  𝝀
                 </ci>
                 <ci id="S3.E1.m1.7.7.cmml" xref="S3.E1.m1.7.7">
                  𝒜
                 </ci>
                </interval>
               </apply>
              </apply>
              <apply id="S3.E1.m1.9.9.2.2.2.2.2.cmml" xref="S3.E1.m1.9.9.2.2.2.2.2">
               <eq id="S3.E1.m1.9.9.2.2.2.2.2.1.cmml" xref="S3.E1.m1.9.9.2.2.2.2.2.1">
               </eq>
               <ci id="S3.E1.m1.9.9.2.2.2.2.2.2.cmml" xref="S3.E1.m1.9.9.2.2.2.2.2.2">
                𝜃
               </ci>
               <apply id="S3.E1.m1.9.9.2.2.2.2.2.3.cmml" xref="S3.E1.m1.9.9.2.2.2.2.2.3">
                <times id="S3.E1.m1.9.9.2.2.2.2.2.3.1.cmml" xref="S3.E1.m1.9.9.2.2.2.2.2.3.1">
                </times>
                <ci id="S3.E1.m1.9.9.2.2.2.2.2.3.2.cmml" xref="S3.E1.m1.9.9.2.2.2.2.2.3.2">
                 𝑘
                </ci>
                <apply id="S3.E1.m1.9.9.2.2.2.2.2.3.3.cmml" xref="S3.E1.m1.9.9.2.2.2.2.2.3.3">
                 <csymbol cd="ambiguous" id="S3.E1.m1.9.9.2.2.2.2.2.3.3.1.cmml" xref="S3.E1.m1.9.9.2.2.2.2.2.3.3">
                  subscript
                 </csymbol>
                 <ci id="S3.E1.m1.9.9.2.2.2.2.2.3.3.2.cmml" xref="S3.E1.m1.9.9.2.2.2.2.2.3.3.2">
                  𝜃
                 </ci>
                 <cn id="S3.E1.m1.9.9.2.2.2.2.2.3.3.3.cmml" type="integer" xref="S3.E1.m1.9.9.2.2.2.2.2.3.3.3">
                  0
                 </cn>
                </apply>
               </apply>
              </apply>
             </apply>
            </apply>
           </apply>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.E1.m1.9c">
           \mathcal{S}=\{(x,y,\theta)|(x,y)\in L(\bm{\lambda},\mathcal{A}),\theta=k\theta_{0}\}
          </annotation>
         </semantics>
        </math>
       </td>
       <td class="ltx_eqn_cell ltx_eqn_center_padright">
       </td>
       <td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1">
        <span class="ltx_tag ltx_tag_equation ltx_align_right">
         (1)
        </span>
       </td>
      </tr>
     </tbody>
    </table>
    <p class="ltx_p" id="S3.SS2.p1.7">
     where
     <math alttext="(x,y,\theta)" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m1.3">
      <semantics id="S3.SS2.p1.2.m1.3a">
       <mrow id="S3.SS2.p1.2.m1.3.4.2" xref="S3.SS2.p1.2.m1.3.4.1.cmml">
        <mo id="S3.SS2.p1.2.m1.3.4.2.1" stretchy="false" xref="S3.SS2.p1.2.m1.3.4.1.cmml">
         (
        </mo>
        <mi id="S3.SS2.p1.2.m1.1.1" xref="S3.SS2.p1.2.m1.1.1.cmml">
         x
        </mi>
        <mo id="S3.SS2.p1.2.m1.3.4.2.2" xref="S3.SS2.p1.2.m1.3.4.1.cmml">
         ,
        </mo>
        <mi id="S3.SS2.p1.2.m1.2.2" xref="S3.SS2.p1.2.m1.2.2.cmml">
         y
        </mi>
        <mo id="S3.SS2.p1.2.m1.3.4.2.3" xref="S3.SS2.p1.2.m1.3.4.1.cmml">
         ,
        </mo>
        <mi id="S3.SS2.p1.2.m1.3.3" xref="S3.SS2.p1.2.m1.3.3.cmml">
         θ
        </mi>
        <mo id="S3.SS2.p1.2.m1.3.4.2.4" stretchy="false" xref="S3.SS2.p1.2.m1.3.4.1.cmml">
         )
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m1.3b">
        <vector id="S3.SS2.p1.2.m1.3.4.1.cmml" xref="S3.SS2.p1.2.m1.3.4.2">
         <ci id="S3.SS2.p1.2.m1.1.1.cmml" xref="S3.SS2.p1.2.m1.1.1">
          𝑥
         </ci>
         <ci id="S3.SS2.p1.2.m1.2.2.cmml" xref="S3.SS2.p1.2.m1.2.2">
          𝑦
         </ci>
         <ci id="S3.SS2.p1.2.m1.3.3.cmml" xref="S3.SS2.p1.2.m1.3.3">
          𝜃
         </ci>
        </vector>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.2.m1.3c">
        (x,y,\theta)
       </annotation>
      </semantics>
     </math>
     represents the location and camera orientation.
     <math alttext="L(\bm{\lambda},\mathcal{A})" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m2.2">
      <semantics id="S3.SS2.p1.3.m2.2a">
       <mrow id="S3.SS2.p1.3.m2.2.3" xref="S3.SS2.p1.3.m2.2.3.cmml">
        <mi id="S3.SS2.p1.3.m2.2.3.2" xref="S3.SS2.p1.3.m2.2.3.2.cmml">
         L
        </mi>
        <mo id="S3.SS2.p1.3.m2.2.3.1" lspace="0em" rspace="0em" xref="S3.SS2.p1.3.m2.2.3.1.cmml">
         ​
        </mo>
        <mrow id="S3.SS2.p1.3.m2.2.3.3.2" xref="S3.SS2.p1.3.m2.2.3.3.1.cmml">
         <mo id="S3.SS2.p1.3.m2.2.3.3.2.1" stretchy="false" xref="S3.SS2.p1.3.m2.2.3.3.1.cmml">
          (
         </mo>
         <mi id="S3.SS2.p1.3.m2.1.1" xref="S3.SS2.p1.3.m2.1.1.cmml">
          𝝀
         </mi>
         <mo id="S3.SS2.p1.3.m2.2.3.3.2.2" xref="S3.SS2.p1.3.m2.2.3.3.1.cmml">
          ,
         </mo>
         <mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.3.m2.2.2" xref="S3.SS2.p1.3.m2.2.2.cmml">
          𝒜
         </mi>
         <mo id="S3.SS2.p1.3.m2.2.3.3.2.3" stretchy="false" xref="S3.SS2.p1.3.m2.2.3.3.1.cmml">
          )
         </mo>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m2.2b">
        <apply id="S3.SS2.p1.3.m2.2.3.cmml" xref="S3.SS2.p1.3.m2.2.3">
         <times id="S3.SS2.p1.3.m2.2.3.1.cmml" xref="S3.SS2.p1.3.m2.2.3.1">
         </times>
         <ci id="S3.SS2.p1.3.m2.2.3.2.cmml" xref="S3.SS2.p1.3.m2.2.3.2">
          𝐿
         </ci>
         <interval closure="open" id="S3.SS2.p1.3.m2.2.3.3.1.cmml" xref="S3.SS2.p1.3.m2.2.3.3.2">
          <ci id="S3.SS2.p1.3.m2.1.1.cmml" xref="S3.SS2.p1.3.m2.1.1">
           𝝀
          </ci>
          <ci id="S3.SS2.p1.3.m2.2.2.cmml" xref="S3.SS2.p1.3.m2.2.2">
           𝒜
          </ci>
         </interval>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.3.m2.2c">
        L(\bm{\lambda},\mathcal{A})
       </annotation>
      </semantics>
     </math>
     means the location selection criteria with the hyperparameter
     <math alttext="\bm{\lambda}" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m3.1">
      <semantics id="S3.SS2.p1.4.m3.1a">
       <mi id="S3.SS2.p1.4.m3.1.1" xref="S3.SS2.p1.4.m3.1.1.cmml">
        𝝀
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m3.1b">
        <ci id="S3.SS2.p1.4.m3.1.1.cmml" xref="S3.SS2.p1.4.m3.1.1">
         𝝀
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.4.m3.1c">
        \bm{\lambda}
       </annotation>
      </semantics>
     </math>
     and all sampled locations are required within the achievable area
     <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m4.1">
      <semantics id="S3.SS2.p1.5.m4.1a">
       <mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.5.m4.1.1" xref="S3.SS2.p1.5.m4.1.1.cmml">
        𝒜
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m4.1b">
        <ci id="S3.SS2.p1.5.m4.1.1.cmml" xref="S3.SS2.p1.5.m4.1.1">
         𝒜
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.5.m4.1c">
        \mathcal{A}
       </annotation>
      </semantics>
     </math>
     . The unit angle for camera rotation is set to
     <math alttext="\theta_{0}" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m5.1">
      <semantics id="S3.SS2.p1.6.m5.1a">
       <msub id="S3.SS2.p1.6.m5.1.1" xref="S3.SS2.p1.6.m5.1.1.cmml">
        <mi id="S3.SS2.p1.6.m5.1.1.2" xref="S3.SS2.p1.6.m5.1.1.2.cmml">
         θ
        </mi>
        <mn id="S3.SS2.p1.6.m5.1.1.3" xref="S3.SS2.p1.6.m5.1.1.3.cmml">
         0
        </mn>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m5.1b">
        <apply id="S3.SS2.p1.6.m5.1.1.cmml" xref="S3.SS2.p1.6.m5.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p1.6.m5.1.1.1.cmml" xref="S3.SS2.p1.6.m5.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p1.6.m5.1.1.2.cmml" xref="S3.SS2.p1.6.m5.1.1.2">
          𝜃
         </ci>
         <cn id="S3.SS2.p1.6.m5.1.1.3.cmml" type="integer" xref="S3.SS2.p1.6.m5.1.1.3">
          0
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.6.m5.1c">
        \theta_{0}
       </annotation>
      </semantics>
     </math>
     , and
     <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.p1.7.m6.1">
      <semantics id="S3.SS2.p1.7.m6.1a">
       <mi id="S3.SS2.p1.7.m6.1.1" xref="S3.SS2.p1.7.m6.1.1.cmml">
        k
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m6.1b">
        <ci id="S3.SS2.p1.7.m6.1.1.cmml" xref="S3.SS2.p1.7.m6.1.1">
         𝑘
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p1.7.m6.1c">
        k
       </annotation>
      </semantics>
     </math>
     is an integer so that the agent collects visual clues in different directions of the scene. The hyperparameter that all location selection criteria share is the grid side length, where we divide the achievable area into grids. Traversal positions choose all grid points for RGB image collection. Random positions only randomly selected part of the grid points for visual information perception, and the hyperparameters also contain the ratio of sampled grid points. The overall center point stands for the center of the whole scene without any hyperparameters. The block-wise center points aim to choose the center of each division in the scene to efficiently acquire fine-grained visual information. Inspired by
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib50" title="">
       50
      </a>
      ,
      <a class="ltx_ref" href="#bib.bib51" title="">
       51
      </a>
      ]
     </cite>
     , clustering methods can effectively divide the entire scene into several sub-regions to improve the performance of perception, so that the prior information of the room layout is embedded into the image collection strategy with the K-means clustering method. Meanwhile, we employ within cluster sum of squared errors (WCSS) principle to select the optimal number of clusters for each scene. Compared to the images collection strategy of traversal points, the block-wise center point only traverses centroids of the subregions to acquire sufficient visual information.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS2.p2">
    <p class="ltx_p" id="S3.SS2.p2.1">
     The embodied task planner requires the information of all existed objects in the scene to generate executable action steps, where we generalize the open-vocabulary object detector to the collected multi-view RGB images for the object list acquisition. The predicted object list
     <math alttext="\hat{X}_{l}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1">
      <semantics id="S3.SS2.p2.1.m1.1a">
       <msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">
        <mover accent="true" id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">
         <mi id="S3.SS2.p2.1.m1.1.1.2.2" xref="S3.SS2.p2.1.m1.1.1.2.2.cmml">
          X
         </mi>
         <mo id="S3.SS2.p2.1.m1.1.1.2.1" xref="S3.SS2.p2.1.m1.1.1.2.1.cmml">
          ^
         </mo>
        </mover>
        <mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">
         l
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b">
        <apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">
          subscript
         </csymbol>
         <apply id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">
          <ci id="S3.SS2.p2.1.m1.1.1.2.1.cmml" xref="S3.SS2.p2.1.m1.1.1.2.1">
           ^
          </ci>
          <ci id="S3.SS2.p2.1.m1.1.1.2.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2.2">
           𝑋
          </ci>
         </apply>
         <ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">
          𝑙
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">
        \hat{X}_{l}
       </annotation>
      </semantics>
     </math>
     for the scene is acquired by removing the duplicated object names in the detection results of multi-view images:
    </p>
    <table class="ltx_equation ltx_eqn_table" id="S3.E2">
     <tbody>
      <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
       <td class="ltx_eqn_cell ltx_eqn_center_padleft">
       </td>
       <td class="ltx_eqn_cell ltx_align_center">
        <math alttext="\hat{X}_{l}={\rm Rd}\big{(}\bigcup_{i}D(\bm{I}_{i})\big{)}" class="ltx_Math" display="block" id="S3.E2.m1.1">
         <semantics id="S3.E2.m1.1a">
          <mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">
           <msub id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml">
            <mover accent="true" id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">
             <mi id="S3.E2.m1.1.1.3.2.2" xref="S3.E2.m1.1.1.3.2.2.cmml">
              X
             </mi>
             <mo id="S3.E2.m1.1.1.3.2.1" xref="S3.E2.m1.1.1.3.2.1.cmml">
              ^
             </mo>
            </mover>
            <mi id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml">
             l
            </mi>
           </msub>
           <mo id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">
            =
           </mo>
           <mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml">
            <mi id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml">
             Rd
            </mi>
            <mo id="S3.E2.m1.1.1.1.2" lspace="0em" rspace="0em" xref="S3.E2.m1.1.1.1.2.cmml">
             ​
            </mo>
            <mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml">
             <mo id="S3.E2.m1.1.1.1.1.1.2" maxsize="120%" minsize="120%" xref="S3.E2.m1.1.1.1.1.1.1.cmml">
              (
             </mo>
             <mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml">
              <munder id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">
               <mo id="S3.E2.m1.1.1.1.1.1.1.2.2" lspace="0em" movablelimits="false" xref="S3.E2.m1.1.1.1.1.1.1.2.2.cmml">
                ⋃
               </mo>
               <mi id="S3.E2.m1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.2.3.cmml">
                i
               </mi>
              </munder>
              <mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">
               <mi id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.cmml">
                D
               </mi>
               <mo id="S3.E2.m1.1.1.1.1.1.1.1.2" lspace="0em" rspace="0em" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml">
                ​
               </mo>
               <mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml">
                <mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml">
                 (
                </mo>
                <msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml">
                 <mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">
                  𝑰
                 </mi>
                 <mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">
                  i
                 </mi>
                </msub>
                <mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml">
                 )
                </mo>
               </mrow>
              </mrow>
             </mrow>
             <mo id="S3.E2.m1.1.1.1.1.1.3" maxsize="120%" minsize="120%" xref="S3.E2.m1.1.1.1.1.1.1.cmml">
              )
             </mo>
            </mrow>
           </mrow>
          </mrow>
          <annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b">
           <apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">
            <eq id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2">
            </eq>
            <apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3">
             <csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3">
              subscript
             </csymbol>
             <apply id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">
              <ci id="S3.E2.m1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.3.2.1">
               ^
              </ci>
              <ci id="S3.E2.m1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.3.2.2">
               𝑋
              </ci>
             </apply>
             <ci id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3">
              𝑙
             </ci>
            </apply>
            <apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1">
             <times id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2">
             </times>
             <ci id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3">
              Rd
             </ci>
             <apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1">
              <apply id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2">
               <csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2">
                subscript
               </csymbol>
               <union id="S3.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2">
               </union>
               <ci id="S3.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.3">
                𝑖
               </ci>
              </apply>
              <apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1">
               <times id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2">
               </times>
               <ci id="S3.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3">
                𝐷
               </ci>
               <apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1">
                <csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1">
                 subscript
                </csymbol>
                <ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2">
                 𝑰
                </ci>
                <ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3">
                 𝑖
                </ci>
               </apply>
              </apply>
             </apply>
            </apply>
           </apply>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.E2.m1.1c">
           \hat{X}_{l}={\rm Rd}\big{(}\bigcup_{i}D(\bm{I}_{i})\big{)}
          </annotation>
         </semantics>
        </math>
       </td>
       <td class="ltx_eqn_cell ltx_eqn_center_padright">
       </td>
       <td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1">
        <span class="ltx_tag ltx_tag_equation ltx_align_right">
         (2)
        </span>
       </td>
      </tr>
     </tbody>
    </table>
    <p class="ltx_p" id="S3.SS2.p2.8">
     where
     <math alttext="\rm{Rd}" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m1.1">
      <semantics id="S3.SS2.p2.2.m1.1a">
       <mi id="S3.SS2.p2.2.m1.1.1" xref="S3.SS2.p2.2.m1.1.1.cmml">
        Rd
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m1.1b">
        <ci id="S3.SS2.p2.2.m1.1.1.cmml" xref="S3.SS2.p2.2.m1.1.1">
         Rd
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p2.2.m1.1c">
        \rm{Rd}
       </annotation>
      </semantics>
     </math>
     is the operation of removing duplicate object names and
     <math alttext="D(\bm{I}_{i})" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m2.1">
      <semantics id="S3.SS2.p2.3.m2.1a">
       <mrow id="S3.SS2.p2.3.m2.1.1" xref="S3.SS2.p2.3.m2.1.1.cmml">
        <mi id="S3.SS2.p2.3.m2.1.1.3" xref="S3.SS2.p2.3.m2.1.1.3.cmml">
         D
        </mi>
        <mo id="S3.SS2.p2.3.m2.1.1.2" lspace="0em" rspace="0em" xref="S3.SS2.p2.3.m2.1.1.2.cmml">
         ​
        </mo>
        <mrow id="S3.SS2.p2.3.m2.1.1.1.1" xref="S3.SS2.p2.3.m2.1.1.1.1.1.cmml">
         <mo id="S3.SS2.p2.3.m2.1.1.1.1.2" stretchy="false" xref="S3.SS2.p2.3.m2.1.1.1.1.1.cmml">
          (
         </mo>
         <msub id="S3.SS2.p2.3.m2.1.1.1.1.1" xref="S3.SS2.p2.3.m2.1.1.1.1.1.cmml">
          <mi id="S3.SS2.p2.3.m2.1.1.1.1.1.2" xref="S3.SS2.p2.3.m2.1.1.1.1.1.2.cmml">
           𝑰
          </mi>
          <mi id="S3.SS2.p2.3.m2.1.1.1.1.1.3" xref="S3.SS2.p2.3.m2.1.1.1.1.1.3.cmml">
           i
          </mi>
         </msub>
         <mo id="S3.SS2.p2.3.m2.1.1.1.1.3" stretchy="false" xref="S3.SS2.p2.3.m2.1.1.1.1.1.cmml">
          )
         </mo>
        </mrow>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m2.1b">
        <apply id="S3.SS2.p2.3.m2.1.1.cmml" xref="S3.SS2.p2.3.m2.1.1">
         <times id="S3.SS2.p2.3.m2.1.1.2.cmml" xref="S3.SS2.p2.3.m2.1.1.2">
         </times>
         <ci id="S3.SS2.p2.3.m2.1.1.3.cmml" xref="S3.SS2.p2.3.m2.1.1.3">
          𝐷
         </ci>
         <apply id="S3.SS2.p2.3.m2.1.1.1.1.1.cmml" xref="S3.SS2.p2.3.m2.1.1.1.1">
          <csymbol cd="ambiguous" id="S3.SS2.p2.3.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.3.m2.1.1.1.1">
           subscript
          </csymbol>
          <ci id="S3.SS2.p2.3.m2.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.3.m2.1.1.1.1.1.2">
           𝑰
          </ci>
          <ci id="S3.SS2.p2.3.m2.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.3.m2.1.1.1.1.1.3">
           𝑖
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p2.3.m2.1c">
        D(\bm{I}_{i})
       </annotation>
      </semantics>
     </math>
     represent the detected object names for the
     <math alttext="i_{th}" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m3.1">
      <semantics id="S3.SS2.p2.4.m3.1a">
       <msub id="S3.SS2.p2.4.m3.1.1" xref="S3.SS2.p2.4.m3.1.1.cmml">
        <mi id="S3.SS2.p2.4.m3.1.1.2" xref="S3.SS2.p2.4.m3.1.1.2.cmml">
         i
        </mi>
        <mrow id="S3.SS2.p2.4.m3.1.1.3" xref="S3.SS2.p2.4.m3.1.1.3.cmml">
         <mi id="S3.SS2.p2.4.m3.1.1.3.2" xref="S3.SS2.p2.4.m3.1.1.3.2.cmml">
          t
         </mi>
         <mo id="S3.SS2.p2.4.m3.1.1.3.1" lspace="0em" rspace="0em" xref="S3.SS2.p2.4.m3.1.1.3.1.cmml">
          ​
         </mo>
         <mi id="S3.SS2.p2.4.m3.1.1.3.3" xref="S3.SS2.p2.4.m3.1.1.3.3.cmml">
          h
         </mi>
        </mrow>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m3.1b">
        <apply id="S3.SS2.p2.4.m3.1.1.cmml" xref="S3.SS2.p2.4.m3.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p2.4.m3.1.1.1.cmml" xref="S3.SS2.p2.4.m3.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p2.4.m3.1.1.2.cmml" xref="S3.SS2.p2.4.m3.1.1.2">
          𝑖
         </ci>
         <apply id="S3.SS2.p2.4.m3.1.1.3.cmml" xref="S3.SS2.p2.4.m3.1.1.3">
          <times id="S3.SS2.p2.4.m3.1.1.3.1.cmml" xref="S3.SS2.p2.4.m3.1.1.3.1">
          </times>
          <ci id="S3.SS2.p2.4.m3.1.1.3.2.cmml" xref="S3.SS2.p2.4.m3.1.1.3.2">
           𝑡
          </ci>
          <ci id="S3.SS2.p2.4.m3.1.1.3.3.cmml" xref="S3.SS2.p2.4.m3.1.1.3.3">
           ℎ
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p2.4.m3.1c">
        i_{th}
       </annotation>
      </semantics>
     </math>
     RGB image collected in the scene. With our inference prompt
     <math alttext="P_{in}" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m4.1">
      <semantics id="S3.SS2.p2.5.m4.1a">
       <msub id="S3.SS2.p2.5.m4.1.1" xref="S3.SS2.p2.5.m4.1.1.cmml">
        <mi id="S3.SS2.p2.5.m4.1.1.2" xref="S3.SS2.p2.5.m4.1.1.2.cmml">
         P
        </mi>
        <mrow id="S3.SS2.p2.5.m4.1.1.3" xref="S3.SS2.p2.5.m4.1.1.3.cmml">
         <mi id="S3.SS2.p2.5.m4.1.1.3.2" xref="S3.SS2.p2.5.m4.1.1.3.2.cmml">
          i
         </mi>
         <mo id="S3.SS2.p2.5.m4.1.1.3.1" lspace="0em" rspace="0em" xref="S3.SS2.p2.5.m4.1.1.3.1.cmml">
          ​
         </mo>
         <mi id="S3.SS2.p2.5.m4.1.1.3.3" xref="S3.SS2.p2.5.m4.1.1.3.3.cmml">
          n
         </mi>
        </mrow>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m4.1b">
        <apply id="S3.SS2.p2.5.m4.1.1.cmml" xref="S3.SS2.p2.5.m4.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p2.5.m4.1.1.1.cmml" xref="S3.SS2.p2.5.m4.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p2.5.m4.1.1.2.cmml" xref="S3.SS2.p2.5.m4.1.1.2">
          𝑃
         </ci>
         <apply id="S3.SS2.p2.5.m4.1.1.3.cmml" xref="S3.SS2.p2.5.m4.1.1.3">
          <times id="S3.SS2.p2.5.m4.1.1.3.1.cmml" xref="S3.SS2.p2.5.m4.1.1.3.1">
          </times>
          <ci id="S3.SS2.p2.5.m4.1.1.3.2.cmml" xref="S3.SS2.p2.5.m4.1.1.3.2">
           𝑖
          </ci>
          <ci id="S3.SS2.p2.5.m4.1.1.3.3.cmml" xref="S3.SS2.p2.5.m4.1.1.3.3">
           𝑛
          </ci>
         </apply>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p2.5.m4.1c">
        P_{in}
       </annotation>
      </semantics>
     </math>
     shown in Table
     <a class="ltx_ref" href="#Ax1.T5" title="Table 5 ‣ Supplementary Material ‣ Embodied Task Planning with Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     of the supplementary material, the human instruction
     <math alttext="X_{q}" class="ltx_Math" display="inline" id="S3.SS2.p2.6.m5.1">
      <semantics id="S3.SS2.p2.6.m5.1a">
       <msub id="S3.SS2.p2.6.m5.1.1" xref="S3.SS2.p2.6.m5.1.1.cmml">
        <mi id="S3.SS2.p2.6.m5.1.1.2" xref="S3.SS2.p2.6.m5.1.1.2.cmml">
         X
        </mi>
        <mi id="S3.SS2.p2.6.m5.1.1.3" xref="S3.SS2.p2.6.m5.1.1.3.cmml">
         q
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m5.1b">
        <apply id="S3.SS2.p2.6.m5.1.1.cmml" xref="S3.SS2.p2.6.m5.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p2.6.m5.1.1.1.cmml" xref="S3.SS2.p2.6.m5.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p2.6.m5.1.1.2.cmml" xref="S3.SS2.p2.6.m5.1.1.2">
          𝑋
         </ci>
         <ci id="S3.SS2.p2.6.m5.1.1.3.cmml" xref="S3.SS2.p2.6.m5.1.1.3">
          𝑞
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p2.6.m5.1c">
        X_{q}
       </annotation>
      </semantics>
     </math>
     and the predicted object list
     <math alttext="X_{l}" class="ltx_Math" display="inline" id="S3.SS2.p2.7.m6.1">
      <semantics id="S3.SS2.p2.7.m6.1a">
       <msub id="S3.SS2.p2.7.m6.1.1" xref="S3.SS2.p2.7.m6.1.1.cmml">
        <mi id="S3.SS2.p2.7.m6.1.1.2" xref="S3.SS2.p2.7.m6.1.1.2.cmml">
         X
        </mi>
        <mi id="S3.SS2.p2.7.m6.1.1.3" xref="S3.SS2.p2.7.m6.1.1.3.cmml">
         l
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m6.1b">
        <apply id="S3.SS2.p2.7.m6.1.1.cmml" xref="S3.SS2.p2.7.m6.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p2.7.m6.1.1.1.cmml" xref="S3.SS2.p2.7.m6.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p2.7.m6.1.1.2.cmml" xref="S3.SS2.p2.7.m6.1.1.2">
          𝑋
         </ci>
         <ci id="S3.SS2.p2.7.m6.1.1.3.cmml" xref="S3.SS2.p2.7.m6.1.1.3">
          𝑙
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p2.7.m6.1c">
        X_{l}
       </annotation>
      </semantics>
     </math>
     are considered in our TaPA to generate the executable action plans
     <math alttext="X_{a}" class="ltx_Math" display="inline" id="S3.SS2.p2.8.m7.1">
      <semantics id="S3.SS2.p2.8.m7.1a">
       <msub id="S3.SS2.p2.8.m7.1.1" xref="S3.SS2.p2.8.m7.1.1.cmml">
        <mi id="S3.SS2.p2.8.m7.1.1.2" xref="S3.SS2.p2.8.m7.1.1.2.cmml">
         X
        </mi>
        <mi id="S3.SS2.p2.8.m7.1.1.3" xref="S3.SS2.p2.8.m7.1.1.3.cmml">
         a
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m7.1b">
        <apply id="S3.SS2.p2.8.m7.1.1.cmml" xref="S3.SS2.p2.8.m7.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p2.8.m7.1.1.1.cmml" xref="S3.SS2.p2.8.m7.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p2.8.m7.1.1.2.cmml" xref="S3.SS2.p2.8.m7.1.1.2">
          𝑋
         </ci>
         <ci id="S3.SS2.p2.8.m7.1.1.3.cmml" xref="S3.SS2.p2.8.m7.1.1.3">
          𝑎
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p2.8.m7.1c">
        X_{a}
       </annotation>
      </semantics>
     </math>
     :
    </p>
    <table class="ltx_equation ltx_eqn_table" id="S3.E3">
     <tbody>
      <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
       <td class="ltx_eqn_cell ltx_eqn_center_padleft">
       </td>
       <td class="ltx_eqn_cell ltx_align_center">
        <math alttext="X_{a}={\rm TaPA}(P_{in},\hat{X}_{l},X_{q})" class="ltx_Math" display="block" id="S3.E3.m1.3">
         <semantics id="S3.E3.m1.3a">
          <mrow id="S3.E3.m1.3.3" xref="S3.E3.m1.3.3.cmml">
           <msub id="S3.E3.m1.3.3.5" xref="S3.E3.m1.3.3.5.cmml">
            <mi id="S3.E3.m1.3.3.5.2" xref="S3.E3.m1.3.3.5.2.cmml">
             X
            </mi>
            <mi id="S3.E3.m1.3.3.5.3" xref="S3.E3.m1.3.3.5.3.cmml">
             a
            </mi>
           </msub>
           <mo id="S3.E3.m1.3.3.4" xref="S3.E3.m1.3.3.4.cmml">
            =
           </mo>
           <mrow id="S3.E3.m1.3.3.3" xref="S3.E3.m1.3.3.3.cmml">
            <mi id="S3.E3.m1.3.3.3.5" xref="S3.E3.m1.3.3.3.5.cmml">
             TaPA
            </mi>
            <mo id="S3.E3.m1.3.3.3.4" lspace="0em" rspace="0em" xref="S3.E3.m1.3.3.3.4.cmml">
             ​
            </mo>
            <mrow id="S3.E3.m1.3.3.3.3.3" xref="S3.E3.m1.3.3.3.3.4.cmml">
             <mo id="S3.E3.m1.3.3.3.3.3.4" stretchy="false" xref="S3.E3.m1.3.3.3.3.4.cmml">
              (
             </mo>
             <msub id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml">
              <mi id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml">
               P
              </mi>
              <mrow id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml">
               <mi id="S3.E3.m1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.3.2.cmml">
                i
               </mi>
               <mo id="S3.E3.m1.1.1.1.1.1.1.3.1" lspace="0em" rspace="0em" xref="S3.E3.m1.1.1.1.1.1.1.3.1.cmml">
                ​
               </mo>
               <mi id="S3.E3.m1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.3.3.cmml">
                n
               </mi>
              </mrow>
             </msub>
             <mo id="S3.E3.m1.3.3.3.3.3.5" xref="S3.E3.m1.3.3.3.3.4.cmml">
              ,
             </mo>
             <msub id="S3.E3.m1.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.2.cmml">
              <mover accent="true" id="S3.E3.m1.2.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.2.2.cmml">
               <mi id="S3.E3.m1.2.2.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.2.2.2.cmml">
                X
               </mi>
               <mo id="S3.E3.m1.2.2.2.2.2.2.2.1" xref="S3.E3.m1.2.2.2.2.2.2.2.1.cmml">
                ^
               </mo>
              </mover>
              <mi id="S3.E3.m1.2.2.2.2.2.2.3" xref="S3.E3.m1.2.2.2.2.2.2.3.cmml">
               l
              </mi>
             </msub>
             <mo id="S3.E3.m1.3.3.3.3.3.6" xref="S3.E3.m1.3.3.3.3.4.cmml">
              ,
             </mo>
             <msub id="S3.E3.m1.3.3.3.3.3.3" xref="S3.E3.m1.3.3.3.3.3.3.cmml">
              <mi id="S3.E3.m1.3.3.3.3.3.3.2" xref="S3.E3.m1.3.3.3.3.3.3.2.cmml">
               X
              </mi>
              <mi id="S3.E3.m1.3.3.3.3.3.3.3" xref="S3.E3.m1.3.3.3.3.3.3.3.cmml">
               q
              </mi>
             </msub>
             <mo id="S3.E3.m1.3.3.3.3.3.7" stretchy="false" xref="S3.E3.m1.3.3.3.3.4.cmml">
              )
             </mo>
            </mrow>
           </mrow>
          </mrow>
          <annotation-xml encoding="MathML-Content" id="S3.E3.m1.3b">
           <apply id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3">
            <eq id="S3.E3.m1.3.3.4.cmml" xref="S3.E3.m1.3.3.4">
            </eq>
            <apply id="S3.E3.m1.3.3.5.cmml" xref="S3.E3.m1.3.3.5">
             <csymbol cd="ambiguous" id="S3.E3.m1.3.3.5.1.cmml" xref="S3.E3.m1.3.3.5">
              subscript
             </csymbol>
             <ci id="S3.E3.m1.3.3.5.2.cmml" xref="S3.E3.m1.3.3.5.2">
              𝑋
             </ci>
             <ci id="S3.E3.m1.3.3.5.3.cmml" xref="S3.E3.m1.3.3.5.3">
              𝑎
             </ci>
            </apply>
            <apply id="S3.E3.m1.3.3.3.cmml" xref="S3.E3.m1.3.3.3">
             <times id="S3.E3.m1.3.3.3.4.cmml" xref="S3.E3.m1.3.3.3.4">
             </times>
             <ci id="S3.E3.m1.3.3.3.5.cmml" xref="S3.E3.m1.3.3.3.5">
              TaPA
             </ci>
             <vector id="S3.E3.m1.3.3.3.3.4.cmml" xref="S3.E3.m1.3.3.3.3.3">
              <apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1">
               <csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1">
                subscript
               </csymbol>
               <ci id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2">
                𝑃
               </ci>
               <apply id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3">
                <times id="S3.E3.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.1">
                </times>
                <ci id="S3.E3.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.2">
                 𝑖
                </ci>
                <ci id="S3.E3.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3.3">
                 𝑛
                </ci>
               </apply>
              </apply>
              <apply id="S3.E3.m1.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2">
               <csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2">
                subscript
               </csymbol>
               <apply id="S3.E3.m1.2.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2.2">
                <ci id="S3.E3.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2.2.1">
                 ^
                </ci>
                <ci id="S3.E3.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2.2.2">
                 𝑋
                </ci>
               </apply>
               <ci id="S3.E3.m1.2.2.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.2.2.3">
                𝑙
               </ci>
              </apply>
              <apply id="S3.E3.m1.3.3.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3.3.3">
               <csymbol cd="ambiguous" id="S3.E3.m1.3.3.3.3.3.3.1.cmml" xref="S3.E3.m1.3.3.3.3.3.3">
                subscript
               </csymbol>
               <ci id="S3.E3.m1.3.3.3.3.3.3.2.cmml" xref="S3.E3.m1.3.3.3.3.3.3.2">
                𝑋
               </ci>
               <ci id="S3.E3.m1.3.3.3.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3.3.3.3">
                𝑞
               </ci>
              </apply>
             </vector>
            </apply>
           </apply>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S3.E3.m1.3c">
           X_{a}={\rm TaPA}(P_{in},\hat{X}_{l},X_{q})
          </annotation>
         </semantics>
        </math>
       </td>
       <td class="ltx_eqn_cell ltx_eqn_center_padright">
       </td>
       <td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1">
        <span class="ltx_tag ltx_tag_equation ltx_align_right">
         (3)
        </span>
       </td>
      </tr>
     </tbody>
    </table>
    <p class="ltx_p" id="S3.SS2.p2.13">
     By combining the perception results of existed objects
     <math alttext="\hat{X}_{l}" class="ltx_Math" display="inline" id="S3.SS2.p2.9.m1.1">
      <semantics id="S3.SS2.p2.9.m1.1a">
       <msub id="S3.SS2.p2.9.m1.1.1" xref="S3.SS2.p2.9.m1.1.1.cmml">
        <mover accent="true" id="S3.SS2.p2.9.m1.1.1.2" xref="S3.SS2.p2.9.m1.1.1.2.cmml">
         <mi id="S3.SS2.p2.9.m1.1.1.2.2" xref="S3.SS2.p2.9.m1.1.1.2.2.cmml">
          X
         </mi>
         <mo id="S3.SS2.p2.9.m1.1.1.2.1" xref="S3.SS2.p2.9.m1.1.1.2.1.cmml">
          ^
         </mo>
        </mover>
        <mi id="S3.SS2.p2.9.m1.1.1.3" xref="S3.SS2.p2.9.m1.1.1.3.cmml">
         l
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m1.1b">
        <apply id="S3.SS2.p2.9.m1.1.1.cmml" xref="S3.SS2.p2.9.m1.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p2.9.m1.1.1.1.cmml" xref="S3.SS2.p2.9.m1.1.1">
          subscript
         </csymbol>
         <apply id="S3.SS2.p2.9.m1.1.1.2.cmml" xref="S3.SS2.p2.9.m1.1.1.2">
          <ci id="S3.SS2.p2.9.m1.1.1.2.1.cmml" xref="S3.SS2.p2.9.m1.1.1.2.1">
           ^
          </ci>
          <ci id="S3.SS2.p2.9.m1.1.1.2.2.cmml" xref="S3.SS2.p2.9.m1.1.1.2.2">
           𝑋
          </ci>
         </apply>
         <ci id="S3.SS2.p2.9.m1.1.1.3.cmml" xref="S3.SS2.p2.9.m1.1.1.3">
          𝑙
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p2.9.m1.1c">
        \hat{X}_{l}
       </annotation>
      </semantics>
     </math>
     with the instructions
     <math alttext="X_{q}" class="ltx_Math" display="inline" id="S3.SS2.p2.10.m2.1">
      <semantics id="S3.SS2.p2.10.m2.1a">
       <msub id="S3.SS2.p2.10.m2.1.1" xref="S3.SS2.p2.10.m2.1.1.cmml">
        <mi id="S3.SS2.p2.10.m2.1.1.2" xref="S3.SS2.p2.10.m2.1.1.2.cmml">
         X
        </mi>
        <mi id="S3.SS2.p2.10.m2.1.1.3" xref="S3.SS2.p2.10.m2.1.1.3.cmml">
         q
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p2.10.m2.1b">
        <apply id="S3.SS2.p2.10.m2.1.1.cmml" xref="S3.SS2.p2.10.m2.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p2.10.m2.1.1.1.cmml" xref="S3.SS2.p2.10.m2.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p2.10.m2.1.1.2.cmml" xref="S3.SS2.p2.10.m2.1.1.2">
          𝑋
         </ci>
         <ci id="S3.SS2.p2.10.m2.1.1.3.cmml" xref="S3.SS2.p2.10.m2.1.1.3">
          𝑞
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p2.10.m2.1c">
        X_{q}
       </annotation>
      </semantics>
     </math>
     , TaPA will give the executable action sequence
     <math alttext="X_{a}" class="ltx_Math" display="inline" id="S3.SS2.p2.11.m3.1">
      <semantics id="S3.SS2.p2.11.m3.1a">
       <msub id="S3.SS2.p2.11.m3.1.1" xref="S3.SS2.p2.11.m3.1.1.cmml">
        <mi id="S3.SS2.p2.11.m3.1.1.2" xref="S3.SS2.p2.11.m3.1.1.2.cmml">
         X
        </mi>
        <mi id="S3.SS2.p2.11.m3.1.1.3" xref="S3.SS2.p2.11.m3.1.1.3.cmml">
         a
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p2.11.m3.1b">
        <apply id="S3.SS2.p2.11.m3.1.1.cmml" xref="S3.SS2.p2.11.m3.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p2.11.m3.1.1.1.cmml" xref="S3.SS2.p2.11.m3.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p2.11.m3.1.1.2.cmml" xref="S3.SS2.p2.11.m3.1.1.2">
          𝑋
         </ci>
         <ci id="S3.SS2.p2.11.m3.1.1.3.cmml" xref="S3.SS2.p2.11.m3.1.1.3">
          𝑎
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p2.11.m3.1c">
        X_{a}
       </annotation>
      </semantics>
     </math>
     to complete the requirements of
     <math alttext="X_{q}" class="ltx_Math" display="inline" id="S3.SS2.p2.12.m4.1">
      <semantics id="S3.SS2.p2.12.m4.1a">
       <msub id="S3.SS2.p2.12.m4.1.1" xref="S3.SS2.p2.12.m4.1.1.cmml">
        <mi id="S3.SS2.p2.12.m4.1.1.2" xref="S3.SS2.p2.12.m4.1.1.2.cmml">
         X
        </mi>
        <mi id="S3.SS2.p2.12.m4.1.1.3" xref="S3.SS2.p2.12.m4.1.1.3.cmml">
         q
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p2.12.m4.1b">
        <apply id="S3.SS2.p2.12.m4.1.1.cmml" xref="S3.SS2.p2.12.m4.1.1">
         <csymbol cd="ambiguous" id="S3.SS2.p2.12.m4.1.1.1.cmml" xref="S3.SS2.p2.12.m4.1.1">
          subscript
         </csymbol>
         <ci id="S3.SS2.p2.12.m4.1.1.2.cmml" xref="S3.SS2.p2.12.m4.1.1.2">
          𝑋
         </ci>
         <ci id="S3.SS2.p2.12.m4.1.1.3.cmml" xref="S3.SS2.p2.12.m4.1.1.3">
          𝑞
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p2.12.m4.1c">
        X_{q}
       </annotation>
      </semantics>
     </math>
     according to the realistic scene constraint. According to our empirical study, we chose the block-wise center point for multi-view RGB image collection. The grid size in our location selection criteria is set to 0.75 and the unit angle for camera rotation is
     <math alttext="2\pi/3" class="ltx_Math" display="inline" id="S3.SS2.p2.13.m5.1">
      <semantics id="S3.SS2.p2.13.m5.1a">
       <mrow id="S3.SS2.p2.13.m5.1.1" xref="S3.SS2.p2.13.m5.1.1.cmml">
        <mrow id="S3.SS2.p2.13.m5.1.1.2" xref="S3.SS2.p2.13.m5.1.1.2.cmml">
         <mn id="S3.SS2.p2.13.m5.1.1.2.2" xref="S3.SS2.p2.13.m5.1.1.2.2.cmml">
          2
         </mn>
         <mo id="S3.SS2.p2.13.m5.1.1.2.1" lspace="0em" rspace="0em" xref="S3.SS2.p2.13.m5.1.1.2.1.cmml">
          ​
         </mo>
         <mi id="S3.SS2.p2.13.m5.1.1.2.3" xref="S3.SS2.p2.13.m5.1.1.2.3.cmml">
          π
         </mi>
        </mrow>
        <mo id="S3.SS2.p2.13.m5.1.1.1" xref="S3.SS2.p2.13.m5.1.1.1.cmml">
         /
        </mo>
        <mn id="S3.SS2.p2.13.m5.1.1.3" xref="S3.SS2.p2.13.m5.1.1.3.cmml">
         3
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p2.13.m5.1b">
        <apply id="S3.SS2.p2.13.m5.1.1.cmml" xref="S3.SS2.p2.13.m5.1.1">
         <divide id="S3.SS2.p2.13.m5.1.1.1.cmml" xref="S3.SS2.p2.13.m5.1.1.1">
         </divide>
         <apply id="S3.SS2.p2.13.m5.1.1.2.cmml" xref="S3.SS2.p2.13.m5.1.1.2">
          <times id="S3.SS2.p2.13.m5.1.1.2.1.cmml" xref="S3.SS2.p2.13.m5.1.1.2.1">
          </times>
          <cn id="S3.SS2.p2.13.m5.1.1.2.2.cmml" type="integer" xref="S3.SS2.p2.13.m5.1.1.2.2">
           2
          </cn>
          <ci id="S3.SS2.p2.13.m5.1.1.2.3.cmml" xref="S3.SS2.p2.13.m5.1.1.2.3">
           𝜋
          </ci>
         </apply>
         <cn id="S3.SS2.p2.13.m5.1.1.3.cmml" type="integer" xref="S3.SS2.p2.13.m5.1.1.3">
          3
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p2.13.m5.1c">
        2\pi/3
       </annotation>
      </semantics>
     </math>
     .
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Experiment
  </h2>
  <div class="ltx_para ltx_noindent" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    In this section, we conduct extensive experiments with our generated multimodal dataset where the visual scenes come from the simulator AI2-THOR. We first introduce the evaluation metric of the generated action plans. Then we compare our TaPA with the state-of-the-art LLMs and LMMs to show our superiority in embodied task planning. To further explore the effectiveness of different scene information embedding approaches, we evaluate various image collection strategies in our ablation study. We employ the LLaMA-7B pre-trained language model as the backbone of our task planner, which is finetuned with our generated multimodal dataset. The maximum token number of our task planner is set to 512, and we leverage the Detic open-vocabulary object detection framework to collect the information of existed objects. All experiments were accelerated by 8 GTX 3090 GPUs.
   </p>
  </div>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    Evaluation Metrics
   </h3>
   <figure class="ltx_table" id="S4.T2">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 2:
     </span>
     Comparison of different LLMs and LMMs on the task of embodied task planning. For the prompt of baseline methods, LLaMA and LLaVA both employ the same prompt in the their original finetuning phase, while GPT-3.5 adopts the same prompt of TaPA for multimodal data generation.
    </figcaption>
    <table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.1">
     <tr class="ltx_tr" id="S4.T2.1.1">
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1" style="padding-top:1pt;padding-bottom:1pt;">
       <span class="ltx_text ltx_inline-block" id="S4.T2.1.1.1.1" style="width:56.4pt;">
        Method
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.2" style="padding-top:1pt;padding-bottom:1pt;">
       <span class="ltx_text ltx_inline-block" id="S4.T2.1.1.2.1" style="width:56.4pt;">
        Kit.
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.3" style="padding-top:1pt;padding-bottom:1pt;">
       <span class="ltx_text ltx_inline-block" id="S4.T2.1.1.3.1" style="width:56.4pt;">
        Living.
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.4" style="padding-top:1pt;padding-bottom:1pt;">
       <span class="ltx_text ltx_inline-block" id="S4.T2.1.1.4.1" style="width:56.4pt;">
        Bed.
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.5" style="padding-top:1pt;padding-bottom:1pt;">
       <span class="ltx_text ltx_inline-block" id="S4.T2.1.1.5.1" style="width:56.4pt;">
        Bath.
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.6" style="padding-top:1pt;padding-bottom:1pt;">
       <span class="ltx_text ltx_inline-block" id="S4.T2.1.1.6.1" style="width:56.4pt;">
        Avg.
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T2.1.2">
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.1" style="padding-top:1pt;padding-bottom:1pt;">
       LLaVA
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.2" style="padding-top:1pt;padding-bottom:1pt;">
       14.29
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.3" style="padding-top:1pt;padding-bottom:1pt;">
       42.11
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.4" style="padding-top:1pt;padding-bottom:1pt;">
       33.33
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.5" style="padding-top:1pt;padding-bottom:1pt;">
       0.00
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.6" style="padding-top:1pt;padding-bottom:1pt;">
       22.43
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T2.1.3">
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.3.1" style="padding-top:1pt;padding-bottom:1pt;">
       GPT-3.5
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.3.2" style="padding-top:1pt;padding-bottom:1pt;">
       28.57
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.3.3" style="padding-top:1pt;padding-bottom:1pt;">
       73.68
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.3.4" style="padding-top:1pt;padding-bottom:1pt;">
       66.67
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.3.5" style="padding-top:1pt;padding-bottom:1pt;">
       50.00
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T2.1.3.6" style="padding-top:1pt;padding-bottom:1pt;">
       54.73
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T2.1.4">
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.4.1" style="padding-top:1pt;padding-bottom:1pt;">
       LLaMA
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.4.2" style="padding-top:1pt;padding-bottom:1pt;">
       0.00
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.4.3" style="padding-top:1pt;padding-bottom:1pt;">
       10.52
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.4.4" style="padding-top:1pt;padding-bottom:1pt;">
       13.33
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.4.5" style="padding-top:1pt;padding-bottom:1pt;">
       0.00
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T2.1.4.6" style="padding-top:1pt;padding-bottom:1pt;">
       5.96
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T2.1.5">
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.5.1" style="padding-top:1pt;padding-bottom:1pt;">
       TaPA
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.5.2" style="padding-top:1pt;padding-bottom:1pt;">
       28.57
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.5.3" style="padding-top:1pt;padding-bottom:1pt;">
       84.21
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.5.4" style="padding-top:1pt;padding-bottom:1pt;">
       73.33
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.5.5" style="padding-top:1pt;padding-bottom:1pt;">
       58.33
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.5.6" style="padding-top:1pt;padding-bottom:1pt;">
       61.11
      </td>
     </tr>
    </table>
   </figure>
   <div class="ltx_para ltx_noindent" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     For the deployment of our TaPA, we feed the instructions and the predicted object list in the scene for the task planner to generate the action steps. We hired 30 researchers in large multimodal models as volunteers to vote for the success of the generated action plans, and each generated action plan is evaluated by three volunteers. The volunteers are shown with the groundtruth object list of each scene, the instruction and the generated action plans, where the volunteers should judge whether implementing the action steps can successfully completes the instructions. There are two types failure cases including counterfactuals and hallucination. Counterfactuals indicate that the plans violate the physical rules in the real world (e.g. grasping the doorknob before moving to the door), and hallucination means the action plans require the agent to interact with objects that do not exist in the scene. An exceptional case is that the interacting objects can be part of the object existed in the scene (e.g. trash can lid and trash can) or a synonym of an object (e.g. mug and cup). The generated action plans are considered to be successful if at least two volunteers out of three regarding the steps can be implemented to satisfy the human instruction. The volunteers were also requested to annotate the type of failure for the unsuccessful cases. We report the ratio of successful cases for different scene types and plan generation models.
    </p>
   </div>
   <figure class="ltx_table" id="S4.T3">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 3:
     </span>
     The average execution success rate of generated action steps for different RGB image collection strategies in scene perception.
     <math alttext="G" class="ltx_Math" display="inline" id="S4.T3.4.m1.1">
      <semantics id="S4.T3.4.m1.1b">
       <mi id="S4.T3.4.m1.1.1" xref="S4.T3.4.m1.1.1.cmml">
        G
       </mi>
       <annotation-xml encoding="MathML-Content" id="S4.T3.4.m1.1c">
        <ci id="S4.T3.4.m1.1.1.cmml" xref="S4.T3.4.m1.1.1">
         𝐺
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.T3.4.m1.1d">
        G
       </annotation>
      </semantics>
     </math>
     represents the side length of grids in location selection.
     <math alttext="\Delta\theta" class="ltx_Math" display="inline" id="S4.T3.5.m2.1">
      <semantics id="S4.T3.5.m2.1b">
       <mrow id="S4.T3.5.m2.1.1" xref="S4.T3.5.m2.1.1.cmml">
        <mi id="S4.T3.5.m2.1.1.2" mathvariant="normal" xref="S4.T3.5.m2.1.1.2.cmml">
         Δ
        </mi>
        <mo id="S4.T3.5.m2.1.1.1" lspace="0em" rspace="0em" xref="S4.T3.5.m2.1.1.1.cmml">
         ​
        </mo>
        <mi id="S4.T3.5.m2.1.1.3" xref="S4.T3.5.m2.1.1.3.cmml">
         θ
        </mi>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.T3.5.m2.1c">
        <apply id="S4.T3.5.m2.1.1.cmml" xref="S4.T3.5.m2.1.1">
         <times id="S4.T3.5.m2.1.1.1.cmml" xref="S4.T3.5.m2.1.1.1">
         </times>
         <ci id="S4.T3.5.m2.1.1.2.cmml" xref="S4.T3.5.m2.1.1.2">
          Δ
         </ci>
         <ci id="S4.T3.5.m2.1.1.3.cmml" xref="S4.T3.5.m2.1.1.3">
          𝜃
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.T3.5.m2.1d">
        \Delta\theta
       </annotation>
      </semantics>
     </math>
     represents the unit angle of camera rotation.
     <math alttext="N" class="ltx_Math" display="inline" id="S4.T3.6.m3.1">
      <semantics id="S4.T3.6.m3.1b">
       <mi id="S4.T3.6.m3.1.1" xref="S4.T3.6.m3.1.1.cmml">
        N
       </mi>
       <annotation-xml encoding="MathML-Content" id="S4.T3.6.m3.1c">
        <ci id="S4.T3.6.m3.1.1.cmml" xref="S4.T3.6.m3.1.1">
         𝑁
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.T3.6.m3.1d">
        N
       </annotation>
      </semantics>
     </math>
     represents the ratio of randomly selected points compared to all grid points in achievable area.
    </figcaption>
    <table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.7">
     <tr class="ltx_tr" id="S4.T3.7.1">
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S4.T3.7.1.1" style="padding-top:1pt;padding-bottom:1pt;">
       Strategy and Parameters
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.1.2" style="padding-top:1pt;padding-bottom:1pt;">
       <span class="ltx_text ltx_inline-block" id="S4.T3.7.1.2.1" style="width:30.4pt;">
        #Images
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.1.3" style="padding-top:1pt;padding-bottom:1pt;">
       <span class="ltx_text ltx_inline-block" id="S4.T3.7.1.3.1" style="width:30.4pt;">
        Kit.
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.1.4" style="padding-top:1pt;padding-bottom:1pt;">
       <span class="ltx_text ltx_inline-block" id="S4.T3.7.1.4.1" style="width:30.4pt;">
        Living.
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.1.5" style="padding-top:1pt;padding-bottom:1pt;">
       <span class="ltx_text ltx_inline-block" id="S4.T3.7.1.5.1" style="width:30.4pt;">
        Bed.
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.1.6" style="padding-top:1pt;padding-bottom:1pt;">
       <span class="ltx_text ltx_inline-block" id="S4.T3.7.1.6.1" style="width:30.4pt;">
        Bath.
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.7" style="padding-top:1pt;padding-bottom:1pt;">
       <span class="ltx_text ltx_inline-block" id="S4.T3.7.1.7.1" style="width:30.4pt;">
        Avg.
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T3.7.2">
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.2.1" rowspan="4" style="padding-top:1pt;padding-bottom:1pt;">
       <span class="ltx_text" id="S4.T3.7.2.1.1">
        Traversal
       </span>
      </td>
      <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.7.2.2" style="padding-top:1pt;padding-bottom:1pt;">
       G=0.25, D=60
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.2.3" style="padding-top:1pt;padding-bottom:1pt;">
       782.4
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.2.4" style="padding-top:1pt;padding-bottom:1pt;">
       14.29
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.2.5" style="padding-top:1pt;padding-bottom:1pt;">
       73.68
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.2.6" style="padding-top:1pt;padding-bottom:1pt;">
       46.67
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.2.7" style="padding-top:1pt;padding-bottom:1pt;">
       33.33
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.2.8" style="padding-top:1pt;padding-bottom:1pt;">
       41.99
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T3.7.3">
      <td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.7.3.1" style="padding-top:1pt;padding-bottom:1pt;">
       G=0.25, D=120
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.3.2" style="padding-top:1pt;padding-bottom:1pt;">
       391.2
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.3.3" style="padding-top:1pt;padding-bottom:1pt;">
       14.29
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.3.4" style="padding-top:1pt;padding-bottom:1pt;">
       73.68
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.3.5" style="padding-top:1pt;padding-bottom:1pt;">
       53.33
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.3.6" style="padding-top:1pt;padding-bottom:1pt;">
       50.00
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T3.7.3.7" style="padding-top:1pt;padding-bottom:1pt;">
       47.83
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T3.7.4">
      <td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.7.4.1" style="padding-top:1pt;padding-bottom:1pt;">
       G=0.75, D=60
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.4.2" style="padding-top:1pt;padding-bottom:1pt;">
       80.7
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.4.3" style="padding-top:1pt;padding-bottom:1pt;">
       28.57
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.4.4" style="padding-top:1pt;padding-bottom:1pt;">
       73.68
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.4.5" style="padding-top:1pt;padding-bottom:1pt;">
       46.67
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.4.6" style="padding-top:1pt;padding-bottom:1pt;">
       33.33
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T3.7.4.7" style="padding-top:1pt;padding-bottom:1pt;">
       45.56
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T3.7.5">
      <td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.7.5.1" style="padding-top:1pt;padding-bottom:1pt;">
       G=0.75, D=120
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.5.2" style="padding-top:1pt;padding-bottom:1pt;">
       40.4
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.5.3" style="padding-top:1pt;padding-bottom:1pt;">
       14.29
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.5.4" style="padding-top:1pt;padding-bottom:1pt;">
       63.16
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.5.5" style="padding-top:1pt;padding-bottom:1pt;">
       60.00
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.5.6" style="padding-top:1pt;padding-bottom:1pt;">
       41.67
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T3.7.5.7" style="padding-top:1pt;padding-bottom:1pt;">
       44.78
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T3.7.6">
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.6.1" rowspan="4" style="padding-top:1pt;padding-bottom:1pt;">
       <span class="ltx_text" id="S4.T3.7.6.1.1">
        <span class="ltx_tabular ltx_align_middle" id="S4.T3.7.6.1.1.1">
         <span class="ltx_tr" id="S4.T3.7.6.1.1.1.1">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.7.6.1.1.1.1.1" style="padding-top:1pt;padding-bottom:1pt;">
           Random
          </span>
         </span>
         <span class="ltx_tr" id="S4.T3.7.6.1.1.1.2">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.7.6.1.1.1.2.1" style="padding-top:1pt;padding-bottom:1pt;">
           (G=0.75)
          </span>
         </span>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.7.6.2" style="padding-top:1pt;padding-bottom:1pt;">
       N=1%, D=60
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.6.3" style="padding-top:1pt;padding-bottom:1pt;">
       6.0
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.6.4" style="padding-top:1pt;padding-bottom:1pt;">
       28.57
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.6.5" style="padding-top:1pt;padding-bottom:1pt;">
       78.95
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.6.6" style="padding-top:1pt;padding-bottom:1pt;">
       26.67
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.6.7" style="padding-top:1pt;padding-bottom:1pt;">
       50.00
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.6.8" style="padding-top:1pt;padding-bottom:1pt;">
       46.05
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T3.7.7">
      <td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.7.7.1" style="padding-top:1pt;padding-bottom:1pt;">
       N=1%, D=120
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.7.2" style="padding-top:1pt;padding-bottom:1pt;">
       3.0
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.7.3" style="padding-top:1pt;padding-bottom:1pt;">
       21.43
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.7.4" style="padding-top:1pt;padding-bottom:1pt;">
       73.68
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.7.5" style="padding-top:1pt;padding-bottom:1pt;">
       46.67
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.7.6" style="padding-top:1pt;padding-bottom:1pt;">
       50.00
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T3.7.7.7" style="padding-top:1pt;padding-bottom:1pt;">
       47.95
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T3.7.8">
      <td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.7.8.1" style="padding-top:1pt;padding-bottom:1pt;">
       N=75%, D=60
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.8.2" style="padding-top:1pt;padding-bottom:1pt;">
       63.0
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.8.3" style="padding-top:1pt;padding-bottom:1pt;">
       35.71
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.8.4" style="padding-top:1pt;padding-bottom:1pt;">
       73.68
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.8.5" style="padding-top:1pt;padding-bottom:1pt;">
       53.33
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.8.6" style="padding-top:1pt;padding-bottom:1pt;">
       25.00
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T3.7.8.7" style="padding-top:1pt;padding-bottom:1pt;">
       46.93
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T3.7.9">
      <td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.7.9.1" style="padding-top:1pt;padding-bottom:1pt;">
       N=75%, D=120
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.9.2" style="padding-top:1pt;padding-bottom:1pt;">
       31.5
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.9.3" style="padding-top:1pt;padding-bottom:1pt;">
       28.57
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.9.4" style="padding-top:1pt;padding-bottom:1pt;">
       73.68
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.9.5" style="padding-top:1pt;padding-bottom:1pt;">
       53.33
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.9.6" style="padding-top:1pt;padding-bottom:1pt;">
       33.33
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T3.7.9.7" style="padding-top:1pt;padding-bottom:1pt;">
       47.23
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T3.7.10">
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.7.10.1" rowspan="2" style="padding-top:1pt;padding-bottom:1pt;">
       <span class="ltx_text" id="S4.T3.7.10.1.1">
        <span class="ltx_tabular ltx_align_middle" id="S4.T3.7.10.1.1.1">
         <span class="ltx_tr" id="S4.T3.7.10.1.1.1.1">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.7.10.1.1.1.1.1" style="padding-top:1pt;padding-bottom:1pt;">
           Layout Priori
          </span>
         </span>
         <span class="ltx_tr" id="S4.T3.7.10.1.1.1.2">
          <span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T3.7.10.1.1.1.2.1" style="padding-top:1pt;padding-bottom:1pt;">
           (G=0.75,D=60)
          </span>
         </span>
        </span>
       </span>
      </td>
      <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.7.10.2" style="padding-top:1pt;padding-bottom:1pt;">
       Overall Center
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.10.3" style="padding-top:1pt;padding-bottom:1pt;">
       6.0
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.10.4" style="padding-top:1pt;padding-bottom:1pt;">
       28.57
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.10.5" style="padding-top:1pt;padding-bottom:1pt;">
       68.42
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.10.6" style="padding-top:1pt;padding-bottom:1pt;">
       33.33
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.10.7" style="padding-top:1pt;padding-bottom:1pt;">
       58.33
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.10.8" style="padding-top:1pt;padding-bottom:1pt;">
       47.16
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T3.7.11">
      <td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S4.T3.7.11.1" style="padding-top:1pt;padding-bottom:1pt;">
       Partial Center
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T3.7.11.2" style="padding-top:1pt;padding-bottom:1pt;">
       23.1
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T3.7.11.3" style="padding-top:1pt;padding-bottom:1pt;">
       28.57
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T3.7.11.4" style="padding-top:1pt;padding-bottom:1pt;">
       84.21
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T3.7.11.5" style="padding-top:1pt;padding-bottom:1pt;">
       73.33
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T3.7.11.6" style="padding-top:1pt;padding-bottom:1pt;">
       58.33
      </td>
      <td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.7.11.7" style="padding-top:1pt;padding-bottom:1pt;">
       61.11
      </td>
     </tr>
    </table>
   </figure>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    Experimental Results
   </h3>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     In this section, we compare our TaPA method with the state-of-the-art LLMs including LLaMA and GPT-3.5 and LMMs containing LLaMA on 60 validation samples, and the success rate of the generated action steps from different methods are shown in Table
     <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4.1 Evaluation Metrics ‣ 4 Experiment ‣ Embodied Task Planning with Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     .
TaPA achieves optimal performance among all large models on all four scenes including kitchen, living room, bedroom and bathroom, and the average success rate of TaPA is 6.38% (61.11% vs. 54.73%) higher than GPT-3.5 on the task of embodied task planning after instruction finetuning.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p2">
    <p class="ltx_p" id="S4.SS2.p2.1">
     Since agents in kitchen scenes usually deal with complex cooking instructions in more steps, the performance of current large models is lower than in other room types. Meanwhile, the poor performance of LLaVA reflects the fact that the overall scene information cannot be represented by a single image in the visual question answering task, and the insufficient scene information leads to a low success rate of task planning. The success rate of LLaMA is far below other methods, which even cannot succeed in completing tasks in the kitchen and bathroom scenes without instruction finetuning. Figure
     <a class="ltx_ref" href="#S2.F2" title="Figure 2 ‣ 2 Related Work ‣ Embodied Task Planning with Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     illustrates the percentage of failure cases in embodied task planning for different large models. Counterfactuals represent that the generated actions violet the physical rule in the realistic world, and hallucinations mean the actions aim to interact with objects that are not in the scene. TaPA is embedded with more expert knowledge in embodied task planning after instruction finetuning, which has the lowest percentage of counterfactual occurrences. Moreover, TaPA can better understand the list of input objects, with a 26.7% (40.0% vs. 13.3%) and 5.0% (18.3% vs. 13.3%) decrease in the percentage of hallucination cases compared to LLaVA and GPT-3.5 respectively.
    </p>
   </div>
   <figure class="ltx_table" id="S4.T4">
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.1" style="width:433.6pt;height:446.5pt;vertical-align:-0.7pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-83.1pt,85.5pt) scale(0.722849464136579,0.722849464136579) ;">
      <table class="ltx_tabular ltx_align_top" id="S4.T4.1.1">
       <tr class="ltx_tr" id="S4.T4.1.1.2">
        <td class="ltx_td ltx_align_left ltx_border_tt" colspan="4" id="S4.T4.1.1.2.1" style="padding-top:1pt;padding-bottom:1pt;">
         <span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.1.1">
          Qualitative results, Living room:
         </span>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T4.1.1.1">
        <td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="S4.T4.1.1.1.1" style="padding-top:1pt;padding-bottom:1pt;">
         <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="204" id="S4.T4.1.1.1.1.g1" src="/html/2307.01848/assets/figs/figure_5_v5.png" width="706"/>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T4.1.1.3">
        <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.1.3.1" style="padding-top:1pt;padding-bottom:1pt;">
         User
        </td>
        <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.1.1.3.2" style="padding-top:1pt;padding-bottom:1pt;">
         I want to watch TV
        </td>
        <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.1.3.3" style="padding-top:1pt;padding-bottom:1pt;">
         User
        </td>
        <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.1.3.4" style="padding-top:1pt;padding-bottom:1pt;">
         I want to watch TV
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T4.1.1.4">
        <td class="ltx_td ltx_align_left" id="S4.T4.1.1.4.1" style="padding-top:1pt;padding-bottom:1pt;">
         TaPA
        </td>
        <td class="ltx_td ltx_align_left ltx_border_r" id="S4.T4.1.1.4.2" style="padding-top:1pt;padding-bottom:1pt;">
         <table class="ltx_tabular ltx_align_top" id="S4.T4.1.1.4.2.1">
          <tr class="ltx_tr" id="S4.T4.1.1.4.2.1.1">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.4.2.1.1.1" style="padding-top:1pt;padding-bottom:1pt;">
            Step 1. Move closer to the television set
           </td>
          </tr>
          <tr class="ltx_tr" id="S4.T4.1.1.4.2.1.2">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.4.2.1.2.1" style="padding-top:1pt;padding-bottom:1pt;">
            Step 2. Grasp the TV remote control
           </td>
          </tr>
          <tr class="ltx_tr" id="S4.T4.1.1.4.2.1.3">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.4.2.1.3.1" style="padding-top:1pt;padding-bottom:1pt;">
            Step 3. Turn on the TV
           </td>
          </tr>
          <tr class="ltx_tr" id="S4.T4.1.1.4.2.1.4">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.4.2.1.4.1" style="padding-top:1pt;padding-bottom:1pt;">
            Step 4. Adjust the volume to a suitable level
           </td>
          </tr>
          <tr class="ltx_tr" id="S4.T4.1.1.4.2.1.5">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.4.2.1.5.1" style="padding-top:1pt;padding-bottom:1pt;">
            Step 5. Select a TV show or movie
           </td>
          </tr>
          <tr class="ltx_tr" id="S4.T4.1.1.4.2.1.6">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.4.2.1.6.1" style="padding-top:1pt;padding-bottom:1pt;">
            Step 6. Watch TV
           </td>
          </tr>
         </table>
        </td>
        <td class="ltx_td ltx_align_left" id="S4.T4.1.1.4.3" style="padding-top:1pt;padding-bottom:1pt;">
         GPT-3.5
        </td>
        <td class="ltx_td ltx_align_left" id="S4.T4.1.1.4.4" style="padding-top:1pt;padding-bottom:1pt;">
         <table class="ltx_tabular ltx_align_top" id="S4.T4.1.1.4.4.1">
          <tr class="ltx_tr" id="S4.T4.1.1.4.4.1.1">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.4.4.1.1.1" style="padding-top:1pt;padding-bottom:1pt;">
            Step 1. Move closer to the TV
           </td>
          </tr>
          <tr class="ltx_tr" id="S4.T4.1.1.4.4.1.2">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.4.4.1.2.1" style="padding-top:1pt;padding-bottom:1pt;">
            Step 2. Grasp the remote control
           </td>
          </tr>
          <tr class="ltx_tr" id="S4.T4.1.1.4.4.1.3">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.4.4.1.3.1" style="padding-top:1pt;padding-bottom:1pt;">
            Step 3. Rotate the remote control to point at the TV
           </td>
          </tr>
          <tr class="ltx_tr" id="S4.T4.1.1.4.4.1.4">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.4.4.1.4.1" style="padding-top:1pt;padding-bottom:1pt;">
            Step 4. Press the power button to turn on the remote control
           </td>
          </tr>
         </table>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T4.1.1.5">
        <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.1.5.1" style="padding-top:1pt;padding-bottom:1pt;">
         User
        </td>
        <td class="ltx_td ltx_align_left ltx_border_t" colspan="3" id="S4.T4.1.1.5.2" style="padding-top:1pt;padding-bottom:1pt;">
         I want to watch TV
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T4.1.1.6">
        <td class="ltx_td ltx_align_left" id="S4.T4.1.1.6.1" style="padding-top:1pt;padding-bottom:1pt;">
         LLaVA
        </td>
        <td class="ltx_td ltx_align_left" colspan="3" id="S4.T4.1.1.6.2" style="padding-top:1pt;padding-bottom:1pt;">
         <table class="ltx_tabular ltx_align_top" id="S4.T4.1.1.6.2.1">
          <tr class="ltx_tr" id="S4.T4.1.1.6.2.1.1">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.6.2.1.1.1" style="padding-top:1pt;padding-bottom:1pt;">
            1.Locate the TV remote control, which is typically placed on a table or mounted on the wall near the TV.
           </td>
          </tr>
          <tr class="ltx_tr" id="S4.T4.1.1.6.2.1.2">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.6.2.1.2.1" style="padding-top:1pt;padding-bottom:1pt;">
            2.Point the remote control at the TV and press the power button. The power button is often located on
           </td>
          </tr>
          <tr class="ltx_tr" id="S4.T4.1.1.6.2.1.3">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.6.2.1.3.1" style="padding-top:1pt;padding-bottom:1pt;">
            the top or bottom of the remote control.
           </td>
          </tr>
          <tr class="ltx_tr" id="S4.T4.1.1.6.2.1.4">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.6.2.1.4.1" style="padding-top:1pt;padding-bottom:1pt;">
            3.If the TV doesn’t power on immediately, press the channel or input button to select the correct input source.
           </td>
          </tr>
          <tr class="ltx_tr" id="S4.T4.1.1.6.2.1.5">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.6.2.1.5.1" style="padding-top:1pt;padding-bottom:1pt;">
            The input button is usually located on the right side of the remote control or may be a menu button that allows
           </td>
          </tr>
          <tr class="ltx_tr" id="S4.T4.1.1.6.2.1.6">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.6.2.1.6.1" style="padding-top:1pt;padding-bottom:1pt;">
            you to access the settings.
           </td>
          </tr>
          <tr class="ltx_tr" id="S4.T4.1.1.6.2.1.7">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.6.2.1.7.1" style="padding-top:1pt;padding-bottom:1pt;">
            4.The TV should now turn on and display the channel or content you have selected.
           </td>
          </tr>
         </table>
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T4.1.1.7">
        <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.1.7.1" style="padding-top:1pt;padding-bottom:1pt;">
         User
        </td>
        <td class="ltx_td ltx_align_left ltx_border_t" colspan="3" id="S4.T4.1.1.7.2" style="padding-top:1pt;padding-bottom:1pt;">
         I want to watch TV
        </td>
       </tr>
       <tr class="ltx_tr" id="S4.T4.1.1.8">
        <td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.1.1.8.1" style="padding-top:1pt;padding-bottom:1pt;">
         LLaMA
        </td>
        <td class="ltx_td ltx_align_left ltx_border_bb" colspan="3" id="S4.T4.1.1.8.2" style="padding-top:1pt;padding-bottom:1pt;">
         <table class="ltx_tabular ltx_align_top" id="S4.T4.1.1.8.2.1">
          <tr class="ltx_tr" id="S4.T4.1.1.8.2.1.1">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.8.2.1.1.1" style="padding-top:1pt;padding-bottom:1pt;">
            Sit in a comfortable chair, place a pillow on the floor, and turn on the TV.
           </td>
          </tr>
          <tr class="ltx_tr" id="S4.T4.1.1.8.2.1.2">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.8.2.1.2.1" style="padding-top:1pt;padding-bottom:1pt;">
            Open up the drawer to get a remote control and place the TV on a table. Adjust the volume on the speakers.
           </td>
          </tr>
          <tr class="ltx_tr" id="S4.T4.1.1.8.2.1.3">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.8.2.1.3.1" style="padding-top:1pt;padding-bottom:1pt;">
            Locate the plug for the television and plug it in. Place the bowl of objects on the table next to the TV.
           </td>
          </tr>
          <tr class="ltx_tr" id="S4.T4.1.1.8.2.1.4">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.8.2.1.4.1" style="padding-top:1pt;padding-bottom:1pt;">
            (Objects: chair, sofa, pen, figurine, table, dog, lampshade, doorknob, bed, toy, drawer, person, statue_(sculpture),
           </td>
          </tr>
          <tr class="ltx_tr" id="S4.T4.1.1.8.2.1.5">
           <td class="ltx_td ltx_nopad_r ltx_align_left" id="S4.T4.1.1.8.2.1.5.1" style="padding-top:1pt;padding-bottom:1pt;">
            flowerpot, stool, monitor, (computer equipment), speaker, computer_monitor, ……)
           </td>
          </tr>
         </table>
        </td>
       </tr>
      </table>
     </span>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 4:
     </span>
     Visualization of generated action plans by different large models. The inputs of LLaMA and GPT-3.5 are the predicted object list by our visual perception module, while LLaVA generates the action steps only based on one single image.
    </figcaption>
   </figure>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p3">
    <p class="ltx_p" id="S4.SS2.p3.1">
     We also investigate the effectiveness of different image collection strategies that perceive the scene information by acquiring the list of existed objects. Specifically, we employ location selection criteria including random positions, traversal positions, the overall center point and block-wise center points with various hyperparameters containing the grid size and the sampling ratio in random positions, and we also change the unit angle for camera rotation. The success rate of different image collection strategies is demonstrated in Table
     <a class="ltx_ref" href="#S4.T3" title="Table 3 ‣ 4.1 Evaluation Metrics ‣ 4 Experiment ‣ Embodied Task Planning with Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     . We also show the number of collected images for various criteria to reveal the collection and computational cost. For the traversal positions, reducing the grid size significantly increases the image collection and the computational cost due to the numerous RGB images, while the average success rate remains similar (47.83 vs. 44.78) because the large grid size can collect images with sufficient information of the small-scale scenes from AI2-THOR. Similar reasons result in the phenomenon for random positions that increasing the sampling ratio and reducing the unit angle for camera rotation by collecting images in more locations cannot boost the success rate (47.95 vs. 47.23, 46.93 vs. 47.23). Since the traversal positions with small grid sizes (G=0.25) collects extremely large number of images, decreasing the unit angle for camera rotation significantly decreases the success rate because the redundant object list degrades the planning capacity of LLMs.
    </p>
   </div>
   <figure class="ltx_figure ltx_align_floatright" id="S4.F4">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="257" id="S4.F4.g1" src="/html/2307.01848/assets/figs/fail_case_v5.png" width="281"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 4:
     </span>
     The percentage of different failure cases in embodied task planning for various large models.
    </figcaption>
   </figure>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p4">
    <p class="ltx_p" id="S4.SS2.p4.1">
     Comparing all location selection criteria, block-wise center points achieve the highest success rate because of the effective representation of the existed objects in the scene. Block-wise center points observe the scene with the high coverage rate, while only a few RGB images are collected for scene representation. Therefore, sufficient scene information is captured by the acquired object list without redundancy. The performance of random positions and the overall center point is similar because the scale of scenes in AI2-THOR is small and one image collection location can also receive sufficient information. The traversal positions obtain the lowest success rate since collecting excess images lead to the higher probability of false positives in open-vocabulary object detection, which degrades the success rate because of the redundant object list.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p5">
    <p class="ltx_p" id="S4.SS2.p5.1">
     Among all room types, the success rate in the kitchen scenes is the lowest since the instruction for kitchen tasks (e.g. sandwich making) usually requires long plans with much more action steps. With the increase of the interacted objects in the task plan, the probability of hallucination is higher so that the plans are more likely to fail. On the contrary, the success rate of tasks in the living rooms is high due to the simple instructions (e.g. turning off lights). By observing the success rate of kitchen tasks across different location selection criteria, false positives in object detection that usually appear in traversal location selection criteria degrade the performance most significantly. Since the object list is redundant, the complex tasks in kitchen scenarios are more prone to the noise in the object list.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p6">
    <p class="ltx_p" id="S4.SS2.p6.1">
     We also show an example of generated action steps from different large models for the given scene in Table
     <a class="ltx_ref" href="#S4.T4" title="Table 4 ‣ 4.2 Experimental Results ‣ 4 Experiment ‣ Embodied Task Planning with Large Language Models">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     . The scene is demonstrated in the top-down view, and we also provide the groundtruth object list for reference. The content from LLaMA is irrelevant to the human instructions, while LLaVA provides plans that are not executable due to the non-existed objects. Although GPT-3.5 can also yield plausible embodied task plans, the action steps from our TaPA are more complete and more consistent with human values.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Conclusion
  </h2>
  <div class="ltx_para ltx_noindent" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    In this paper, we have presented a task planning agent called TaPA for embodied task planning, where the executable action steps are generated for subsequent robot navigation and manipulation to complete human instructions. We first construct a multimodal dataset where each sample is a triplet including the visual scenes, the instructions and the corresponding plans. The dataset is generated with GPT-3.5 by considering the list of all objects in the scene and the designed text prompt, which is leveraged to tune the instruction model to generate executable actions. For inference, we collect multi-view RGB images in different achievable locations, and leverage an open-vocabulary object detection framework to discover the object list of the scene for the finetuned instruction model. The statistics of our collected multimodal dataset indicate that our tasks are much more complex than conventional benchmarks on instruction-following tasks with longer implementation steps, and the extensive evaluation results show that our TaPA outperforms the state-of-the-art LLMs and LMMs on the plausibility of generated action plans.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al. [2023]
    </span>
    <span class="ltx_bibblock">
     J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg,
S. Rusinkiewicz, and T. Funkhouser.
    </span>
    <span class="ltx_bibblock">
     Tidybot: Personalized robot assistance with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">
      arXiv preprint arXiv:2305.05658
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. [2023]
    </span>
    <span class="ltx_bibblock">
     C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and
J. Gao.
    </span>
    <span class="ltx_bibblock">
     Llava-med: Training a large language-and-vision assistant for
biomedicine in one day.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">
      arXiv preprint arXiv:2306.00890
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhao et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Z. Zhao, S. Wang, J. Gu, Y. Zhu, L. Mei, Z. Zhuang, Z. Cui, Q. Wang, and
D. Shen.
    </span>
    <span class="ltx_bibblock">
     Chatcad+: Towards a universal and reliable interactive cad using
llms.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">
      arXiv preprint arXiv:2305.15964
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sun et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Y. Sun, C. Zhu, S. Zheng, K. Zhang, Z. Shui, X. Yu, Y. Zhao, H. Li, Y. Zhang,
R. Zhao, et al.
    </span>
    <span class="ltx_bibblock">
     Pathasst: Redefining pathology through generative foundation ai
assistant for pathology.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      arXiv preprint arXiv:2305.15072
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Vemprala et al. [2023]
    </span>
    <span class="ltx_bibblock">
     S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor.
    </span>
    <span class="ltx_bibblock">
     Chatgpt for robotics: Design principles and model abilities.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">
      Microsoft Auton. Syst. Robot. Res
     </em>
     , 2:20, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Stella et al. [2023]
    </span>
    <span class="ltx_bibblock">
     F. Stella, C. Della Santina, and J. Hughes.
    </span>
    <span class="ltx_bibblock">
     How can llms transform the robotic design process?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">
      Nature Machine Intelligence
     </em>
     , pages 1–4, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Touvron et al. [2023]
    </span>
    <span class="ltx_bibblock">
     H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al.
    </span>
    <span class="ltx_bibblock">
     Llama: Open and efficient foundation language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">
      arXiv preprint arXiv:2302.13971
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. [2023]
    </span>
    <span class="ltx_bibblock">
     R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li, P. Gao, and Y. Qiao.
    </span>
    <span class="ltx_bibblock">
     Llama-adapter: Efficient fine-tuning of language models with
zero-init attention.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">
      arXiv preprint arXiv:2303.16199
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Peng et al. [2023]
    </span>
    <span class="ltx_bibblock">
     B. Peng, C. Li, P. He, M. Galley, and J. Gao.
    </span>
    <span class="ltx_bibblock">
     Instruction tuning with gpt-4.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">
      arXiv preprint arXiv:2304.03277
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhu et al. [2023]
    </span>
    <span class="ltx_bibblock">
     D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny.
    </span>
    <span class="ltx_bibblock">
     Minigpt-4: Enhancing vision-language understanding with advanced
large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      arXiv preprint arXiv:2304.10592
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shridhar et al. [2022]
    </span>
    <span class="ltx_bibblock">
     M. Shridhar, L. Manuelli, and D. Fox.
    </span>
    <span class="ltx_bibblock">
     Cliport: What and where pathways for robotic manipulation.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">
      Conference on Robot Learning
     </em>
     , pages 894–906. PMLR, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nair et al. [2022]
    </span>
    <span class="ltx_bibblock">
     S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta.
    </span>
    <span class="ltx_bibblock">
     R3m: A universal visual representation for robot manipulation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">
      arXiv preprint arXiv:2203.12601
     </em>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jang et al. [2022]
    </span>
    <span class="ltx_bibblock">
     E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and
C. Finn.
    </span>
    <span class="ltx_bibblock">
     Bc-z: Zero-shot task generalization with robotic imitation learning.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">
      Conference on Robot Learning
     </em>
     , pages 991–1002. PMLR, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Brohan et al. [2023]
    </span>
    <span class="ltx_bibblock">
     A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho, J. Ibarz,
A. Irpan, E. Jang, R. Julian, et al.
    </span>
    <span class="ltx_bibblock">
     Do as i can, not as i say: Grounding language in robotic affordances.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">
      Conference on Robot Learning
     </em>
     , pages 287–318. PMLR, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Song et al. [2022]
    </span>
    <span class="ltx_bibblock">
     C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y. Su.
    </span>
    <span class="ltx_bibblock">
     Llm-planner: Few-shot grounded planning for embodied agents with
large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">
      arXiv preprint arXiv:2212.04088
     </em>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shridhar et al. [2020]
    </span>
    <span class="ltx_bibblock">
     M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi,
L. Zettlemoyer, and D. Fox.
    </span>
    <span class="ltx_bibblock">
     Alfred: A benchmark for interpreting grounded instructions for
everyday tasks.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">
      Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition
     </em>
     , pages 10740–10749, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. [2023]
    </span>
    <span class="ltx_bibblock">
     H. Liu, C. Li, Q. Wu, and Y. J. Lee.
    </span>
    <span class="ltx_bibblock">
     Visual instruction tuning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">
      arXiv preprint arXiv:2304.08485
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Brown et al. [2020]
    </span>
    <span class="ltx_bibblock">
     T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al.
    </span>
    <span class="ltx_bibblock">
     Language models are few-shot learners.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">
      Advances in neural information processing systems
     </em>
     ,
33:1877–1901, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kenton and Toutanova [2019]
    </span>
    <span class="ltx_bibblock">
     J. D. M.-W. C. Kenton and L. K. Toutanova.
    </span>
    <span class="ltx_bibblock">
     Bert: Pre-training of deep bidirectional transformers for language
understanding.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">
      Proceedings of naacL-HLT
     </em>
     , volume 1, page 2, 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Hu et al. [2021]
    </span>
    <span class="ltx_bibblock">
     E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
W. Chen.
    </span>
    <span class="ltx_bibblock">
     Lora: Low-rank adaptation of large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      arXiv preprint arXiv:2106.09685
     </em>
     , 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kirillov et al. [2023]
    </span>
    <span class="ltx_bibblock">
     A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao,
S. Whitehead, A. C. Berg, W.-Y. Lo, et al.
    </span>
    <span class="ltx_bibblock">
     Segment anything.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">
      arXiv preprint arXiv:2304.02643
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bangalath et al. [2022]
    </span>
    <span class="ltx_bibblock">
     H. Bangalath, M. Maaz, M. U. Khattak, S. H. Khan, and F. Shahbaz Khan.
    </span>
    <span class="ltx_bibblock">
     Bridging the gap between object and image-level representations for
open-vocabulary detection.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">
      Advances in Neural Information Processing Systems
     </em>
     ,
35:33781–33794, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhou et al. [2022]
    </span>
    <span class="ltx_bibblock">
     X. Zhou, R. Girdhar, A. Joulin, P. Krähenbühl, and I. Misra.
    </span>
    <span class="ltx_bibblock">
     Detecting twenty-thousand classes using image-level supervision.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">
      Computer Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part IX
     </em>
     , pages 350–368.
Springer, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Floridi and Chiriatti [2020]
    </span>
    <span class="ltx_bibblock">
     L. Floridi and M. Chiriatti.
    </span>
    <span class="ltx_bibblock">
     Gpt-3: Its nature, scope, limits, and consequences.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">
      Minds and Machines
     </em>
     , 30:681–694, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. [2023]
    </span>
    <span class="ltx_bibblock">
     J. Li, D. Li, S. Savarese, and S. Hoi.
    </span>
    <span class="ltx_bibblock">
     Blip-2: Bootstrapping language-image pre-training with frozen image
encoders and large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">
      arXiv preprint arXiv:2301.12597
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Sammani et al. [2022]
    </span>
    <span class="ltx_bibblock">
     F. Sammani, T. Mukherjee, and N. Deligiannis.
    </span>
    <span class="ltx_bibblock">
     Nlx-gpt: A model for natural language explanations in vision and
vision-language tasks.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">
      Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition
     </em>
     , pages 8322–8332, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bubeck et al. [2023]
    </span>
    <span class="ltx_bibblock">
     S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar,
P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al.
    </span>
    <span class="ltx_bibblock">
     Sparks of artificial general intelligence: Early experiments with
gpt-4.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">
      arXiv preprint arXiv:2303.12712
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schick et al. [2023]
    </span>
    <span class="ltx_bibblock">
     T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli,
L. Zettlemoyer, N. Cancedda, and T. Scialom.
    </span>
    <span class="ltx_bibblock">
     Toolformer: Language models can teach themselves to use tools.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">
      arXiv preprint arXiv:2302.04761
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zong and Krishnamachari [2022]
    </span>
    <span class="ltx_bibblock">
     M. Zong and B. Krishnamachari.
    </span>
    <span class="ltx_bibblock">
     Solving math word problems concerning systems of equations with
gpt-3.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">
      Proceedings of the Thirteenth AAAI Symposium on Educational
Advances in Artificial Intelligence
     </em>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zang et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Y. Zang, W. Li, J. Han, K. Zhou, and C. C. Loy.
    </span>
    <span class="ltx_bibblock">
     Contextual object detection with multimodal large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">
      arXiv preprint arXiv:2305.18279
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ghiasi et al. [2021]
    </span>
    <span class="ltx_bibblock">
     G. Ghiasi, X. Gu, Y. Cui, and T.-Y. Lin.
    </span>
    <span class="ltx_bibblock">
     Open-vocabulary image segmentation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">
      arXiv preprint arXiv:2112.12143
     </em>
     , 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Radford et al. [2021]
    </span>
    <span class="ltx_bibblock">
     A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
A. Askell, P. Mishkin, J. Clark, et al.
    </span>
    <span class="ltx_bibblock">
     Learning transferable visual models from natural language
supervision.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">
      International conference on machine learning
     </em>
     , pages
8748–8763. PMLR, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al. [2022]
    </span>
    <span class="ltx_bibblock">
     Z. Wu, Z. Wang, Z. Wei, Y. Wei, and H. Yan.
    </span>
    <span class="ltx_bibblock">
     Smart explorer: Recognizing objects in dense clutter via interactive
exploration.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">
      2022 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)
     </em>
     , pages 6600–6607. IEEE, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. [2022]
    </span>
    <span class="ltx_bibblock">
     Z. Liu, Z. Wang, S. Huang, J. Zhou, and J. Lu.
    </span>
    <span class="ltx_bibblock">
     Ge-grasp: Efficient target-oriented grasping in dense clutter.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">
      2022 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)
     </em>
     , pages 1388–1395. IEEE, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. [2023]
    </span>
    <span class="ltx_bibblock">
     X. Xu, Z. Sun, Z. Wang, H. Liu, J. Zhou, and J. Lu.
    </span>
    <span class="ltx_bibblock">
     Dspdet3d: Dynamic spatial pruning for 3d small object detection.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">
      arXiv preprint arXiv:2305.03716
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. [2022]
    </span>
    <span class="ltx_bibblock">
     X. Xu, Y. Wang, Y. Zheng, Y. Rao, J. Zhou, and J. Lu.
    </span>
    <span class="ltx_bibblock">
     Back to reality: Weakly-supervised 3d object detection with
shape-guided label enhancement.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">
      Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition
     </em>
     , pages 8438–8447, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Blukis et al. [2022]
    </span>
    <span class="ltx_bibblock">
     V. Blukis, C. Paxton, D. Fox, A. Garg, and Y. Artzi.
    </span>
    <span class="ltx_bibblock">
     A persistent spatial semantic representation for high-level natural
language instruction execution.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">
      Conference on Robot Learning
     </em>
     , pages 706–717. PMLR, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zellers et al. [2021]
    </span>
    <span class="ltx_bibblock">
     R. Zellers, A. Holtzman, M. Peters, R. Mottaghi, A. Kembhavi, A. Farhadi, and
Y. Choi.
    </span>
    <span class="ltx_bibblock">
     Piglet: Language grounding through neuro-symbolic interaction in a 3d
world.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">
      arXiv preprint arXiv:2106.00188
     </em>
     , 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Akakzia et al. [2020]
    </span>
    <span class="ltx_bibblock">
     A. Akakzia, C. Colas, P.-Y. Oudeyer, M. Chetouani, and O. Sigaud.
    </span>
    <span class="ltx_bibblock">
     Grounding language to autonomously-acquired skills via goal
generation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">
      arXiv preprint arXiv:2006.07185
     </em>
     , 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al. [2022]
    </span>
    <span class="ltx_bibblock">
     W. Huang, P. Abbeel, D. Pathak, and I. Mordatch.
    </span>
    <span class="ltx_bibblock">
     Language models as zero-shot planners: Extracting actionable
knowledge for embodied agents.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">
      International Conference on Machine Learning
     </em>
     , pages
9118–9147. PMLR, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. [2022]
    </span>
    <span class="ltx_bibblock">
     S. Li, X. Puig, C. Paxton, Y. Du, C. Wang, L. Fan, T. Chen, D.-A. Huang,
E. Akyürek, A. Anandkumar, et al.
    </span>
    <span class="ltx_bibblock">
     Pre-trained language models for interactive decision-making.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">
      Advances in Neural Information Processing Systems
     </em>
     ,
35:31199–31212, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Puig et al. [2018]
    </span>
    <span class="ltx_bibblock">
     X. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba.
    </span>
    <span class="ltx_bibblock">
     Virtualhome: Simulating household activities via programs.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">
      Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition
     </em>
     , pages 8494–8502, 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. [2023]
    </span>
    <span class="ltx_bibblock">
     B. Li, Y. Zhang, L. Chen, J. Wang, J. Yang, and Z. Liu.
    </span>
    <span class="ltx_bibblock">
     Otter: A multi-modal model with in-context instruction tuning.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">
      arXiv preprint arXiv:2305.03726
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib44">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lyu et al. [2023]
    </span>
    <span class="ltx_bibblock">
     C. Lyu, M. Wu, L. Wang, X. Huang, B. Liu, Z. Du, S. Shi, and Z. Tu.
    </span>
    <span class="ltx_bibblock">
     Macaw-llm: Multi-modal language modeling with image, audio, video,
and text integration.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">
      arXiv preprint arXiv:2306.09093
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib45">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ye et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi,
et al.
    </span>
    <span class="ltx_bibblock">
     mplug-owl: Modularization empowers large language models with
multimodality.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">
      arXiv preprint arXiv:2304.14178
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib46">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gao et al. [2023]
    </span>
    <span class="ltx_bibblock">
     D. Gao, L. Ji, L. Zhou, K. Q. Lin, J. Chen, Z. Fan, and M. Z. Shou.
    </span>
    <span class="ltx_bibblock">
     Assistgpt: A general multi-modal assistant that can plan, execute,
inspect, and learn.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">
      arXiv preprint arXiv:2306.08640
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib47">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhao et al. [2023]
    </span>
    <span class="ltx_bibblock">
     Z. Zhao, L. Guo, T. Yue, S. Chen, S. Shao, X. Zhu, Z. Yuan, and J. Liu.
    </span>
    <span class="ltx_bibblock">
     Chatbridge: Bridging modalities with large language model as a
language catalyst.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">
      arXiv preprint arXiv:2305.16103
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib48">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. [2023]
    </span>
    <span class="ltx_bibblock">
     F. Chen, M. Han, H. Zhao, Q. Zhang, J. Shi, S. Xu, and B. Xu.
    </span>
    <span class="ltx_bibblock">
     X-llm: Bootstrapping advanced large language models by treating
multi-modalities as foreign languages.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">
      arXiv preprint arXiv:2305.04160
     </em>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib49">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kolve et al. [2017]
    </span>
    <span class="ltx_bibblock">
     E. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti, M. Deitke,
K. Ehsani, D. Gordon, Y. Zhu, et al.
    </span>
    <span class="ltx_bibblock">
     Ai2-thor: An interactive 3d environment for visual ai.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">
      arXiv preprint arXiv:1712.05474
     </em>
     , 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib50">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jiang et al. [2020]
    </span>
    <span class="ltx_bibblock">
     X.-Y. Jiang, N.-Y. Pa, W.-C. Wang, T.-T. Yang, and W.-T. Pan.
    </span>
    <span class="ltx_bibblock">
     Site selection and layout of earthquake rescue center based on
k-means clustering and fruit fly optimization algorithm.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">
      2020 IEEE International Conference on Artificial
Intelligence and Computer Applications (ICAICA)
     </em>
     , pages 1381–1389. IEEE,
2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib51">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu [2012]
    </span>
    <span class="ltx_bibblock">
     X. Liu.
    </span>
    <span class="ltx_bibblock">
     The site selection of distribution center based on linear programming
transportation method.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">
      Proceedings of the 10th World Congress on Intelligent
Control and Automation
     </em>
     , pages 3538–3542. IEEE, 2012.
    </span>
   </li>
  </ul>
 </section>
 <div class="ltx_pagination ltx_role_newpage">
 </div>
 <section class="ltx_appendix" id="Ax1">
  <h2 class="ltx_title ltx_title_appendix">
   Supplementary Material
  </h2>
  <figure class="ltx_table" id="Ax1.T5">
   <div class="ltx_block ltx_minipage ltx_align_center ltx_align_middle" id="Ax1.T5.3" style="width:433.6pt;">
    <div class="ltx_para ltx_noindent" id="Ax1.T5.1.p1">
     <svg class="ltx_picture" height="1010.88" id="Ax1.T5.1.p1.pic1" overflow="visible" version="1.1" width="600">
      <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,1010.88) matrix(1 0 0 -1 0 0)">
       <g fill="#404040" fill-opacity="1.0">
        <path d="M 0 5.91 L 0 1004.97 C 0 1008.23 2.64 1010.88 5.91 1010.88 L 594.09 1010.88 C 597.36 1010.88 600 1008.23 600 1004.97 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none">
        </path>
       </g>
       <g fill="#F2F2F2" fill-opacity="1.0">
        <path d="M 1.97 5.91 L 1.97 1004.97 C 1.97 1007.15 3.73 1008.91 5.91 1008.91 L 594.09 1008.91 C 596.27 1008.91 598.03 1007.15 598.03 1004.97 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none">
        </path>
       </g>
       <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)">
        <foreignobject color="#000000" height="983.32" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
         <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3" style="width:402.3pt;">
          <span class="ltx_tabular ltx_centering ltx_align_middle" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3">
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.4">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.4.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.4.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.4.1.1.1" style="width:420.6pt;">
               <span class="ltx_text ltx_font_typewriter" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.4.1.1.1.1">
               </span>
               <span class="ltx_text ltx_font_bold" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.4.1.1.1.2">
                Rule description:
               </span>
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.4.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.5">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.5.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.5.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.5.1.1.1" style="width:420.6pt;">
               You are an indoor service robot named Garybot and you are inside a room. What you see is provided with a list of objects that contains all the objects in the room you are in. The location of the objects in the list you are guided in advance, without reasoning about the spatial relations of the objects. Execute all the instructions as you are located in the room.
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.5.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.6">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.6.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.6.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.6.1.1.1" style="width:420.6pt;">
               Design a conversation between you and the person you are serving in the room. The answer should be the tone of the service robot located in the room and performing the action specifically.
The generated instructions can be described in different tones. Ask for various instructions and give the corresponding series of actions with a maximum of 15 steps.
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.6.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.7">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.7.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.7.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.7.1.1.1" style="width:420.6pt;">
               Only include instructions for their corresponding actions only utilizing atomic motions (Grasp, Release, Lift, Place, Rotate, Push, Pull, Align, Press, Pour, Move):
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.7.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.8">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.8.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.8.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.8.1.1.1" style="width:420.6pt;">
               (1) Generate operation instructions using only the objects in the list with the actions that must be performed to complete the operating instructions;
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.8.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.9">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.9.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.9.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.9.1.1.1" style="width:420.6pt;">
               (2) Do not generate any instructions or actions that cannot be executed with confidence;
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.9.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.10">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.10.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.10.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.10.1.1.1" style="width:420.6pt;">
               (3) Do not generate any instructions or actions with (Target:
               <span class="ltx_text ltx_font_typewriter" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.10.1.1.1.1">
                [Object]
               </span>
               ),
               <span class="ltx_text ltx_font_typewriter" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.10.1.1.1.2">
                [Object]
               </span>
               is outside the list of objects.
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.10.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.11">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.11.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.11.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.11.1.1.1" style="width:420.6pt;">
               Again, the object being manipulated cannot be located outside the list.
Please double-check that Target:
               <span class="ltx_text ltx_font_typewriter" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.11.1.1.1.1">
                [Object]
               </span>
               is in the list at each step and that
               <span class="ltx_text ltx_font_typewriter" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.11.1.1.1.2">
                [Object]
               </span>
               is in the list.
When evaluating the existence of
               <span class="ltx_text ltx_font_typewriter" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.11.1.1.1.3">
                [Object]
               </span>
               , consider its original part or component, its function, and whether it can be replaced by an object in the list, and if it is satisfied, you can iterate over each element in the list to find an alternative and replace
               <span class="ltx_text ltx_font_typewriter" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.11.1.1.1.4">
                [Object]
               </span>
               .
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.11.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.12">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.12.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.12.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.12.1.1.1" style="width:420.6pt;">
               <span class="ltx_text ltx_font_typewriter" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.12.1.1.1.1">
               </span>
               <span class="ltx_text ltx_font_bold" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.12.1.1.1.2">
                Few-shot samples:
               </span>
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.12.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.13">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.13.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.13.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.13.1.1.1" style="width:420.6pt;">
               List of objects: [wine, cup, glass, remote control, TV, table, desk, chair]
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.13.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.14">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.14.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.14.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.14.1.1.1" style="width:420.6pt;">
               Generate the instruction: Give me a drink
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.14.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.15">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.15.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.15.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.15.1.1.1" style="width:420.6pt;">
               Necessary actions:
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.15.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.16">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.16.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.16.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.16.1.1.1" style="width:420.6pt;">
               Step 1. Grasp a bottle of wine (Target: wine)
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.16.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.17">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.17.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.17.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.17.1.1.1" style="width:420.6pt;">
               Step 2. Grasp a glass (Target: bowl)
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.17.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.18">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.18.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.18.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.18.1.1.1" style="width:420.6pt;">
               Step 3. Place the cup on the table (Target: glass, table)
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.18.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.19">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.19.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.19.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.19.1.1.1" style="width:420.6pt;">
               Step 4. Pour the wine into the glass (Target: wine, glass)
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.19.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.20">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.20.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.20.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.20.1.1.1" style="width:420.6pt;">
               Step 5. Grasp the glass with wine (Target: glass)
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.20.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.21">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.21.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.21.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.21.1.1.1" style="width:420.6pt;">
               Step 6. Move to the person and hand over it
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.21.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.22">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.22.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.22.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.22.1.1.1" style="width:420.6pt;">
               Step 7. Done
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.22.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.23">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.23.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.23.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.23.1.1.1" style="width:420.6pt;">
               Generate the instruction: Please turn on the TV
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.23.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.24">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.24.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.24.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.24.1.1.1" style="width:420.6pt;">
               Necessary actions:
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.24.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.25">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.25.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.25.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.25.1.1.1" style="width:420.6pt;">
               Step 1. Grasp the remote control (Target: remote control)
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.25.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.26">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.26.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.26.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.26.1.1.1" style="width:420.6pt;">
               Step 2. Move closer to the TV (Target: TV)
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.26.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.27">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.27.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.27.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.27.1.1.1" style="width:420.6pt;">
               Step 3. Rotate the remote control to point at the TV (Target: remote control, TV)
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.27.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.28">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.28.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.28.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.28.1.1.1" style="width:420.6pt;">
               Step 4. Press the power button to turn on the remote control (Target: remote control)
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.28.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.29">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.29.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.29.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.29.1.1.1" style="width:420.6pt;">
               Step 5. Done
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.29.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.30">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.30.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.30.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.30.1.1.1" style="width:420.6pt;">
               <span class="ltx_text ltx_font_typewriter" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.30.1.1.1.1">
               </span>
               <span class="ltx_text ltx_font_bold" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.30.1.1.1.2">
                Prompt for training and inference:
               </span>
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.30.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.31">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.31.1">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.31.1.1">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.31.1.1.1" style="width:420.6pt;">
               Below is an instruction that describes a task, paired with an input that provides further context.
Write a response that appropriately completes the request.
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.31.2">
            </span>
           </span>
           <span class="ltx_tr" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3">
            <span class="ltx_td ltx_align_justify ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3">
             <span class="ltx_inline-block ltx_align_top" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3">
              <span class="ltx_p" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3" style="width:420.6pt;">
               Instruction:
               <math alttext="X_{q}" class="ltx_Math" display="inline" id="Ax1.T5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1">
                <semantics id="Ax1.T5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a">
                 <msub id="Ax1.T5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="Ax1.T5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">
                  <mi id="Ax1.T5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="Ax1.T5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">
                   X
                  </mi>
                  <mi id="Ax1.T5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="Ax1.T5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">
                   q
                  </mi>
                 </msub>
                 <annotation-xml encoding="MathML-Content" id="Ax1.T5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b">
                  <apply id="Ax1.T5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="Ax1.T5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">
                   <csymbol cd="ambiguous" id="Ax1.T5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="Ax1.T5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">
                    subscript
                   </csymbol>
                   <ci id="Ax1.T5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="Ax1.T5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.2">
                    𝑋
                   </ci>
                   <ci id="Ax1.T5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="Ax1.T5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.3">
                    𝑞
                   </ci>
                  </apply>
                 </annotation-xml>
                 <annotation encoding="application/x-tex" id="Ax1.T5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">
                  X_{q}
                 </annotation>
                </semantics>
               </math>
               . Input:
               <math alttext="X_{l}" class="ltx_Math" display="inline" id="Ax1.T5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1">
                <semantics id="Ax1.T5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1a">
                 <msub id="Ax1.T5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1" xref="Ax1.T5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml">
                  <mi id="Ax1.T5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.2" xref="Ax1.T5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.2.cmml">
                   X
                  </mi>
                  <mi id="Ax1.T5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.3" xref="Ax1.T5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.3.cmml">
                   l
                  </mi>
                 </msub>
                 <annotation-xml encoding="MathML-Content" id="Ax1.T5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1b">
                  <apply id="Ax1.T5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml" xref="Ax1.T5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1">
                   <csymbol cd="ambiguous" id="Ax1.T5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.1.cmml" xref="Ax1.T5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1">
                    subscript
                   </csymbol>
                   <ci id="Ax1.T5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.2.cmml" xref="Ax1.T5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.2">
                    𝑋
                   </ci>
                   <ci id="Ax1.T5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.3.cmml" xref="Ax1.T5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.3">
                    𝑙
                   </ci>
                  </apply>
                 </annotation-xml>
                 <annotation encoding="application/x-tex" id="Ax1.T5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1c">
                  X_{l}
                 </annotation>
                </semantics>
               </math>
               . Response:
               <math alttext="X_{a}" class="ltx_Math" display="inline" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1">
                <semantics id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1a">
                 <msub id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1" xref="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1.cmml">
                  <mi id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1.2" xref="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1.2.cmml">
                   X
                  </mi>
                  <mi id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1.3" xref="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1.3.cmml">
                   a
                  </mi>
                 </msub>
                 <annotation-xml encoding="MathML-Content" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1b">
                  <apply id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1.cmml" xref="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1">
                   <csymbol cd="ambiguous" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1.1.cmml" xref="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1">
                    subscript
                   </csymbol>
                   <ci id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1.2.cmml" xref="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1.2">
                    𝑋
                   </ci>
                   <ci id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1.3.cmml" xref="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1.3">
                    𝑎
                   </ci>
                  </apply>
                 </annotation-xml>
                 <annotation encoding="application/x-tex" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1c">
                  X_{a}
                 </annotation>
                </semantics>
               </math>
               .
              </span>
             </span>
            </span>
            <span class="ltx_td" id="Ax1.T5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.4">
            </span>
           </span>
          </span>
         </span>
        </foreignobject>
       </g>
      </g>
     </svg>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_block">
      Table 5:
     </span>
     Our prompt for multimodal dataset generation (upper) and training/inference of TaPA (bottom).
     <math alttext="X_{a}" class="ltx_Math" display="inline" id="Ax1.T5.3.2.m1.1">
      <semantics id="Ax1.T5.3.2.m1.1a">
       <msub id="Ax1.T5.3.2.m1.1.1" xref="Ax1.T5.3.2.m1.1.1.cmml">
        <mi id="Ax1.T5.3.2.m1.1.1.2" mathcolor="#FF8000" xref="Ax1.T5.3.2.m1.1.1.2.cmml">
         X
        </mi>
        <mi id="Ax1.T5.3.2.m1.1.1.3" mathcolor="#FF8000" xref="Ax1.T5.3.2.m1.1.1.3.cmml">
         a
        </mi>
       </msub>
       <annotation-xml encoding="MathML-Content" id="Ax1.T5.3.2.m1.1b">
        <apply id="Ax1.T5.3.2.m1.1.1.cmml" xref="Ax1.T5.3.2.m1.1.1">
         <csymbol cd="ambiguous" id="Ax1.T5.3.2.m1.1.1.1.cmml" xref="Ax1.T5.3.2.m1.1.1">
          subscript
         </csymbol>
         <ci id="Ax1.T5.3.2.m1.1.1.2.cmml" xref="Ax1.T5.3.2.m1.1.1.2">
          𝑋
         </ci>
         <ci id="Ax1.T5.3.2.m1.1.1.3.cmml" xref="Ax1.T5.3.2.m1.1.1.3">
          𝑎
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="Ax1.T5.3.2.m1.1c">
        X_{a}
       </annotation>
      </semantics>
     </math>
     is empty unless the prompt serves as a ground-truth.
    </figcaption>
   </div>
  </figure>
  <div class="ltx_para ltx_noindent" id="Ax1.p1">
   <p class="ltx_p" id="Ax1.p1.1">
    The prompts utilized to generate the instruction-following dataset from GPT-3.5 are illustrated in Table
    <a class="ltx_ref" href="#Ax1.T5" title="Table 5 ‣ Supplementary Material ‣ Embodied Task Planning with Large Language Models">
     <span class="ltx_text ltx_ref_tag">
      5
     </span>
    </a>
    . Specifically, we set a specific work scene for GPT-3.5 and indicate the need to generate instructions and corresponding actions by the agent itself.
We also set the rules to constrain the instructions generated by GPT-3.5 with improved executability and confidence. Meanwhile, we require GPT-3.5 to add an additional (Target:
    <span class="ltx_text ltx_font_typewriter" id="Ax1.p1.1.1" style="color:#0000E6;">
     [Object]
    </span>
    ) query to each generated action to further check for hallucinations. If the interacting object in the generated plan is not in the input
    <math alttext="X_{l}" class="ltx_Math" display="inline" id="Ax1.p1.1.m1.1">
     <semantics id="Ax1.p1.1.m1.1a">
      <msub id="Ax1.p1.1.m1.1.1" xref="Ax1.p1.1.m1.1.1.cmml">
       <mi id="Ax1.p1.1.m1.1.1.2" xref="Ax1.p1.1.m1.1.1.2.cmml">
        X
       </mi>
       <mi id="Ax1.p1.1.m1.1.1.3" xref="Ax1.p1.1.m1.1.1.3.cmml">
        l
       </mi>
      </msub>
      <annotation-xml encoding="MathML-Content" id="Ax1.p1.1.m1.1b">
       <apply id="Ax1.p1.1.m1.1.1.cmml" xref="Ax1.p1.1.m1.1.1">
        <csymbol cd="ambiguous" id="Ax1.p1.1.m1.1.1.1.cmml" xref="Ax1.p1.1.m1.1.1">
         subscript
        </csymbol>
        <ci id="Ax1.p1.1.m1.1.1.2.cmml" xref="Ax1.p1.1.m1.1.1.2">
         𝑋
        </ci>
        <ci id="Ax1.p1.1.m1.1.1.3.cmml" xref="Ax1.p1.1.m1.1.1.3">
         𝑙
        </ci>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="Ax1.p1.1.m1.1c">
       X_{l}
      </annotation>
     </semantics>
    </math>
    , it is necessary to check if there is an alternative item to replace it.
An exceptional case is that the interacting objects can be part of the existing objects or a synonym of an object. We also provide some examples to standardize the form of the input and output for the generated data.
   </p>
  </div>
  <div class="ltx_pagination ltx_role_newpage">
  </div>
 </section>
 <div about="" class="ltx_rdf" content="Anonymous Submission" property="dcterms:creator">
 </div>
 <div about="" class="ltx_rdf" content="Embodied task planning, large language models, open-vocabulary detection" property="dcterms:subject">
 </div>
 <div about="" class="ltx_rdf" content="Proceedings of the 7th Conference on Robot Learning (CoRL 2023)" property="dcterms:subject">
 </div>
 <div about="" class="ltx_rdf" property="dcterms:title">
 </div>
</article>
