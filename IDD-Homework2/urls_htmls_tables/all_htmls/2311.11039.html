<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2311.11039] Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment</title><meta property="og:description" content="Synthetic data is being used lately for training deep neural networks in computer vision applications such as object detection, object segmentation and 6D object pose estimation. Domain randomization hereby plays an im…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2311.11039">

<!--Generated on Tue Feb 27 18:49:41 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\jyear</span>
<p id="p1.2" class="ltx_p">2023</p>
</div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p">[1]<span id="p2.1.1" class="ltx_ERROR undefined">\fnm</span>Parth <span id="p2.1.2" class="ltx_ERROR undefined">\sur</span>Rawal</p>
</div>
<div id="p3" class="ltx_para">
<p id="p3.1" class="ltx_p">1]<span id="p3.1.1" class="ltx_ERROR undefined">\orgname</span>Fraunhofer Institute for Manufacturing Technology and Advanced Materials (IFAM), <span id="p3.1.2" class="ltx_ERROR undefined">\orgaddress</span><span id="p3.1.3" class="ltx_ERROR undefined">\street</span>Ottenbecker Damm 12, <span id="p3.1.4" class="ltx_ERROR undefined">\postcode</span>21684 <span id="p3.1.5" class="ltx_ERROR undefined">\city</span>Stade, <span id="p3.1.6" class="ltx_ERROR undefined">\country</span>Germany</p>
</div>
<div id="p4" class="ltx_para">
<p id="p4.1" class="ltx_p">2]<span id="p4.1.1" class="ltx_ERROR undefined">\orgdiv</span>Institute of Production Management and Technology (IPMT), <span id="p4.1.2" class="ltx_ERROR undefined">\orgname</span>Hamburg University of Technology TUHH, <span id="p4.1.3" class="ltx_ERROR undefined">\orgaddress</span><span id="p4.1.4" class="ltx_ERROR undefined">\street</span>Denickestraße 15, <span id="p4.1.5" class="ltx_ERROR undefined">\postcode</span>21071 <span id="p4.1.6" class="ltx_ERROR undefined">\city</span>Hamburg, <span id="p4.1.7" class="ltx_ERROR undefined">\country</span>Germany</p>
</div>
<h1 class="ltx_title ltx_title_document">Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:parth.rawal@ifam.fraunhofer.de">parth.rawal@ifam.fraunhofer.de</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id1.1.id1" class="ltx_ERROR undefined">\fnm</span>Mrunal <span id="id2.2.id2" class="ltx_ERROR undefined">\sur</span>Sompura
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id3.1.id1" class="ltx_ERROR undefined">\fnm</span>Wolfgang <span id="id4.2.id2" class="ltx_ERROR undefined">\sur</span>Hintze
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Synthetic data is being used lately for training deep neural networks in computer vision applications such as object detection, object segmentation and 6D object pose estimation. Domain randomization hereby plays an important role in reducing the simulation to reality gap. However, this generalization might not be effective in specialized domains like a production environment involving complex assemblies. Either the individual parts, trained with synthetic images, are integrated in much larger assemblies making them indistinguishable from their counterparts and result in false positives or are partially occluded just enough to give rise to false negatives. Domain knowledge is vital in these cases and if conceived effectively while generating synthetic data, can show a considerable improvement in bridging the simulation to reality gap. This paper focuses on synthetic data generation procedures for parts and assemblies used in a production environment. The basic procedures for synthetic data generation and their various combinations are evaluated and compared on images captured in a production environment, where results show up to 15% improvement using combinations of basic procedures. Reducing the simulation to reality gap in this way can aid to utilize the true potential of robot assisted production using artificial intelligence.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>synthetic data, photorealistic rendering, production, sim2real gap, object detection
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The revolution in the computer graphic hardwares in the last decade has given birth to countless new Artificial Intelligence (AI) applications. Some of these applications are object detection, segmentation as well as pose estimation using deep learning. The time and effort needed for labelling data varies in different applications. While object classification only needs category labels for training, object detection and segmentation need bounding box and pixels coordinates respectively. In case of object pose estimation, scene data such as 6D pose of all the objects in the images is needed which makes annotating real images extremely challenging <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib1" title="" class="ltx_ref">Borrego.16.07.2018 </a>; <a href="#bib.bib2" title="" class="ltx_ref">Tremblay.18.04.2018 </a>; <a href="#bib.bib3" title="" class="ltx_ref">Mayershofer.2021 </a></cite>. Hence, for these applications, generating fast and easy synthetic data with annotations is indeed the preferred way in comparison to the time consuming and manual practice of labelling real data.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">As far as training with synthetic data is concerned, a number of successful approaches have been proposed and validated for datasets containing household objects with considerate amount of features. However, the components used in production are often textureless, reflective and colorless making them difficult to detect as compared to objects with varying features <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib4" title="" class="ltx_ref">C.A.Akar.2022 </a>; <a href="#bib.bib5" title="" class="ltx_ref">Moonen.2023 </a></cite>. Moreover, objects trained in a generic environment with domain randomization tend not to work so well for a specialized domain such as production environment <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib6" title="" class="ltx_ref">A.Prakash.2019 </a></cite>. For such scenarios, target domain knowledge is extremely essential to further bridge the simulation to reality gap <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib3" title="" class="ltx_ref">Mayershofer.2021 </a>; <a href="#bib.bib7" title="" class="ltx_ref">Eversberg.2021 </a></cite>. Particularly for the manufacturing industry, there are several challenges to be addressed while working with Convolution Neural Network (CNN). Complex assembly CAD (Computer Aided Design) models contain thousands of parts which have to be manually selected and exported as meshes for training. The individual parts after their integration into assemblies are often occluded and go undetected. Another problem is true detection of similar parts with slight differences. In addition to that, hundreds of unseen objects lying inside a production cell, pose a risk of getting detected as false positives.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">This paper presents a solution to the previously mentioned problems in production facilities. In the beginning, an approach is proposed and implemented to automatically segregate the target parts from their assembly CAD model and export them in the form of mesh, which is used later for generating data. In case of synthetic data generation, not all of the domain randomization aspects can be combined in a single scene. For example, it has to be decided whether a texture or a background image needs to be chosen for a scene. In these cases, the generated dataset is a combined one without prior knowledge of their individual efficacies in real environments <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib8" title="" class="ltx_ref">Tobin.2017 </a>; <a href="#bib.bib2" title="" class="ltx_ref">Tremblay.18.04.2018 </a>; <a href="#bib.bib9" title="" class="ltx_ref">Nowruzi.16.07.2019 </a></cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2311.11039/assets/fig_recipes.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="505" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Synthetic images generated with five different domain randomization procedures</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, the focus lies completely on various data generation procedures and photorealistic images are generated with the state of the art data generation tools. Five basic procedures with different levels of domain randomization and domain adaption are presented. Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows images from different basic procedures with different levels of domain randomization. The data generated from the basic procedures is then evaluated on real images inside a production cell. Based on the performance of the basic procedures, synthetic data is generated by combining basic procedures in different proportions and is thereafter evaluated. Some of the combinations boost up the performance by 15% inside a production cell. The proposed work is developed in the form of an automated pipeline being able to generate thousands of images within few hours with a desired combination of basic procedures. The generated data can be used in robot assisted manufacturing applications for object classification, detection, segmentation and 6D pose estimation tasks.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The following section provides an overview of related work in the area of synthetic data generation and domain randomization and their applications in industrial environments. The reusable procedure based data generation pipeline is described in detail in Section <a href="#S3" title="3 Synthetic Data Generation Pipeline ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The evaluation of the data generated by procedures and their combinations on real images captured in a production environment is presented in Section <a href="#S4" title="4 Validation ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Finally, the discussion and outlook are mentioned in Section <a href="#S5" title="5 Conclusion ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Synthetic data generation is a dominating field in computer vision research in the last five years. Over the years, numerous different ways of generating data are presented. Based on the state of the art methods, these can be broadly divided into four parts. Firstly, cut and paste or render and paste where patches of objects are cropped and pasted on randomly or at specific realistic locations on background images after scaling and transforming them <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib10" title="" class="ltx_ref">D.Dwibedi.2017 </a>; <a href="#bib.bib11" title="" class="ltx_ref">Georgakis.25.02.2017 </a></cite>. Data can be generated cheaply in this way, but they lack 3D scene information due to which they are not suitable for generating annotations for object pose estimation. Second approach uses physics engines or simulators, whose reviews are compiled by Collins et al. in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib12" title="" class="ltx_ref">Collins.2021 </a></cite>. The physics simulators are primarily used in the field of reinforcement learning due to a vast range of availability of sensors in the test environment. However, Borrego et al. in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib1" title="" class="ltx_ref">Borrego.16.07.2018 </a></cite> and Tobin et al. in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib8" title="" class="ltx_ref">Tobin.2017 </a></cite> point out a number of limitations using MuJoCo and Gazebo for generation of synthetic images. The third approach for generating quick domain randomized data uses game engines like Unreal Engine (UE4) and Unity3D and is depicted by Tremblay et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib2" title="" class="ltx_ref">Tremblay.18.04.2018 </a></cite> and To et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib13" title="" class="ltx_ref">To.2018 </a></cite>. Eventhough the game engines can render synthetic images reasonably well, they are tuned to provide realtime performance and lack dynamic characteristics of photorealism <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib14" title="" class="ltx_ref">Smid.2017 </a>; <a href="#bib.bib15" title="" class="ltx_ref">Morrical.28.05.2021 </a></cite>. Finally, the last method consists of physically based rendering (PBR), a technique which can achieve high level of photorealism. Cycles engine from Blender is an example of it. Mayershofer et al. in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib3" title="" class="ltx_ref">Mayershofer.2021 </a></cite> conclude that a network trained on images generated with physics based rendering (Cycles) outperforms the one that was trained on images rendered by a game engine (EEVEE).</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Normally, generation of synthetic data, owing to the unavailability of the application programming interface (API), can be a daunting process and is not scalable. However, a number of recently developed pipeline projects on rendering engines have enabled a completely scalable data generation process. NDDS <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib13" title="" class="ltx_ref">To.2018 </a></cite> project, based on Unreal Engine (UE4), was used to generate a dataset in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib16" title="" class="ltx_ref">J.Tremblay.2018 </a></cite>. BlenderProc <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref">Denninger.25.10.2019 </a></cite> procedural pipeline supporting scripts, based on the Cycles engine from Blender, also showed improved performance by their generated data <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib18" title="" class="ltx_ref">Denninger.2020 </a></cite>. A very similar project, NViSII <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib15" title="" class="ltx_ref">Morrical.28.05.2021 </a></cite> was demonstrated on NVIDIA’s OptiX ray tracing engine. A more recent pipeline Kubric <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib19" title="" class="ltx_ref">Greff.07.03.2022 </a></cite> used Blender and PyBullet to generate photorealistic data with ground truth.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The techniques of domain randomization and domain adaption have been widely used to bridge Sim2Real gap <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib1" title="" class="ltx_ref">Borrego.16.07.2018 </a>; <a href="#bib.bib20" title="" class="ltx_ref">Ren.10.03.2019 </a>; <a href="#bib.bib21" title="" class="ltx_ref">S.Hinterstoisser.2019 </a>; <a href="#bib.bib8" title="" class="ltx_ref">Tobin.2017 </a>; <a href="#bib.bib2" title="" class="ltx_ref">Tremblay.18.04.2018 </a></cite>. While domain randomization is very robust to varying environment, domain adaption helps to achieve a higher precision across domains. These researches show that domain randomization can be easily implemented for synthetic data thereby generalizing the trained model to the real world and improve its performance. Domain adaption, on the other hand, reduces the gap by increasing the resemblance between the two domains. This can be done by either using a combination of synthetic and real images <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib22" title="" class="ltx_ref">Y.Huangfu.2021 </a>; <a href="#bib.bib23" title="" class="ltx_ref">Baaz.09.12.2022 </a></cite>, or by using synthetic photorealistic images resembling to the target domain <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib24" title="" class="ltx_ref">J.Dummel.2021 </a></cite>, or by using generative adversarial networks (GAN) <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib25" title="" class="ltx_ref">Goodfellow.2014 </a></cite> based methods which can be used to transform generated synthetic images on the target domain <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib26" title="" class="ltx_ref">A.Shrivastava.2017 </a>; <a href="#bib.bib27" title="" class="ltx_ref">S.Sankaranarayanan.2018 </a>; <a href="#bib.bib28" title="" class="ltx_ref">X.Peng.2018 </a>; <a href="#bib.bib29" title="" class="ltx_ref">P.Rojtberg.2020 </a></cite>. However, domain adaption techniques, often being tricky, require greater manual effort compared to domain randomization as mentioned by Borrego et al. in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib1" title="" class="ltx_ref">Borrego.16.07.2018 </a></cite>. The use of photorealism together with domain randomization leads to higher confidence values and enables training models without freezing the backbone layers <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib30" title="" class="ltx_ref">T.Hodan.2019 </a>; <a href="#bib.bib18" title="" class="ltx_ref">Denninger.2020 </a></cite>. Some of the later researches have shown that combining domain randomization with target domain knowledge improves detection performance <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib6" title="" class="ltx_ref">A.Prakash.2019 </a>; <a href="#bib.bib3" title="" class="ltx_ref">Mayershofer.2021 </a>; <a href="#bib.bib7" title="" class="ltx_ref">Eversberg.2021 </a></cite>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">As far as reducing Sim2Real gap in a target domain is considered, almost all research papers use domain adaption techniques along with domain randomization. These have been presented for different applications in the fields of autonomous driving, logistics and industrial production <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib31" title="" class="ltx_ref">F.Reway.2020 </a>; <a href="#bib.bib22" title="" class="ltx_ref">Y.Huangfu.2021 </a>; <a href="#bib.bib3" title="" class="ltx_ref">Mayershofer.2021 </a>; <a href="#bib.bib32" title="" class="ltx_ref">Schoepflin.2022 </a>; <a href="#bib.bib33" title="" class="ltx_ref">Dummel.2021 </a>; <a href="#bib.bib7" title="" class="ltx_ref">Eversberg.2021 </a>; <a href="#bib.bib23" title="" class="ltx_ref">Baaz.09.12.2022 </a></cite>. Dümmel et al. in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib33" title="" class="ltx_ref">Dummel.2021 </a></cite> use Autodesk Inventor modelling software for data generation and clearly show a performance improvement for an assembly use case. This was achieved by loading the relevant CAD models directly for rendering images of individual parts as well as entire assemblies. However, the data generation software lacks physics simulation and uses Unity3D as a simulation engine to get realistic orientations of the parts. While Moonen et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib5" title="" class="ltx_ref">Moonen.2023 </a></cite> and Baaz et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib23" title="" class="ltx_ref">Baaz.09.12.2022 </a></cite> generate synthetic images based on rendering pipeline from Unity, Mayershofer et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib3" title="" class="ltx_ref">Mayershofer.2021 </a></cite>, Eversberg and Lambrecht <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib7" title="" class="ltx_ref">Eversberg.2021 </a></cite> and Andulkar et al. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib34" title="" class="ltx_ref">M.Andulkar.2018 </a></cite> use Blender API for data generation, making all the mentioned approaches scalable. <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib5" title="" class="ltx_ref">Moonen.2023 </a></cite> and <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib7" title="" class="ltx_ref">Eversberg.2021 </a></cite> also show improvements for industrial use cases. Out of all the target domain based studies, only two works <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib3" title="" class="ltx_ref">Mayershofer.2021 </a>; <a href="#bib.bib7" title="" class="ltx_ref">Eversberg.2021 </a></cite> show a comparison between data generated with different parameters and their effect on results. This paper generalizes the data generation methods using an approach of basic domain randomization procedures. Further, it compares the object detection performance for different combination of procedures in a production-related environment. Additionally, the paper also proposes a pipeline to handle complicated CAD models and export the parts and assembly models to mesh format with ease. The final outcome is an overall pipeline to generate synthetic data with a desired combination of procedures. This pipeline being already built on the scalable BlenderProc <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref">Denninger.25.10.2019 </a></cite> pipeline, makes the entire data generation process scalable for multiple applications.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Synthetic Data Generation Pipeline</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section introduces the scalable pipeline approach for synthetic data generation in detail and the demonstrator used later for validation of the generated images.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overall Concept</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">One of the attractive features of data generation is to have a reusable pipeline, where desired training data can be generated without additional effort or with minimum effort. For this reason, the current pipeline is built on the top of the scalable BlenderProc <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref">Denninger.25.10.2019 </a></cite> pipeline. A comparable framework, NViSII <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib15" title="" class="ltx_ref">Morrical.28.05.2021 </a></cite> can also be used as an alternative to BlenderProc pipeline. The goal here is to generate training data from CAD models without manual efforts, making the framework reusable across multiple industrial applications. The procedures for data generation are implemented within the framework and enable the user to generate desired training data within a few hours. The vision of the framework is to generate the training data only once, while using it for training of multiple AI applications such as object classification, detection, segmentation and pose estimation.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2311.11039/assets/x1.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Scalable pipeline concept from CAD model to trained model with interfaces inbetween</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The structure of the pipeline is depicted in Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.1 Overall Concept ‣ 3 Synthetic Data Generation Pipeline ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The pipeline can be divided into further three pipelines. Pipeline A reads the CAD model in the form of a STEP file and exports the parts of interest in mesh format for data generation. This is implemented using the free and open-source tool <em id="S3.SS1.p2.1.1" class="ltx_emph ltx_font_italic">FreeCAD</em><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://www.freecad.org/" title="" class="ltx_ref ltx_href">https://www.freecad.org/</a></span></span></span>. Pipeline B takes the mesh data and generates training data with predefined procedures using <em id="S3.SS1.p2.1.2" class="ltx_emph ltx_font_italic">BlenderProc</em><span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/DLR-RM/BlenderProc/" title="" class="ltx_ref ltx_href">https://github.com/DLR-RM/BlenderProc/</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref">Denninger.25.10.2019 </a></cite>. Finally, pipeline C is the neural network training pipeline, which is not within the scope of this paper. Pipeline A and pipeline B are presented in detail in Section <a href="#S3.SS2" title="3.2 Exporting Meshes ‣ 3 Synthetic Data Generation Pipeline ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> and in Section <a href="#S3.SS3" title="3.3 Procedure based Data Generation ‣ 3 Synthetic Data Generation Pipeline ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> respectively. This pipeline approach encompasses overall data generation process starting from CAD model till training data generation. Additional tools are also developed to aid the visualization of ground truth generated by the data generation pipeline.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">In order to validate the generated data in a real production related environment, a special demonstrator constructed for Skotty research project<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Skotty - Smart, collaborative, multitechnology production systems for coating aircraft components, Project timeline: 08.2021-12.2023 under grant ZW 1 80159842 is conducted with support of the Lower Saxony Ministry of Economic Affairs, Employment, Transport and Digitalization and the N-Bank</span></span></span> was used. The demonstrator consists of multiple parts which are simplified versions of components used in aviation industry in case of passenger aircrafts. The final application in Skotty research project is spray coating the components with wax.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2311.11039/assets/fig_demonstrator.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="768" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Demonstrator used for validation of synthetic data in real environment</figcaption>
</figure>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.1 Overall Concept ‣ 3 Synthetic Data Generation Pipeline ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows an image of the demonstrator used along with its various parts and sub-assemblies mounted on an industrial hand cart. The STEP model of the main assembly consists of altogether 148 components, comprising of various parts, sub-assemblies and the main assembly. Out of all components, only <em id="S3.SS1.p4.1.1" class="ltx_emph ltx_font_italic">ManholeBox</em> and <em id="S3.SS1.p4.1.2" class="ltx_emph ltx_font_italic">GeometricPlate</em> are chosen as categories for data generation whereas others are considered as passive objects in the scene. Both the components are asymmetric and contain some features helpful for object detection. While <em id="S3.SS1.p4.1.3" class="ltx_emph ltx_font_italic">ManholeBox</em> has distinctive features on all six sides, <em id="S3.SS1.p4.1.4" class="ltx_emph ltx_font_italic">GeometricPlate</em>, being a planar object, has significant features only on its top and bottom sides. This helps to validate the synthetic data for industrial parts having a planar geometry and their overall effect on the trained model.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Exporting Meshes</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">STEP file format is widely used during drafting components due to its accurate geometric representation. In the field of computer graphics, mesh formats like STL and PLY are however more popular, which are approximations of geometric models. Hence, conversion from STEP format to mesh formats is an essential step for data generation as mesh formats are widely used for photorealistic renderings.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Segregating part files from assembly CAD model containing thousands of parts can be cumbersome and labour intensive. This pipeline partly automates the mesh generation process from assembly CAD model using scripts. For this, the <em id="S3.SS2.p2.1.1" class="ltx_emph ltx_font_italic">FreeCAD</em> open-source tool is used which supports macros. The steps undertaken to export the target mesh files from a CAD model are depicted in Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.2 Exporting Meshes ‣ 3 Synthetic Data Generation Pipeline ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. At first, the assembly STEP model is loaded in <em id="S3.SS2.p2.1.2" class="ltx_emph ltx_font_italic">FreeCAD</em>. A macro exports all the part, sub-assembly and main assembly file names to a CSV file called here as <em id="S3.SS2.p2.1.3" class="ltx_emph ltx_font_italic">PartList.csv</em>. As the next step, the user manually assigns a category name to all the parts or sub-assemblies to be classified and is saved as <em id="S3.SS2.p2.1.4" class="ltx_emph ltx_font_italic">CategoryList.csv</em>. This step is not extensive as normally not all the components from the assembly CAD model are needed for training. In the final step, another macro automatically generates the mesh files for all the components mentioned in <em id="S3.SS2.p2.1.5" class="ltx_emph ltx_font_italic">CategoryList.csv</em> and places them in a local folder <em id="S3.SS2.p2.1.6" class="ltx_emph ltx_font_italic">Classes</em>. The components which are in <em id="S3.SS2.p2.1.7" class="ltx_emph ltx_font_italic">PartList.csv</em> but not <em id="S3.SS2.p2.1.8" class="ltx_emph ltx_font_italic">CategoryList.csv</em> are considered as passive objects and their meshes are exported in a local folder <em id="S3.SS2.p2.1.9" class="ltx_emph ltx_font_italic">Structure</em>. It is noteworthy that the meshes are exported in the local coordinate frame, in which the parts were drafted and not in the main assembly coordinate frame. In addition, the transformation of the individual components in the main assembly coordinate frame are exported in JSON format as text files in the folders <em id="S3.SS2.p2.1.10" class="ltx_emph ltx_font_italic">Classes</em> and <em id="S3.SS2.p2.1.11" class="ltx_emph ltx_font_italic">Structure</em>. These transformations are a part of target domain knowledge and can be later used to recreate the scene where the components can be placed with positions and orientations as in the main assembly model. Transformations are saved in millimeter and as Euler angles in Roll-Pitch-Yaw convention as shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.2 Exporting Meshes ‣ 3 Synthetic Data Generation Pipeline ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">The macro scripts used here are based on Python having file extension <em id="S3.SS2.p3.1.1" class="ltx_emph ltx_font_italic">FCMacro</em> and use <em id="S3.SS2.p3.1.2" class="ltx_emph ltx_font_italic">FreeCAD API</em> to extract useful information from the CAD models. The final outputs are the meshes and transformations in the folders <em id="S3.SS2.p3.1.3" class="ltx_emph ltx_font_italic">Classes</em> and <em id="S3.SS2.p3.1.4" class="ltx_emph ltx_font_italic">Structure</em>. The proposed pipeline is easy to use without much adaption and can export the necessary meshes within a few minutes.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2311.11039/assets/x2.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="467" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Multiple steps for exporting part meshes and their transformations in assembly origin</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Procedure based Data Generation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The creation of synthetic datasets involves artificial generation of data to mimic real world scenarios, which in turn requires the use of object meshes, photorealistic textures, real world backgrounds, and other elements for PBR. The resulting images can be either photorealistic or heavily randomized with random choice of colors. In order to establish a pipeline that can be used with different objects or combinations of rendering techniques, various procedures are proposed. These procedures can be imagined to be similar to cooking recipes. Just like the cooked dish may vary based on the cooking recipe followed, the synthetic images generated here also depend on the type of procedure used. These procedures include creating a 3D scene with textured planes or by using a background image with high dynamic range (HDR). Objects are either simulated to fall on the ground plane using physics simulation or are positioned randomly thereby floating in the 3D space. In context of image generation, objects on the ground plane exhibit a deterministic behavior in the ground truth, whereas randomly posed floating objects demonstrate a stochastic behavior. For instance, a circular plate shaped planar object on being dropped onto a surface is unlikely to assume a vertical or angled position after undergoing physics simulation, thereby reducing randomness in the dataset. However, if the same plate is floating in 3D space with a camera perspective such that only the flat edges are seen and not its distinctive features, high losses while training can be observed. Combining these approaches helps to overcome these limitations and reduce Sim2Real gap. Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Procedure based Data Generation ‣ 3 Synthetic Data Generation Pipeline ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides a list of all the procedures and their respective variations.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>List of procedures for synthetic data generation</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.1.1.1" class="ltx_p" style="width:65.0pt;">Procedures</span>
</span>
</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.2.1.1" class="ltx_p" style="width:325.2pt;">Variation</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.1.1.1" class="ltx_p" style="width:65.0pt;">Procedure1</span>
</span>
</td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.2.1.1" class="ltx_p" style="width:325.2pt;">Textured planes with objects on the ground</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.1.1.1" class="ltx_p" style="width:65.0pt;">Procedure2</span>
</span>
</td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="S3.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.2.1.1" class="ltx_p" style="width:325.2pt;">Textured planes with objects in 3D space</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.3.1.1.1" class="ltx_p" style="width:65.0pt;">Procedure3</span>
</span>
</td>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="S3.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.3.2.1.1" class="ltx_p" style="width:325.2pt;">HDRI background with objects on the ground</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<td id="S3.T1.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.4.1.1.1" class="ltx_p" style="width:65.0pt;">Procedure4</span>
</span>
</td>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="S3.T1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.4.2.1.1" class="ltx_p" style="width:325.2pt;">HDRI background with objects in 3D space</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<td id="S3.T1.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.5.1.1.1" class="ltx_p" style="width:65.0pt;">Procedure5</span>
</span>
</td>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top">
<span id="S3.T1.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.5.2.1.1" class="ltx_p" style="width:325.2pt;">HDRI background and target scene reconstruction</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.7.6" class="ltx_tr">
<td id="S3.T1.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.6.1.1.1" class="ltx_p" style="width:65.0pt;"><span id="S3.T1.1.7.6.1.1.1.1" class="ltx_ERROR undefined">\botrule</span></span>
</span>
</td>
<td id="S3.T1.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top"></td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.3 Procedure based Data Generation ‣ 3 Synthetic Data Generation Pipeline ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> provides a comprehensive explanation of the step by step data generation process for all procedures in a top-down approach. The configuration file stores various parameters such as the procedure(s) to be used for datasets, camera intrinsic matrix, camera resolution and simuation parameters. To generate the required number of images, each procedure has an outer loop that deals with the number of images required and an inner loop that handles the rendering process, including the addition of objects and distractors to the scene. Object transformations, such as scaling, sampling the pose, etc. are performed for each procedure. Procedure5 differs from all other procedures in terms of setting the object’s pose. Specifically in Procedure5, the object’s pose is determined using transformations exported to the JSON file in assembly origin, as outlined in Section <a href="#S3.SS2" title="3.2 Exporting Meshes ‣ 3 Synthetic Data Generation Pipeline ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>. On the other hand, for all the remaining procedures, the pose is sampled randomly after the object has been added to the scene. If any objects overlap, they are reset until the condition is satisfied. Procedure1 and Procedure3 use a physics-based approach, allowing the objects to fall down on the ground plane.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Various objects within the scene possess surface appearance properties that are randomized, including but not limited to base colour, roughness, specularity, and metalness. Additionally, the scene incorporates textured planes or backgrounds based on a predetermined procedure. The size of these planes can be customized as per requirement, and they are further enhanced with randomly generated textures sourced from <em id="S3.SS3.p3.1.1" class="ltx_emph ltx_font_italic">CC0Textures</em><span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://ambientcg.com/" title="" class="ltx_ref ltx_href">https://ambientcg.com/</a></span></span></span>. In case of procedures with backgrounds, only indoor HDRI backgrounds are used from <em id="S3.SS3.p3.1.2" class="ltx_emph ltx_font_italic">PolyHaven</em><span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://polyhaven.com/hdris" title="" class="ltx_ref ltx_href">https://polyhaven.com/hdris</a></span></span></span>. For Procedure3, an invisible plane has been added to the scene as a floor to enable working with physics simulation.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2311.11039/assets/x3.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="615" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Five basic data generation procedures explained in top-down approach</figcaption>
</figure>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">In order to achieve photorealistic results, random lighting conditions are introduced in the rendered scene. Three types of lights are employed: sun, point light source, and light plane. The intensity, colour, and position of each light source are randomized in accordance with the size of the textured plane. In each scene, one of these lights is randomly selected along with random parameters. Once the scene is prepared for image capture, the camera intrinsic matrix is defined. The camera position is sampled on a sphere around the mean location of the objects in the scene. The location of the camera is sampled on the sphere as per the defined radius and the number of image samples required in the inner loop.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">The images based on the sampling of camera locations are rendered in accordance with the defined camera intrinsic matrix. Some samples of these generated images are presented in Fig. <a href="#S3.F6" title="Figure 6 ‣ 3.3 Procedure based Data Generation ‣ 3 Synthetic Data Generation Pipeline ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. The RGB images are saved as arrays within an HDF5 container, along with depth images, class segmentation images, and instance segmentation images that contain information regarding the category ID and name of all objects in the image. This is done using the BlenderProc <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref">Denninger.25.10.2019 </a></cite> pipeline. The use of the HDF5 container is due to its compressibility, extensibility, and simplified I/O operations, which make data storage more manageable. Furthermore, a JSON file in COCO annotation format <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref">Lin.01.05.2014 </a></cite> containing information about the 2D bounding boxes for each category ID, is stored alongside the HDF5 files. In order to utilize the dataset for object pose estimation, 6D object poses are also saved in a JSON file. Finally, camera intrinsic matrix parameters are also stored for future usage. This generated dataset can be utilized in various applications that employ deep learning techniques.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2311.11039/assets/x4.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="605" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Synthetic images generated from basic procedures</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Ground Truth Visualization</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The generated dataset contains a diverse array of images featuring disparate textures and backgrounds. For each image, the pose of an object is stored in the camera coordinate system in the form of homogenous transformation matrix. The translation vector is saved in meters as [X, Y, Z], while the rotation matrix is a 3x3 matrix. This dataset conforms to the BOP <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib36" title="" class="ltx_ref">Hodan.24.08.2018 </a></cite> dataset format, which is commonly used for 6D pose estimation. As a result, existing deep learning models can be trained on the generated dataset with only a few minor modifications to the data loader.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2311.11039/assets/fig_visualization.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="296" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Visualization of annotations on images. Left image: Plotting 2D bounding boxes and category labels on image. Right image: Plotting 3D bounding boxes and object coordinate system on image</figcaption>
</figure>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">In order to verify the ground truth generated along with the synthetic images and for preventing any potential issues arising from category ID discrepancies, the saved ground truth data is visualized where it is superimposed onto the generated RGB images. In Fig. <a href="#S3.F7" title="Figure 7 ‣ 3.4 Ground Truth Visualization ‣ 3 Synthetic Data Generation Pipeline ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, both 2D and 3D bounding boxes are visualized and plotted on the images for all the present objects. The 2D bounding box information was extracted from the COCO annotations JSON file for each object in each image. For the 3D bounding boxes with origin, object mesh files in PLY format were loaded, and their dimensions were saved in separate JSON files containing information such as diameter, minimum dimensions, and size in the X, Y, and Z directions. Once the position, orientation, and 3D bounding box of an object are obtained, a transformation is applied from the camera coordinate system to the image coordinate system using the camera intrinsic matrix and the 3D bounding box is projected on the image as seen in the right image of Fig. <a href="#S3.F7" title="Figure 7 ‣ 3.4 Ground Truth Visualization ‣ 3 Synthetic Data Generation Pipeline ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Validation</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The datasets generated with all five basic procedures and their combinations were evaluated for object detection using YOLOv7 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib37" title="" class="ltx_ref">Wang.06.07.2022 </a></cite>. YOLOv7 implementation, with its evaluation scripts, enables quick and easy calculation of object detection metrics. For a fair comparison, all the configuration parameters and hyperparameters were kept unchanged. The normal YOLOv7 model mentioned in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib37" title="" class="ltx_ref">Wang.06.07.2022 </a></cite> was chosen and was trained for 100 epochs. Datasets with 15 000 images and 70/30 train test split ratio were generated for both the objects together. For combination datasets with a mixed procedures, the same proportion which was used while dataset generation, was maintained for train and test datasets. The time taken to generate a single dataset on a computer equipped with NVIDIA RTX A4000 took between 9 to 12 hours depending on the procedure(s) used. Altogether, five basic procedures and five combination sets of basic procedures were validated on 500 synthetic and 150 real images.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The synthetic images for validation were generated separately with each of the five basic procedures, while the real images were captured with a camera in a production environment and annotated later. The captured real images were taken in two scenarious: first, where both the objects were loose and second, where they were assembled in the demonstrator. For all the scenarios, COCO’s standard metric <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib35" title="" class="ltx_ref">Lin.01.05.2014 </a></cite> denoted as mAP@[0.5, 0.95] and PASCAL VOC’s metric <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib38" title="" class="ltx_ref">Everingham.2010 </a></cite> denoted as mAP@0.5 are calculated. However, only mAP@[0.5,0.95] results are presented here as they put a larger emphasis on the localization of bounding boxes <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib39" title="" class="ltx_ref">S.Bell.2017 </a></cite>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation of Basic Procedures</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Evaluation of Basic Procedures ‣ 4 Validation ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and Table <a href="#S4.T3" title="Table 3 ‣ 4.1 Evaluation of Basic Procedures ‣ 4 Validation ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> draw a performance comparison between basic procedures for <em id="S4.SS1.p1.1.1" class="ltx_emph ltx_font_italic">ManholeBox</em> and <em id="S4.SS1.p1.1.2" class="ltx_emph ltx_font_italic">GeometricPlate</em> respectively. Here, P is the short form for representing the procedures. It can be seen that all the diagonal elements of the 5x5 matrix have the best results across different synthetic validation sets. This behavior is expected as models trained on synthetic data using any single procedure have best results if it is validated on the synthetic dataset created with the same procedure. However, some procedures also perform better on validation datasets generated by other procedures. As an example, the average simulation values for P2 and P4 show that they perform overall better on all procedures.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figure id="S4.T2.1.fig1" class="ltx_figure ltx_minipage ltx_align_center ltx_align_middle" style="width:433.6pt;">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Table 2: </span>Performance comparison of basic procedures for <em id="S4.T2.1.fig1.2.1" class="ltx_emph ltx_font_italic">ManholeBox</em></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T2.1.fig1.3" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.fig1.3.1.1" class="ltx_tr">
<td id="S4.T2.1.fig1.3.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S4.T2.1.fig1.3.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.fig1.3.1.1.1.1.1" class="ltx_tr">
<td id="S4.T2.1.fig1.3.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><em id="S4.T2.1.fig1.3.1.1.1.1.1.1.1" class="ltx_emph ltx_font_italic">ManholeBox</em></td>
</tr>
<tr id="S4.T2.1.fig1.3.1.1.1.1.2" class="ltx_tr">
<td id="S4.T2.1.fig1.3.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">mAP@[0.5:0.95]</td>
</tr>
</table>
</td>
<td id="S4.T2.1.fig1.3.1.1.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" colspan="5">Trained on</td>
</tr>
<tr id="S4.T2.1.fig1.3.2.2" class="ltx_tr">
<td id="S4.T2.1.fig1.3.2.2.1" class="ltx_td ltx_align_center ltx_border_t">Validated on</td>
<td id="S4.T2.1.fig1.3.2.2.2" class="ltx_td ltx_align_center ltx_border_t">P1</td>
<td id="S4.T2.1.fig1.3.2.2.3" class="ltx_td ltx_align_center ltx_border_t">P2</td>
<td id="S4.T2.1.fig1.3.2.2.4" class="ltx_td ltx_align_center ltx_border_t">P3</td>
<td id="S4.T2.1.fig1.3.2.2.5" class="ltx_td ltx_align_center ltx_border_t">P4</td>
<td id="S4.T2.1.fig1.3.2.2.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">P5</td>
</tr>
<tr id="S4.T2.1.fig1.3.3.3" class="ltx_tr">
<td id="S4.T2.1.fig1.3.3.3.1" class="ltx_td ltx_align_center ltx_border_t">P1</td>
<td id="S4.T2.1.fig1.3.3.3.2" class="ltx_td ltx_align_center ltx_border_t">0.98</td>
<td id="S4.T2.1.fig1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t">0.99</td>
<td id="S4.T2.1.fig1.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t">0.98</td>
<td id="S4.T2.1.fig1.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t">0.99</td>
<td id="S4.T2.1.fig1.3.3.3.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.48</td>
</tr>
<tr id="S4.T2.1.fig1.3.4.4" class="ltx_tr">
<td id="S4.T2.1.fig1.3.4.4.1" class="ltx_td ltx_align_center">P2</td>
<td id="S4.T2.1.fig1.3.4.4.2" class="ltx_td ltx_align_center">0.75</td>
<td id="S4.T2.1.fig1.3.4.4.3" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T2.1.fig1.3.4.4.4" class="ltx_td ltx_align_center">0.83</td>
<td id="S4.T2.1.fig1.3.4.4.5" class="ltx_td ltx_align_center">0.98</td>
<td id="S4.T2.1.fig1.3.4.4.6" class="ltx_td ltx_nopad_r ltx_align_center">0.53</td>
</tr>
<tr id="S4.T2.1.fig1.3.5.5" class="ltx_tr">
<td id="S4.T2.1.fig1.3.5.5.1" class="ltx_td ltx_align_center">P3</td>
<td id="S4.T2.1.fig1.3.5.5.2" class="ltx_td ltx_align_center">0.98</td>
<td id="S4.T2.1.fig1.3.5.5.3" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T2.1.fig1.3.5.5.4" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T2.1.fig1.3.5.5.5" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T2.1.fig1.3.5.5.6" class="ltx_td ltx_nopad_r ltx_align_center">0.40</td>
</tr>
<tr id="S4.T2.1.fig1.3.6.6" class="ltx_tr">
<td id="S4.T2.1.fig1.3.6.6.1" class="ltx_td ltx_align_center">P4</td>
<td id="S4.T2.1.fig1.3.6.6.2" class="ltx_td ltx_align_center">0.75</td>
<td id="S4.T2.1.fig1.3.6.6.3" class="ltx_td ltx_align_center">0.98</td>
<td id="S4.T2.1.fig1.3.6.6.4" class="ltx_td ltx_align_center">0.89</td>
<td id="S4.T2.1.fig1.3.6.6.5" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T2.1.fig1.3.6.6.6" class="ltx_td ltx_nopad_r ltx_align_center">0.56</td>
</tr>
<tr id="S4.T2.1.fig1.3.7.7" class="ltx_tr">
<td id="S4.T2.1.fig1.3.7.7.1" class="ltx_td ltx_align_center">P5</td>
<td id="S4.T2.1.fig1.3.7.7.2" class="ltx_td ltx_align_center">0.43</td>
<td id="S4.T2.1.fig1.3.7.7.3" class="ltx_td ltx_align_center">0.91</td>
<td id="S4.T2.1.fig1.3.7.7.4" class="ltx_td ltx_align_center">0.69</td>
<td id="S4.T2.1.fig1.3.7.7.5" class="ltx_td ltx_align_center">0.90</td>
<td id="S4.T2.1.fig1.3.7.7.6" class="ltx_td ltx_nopad_r ltx_align_center">0.99</td>
</tr>
<tr id="S4.T2.1.fig1.3.8.8" class="ltx_tr">
<td id="S4.T2.1.fig1.3.8.8.1" class="ltx_td ltx_align_center ltx_border_t">Average Sim</td>
<td id="S4.T2.1.fig1.3.8.8.2" class="ltx_td ltx_align_center ltx_border_t">0.78</td>
<td id="S4.T2.1.fig1.3.8.8.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.fig1.3.8.8.3.1" class="ltx_text ltx_font_bold">0.97</span></td>
<td id="S4.T2.1.fig1.3.8.8.4" class="ltx_td ltx_align_center ltx_border_t">0.88</td>
<td id="S4.T2.1.fig1.3.8.8.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.fig1.3.8.8.5.1" class="ltx_text ltx_font_bold">0.97</span></td>
<td id="S4.T2.1.fig1.3.8.8.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.59</td>
</tr>
<tr id="S4.T2.1.fig1.3.9.9" class="ltx_tr">
<td id="S4.T2.1.fig1.3.9.9.1" class="ltx_td ltx_align_center ltx_border_t">Real parts loose</td>
<td id="S4.T2.1.fig1.3.9.9.2" class="ltx_td ltx_align_center ltx_border_t">0.25</td>
<td id="S4.T2.1.fig1.3.9.9.3" class="ltx_td ltx_align_center ltx_border_t">0.70</td>
<td id="S4.T2.1.fig1.3.9.9.4" class="ltx_td ltx_align_center ltx_border_t">0.35</td>
<td id="S4.T2.1.fig1.3.9.9.5" class="ltx_td ltx_align_center ltx_border_t">0.46</td>
<td id="S4.T2.1.fig1.3.9.9.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.31</td>
</tr>
<tr id="S4.T2.1.fig1.3.10.10" class="ltx_tr">
<td id="S4.T2.1.fig1.3.10.10.1" class="ltx_td ltx_align_center">Real assembly</td>
<td id="S4.T2.1.fig1.3.10.10.2" class="ltx_td ltx_align_center">0.00</td>
<td id="S4.T2.1.fig1.3.10.10.3" class="ltx_td ltx_align_center">0.51</td>
<td id="S4.T2.1.fig1.3.10.10.4" class="ltx_td ltx_align_center">0.00</td>
<td id="S4.T2.1.fig1.3.10.10.5" class="ltx_td ltx_align_center">0.58</td>
<td id="S4.T2.1.fig1.3.10.10.6" class="ltx_td ltx_nopad_r ltx_align_center">0.74</td>
</tr>
<tr id="S4.T2.1.fig1.3.11.11" class="ltx_tr">
<td id="S4.T2.1.fig1.3.11.11.1" class="ltx_td ltx_align_center ltx_border_t">Average Real</td>
<td id="S4.T2.1.fig1.3.11.11.2" class="ltx_td ltx_align_center ltx_border_t">0.12</td>
<td id="S4.T2.1.fig1.3.11.11.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.fig1.3.11.11.3.1" class="ltx_text ltx_font_bold">0.61</span></td>
<td id="S4.T2.1.fig1.3.11.11.4" class="ltx_td ltx_align_center ltx_border_t">0.17</td>
<td id="S4.T2.1.fig1.3.11.11.5" class="ltx_td ltx_align_center ltx_border_t">0.52</td>
<td id="S4.T2.1.fig1.3.11.11.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.53</td>
</tr>
<tr id="S4.T2.1.fig1.3.12.12" class="ltx_tr">
<td id="S4.T2.1.fig1.3.12.12.1" class="ltx_td ltx_align_center"><span id="S4.T2.1.fig1.3.12.12.1.1" class="ltx_ERROR undefined">\botrule</span></td>
<td id="S4.T2.1.fig1.3.12.12.2" class="ltx_td"></td>
<td id="S4.T2.1.fig1.3.12.12.3" class="ltx_td"></td>
<td id="S4.T2.1.fig1.3.12.12.4" class="ltx_td"></td>
<td id="S4.T2.1.fig1.3.12.12.5" class="ltx_td"></td>
<td id="S4.T2.1.fig1.3.12.12.6" class="ltx_td"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S4.I1" class="ltx_itemize ltx_figure_panel">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Note: Values in bold indicate the maximum average value across different trained models.</p>
</div>
</li>
</ul>
</div>
</div>
</figure>
</figure>
<figure id="S4.T3" class="ltx_table">
<figure id="S4.T3.1.fig1" class="ltx_figure ltx_minipage ltx_align_center ltx_align_middle" style="width:433.6pt;">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Table 3: </span>Performance comparison of basic procedures for <em id="S4.T3.1.fig1.2.1" class="ltx_emph ltx_font_italic">GeometricPlate</em></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T3.1.fig1.3" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.fig1.3.1.1" class="ltx_tr">
<td id="S4.T3.1.fig1.3.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S4.T3.1.fig1.3.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.fig1.3.1.1.1.1.1" class="ltx_tr">
<td id="S4.T3.1.fig1.3.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><em id="S4.T3.1.fig1.3.1.1.1.1.1.1.1" class="ltx_emph ltx_font_italic">GeometricPlate</em></td>
</tr>
<tr id="S4.T3.1.fig1.3.1.1.1.1.2" class="ltx_tr">
<td id="S4.T3.1.fig1.3.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">mAP@[0.5:0.95]</td>
</tr>
</table>
</td>
<td id="S4.T3.1.fig1.3.1.1.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" colspan="5">Trained on</td>
</tr>
<tr id="S4.T3.1.fig1.3.2.2" class="ltx_tr">
<td id="S4.T3.1.fig1.3.2.2.1" class="ltx_td ltx_align_center ltx_border_t">Validated on</td>
<td id="S4.T3.1.fig1.3.2.2.2" class="ltx_td ltx_align_center ltx_border_t">P1</td>
<td id="S4.T3.1.fig1.3.2.2.3" class="ltx_td ltx_align_center ltx_border_t">P2</td>
<td id="S4.T3.1.fig1.3.2.2.4" class="ltx_td ltx_align_center ltx_border_t">P3</td>
<td id="S4.T3.1.fig1.3.2.2.5" class="ltx_td ltx_align_center ltx_border_t">P4</td>
<td id="S4.T3.1.fig1.3.2.2.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">P5</td>
</tr>
<tr id="S4.T3.1.fig1.3.3.3" class="ltx_tr">
<td id="S4.T3.1.fig1.3.3.3.1" class="ltx_td ltx_align_center ltx_border_t">P1</td>
<td id="S4.T3.1.fig1.3.3.3.2" class="ltx_td ltx_align_center ltx_border_t">0.96</td>
<td id="S4.T3.1.fig1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t">0.98</td>
<td id="S4.T3.1.fig1.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t">0.97</td>
<td id="S4.T3.1.fig1.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t">0.98</td>
<td id="S4.T3.1.fig1.3.3.3.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.19</td>
</tr>
<tr id="S4.T3.1.fig1.3.4.4" class="ltx_tr">
<td id="S4.T3.1.fig1.3.4.4.1" class="ltx_td ltx_align_center">P2</td>
<td id="S4.T3.1.fig1.3.4.4.2" class="ltx_td ltx_align_center">0.52</td>
<td id="S4.T3.1.fig1.3.4.4.3" class="ltx_td ltx_align_center">0.94</td>
<td id="S4.T3.1.fig1.3.4.4.4" class="ltx_td ltx_align_center">0.51</td>
<td id="S4.T3.1.fig1.3.4.4.5" class="ltx_td ltx_align_center">0.92</td>
<td id="S4.T3.1.fig1.3.4.4.6" class="ltx_td ltx_nopad_r ltx_align_center">0.11</td>
</tr>
<tr id="S4.T3.1.fig1.3.5.5" class="ltx_tr">
<td id="S4.T3.1.fig1.3.5.5.1" class="ltx_td ltx_align_center">P3</td>
<td id="S4.T3.1.fig1.3.5.5.2" class="ltx_td ltx_align_center">0.96</td>
<td id="S4.T3.1.fig1.3.5.5.3" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T3.1.fig1.3.5.5.4" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T3.1.fig1.3.5.5.5" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T3.1.fig1.3.5.5.6" class="ltx_td ltx_nopad_r ltx_align_center">0.16</td>
</tr>
<tr id="S4.T3.1.fig1.3.6.6" class="ltx_tr">
<td id="S4.T3.1.fig1.3.6.6.1" class="ltx_td ltx_align_center">P4</td>
<td id="S4.T3.1.fig1.3.6.6.2" class="ltx_td ltx_align_center">0.58</td>
<td id="S4.T3.1.fig1.3.6.6.3" class="ltx_td ltx_align_center">0.95</td>
<td id="S4.T3.1.fig1.3.6.6.4" class="ltx_td ltx_align_center">0.60</td>
<td id="S4.T3.1.fig1.3.6.6.5" class="ltx_td ltx_align_center">0.96</td>
<td id="S4.T3.1.fig1.3.6.6.6" class="ltx_td ltx_nopad_r ltx_align_center">0.07</td>
</tr>
<tr id="S4.T3.1.fig1.3.7.7" class="ltx_tr">
<td id="S4.T3.1.fig1.3.7.7.1" class="ltx_td ltx_align_center">P5</td>
<td id="S4.T3.1.fig1.3.7.7.2" class="ltx_td ltx_align_center">0.37</td>
<td id="S4.T3.1.fig1.3.7.7.3" class="ltx_td ltx_align_center">0.73</td>
<td id="S4.T3.1.fig1.3.7.7.4" class="ltx_td ltx_align_center">0.35</td>
<td id="S4.T3.1.fig1.3.7.7.5" class="ltx_td ltx_align_center">0.66</td>
<td id="S4.T3.1.fig1.3.7.7.6" class="ltx_td ltx_nopad_r ltx_align_center">0.99</td>
</tr>
<tr id="S4.T3.1.fig1.3.8.8" class="ltx_tr">
<td id="S4.T3.1.fig1.3.8.8.1" class="ltx_td ltx_align_center ltx_border_t">Average Sim</td>
<td id="S4.T3.1.fig1.3.8.8.2" class="ltx_td ltx_align_center ltx_border_t">0.68</td>
<td id="S4.T3.1.fig1.3.8.8.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.fig1.3.8.8.3.1" class="ltx_text ltx_font_bold">0.92</span></td>
<td id="S4.T3.1.fig1.3.8.8.4" class="ltx_td ltx_align_center ltx_border_t">0.68</td>
<td id="S4.T3.1.fig1.3.8.8.5" class="ltx_td ltx_align_center ltx_border_t">0.90</td>
<td id="S4.T3.1.fig1.3.8.8.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.30</td>
</tr>
<tr id="S4.T3.1.fig1.3.9.9" class="ltx_tr">
<td id="S4.T3.1.fig1.3.9.9.1" class="ltx_td ltx_align_center ltx_border_t">Real parts loose</td>
<td id="S4.T3.1.fig1.3.9.9.2" class="ltx_td ltx_align_center ltx_border_t">0.70</td>
<td id="S4.T3.1.fig1.3.9.9.3" class="ltx_td ltx_align_center ltx_border_t">0.03</td>
<td id="S4.T3.1.fig1.3.9.9.4" class="ltx_td ltx_align_center ltx_border_t">0.34</td>
<td id="S4.T3.1.fig1.3.9.9.5" class="ltx_td ltx_align_center ltx_border_t">0.52</td>
<td id="S4.T3.1.fig1.3.9.9.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.00</td>
</tr>
<tr id="S4.T3.1.fig1.3.10.10" class="ltx_tr">
<td id="S4.T3.1.fig1.3.10.10.1" class="ltx_td ltx_align_center">Real assembly</td>
<td id="S4.T3.1.fig1.3.10.10.2" class="ltx_td ltx_align_center">0.72</td>
<td id="S4.T3.1.fig1.3.10.10.3" class="ltx_td ltx_align_center">0.70</td>
<td id="S4.T3.1.fig1.3.10.10.4" class="ltx_td ltx_align_center">0.30</td>
<td id="S4.T3.1.fig1.3.10.10.5" class="ltx_td ltx_align_center">0.86</td>
<td id="S4.T3.1.fig1.3.10.10.6" class="ltx_td ltx_nopad_r ltx_align_center">0.24</td>
</tr>
<tr id="S4.T3.1.fig1.3.11.11" class="ltx_tr">
<td id="S4.T3.1.fig1.3.11.11.1" class="ltx_td ltx_align_center ltx_border_t">Average Real</td>
<td id="S4.T3.1.fig1.3.11.11.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.fig1.3.11.11.2.1" class="ltx_text ltx_font_bold">0.71</span></td>
<td id="S4.T3.1.fig1.3.11.11.3" class="ltx_td ltx_align_center ltx_border_t">0.36</td>
<td id="S4.T3.1.fig1.3.11.11.4" class="ltx_td ltx_align_center ltx_border_t">0.32</td>
<td id="S4.T3.1.fig1.3.11.11.5" class="ltx_td ltx_align_center ltx_border_t">0.69</td>
<td id="S4.T3.1.fig1.3.11.11.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.12</td>
</tr>
<tr id="S4.T3.1.fig1.3.12.12" class="ltx_tr">
<td id="S4.T3.1.fig1.3.12.12.1" class="ltx_td ltx_align_center"><span id="S4.T3.1.fig1.3.12.12.1.1" class="ltx_ERROR undefined">\botrule</span></td>
<td id="S4.T3.1.fig1.3.12.12.2" class="ltx_td"></td>
<td id="S4.T3.1.fig1.3.12.12.3" class="ltx_td"></td>
<td id="S4.T3.1.fig1.3.12.12.4" class="ltx_td"></td>
<td id="S4.T3.1.fig1.3.12.12.5" class="ltx_td"></td>
<td id="S4.T3.1.fig1.3.12.12.6" class="ltx_td"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S4.I2" class="ltx_itemize ltx_figure_panel">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p">Note: Values in bold indicate the maximum average value across different trained models.</p>
</div>
</li>
</ul>
</div>
</div>
</figure>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The performance of the basic procedures on real dataset drops to a considerable amount which is due to simulation to reality gap. In case of <em id="S4.SS1.p2.1.1" class="ltx_emph ltx_font_italic">ManholeBox</em>, model trained on P2 performs the best on real datasets overall. P2 consists of object images with textured planes without simulating physics or letting the objects float in the space. In contrast to that, for <em id="S4.SS1.p2.1.2" class="ltx_emph ltx_font_italic">GeometricPlate</em>, model trained on P1 performs the best. P1 consists of object images with textured plate with physics simulation, meaning that the objects are lying on the ground plane.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">This behaviour too can be explained from the geometry of the objects. <em id="S4.SS1.p3.1.1" class="ltx_emph ltx_font_italic">ManholeBox</em> having geometry on all six sides can be detected easily when placed with random 3D poses in space. On the contrary, <em id="S4.SS1.p3.1.2" class="ltx_emph ltx_font_italic">GeometricPlate</em> being a planar object, has geometrical features only on two of the six sides. A random sampling of poses in space will not work the best as the features are not always visible from all perspectives. In this case, when the object falls on the ground plane, most of the features are always visible to the camera as the object has less probability to fall in a position which places itself into an unstable state. Letting objects fall on the ground plane resemble to a deterministic behavior whereas the objects lying in 3D space resemble to a stochastic behavior. The results show that for planar objects like <em id="S4.SS1.p3.1.3" class="ltx_emph ltx_font_italic">GeometricPlate</em>, synthetic images taken after letting them fall on the ground plane yield better results. For other objects, random sampling of poses as shown in P2 is helpful. Model trained on P4, which is rendering with a random background in 3D space, also shows good results for both objects. This is because backgrounds, even though not photorealistic, do contain high variance of features as compared to textures which are photorealistic.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Composition of combination datasets from basic procedures</figcaption>
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<td id="S4.T4.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T4.1.1.1.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" colspan="5">Basic procedures</td>
</tr>
<tr id="S4.T4.1.2.2" class="ltx_tr">
<td id="S4.T4.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">Combinations</td>
<td id="S4.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">P1</td>
<td id="S4.T4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">P2</td>
<td id="S4.T4.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">P3</td>
<td id="S4.T4.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">P4</td>
<td id="S4.T4.1.2.2.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">P5</td>
</tr>
<tr id="S4.T4.1.3.3" class="ltx_tr">
<td id="S4.T4.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t">C1</td>
<td id="S4.T4.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">20%</td>
<td id="S4.T4.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">20%</td>
<td id="S4.T4.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">10%</td>
<td id="S4.T4.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">30%</td>
<td id="S4.T4.1.3.3.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">20%</td>
</tr>
<tr id="S4.T4.1.4.4" class="ltx_tr">
<td id="S4.T4.1.4.4.1" class="ltx_td ltx_align_center">C2</td>
<td id="S4.T4.1.4.4.2" class="ltx_td ltx_align_center">40%</td>
<td id="S4.T4.1.4.4.3" class="ltx_td ltx_align_center">0%</td>
<td id="S4.T4.1.4.4.4" class="ltx_td ltx_align_center">0%</td>
<td id="S4.T4.1.4.4.5" class="ltx_td ltx_align_center">40%</td>
<td id="S4.T4.1.4.4.6" class="ltx_td ltx_nopad_r ltx_align_center">20%</td>
</tr>
<tr id="S4.T4.1.5.5" class="ltx_tr">
<td id="S4.T4.1.5.5.1" class="ltx_td ltx_align_center">C3</td>
<td id="S4.T4.1.5.5.2" class="ltx_td ltx_align_center">0%</td>
<td id="S4.T4.1.5.5.3" class="ltx_td ltx_align_center">40%</td>
<td id="S4.T4.1.5.5.4" class="ltx_td ltx_align_center">0%</td>
<td id="S4.T4.1.5.5.5" class="ltx_td ltx_align_center">40%</td>
<td id="S4.T4.1.5.5.6" class="ltx_td ltx_nopad_r ltx_align_center">20%</td>
</tr>
<tr id="S4.T4.1.6.6" class="ltx_tr">
<td id="S4.T4.1.6.6.1" class="ltx_td ltx_align_center">C4</td>
<td id="S4.T4.1.6.6.2" class="ltx_td ltx_align_center">0%</td>
<td id="S4.T4.1.6.6.3" class="ltx_td ltx_align_center">0%</td>
<td id="S4.T4.1.6.6.4" class="ltx_td ltx_align_center">0%</td>
<td id="S4.T4.1.6.6.5" class="ltx_td ltx_align_center">80%</td>
<td id="S4.T4.1.6.6.6" class="ltx_td ltx_nopad_r ltx_align_center">20%</td>
</tr>
<tr id="S4.T4.1.7.7" class="ltx_tr">
<td id="S4.T4.1.7.7.1" class="ltx_td ltx_align_center">C5</td>
<td id="S4.T4.1.7.7.2" class="ltx_td ltx_align_center">50%</td>
<td id="S4.T4.1.7.7.3" class="ltx_td ltx_align_center">0%</td>
<td id="S4.T4.1.7.7.4" class="ltx_td ltx_align_center">0%</td>
<td id="S4.T4.1.7.7.5" class="ltx_td ltx_align_center">50%</td>
<td id="S4.T4.1.7.7.6" class="ltx_td ltx_nopad_r ltx_align_center">0%</td>
</tr>
<tr id="S4.T4.1.8.8" class="ltx_tr">
<td id="S4.T4.1.8.8.1" class="ltx_td ltx_align_center"><span id="S4.T4.1.8.8.1.1" class="ltx_ERROR undefined">\botrule</span></td>
<td id="S4.T4.1.8.8.2" class="ltx_td"></td>
<td id="S4.T4.1.8.8.3" class="ltx_td"></td>
<td id="S4.T4.1.8.8.4" class="ltx_td"></td>
<td id="S4.T4.1.8.8.5" class="ltx_td"></td>
<td id="S4.T4.1.8.8.6" class="ltx_td"></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation of Combination Datasets</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Based on the performance of the basic procedures on the real datasets, five different combination datasets made up of different proportion of synthetic data from basic procedures were generated. Table <a href="#S4.T4" title="Table 4 ‣ 4.1 Evaluation of Basic Procedures ‣ 4 Validation ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the composition of different combination datasets represented from C1 till C5. These datasets were used for training and were validated on simulated and real images.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figure id="S4.T5.1.fig1" class="ltx_figure ltx_minipage ltx_align_center ltx_align_middle" style="width:433.6pt;">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Table 5: </span>Performance comparison of combination datasets for <em id="S4.T5.1.fig1.2.1" class="ltx_emph ltx_font_italic">ManholeBox</em></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T5.1.fig1.3" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T5.1.fig1.3.1.1" class="ltx_tr">
<td id="S4.T5.1.fig1.3.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S4.T5.1.fig1.3.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.fig1.3.1.1.1.1.1" class="ltx_tr">
<td id="S4.T5.1.fig1.3.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><em id="S4.T5.1.fig1.3.1.1.1.1.1.1.1" class="ltx_emph ltx_font_italic">ManholeBox</em></td>
</tr>
<tr id="S4.T5.1.fig1.3.1.1.1.1.2" class="ltx_tr">
<td id="S4.T5.1.fig1.3.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">mAP@[0.5:0.95]</td>
</tr>
</table>
</td>
<td id="S4.T5.1.fig1.3.1.1.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" colspan="5">Trained on</td>
</tr>
<tr id="S4.T5.1.fig1.3.2.2" class="ltx_tr">
<td id="S4.T5.1.fig1.3.2.2.1" class="ltx_td ltx_align_center ltx_border_t">Validated on</td>
<td id="S4.T5.1.fig1.3.2.2.2" class="ltx_td ltx_align_center ltx_border_t">C1</td>
<td id="S4.T5.1.fig1.3.2.2.3" class="ltx_td ltx_align_center ltx_border_t">C2</td>
<td id="S4.T5.1.fig1.3.2.2.4" class="ltx_td ltx_align_center ltx_border_t">C3</td>
<td id="S4.T5.1.fig1.3.2.2.5" class="ltx_td ltx_align_center ltx_border_t">C4</td>
<td id="S4.T5.1.fig1.3.2.2.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">C5</td>
</tr>
<tr id="S4.T5.1.fig1.3.3.3" class="ltx_tr">
<td id="S4.T5.1.fig1.3.3.3.1" class="ltx_td ltx_align_center ltx_border_t">P1</td>
<td id="S4.T5.1.fig1.3.3.3.2" class="ltx_td ltx_align_center ltx_border_t">0.99</td>
<td id="S4.T5.1.fig1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t">0.99</td>
<td id="S4.T5.1.fig1.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t">0.99</td>
<td id="S4.T5.1.fig1.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t">0.99</td>
<td id="S4.T5.1.fig1.3.3.3.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.98</td>
</tr>
<tr id="S4.T5.1.fig1.3.4.4" class="ltx_tr">
<td id="S4.T5.1.fig1.3.4.4.1" class="ltx_td ltx_align_center">P2</td>
<td id="S4.T5.1.fig1.3.4.4.2" class="ltx_td ltx_align_center">0.98</td>
<td id="S4.T5.1.fig1.3.4.4.3" class="ltx_td ltx_align_center">0.98</td>
<td id="S4.T5.1.fig1.3.4.4.4" class="ltx_td ltx_align_center">0.98</td>
<td id="S4.T5.1.fig1.3.4.4.5" class="ltx_td ltx_align_center">0.98</td>
<td id="S4.T5.1.fig1.3.4.4.6" class="ltx_td ltx_nopad_r ltx_align_center">0.96</td>
</tr>
<tr id="S4.T5.1.fig1.3.5.5" class="ltx_tr">
<td id="S4.T5.1.fig1.3.5.5.1" class="ltx_td ltx_align_center">P3</td>
<td id="S4.T5.1.fig1.3.5.5.2" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T5.1.fig1.3.5.5.3" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T5.1.fig1.3.5.5.4" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T5.1.fig1.3.5.5.5" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T5.1.fig1.3.5.5.6" class="ltx_td ltx_nopad_r ltx_align_center">0.99</td>
</tr>
<tr id="S4.T5.1.fig1.3.6.6" class="ltx_tr">
<td id="S4.T5.1.fig1.3.6.6.1" class="ltx_td ltx_align_center">P4</td>
<td id="S4.T5.1.fig1.3.6.6.2" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T5.1.fig1.3.6.6.3" class="ltx_td ltx_align_center">0.98</td>
<td id="S4.T5.1.fig1.3.6.6.4" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T5.1.fig1.3.6.6.5" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T5.1.fig1.3.6.6.6" class="ltx_td ltx_nopad_r ltx_align_center">0.97</td>
</tr>
<tr id="S4.T5.1.fig1.3.7.7" class="ltx_tr">
<td id="S4.T5.1.fig1.3.7.7.1" class="ltx_td ltx_align_center">P5</td>
<td id="S4.T5.1.fig1.3.7.7.2" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T5.1.fig1.3.7.7.3" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T5.1.fig1.3.7.7.4" class="ltx_td ltx_align_center">1.00</td>
<td id="S4.T5.1.fig1.3.7.7.5" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T5.1.fig1.3.7.7.6" class="ltx_td ltx_nopad_r ltx_align_center">0.87</td>
</tr>
<tr id="S4.T5.1.fig1.3.8.8" class="ltx_tr">
<td id="S4.T5.1.fig1.3.8.8.1" class="ltx_td ltx_align_center ltx_border_t">Average Sim</td>
<td id="S4.T5.1.fig1.3.8.8.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.fig1.3.8.8.2.1" class="ltx_text ltx_font_bold">0.99</span></td>
<td id="S4.T5.1.fig1.3.8.8.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.fig1.3.8.8.3.1" class="ltx_text ltx_font_bold">0.99</span></td>
<td id="S4.T5.1.fig1.3.8.8.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.fig1.3.8.8.4.1" class="ltx_text ltx_font_bold">0.99</span></td>
<td id="S4.T5.1.fig1.3.8.8.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.fig1.3.8.8.5.1" class="ltx_text ltx_font_bold">0.99</span></td>
<td id="S4.T5.1.fig1.3.8.8.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.95</td>
</tr>
<tr id="S4.T5.1.fig1.3.9.9" class="ltx_tr">
<td id="S4.T5.1.fig1.3.9.9.1" class="ltx_td ltx_align_center ltx_border_t">Real parts loose</td>
<td id="S4.T5.1.fig1.3.9.9.2" class="ltx_td ltx_align_center ltx_border_t">0.51</td>
<td id="S4.T5.1.fig1.3.9.9.3" class="ltx_td ltx_align_center ltx_border_t">0.61</td>
<td id="S4.T5.1.fig1.3.9.9.4" class="ltx_td ltx_align_center ltx_border_t">0.52</td>
<td id="S4.T5.1.fig1.3.9.9.5" class="ltx_td ltx_align_center ltx_border_t">0.47</td>
<td id="S4.T5.1.fig1.3.9.9.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.41</td>
</tr>
<tr id="S4.T5.1.fig1.3.10.10" class="ltx_tr">
<td id="S4.T5.1.fig1.3.10.10.1" class="ltx_td ltx_align_center">Real assembly</td>
<td id="S4.T5.1.fig1.3.10.10.2" class="ltx_td ltx_align_center">0.27</td>
<td id="S4.T5.1.fig1.3.10.10.3" class="ltx_td ltx_align_center">0.78</td>
<td id="S4.T5.1.fig1.3.10.10.4" class="ltx_td ltx_align_center">0.72</td>
<td id="S4.T5.1.fig1.3.10.10.5" class="ltx_td ltx_align_center">0.85</td>
<td id="S4.T5.1.fig1.3.10.10.6" class="ltx_td ltx_nopad_r ltx_align_center">0.20</td>
</tr>
<tr id="S4.T5.1.fig1.3.11.11" class="ltx_tr">
<td id="S4.T5.1.fig1.3.11.11.1" class="ltx_td ltx_align_center ltx_border_t">Average Real</td>
<td id="S4.T5.1.fig1.3.11.11.2" class="ltx_td ltx_align_center ltx_border_t">0.39</td>
<td id="S4.T5.1.fig1.3.11.11.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.1.fig1.3.11.11.3.1" class="ltx_text ltx_font_bold">0.69<sup id="S4.T5.1.fig1.3.11.11.3.1.1" class="ltx_sup"><span id="S4.T5.1.fig1.3.11.11.3.1.1.1" class="ltx_text ltx_font_medium">*</span></sup></span></td>
<td id="S4.T5.1.fig1.3.11.11.4" class="ltx_td ltx_align_center ltx_border_t">0.62<sup id="S4.T5.1.fig1.3.11.11.4.1" class="ltx_sup">*</sup>
</td>
<td id="S4.T5.1.fig1.3.11.11.5" class="ltx_td ltx_align_center ltx_border_t">0.66<sup id="S4.T5.1.fig1.3.11.11.5.1" class="ltx_sup">*</sup>
</td>
<td id="S4.T5.1.fig1.3.11.11.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.31</td>
</tr>
<tr id="S4.T5.1.fig1.3.12.12" class="ltx_tr">
<td id="S4.T5.1.fig1.3.12.12.1" class="ltx_td ltx_align_center"><span id="S4.T5.1.fig1.3.12.12.1.1" class="ltx_ERROR undefined">\botrule</span></td>
<td id="S4.T5.1.fig1.3.12.12.2" class="ltx_td"></td>
<td id="S4.T5.1.fig1.3.12.12.3" class="ltx_td"></td>
<td id="S4.T5.1.fig1.3.12.12.4" class="ltx_td"></td>
<td id="S4.T5.1.fig1.3.12.12.5" class="ltx_td"></td>
<td id="S4.T5.1.fig1.3.12.12.6" class="ltx_td"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S4.I3" class="ltx_itemize ltx_figure_panel">
<li id="S4.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i1.p1" class="ltx_para">
<p id="S4.I3.i1.p1.1" class="ltx_p">Note: Values in bold indicate the maximum average value across different trained models.</p>
</div>
</li>
<li id="S4.I3.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">*</span> 
<div id="S4.I3.ix1.p1" class="ltx_para">
<p id="S4.I3.ix1.p1.1" class="ltx_p">Better results than the best performing basic procedure.</p>
</div>
</li>
</ul>
</div>
</div>
</figure>
</figure>
<figure id="S4.T6" class="ltx_table">
<figure id="S4.T6.1.fig1" class="ltx_figure ltx_minipage ltx_align_center ltx_align_middle" style="width:433.6pt;">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Table 6: </span>Performance comparison of combination datasets for <em id="S4.T6.1.fig1.2.1" class="ltx_emph ltx_font_italic">GeometricPlate</em></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T6.1.fig1.3" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T6.1.fig1.3.1.1" class="ltx_tr">
<td id="S4.T6.1.fig1.3.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S4.T6.1.fig1.3.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T6.1.fig1.3.1.1.1.1.1" class="ltx_tr">
<td id="S4.T6.1.fig1.3.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><em id="S4.T6.1.fig1.3.1.1.1.1.1.1.1" class="ltx_emph ltx_font_italic">GeometricPlate</em></td>
</tr>
<tr id="S4.T6.1.fig1.3.1.1.1.1.2" class="ltx_tr">
<td id="S4.T6.1.fig1.3.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">mAP@[0.5:0.95]</td>
</tr>
</table>
</td>
<td id="S4.T6.1.fig1.3.1.1.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" colspan="5">Trained on</td>
</tr>
<tr id="S4.T6.1.fig1.3.2.2" class="ltx_tr">
<td id="S4.T6.1.fig1.3.2.2.1" class="ltx_td ltx_align_center ltx_border_t">Validated on</td>
<td id="S4.T6.1.fig1.3.2.2.2" class="ltx_td ltx_align_center ltx_border_t">C1</td>
<td id="S4.T6.1.fig1.3.2.2.3" class="ltx_td ltx_align_center ltx_border_t">C2</td>
<td id="S4.T6.1.fig1.3.2.2.4" class="ltx_td ltx_align_center ltx_border_t">C3</td>
<td id="S4.T6.1.fig1.3.2.2.5" class="ltx_td ltx_align_center ltx_border_t">C4</td>
<td id="S4.T6.1.fig1.3.2.2.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">C5</td>
</tr>
<tr id="S4.T6.1.fig1.3.3.3" class="ltx_tr">
<td id="S4.T6.1.fig1.3.3.3.1" class="ltx_td ltx_align_center ltx_border_t">P1</td>
<td id="S4.T6.1.fig1.3.3.3.2" class="ltx_td ltx_align_center ltx_border_t">0.98</td>
<td id="S4.T6.1.fig1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t">0.99</td>
<td id="S4.T6.1.fig1.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t">0.98</td>
<td id="S4.T6.1.fig1.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t">0.98</td>
<td id="S4.T6.1.fig1.3.3.3.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.97</td>
</tr>
<tr id="S4.T6.1.fig1.3.4.4" class="ltx_tr">
<td id="S4.T6.1.fig1.3.4.4.1" class="ltx_td ltx_align_center">P2</td>
<td id="S4.T6.1.fig1.3.4.4.2" class="ltx_td ltx_align_center">0.94</td>
<td id="S4.T6.1.fig1.3.4.4.3" class="ltx_td ltx_align_center">0.90</td>
<td id="S4.T6.1.fig1.3.4.4.4" class="ltx_td ltx_align_center">0.94</td>
<td id="S4.T6.1.fig1.3.4.4.5" class="ltx_td ltx_align_center">0.91</td>
<td id="S4.T6.1.fig1.3.4.4.6" class="ltx_td ltx_nopad_r ltx_align_center">0.83</td>
</tr>
<tr id="S4.T6.1.fig1.3.5.5" class="ltx_tr">
<td id="S4.T6.1.fig1.3.5.5.1" class="ltx_td ltx_align_center">P3</td>
<td id="S4.T6.1.fig1.3.5.5.2" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T6.1.fig1.3.5.5.3" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T6.1.fig1.3.5.5.4" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T6.1.fig1.3.5.5.5" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T6.1.fig1.3.5.5.6" class="ltx_td ltx_nopad_r ltx_align_center">0.97</td>
</tr>
<tr id="S4.T6.1.fig1.3.6.6" class="ltx_tr">
<td id="S4.T6.1.fig1.3.6.6.1" class="ltx_td ltx_align_center">P4</td>
<td id="S4.T6.1.fig1.3.6.6.2" class="ltx_td ltx_align_center">0.96</td>
<td id="S4.T6.1.fig1.3.6.6.3" class="ltx_td ltx_align_center">0.95</td>
<td id="S4.T6.1.fig1.3.6.6.4" class="ltx_td ltx_align_center">0.96</td>
<td id="S4.T6.1.fig1.3.6.6.5" class="ltx_td ltx_align_center">0.96</td>
<td id="S4.T6.1.fig1.3.6.6.6" class="ltx_td ltx_nopad_r ltx_align_center">0.88</td>
</tr>
<tr id="S4.T6.1.fig1.3.7.7" class="ltx_tr">
<td id="S4.T6.1.fig1.3.7.7.1" class="ltx_td ltx_align_center">P5</td>
<td id="S4.T6.1.fig1.3.7.7.2" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T6.1.fig1.3.7.7.3" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T6.1.fig1.3.7.7.4" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T6.1.fig1.3.7.7.5" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T6.1.fig1.3.7.7.6" class="ltx_td ltx_nopad_r ltx_align_center">0.54</td>
</tr>
<tr id="S4.T6.1.fig1.3.8.8" class="ltx_tr">
<td id="S4.T6.1.fig1.3.8.8.1" class="ltx_td ltx_align_center ltx_border_t">Average Sim</td>
<td id="S4.T6.1.fig1.3.8.8.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.fig1.3.8.8.2.1" class="ltx_text ltx_font_bold">0.97</span></td>
<td id="S4.T6.1.fig1.3.8.8.3" class="ltx_td ltx_align_center ltx_border_t">0.96</td>
<td id="S4.T6.1.fig1.3.8.8.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.fig1.3.8.8.4.1" class="ltx_text ltx_font_bold">0.97</span></td>
<td id="S4.T6.1.fig1.3.8.8.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T6.1.fig1.3.8.8.5.1" class="ltx_text ltx_font_bold">0.97</span></td>
<td id="S4.T6.1.fig1.3.8.8.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.84</td>
</tr>
<tr id="S4.T6.1.fig1.3.9.9" class="ltx_tr">
<td id="S4.T6.1.fig1.3.9.9.1" class="ltx_td ltx_align_center ltx_border_t">Real parts loose</td>
<td id="S4.T6.1.fig1.3.9.9.2" class="ltx_td ltx_align_center ltx_border_t">0.06</td>
<td id="S4.T6.1.fig1.3.9.9.3" class="ltx_td ltx_align_center ltx_border_t">0.36</td>
<td id="S4.T6.1.fig1.3.9.9.4" class="ltx_td ltx_align_center ltx_border_t">0.18</td>
<td id="S4.T6.1.fig1.3.9.9.5" class="ltx_td ltx_align_center ltx_border_t">0.49</td>
<td id="S4.T6.1.fig1.3.9.9.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.68</td>
</tr>
<tr id="S4.T6.1.fig1.3.10.10" class="ltx_tr">
<td id="S4.T6.1.fig1.3.10.10.1" class="ltx_td ltx_align_center">Real assembly</td>
<td id="S4.T6.1.fig1.3.10.10.2" class="ltx_td ltx_align_center">0.86</td>
<td id="S4.T6.1.fig1.3.10.10.3" class="ltx_td ltx_align_center">0.94</td>
<td id="S4.T6.1.fig1.3.10.10.4" class="ltx_td ltx_align_center">0.82</td>
<td id="S4.T6.1.fig1.3.10.10.5" class="ltx_td ltx_align_center">0.96</td>
<td id="S4.T6.1.fig1.3.10.10.6" class="ltx_td ltx_nopad_r ltx_align_center">0.81</td>
</tr>
<tr id="S4.T6.1.fig1.3.11.11" class="ltx_tr">
<td id="S4.T6.1.fig1.3.11.11.1" class="ltx_td ltx_align_center ltx_border_t">Average Real</td>
<td id="S4.T6.1.fig1.3.11.11.2" class="ltx_td ltx_align_center ltx_border_t">0.46</td>
<td id="S4.T6.1.fig1.3.11.11.3" class="ltx_td ltx_align_center ltx_border_t">0.65</td>
<td id="S4.T6.1.fig1.3.11.11.4" class="ltx_td ltx_align_center ltx_border_t">0.50</td>
<td id="S4.T6.1.fig1.3.11.11.5" class="ltx_td ltx_align_center ltx_border_t">0.73<sup id="S4.T6.1.fig1.3.11.11.5.1" class="ltx_sup">*</sup>
</td>
<td id="S4.T6.1.fig1.3.11.11.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T6.1.fig1.3.11.11.6.1" class="ltx_text ltx_font_bold">0.74<sup id="S4.T6.1.fig1.3.11.11.6.1.1" class="ltx_sup"><span id="S4.T6.1.fig1.3.11.11.6.1.1.1" class="ltx_text ltx_font_medium">*</span></sup></span></td>
</tr>
<tr id="S4.T6.1.fig1.3.12.12" class="ltx_tr">
<td id="S4.T6.1.fig1.3.12.12.1" class="ltx_td ltx_align_center"><span id="S4.T6.1.fig1.3.12.12.1.1" class="ltx_ERROR undefined">\botrule</span></td>
<td id="S4.T6.1.fig1.3.12.12.2" class="ltx_td"></td>
<td id="S4.T6.1.fig1.3.12.12.3" class="ltx_td"></td>
<td id="S4.T6.1.fig1.3.12.12.4" class="ltx_td"></td>
<td id="S4.T6.1.fig1.3.12.12.5" class="ltx_td"></td>
<td id="S4.T6.1.fig1.3.12.12.6" class="ltx_td"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S4.I4" class="ltx_itemize ltx_figure_panel">
<li id="S4.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I4.i1.p1" class="ltx_para">
<p id="S4.I4.i1.p1.1" class="ltx_p">Note: Values in bold indicate the maximum average value across different trained models.</p>
</div>
</li>
<li id="S4.I4.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">*</span> 
<div id="S4.I4.ix1.p1" class="ltx_para">
<p id="S4.I4.ix1.p1.1" class="ltx_p">Better results than the best performing basic procedure.</p>
</div>
</li>
</ul>
</div>
</div>
</figure>
</figure>
<figure id="S4.T7" class="ltx_table">
<figure id="S4.T7.1.fig1" class="ltx_figure ltx_minipage ltx_align_center ltx_align_middle" style="width:433.6pt;">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Table 7: </span>Results summary</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T7.1.fig1.1" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T7.1.fig1.1.1.1" class="ltx_tr">
<td id="S4.T7.1.fig1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S4.T7.1.fig1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T7.1.fig1.1.1.1.1.1.1" class="ltx_tr">
<td id="S4.T7.1.fig1.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Both objects</td>
</tr>
<tr id="S4.T7.1.fig1.1.1.1.1.1.2" class="ltx_tr">
<td id="S4.T7.1.fig1.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">mAP@[0.5:0.95]</td>
</tr>
</table>
</td>
<td id="S4.T7.1.fig1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="10">Trained on</td>
</tr>
<tr id="S4.T7.1.fig1.1.2.2" class="ltx_tr">
<td id="S4.T7.1.fig1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">Validated on</td>
<td id="S4.T7.1.fig1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">P1</td>
<td id="S4.T7.1.fig1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">P2</td>
<td id="S4.T7.1.fig1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">P3</td>
<td id="S4.T7.1.fig1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">P4</td>
<td id="S4.T7.1.fig1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">P5</td>
<td id="S4.T7.1.fig1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t">C1</td>
<td id="S4.T7.1.fig1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t">C2</td>
<td id="S4.T7.1.fig1.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t">C3</td>
<td id="S4.T7.1.fig1.1.2.2.10" class="ltx_td ltx_align_center ltx_border_t">C4</td>
<td id="S4.T7.1.fig1.1.2.2.11" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">C5</td>
</tr>
<tr id="S4.T7.1.fig1.1.3.3" class="ltx_tr">
<td id="S4.T7.1.fig1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t">P1</td>
<td id="S4.T7.1.fig1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">0.97</td>
<td id="S4.T7.1.fig1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">0.99</td>
<td id="S4.T7.1.fig1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">0.98</td>
<td id="S4.T7.1.fig1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">0.99</td>
<td id="S4.T7.1.fig1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">0.34</td>
<td id="S4.T7.1.fig1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">0.99</td>
<td id="S4.T7.1.fig1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">0.99</td>
<td id="S4.T7.1.fig1.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t">0.99</td>
<td id="S4.T7.1.fig1.1.3.3.10" class="ltx_td ltx_align_center ltx_border_t">0.99</td>
<td id="S4.T7.1.fig1.1.3.3.11" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.98</td>
</tr>
<tr id="S4.T7.1.fig1.1.4.4" class="ltx_tr">
<td id="S4.T7.1.fig1.1.4.4.1" class="ltx_td ltx_align_center">P2</td>
<td id="S4.T7.1.fig1.1.4.4.2" class="ltx_td ltx_align_center">0.64</td>
<td id="S4.T7.1.fig1.1.4.4.3" class="ltx_td ltx_align_center">0.97</td>
<td id="S4.T7.1.fig1.1.4.4.4" class="ltx_td ltx_align_center">0.67</td>
<td id="S4.T7.1.fig1.1.4.4.5" class="ltx_td ltx_align_center">0.95</td>
<td id="S4.T7.1.fig1.1.4.4.6" class="ltx_td ltx_align_center">0.32</td>
<td id="S4.T7.1.fig1.1.4.4.7" class="ltx_td ltx_align_center">0.96</td>
<td id="S4.T7.1.fig1.1.4.4.8" class="ltx_td ltx_align_center">0.94</td>
<td id="S4.T7.1.fig1.1.4.4.9" class="ltx_td ltx_align_center">0.96</td>
<td id="S4.T7.1.fig1.1.4.4.10" class="ltx_td ltx_align_center">0.95</td>
<td id="S4.T7.1.fig1.1.4.4.11" class="ltx_td ltx_nopad_r ltx_align_center">0.90</td>
</tr>
<tr id="S4.T7.1.fig1.1.5.5" class="ltx_tr">
<td id="S4.T7.1.fig1.1.5.5.1" class="ltx_td ltx_align_center">P3</td>
<td id="S4.T7.1.fig1.1.5.5.2" class="ltx_td ltx_align_center">0.97</td>
<td id="S4.T7.1.fig1.1.5.5.3" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T7.1.fig1.1.5.5.4" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T7.1.fig1.1.5.5.5" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T7.1.fig1.1.5.5.6" class="ltx_td ltx_align_center">0.28</td>
<td id="S4.T7.1.fig1.1.5.5.7" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T7.1.fig1.1.5.5.8" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T7.1.fig1.1.5.5.9" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T7.1.fig1.1.5.5.10" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T7.1.fig1.1.5.5.11" class="ltx_td ltx_nopad_r ltx_align_center">0.98</td>
</tr>
<tr id="S4.T7.1.fig1.1.6.6" class="ltx_tr">
<td id="S4.T7.1.fig1.1.6.6.1" class="ltx_td ltx_align_center">P4</td>
<td id="S4.T7.1.fig1.1.6.6.2" class="ltx_td ltx_align_center">0.66</td>
<td id="S4.T7.1.fig1.1.6.6.3" class="ltx_td ltx_align_center">0.97</td>
<td id="S4.T7.1.fig1.1.6.6.4" class="ltx_td ltx_align_center">0.74</td>
<td id="S4.T7.1.fig1.1.6.6.5" class="ltx_td ltx_align_center">0.98</td>
<td id="S4.T7.1.fig1.1.6.6.6" class="ltx_td ltx_align_center">0.31</td>
<td id="S4.T7.1.fig1.1.6.6.7" class="ltx_td ltx_align_center">0.97</td>
<td id="S4.T7.1.fig1.1.6.6.8" class="ltx_td ltx_align_center">0.96</td>
<td id="S4.T7.1.fig1.1.6.6.9" class="ltx_td ltx_align_center">0.97</td>
<td id="S4.T7.1.fig1.1.6.6.10" class="ltx_td ltx_align_center">0.97</td>
<td id="S4.T7.1.fig1.1.6.6.11" class="ltx_td ltx_nopad_r ltx_align_center">0.92</td>
</tr>
<tr id="S4.T7.1.fig1.1.7.7" class="ltx_tr">
<td id="S4.T7.1.fig1.1.7.7.1" class="ltx_td ltx_align_center">P5</td>
<td id="S4.T7.1.fig1.1.7.7.2" class="ltx_td ltx_align_center">0.40</td>
<td id="S4.T7.1.fig1.1.7.7.3" class="ltx_td ltx_align_center">0.82</td>
<td id="S4.T7.1.fig1.1.7.7.4" class="ltx_td ltx_align_center">0.52</td>
<td id="S4.T7.1.fig1.1.7.7.5" class="ltx_td ltx_align_center">0.78</td>
<td id="S4.T7.1.fig1.1.7.7.6" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T7.1.fig1.1.7.7.7" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T7.1.fig1.1.7.7.8" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T7.1.fig1.1.7.7.9" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T7.1.fig1.1.7.7.10" class="ltx_td ltx_align_center">0.99</td>
<td id="S4.T7.1.fig1.1.7.7.11" class="ltx_td ltx_nopad_r ltx_align_center">0.70</td>
</tr>
<tr id="S4.T7.1.fig1.1.8.8" class="ltx_tr">
<td id="S4.T7.1.fig1.1.8.8.1" class="ltx_td ltx_align_center ltx_border_t">Average Sim</td>
<td id="S4.T7.1.fig1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_t">0.73</td>
<td id="S4.T7.1.fig1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_t">0.95</td>
<td id="S4.T7.1.fig1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_t">0.78</td>
<td id="S4.T7.1.fig1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_t">0.94</td>
<td id="S4.T7.1.fig1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_t">0.45</td>
<td id="S4.T7.1.fig1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.fig1.1.8.8.7.1" class="ltx_text ltx_font_bold">0.98</span></td>
<td id="S4.T7.1.fig1.1.8.8.8" class="ltx_td ltx_align_center ltx_border_t">0.97</td>
<td id="S4.T7.1.fig1.1.8.8.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.fig1.1.8.8.9.1" class="ltx_text ltx_font_bold">0.98</span></td>
<td id="S4.T7.1.fig1.1.8.8.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.fig1.1.8.8.10.1" class="ltx_text ltx_font_bold">0.98</span></td>
<td id="S4.T7.1.fig1.1.8.8.11" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.90</td>
</tr>
<tr id="S4.T7.1.fig1.1.9.9" class="ltx_tr">
<td id="S4.T7.1.fig1.1.9.9.1" class="ltx_td ltx_align_center ltx_border_t">Real parts loose</td>
<td id="S4.T7.1.fig1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_t">0.47</td>
<td id="S4.T7.1.fig1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_t">0.36</td>
<td id="S4.T7.1.fig1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_t">0.34</td>
<td id="S4.T7.1.fig1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_t">0.49</td>
<td id="S4.T7.1.fig1.1.9.9.6" class="ltx_td ltx_align_center ltx_border_t">0.15</td>
<td id="S4.T7.1.fig1.1.9.9.7" class="ltx_td ltx_align_center ltx_border_t">0.28</td>
<td id="S4.T7.1.fig1.1.9.9.8" class="ltx_td ltx_align_center ltx_border_t">0.48</td>
<td id="S4.T7.1.fig1.1.9.9.9" class="ltx_td ltx_align_center ltx_border_t">0.35</td>
<td id="S4.T7.1.fig1.1.9.9.10" class="ltx_td ltx_align_center ltx_border_t">0.48</td>
<td id="S4.T7.1.fig1.1.9.9.11" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.54</td>
</tr>
<tr id="S4.T7.1.fig1.1.10.10" class="ltx_tr">
<td id="S4.T7.1.fig1.1.10.10.1" class="ltx_td ltx_align_center">Real assembly</td>
<td id="S4.T7.1.fig1.1.10.10.2" class="ltx_td ltx_align_center">0.36</td>
<td id="S4.T7.1.fig1.1.10.10.3" class="ltx_td ltx_align_center">0.60</td>
<td id="S4.T7.1.fig1.1.10.10.4" class="ltx_td ltx_align_center">0.15</td>
<td id="S4.T7.1.fig1.1.10.10.5" class="ltx_td ltx_align_center">0.72</td>
<td id="S4.T7.1.fig1.1.10.10.6" class="ltx_td ltx_align_center">0.49</td>
<td id="S4.T7.1.fig1.1.10.10.7" class="ltx_td ltx_align_center">0.56</td>
<td id="S4.T7.1.fig1.1.10.10.8" class="ltx_td ltx_align_center">0.86</td>
<td id="S4.T7.1.fig1.1.10.10.9" class="ltx_td ltx_align_center">0.77</td>
<td id="S4.T7.1.fig1.1.10.10.10" class="ltx_td ltx_align_center">0.90</td>
<td id="S4.T7.1.fig1.1.10.10.11" class="ltx_td ltx_nopad_r ltx_align_center">0.51</td>
</tr>
<tr id="S4.T7.1.fig1.1.11.11" class="ltx_tr">
<td id="S4.T7.1.fig1.1.11.11.1" class="ltx_td ltx_align_center ltx_border_t">Average Real</td>
<td id="S4.T7.1.fig1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_t">0.42</td>
<td id="S4.T7.1.fig1.1.11.11.3" class="ltx_td ltx_align_center ltx_border_t">0.48</td>
<td id="S4.T7.1.fig1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_t">0.25</td>
<td id="S4.T7.1.fig1.1.11.11.5" class="ltx_td ltx_align_center ltx_border_t">0.60</td>
<td id="S4.T7.1.fig1.1.11.11.6" class="ltx_td ltx_align_center ltx_border_t">0.32</td>
<td id="S4.T7.1.fig1.1.11.11.7" class="ltx_td ltx_align_center ltx_border_t">0.42</td>
<td id="S4.T7.1.fig1.1.11.11.8" class="ltx_td ltx_align_center ltx_border_t">0.67<sup id="S4.T7.1.fig1.1.11.11.8.1" class="ltx_sup">*</sup>
</td>
<td id="S4.T7.1.fig1.1.11.11.9" class="ltx_td ltx_align_center ltx_border_t">0.56</td>
<td id="S4.T7.1.fig1.1.11.11.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.fig1.1.11.11.10.1" class="ltx_text ltx_font_bold">0.69<sup id="S4.T7.1.fig1.1.11.11.10.1.1" class="ltx_sup"><span id="S4.T7.1.fig1.1.11.11.10.1.1.1" class="ltx_text ltx_font_medium">*</span></sup></span></td>
<td id="S4.T7.1.fig1.1.11.11.11" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.53</td>
</tr>
<tr id="S4.T7.1.fig1.1.12.12" class="ltx_tr">
<td id="S4.T7.1.fig1.1.12.12.1" class="ltx_td ltx_align_center"><span id="S4.T7.1.fig1.1.12.12.1.1" class="ltx_ERROR undefined">\botrule</span></td>
<td id="S4.T7.1.fig1.1.12.12.2" class="ltx_td"></td>
<td id="S4.T7.1.fig1.1.12.12.3" class="ltx_td"></td>
<td id="S4.T7.1.fig1.1.12.12.4" class="ltx_td"></td>
<td id="S4.T7.1.fig1.1.12.12.5" class="ltx_td"></td>
<td id="S4.T7.1.fig1.1.12.12.6" class="ltx_td"></td>
<td id="S4.T7.1.fig1.1.12.12.7" class="ltx_td"></td>
<td id="S4.T7.1.fig1.1.12.12.8" class="ltx_td"></td>
<td id="S4.T7.1.fig1.1.12.12.9" class="ltx_td"></td>
<td id="S4.T7.1.fig1.1.12.12.10" class="ltx_td"></td>
<td id="S4.T7.1.fig1.1.12.12.11" class="ltx_td"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S4.I5" class="ltx_itemize ltx_figure_panel">
<li id="S4.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I5.i1.p1" class="ltx_para">
<p id="S4.I5.i1.p1.1" class="ltx_p">Note: Values in bold indicate the maximum average value across different trained models.</p>
</div>
</li>
<li id="S4.I5.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">*</span> 
<div id="S4.I5.ix1.p1" class="ltx_para">
<p id="S4.I5.ix1.p1.1" class="ltx_p">Better results than the best performing basic procedure.</p>
</div>
</li>
</ul>
</div>
</div>
</figure>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Table <a href="#S4.T5" title="Table 5 ‣ 4.2 Evaluation of Combination Datasets ‣ 4 Validation ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and Table <a href="#S4.T6" title="Table 6 ‣ 4.2 Evaluation of Combination Datasets ‣ 4 Validation ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> draw a performance comparison between combination datasets for <em id="S4.SS2.p2.1.1" class="ltx_emph ltx_font_italic">ManholeBox</em> and <em id="S4.SS2.p2.1.2" class="ltx_emph ltx_font_italic">GeometricPlate</em> respectively. It can be easily seen that in general the combination datasets perform way better for synthetic images generated from all procedure unlike in case of basic procedure mentioned in Section <a href="#S4.SS1" title="4.1 Evaluation of Basic Procedures ‣ 4 Validation ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>. For real images in a production environment, C2, C3 and C4 perform better than the best performing basic procedure for <em id="S4.SS2.p2.1.3" class="ltx_emph ltx_font_italic">Manholebox</em>, while C4 and C5 perform better than the best performing basic procedure for <em id="S4.SS2.p2.1.4" class="ltx_emph ltx_font_italic">GeometricPlate</em>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Summarizing the results of models trained on combination datasets, in case of <em id="S4.SS2.p3.1.1" class="ltx_emph ltx_font_italic">Manholebox</em>, there is upto 13% improvement whereas for <em id="S4.SS2.p3.1.2" class="ltx_emph ltx_font_italic">GeometricPlate</em>, performance improvement of about 5% is observed. A proper mixture of procedures can yield better performance thereby bridging the simulation to reality gap.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Final results and discussion</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The results from basics and combination datasets for both objects are summarized in Table <a href="#S4.T7" title="Table 7 ‣ 4.2 Evaluation of Combination Datasets ‣ 4 Validation ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. These values are the mean values of the performance of <em id="S4.SS3.p1.1.1" class="ltx_emph ltx_font_italic">ManholeBox</em> and <em id="S4.SS3.p1.1.2" class="ltx_emph ltx_font_italic">GeometricPlate</em> on validation datasets.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Combination datasets show varied overall performance. While the C2 and C4 combinations show improved performance of 11% and 15% respectively with respect to the best performing basic procedure, other combinations do not prove to be effective enough. Referring to Table <a href="#S4.T4" title="Table 4 ‣ 4.1 Evaluation of Basic Procedures ‣ 4 Validation ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> again, C2 is composed of three basic procedures: objects on the ground plane with textures, objects in 3D space with indoor background and objects placed as defined in assembly CAD model with indoor backgrounds. C4 is only composed of two basic procedures: objects in 3D space with indoor background and objects placed as defined in assembly CAD model with indoor backgrounds. Textured planes introduce photorealistic characteristics such as reflection and shadows to the rendered images whereas indoor backgrounds introduce versatility in the images reducing the detected false positives. The standalone performance of the target domain based P5 using assembly CAD and indoor backgrounds is not satisfactory on real images but when this procedure is combined in given proportions with other procedures, a performance boost is observed. However, the best performance yielded by C4 does not consist of photorealistic images with textured planes and implies that the need of photorealistic data using textured planes, even after delivering good results, is not a must for performance improvement.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2311.11039/assets/x5.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="518" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Object detection results for both objects with model trained on C2 dataset in a production environment with different scenarios</figcaption>
</figure>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2311.11039/assets/x6.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="317" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Validation images for model trained on C2 dataset showing improved precision and recall over model trained on basic procedure P4. Left images in (a) and (b) show a false positive and a false negative respectively which was correctly handled in the right images using model trained on C2</figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Fig. <a href="#S4.F8" title="Figure 8 ‣ 4.3 Final results and discussion ‣ 4 Validation ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows some real world object detection results in a production environment using the trained model on C2 dataset. A score threshold of 0.80 was used while inferencing. Fig. <a href="#S4.F9" title="Figure 9 ‣ 4.3 Final results and discussion ‣ 4 Validation ‣ Synthetic Data Generation for Bridging Sim2Real Gap in a Production Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> depicts examples of improved performance on combination dataset C2 compared to a basic procedure P4. A reduction in the occurrence of false positives and false negetives can be observed here. For the current example, model trained on C2 dataset was compared with model trained on P4.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, a scalable pipeline based approach with procedures based on domain randomization and domain adaption techniques for photorealistic synthetic data generation is presented. Object detection models trained on five basic procedures are validated on real world images inside a production environment and the results are used to derive combination datasets from the best performing procedures. The models thus trained on some of the combination datasets show an improved performance than those which were purely trained on datasets with basic procedures. The final results bridge the Sim2Real gap with about 15% of performance improvement using combination of basic procedures on real images as compared to the best performing basic procedure. The achieved drastic performance improvement is due to the knowledge of the target domain undertaken while generating synthetic data. This is done by constructing a scene similar to the structure in the CAD model. The models trained with only synthetic images on YOLOv7 <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib37" title="" class="ltx_ref">Wang.06.07.2022 </a></cite> show promising performance in a real world production environment.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The pipeline approach presented here starts from generating the mesh files and their transformations in assembly origin from the assembly STEP model using scripts. Then next module is built on the top of BlenderProc pipeline <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib17" title="" class="ltx_ref">Denninger.25.10.2019 </a></cite> and generates a synthetic dataset with a desired combination of basic procedures, which can then be used for training neural networks for tasks such that object detection and object pose estimation. This enables easy generation of thousands of annotated images in few hours.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">From the validation of the dataset, it can be concluded that inclusion of target domain plays an important role for bridging Sim2Real gap. The use of photorealistic images leads to large improvements and hence reduces the size of training datasets and time required for training. For the future work, some of the GAN based approaches can be tested and evaluated in combination with these procedures. Moreover, the generated synthetic data can be validated for object pose estimation neural networks to check if the same procedures also yield better performance in those cases.</p>
</div>
<div id="S5.p4" class="ltx_para">
<span id="S5.p4.1" class="ltx_ERROR undefined">\bmhead</span>
<p id="S5.p4.2" class="ltx_p">Acknowledgments</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">We are grateful for the valuable expert information provided especially by every member of the project team Integrated Production Systems as well as the Head of Stade Branch Dr. Dirk Niermann at Fraunhofer IFAM.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.1.1" class="ltx_bibitem">
<span class="ltx_bibblock"><span id="bib.1.1.1.1" class="ltx_ERROR undefined">\bibcommenthead</span>
</span>
</li>
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
Borrego J, Dehban A, Figueiredo R, Moreno P, Bernardino A, Santos-Victor J.:
Applying Domain Randomization to Synthetic Data for Object Category
Detection.

</span>
<span class="ltx_bibblock">Available from: <a target="_blank" href="https://arxiv.org/pdf/1807.09834" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/pdf/1807.09834</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(2)</span>
<span class="ltx_bibblock">
Tremblay J, Prakash A, Acuna D, Brophy M, Jampani V, Anil C, et al.: Training
Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain
Randomization.

</span>
<span class="ltx_bibblock">Available from: <a target="_blank" href="https://arxiv.org/pdf/1804.06516" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/pdf/1804.06516</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(3)</span>
<span class="ltx_bibblock">
Mayershofer C, Ge T, Fottner J.

</span>
<span class="ltx_bibblock">Towards Fully-Synthetic Training for Industrial Applications.

</span>
<span class="ltx_bibblock">In: LISS 2020. Springer, Singapore; 2021. p. 765–782.

</span>
<span class="ltx_bibblock">Available from:
<a target="_blank" href="https://link.springer.com/chapter/10.1007/978-981-33-4359-7_53" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://link.springer.com/chapter/10.1007/978-981-33-4359-7_53</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(4)</span>
<span class="ltx_bibblock">
C A Akar, J Tekli, D Jess, M Khoury, M Kamradt, M Guthe.

</span>
<span class="ltx_bibblock">Synthetic Object Recognition Dataset for Industries.

</span>
<span class="ltx_bibblock">In: 2022 35th SIBGRAPI Conference on Graphics, Patterns and Images
(SIBGRAPI); 2022. p. 150–155.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(5)</span>
<span class="ltx_bibblock">
Moonen S, Vanherle B, de Hoog J, Bourgana T, Bey-Temsamani A, Michiels N.

</span>
<span class="ltx_bibblock">CAD2Render: A Modular Toolkit for GPU-Accelerated Photorealistic
Synthetic Data Generation for the Manufacturing Industry; 2023. p. 583–592.

</span>
<span class="ltx_bibblock">Available from:
<a target="_blank" href="https://openaccess.thecvf.com/content/WACV2023W/PIES-CV/html/Moonen_CAD2Render_A_Modular_Toolkit_for_GPU-Accelerated_Photorealistic_Synthetic_Data_Generation_WACVW_2023_paper.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openaccess.thecvf.com/content/WACV2023W/PIES-CV/html/Moonen_CAD2Render_A_Modular_Toolkit_for_GPU-Accelerated_Photorealistic_Synthetic_Data_Generation_WACVW_2023_paper.html</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(6)</span>
<span class="ltx_bibblock">
A Prakash, S Boochoon, M Brophy, D Acuna, E Cameracci, G
State, et al.

</span>
<span class="ltx_bibblock">Structured Domain Randomization: Bridging the Reality Gap by
Context-Aware Synthetic Data.

</span>
<span class="ltx_bibblock">In: 2019 International Conference on Robotics and Automation (ICRA);
2019. p. 7249–7255.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(7)</span>
<span class="ltx_bibblock">
Eversberg L, Lambrecht J.

</span>
<span class="ltx_bibblock">Generating Images with Physics-Based Rendering for an Industrial
Object Detection Task: Realism versus Domain Randomization.

</span>
<span class="ltx_bibblock">Sensors. 2021;21(23):7901.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.3390/s21237901" title="" class="ltx_ref">10.3390/s21237901</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(8)</span>
<span class="ltx_bibblock">
Tobin J, Fong R, Ray A, Schneider J, Zaremba W, Abbeel P.

</span>
<span class="ltx_bibblock">Domain randomization for transferring deep neural networks from
simulation to the real world.

</span>
<span class="ltx_bibblock">In: IROS Vancouver 2017. Piscataway, NJ: IEEE; 2017. p. 23–30.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(9)</span>
<span class="ltx_bibblock">
Nowruzi FE, Kapoor P, Kolhatkar D, Hassanat FA, Laganiere R, Rebut J.: How much
real data do we actually need: Analyzing object detection performance using
synthetic and real data.

</span>
<span class="ltx_bibblock">Available from: <a target="_blank" href="https://arxiv.org/pdf/1907.07061" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/pdf/1907.07061</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(10)</span>
<span class="ltx_bibblock">
D Dwibedi, I Misra, M Hebert.

</span>
<span class="ltx_bibblock">Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance
Detection.

</span>
<span class="ltx_bibblock">In: 2017 IEEE International Conference on Computer Vision (ICCV);
2017. p. 1310–1319.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(11)</span>
<span class="ltx_bibblock">
Georgakis G, Mousavian A, Berg AC, Kosecka J.: Synthesizing Training Data for
Object Detection in Indoor Scenes.

</span>
<span class="ltx_bibblock">Available from: <a target="_blank" href="https://arxiv.org/pdf/1702.07836" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/pdf/1702.07836</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(12)</span>
<span class="ltx_bibblock">
Collins J, Chand S, Vanderkop A, Howard D.

</span>
<span class="ltx_bibblock">A Review of Physics Simulators for Robotic Applications.

</span>
<span class="ltx_bibblock">IEEE Access. 2021;9:51416–51431.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1109/ACCESS.2021.3068769" title="" class="ltx_ref">10.1109/ACCESS.2021.3068769</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(13)</span>
<span class="ltx_bibblock">
To T, Tremblay J, McKay D, Yamaguchi Y, Leung K, Balanon A, et al.: NDDS:
NVIDIA Deep Learning Dataset Synthesizer.

</span>
<span class="ltx_bibblock">Available from: <a target="_blank" href="https://github.com/NVIDIA/Dataset_Synthesizer" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/NVIDIA/Dataset_Synthesizer</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(14)</span>
<span class="ltx_bibblock">
Šmíd A.: Comparison of unity and unreal engine.

</span>
<span class="ltx_bibblock">Available from: <a target="_blank" href="https://core.ac.uk/download/pdf/84832291.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://core.ac.uk/download/pdf/84832291.pdf</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(15)</span>
<span class="ltx_bibblock">
Morrical N, Tremblay J, Lin Y, Tyree S, Birchfield S, Pascucci V, et al.:
NViSII: A Scriptable Tool for Photorealistic Image Generation.

</span>
<span class="ltx_bibblock">Available from: <a target="_blank" href="https://arxiv.org/pdf/2105.13962" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/pdf/2105.13962</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(16)</span>
<span class="ltx_bibblock">
J Tremblay, T To, S Birchfield.

</span>
<span class="ltx_bibblock">Falling Things: A Synthetic Dataset for 3D Object Detection and Pose
Estimation.

</span>
<span class="ltx_bibblock">In: 2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops (CVPRW); 2018. p. 2119–21193.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(17)</span>
<span class="ltx_bibblock">
Denninger M, Sundermeyer M, Winkelbauer D, Zidan Y, Olefir D, Elbadrawy M,
et al.: BlenderProc.

</span>
<span class="ltx_bibblock">Available from: <a target="_blank" href="https://arxiv.org/pdf/1911.01911" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/pdf/1911.01911</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(18)</span>
<span class="ltx_bibblock">
Denninger M, Sundermeyer M, Winkelbauer D, Olefir D, Hodan T, Zidan Y, et al.

</span>
<span class="ltx_bibblock">BlenderProc: Reducing the Reality Gap with Photorealistic Rendering.

</span>
<span class="ltx_bibblock">In: International Conference on Robotics: Sciene and Systems, RSS
2020; 2020. Available from: <a target="_blank" href="https://elib.dlr.de/139317/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://elib.dlr.de/139317/</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(19)</span>
<span class="ltx_bibblock">
Greff K, Belletti F, Beyer L, Doersch C, Du Yilun, Duckworth D, et al.:
Kubric: A scalable dataset generator.

</span>
<span class="ltx_bibblock">Available from: <a target="_blank" href="https://arxiv.org/pdf/2203.03570" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/pdf/2203.03570</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(20)</span>
<span class="ltx_bibblock">
Ren X, Luo J, Solowjow E, Ojea JA, Gupta A, Tamar A, et al.: Domain
Randomization for Active Pose Estimation.

</span>
<span class="ltx_bibblock">Available from: <a target="_blank" href="https://arxiv.org/pdf/1903.03953" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/pdf/1903.03953</a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(21)</span>
<span class="ltx_bibblock">
S Hinterstoisser, O Pauly, H Heibel, M Martina, M Bokeloh.

</span>
<span class="ltx_bibblock">An Annotation Saved is an Annotation Earned: Using Fully Synthetic
Training for Object Detection.

</span>
<span class="ltx_bibblock">In: 2019 IEEE/CVF International Conference on Computer Vision
Workshop (ICCVW); 2019. p. 2787–2796.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(22)</span>
<span class="ltx_bibblock">
Y Huangfu, W Deng, B Ren, J Ding.

</span>
<span class="ltx_bibblock">A Generation Method of Synthetic Images with Reduced Domain Gap for
Car Detection.

</span>
<span class="ltx_bibblock">In: 2021 5th CAA International Conference on Vehicular Control and
Intelligence (CVCI); 2021. p. 1–6.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(23)</span>
<span class="ltx_bibblock">
Baaz A, Yonan Y, Hernandez-Diaz K, Alonso-Fernandez F, Nilsson F.: Synthetic
Data for Object Classification in Industrial Applications.

</span>
<span class="ltx_bibblock">Available from: <a target="_blank" href="https://arxiv.org/pdf/2212.04790" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/pdf/2212.04790</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(24)</span>
<span class="ltx_bibblock">
J Dümmel, X Gao.

</span>
<span class="ltx_bibblock">Object Re-Identification with Synthetic Training Data in Industrial
Environments.

</span>
<span class="ltx_bibblock">In: 2021 27th International Conference on Mechatronics and Machine
Vision in Practice (M2VIP); 2021. p. 504–508.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(25)</span>
<span class="ltx_bibblock">
Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, et al.

</span>
<span class="ltx_bibblock">Generative Adversarial Nets.

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems. 2014;27.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(26)</span>
<span class="ltx_bibblock">
A Shrivastava, T Pfister, O Tuzel, J Susskind, W Wang, R
Webb.

</span>
<span class="ltx_bibblock">Learning from Simulated and Unsupervised Images through Adversarial
Training.

</span>
<span class="ltx_bibblock">In: 2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR); 2017. p. 2242–2251.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(27)</span>
<span class="ltx_bibblock">
S Sankaranarayanan, Y Balaji, A Jain, S N Lim, R Chellappa.

</span>
<span class="ltx_bibblock">Learning from Synthetic Data: Addressing Domain Shift for Semantic
Segmentation.

</span>
<span class="ltx_bibblock">In: 2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition; 2018. p. 3752–3761.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(28)</span>
<span class="ltx_bibblock">
X Peng, K Saenko.

</span>
<span class="ltx_bibblock">Synthetic to Real Adaptation with Generative Correlation Alignment
Networks.

</span>
<span class="ltx_bibblock">In: 2018 IEEE Winter Conference on Applications of Computer Vision
(WACV); 2018. p. 1982–1991.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(29)</span>
<span class="ltx_bibblock">
P Rojtberg, T Pöllabauer, A Kuijper.

</span>
<span class="ltx_bibblock">Style-transfer GANs for bridging the domain gap in synthetic pose
estimator training.

</span>
<span class="ltx_bibblock">In: 2020 IEEE International Conference on Artificial Intelligence and
Virtual Reality (AIVR); 2020. p. 188–195.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(30)</span>
<span class="ltx_bibblock">
T Hodaň, V Vineet, R Gal, E Shalev, J Hanzelka, T
Connell, et al.

</span>
<span class="ltx_bibblock">Photorealistic Image Synthesis for Object Instance Detection.

</span>
<span class="ltx_bibblock">In: 2019 IEEE International Conference on Image Processing (ICIP);
2019. p. 66–70.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(31)</span>
<span class="ltx_bibblock">
F Reway, A Hoffmann, D Wachtel, W Huber, A Knoll, E Ribeiro.

</span>
<span class="ltx_bibblock">Test Method for Measuring the Simulation-to-Reality Gap of
Camera-based Object Detection Algorithms for Autonomous Driving.

</span>
<span class="ltx_bibblock">In: 2020 IEEE Intelligent Vehicles Symposium (IV); 2020. p.
1249–1256.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(32)</span>
<span class="ltx_bibblock">
Schoepflin D, Iyer K, Gomse M, Schüppstuhl T.

</span>
<span class="ltx_bibblock">Towards Synthetic AI Training Data for Image Classification in
Intralogistic Settings.

</span>
<span class="ltx_bibblock">In: Annals of Scientific Society for Assembly, Handling and
Industrial Robotics 2021. Springer, Cham; 2022. p. 325–336.

</span>
<span class="ltx_bibblock">Available from:
<a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-030-74032-0_27" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://link.springer.com/chapter/10.1007/978-3-030-74032-0_27</a>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(33)</span>
<span class="ltx_bibblock">
Dümmel J, Kostik V, Oellerich J.

</span>
<span class="ltx_bibblock">Generating Synthetic Training Data for Assembly Processes.

</span>
<span class="ltx_bibblock">Springer, Cham; 2021. p. 119–128.

</span>
<span class="ltx_bibblock">Available from:
<a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-030-85910-7_13" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://link.springer.com/chapter/10.1007/978-3-030-85910-7_13</a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(34)</span>
<span class="ltx_bibblock">
M Andulkar, J Hodapp, T Reichling, M Reichenbach, U Berger.

</span>
<span class="ltx_bibblock">Training CNNs from Synthetic Data for Part Handling in Industrial
Environments.

</span>
<span class="ltx_bibblock">In: 2018 IEEE 14th International Conference on Automation Science and
Engineering (CASE); 2018. p. 624–629.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(35)</span>
<span class="ltx_bibblock">
Lin TY, Maire M, Belongie S, Bourdev L, Girshick R, Hays J, et al.: Microsoft
COCO: Common Objects in Context.

</span>
<span class="ltx_bibblock">Available from: <a target="_blank" href="https://arxiv.org/pdf/1405.0312" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/pdf/1405.0312</a>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(36)</span>
<span class="ltx_bibblock">
Hodan T, Michel F, Brachmann E, Kehl W, Buch AG, Kraft D, et al.: BOP:
Benchmark for 6D Object Pose Estimation.

</span>
<span class="ltx_bibblock">Available from: <a target="_blank" href="https://arxiv.org/pdf/1808.08319" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/pdf/1808.08319</a>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(37)</span>
<span class="ltx_bibblock">
Wang CY, Bochkovskiy A, Liao HYM.: YOLOv7: Trainable bag-of-freebies sets new
state-of-the-art for real-time object detectors.

</span>
<span class="ltx_bibblock">Available from: <a target="_blank" href="https://arxiv.org/pdf/2207.02696" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/pdf/2207.02696</a>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(38)</span>
<span class="ltx_bibblock">
Everingham M, van Gool L, Williams CKI, Winn J, Zisserman A.

</span>
<span class="ltx_bibblock">The Pascal Visual Object Classes (VOC) Challenge.

</span>
<span class="ltx_bibblock">International Journal of Computer Vision. 2010;88(2):303–338.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1007/s11263-009-0275-4" title="" class="ltx_ref">10.1007/s11263-009-0275-4</a>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(39)</span>
<span class="ltx_bibblock">
S Bell, C L Zitnick, K Bala, R Girshick.

</span>
<span class="ltx_bibblock">Inside-Outside Net: Detecting Objects in Context with Skip Pooling
and Recurrent Neural Networks.

</span>
<span class="ltx_bibblock">In: 2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR); 2017. p. 2874–2883.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2311.11038" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2311.11039" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2311.11039">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2311.11039" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2311.11040" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 18:49:41 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
