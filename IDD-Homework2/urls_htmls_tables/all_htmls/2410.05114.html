<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization</title>
<!--Generated on Mon Oct  7 14:52:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Generative Adversarial Network Image Synthesis Dermatoscopy" lang="en" name="keywords"/>
<base href="/html/2410.05114v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S1" title="In Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S2" title="In Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Our Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S2.SS1" title="In 2 Our Approach ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Background</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S2.SS2" title="In 2 Our Approach ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Unsupervised Transformation Pipeline</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S2.SS2.SSS1" title="In 2.2 Unsupervised Transformation Pipeline ‣ 2 Our Approach ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>GAN Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S2.SS2.SSS2" title="In 2.2 Unsupervised Transformation Pipeline ‣ 2 Our Approach ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Latent-Space Factorization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S2.SS2.SSS3" title="In 2.2 Unsupervised Transformation Pipeline ‣ 2 Our Approach ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>GAN Inversion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S2.SS2.SSS4" title="In 2.2 Unsupervised Transformation Pipeline ‣ 2 Our Approach ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.4 </span>Identify Relevant Transformations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S2.SS3" title="In 2 Our Approach ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Classifier Training Enhancement With Synthetic Data</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S2.SS3.SSS1" title="In 2.3 Classifier Training Enhancement With Synthetic Data ‣ 2 Our Approach ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S2.SS3.SSS2" title="In 2.3 Classifier Training Enhancement With Synthetic Data ‣ 2 Our Approach ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.2 </span>Model, Task, and Training</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S3" title="In Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S3.SS1" title="In 3 Results and Discussion ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Transformations Developed</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S3.SS2" title="In 3 Results and Discussion ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S3.SS3" title="In 3 Results and Discussion ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Model Analysis with Explainable AI</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S4" title="In Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S1a" title="In Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Exemplary transformations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S2a" title="In Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Explanations of the Baseline and the Augmented Models for the class MEL</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S3a" title="In Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Understanding Prediction Strategies with Prototypical Concept-based Explanations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S4a" title="In Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Understanding Prediction Strategies with Prototypical Concept-based Explanations</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">(eccv)                Package eccv Warning: Package ‘hyperref’ is loaded with option ‘pagebackref’, which is *not* recommended for camera-ready version</p>
</div>
<span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Fraunhofer USA Center Mid-Atlantic, 20737-1250 Riverdale, MD, USA
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id1.1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>rreddy@fraunhofer.org</span></span></span>
<br class="ltx_break"/><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cma.fraunhofer.org/" title="">https://www.cma.fraunhofer.org/</a> </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Fraunhofer Heinrich-Hertz-Institut, 10587 Berlin, Germany
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id2.1"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>markus.wenzel@hhi.fraunhofer.de</span></span></span>
<br class="ltx_break"/><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.hhi.fraunhofer.de/" title="">https://www.hhi.fraunhofer.de/</a></span></span></span>
<h1 class="ltx_title ltx_title_document">Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Rohan Reddy Mekala <span class="ltx_ERROR undefined" id="id1.1.id1">\orcidlink</span>0000-0002-2274-9560
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Frederik Pahde <span class="ltx_ERROR undefined" id="id2.1.id1">\orcidlink</span>0000-0002-5681-6231
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Simon Baur <span class="ltx_ERROR undefined" id="id3.1.id1">\orcidlink</span>0009-0009-4307-3078
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sneha Chandrashekar <span class="ltx_ERROR undefined" id="id4.1.id1">\orcidlink</span>0009-0001-7835-6061
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Madeline Diep <span class="ltx_ERROR undefined" id="id5.1.id1">\orcidlink</span>0000-0002-9908-0367
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Markus Wenzel <span class="ltx_ERROR undefined" id="id6.1.id1">\orcidlink</span>0000-0002-6540-1476
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Eric L. Wisotzky  <span class="ltx_ERROR undefined" id="id7.1.id1">\orcidlink</span>0000-0001-5731-7058
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Galip Ümit Yolcu  <span class="ltx_ERROR undefined" id="id8.1.id1">\orcidlink</span>0009-0004-1358-8955
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sebastian Lapuschkin <span class="ltx_ERROR undefined" id="id9.1.id1">\orcidlink</span>0000-0002-0762-7258
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jackie Ma <span class="ltx_ERROR undefined" id="id10.1.id1">\orcidlink</span>0000-0002-2268-1690
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Peter Eisert <span class="ltx_ERROR undefined" id="id11.1.id1">\orcidlink</span>0000-0001-8378-4805
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mikael Lindvall <span class="ltx_ERROR undefined" id="id12.1.id1">\orcidlink</span>0009-0001-7457-4002
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Adam Porter
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wojciech Samek <span class="ltx_ERROR undefined" id="id13.1.id1">\orcidlink</span>0000-0002-6283-3265
</span><span class="ltx_author_notes">22</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id14.id1">In the realm of dermatological diagnoses, where the analysis of dermatoscopic and microscopic skin lesion images is pivotal for the accurate and early detection of various medical conditions, the costs associated with creating diverse and high-quality annotated datasets have hampered the accuracy and generalizability of machine learning models. We propose an innovative unsupervised augmentation solution that harnesses Generative Adversarial Network (GAN) based models and associated techniques over their latent space to generate controlled “semi-automatically-discovered” semantic variations in dermatoscopic images. We created synthetic images to incorporate the semantic variations and augmented the training data with these images. With this approach, we were able to increase the performance of machine learning models and set a new benchmark amongst non-ensemble based models in skin lesion classification on the HAM10000 dataset; and used the observed analytics and generated models for detailed studies on model explainability, affirming the effectiveness of our solution.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Generative Adversarial Network Image Synthesis Dermatoscopy
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The application of artificial intelligence (AI) and machine learning (ML) in the medical domain has garnered substantial interest due to its potential to aid health practitioners in diagnosing conditions, predicting patient outcomes, and personalizing patient care. In dermatology, AI and ML have demonstrated superior performance compared to dermatologists in analyzing dermatoscopy images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib10" title="">10</a>]</cite>.
AI/ML models can process vast quantities of images rapidly, assisting dermatologists in making faster and more accurate diagnoses, thereby improving patient care, and potentially saving lives. However, the success of such AI/ML models is fundamentally limited by the lack of availability of datasets with sufficient variations to reflect semantic occurrences in the real world. Additionally, privacy concerns and regulatory constraints pose a major hindrance towards procuring additional annotated medical image datasets, making it necessary to explore alternate ways of synthesizing these image variants in a reliable manner, while ensuring the photorealism and fidelity of the generated images.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In the domain of medical imaging, the concept of synthetic data generation has manifested remarkable strides across various disciplines and applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib24" title="">24</a>]</cite>. Image synthesis, particularly through methods such as GANs and diffusion models, makes it possible to augment low-volume training data. These generative approaches have also been explored within the realms of dermatoscopy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib4" title="">4</a>]</cite> and histopathology diagnostics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">However, existing methods for developing transformations in the GAN latent space predominantly rely on classification models to ensure the generated images have specific attributes. While these models have been instrumental in driving advances in image synthesis and manipulation, they come with significant drawbacks. Classification-based methods require large amounts of labeled data, which are often difficult to obtain due to privacy concerns, regulatory constraints, and the high costs associated with manual annotation. This reliance on labeled data can severely limit the scalability and applicability of these models, particularly for medical data modalities like dermatoscopy where annotated datasets for various style variations are scarce. Moreover, since classification models are constrained to predefined categories of semantics, the scope of transformations that can be learned is considerably restricted. This results in dependence on domain experts to identify semantics, with the added costs of procuring and annotating images with such semantics.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we present a novel approach towards developing variations in medical images using this unsupervised method based on the latent space of GANs. Our approach leverages the capabilities of two advanced GAN models: StyleGAN2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib22" title="">22</a>]</cite> and HyperStyle <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib5" title="">5</a>]</cite>. Initially, we train the StyleGAN2 model on a comprehensive dataset of dermatoscopic images to generate high-quality synthetic images. Following this, we employ HyperStyle for GAN inversion, optimizing latent features extracted from real images. We then implement closed-form factorization to identify meaningful and orthogonal latent semantic directions within the latent space. Finally, we validate and refine these directions to ensure they correspond to human-understandable and domain-relevant transformations. Our research extends beyond the realm of image generation, addressing the crucial need for evaluation metrics in the context of synthetic skin lesion images. We assess the perceptual similarity of the generated images using state-of-the-art metrics such as the <span class="ltx_glossaryref" title="">Learned Perceptual Image Patch Similarity (LPIPS)</span>.
These metrics provide a quantitative foundation for evaluating the fidelity of synthetic images in comparison to their real counterparts.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To further show the efficacy of our approach, we train classification models on the augmented dataset, achieving state-of-the-art performance in lesion classification. This pipeline not only mitigates the challenges associated with traditional classification models but can potentially enhance the scalability, efficiency, and interpretability of transformation development in medical image analysis.
Thus, this research work has three important contributions:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Through generating high-fidelity synthetic skin lesion images, we pioneer the application of advanced GAN models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib5" title="">5</a>]</cite>, and explore the effectiveness of these models in capturing nuanced details and variations in skin lesions.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">By identifying transformations relevant to the skin lesion domain, we contribute to the field of unsupervised transformation development. These transformations are crucial for data augmentation and enhancing the diversity of synthetic skin lesion images.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We demonstrate the practical impact of synthetic data in improving the performance and explainability of machine learning models for skin lesion analysis. The transformed images significantly contributed to the training of a skin lesion classification model, resulting in a notable increase in accuracy compared to conventional datasets.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In the following sections, we describe our method in greater detail, apply it to a case study, report on the result, and discuss potential future work.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Our Approach</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Background</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">GANs have revolutionized the field of medical imaging by providing innovative augmentation-driven solutions to data scarcity and enhancing the quality of synthetic medical images. These GAN-based solutions have had a particular impact in domains such as radiology, pathology, and dermatology, where obtaining high-quality labeled data is often challenging due to privacy concerns, regulatory constraints, and the high cost of manual annotation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib38" title="">38</a>]</cite>. In dermatoscopy, GANs have shown significant promise in synthesizing skin lesion images, which can be used to augment existing datasets and improve the performance of diagnostic models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib9" title="">9</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">A typical GAN architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib18" title="">18</a>]</cite> consists of two neural networks: the generator and the discriminator which work collaboratively through an adversarial training process. The generator creates new data samples, while the discriminator evaluates them against real data to train the generator to produce images aimed at being indistinguishable from real images. This adversarial process continues until the generator produces high-quality realistic images.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Our approach leverages two state-of-the-art GAN models: StyleGAN2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib22" title="">22</a>]</cite> and HyperStyle <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib5" title="">5</a>]</cite>. StyleGAN2 stands out due to its architectural innovations, which include redesigned generator normalization, progressive growing, and the introduction of a style-based generator architecture. These enhancements enable the generation of highly realistic and detailed images by allowing the model to control different levels of detail through the latent space. Compared to Variational Autoencoders (VAEs), StyleGAN2 generally produces higher quality images with sharper details and more coherent structures. While VAEs are effective for generating diverse samples, they often suffer from blurrier outputs. On the other hand, compared to conditional diffusion models, StyleGAN2 typically achieves faster generation times and requires less computational resources, as diffusion models often involve iterative processes that are more computationally intensive. These advantages make StyleGAN2 particularly suitable for applications requiring high fidelity and variability of the generated images. On the other hand, HyperStyle focuses on the challenge of image inversion, which involves mapping real images into the latent space of a GAN which is used by the generator to manipulate the image. HyperStyle employs a hybrid approach that combines the strengths of encoder- and optimization-based inversion techniques. By balancing image reconstruction and image editability, HyperStyle allows for accurate and flexible modifications of real images. This makes it a powerful tool for tasks that require fine-grained control over image attributes, such as generating synthetic variations of medical images for training data augmentation. Together, these models provide a robust framework for our unsupervised transformation pipeline, enabling us to generate high-quality synthetic images using inverted codes from images obtained in the real world.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">Towards the final step of controlled augmentation generation, existing medical imaging research for developing transformations in the GAN latent space predominantly rely on classification models. While these models have been instrumental in driving advances in image synthesis and manipulation, they come with significant drawbacks, as mentioned in  <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S1" title="1 Introduction ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">1</span></a>. To address these concerns, we explore factorizing the latent space of the generator model as an alternative approach to extract semantics in an unsupervised manner. Our proposed pipeline significantly reduces the dependency on scarce and costly labeled data. This unsupervised approach is inherently more scalable, as it can leverage vast amounts of unlabeled data, which is more readily available, thus facilitating the training of models on a broader spectrum of semantic variations.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">The overall augmentation pipeline, detailed in <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S2.SS2" title="2.2 Unsupervised Transformation Pipeline ‣ 2 Our Approach ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.2</span></a>, progresses step-by-step from a set of original images to the final outputs, incorporating semantic variations based on semi-automatically extracted features into the original images. First, we apply closed-form factorization to identify meaningful and orthogonal latent semantic directions within the latent space. Next, we utilize the GAN inversion function to map real images into the latent space accurately. Finally, using the semantic directions extracted through factorization, we produce new variants of the original images based on the identified semantics. This approach allows for the exploration of a broad range of semantic variations without the need for labeled data, ensuring that the synthetic outputs closely resemble the original inputs.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Unsupervised Transformation Pipeline</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The proposed transformation development pipeline (see <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S2.F1" title="In 2.2 Unsupervised Transformation Pipeline ‣ 2 Our Approach ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>) consists of four stages that perform sequential tasks to achieve the goal of unsupervised semantic extraction. The end product is an unsupervised technique for transformation development, enabling the semi-automatic extraction of extensive semantic transformations reflected within a dataset and corresponding domain.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">GAN training:</span> Training a model based on the StyleGAN2 architecture.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Factorization</span> to extract eigenvectors of maximal variance (Transformations): This crucial step identifies meaningful, orthogonal latent semantic directions within the latent space with closed-form factorization.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.1">GAN inversion:</span> We train a HyperStyle-based GAN inversion model that comprises encoder and optimizer units, with the goal of obtaining latent features corresponding to real images.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S2.I1.i4.p1">
<p class="ltx_p" id="S2.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i4.p1.1.1">Identify relevant transformations:</span> The orthogonal latent semantic directions from the previous step correspond to a mix of human-understandable and domain-related concepts. Being able to translate the directions to these concepts contributes to the interpretability of the generated images. Besides, not all the directions produce relevant and unique transformations (i.e., multiple directions may produce very similar transformations). A validation step is incorporated to ensure that only relevant transformations are considered.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="277" id="S2.F1.g1" src="extracted/5907388/Figures/HHIpaper_flow.png" width="509"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">Transformation development pipeline</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">In the following sub-sections, we will elaborate on our approach in the context of our case study within the dermatoscopy domain.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>GAN Training</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">We trained the GAN with 10,758 images predominantly from the HAM10000 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib36" title="">36</a>]</cite> used in the ISIC 2018 challenge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib12" title="">12</a>]</cite>. The 10k images from HAM10000 stem from various populations and modalities, with each image annotated with specific diagnoses such as melanoma (MEL), melanocytic nevus (NV), basal cell carcinoma (BCC), actinic keratosis/Bowen’s disease (AKIEC), benign keratosis (BKL), dermatofibroma (DF), and vascular lesion (VASC).</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.p2.1">To increase the variability of transformations, we incorporated additional datasets into the HAM10000 dataset. We selected 368 images from the Fitzpatrick dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib19" title="">19</a>]</cite>, filtering out images outside the lesion domain; and utilized 390 dermatoscopic images from the Seven-Point Checklist Dermatology dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib23" title="">23</a>]</cite>, ensuring that only non-augmented images were included. Additionally, we considered images from the Stanford University dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib13" title="">13</a>]</cite>, but found that the images contained demarcations and augmentations, leading us to exclude them from our dataset. Demarcated images present visible boundaries and markers that can introduce biases into the training process of the StyleGAN. These markers can disrupt the network’s ability to learn the underlying patterns and features of the data, leading to sub-optimal generation of synthetic images. Furthermore, augmentations may alter the natural appearance of the images, causing the model to learn and replicate these alterations rather than the true characteristics of the original images. Therefore, to ensure the integrity and quality of our training data, we opted to exclude this dataset.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p3">
<p class="ltx_p" id="S2.SS2.SSS1.p3.1">We used the StyleGAN2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib22" title="">22</a>]</cite> architecture for GAN training. We formatted the dataset in LMDB (Lightning Memory-Mapped Database) to take advantage of its speed and low memory usage, which makes it suitable for large-scale data processing. The images were standardized to a fixed resolution of 512 x 512 pixels prior to training to enable an optimal generative quality of the augmented images intended for training lesion classification models at the same resolution.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p4">
<p class="ltx_p" id="S2.SS2.SSS1.p4.1">Through the training process, we fine-tuned the StyleGAN2 model to generate high-quality synthetic skin lesion images. We used the StyleGAN2 architecture, which features redesigned generator normalization, progressive growing, and style-based synthesis blocks to enhance image quality. We kept most of the design details unchanged from the original implementation, including the dimensionality of Z and W spaces (512) and mapping network architecture (8 fully connected layers, 100× lower learning rate). Using Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib25" title="">25</a>]</cite> with a learning rate of 0.001 and a batch size of 64, the training spanned 450k iterations, with data augmentation techniques such as random cropping and horizontal flipping to enhance the robustness of the model. We performed the training on a stack of 4 NVIDIA RTX 8000 GPUs in a distributed setup.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p5">
<p class="ltx_p" id="S2.SS2.SSS1.p5.1">The model’s performance was evaluated using the Fréchet Inception Distance (FID) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib20" title="">20</a>]</cite>, which yielded a score of approximately 3.7, indicating a high level of conformance in distributional similarity between the generated and real images. <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S2.F2" title="In 2.2.1 GAN Training ‣ 2.2 Unsupervised Transformation Pipeline ‣ 2 Our Approach ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a> showcases samples of synthetically generated dermatoscopic skin images, demonstrating the photorealism achieved by our trained model.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="117" id="S2.F2.g1" src="extracted/5907388/Figures/Generated_Images.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">Samples of synthetically generated skin lesions.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Latent-Space Factorization</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">As part of this sub-pipeline, to extract semantic directions or transformations, we employed closed-form factorization<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib22" title="">22</a>]</cite> within the latent space (z, w) of the generator to identify meaningful semantic directions. In other words, we analyzed the generator’s internal structure to uncover directions in the latent space that correspond to groupings of maximal semantic variance in the generated images.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.p2.1">The process starts with the extraction of specific weights from various layers of the generator. These weights capture the learned features of the model and are critical for generating high-quality images. We then constructed a weight matrix <math alttext="W" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p2.1.m1.1"><semantics id="S2.SS2.SSS2.p2.1.m1.1a"><mi id="S2.SS2.SSS2.p2.1.m1.1.1" xref="S2.SS2.SSS2.p2.1.m1.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.1.m1.1b"><ci id="S2.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p2.1.m1.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.1.m1.1c">W</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p2.1.m1.1d">italic_W</annotation></semantics></math> that consolidates the weight information from the selected layers. This matrix encapsulates the combined influence of these layers on the generated images.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p3">
<p class="ltx_p" id="S2.SS2.SSS2.p3.6">Next, we applied Singular Value Decomposition (SVD) to the weight matrix <math alttext="W" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p3.1.m1.1"><semantics id="S2.SS2.SSS2.p3.1.m1.1a"><mi id="S2.SS2.SSS2.p3.1.m1.1.1" xref="S2.SS2.SSS2.p3.1.m1.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p3.1.m1.1b"><ci id="S2.SS2.SSS2.p3.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p3.1.m1.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p3.1.m1.1c">W</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p3.1.m1.1d">italic_W</annotation></semantics></math>. SVD decomposes <math alttext="W" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p3.2.m2.1"><semantics id="S2.SS2.SSS2.p3.2.m2.1a"><mi id="S2.SS2.SSS2.p3.2.m2.1.1" xref="S2.SS2.SSS2.p3.2.m2.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p3.2.m2.1b"><ci id="S2.SS2.SSS2.p3.2.m2.1.1.cmml" xref="S2.SS2.SSS2.p3.2.m2.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p3.2.m2.1c">W</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p3.2.m2.1d">italic_W</annotation></semantics></math> into three components: <math alttext="U" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p3.3.m3.1"><semantics id="S2.SS2.SSS2.p3.3.m3.1a"><mi id="S2.SS2.SSS2.p3.3.m3.1.1" xref="S2.SS2.SSS2.p3.3.m3.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p3.3.m3.1b"><ci id="S2.SS2.SSS2.p3.3.m3.1.1.cmml" xref="S2.SS2.SSS2.p3.3.m3.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p3.3.m3.1c">U</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p3.3.m3.1d">italic_U</annotation></semantics></math> (an orthogonal matrix), <math alttext="E" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p3.4.m4.1"><semantics id="S2.SS2.SSS2.p3.4.m4.1a"><mi id="S2.SS2.SSS2.p3.4.m4.1.1" xref="S2.SS2.SSS2.p3.4.m4.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p3.4.m4.1b"><ci id="S2.SS2.SSS2.p3.4.m4.1.1.cmml" xref="S2.SS2.SSS2.p3.4.m4.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p3.4.m4.1c">E</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p3.4.m4.1d">italic_E</annotation></semantics></math> (a diagonal matrix containing singular values), and <math alttext="V^{T}" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p3.5.m5.1"><semantics id="S2.SS2.SSS2.p3.5.m5.1a"><msup id="S2.SS2.SSS2.p3.5.m5.1.1" xref="S2.SS2.SSS2.p3.5.m5.1.1.cmml"><mi id="S2.SS2.SSS2.p3.5.m5.1.1.2" xref="S2.SS2.SSS2.p3.5.m5.1.1.2.cmml">V</mi><mi id="S2.SS2.SSS2.p3.5.m5.1.1.3" xref="S2.SS2.SSS2.p3.5.m5.1.1.3.cmml">T</mi></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p3.5.m5.1b"><apply id="S2.SS2.SSS2.p3.5.m5.1.1.cmml" xref="S2.SS2.SSS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p3.5.m5.1.1.1.cmml" xref="S2.SS2.SSS2.p3.5.m5.1.1">superscript</csymbol><ci id="S2.SS2.SSS2.p3.5.m5.1.1.2.cmml" xref="S2.SS2.SSS2.p3.5.m5.1.1.2">𝑉</ci><ci id="S2.SS2.SSS2.p3.5.m5.1.1.3.cmml" xref="S2.SS2.SSS2.p3.5.m5.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p3.5.m5.1c">V^{T}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p3.5.m5.1d">italic_V start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT</annotation></semantics></math> (the transpose of an orthogonal matrix). The columns of <math alttext="V" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p3.6.m6.1"><semantics id="S2.SS2.SSS2.p3.6.m6.1a"><mi id="S2.SS2.SSS2.p3.6.m6.1.1" xref="S2.SS2.SSS2.p3.6.m6.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p3.6.m6.1b"><ci id="S2.SS2.SSS2.p3.6.m6.1.1.cmml" xref="S2.SS2.SSS2.p3.6.m6.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p3.6.m6.1c">V</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p3.6.m6.1d">italic_V</annotation></semantics></math> (the eigenvectors) represent distinct directions in the latent space along which the data varies the most. These eigenvectors correspond to semantic transformations that can be applied to the latent vectors.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p4">
<p class="ltx_p" id="S2.SS2.SSS2.p4.1">By projecting latent vectors along these eigenvectors, we can create a variety of image transformations the magnitude of which can be varied through the eigenvalue over that direction. Through this process, we were able to uncover transformations for size, texture, geometric properties and background properties of the skin lesion, amongst others. This method allows for extracting subtle variations and intricate features, enhancing the richness and diversity of the synthetic images. This is a pivotal step in our unsupervised transformation development pipeline and empowers testers and domain experts alike to navigate the landscape of transformation variations without relying on extensive manual classification models as discussed earlier.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>GAN Inversion</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">As part of the GAN inversion sub-pipeline implementation, we trained a HyperStyle <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib5" title="">5</a>]</cite> based inversion model comprising dedicated encoder and optimizer units towards the task of latent code (w-space) computation of any image in the real world. For our implementation, we used the e4e (Encoder for Editing) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib35" title="">35</a>]</cite> encoder, which is a specific type of encoder used to map real images into the latent space of the StyleGAN2 generator.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS3.p2">
<p class="ltx_p" id="S2.SS2.SSS3.p2.1">The e4e encoder was trained on the same dataset used for GAN training in the first phase of the pipeline. The training process involved minimizing the L2 loss (Mean Squared Error), a common metric for measuring the difference between the predicted output of the encoder and the actual target values. We achieved an L2 loss of 0.009, indicating the encoder’s success towards distilling and capturing meaningful representations from the dataset.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS3.p3">
<p class="ltx_p" id="S2.SS2.SSS3.p3.1">Following the encoder training, we trained the HyperStyle component using the same dataset. The training process for the HyperStyle module resulted in an L2 loss of 0.002, highlighting its efficacy in refining the latent space for accurate image reconstruction. This low L2 loss value underscores the model’s proficiency in transforming latent features into realistic skin lesion images.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS3.p4">
<p class="ltx_p" id="S2.SS2.SSS3.p4.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S2.F3" title="In 2.2.3 GAN Inversion ‣ 2.2 Unsupervised Transformation Pipeline ‣ 2 Our Approach ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a> showcases the inversion results of several original images. The faithful reconstruction achieved through the synergy between the encoder and HyperStyle components demonstrates the success of the inversion process in capturing intricate semantic properties and detail from the original images.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="104" id="S2.F3.g1" src="extracted/5907388/Figures/Inverted_Images_in_one_row.png" width="419"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S2.F3.3.2" style="font-size:90%;">Two pairs of original (left in each case) and HyperStyle inverted images (right in each case).</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.4 </span>Identify Relevant Transformations</h4>
<div class="ltx_para" id="S2.SS2.SSS4.p1">
<p class="ltx_p" id="S2.SS2.SSS4.p1.1">As part of this phase, we utilized a human-in-the-loop approach to systematically review and validate the semantic directions identified during the closed-form factorization phase. To facilitate this process, we developed a user-friendly dashboard by adapting and adding features to the SeFa (Semantic Factorization) dashboard <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib33" title="">33</a>]</cite>. The dashboard allows the interactive exploration and validation of the semantic transformations. After our modifications, the dashboard supports functionalities such as uploading or browsing images from the dataset, selecting a semantic direction, adjusting the magnitude of the transformation, and visually reviewing the outcome of the applied transformation. This interface is crucial for the interpretation and validation of the semantic meanings and of the relevance of the latent directions identified during factorization (corresponding to this direction and magnitude).</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS4.p2">
<p class="ltx_p" id="S2.SS2.SSS4.p2.1">To transition from factorization to identifying transformations, we implemented a method to apply the extracted eigenvectors of maximal variance to the latent vectors of real images. This process involves the following steps:</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS4.p3">
<ol class="ltx_enumerate" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p" id="S2.I2.i1.p1.1">We mapped dermatoscopic images into the latent space with the previously trained HyperStyle GAN inversion model.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p" id="S2.I2.i2.p1.1">The identified semantic directions (eigenvectors) are then applied to the latent vectors of these images. By adjusting the magnitude of the directions, we can modulate specific attributes of the images, such as size, pigmentation, and texture of skin lesions.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I2.i3.p1">
<p class="ltx_p" id="S2.I2.i3.p1.1">We used the dashboard to systematically review the transformations, ensuring they are meaningful and relevant to the domain. Multiple directions might produce similar transformations, so this step ensures that only unique and significant transformations are considered.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S2.SS2.SSS4.p4">
<p class="ltx_p" id="S2.SS2.SSS4.p4.1">At the end of this process, we identified 13 distinct transformations tailored to the semantic variance permutations of the skin lesion domain. These transformations include changing the skin lesion size, altering the pigmentation of skin lesions, modifying the overall pigmentation of the skin, adjusting the texture and shape of skin lesions, etc. Each transformation represents a modulating force applied to the latent vector of the original image, contributing to a diverse range of augmentation possibilities. This integration of humans-in-the-loop for the semi-automatic semantic extraction is essential for generating diverse transformations that are both relevant and accurate, thereby enabling training robust machine learning models that can generalize well to real-world data. <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S2.F4" title="In 2.2.4 Identify Relevant Transformations ‣ 2.2 Unsupervised Transformation Pipeline ‣ 2 Our Approach ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a> shows exemplary original images and their corresponding transformations (additional examples of the transformed images are included the Appendix).</p>
</div>
<figure class="ltx_figure" id="S2.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="300" id="S2.F4.sf1.g1" src="extracted/5907388/Figures/BackgroundChanges.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S2.F4.sf1.3.2" style="font-size:90%;">Skin pigmentation change is applied to the original image (left) which resulted in the synthetic image (right).</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="301" id="S2.F4.sf2.g1" src="extracted/5907388/Figures/PigmentChanges.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S2.F4.sf2.3.2" style="font-size:90%;">The lesion pigmentation change is applied to the original image (left) which resulted in the synthetic image (right).</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F4.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="297" id="S2.F4.sf3.g1" src="extracted/5907388/Figures/SizeChanges.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S2.F4.sf3.3.2" style="font-size:90%;">Skin lesion size change is applied to the original image (left) which resulted in the synthetic image (right).</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S2.F4.3.2" style="font-size:90%;">Examples of original images and their generated/synthetic images after the transformations have been applied to the original images.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Classifier Training Enhancement With Synthetic Data</h3>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Experimental Setup</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">We compared the predictive performance of a skin lesion classification model trained on data augmented with synthetic images with a baseline. As baseline scenario, we trained on the original HAM10000 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib36" title="">36</a>]</cite> dermatoscopy training dataset split and evaluated the performance of the model using the original test dataset split. HAM10000 includes 10,015 high-quality dermatoscopic images in the training set and 1,512 image in the test set. Each image is labeled with one specific diagnosis (see <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S2.SS2.SSS1" title="2.2.1 GAN Training ‣ 2.2 Unsupervised Transformation Pipeline ‣ 2 Our Approach ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.2.1</span></a> above). Class label distribution is highly imbalanced in training and test set, with NV being over-represented with a share of 60 % respectively 67 %, while the other six classes share the remaining fraction to varying degrees.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p2">
<p class="ltx_p" id="S2.SS3.SSS1.p2.1">To train the model with additional synthetic images, we first augmented the original training dataset with synthetic data as follows: we generated new images from the original training dataset using five out of thirteen transformations that we had identified. The five transformations correspond to Size and Pigment Variation (SPV), Size Variation (SV), Background Color Variation (BCV), Geometric Variation (GV), and to Positional Shift (PS). The other transformations we excluded were variants of these five transformations (e.g., they correspond to different semantic layers in the generator).</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p3">
<p class="ltx_p" id="S2.SS3.SSS1.p3.1">In total, we obtained five times the amount of the original dataset (total of 50,075 samples). Since we work with a much larger training dataset after the augmentation in comparison to the baseline scenario, we ensured that the observed classification performance change results not solely from the much larger dataset and thus longer training, but from the higher variety in the training data. For this reason, we employed early stopping (with a relatively high criterion of 25 epochs prior to initiating the early stop; assuring that the model would not profit from longer training) and created multiple “synthetically augmented” models by varying the number of synthetic images used for augmenting the training datasets. Specifically, we randomly selected 400, 800, 1200, 1600, and 2000 of the generated synthetic images from each of the five transformations, and augmented the original training dataset with 2000, 4000, 6000, 8000, and 10000 images respectively. Note that the selections of synthetic images for each augmented dataset were done independently; i.e., the 2000 additional images were not a subset to the 4000 additional images.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p4">
<p class="ltx_p" id="S2.SS3.SSS1.p4.1">To ensure that we are only adding “good” synthetic images, we also generated a “filtered” augmented training dataset by removing the synthetic images which the unfiltered synthetically augmented models classified incorrectly. We filtered out 136 (6%), 367 (9%), 274 (4.5%), 916 (11.45%), and 505 (5%) images from the 2000, 4000, 6000, 8000, 10000 augmented images respectively.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p5">
<p class="ltx_p" id="S2.SS3.SSS1.p5.1">For each augmented training datasets, we trained a classification model. This results in 10 synthetically augmented models: five models were trained using the unfiltered augmented datasets and five models were trained with the filtered augmented datasets. Hereafter, we will refer to the models trained using the unfiltered augmented dataset as SA-2k, SA-4k, SA-6k SA-8k, SA-10k (when the original dataset was augmented with the 2000, 4000, 6000, 8000, and 10000 synthetic images respectively) and the models trained using the filtered augmented dataset as SA-2k-filter, SA-4k-filter, SA-6k-filter, SA-8k-filter and SA-10k-filter.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Model, Task, and Training</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.1">We employed a DenseNet121 (8M parameters) and a DenseNet169 (14M parameters), initialized with weights pretrained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib14" title="">14</a>]</cite>, for multi-class classification. The two architectures enabled us to compare the impact of augmenting the training dataset with synthetic images across varying architecture complexity. Because the label distributions are highly imbalanced, we used weighted oversampling to balance class distributions within training batches. Additional basic transformations (horizontal/vertical flip, cutout) and a dropout rate of 0.1 were employed. We used an Adam optimizer with a learning rate of 1e-5 and weight decay of 1e-4 and trained for 100 epochs, while initiating early stopping when the performance on the validation split did not improve for 25 epochs.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results and Discussion</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Transformations Developed</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To compare our transformed images, we used the <span class="ltx_glossaryref" title="">LPIPS</span> metric for evaluating the transformed images as it provides a more nuanced 1:1 image-level comparison than FID (which is a measure for comparison of overall image distributions). The <span class="ltx_glossaryref" title="">LPIPS</span> score we employed is conditioned on the last three layers of the AlexNet architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib26" title="">26</a>]</cite> trained on the ImageNet dataset and serves to quantify perceptual similarity by comparing deep feature representations extracted across the layers, empirically proven to align with human perceptual judgments.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">We calculated the <span class="ltx_glossaryref" title="">LPIPS</span> metrics for the five transformations used in augmenting the training datasets (see <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S3.T1" title="In 3.1 Transformations Developed ‣ 3 Results and Discussion ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>.) <span class="ltx_glossaryref" title="">LPIPS</span> score ranges between 0 to 1 where a lower <span class="ltx_glossaryref" title="">LPIPS</span> score denotes higher perceptual similarity.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">We observe scores close to or lower than 0.1 for all our selected transformations. In general, “wayward or low-fidelity” transformations exhibited scores &gt; 0.2, which was the threshold used in selecting transformations for our task. Although the scores for our selected transformations show minimal perceptual change, we acknowledge the importance of domain expert validation to enhance confidence in the fidelity of our transformed images (note that downstream classifiers were always tested on non-modified images). Additionally, in future work, we intend to condition the metric on an AlexNet architecture trained specifically on an unbiased skin lesion dataset to ensure higher resonance in comparison over the feature space.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.2.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_glossaryref" style="font-size:90%;" title="">LPIPS</span><span class="ltx_text" id="S3.T1.3.2" style="font-size:90%;"> metrics for the five transformations employed in training the classification model.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.4.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T1.4.1.1.1">Transformation</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.4.1.1.2">SPV</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.4.1.1.3">SV</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.4.1.1.4">BCV</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.4.1.1.5">GV</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.4.1.1.6">PS</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.4.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.4.2.1.1"><span class="ltx_glossaryref" title="">LPIPS</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.4.2.1.2">0.101</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.4.2.1.3">0.098</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.4.2.1.4">0.098</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.4.2.1.5">0.098</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.4.2.1.6">0.099</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Evaluation</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We evaluated the model performance on the 1,512 images (512 x 512 pixels) of the original HAM10000 test split, which were neither transformed nor seen during training, using balanced multi-class accuracy.
First, we compared our results with the existing benchmark<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib1" title="">1</a>]</cite> (‘Task 3: Lesion Diagnosis’) in the ISIC2018 challenge. Our best performing model was based on the DenseNet169 architecture, synthetically augmented with 6000 additional synthetic images (60 % of the original training dataset), achieved a balanced accuracy of 0.856 (see <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S3.T2" title="In 3.2 Evaluation ‣ 3 Results and Discussion ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>). Comparing with other models evaluated in the challenge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib1" title="">1</a>]</cite>, we ranked 3rd on the evaluation metrics, with only the two ensemble based methods achieving a higher average balanced accuracy of 0.885 (‘Top 10 Models Averaged’) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib29" title="">29</a>]</cite> and 0.856 (‘Large Ensemble with heavy multi-cropping and loss weighting’) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib17" title="">17</a>]</cite> respectively.
Our model even surpasses the larger DenseNet201 on rank 4 in the challenge with 0.815 (‘densenet’ submitted by Li and Li <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib1" title="">1</a>]</cite>).</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Then, we compared the performance of the synthetically augmented classification models with the baseline model.
In <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S3.T2" title="In 3.2 Evaluation ‣ 3 Results and Discussion ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, we can observe that all synthetically augmented models outperform the baseline model by 1.9% to 3.5%. However, the performance does not always increase with the number of synthetic images added to the original dataset and seems to plateau after adding 6,000 images to the original training dataset. We also observe that the filtered method helps to increase the performance gain only to a certain point. This suggests that more research is needed to understand the nature of the synthetic images that increase and/or decrease the models’ performance.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T2.2.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S3.T2.3.2" style="font-size:90%;">Classification performance improvement of the synthetically augmented models in comparison to the baseline model (trained with real training data only), measured with weighted multi-class accuracy.
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.4.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T2.4.1.1.1">Architecture</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T2.4.1.1.2">Filter</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T2.4.1.1.3">Baseline</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T2.4.1.1.4">2k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T2.4.1.1.5">4k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T2.4.1.1.6">6k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T2.4.1.1.7">8k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.4.1.1.8">10k</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.4.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.4.2.1.1">DenseNet121</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.4.2.1.2">No</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.4.2.1.3">81.9%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.4.2.1.4">+1.9%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.4.2.1.5">+2.2%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.4.2.1.6">+2.6%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.4.2.1.7">+1.2%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.2.1.8">+1.8%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.3.2.1">DenseNet169</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.3.2.2">No</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.3.2.3">82.1%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.3.2.4">+1.9%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.3.2.5">+2.2%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.3.2.6">+3.5%</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.3.2.7">+3.3%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.3.2.8">+3.1%</td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.4.3">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T2.4.4.3.1">DenseNet169</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T2.4.4.3.2">Yes</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T2.4.4.3.3">82.1%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T2.4.4.3.4">+2.3%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T2.4.4.3.5">+3.3%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T2.4.4.3.6">+3.5%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T2.4.4.3.7">+3.1%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.4.4.3.8">+2.8%</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">From here on we will report only on the DenseNet169, as it consistently outperforms the smaller DenseNet121. <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S3.F5" title="In 3.2 Evaluation ‣ 3 Results and Discussion ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a> shows the recall results for each diagnostic class for the un/filtered synthetically augmented models in comparison to the baseline model.
Synthetically augmented (un/filtered) models show a better recall for the classes MEL, BCC, and DF, when considering all sizes of augmented data. Note that all three are underrepresented classes, which shows our synthetic augmentations are fit to counteract class imbalances. The synthetically augmented models have better recall performance for all classes, except for BKL and VASC, when only considering the SA-6k and SA-6k-filter. <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S3.F6" title="In 3.2 Evaluation ‣ 3 Results and Discussion ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a> shows the confusion matrices for the baseline, un/filtered synthetically augmented models (using 6000 augmented images). <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S3.T3" title="In 3.2 Evaluation ‣ 3 Results and Discussion ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a> shows the area under the curve of the receiver operating characteristic (AUC ROC) for each class for the same models. The table demonstrates that both optimized models (SA-6k and SA-6k-filter) outperform the baseline model in almost all classes. Furthermore, the average AUC ROC for SA-6k is 0.945, higher than the baseline’s 0.924, indicating a general performance boost. Meanwhile, the average AUC ROC for SA-6k-filter is the highest at 0.947, suggesting it is the most effective model overall. This indicates that the additional enhancements and filtering techniques applied in SA-6k-filter lead to the most reliable and accurate model for distinguishing between different types of skin lesions.</p>
</div>
<figure class="ltx_figure" id="S3.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="479" id="S3.F5.sf1.g1" src="extracted/5907388/Figures/model-performance-non-filter.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.sf1.3.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F5.sf1.4.2" style="font-size:90%;">Models trained on <span class="ltx_text ltx_font_bold" id="S3.F5.sf1.4.2.1">unfiltered</span> augmented datasets of varying size compared to baseline.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="476" id="S3.F5.sf2.g1" src="extracted/5907388/Figures/model-performance-filter.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.sf2.3.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F5.sf2.4.2" style="font-size:90%;">Models trained on <span class="ltx_text ltx_font_bold" id="S3.F5.sf2.4.2.1">filtered</span> augmented datasets of varying size compared to baseline.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.3.2" style="font-size:90%;">Predictive performance (recall) of the synthetically augmented models compared to the baseline model.</span></figcaption>
</figure>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T3.2.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S3.T3.3.2" style="font-size:90%;">AUC ROC per class for baseline, and un/filtered models (best in bold font).</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T3.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.4.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T3.4.1.1.1" style="padding-left:5.0pt;padding-right:5.0pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.4.1.1.2" style="padding-left:5.0pt;padding-right:5.0pt;">MEL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.4.1.1.3" style="padding-left:5.0pt;padding-right:5.0pt;">NV</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.4.1.1.4" style="padding-left:5.0pt;padding-right:5.0pt;">BCC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.4.1.1.5" style="padding-left:5.0pt;padding-right:5.0pt;">AKIEC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.4.1.1.6" style="padding-left:5.0pt;padding-right:5.0pt;">BKL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.4.1.1.7" style="padding-left:5.0pt;padding-right:5.0pt;">DF</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T3.4.1.1.8" style="padding-left:5.0pt;padding-right:5.0pt;">VASC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.4.1.1.9" style="padding-left:5.0pt;padding-right:5.0pt;">Average</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.4.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.4.2.1.1" style="padding-left:5.0pt;padding-right:5.0pt;">Baseline</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.2.1.2" style="padding-left:5.0pt;padding-right:5.0pt;">0.832</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.2.1.3" style="padding-left:5.0pt;padding-right:5.0pt;">0.938</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.2.1.4" style="padding-left:5.0pt;padding-right:5.0pt;">0.961</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.2.1.5" style="padding-left:5.0pt;padding-right:5.0pt;">0.951</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.2.1.6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.4.2.1.6.1">0.945</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.2.1.7" style="padding-left:5.0pt;padding-right:5.0pt;">0.891</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.4.2.1.8" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.4.2.1.8.1">0.950</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.2.1.9" style="padding-left:5.0pt;padding-right:5.0pt;">0.924</td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S3.T3.4.3.2.1" style="padding-left:5.0pt;padding-right:5.0pt;">SA-6k</th>
<td class="ltx_td ltx_align_center" id="S3.T3.4.3.2.2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.4.3.2.2.1">0.908</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.3.2.3" style="padding-left:5.0pt;padding-right:5.0pt;">0.948</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.3.2.4" style="padding-left:5.0pt;padding-right:5.0pt;">0.972</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.3.2.5" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.4.3.2.5.1">0.973</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.3.2.6" style="padding-left:5.0pt;padding-right:5.0pt;">0.937</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.3.2.7" style="padding-left:5.0pt;padding-right:5.0pt;">0.930</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.4.3.2.8" style="padding-left:5.0pt;padding-right:5.0pt;">0.944</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.3.2.9" style="padding-left:5.0pt;padding-right:5.0pt;">0.945</td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S3.T3.4.4.3.1" style="padding-left:5.0pt;padding-right:5.0pt;">SA-6k-filter</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.4.3.2" style="padding-left:5.0pt;padding-right:5.0pt;">0.893</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.4.3.3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.4.4.3.3.1">0.952</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.4.3.4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.4.4.3.4.1">0.976</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.4.3.5" style="padding-left:5.0pt;padding-right:5.0pt;">0.964</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.4.3.6" style="padding-left:5.0pt;padding-right:5.0pt;">0.938</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.4.3.7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.4.4.3.7.1">0.956</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T3.4.4.3.8" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.4.4.3.8.1">0.950</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.4.3.9" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.4.4.3.9.1">0.947</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S3.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F6.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="S3.F6.sf1.g1" src="extracted/5907388/Figures/confusion_matrix_baseline.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F6.sf1.3.2" style="font-size:90%;">Baseline</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F6.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="S3.F6.sf2.g1" src="extracted/5907388/Figures/confusion_matrix_without_filter.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F6.sf2.3.2" style="font-size:90%;">SA-6K model</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F6.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="S3.F6.sf3.g1" src="extracted/5907388/Figures/confusion_matrix_filtered.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S3.F6.sf3.3.2" style="font-size:90%;">SA-6k-filter model</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S3.F6.3.2" style="font-size:90%;">Confusion matrices, baseline vs. synthetically augmented un/filtered models.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Model Analysis with Explainable AI</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The confusion matrices in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S3.F6" title="Figure 6 ‣ 3.2 Evaluation ‣ 3 Results and Discussion ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">6</span></a> reveal a significant improvement of the model’s ability to correctly classify samples from class MEL.
Whereas the baseline model only classified 54% of true Melanoma samples correctly, the model trained on (filtered) synthetic samples correctly labeled 67% of these samples.
As the baseline model misclassified many true MEL test samples as BKL, which is particularly dangerous as this is a benign class, we further analyze the prediction behavior for this set of test samples.
Specifically, we apply <span class="ltx_glossaryref" title="">Concept Relevance Propagation (CRP)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib3" title="">3</a>]</cite> to compute concept-based explanations for individual predictions.
<span class="ltx_glossaryref" title="">CRP</span> disentangles local explanations into concept-specific explanations.
The concepts are defined by individual neurons in a chosen layer (e.g., last Conv layer) and their relevance scores can be computed with backpropagation-based explainers, for instance <span class="ltx_glossaryref" title="">Layer-wise Relevance Propagation (LRP)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib7" title="">7</a>]</cite>.
The concepts can be visualized in a human-understandable manner by a set of representative samples from a reference dataset, e.g., the training data.
Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S3.F7" title="Figure 7 ‣ 3.3 Model Analysis with Explainable AI ‣ 3 Results and Discussion ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">7</span></a> shows <span class="ltx_glossaryref" title="">CRP</span> explanations for a test sample misclassified as BKL by the baseline model (<em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.1.1">left</em>) but classified correctly by the model augmented with synthetic data (<em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.1.2">right</em>).
While the baseline model is distracted by surroundings (e.g., concept 242), the augmented model uses features easier to interpret and more related to the task, such as the border of the mole (concept 274). We include Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S2.F1a" title="Figure B.1 ‣ B Explanations of the Baseline and the Augmented Models for the class MEL ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">B.1</span></a> in the appendices, which shows explanations of the MEL class for the baseline model. These explanations reveal that the baseline model is not capable of detecting any interesting features which indicate membership to the MEL class, as opposed to the augmented model. Furthermore, to understand the global prediction (sub-)strategies employed by the model, we compute <span class="ltx_glossaryref" title="">Prototypical Concept-based eXplanations (PCX)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib16" title="">16</a>]</cite> for class MEL. These explanations can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S3a" title="C Understanding Prediction Strategies with Prototypical Concept-based Explanations ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F7.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="726" id="S3.F7.sf1.g1" src="x1.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F7.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F7.sf1.3.2" style="font-size:90%;">Baseline</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F7.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="726" id="S3.F7.sf2.g1" src="x2.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F7.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F7.sf2.3.2" style="font-size:90%;">Synthetic (Filtered)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F7.4.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S3.F7.5.2" style="font-size:90%;">CRP analysis for a test sample misclassified as BKL by the baseline model (<em class="ltx_emph ltx_font_italic" id="S3.F7.5.2.1">left</em>), but correctly classified as MEL by the augmented model (<em class="ltx_emph ltx_font_italic" id="S3.F7.5.2.2">right</em>): We show concept-conditional heatmaps for the most relevant concepts for the predictions and concept visualizations with a set of reference images. For interpretability, we zoom into the most relevant regions of reference samples and mask out irrelevant areas.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion and Future Work</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our research has established a robust foundation for the efficient and effective utilization of controlled augmentation using generative models to generate synthetic skin lesion images and consequently more accurate AI classification models. Through our experiments, we have demonstrated significant improvements in model performance for skin lesion classification by using augmented datasets generated in a controlled manner with our implementation. In these experiments, the selection of augmented datasets for training was done by randomly sampling from synthetically generated images. To enhance the efficiency of this approach, in future work, we plan to modify our data selection strategy to be based on clustering characteristics within the latent space, thereby selecting images from which the model stands to learn the most. Additionally, we intend to implement a filtration module prior to augmented data selection, based on the development of fidelity and photorealism metrics and thresholds. To achieve this, we will build on the Learned Perceptual Image Patch Similarity (LPIPS) metric and aim to establish thresholds for the photorealism and fidelity of the augmented datasets selected for model training, thereby preventing unintentional data poisoning.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">We plan to further explore the trade-off between photorealism and editability and investigate other inversion techniques to improve the optimization process. Additionally, future work will focus on enhancing our factorization techniques to produce high-fidelity directions of disentangled semantic variance. While our results currently lead the ISIC leaderboard for non-ensemble-based models, we believe that by shifting to an ensemble-based approach, we can surpass the performance of the leading ensemble-based models.
We believe that our research can pave the way for future advancements in transfer learning and domain adaptation within dermatological diagnoses. We will further explore generalizability to other diagnostic tasks and datasets, as well as higher-dimensional image analysis such as hyperspectral tissue differentiation.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
ISIC challenge. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://challenge.isic-archive.com/leaderboards/2018/" title="">https://challenge.isic-archive.com/leaderboards/2018/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Abhishek, K., Hamarneh, G.: Mask2Lesion: Mask-constrained adversarial skin lesion image synthesis (2019), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1906.05845" title="">https://arxiv.org/abs/1906.05845</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Achtibat, R., Dreyer, M., Eisenbraun, I., Bosse, S., Wiegand, T., Samek, W., Lapuschkin, S.: From attribution maps to human-understandable explanations through concept relevance propagation. Nat. Mach. Intell. <span class="ltx_text ltx_font_bold" id="bib.bib3.1.1">5</span>(9), 1006–19 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Akrout, M., Gyepesi, B., Holló, P., Poór, A., Kincső, B., Solis, S., Cirone, K., Kawahara, J., Slade, D., Abid, L., Kovács, M., Fazekas, I.: Diffusion-based data augmentation for skin disease classification: Impact across original medical datasets to fully synthetic images. In: Mukhopadhyay, A., Oksuz, I., Engelhardt, S., Zhu, D., Yuan, Y. (eds.) Deep Generative Models. pp. 99–109. Springer, Cham (2024). https://doi.org/10.1007/978-3-031-53767-7_10

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Alaluf, Y., Tov, O., Mokady, R., Gal, R., Bermano, A.: HyperStyle: StyleGAN inversion with hypernetworks for real image editing. In: Proc. IEEE/CVF CVPR. pp. 18511–18521. IEEE, NYC, USA (June 2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Aversa, M., Nobis, G., Hägele, M., Standvoss, K., Chirica, M., Murray-Smith, R., Alaa, A.M., Ruff, L., Ivanova, D., Samek, W., Klauschen, F., Sanguinetti, B., Oala, L.: DiffInfinite: Large mask-image synthesis via parallel random patch diffusion in histopathology. In: Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., Levine, S. (eds.) Advances in Neural Information Processing Systems. vol. 36, pp. 78126–78141. Curran Assoc., Inc., NYC, USA (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Bach, S., Binder, A., Montavon, G., Klauschen, F., Müller, K.R., Samek, W.: On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one <span class="ltx_text ltx_font_bold" id="bib.bib7.1.1">10</span>(7), e0130140 (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Baur, C., Albarqouni, S., Navab, N.: Generating Highly Realistic Images of Skin Lesions with GANs, p. 260–267. Springer, London, UK (2018). https://doi.org/10.1007/978-3-030-01201-4_28

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Baur, C., Albarqouni, S., Navab, N.: MelanoGANs: High resolution skin lesion synthesis with GANs (2018), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1804.04338" title="">https://arxiv.org/abs/1804.04338</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Brinker, T.J., Hekler, A., Enk, A.H., Klode, J., Hauschild, A., Berking, C., Schilling, B., Haferkamp, S., Schadendorf, D., Holland-Letz, T., et al.: Deep learning outperformed 136 of 157 dermatologists in a head-to-head dermoscopic melanoma image classification task. Eur. J. Cancer <span class="ltx_text ltx_font_bold" id="bib.bib10.1.1">113</span>, 47–54 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Chen, Y., Yang, X.H., Wei, Z., Heidari, A.A., Zheng, N., Li, Z., Chen, H., Hu, H., Zhou, Q., Guan, Q.: Generative adversarial networks in medical image augmentation: A review. Comput. Biol. Med. <span class="ltx_text ltx_font_bold" id="bib.bib11.1.1">144</span>, 105382 (2022). https://doi.org/10.1016/j.compbiomed.2022.105382

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Codella, N., Rotemberg, V., Tschandl, P., Celebi, M.E., Dusza, S., Gutman, D., Helba, B., Kalloo, A., Liopyris, K., Marchetti, M., Kittler, H., Halpern, A.: Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (ISIC) (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Daneshjou, R., Vodrahalli, K., Novoa, R.A., Jenkins, M., Liang, W., Rotemberg, V., Ko, J., Swetter, S.M., Bailey, E.E., Gevaert, O., Mukherjee, P., Phung, M., Yekrang, K., Fong, B., Sahasrabudhe, R., Allerup, J.A.C., Okata-Karigane, U., Zou, J., Chiou, A.S.: Disparities in dermatology AI performance on a diverse, curated clinical image set. Science Advances <span class="ltx_text ltx_font_bold" id="bib.bib13.1.1">8</span>(32), eabq6147 (2022). https://doi.org/10.1126/sciadv.abq6147

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition. pp. 248–255 (2009). https://doi.org/10.1109/CVPR.2009.5206848

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Dolezal, J.M., Wolk, R., Hieromnimon, H.M., Howard, F.M., Srisuwananukorn, A., Karpeyev, D., Ramesh, S., Kochanny, S., Kwon, J.W., Agni, M., Simon, R.C., Desai, C., Kherallah, R., Nguyen, T.D., Schulte, J.J., Cole, K., Khramtsova, G., Garassino, M.C., Husain, A.N., Li, H., Grossman, R., Cipriani, N.A., Pearson, A.T.: Deep learning generates synthetic cancer histology for explainability and education. npj Precis. Oncol. <span class="ltx_text ltx_font_bold" id="bib.bib15.1.1">7</span>(1),  49 (May 2023). https://doi.org/10.1038/s41698-023-00399-4

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Dreyer, M., Achtibat, R., Samek, W., Lapuschkin, S.: Understanding the (extra-) ordinary: Validating deep model decisions with prototypical concept-based explanations. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3491–3501 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Gessert, N., Sentker, T., Madesta, F., Schmitz, R., Kniep, H., Baltruschat, I., Werner, R., Schlaefer, A.: Skin lesion diagnosis using ensembles, unscaled multi-crop evaluation and loss weighting (2018), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1808.01694" title="">https://arxiv.org/abs/1808.01694</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. Advances in neural information processing systems <span class="ltx_text ltx_font_bold" id="bib.bib18.1.1">27</span> (2014)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Groh, M., Harris, C., Soenksen, L., Lau, F., Han, R., Kim, A., Koochek, A., Badri, O.: Evaluating deep neural networks trained on clinical images in dermatology with the Fitzpatrick 17k dataset. In: 2021 IEEE/CVF CVPRW. pp. 1820–1828. IEEE, NYC, USA (2021). https://doi.org/10.1109/CVPRW53098.2021.00201

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: GANs trained by a two time-scale update rule converge to a local Nash equilibrium. Advances in Neural Information Processing Systems (NIPS 2017) <span class="ltx_text ltx_font_bold" id="bib.bib20.1.1">30</span> (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Jeong, J.J., Tariq, A., Adejumo, T., Trivedi, H., Gichoya, J.W., Banerjee, I.: Systematic review of generative adversarial networks (GANs) for medical image classification and segmentation. J. Digit. Imaging <span class="ltx_text ltx_font_bold" id="bib.bib21.1.1">35</span>(2), 137–152 (2022). https://doi.org/https://doi.org/10.1007/s10278-021-00556-w

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing and improving the image quality of StyleGAN. In: 2020 IEEE/CVF CVPR. pp. 8107–8116. IEEE, NYC, USA (2020). https://doi.org/10.1109/CVPR42600.2020.00813

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Kawahara, J., Daneshvar, S., Argenziano, G., Hamarneh, G.: Seven-point checklist and skin lesion classification using multitask multimodal neural nets. IEEE J. Biomed. Health Inform. <span class="ltx_text ltx_font_bold" id="bib.bib23.1.1">23</span>(2), 538–546 (2019). https://doi.org/10.1109/JBHI.2018.2824327

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Kebaili, A., Lapuyade-Lahorgue, J., Ruan, S.: Deep learning approaches for data augmentation in medical imaging: a review. J. Imaging <span class="ltx_text ltx_font_bold" id="bib.bib24.1.1">9</span>(4),  81 (2023). https://doi.org/10.3390/jimaging9040081

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization (2017), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1412.6980" title="">https://arxiv.org/abs/1412.6980</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. Communications of the ACM <span class="ltx_text ltx_font_bold" id="bib.bib26.1.1">60</span>, 84 – 90 (2012), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:195908774" title="">https://api.semanticscholar.org/CorpusID:195908774</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Levine, A.B., Peng, J., Farnell, D., Nursey, M., Wang, Y., Naso, J.R., Ren, H., Farahani, H., Chen, C., Chiu, D., Talhouk, A., Sheffield, B., Riazy, M., Ip, P.P., Parra-Herran, C., Mills, A., Singh, N., Tessier-Cloutier, B., Salisbury, T., Lee, J., Salcudean, T., Jones, S.J., Huntsman, D.G., Gilks, C.B., Yip, S., Bashashati, A.: Synthesis of diagnostic quality cancer pathology images by generative adversarial networks. J. Pathol. <span class="ltx_text ltx_font_bold" id="bib.bib27.1.1">252</span>(2), 178–188 (Sep 2020). https://doi.org/10.1002/path.5509

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Makhlouf, A., Maayah, M., Abughanam, N., Catal, C.: The use of generative adversarial networks in medical image augmentation. Neural Comput. Appl. <span class="ltx_text ltx_font_bold" id="bib.bib28.1.1">35</span>(34), 24055–24068 (2023). https://doi.org/10.1007/s00521-023-09100-z

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Nozdryn-Plotnicki, A., Yap, J., Yolland, W.: Ensembling convolutional neural networks for skin cancer classification. In: International Skin Imaging Collaboration (ISIC) Challenge on Skin Image Analysis for Melanoma Detection, MICCAI (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Qasim, A.B., Ezhov, I., Shit, S., Schoppe, O., Paetzold, J.C., Sekuboyina, A., Kofler, F., Lipkova, J., Li, H., Menze, B.: Red-GAN: Attacking class imbalance via conditioned generation. Yet another medical imaging perspective. In: Arbel, T., Ben Ayed, I., de Bruijne, M., Descoteaux, M., Lombaert, H., Pal, C. (eds.) Proc. Third Conference on Medical Imaging with Deep Learning. Proc. Machine Learning Research, vol. 121, pp. 655–668. PMLR (06–08 Jul 2020), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v121/qasim20a.html" title="">https://proceedings.mlr.press/v121/qasim20a.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Quiros, A.C., Murray-Smith, R., Yuan, K.: PathologyGAN: Learning deep representations of cancer tissue. Machine Learning for Biomedical Imaging <span class="ltx_text ltx_font_bold" id="bib.bib31.1.1">1</span>, 1–47 (2021). https://doi.org/10.59275/j.melba.2021-gfgg

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Sagers, L.W., Diao, J.A., Groh, M., Rajpurkar, P., Adamson, A., Manrai, A.K.: Improving dermatology classifiers across populations using images generated by large diffusion models. In: NeurIPS 2022 Workshop on Synthetic Data for Empowering ML Research (2022), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Vzdbjtz6Tys" title="">https://openreview.net/forum?id=Vzdbjtz6Tys</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Shen, Y., Zhou, B.: Closed-form factorization of latent semantics in GANs. CoRR (2020), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2007.06600" title="">https://arxiv.org/abs/2007.06600</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Solanki, A., Naved, M.: GANs for Data Augmentation in Healthcare. Springer, Cham (2023). https://doi.org/10.1007/978-3-031-43205-7

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Tov, O., Alaluf, Y., Nitzan, Y., Patashnik, O., Cohen-Or, D.: Designing an encoder for StyleGAN image manipulation (2021), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2102.02766" title="">https://arxiv.org/abs/2102.02766</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Tschandl, P., Rosendahl, C., Kittler, H.: The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Sci Data <span class="ltx_text ltx_font_bold" id="bib.bib36.1.1">5</span>(1),  1–9 (2018). https://doi.org/10.1038/sdata.2018.161

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Yi, X., Walia, E., Babyn, P.: Unsupervised and semi-supervised learning with categorical generative adversarial networks assisted by Wasserstein distance for dermoscopy image classification (2018), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1804.03700" title="">https://arxiv.org/abs/1804.03700</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Yi, X., Walia, E., Babyn, P.: Generative adversarial network in medical imaging: A review. Med. Image Anal. <span class="ltx_text ltx_font_bold" id="bib.bib38.1.1">58</span>, 101552 (Dec 2019). https://doi.org/10.1016/j.media.2019.101552

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Appendices</h2>
</section>
<section class="ltx_section" id="S1a">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">A </span>Exemplary transformations</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="263" id="S1.F1.g1" src="extracted/5907388/Figures/Combined_Transformations.png" width="509"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure A.1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Original image (left) and its five transformations: BCV, GV, PS, SV, SPV (from left to right). Each transformation modifies the image in distinct ways.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S2a">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">B </span>Explanations of the Baseline and the Augmented Models for the class MEL</h2>
<div class="ltx_para" id="S2a.p1">
<p class="ltx_p" id="S2a.p1.1">CRP requires preselecting an output neuron to explain the network decision to. Whichever output we choose, the heatmaps will show how relevant each part of the input is for this output. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S2.F1a" title="Figure B.1 ‣ B Explanations of the Baseline and the Augmented Models for the class MEL ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">B.1</span></a> shows explanations for the wrong classification class. Here we provide explanations for the ground truth class. They indicate that the baseline model is incapable of finding any supporting evidence for the MEL class, contrary to the augmented model.</p>
</div>
<figure class="ltx_figure" id="S2.F1a"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="363" id="S2.F1a.g1" src="x3.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1a.2.1.1" style="font-size:90%;">Figure B.1</span>: </span><span class="ltx_text" id="S2.F1a.3.2" style="font-size:90%;">Explanation of the same test sample from Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S3.F7" title="Figure 7 ‣ 3.3 Model Analysis with Explainable AI ‣ 3 Results and Discussion ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">7</span></a>, for the baseline model and the ground truth class MEL. </span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S3a">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">C </span>Understanding Prediction Strategies with Prototypical Concept-based Explanations</h2>
<div class="ltx_para" id="S3a.p1">
<p class="ltx_p" id="S3a.p1.1">One of the major categorizations of Explainable AI methods is the contrast between local and global explanations. Local explanations shed light on the model behavior on a specific test sample, whereas global methods explain the model’s reasoning in general, in a holistic fashion. CRP outputs local concept conditional heatmaps, as well as global explanations of each concept. This is why it is referred to as a glocal method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib3" title="">3</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3a.p2">
<p class="ltx_p" id="S3a.p2.1">Similarly, Prototypical Concept Explanations (PCX) focus on a single class to provide explanations revealing the different substrategies used for the classification decisions to this class. Each substrategy is further analyzed to identify the driving concepts for decisions using this substrategy. This constitutes a global explanation. Furthermore, each local decision can be attributed to a substrategy, or identified as a novelty for the model.</p>
</div>
<div class="ltx_para" id="S3a.p3">
<p class="ltx_p" id="S3a.p3.1">Specifically, <span class="ltx_glossaryref" title="">PCX</span> clusters latent relevances, for instance obtained with <span class="ltx_glossaryref" title="">LRP</span>, for samples from one class (here: MEL), followed by a cluster analysis, e.g., with Gaussian Mixture Models.
This produces clusters of samples for which the model uses similar prediction strategies.
Each cluster can be represented with prototypical samples and viewed as distribution over concepts. These concepts can further be visualized using <span class="ltx_glossaryref" title="">CRP</span>. Figures <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S4.F1" title="Figure D.1 ‣ D Understanding Prediction Strategies with Prototypical Concept-based Explanations ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">D.1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S4.F2" title="Figure D.2 ‣ D Understanding Prediction Strategies with Prototypical Concept-based Explanations ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">D.2</span></a> portray global explanations for the MEL class, for the models trained on the vanilla and synthetically augmented datasets, respectively. Specifically, columns show prototypes per cluster and rows represent concepts visualized with CRP.
The values in the matrix indicate how much a concept is used by a prototype. Note that each sub-strategy can be considered as a distribution over concepts. The weight of each concept per substrategy is visualized as percentage in the matrix. While the baseline model heavily relies on distractor concepts, such as concepts 624 and 180, focusing on hair and skin markers, the model augmented with synthetic data uses clean and, to the best of our knowledge, clinically meaningful features.</p>
</div>
</section>
<section class="ltx_section" id="S4a">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">D </span>Understanding Prediction Strategies with Prototypical Concept-based Explanations</h2>
<div class="ltx_para" id="S4a.p1">
<p class="ltx_p" id="S4a.p1.1">One of the major categorizations of Explainable AI methods is the contrast between local and global explanations. Local explanations shed light on the model behavior on a specific test sample, whereas global methods explain the model’s reasoning in general, in a holistic fashion. CRP outputs local concept conditional heatmaps, as well as global explanations of each concept. This is why it is referred to as a glocal method<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#bib.bib3" title="">3</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4a.p2">
<p class="ltx_p" id="S4a.p2.1">Similarly, Prototypical Concept Explanations (PCX) focus on a single class to provide explanations revealing the different substrategies used for the classification decisions to this class. Each substrategy is further analyzed to identify the driving concepts for decisions using this substrategy. This constitutes a global explanation. Furthermore, each local decision can be attributed to a substrategy, or identified as a novelty for the model.</p>
</div>
<div class="ltx_para" id="S4a.p3">
<p class="ltx_p" id="S4a.p3.1">Figures <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S4.F1" title="Figure D.1 ‣ D Understanding Prediction Strategies with Prototypical Concept-based Explanations ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">D.1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.05114v1#S4.F2" title="Figure D.2 ‣ D Understanding Prediction Strategies with Prototypical Concept-based Explanations ‣ Synthetic Generation of Dermatoscopic Images with GAN and Closed-Form Factorization"><span class="ltx_text ltx_ref_tag">D.2</span></a> portray global explanations for the MEL class, for the models trained on the vanilla and synthetically augmented datasets, respectively. The columns in the figures correspond to different substrategies from the classification model, as discovered by a Gaussian Mixture Model trained on latent relevance scores. Subtrategies are visualized with exemplary prototypes from the training dataset. The rows correspond to different concepts on a preselected layer (here: activations <em class="ltx_emph ltx_font_italic" id="S4a.p3.1.1">after</em> the last transition block) and show CRP-style concept representatives. Note, that each sub-strategy can be considered as distribution over concepts. The weight of each concept per substrategy is visualized as percentage in the matrix. While the baseline model heavily relies on distractor concepts, such as concepts 624 and 180, focusing on hair and skin markers, the model augmented with synthetic data uses clean and, to the best of our knowledge, clinically meaningful features.</p>
</div>
<figure class="ltx_figure" id="S4.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="661" id="S4.F1.g1" src="extracted/5907388/Figures/xai/baseline_0_prototypes_with_concepts_features.transition3.identity_zplus.jpg" width="509"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F1.2.1.1" style="font-size:90%;">Figure D.1</span>: </span><span class="ltx_text" id="S4.F1.3.2" style="font-size:90%;">PCX visualization of baseline model for class MEL</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="661" id="S4.F2.g1" src="extracted/5907388/Figures/xai/synth_0_prototypes_with_concepts_features.transition3.identity_zplus.jpg" width="509"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.2.1.1" style="font-size:90%;">Figure D.2</span>: </span><span class="ltx_text" id="S4.F2.3.2" style="font-size:90%;">PCX visualization of model trained with additional (filtered) synthetic samples for class MEL</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Oct  7 14:52:14 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
