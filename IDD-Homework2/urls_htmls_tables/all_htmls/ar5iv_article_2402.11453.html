<article class="ltx_document" lang="en">
 <h1 class="ltx_title ltx_title_document">
  <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="id1.g1" src="/html/2402.11453/assets/pictures/logo.jpg" width="14"/>
  MatPlotAgent: Method and Evaluation for
  <br class="ltx_break"/>
  LLM-Based Agentic Scientific Data Visualization
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Zhiyu Yang
    <sup class="ltx_sup" id="id20.19.id1">
     <span class="ltx_text ltx_font_italic" id="id20.19.id1.1">
      ∗2
     </span>
    </sup>
    Zihan Zhou
    <sup class="ltx_sup" id="id21.20.id2">
     <span class="ltx_text ltx_font_italic" id="id21.20.id2.1">
      3
     </span>
    </sup>
    Shuo Wang
    <sup class="ltx_sup" id="id22.21.id3">
     <span class="ltx_text ltx_font_italic" id="id22.21.id3.1">
      †1
     </span>
    </sup>
    Xin Cong
    <sup class="ltx_sup" id="id23.22.id4">
     <span class="ltx_text ltx_font_italic" id="id23.22.id4.1">
      1
     </span>
    </sup>
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_bold" id="id6.5.1">
     Xu Han
     <sup class="ltx_sup" id="id6.5.1.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id6.5.1.1.1">
       1
      </span>
     </sup>
    </span>
    <span class="ltx_text ltx_font_bold" id="id7.6.2">
     Yukun Yan
     <sup class="ltx_sup" id="id7.6.2.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id7.6.2.1.1">
       1
      </span>
     </sup>
    </span>
    <span class="ltx_text ltx_font_bold" id="id8.7.3">
     Zhenghao Liu
     <sup class="ltx_sup" id="id8.7.3.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id8.7.3.1.1">
       4
      </span>
     </sup>
    </span>
    <span class="ltx_text ltx_font_bold" id="id9.8.4">
     Zhixing Tan
     <sup class="ltx_sup" id="id9.8.4.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id9.8.4.1.1">
       5
      </span>
     </sup>
    </span>
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_bold" id="id10.9.5">
     Pengyuan Liu
     <sup class="ltx_sup" id="id10.9.5.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id10.9.5.1.1">
       2
      </span>
     </sup>
    </span>
    <span class="ltx_text ltx_font_bold" id="id11.10.6">
     Dong Yu
     <sup class="ltx_sup" id="id11.10.6.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id11.10.6.1.1">
       2
      </span>
     </sup>
    </span>
    <span class="ltx_text ltx_font_bold" id="id12.11.7">
     Zhiyuan Liu
     <sup class="ltx_sup" id="id12.11.7.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id12.11.7.1.1">
       1
      </span>
     </sup>
    </span>
    <span class="ltx_text ltx_font_bold" id="id13.12.8">
     Xiaodong Shi
     <sup class="ltx_sup" id="id13.12.8.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id13.12.8.1.1">
       3
      </span>
     </sup>
    </span>
    <span class="ltx_text ltx_font_bold" id="id14.13.9">
     Maosong Sun
     <sup class="ltx_sup" id="id14.13.9.1">
      <span class="ltx_text ltx_font_medium ltx_font_italic" id="id14.13.9.1.1">
       1
      </span>
     </sup>
    </span>
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id24.23.id5">
     1
    </sup>
    Tsinghua University
    <sup class="ltx_sup" id="id25.24.id6">
     2
    </sup>
    Beijing Language and Culture University
    <sup class="ltx_sup" id="id26.25.id7">
     3
    </sup>
    Xiamen University
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id27.26.id8">
     4
    </sup>
    Northeastern University, China
    <sup class="ltx_sup" id="id28.27.id9">
     5
    </sup>
    Zhongguancun Laboratory, Beijing, China
    <br class="ltx_break"/>
   </span>
   <span class="ltx_author_notes">
    Equal contribution.  Corresponding authors.
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id29.id1">
   <span class="ltx_text" id="id29.id1.1">
    Scientific data visualization plays a crucial role in research by enabling the direct display of complex information and assisting researchers in identifying implicit patterns. Despite its importance, the use of Large Language Models (LLMs) for scientific data visualization remains rather unexplored. In this study, we introduce MatPlotAgent, an efficient model-agnostic LLM agent framework designed to automate scientific data visualization tasks. Leveraging the capabilities of both code LLMs and multi-modal LLMs, MatPlotAgent consists of three core modules: query understanding, code generation with iterative debugging, and a visual feedback mechanism for error correction.
To address the lack of benchmarks in this field, we present MatPlotBench, a high-quality benchmark consisting of 100 human-verified test cases. Additionally, we introduce a scoring approach that utilizes GPT-4V for automatic evaluation. Experimental results demonstrate that MatPlotAgent can improve the performance of various LLMs, including both commercial and open-source models. Furthermore, the proposed evaluation method shows a strong correlation with human-annotated scores.
    <span class="ltx_note ltx_role_footnote" id="footnote1">
     <sup class="ltx_note_mark">
      1
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        1
       </sup>
       <span class="ltx_tag ltx_tag_note">
        1
       </span>
       MatPlotAgent and MatPlotBench will be publicly available at
       <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/OpenBMB/MatPlotAgent" target="_blank" title="">
        https://github.com/OpenBMB/MatPlotAgent
       </a>
       .
      </span>
     </span>
    </span>
   </span>
  </p>
 </div>
 <div class="ltx_para ltx_noindent" id="p1">
  <div class="ltx_block ltx_align_bottom" id="p1.19">
   <img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="14" id="p1.1.g1" src="/html/2402.11453/assets/pictures/logo.jpg" width="14"/>
   <p class="ltx_p ltx_align_center" id="p1.19.19">
    <span class="ltx_text ltx_font_bold" id="p1.19.19.1">
     MatPlotAgent: Method and Evaluation for
     <br class="ltx_break"/>
     LLM-Based Agentic Scientific Data Visualization
    </span>
   </p>
   <br class="ltx_break ltx_centering"/>
   <p class="ltx_p ltx_align_center" id="p1.19.18" style="width:433.6pt;">
    <span class="ltx_text ltx_inline-block" id="p1.19.18.18" style="width:0.0pt;">
     <span class="ltx_tabular ltx_align_top" id="p1.19.18.18.18">
      <span class="ltx_tr" id="p1.5.4.4.4.4">
       <span class="ltx_td ltx_align_center" id="p1.5.4.4.4.4.4">
        <span class="ltx_text ltx_font_bold" id="p1.5.4.4.4.4.4.4">
         Zhiyu Yang
         <sup class="ltx_sup" id="p1.5.4.4.4.4.4.4.1">
          <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.4.4.4.4.4.4.1.1">
           ∗2
          </span>
         </sup>
         Zihan Zhou
         <span class="ltx_note ltx_role_thanks" id="p1.5.4.4.4.4.4.4.2">
          <sup class="ltx_note_mark">
           †
          </sup>
          <span class="ltx_note_outer">
           <span class="ltx_note_content">
            <sup class="ltx_note_mark">
             †
            </sup>
            <span class="ltx_note_type">
             thanks:
            </span>
            Equal contribution.
           </span>
          </span>
         </span>
         <sup class="ltx_sup" id="p1.5.4.4.4.4.4.4.3">
          <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.4.4.4.4.4.4.3.1">
           3
          </span>
         </sup>
         Shuo Wang
         <sup class="ltx_sup" id="p1.5.4.4.4.4.4.4.4">
          <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.4.4.4.4.4.4.4.1">
           †1
          </span>
         </sup>
         Xin Cong
         <sup class="ltx_sup" id="p1.5.4.4.4.4.4.4.5">
          <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.4.4.4.4.4.4.5.1">
           1
          </span>
         </sup>
        </span>
       </span>
      </span>
      <span class="ltx_tr" id="p1.9.8.8.8.8">
       <span class="ltx_td ltx_align_center" id="p1.9.8.8.8.8.4">
        <span class="ltx_text ltx_font_bold" id="p1.6.5.5.5.5.1.1">
         Xu Han
         <sup class="ltx_sup" id="p1.6.5.5.5.5.1.1.1">
          <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.6.5.5.5.5.1.1.1.1">
           1
          </span>
         </sup>
        </span>
        <span class="ltx_text ltx_font_bold" id="p1.7.6.6.6.6.2.2">
         Yukun Yan
         <sup class="ltx_sup" id="p1.7.6.6.6.6.2.2.1">
          <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.7.6.6.6.6.2.2.1.1">
           1
          </span>
         </sup>
        </span>
        <span class="ltx_text ltx_font_bold" id="p1.8.7.7.7.7.3.3">
         Zhenghao Liu
         <sup class="ltx_sup" id="p1.8.7.7.7.7.3.3.1">
          <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.8.7.7.7.7.3.3.1.1">
           4
          </span>
         </sup>
        </span>
        <span class="ltx_text ltx_font_bold" id="p1.9.8.8.8.8.4.4">
         Zhixing Tan
         <sup class="ltx_sup" id="p1.9.8.8.8.8.4.4.1">
          <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.9.8.8.8.8.4.4.1.1">
           5
          </span>
         </sup>
        </span>
       </span>
      </span>
      <span class="ltx_tr" id="p1.14.13.13.13.13">
       <span class="ltx_td ltx_align_center" id="p1.14.13.13.13.13.5">
        <span class="ltx_text ltx_font_bold" id="p1.10.9.9.9.9.1.1">
         Pengyuan Liu
         <sup class="ltx_sup" id="p1.10.9.9.9.9.1.1.1">
          <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.10.9.9.9.9.1.1.1.1">
           2
          </span>
         </sup>
        </span>
        <span class="ltx_text ltx_font_bold" id="p1.11.10.10.10.10.2.2">
         Dong Yu
         <sup class="ltx_sup" id="p1.11.10.10.10.10.2.2.1">
          <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.11.10.10.10.10.2.2.1.1">
           2
          </span>
         </sup>
        </span>
        <span class="ltx_text ltx_font_bold" id="p1.12.11.11.11.11.3.3">
         Zhiyuan Liu
         <span class="ltx_note ltx_role_thanks" id="p1.12.11.11.11.11.3.3.1">
          <sup class="ltx_note_mark">
           †
          </sup>
          <span class="ltx_note_outer">
           <span class="ltx_note_content">
            <sup class="ltx_note_mark">
             †
            </sup>
            <span class="ltx_note_type">
             thanks:
            </span>
            <span class="ltx_text ltx_font_medium" id="p1.12.11.11.11.11.3.3.1.1">
             Corresponding authors.
            </span>
           </span>
          </span>
         </span>
         <sup class="ltx_sup" id="p1.12.11.11.11.11.3.3.2">
          <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.12.11.11.11.11.3.3.2.1">
           1
          </span>
         </sup>
        </span>
        <span class="ltx_text ltx_font_bold" id="p1.13.12.12.12.12.4.4">
         Xiaodong Shi
         <sup class="ltx_sup" id="p1.13.12.12.12.12.4.4.1">
          <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.13.12.12.12.12.4.4.1.1">
           3
          </span>
         </sup>
        </span>
        <span class="ltx_text ltx_font_bold" id="p1.14.13.13.13.13.5.5">
         Maosong Sun
         <sup class="ltx_sup" id="p1.14.13.13.13.13.5.5.1">
          <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.14.13.13.13.13.5.5.1.1">
           1
          </span>
         </sup>
        </span>
       </span>
      </span>
      <span class="ltx_tr" id="p1.17.16.16.16.16">
       <span class="ltx_td ltx_align_center" id="p1.17.16.16.16.16.3">
        <sup class="ltx_sup" id="p1.17.16.16.16.16.3.1">
         1
        </sup>
        Tsinghua University
        <sup class="ltx_sup" id="p1.17.16.16.16.16.3.2">
         2
        </sup>
        Beijing Language and Culture University
        <sup class="ltx_sup" id="p1.17.16.16.16.16.3.3">
         3
        </sup>
        Xiamen University
       </span>
      </span>
      <span class="ltx_tr" id="p1.19.18.18.18.18">
       <span class="ltx_td ltx_align_center" id="p1.19.18.18.18.18.2">
        <sup class="ltx_sup" id="p1.19.18.18.18.18.2.1">
         4
        </sup>
        Northeastern University, China
        <sup class="ltx_sup" id="p1.19.18.18.18.18.2.2">
         5
        </sup>
        Zhongguancun Laboratory, Beijing, China
       </span>
      </span>
     </span>
    </span>
   </p>
   <br class="ltx_break ltx_centering"/>
  </div>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">
     A picture is worth a thousand words.
    </span>
    Data visualization is an essential process in scientific research, facilitating the more direct conveyance of complex information and aiding researchers in uncovering implicit patterns. There are many advanced toolkits, such as Matplotlib
    <span class="ltx_note ltx_role_footnote" id="footnote2">
     <sup class="ltx_note_mark">
      2
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        2
       </sup>
       <span class="ltx_tag ltx_tag_note">
        2
       </span>
       <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://matplotlib.org" target="_blank" title="">
        https://matplotlib.org
       </a>
      </span>
     </span>
    </span>
    and Origin
    <span class="ltx_note ltx_role_footnote" id="footnote3">
     <sup class="ltx_note_mark">
      3
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        3
       </sup>
       <span class="ltx_tag ltx_tag_note">
        3
       </span>
       <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.originlab.com" target="_blank" title="">
        https://www.originlab.com
       </a>
      </span>
     </span>
    </span>
    , that can help researchers plot various types of figures for complex data distributions. However, transforming raw data into informative and easy-to-understand visualizations is still time-consuming and labor-intensive. Before the invention of large language models (LLMs)
    <cite class="ltx_cite ltx_citemacro_cite">
     OpenAI (
     <a class="ltx_ref" href="#bib.bib19" title="">
      2023
     </a>
     )
    </cite>
    , automating this process with AI models is almost impossible.
   </p>
  </div>
  <figure class="ltx_figure" id="S1.F1">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="231" id="S1.F1.g1" src="/html/2402.11453/assets/x1.png" width="457"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 1:
    </span>
    Examples in the proposed MatPlotBench. Given the raw data and user queries, the AI agent is expected to generate a figure accordingly. We only display partial raw data and user queries due to space limitations.
   </figcaption>
  </figure>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    With large-scale parameters and extensive training data, LLMs have demonstrated remarkable capabilities in a wide range of complex tasks, including reasoning
    <cite class="ltx_cite ltx_citemacro_cite">
     Wei et al. (
     <a class="ltx_ref" href="#bib.bib33" title="">
      2022
     </a>
     ); Kojima et al. (
     <a class="ltx_ref" href="#bib.bib8" title="">
      2022a
     </a>
     ); Yao et al. (
     <a class="ltx_ref" href="#bib.bib38" title="">
      2023a
     </a>
     )
    </cite>
    , mathematics
    <cite class="ltx_cite ltx_citemacro_cite">
     Yu et al. (
     <a class="ltx_ref" href="#bib.bib40" title="">
      2024
     </a>
     ); Luo et al. (
     <a class="ltx_ref" href="#bib.bib16" title="">
      2023a
     </a>
     ); Azerbayev et al. (
     <a class="ltx_ref" href="#bib.bib2" title="">
      2024
     </a>
     ); Shao et al. (
     <a class="ltx_ref" href="#bib.bib30" title="">
      2024
     </a>
     )
    </cite>
    and coding
    <cite class="ltx_cite ltx_citemacro_cite">
     Rozière et al. (
     <a class="ltx_ref" href="#bib.bib27" title="">
      2024
     </a>
     ); Luo et al. (
     <a class="ltx_ref" href="#bib.bib17" title="">
      2023b
     </a>
     ); Guo et al. (
     <a class="ltx_ref" href="#bib.bib7" title="">
      2024
     </a>
     ); Wei et al. (
     <a class="ltx_ref" href="#bib.bib34" title="">
      2023
     </a>
     )
    </cite>
    . This breakthrough has unlocked new opportunities for utilizing LLMs as autonomous agents in a diverse range of practical scenarios, such as web browsing
    <cite class="ltx_cite ltx_citemacro_cite">
     Nakano et al. (
     <a class="ltx_ref" href="#bib.bib18" title="">
      2021
     </a>
     ); Yao et al. (
     <a class="ltx_ref" href="#bib.bib37" title="">
      2022
     </a>
     ); Qin et al. (
     <a class="ltx_ref" href="#bib.bib23" title="">
      2023
     </a>
     ); Zhou et al. (
     <a class="ltx_ref" href="#bib.bib41" title="">
      2023
     </a>
     ); Deng et al. (
     <a class="ltx_ref" href="#bib.bib6" title="">
      2023
     </a>
     ); Yao et al. (
     <a class="ltx_ref" href="#bib.bib39" title="">
      2023b
     </a>
     ); Xie et al. (
     <a class="ltx_ref" href="#bib.bib35" title="">
      2023
     </a>
     )
    </cite>
    , social simulations
    <cite class="ltx_cite ltx_citemacro_cite">
     Park et al. (
     <a class="ltx_ref" href="#bib.bib20" title="">
      2023
     </a>
     ); Xu et al. (
     <a class="ltx_ref" href="#bib.bib36" title="">
      2023
     </a>
     ); Chen et al. (
     <a class="ltx_ref" href="#bib.bib4" title="">
      2024a
     </a>
     ); Wang et al. (
     <a class="ltx_ref" href="#bib.bib32" title="">
      2023
     </a>
     )
    </cite>
    , tool utilization
    <cite class="ltx_cite ltx_citemacro_cite">
     Qin et al. (
     <a class="ltx_ref" href="#bib.bib24" title="">
      2024
     </a>
     ); Schick et al. (
     <a class="ltx_ref" href="#bib.bib29" title="">
      2023
     </a>
     ); Liu et al. (
     <a class="ltx_ref" href="#bib.bib14" title="">
      2024
     </a>
     ); Li et al. (
     <a class="ltx_ref" href="#bib.bib12" title="">
      2023a
     </a>
     ); Lu et al. (
     <a class="ltx_ref" href="#bib.bib15" title="">
      2023
     </a>
     ); Qian et al. (
     <a class="ltx_ref" href="#bib.bib22" title="">
      2023b
     </a>
     ); Shinn et al. (
     <a class="ltx_ref" href="#bib.bib31" title="">
      2023
     </a>
     )
    </cite>
    , and software development
    <cite class="ltx_cite ltx_citemacro_cite">
     Qian et al. (
     <a class="ltx_ref" href="#bib.bib21" title="">
      2023a
     </a>
     )
    </cite>
    . Using LLMs to enhance human productivity in specialized areas is now a key research focus with great potential.
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    Recent advancements in LLM-based agents inspire us to explore the utilization of LLMs for scientific data visualization, a realm that remains rather unexplored in existing studies.
A closely related line of research is text-to-image generation
    <cite class="ltx_cite ltx_citemacro_cite">
     Ramesh et al. (
     <a class="ltx_ref" href="#bib.bib25" title="">
      2021
     </a>
     ); Saharia et al. (
     <a class="ltx_ref" href="#bib.bib28" title="">
      2022
     </a>
     )
    </cite>
    , where diffusion models
    <cite class="ltx_cite ltx_citemacro_cite">
     Rombach et al. (
     <a class="ltx_ref" href="#bib.bib26" title="">
      2022
     </a>
     )
    </cite>
    have shown great potential in generating various types of images.
However, existing text-to-image generation methods predominantly focus on artistic expression, potentially misaligning with the needs of scientific data visualization, where clarity and precision in conveying information are the most important principles. This work aims to automatically generate figures with precise information.
   </p>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    We propose leveraging modern code LLMs and multi-modal LLMs to develop scientific data visualization agents that can significantly enhance human efficiency. The resulting
    <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">
     MatPlotAgent
     <span class="ltx_note ltx_role_footnote" id="footnote4">
      <sup class="ltx_note_mark">
       4
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         4
        </sup>
        <span class="ltx_tag ltx_tag_note">
         <span class="ltx_text ltx_font_medium" id="footnote4.1.1.1">
          4
         </span>
        </span>
        <span class="ltx_text ltx_font_medium" id="footnote4.5">
         This name is in homage to the well-known Matplotlib.
        </span>
       </span>
      </span>
     </span>
    </span>
    is comprised of three modules: (1)
    <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">
     the query understanding
    </span>
    that can thoroughly understand user-provided requirements; (2)
    <span class="ltx_text ltx_font_italic" id="S1.p4.1.3">
     the code generation module
    </span>
    with iterative debugging capabilities that use code to precisely preprocess raw data and generate figures; and (3)
    <span class="ltx_text ltx_font_italic" id="S1.p4.1.4">
     the visual feedback module
    </span>
    that possesses visual perceptual abilities to find errors in the plotted draft and provide visual feedback to the code generation module to rectify the errors.
Our method is model-agnostic, which can be driven with any code LLMs and multi-modal LLMs. Through experiments, we find MatPlotAgent can work with both closed-source LLMs (e.g., GPT-4
    <cite class="ltx_cite ltx_citemacro_cite">
     OpenAI (
     <a class="ltx_ref" href="#bib.bib19" title="">
      2023
     </a>
     )
    </cite>
    ) and open-source LLMs (e.g., Magicoder
    <cite class="ltx_cite ltx_citemacro_cite">
     Wei et al. (
     <a class="ltx_ref" href="#bib.bib34" title="">
      2023
     </a>
     )
    </cite>
    ).
   </p>
  </div>
  <div class="ltx_para" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    Another critical challenge in the field of automatic scientific data visualization is the absence of benchmarks for evaluation purposes. To address this issue, we introduce a meticulously crafted benchmark called
    <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">
     MatPlotBench
    </span>
    to quantitatively evaluate the approaches involved. Specifically, MatPlotBench contains 100 carefully hand-crafted test examples, each of which contains a user query, the corresponding input data, and a ground-truth figure verified by human experts. We believe that high-quality test sets play a crucial role in driving advancements in the field.
   </p>
  </div>
  <div class="ltx_para" id="S1.p6">
   <p class="ltx_p" id="S1.p6.1">
    To facilitate automatic quantitative evaluation, we also design a scoring mechanism based on GPT-4V
    <cite class="ltx_cite ltx_citemacro_cite">
     OpenAI (
     <a class="ltx_ref" href="#bib.bib19" title="">
      2023
     </a>
     )
    </cite>
    , which is one of the strongest multi-modal LLMs that can effectively understand text and figures.
Specifically, GPT-4V is prompted to produce a score between 0 and 100 based on the ground-truth figure and the one generated by AI models. Additionally, we conduct human evaluation and estimate the correlation coefficient between human-annotated scores and the automatically calculated scores. The results reveal a strong correlation between the automatic score and the human-annotated score, thus affirming the reliability of the scoring mechanism.
In summary, our contribution can be listed as follows:
   </p>
   <ul class="ltx_itemize" id="S1.I1">
    <li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i1.p1">
      <p class="ltx_p" id="S1.I1.i1.p1.1">
       We introduce MatPlotBench to enable automatic quantitative evaluation of AI methods designed for scientific data visualization. Through comparison with human evaluation, we observe that MatPlotBench can effectively capture the performance of AI approaches in this cutting-edge task.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i2.p1">
      <p class="ltx_p" id="S1.I1.i2.p1.1">
       We propose an effective and generalizable LLM agent framework, MatPlotAgent, that can improve the performance of a wide range of LLMs based on the newly proposed visual feedback mechanism.
      </p>
     </div>
    </li>
   </ul>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Task Description
  </h2>
  <div class="ltx_para" id="S2.p1">
   <p class="ltx_p" id="S2.p1.3">
    We first introduce the scientific data visualization task investigated in this work. Given a user query
    <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S2.p1.1.m1.1">
     <semantics id="S2.p1.1.m1.1a">
      <mi id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">
       𝐱
      </mi>
      <annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b">
       <ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">
        𝐱
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">
       \mathbf{x}
      </annotation>
     </semantics>
    </math>
    described in text and the corresponding data
    <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S2.p1.2.m2.1">
     <semantics id="S2.p1.2.m2.1a">
      <mi class="ltx_font_mathcaligraphic" id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">
       𝒟
      </mi>
      <annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b">
       <ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">
        𝒟
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">
       \mathcal{D}
      </annotation>
     </semantics>
    </math>
    , the AI system is expected to output a figure
    <math alttext="V" class="ltx_Math" display="inline" id="S2.p1.3.m3.1">
     <semantics id="S2.p1.3.m3.1a">
      <mi id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">
       V
      </mi>
      <annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b">
       <ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">
        𝑉
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">
       V
      </annotation>
     </semantics>
    </math>
    that can satisfy the user’s demand:
   </p>
   <table class="ltx_equation ltx_eqn_table" id="S2.E1">
    <tbody>
     <tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
      <td class="ltx_eqn_cell ltx_eqn_center_padleft">
      </td>
      <td class="ltx_eqn_cell ltx_align_center">
       <math alttext="V=f(\mathbf{x},\mathcal{D})," class="ltx_Math" display="block" id="S2.E1.m1.3">
        <semantics id="S2.E1.m1.3a">
         <mrow id="S2.E1.m1.3.3.1" xref="S2.E1.m1.3.3.1.1.cmml">
          <mrow id="S2.E1.m1.3.3.1.1" xref="S2.E1.m1.3.3.1.1.cmml">
           <mi id="S2.E1.m1.3.3.1.1.2" xref="S2.E1.m1.3.3.1.1.2.cmml">
            V
           </mi>
           <mo id="S2.E1.m1.3.3.1.1.1" xref="S2.E1.m1.3.3.1.1.1.cmml">
            =
           </mo>
           <mrow id="S2.E1.m1.3.3.1.1.3" xref="S2.E1.m1.3.3.1.1.3.cmml">
            <mi id="S2.E1.m1.3.3.1.1.3.2" xref="S2.E1.m1.3.3.1.1.3.2.cmml">
             f
            </mi>
            <mo id="S2.E1.m1.3.3.1.1.3.1" lspace="0em" rspace="0em" xref="S2.E1.m1.3.3.1.1.3.1.cmml">
             ​
            </mo>
            <mrow id="S2.E1.m1.3.3.1.1.3.3.2" xref="S2.E1.m1.3.3.1.1.3.3.1.cmml">
             <mo id="S2.E1.m1.3.3.1.1.3.3.2.1" stretchy="false" xref="S2.E1.m1.3.3.1.1.3.3.1.cmml">
              (
             </mo>
             <mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">
              𝐱
             </mi>
             <mo id="S2.E1.m1.3.3.1.1.3.3.2.2" xref="S2.E1.m1.3.3.1.1.3.3.1.cmml">
              ,
             </mo>
             <mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">
              𝒟
             </mi>
             <mo id="S2.E1.m1.3.3.1.1.3.3.2.3" stretchy="false" xref="S2.E1.m1.3.3.1.1.3.3.1.cmml">
              )
             </mo>
            </mrow>
           </mrow>
          </mrow>
          <mo id="S2.E1.m1.3.3.1.2" xref="S2.E1.m1.3.3.1.1.cmml">
           ,
          </mo>
         </mrow>
         <annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b">
          <apply id="S2.E1.m1.3.3.1.1.cmml" xref="S2.E1.m1.3.3.1">
           <eq id="S2.E1.m1.3.3.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1">
           </eq>
           <ci id="S2.E1.m1.3.3.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.2">
            𝑉
           </ci>
           <apply id="S2.E1.m1.3.3.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.3">
            <times id="S2.E1.m1.3.3.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.3.1">
            </times>
            <ci id="S2.E1.m1.3.3.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.3.2">
             𝑓
            </ci>
            <interval closure="open" id="S2.E1.m1.3.3.1.1.3.3.1.cmml" xref="S2.E1.m1.3.3.1.1.3.3.2">
             <ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">
              𝐱
             </ci>
             <ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">
              𝒟
             </ci>
            </interval>
           </apply>
          </apply>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S2.E1.m1.3c">
          V=f(\mathbf{x},\mathcal{D}),
         </annotation>
        </semantics>
       </math>
      </td>
      <td class="ltx_eqn_cell ltx_eqn_center_padright">
      </td>
      <td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1">
       <span class="ltx_tag ltx_tag_equation ltx_align_right">
        (1)
       </span>
      </td>
     </tr>
    </tbody>
   </table>
   <p class="ltx_p" id="S2.p1.4">
    where
    <math alttext="f" class="ltx_Math" display="inline" id="S2.p1.4.m1.1">
     <semantics id="S2.p1.4.m1.1a">
      <mi id="S2.p1.4.m1.1.1" xref="S2.p1.4.m1.1.1.cmml">
       f
      </mi>
      <annotation-xml encoding="MathML-Content" id="S2.p1.4.m1.1b">
       <ci id="S2.p1.4.m1.1.1.cmml" xref="S2.p1.4.m1.1.1">
        𝑓
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S2.p1.4.m1.1c">
       f
      </annotation>
     </semantics>
    </math>
    denotes the involved AI system that can be either an LLM or an LLM-based agent.
   </p>
  </div>
  <div class="ltx_para" id="S2.p2">
   <p class="ltx_p" id="S2.p2.3">
    Specifically,
    <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S2.p2.1.m1.1">
     <semantics id="S2.p2.1.m1.1a">
      <mi id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">
       𝐱
      </mi>
      <annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b">
       <ci id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">
        𝐱
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">
       \mathbf{x}
      </annotation>
     </semantics>
    </math>
    specifies the visualization requirements, encompassing the visualization type, data to plot, structural or spatial requirements for individual elements or the entire plot, and aesthetic preferences.
    <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S2.p2.2.m2.1">
     <semantics id="S2.p2.2.m2.1a">
      <mi class="ltx_font_mathcaligraphic" id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">
       𝒟
      </mi>
      <annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b">
       <ci id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">
        𝒟
       </ci>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">
       \mathcal{D}
      </annotation>
     </semantics>
    </math>
    represents the data, a collection of data points
    <math alttext="\left\{d_{1},\cdots,d_{n}\right\}" class="ltx_Math" display="inline" id="S2.p2.3.m3.3">
     <semantics id="S2.p2.3.m3.3a">
      <mrow id="S2.p2.3.m3.3.3.2" xref="S2.p2.3.m3.3.3.3.cmml">
       <mo id="S2.p2.3.m3.3.3.2.3" xref="S2.p2.3.m3.3.3.3.cmml">
        {
       </mo>
       <msub id="S2.p2.3.m3.2.2.1.1" xref="S2.p2.3.m3.2.2.1.1.cmml">
        <mi id="S2.p2.3.m3.2.2.1.1.2" xref="S2.p2.3.m3.2.2.1.1.2.cmml">
         d
        </mi>
        <mn id="S2.p2.3.m3.2.2.1.1.3" xref="S2.p2.3.m3.2.2.1.1.3.cmml">
         1
        </mn>
       </msub>
       <mo id="S2.p2.3.m3.3.3.2.4" xref="S2.p2.3.m3.3.3.3.cmml">
        ,
       </mo>
       <mi id="S2.p2.3.m3.1.1" mathvariant="normal" xref="S2.p2.3.m3.1.1.cmml">
        ⋯
       </mi>
       <mo id="S2.p2.3.m3.3.3.2.5" xref="S2.p2.3.m3.3.3.3.cmml">
        ,
       </mo>
       <msub id="S2.p2.3.m3.3.3.2.2" xref="S2.p2.3.m3.3.3.2.2.cmml">
        <mi id="S2.p2.3.m3.3.3.2.2.2" xref="S2.p2.3.m3.3.3.2.2.2.cmml">
         d
        </mi>
        <mi id="S2.p2.3.m3.3.3.2.2.3" xref="S2.p2.3.m3.3.3.2.2.3.cmml">
         n
        </mi>
       </msub>
       <mo id="S2.p2.3.m3.3.3.2.6" xref="S2.p2.3.m3.3.3.3.cmml">
        }
       </mo>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.3b">
       <set id="S2.p2.3.m3.3.3.3.cmml" xref="S2.p2.3.m3.3.3.2">
        <apply id="S2.p2.3.m3.2.2.1.1.cmml" xref="S2.p2.3.m3.2.2.1.1">
         <csymbol cd="ambiguous" id="S2.p2.3.m3.2.2.1.1.1.cmml" xref="S2.p2.3.m3.2.2.1.1">
          subscript
         </csymbol>
         <ci id="S2.p2.3.m3.2.2.1.1.2.cmml" xref="S2.p2.3.m3.2.2.1.1.2">
          𝑑
         </ci>
         <cn id="S2.p2.3.m3.2.2.1.1.3.cmml" type="integer" xref="S2.p2.3.m3.2.2.1.1.3">
          1
         </cn>
        </apply>
        <ci id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1">
         ⋯
        </ci>
        <apply id="S2.p2.3.m3.3.3.2.2.cmml" xref="S2.p2.3.m3.3.3.2.2">
         <csymbol cd="ambiguous" id="S2.p2.3.m3.3.3.2.2.1.cmml" xref="S2.p2.3.m3.3.3.2.2">
          subscript
         </csymbol>
         <ci id="S2.p2.3.m3.3.3.2.2.2.cmml" xref="S2.p2.3.m3.3.3.2.2.2">
          𝑑
         </ci>
         <ci id="S2.p2.3.m3.3.3.2.2.3.cmml" xref="S2.p2.3.m3.3.3.2.2.3">
          𝑛
         </ci>
        </apply>
       </set>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="S2.p2.3.m3.3c">
       \left\{d_{1},\cdots,d_{n}\right\}
      </annotation>
     </semantics>
    </math>
    whether specified by the user or stored in the external data file. Figure
    <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    provides some examples for this task.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   MatPlotBench
  </h2>
  <div class="ltx_para" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    Automatic evaluation is important in AI tasks as it enables researchers to efficiently assess the performance of various methods, thereby guiding the development of the field. While the DS-1000 benchmark
    <cite class="ltx_cite ltx_citemacro_cite">
     Lai et al. (
     <a class="ltx_ref" href="#bib.bib11" title="">
      2023
     </a>
     )
    </cite>
    includes coding problems about Matplotlib, the solutions’ average length is merely three lines, rendering them too simplistic to gauge the proficiency of contemporary AI agents in tackling practical challenges. Therefore, we propose to construct MatPlotBench with complex data visualization problems that are more close to real-world scenarios. We will illustrate the data collection process in Section
    <a class="ltx_ref" href="#S3.SS1" title="3.1 Data Collection ‣ 3 MatPlotBench ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization">
     <span class="ltx_text ltx_ref_tag">
      3.1
     </span>
    </a>
    and then explain the scoring mechanism in Section
    <a class="ltx_ref" href="#S3.SS2" title="3.2 Automatic Quantitative Evaluation ‣ 3 MatPlotBench ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization">
     <span class="ltx_text ltx_ref_tag">
      3.2
     </span>
    </a>
    .
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Data Collection
   </h3>
   <section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Principles
    </h4>
    <div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
     <p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">
      To enhance the quality of MatPlotBench, we adhere to the following principles for data collection: (1)
      <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px1.p1.1.1">
       Covering diverse types
      </span>
      : encompassing a broad range of plot types, including not only the most commonly used but also rare but useful ones; (2)
      <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px1.p1.1.2">
       Containing representative instances
      </span>
      : ensuring that the test examples reflect the representative features of scientific data visualization, such as varying data complexity; and (3)
      <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px1.p1.1.3">
       Balancing easy and challenging problems
      </span>
      : including problems of varying levels of difficulty in the benchmark.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Selecting Original Examples
    </h4>
    <div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
     <p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">
      In accordance with the principles outlined above, we first select some original examples from reputable online scientific data visualization forums. These examples are carefully selected from the Matplotlib Gallery and OriginLab GraphGallery, encompassing diverse and representative instances with varying levels of difficulty.
Specifically, we select 1 or 2 examples from every section in the Matplotlib Gallery, covering bars, lines, markers, pie charts, polar plots, contour plots, statistics plots, 3D plots, text annotations, radar charts, shapes, scales, axes, spines, subplots, and so on. We also seek more advanced test examples from the OriginLab GraphGallery, focusing on those that are more aesthetically appealing or complex, such as Sankey diagrams, sunburst charts, radial plots, chord diagrams, streamplots, and others. Finally, 75 original examples come from the Matplotlib Gallery and the 25 other original examples come from the OriginLab GraphGallery. Subsequently, these examples undergo several modifications to become the final test cases in MatPlotBench.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
    <h4 class="ltx_title ltx_title_paragraph">
     Preliminary Query Generation
    </h4>
    <div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
     <p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">
      Based on the selected original examples, we use LLMs to generate preliminary queries, which are then revised by humans. For original examples from the Matplotlib Gallery, we use GPT-4 to convert the code in each original example into preliminary queries. For the examples from the OriginLab GraphGallery, there are only images. We thus use GPT-4V to convert each image into a preliminary query.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S3.SS1.SSS0.Px4">
    <h4 class="ltx_title ltx_title_paragraph">
     Data Replacement
    </h4>
    <div class="ltx_para" id="S3.SS1.SSS0.Px4.p1">
     <p class="ltx_p" id="S3.SS1.SSS0.Px4.p1.1">
      Based on these preliminary queries, we begin data replacement for examples from the Matplotlib Gallery due to the observed phenomenon of memorization by GPT-4. In this process, we replace the original data points with newly generated ones, while keeping other factors such as the plot type unchanged. For examples from OriginLab, we find that the data is inherently complex, and even GPT-4 does not exhibit memorization with these examples. As a result, we only perform data replacement for Matplotlib examples.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S3.SS1.SSS0.Px5">
    <h4 class="ltx_title ltx_title_paragraph">
     Human Modification
    </h4>
    <div class="ltx_para" id="S3.SS1.SSS0.Px5.p1">
     <p class="ltx_p" id="S3.SS1.SSS0.Px5.p1.1">
      After completing the data replacement process, we engage human annotators to refine the preliminary queries. These annotators are tasked with correcting errors, eliminating ambiguity, and adding any omitted essential information. Each annotator involved has a minimum of three years of experience in coding and NLP. Furthermore, each query undergoes refinement by two independent human annotators.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S3.SS1.SSS0.Px6">
    <h4 class="ltx_title ltx_title_paragraph">
     Updating Ground-Truth Figures
    </h4>
    <div class="ltx_para" id="S3.SS1.SSS0.Px6.p1">
     <p class="ltx_p" id="S3.SS1.SSS0.Px6.p1.1">
      After obtaining the human-annotated queries, as the data in Matplotlib examples are altered, we cannot directly use the images in the original example as the ground truth. To this end, we manually wrote code to plot the ground truth for the Matplotlib examples. For examples from OriginLab, as the data remains unaltered, we extract the images from their website to serve as the ground truth.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S3.SS1.SSS0.Px7">
    <h4 class="ltx_title ltx_title_paragraph">
     Human Verification
    </h4>
    <div class="ltx_para" id="S3.SS1.SSS0.Px7.p1">
     <p class="ltx_p" id="S3.SS1.SSS0.Px7.p1.1">
      After obtaining the queries and their corresponding ground truths, we performed a final round of manual verification. Three NLP researchers were asked to conduct this verification. In this turn, the focus is mainly on checking whether the user queries and the ground truths are well aligned. The researchers meticulously checked each element in the ground truth image and looked for their corresponding descriptions in the user query. Ill-described elements and those missing clarifications are corrected. Redundant and incorrect descriptions are removed. This process results in 100 high-quality (
      <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px7.p1.1.1">
       query, raw data, ground-truth figure
      </span>
      ) triples, which comprise our final benchmark.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Automatic Quantitative Evaluation
   </h3>
   <div class="ltx_para" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     To ease the burden of manual evaluation and broaden the applicability of our benchmark for research purposes, we suggest employing GPT-4V, a cutting-edge multi-modal LLM, to conduct automatic evaluations on our proposed benchmark.
We carefully prompt GPT-4V to give a score from 0 to 100 on model-generated visualizations using the corresponding ground truths as the reference. The prompt is shown in Figure
     <a class="ltx_ref" href="#A1.F6" title="Figure 6 ‣ A.1 Evaluation Prompts ‣ Appendix A Detailed Prompts ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization">
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     in Appendix.
    </p>
   </div>
   <section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Correlation with Human Evaluation
    </h4>
    <div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
     <p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.10">
      To assess the reliability of GPT-4V as an automatic evaluator for scientific visualizations, we calculate the correlation between the automatic scores and human-evaluated scores. Specifically, we employ GPT-3.5 and GPT-4 to generate figures on MatPlotBench, and then conduct both automatic and human evaluation for the generated figures. For each model, we iteratively sample a subset that consists of
      <math alttext="n" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.1.m1.1">
       <semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a">
        <mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">
         n
        </mi>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b">
         <ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1">
          𝑛
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">
         n
        </annotation>
       </semantics>
      </math>
      examples from the total benchmark, and then calculate the average score of both automatic and human evaluation. This process repeats
      <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.2.m2.1">
       <semantics id="S3.SS2.SSS0.Px1.p1.2.m2.1a">
        <mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml">
         k
        </mi>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.1b">
         <ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1">
          𝑘
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.1c">
         k
        </annotation>
       </semantics>
      </math>
      times and we get
      <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.3.m3.1">
       <semantics id="S3.SS2.SSS0.Px1.p1.3.m3.1a">
        <mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml">
         k
        </mi>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.3.m3.1b">
         <ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1">
          𝑘
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.3.m3.1c">
         k
        </annotation>
       </semantics>
      </math>
      data points for each type of evaluation, which can be represented by
      <math alttext="\mathcal{A}=\{a_{1},\cdots,a_{k}\}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.4.m4.3">
       <semantics id="S3.SS2.SSS0.Px1.p1.4.m4.3a">
        <mrow id="S3.SS2.SSS0.Px1.p1.4.m4.3.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.3.3.cmml">
         <mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS0.Px1.p1.4.m4.3.3.4" xref="S3.SS2.SSS0.Px1.p1.4.m4.3.3.4.cmml">
          𝒜
         </mi>
         <mo id="S3.SS2.SSS0.Px1.p1.4.m4.3.3.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.3.3.3.cmml">
          =
         </mo>
         <mrow id="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.3.cmml">
          <mo id="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.2.3" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.3.cmml">
           {
          </mo>
          <msub id="S3.SS2.SSS0.Px1.p1.4.m4.2.2.1.1.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.2.1.1.1.cmml">
           <mi id="S3.SS2.SSS0.Px1.p1.4.m4.2.2.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.2.1.1.1.2.cmml">
            a
           </mi>
           <mn id="S3.SS2.SSS0.Px1.p1.4.m4.2.2.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.2.1.1.1.3.cmml">
            1
           </mn>
          </msub>
          <mo id="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.2.4" xref="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.3.cmml">
           ,
          </mo>
          <mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1" mathvariant="normal" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml">
           ⋯
          </mi>
          <mo id="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.2.5" xref="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.3.cmml">
           ,
          </mo>
          <msub id="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.2.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.2.2.cmml">
           <mi id="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.2.2.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.2.2.2.cmml">
            a
           </mi>
           <mi id="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.2.2.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.2.2.3.cmml">
            k
           </mi>
          </msub>
          <mo id="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.2.6" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.3.cmml">
           }
          </mo>
         </mrow>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.4.m4.3b">
         <apply id="S3.SS2.SSS0.Px1.p1.4.m4.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.3.3">
          <eq id="S3.SS2.SSS0.Px1.p1.4.m4.3.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.3.3.3">
          </eq>
          <ci id="S3.SS2.SSS0.Px1.p1.4.m4.3.3.4.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.3.3.4">
           𝒜
          </ci>
          <set id="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.2">
           <apply id="S3.SS2.SSS0.Px1.p1.4.m4.2.2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.2.1.1.1">
            <csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.4.m4.2.2.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.2.1.1.1">
             subscript
            </csymbol>
            <ci id="S3.SS2.SSS0.Px1.p1.4.m4.2.2.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.2.1.1.1.2">
             𝑎
            </ci>
            <cn id="S3.SS2.SSS0.Px1.p1.4.m4.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.4.m4.2.2.1.1.1.3">
             1
            </cn>
           </apply>
           <ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1">
            ⋯
           </ci>
           <apply id="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.2.2">
            <csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.2.2">
             subscript
            </csymbol>
            <ci id="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.2.2.2">
             𝑎
            </ci>
            <ci id="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.3.3.2.2.2.3">
             𝑘
            </ci>
           </apply>
          </set>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.4.m4.3c">
         \mathcal{A}=\{a_{1},\cdots,a_{k}\}
        </annotation>
       </semantics>
      </math>
      and
      <math alttext="\mathcal{H}=\{h_{1},\cdots,h_{k}\}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.5.m5.3">
       <semantics id="S3.SS2.SSS0.Px1.p1.5.m5.3a">
        <mrow id="S3.SS2.SSS0.Px1.p1.5.m5.3.3" xref="S3.SS2.SSS0.Px1.p1.5.m5.3.3.cmml">
         <mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS0.Px1.p1.5.m5.3.3.4" xref="S3.SS2.SSS0.Px1.p1.5.m5.3.3.4.cmml">
          ℋ
         </mi>
         <mo id="S3.SS2.SSS0.Px1.p1.5.m5.3.3.3" xref="S3.SS2.SSS0.Px1.p1.5.m5.3.3.3.cmml">
          =
         </mo>
         <mrow id="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.2" xref="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.3.cmml">
          <mo id="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.2.3" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.3.cmml">
           {
          </mo>
          <msub id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.1.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.1.1.1.cmml">
           <mi id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.1.1.1.2.cmml">
            h
           </mi>
           <mn id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.1.1.1.3.cmml">
            1
           </mn>
          </msub>
          <mo id="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.2.4" xref="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.3.cmml">
           ,
          </mo>
          <mi id="S3.SS2.SSS0.Px1.p1.5.m5.1.1" mathvariant="normal" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.cmml">
           ⋯
          </mi>
          <mo id="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.2.5" xref="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.3.cmml">
           ,
          </mo>
          <msub id="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.2.2" xref="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.2.2.cmml">
           <mi id="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.2.2.2" xref="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.2.2.2.cmml">
            h
           </mi>
           <mi id="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.2.2.3" xref="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.2.2.3.cmml">
            k
           </mi>
          </msub>
          <mo id="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.2.6" stretchy="false" xref="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.3.cmml">
           }
          </mo>
         </mrow>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.5.m5.3b">
         <apply id="S3.SS2.SSS0.Px1.p1.5.m5.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.3.3">
          <eq id="S3.SS2.SSS0.Px1.p1.5.m5.3.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.3.3.3">
          </eq>
          <ci id="S3.SS2.SSS0.Px1.p1.5.m5.3.3.4.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.3.3.4">
           ℋ
          </ci>
          <set id="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.2">
           <apply id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.1.1.1">
            <csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.1.1.1">
             subscript
            </csymbol>
            <ci id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.1.1.1.2">
             ℎ
            </ci>
            <cn id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.1.1.1.3">
             1
            </cn>
           </apply>
           <ci id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1">
            ⋯
           </ci>
           <apply id="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.2.2">
            <csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.2.2">
             subscript
            </csymbol>
            <ci id="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.2.2.2">
             ℎ
            </ci>
            <ci id="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.3.3.2.2.2.3">
             𝑘
            </ci>
           </apply>
          </set>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.5.m5.3c">
         \mathcal{H}=\{h_{1},\cdots,h_{k}\}
        </annotation>
       </semantics>
      </math>
      .
      <math alttext="a_{i}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.6.m6.1">
       <semantics id="S3.SS2.SSS0.Px1.p1.6.m6.1a">
        <msub id="S3.SS2.SSS0.Px1.p1.6.m6.1.1" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.cmml">
         <mi id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.2" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.2.cmml">
          a
         </mi>
         <mi id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.3" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.3.cmml">
          i
         </mi>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.6.m6.1b">
         <apply id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1">
          <csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1">
           subscript
          </csymbol>
          <ci id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.2">
           𝑎
          </ci>
          <ci id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.3">
           𝑖
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.6.m6.1c">
         a_{i}
        </annotation>
       </semantics>
      </math>
      denotes the average automatic score on the
      <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.7.m7.1">
       <semantics id="S3.SS2.SSS0.Px1.p1.7.m7.1a">
        <mi id="S3.SS2.SSS0.Px1.p1.7.m7.1.1" xref="S3.SS2.SSS0.Px1.p1.7.m7.1.1.cmml">
         i
        </mi>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.7.m7.1b">
         <ci id="S3.SS2.SSS0.Px1.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m7.1.1">
          𝑖
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.7.m7.1c">
         i
        </annotation>
       </semantics>
      </math>
      -th randomly sampled subset, and
      <math alttext="h_{i}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.8.m8.1">
       <semantics id="S3.SS2.SSS0.Px1.p1.8.m8.1a">
        <msub id="S3.SS2.SSS0.Px1.p1.8.m8.1.1" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.cmml">
         <mi id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.2" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.2.cmml">
          h
         </mi>
         <mi id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.3" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.3.cmml">
          i
         </mi>
        </msub>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.8.m8.1b">
         <apply id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1">
          <csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1">
           subscript
          </csymbol>
          <ci id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.2">
           ℎ
          </ci>
          <ci id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.3">
           𝑖
          </ci>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.8.m8.1c">
         h_{i}
        </annotation>
       </semantics>
      </math>
      represents the average human-evaluated score in the same subset.
      <math alttext="n" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.9.m9.1">
       <semantics id="S3.SS2.SSS0.Px1.p1.9.m9.1a">
        <mi id="S3.SS2.SSS0.Px1.p1.9.m9.1.1" xref="S3.SS2.SSS0.Px1.p1.9.m9.1.1.cmml">
         n
        </mi>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.9.m9.1b">
         <ci id="S3.SS2.SSS0.Px1.p1.9.m9.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.9.m9.1.1">
          𝑛
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.9.m9.1c">
         n
        </annotation>
       </semantics>
      </math>
      and
      <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.10.m10.1">
       <semantics id="S3.SS2.SSS0.Px1.p1.10.m10.1a">
        <mi id="S3.SS2.SSS0.Px1.p1.10.m10.1.1" xref="S3.SS2.SSS0.Px1.p1.10.m10.1.1.cmml">
         k
        </mi>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.10.m10.1b">
         <ci id="S3.SS2.SSS0.Px1.p1.10.m10.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.10.m10.1.1">
          𝑘
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.10.m10.1c">
         k
        </annotation>
       </semantics>
      </math>
      are set to 25 and 100, respectively.
     </p>
    </div>
    <figure class="ltx_figure" id="S3.F2">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="310" id="S3.F2.g1" src="/html/2402.11453/assets/x2.png" width="415"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       Figure 2:
      </span>
      Correlation between the proposed automatic evaluation mechanism and human evaluation.
     </figcaption>
    </figure>
    <div class="ltx_para" id="S3.SS2.SSS0.Px1.p2">
     <p class="ltx_p" id="S3.SS2.SSS0.Px1.p2.8">
      We utilize the statistical functions provided by
      <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS0.Px1.p2.8.1">
       scipy
       <span class="ltx_note ltx_role_footnote" id="footnote5">
        <sup class="ltx_note_mark">
         5
        </sup>
        <span class="ltx_note_outer">
         <span class="ltx_note_content">
          <sup class="ltx_note_mark">
           5
          </sup>
          <span class="ltx_tag ltx_tag_note">
           <span class="ltx_text ltx_font_serif" id="footnote5.1.1.1">
            5
           </span>
          </span>
          <a class="ltx_ref ltx_url" href="https://docs.scipy.org/doc/scipy/reference/stats.html" target="_blank" title="">
           https://docs.scipy.org/doc/scipy/reference/stats.html
          </a>
         </span>
        </span>
       </span>
      </span>
      to compute the Pearson correlation coefficient
      <math alttext="r" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p2.1.m1.1">
       <semantics id="S3.SS2.SSS0.Px1.p2.1.m1.1a">
        <mi id="S3.SS2.SSS0.Px1.p2.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p2.1.m1.1.1.cmml">
         r
        </mi>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.1.m1.1b">
         <ci id="S3.SS2.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.1.m1.1.1">
          𝑟
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.1.m1.1c">
         r
        </annotation>
       </semantics>
      </math>
      and the corresponding p-value
      <math alttext="p" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p2.2.m2.1">
       <semantics id="S3.SS2.SSS0.Px1.p2.2.m2.1a">
        <mi id="S3.SS2.SSS0.Px1.p2.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1.cmml">
         p
        </mi>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.2.m2.1b">
         <ci id="S3.SS2.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.2.m2.1.1">
          𝑝
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.2.m2.1c">
         p
        </annotation>
       </semantics>
      </math>
      . For GPT-4, we obtain
      <math alttext="r" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p2.3.m3.1">
       <semantics id="S3.SS2.SSS0.Px1.p2.3.m3.1a">
        <mi id="S3.SS2.SSS0.Px1.p2.3.m3.1.1" xref="S3.SS2.SSS0.Px1.p2.3.m3.1.1.cmml">
         r
        </mi>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.3.m3.1b">
         <ci id="S3.SS2.SSS0.Px1.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.3.m3.1.1">
          𝑟
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.3.m3.1c">
         r
        </annotation>
       </semantics>
      </math>
      =0.876 and
      <math alttext="p" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p2.4.m4.1">
       <semantics id="S3.SS2.SSS0.Px1.p2.4.m4.1a">
        <mi id="S3.SS2.SSS0.Px1.p2.4.m4.1.1" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1.cmml">
         p
        </mi>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.4.m4.1b">
         <ci id="S3.SS2.SSS0.Px1.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.4.m4.1.1">
          𝑝
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.4.m4.1c">
         p
        </annotation>
       </semantics>
      </math>
      =7.41e-33, while for GPT-3.5, the values are
      <math alttext="r" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p2.5.m5.1">
       <semantics id="S3.SS2.SSS0.Px1.p2.5.m5.1a">
        <mi id="S3.SS2.SSS0.Px1.p2.5.m5.1.1" xref="S3.SS2.SSS0.Px1.p2.5.m5.1.1.cmml">
         r
        </mi>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.5.m5.1b">
         <ci id="S3.SS2.SSS0.Px1.p2.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.5.m5.1.1">
          𝑟
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.5.m5.1c">
         r
        </annotation>
       </semantics>
      </math>
      =0.836 and
      <math alttext="p" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p2.6.m6.1">
       <semantics id="S3.SS2.SSS0.Px1.p2.6.m6.1a">
        <mi id="S3.SS2.SSS0.Px1.p2.6.m6.1.1" xref="S3.SS2.SSS0.Px1.p2.6.m6.1.1.cmml">
         p
        </mi>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.6.m6.1b">
         <ci id="S3.SS2.SSS0.Px1.p2.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.6.m6.1.1">
          𝑝
         </ci>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.6.m6.1c">
         p
        </annotation>
       </semantics>
      </math>
      =2.67e-27. Figure
      <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ Correlation with Human Evaluation ‣ 3.2 Automatic Quantitative Evaluation ‣ 3 MatPlotBench ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization">
       <span class="ltx_text ltx_ref_tag">
        2
       </span>
      </a>
      shows the data points for GPT-4. Given that
      <math alttext="r&gt;0.8" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p2.7.m7.1">
       <semantics id="S3.SS2.SSS0.Px1.p2.7.m7.1a">
        <mrow id="S3.SS2.SSS0.Px1.p2.7.m7.1.1" xref="S3.SS2.SSS0.Px1.p2.7.m7.1.1.cmml">
         <mi id="S3.SS2.SSS0.Px1.p2.7.m7.1.1.2" xref="S3.SS2.SSS0.Px1.p2.7.m7.1.1.2.cmml">
          r
         </mi>
         <mo id="S3.SS2.SSS0.Px1.p2.7.m7.1.1.1" xref="S3.SS2.SSS0.Px1.p2.7.m7.1.1.1.cmml">
          &gt;
         </mo>
         <mn id="S3.SS2.SSS0.Px1.p2.7.m7.1.1.3" xref="S3.SS2.SSS0.Px1.p2.7.m7.1.1.3.cmml">
          0.8
         </mn>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.7.m7.1b">
         <apply id="S3.SS2.SSS0.Px1.p2.7.m7.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.7.m7.1.1">
          <gt id="S3.SS2.SSS0.Px1.p2.7.m7.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.7.m7.1.1.1">
          </gt>
          <ci id="S3.SS2.SSS0.Px1.p2.7.m7.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p2.7.m7.1.1.2">
           𝑟
          </ci>
          <cn id="S3.SS2.SSS0.Px1.p2.7.m7.1.1.3.cmml" type="float" xref="S3.SS2.SSS0.Px1.p2.7.m7.1.1.3">
           0.8
          </cn>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.7.m7.1c">
         r&gt;0.8
        </annotation>
       </semantics>
      </math>
      and
      <math alttext="p&lt;" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p2.8.m8.1">
       <semantics id="S3.SS2.SSS0.Px1.p2.8.m8.1a">
        <mrow id="S3.SS2.SSS0.Px1.p2.8.m8.1.1" xref="S3.SS2.SSS0.Px1.p2.8.m8.1.1.cmml">
         <mi id="S3.SS2.SSS0.Px1.p2.8.m8.1.1.2" xref="S3.SS2.SSS0.Px1.p2.8.m8.1.1.2.cmml">
          p
         </mi>
         <mo id="S3.SS2.SSS0.Px1.p2.8.m8.1.1.1" xref="S3.SS2.SSS0.Px1.p2.8.m8.1.1.1.cmml">
          &lt;
         </mo>
         <mi id="S3.SS2.SSS0.Px1.p2.8.m8.1.1.3" xref="S3.SS2.SSS0.Px1.p2.8.m8.1.1.3.cmml">
         </mi>
        </mrow>
        <annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p2.8.m8.1b">
         <apply id="S3.SS2.SSS0.Px1.p2.8.m8.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.8.m8.1.1">
          <lt id="S3.SS2.SSS0.Px1.p2.8.m8.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p2.8.m8.1.1.1">
          </lt>
          <ci id="S3.SS2.SSS0.Px1.p2.8.m8.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p2.8.m8.1.1.2">
           𝑝
          </ci>
          <csymbol cd="latexml" id="S3.SS2.SSS0.Px1.p2.8.m8.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p2.8.m8.1.1.3">
           absent
          </csymbol>
         </apply>
        </annotation-xml>
        <annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p2.8.m8.1c">
         p&lt;
        </annotation>
       </semantics>
      </math>
      0.05, we conclude that the automatic evaluation scores are strongly correlated with human evaluation results. This demonstrates the reliability of the proposed scoring mechanism in assessing the quality of model-generated figures on MatPlotBench.
     </p>
    </div>
   </section>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   MatPlotAgent
  </h2>
  <figure class="ltx_figure" id="S4.F3">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="257" id="S4.F3.g1" src="/html/2402.11453/assets/x3.png" width="456"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 3:
    </span>
    Workflow of MatPlotAgent: The query expansion module converts the user query into detailed multi-step instructions. These instructions are then passed to the code agent, which generates the plotting code. The visual agent provides informative feedback based on the current draft, guiding the refinement of the figure.
   </figcaption>
  </figure>
  <div class="ltx_para" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    To improve the capabilities of LLMs for scientific data visualization, we propose an agentic framework that mimics the plotting process of human experts. The proposed MatPlotAgent is comprised of three modules, including the query expansion module, the code agent, and the visual agent. Figure
    <a class="ltx_ref" href="#S4.F3" title="Figure 3 ‣ 4 MatPlotAgent ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization">
     <span class="ltx_text ltx_ref_tag">
      3
     </span>
    </a>
    illustrates the workflow of MatPlotAgent.
   </p>
  </div>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    Query Expansion
   </h3>
   <div class="ltx_para" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     The query expansion module interprets and refines the user query, converting the high-level requirements into a sequence of explicit and detailed instructions that are easy for LLMs to follow. This module can also be viewed as a planning module, creating an overall plan before generating the figure. Specifically, this module is based on the involved code LLM, which is prompted to give detailed instructions on how to use code to fulfill the requirement specified by the user, including what libraries to import, what library functions to call, how to set the parameters in each function correctly, how to prepare the data, how to manipulate the data, and so on.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    Code Agent
   </h3>
   <div class="ltx_para" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     The code agent is the core component in MatPlotAgent, responsible for generating the code to plot figures.
Given detailed instructions from the query expansion module, the code agent first generates the code using appropriate libraries and functions. To improve the success rate of the generated code, we also employ the self-debugging mechanism
     <cite class="ltx_cite ltx_citemacro_cite">
      Chen et al. (
      <a class="ltx_ref" href="#bib.bib5" title="">
       2024b
      </a>
      )
     </cite>
     , which helps the involved code LLM iteratively identify and correct bugs in the code. To prevent an infinite loop, we set the maximum iterations of self-debugging to 3.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p2">
    <p class="ltx_p" id="S4.SS2.p2.1">
     Similar to humans, who need to repeatedly refine the figure based on current drafts, we also introduce a visual feedback mechanism. This mechanism employs multi-modal LLMs to provide suggestions to improve the figure and better fulfill the user’s queries. These suggestions, which we call
     <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1">
      visual feedback
     </span>
     , are then provided to the code agent to further improve the code.
Our experiments in Section
     <a class="ltx_ref" href="#S5.SS2" title="5.2 Main Results ‣ 5 Experiments ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization">
      <span class="ltx_text ltx_ref_tag">
       5.2
      </span>
     </a>
     demonstrate that MatPlotAgent is compatible with several modern code LLMs, including both some well-known closed-source models and some open-source models.
    </p>
   </div>
   <figure class="ltx_table" id="S4.T1">
    <table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.14">
     <tr class="ltx_tr" id="S4.T1.14.15">
      <td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.14.15.1" rowspan="2">
       <span class="ltx_text ltx_font_bold" id="S4.T1.14.15.1.1">
        Model
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.14.15.2">
       <span class="ltx_text ltx_font_bold" id="S4.T1.14.15.2.1">
        Direct
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T1.14.15.3">
       <span class="ltx_text ltx_font_bold" id="S4.T1.14.15.3.1">
        Zero-Shot
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T1.14.15.4" rowspan="2">
       <span class="ltx_text ltx_font_bold" id="S4.T1.14.15.4.1">
        MatPlotAgent
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T1.14.16">
      <td class="ltx_td ltx_align_center" id="S4.T1.14.16.1">
       <span class="ltx_text ltx_font_bold" id="S4.T1.14.16.1.1">
        Decod.
       </span>
      </td>
      <td class="ltx_td ltx_align_center" colspan="2" id="S4.T1.14.16.2">
       <span class="ltx_text ltx_font_bold" id="S4.T1.14.16.2.1">
        CoT
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T1.2.2">
      <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.2.2.3">
       GPT-4
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.4">
       48.86
      </td>
      <td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.2.2.5">
       45.42
      </td>
      <td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.1.1" style="background-color:#C7C7FF;">
       <math alttext="-" class="ltx_Math" display="inline" id="S4.T1.1.1.1.m1.1">
        <semantics id="S4.T1.1.1.1.m1.1a">
         <mo id="S4.T1.1.1.1.m1.1.1" mathbackground="#C7C7FF" xref="S4.T1.1.1.1.m1.1.1.cmml">
          −
         </mo>
         <annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b">
          <minus id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">
          </minus>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">
          -
         </annotation>
        </semantics>
       </math>
       3.44
      </td>
      <td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.2.2.6">
       61.16
      </td>
      <td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.2.2.2" style="background-color:#FFC7C7;">
       <math alttext="+" class="ltx_Math" display="inline" id="S4.T1.2.2.2.m1.1">
        <semantics id="S4.T1.2.2.2.m1.1a">
         <mo id="S4.T1.2.2.2.m1.1.1" mathbackground="#FFC7C7" xref="S4.T1.2.2.2.m1.1.1.cmml">
          +
         </mo>
         <annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b">
          <plus id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1">
          </plus>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">
          +
         </annotation>
        </semantics>
       </math>
       12.30
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T1.4.4">
      <td class="ltx_td ltx_align_left" id="S4.T1.4.4.3">
       GPT-3.5
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T1.4.4.4">
       38.03
      </td>
      <td class="ltx_td ltx_align_right" id="S4.T1.4.4.5">
       37.14
      </td>
      <td class="ltx_td ltx_align_right" id="S4.T1.3.3.1" style="background-color:#C7C7FF;">
       <math alttext="-" class="ltx_Math" display="inline" id="S4.T1.3.3.1.m1.1">
        <semantics id="S4.T1.3.3.1.m1.1a">
         <mo id="S4.T1.3.3.1.m1.1.1" mathbackground="#C7C7FF" xref="S4.T1.3.3.1.m1.1.1.cmml">
          −
         </mo>
         <annotation-xml encoding="MathML-Content" id="S4.T1.3.3.1.m1.1b">
          <minus id="S4.T1.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1">
          </minus>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T1.3.3.1.m1.1c">
          -
         </annotation>
        </semantics>
       </math>
       0.89
      </td>
      <td class="ltx_td ltx_align_right" id="S4.T1.4.4.6">
       47.51
      </td>
      <td class="ltx_td ltx_align_right" id="S4.T1.4.4.2" style="background-color:#FFC7C7;">
       <math alttext="+" class="ltx_Math" display="inline" id="S4.T1.4.4.2.m1.1">
        <semantics id="S4.T1.4.4.2.m1.1a">
         <mo id="S4.T1.4.4.2.m1.1.1" mathbackground="#FFC7C7" xref="S4.T1.4.4.2.m1.1.1.cmml">
          +
         </mo>
         <annotation-xml encoding="MathML-Content" id="S4.T1.4.4.2.m1.1b">
          <plus id="S4.T1.4.4.2.m1.1.1.cmml" xref="S4.T1.4.4.2.m1.1.1">
          </plus>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T1.4.4.2.m1.1c">
          +
         </annotation>
        </semantics>
       </math>
       9.48
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T1.6.6">
      <td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.6.6.3">
       Magicoder-S-DS-6.7B
       <cite class="ltx_cite ltx_citemacro_cite">
        Wei et al. (
        <a class="ltx_ref" href="#bib.bib34" title="">
         2023
        </a>
        )
       </cite>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.6.6.4">
       38.49
      </td>
      <td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.6.6.5">
       37.95
      </td>
      <td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.5.5.1" style="background-color:#C7C7FF;">
       <math alttext="-" class="ltx_Math" display="inline" id="S4.T1.5.5.1.m1.1">
        <semantics id="S4.T1.5.5.1.m1.1a">
         <mo id="S4.T1.5.5.1.m1.1.1" mathbackground="#C7C7FF" xref="S4.T1.5.5.1.m1.1.1.cmml">
          −
         </mo>
         <annotation-xml encoding="MathML-Content" id="S4.T1.5.5.1.m1.1b">
          <minus id="S4.T1.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.1.m1.1.1">
          </minus>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T1.5.5.1.m1.1c">
          -
         </annotation>
        </semantics>
       </math>
       0.54
      </td>
      <td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.6.6.6">
       51.70
      </td>
      <td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.6.6.2" style="background-color:#FFC7C7;">
       <math alttext="+" class="ltx_Math" display="inline" id="S4.T1.6.6.2.m1.1">
        <semantics id="S4.T1.6.6.2.m1.1a">
         <mo id="S4.T1.6.6.2.m1.1.1" mathbackground="#FFC7C7" xref="S4.T1.6.6.2.m1.1.1.cmml">
          +
         </mo>
         <annotation-xml encoding="MathML-Content" id="S4.T1.6.6.2.m1.1b">
          <plus id="S4.T1.6.6.2.m1.1.1.cmml" xref="S4.T1.6.6.2.m1.1.1">
          </plus>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T1.6.6.2.m1.1c">
          +
         </annotation>
        </semantics>
       </math>
       13.21
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T1.8.8">
      <td class="ltx_td ltx_align_left" id="S4.T1.8.8.3">
       Deepseek-coder-6.7B-instruct
       <cite class="ltx_cite ltx_citemacro_cite">
        Guo et al. (
        <a class="ltx_ref" href="#bib.bib7" title="">
         2024
        </a>
        )
       </cite>
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T1.8.8.4">
       31.53
      </td>
      <td class="ltx_td ltx_align_right" id="S4.T1.8.8.5">
       29.16
      </td>
      <td class="ltx_td ltx_align_right" id="S4.T1.7.7.1" style="background-color:#C7C7FF;">
       <math alttext="-" class="ltx_Math" display="inline" id="S4.T1.7.7.1.m1.1">
        <semantics id="S4.T1.7.7.1.m1.1a">
         <mo id="S4.T1.7.7.1.m1.1.1" mathbackground="#C7C7FF" xref="S4.T1.7.7.1.m1.1.1.cmml">
          −
         </mo>
         <annotation-xml encoding="MathML-Content" id="S4.T1.7.7.1.m1.1b">
          <minus id="S4.T1.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.1.m1.1.1">
          </minus>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T1.7.7.1.m1.1c">
          -
         </annotation>
        </semantics>
       </math>
       2.37
      </td>
      <td class="ltx_td ltx_align_right" id="S4.T1.8.8.6">
       39.45
      </td>
      <td class="ltx_td ltx_align_right" id="S4.T1.8.8.2" style="background-color:#FFC7C7;">
       <math alttext="+" class="ltx_Math" display="inline" id="S4.T1.8.8.2.m1.1">
        <semantics id="S4.T1.8.8.2.m1.1a">
         <mo id="S4.T1.8.8.2.m1.1.1" mathbackground="#FFC7C7" xref="S4.T1.8.8.2.m1.1.1.cmml">
          +
         </mo>
         <annotation-xml encoding="MathML-Content" id="S4.T1.8.8.2.m1.1b">
          <plus id="S4.T1.8.8.2.m1.1.1.cmml" xref="S4.T1.8.8.2.m1.1.1">
          </plus>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T1.8.8.2.m1.1c">
          +
         </annotation>
        </semantics>
       </math>
       7.92
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T1.10.10">
      <td class="ltx_td ltx_align_left" id="S4.T1.10.10.3">
       CodeLlama-34B-Instruct
       <cite class="ltx_cite ltx_citemacro_cite">
        Rozière et al. (
        <a class="ltx_ref" href="#bib.bib27" title="">
         2024
        </a>
        )
       </cite>
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T1.10.10.4">
       16.54
      </td>
      <td class="ltx_td ltx_align_right" id="S4.T1.10.10.5">
       12.40
      </td>
      <td class="ltx_td ltx_align_right" id="S4.T1.9.9.1" style="background-color:#C7C7FF;">
       <math alttext="-" class="ltx_Math" display="inline" id="S4.T1.9.9.1.m1.1">
        <semantics id="S4.T1.9.9.1.m1.1a">
         <mo id="S4.T1.9.9.1.m1.1.1" mathbackground="#C7C7FF" xref="S4.T1.9.9.1.m1.1.1.cmml">
          −
         </mo>
         <annotation-xml encoding="MathML-Content" id="S4.T1.9.9.1.m1.1b">
          <minus id="S4.T1.9.9.1.m1.1.1.cmml" xref="S4.T1.9.9.1.m1.1.1">
          </minus>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T1.9.9.1.m1.1c">
          -
         </annotation>
        </semantics>
       </math>
       4.14
      </td>
      <td class="ltx_td ltx_align_right" id="S4.T1.10.10.6">
       14.18
      </td>
      <td class="ltx_td ltx_align_right" id="S4.T1.10.10.2" style="background-color:#C7C7FF;">
       <math alttext="-" class="ltx_Math" display="inline" id="S4.T1.10.10.2.m1.1">
        <semantics id="S4.T1.10.10.2.m1.1a">
         <mo id="S4.T1.10.10.2.m1.1.1" mathbackground="#C7C7FF" xref="S4.T1.10.10.2.m1.1.1.cmml">
          −
         </mo>
         <annotation-xml encoding="MathML-Content" id="S4.T1.10.10.2.m1.1b">
          <minus id="S4.T1.10.10.2.m1.1.1.cmml" xref="S4.T1.10.10.2.m1.1.1">
          </minus>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T1.10.10.2.m1.1c">
          -
         </annotation>
        </semantics>
       </math>
       2.36
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T1.12.12">
      <td class="ltx_td ltx_align_left" id="S4.T1.12.12.3">
       Deepseek-coder-33B-instruct
       <cite class="ltx_cite ltx_citemacro_cite">
        Guo et al. (
        <a class="ltx_ref" href="#bib.bib7" title="">
         2024
        </a>
        )
       </cite>
      </td>
      <td class="ltx_td ltx_align_center" id="S4.T1.12.12.4">
       30.88
      </td>
      <td class="ltx_td ltx_align_right" id="S4.T1.12.12.5">
       36.10
      </td>
      <td class="ltx_td ltx_align_right" id="S4.T1.11.11.1" style="background-color:#FFC7C7;">
       <math alttext="+" class="ltx_Math" display="inline" id="S4.T1.11.11.1.m1.1">
        <semantics id="S4.T1.11.11.1.m1.1a">
         <mo id="S4.T1.11.11.1.m1.1.1" mathbackground="#FFC7C7" xref="S4.T1.11.11.1.m1.1.1.cmml">
          +
         </mo>
         <annotation-xml encoding="MathML-Content" id="S4.T1.11.11.1.m1.1b">
          <plus id="S4.T1.11.11.1.m1.1.1.cmml" xref="S4.T1.11.11.1.m1.1.1">
          </plus>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T1.11.11.1.m1.1c">
          +
         </annotation>
        </semantics>
       </math>
       5.22
      </td>
      <td class="ltx_td ltx_align_right" id="S4.T1.12.12.6">
       32.18
      </td>
      <td class="ltx_td ltx_align_right" id="S4.T1.12.12.2" style="background-color:#FFC7C7;">
       <math alttext="+" class="ltx_Math" display="inline" id="S4.T1.12.12.2.m1.1">
        <semantics id="S4.T1.12.12.2.m1.1a">
         <mo id="S4.T1.12.12.2.m1.1.1" mathbackground="#FFC7C7" xref="S4.T1.12.12.2.m1.1.1.cmml">
          +
         </mo>
         <annotation-xml encoding="MathML-Content" id="S4.T1.12.12.2.m1.1b">
          <plus id="S4.T1.12.12.2.m1.1.1.cmml" xref="S4.T1.12.12.2.m1.1.1">
          </plus>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T1.12.12.2.m1.1c">
          +
         </annotation>
        </semantics>
       </math>
       1.30
      </td>
     </tr>
     <tr class="ltx_tr" id="S4.T1.14.14">
      <td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.14.14.3">
       WizardCoder-Python-33B-V1.1
       <cite class="ltx_cite ltx_citemacro_cite">
        Luo et al. (
        <a class="ltx_ref" href="#bib.bib17" title="">
         2023b
        </a>
        )
       </cite>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.14.14.4">
       36.94
      </td>
      <td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.14.14.5">
       35.81
      </td>
      <td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.13.13.1" style="background-color:#C7C7FF;">
       <math alttext="-" class="ltx_Math" display="inline" id="S4.T1.13.13.1.m1.1">
        <semantics id="S4.T1.13.13.1.m1.1a">
         <mo id="S4.T1.13.13.1.m1.1.1" mathbackground="#C7C7FF" xref="S4.T1.13.13.1.m1.1.1.cmml">
          −
         </mo>
         <annotation-xml encoding="MathML-Content" id="S4.T1.13.13.1.m1.1b">
          <minus id="S4.T1.13.13.1.m1.1.1.cmml" xref="S4.T1.13.13.1.m1.1.1">
          </minus>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T1.13.13.1.m1.1c">
          -
         </annotation>
        </semantics>
       </math>
       1.13
      </td>
      <td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.14.14.6">
       45.96
      </td>
      <td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T1.14.14.2" style="background-color:#FFC7C7;">
       <math alttext="+" class="ltx_Math" display="inline" id="S4.T1.14.14.2.m1.1">
        <semantics id="S4.T1.14.14.2.m1.1a">
         <mo id="S4.T1.14.14.2.m1.1.1" mathbackground="#FFC7C7" xref="S4.T1.14.14.2.m1.1.1.cmml">
          +
         </mo>
         <annotation-xml encoding="MathML-Content" id="S4.T1.14.14.2.m1.1b">
          <plus id="S4.T1.14.14.2.m1.1.1.cmml" xref="S4.T1.14.14.2.m1.1.1">
          </plus>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S4.T1.14.14.2.m1.1c">
          +
         </annotation>
        </semantics>
       </math>
       9.02
      </td>
     </tr>
    </table>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 1:
     </span>
     Performance of different LLMs on MatPlotBench. For each model, improvements over the direct decoding are highlighted in
     <span class="ltx_text" id="S4.T1.17.1" style="color:#FF8080;">
      red
     </span>
     , while results worse than that of the direct decoding are highlighted in
     <span class="ltx_text" id="S4.T1.18.2" style="color:#8080FF;">
      blue
     </span>
     .
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3
    </span>
    Visual Agent
   </h3>
   <div class="ltx_para" id="S4.SS3.p1">
    <p class="ltx_p" id="S4.SS3.p1.1">
     The major difference between MatPlotAgent and previous LLM-based coding agents
     <cite class="ltx_cite ltx_citemacro_cite">
      Qian et al. (
      <a class="ltx_ref" href="#bib.bib21" title="">
       2023a
      </a>
      ); Chen et al. (
      <a class="ltx_ref" href="#bib.bib5" title="">
       2024b
      </a>
      )
     </cite>
     is that we take the visual signal into account, which is important in scientific data visualization.
Some errors or weaknesses may be difficult to identify in the code but become apparent when observing the output figure through “eyes”. The visual agent is the “eyes” for MatPlotAgent, while the aforementioned code agent acts as the “hands” for MatPlotAgent.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS3.p2">
    <p class="ltx_p" id="S4.SS3.p2.1">
     Specifically, the visual agent is powered by multi-modal LLMs. In our experiments, we utilize GPT-4V
     <cite class="ltx_cite ltx_citemacro_cite">
      OpenAI (
      <a class="ltx_ref" href="#bib.bib19" title="">
       2023
      </a>
      )
     </cite>
     to drive this agent. We introduce several guiding principles for the visual agent, including verifying whether the figure aligns with the provided data, and enhancing the colors or labels to improve the figure’s informativeness. Based on the principles, the user query, and the current draft of the figure, the visual agent generates some suggestions to refine to figure. These suggestions serve as feedback for the code agent to refine the code.
Experimental results in Section
     <a class="ltx_ref" href="#S5.SS3" title="5.3 Ablation Study ‣ 5 Experiments ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization">
      <span class="ltx_text ltx_ref_tag">
       5.3
      </span>
     </a>
     show that our visual feedback mechanism can significantly improve the quality of the plotted figures, demonstrating the effectiveness of the proposed visual feedback mechanism.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Experiments
  </h2>
  <section class="ltx_subsection" id="S5.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.1
    </span>
    Setup
   </h3>
   <section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
    <h4 class="ltx_title ltx_title_paragraph">
     Models
    </h4>
    <div class="ltx_para" id="S5.SS1.SSS0.Px1.p1">
     <p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.1">
      Since the proposed MatPlotAgent is model-agnostic, we can employ various LLMs in this framework. The code LLMs we use in our experiments include GPT-4, GPT-3.5, Magicoder-S-DS-6.7B
      <cite class="ltx_cite ltx_citemacro_cite">
       Wei et al. (
       <a class="ltx_ref" href="#bib.bib34" title="">
        2023
       </a>
       )
      </cite>
      , Deepseek-coder-6.7B-instruct
      <cite class="ltx_cite ltx_citemacro_cite">
       Guo et al. (
       <a class="ltx_ref" href="#bib.bib7" title="">
        2024
       </a>
       )
      </cite>
      , Deepseek-coder-33B-instruct
      <cite class="ltx_cite ltx_citemacro_cite">
       Guo et al. (
       <a class="ltx_ref" href="#bib.bib7" title="">
        2024
       </a>
       )
      </cite>
      , WizardCoder-Python-33B-V1.1
      <cite class="ltx_cite ltx_citemacro_cite">
       Luo et al. (
       <a class="ltx_ref" href="#bib.bib17" title="">
        2023b
       </a>
       )
      </cite>
      , and CodeLlama-34B-Instruct
      <cite class="ltx_cite ltx_citemacro_cite">
       Rozière et al. (
       <a class="ltx_ref" href="#bib.bib27" title="">
        2024
       </a>
       )
      </cite>
      . The decoding temperature is set to 0.0 for all the involved code LLMs. For GPT-4 and GPT-3.5, we use the API provided by OpenAI
      <span class="ltx_note ltx_role_footnote" id="footnote6">
       <sup class="ltx_note_mark">
        6
       </sup>
       <span class="ltx_note_outer">
        <span class="ltx_note_content">
         <sup class="ltx_note_mark">
          6
         </sup>
         <span class="ltx_tag ltx_tag_note">
          6
         </span>
         <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/product" target="_blank" title="">
          https://openai.com/product
         </a>
        </span>
       </span>
      </span>
      . For the other five open-source LLMs, we use
      <span class="ltx_text ltx_font_typewriter" id="S5.SS1.SSS0.Px1.p1.1.1">
       vLLM
      </span>
      <cite class="ltx_cite ltx_citemacro_cite">
       Kwon et al. (
       <a class="ltx_ref" href="#bib.bib10" title="">
        2023
       </a>
       )
      </cite>
      for model inference.
For the visual agent, we utilize GPT-4V
      <cite class="ltx_cite ltx_citemacro_cite">
       OpenAI (
       <a class="ltx_ref" href="#bib.bib19" title="">
        2023
       </a>
       )
      </cite>
      , a state-of-the-art multi-modal LLM. We leave the exploration of using open-source multi-modal LLMs to power the visual agent for future work.
     </p>
    </div>
   </section>
   <section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
    <h4 class="ltx_title ltx_title_paragraph">
     Evaluation
    </h4>
    <div class="ltx_para" id="S5.SS1.SSS0.Px2.p1">
     <p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.1">
      We evaluate the involved methods on MatPlotBench, using the proposed automatic scoring mechanism that is shown reliable in Section
      <a class="ltx_ref" href="#S3.SS2" title="3.2 Automatic Quantitative Evaluation ‣ 3 MatPlotBench ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization">
       <span class="ltx_text ltx_ref_tag">
        3.2
       </span>
      </a>
      .
For each code LLM, we evaluate its performance in three ways:
     </p>
     <ul class="ltx_itemize" id="S5.I1">
      <li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        •
       </span>
       <div class="ltx_para" id="S5.I1.i1.p1">
        <p class="ltx_p" id="S5.I1.i1.p1.1">
         Direct decoding: given the query, the model directly generates the plotting code.
        </p>
       </div>
      </li>
      <li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        •
       </span>
       <div class="ltx_para" id="S5.I1.i2.p1">
        <p class="ltx_p" id="S5.I1.i2.p1.1">
         Zero-Shot Chain-of-thought
         <cite class="ltx_cite ltx_citemacro_cite">
          Kojima et al. (
          <a class="ltx_ref" href="#bib.bib9" title="">
           2022b
          </a>
          )
         </cite>
         : the model is prompted to inference with the zero-shot CoT mechanism.
        </p>
       </div>
      </li>
      <li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
       <span class="ltx_tag ltx_tag_item">
        •
       </span>
       <div class="ltx_para" id="S5.I1.i3.p1">
        <p class="ltx_p" id="S5.I1.i3.p1.1">
         MatPlotAgent: the model is equipped with the proposed MatPlotAgent framework, driving the query expansion module and the code agent, as illustrated in Section
         <a class="ltx_ref" href="#S4" title="4 MatPlotAgent ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization">
          <span class="ltx_text ltx_ref_tag">
           4
          </span>
         </a>
         .
        </p>
       </div>
      </li>
     </ul>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S5.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.2
    </span>
    Main Results
   </h3>
   <div class="ltx_para" id="S5.SS2.p1">
    <p class="ltx_p" id="S5.SS2.p1.1">
     Table
     <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.2 Code Agent ‣ 4 MatPlotAgent ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     presents the results of different methods on the scientific data visualization task. In the direct decoding setting, GPT-4 achieves the highest score of 48.86. Surprisingly, the open-source model Magicoder-S-DS-6.7B
     <cite class="ltx_cite ltx_citemacro_cite">
      Wei et al. (
      <a class="ltx_ref" href="#bib.bib34" title="">
       2023
      </a>
      )
     </cite>
     achieves the second-best performance, surpassing models with substantially larger parameter sizes, such as WizardCoder-Python-33B-V1.1.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS2.p2">
    <p class="ltx_p" id="S5.SS2.p2.1">
     The results also suggest that the zero-shot CoT mechanism does not effectively enhance the performance of many recent code LLMs. Zero-shot CoT only improves the results of Deepseek-coder-33B-instruct
     <cite class="ltx_cite ltx_citemacro_cite">
      Guo et al. (
      <a class="ltx_ref" href="#bib.bib7" title="">
       2024
      </a>
      )
     </cite>
     from 30.88 to 36.10. Conversely, for other models, implementing zero-shot CoT results in poorer performance. For example, when zero-shot CoT is applied, the performance of GPT-4 drops to 45.42, which is lower than the direct decoding result of 48.86.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS2.p3">
    <p class="ltx_p" id="S5.SS2.p3.1">
     From Table
     <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.2 Code Agent ‣ 4 MatPlotAgent ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     , we find the proposed MatPlotAgent can improve the plotting capabilities of several models. For GPT-4 and GPT-3.5, MatPlotAgent leads to significant improvements of 12.30 and 9.48, respectively. For the other five open-source LLMs, MatPlotAgent improves the performance of four models. With MatPlotAgent, the open-source Magicoder-S-DS-6.7B model even surpasses GPT-4 with direct decoding (51.70 vs. 48.86), showcasing the effectiveness of our method. In the following experiments, we will perform ablation studies to investigate the effects of different components in MatPlotAgent, and provide some plotting examples to better interpret the quantitative results.
    </p>
   </div>
   <figure class="ltx_figure" id="S5.F4">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="287" id="S5.F4.g1" src="/html/2402.11453/assets/x4.png" width="457"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 4:
     </span>
     Examples to illustrate the effect of visual feedback. To investigate the effect of the visual feedback mechanism on different models, we display the outputs of two representative LLMs. Case A, B, and C are generated by GPT-4. Case D is generated by Magicoder-S-DS-6.7B.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S5.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.3
    </span>
    Ablation Study
   </h3>
   <div class="ltx_para" id="S5.SS3.p1">
    <p class="ltx_p" id="S5.SS3.p1.1">
     Compared to previous LLM-based coding agents
     <cite class="ltx_cite ltx_citemacro_cite">
      Qian et al. (
      <a class="ltx_ref" href="#bib.bib21" title="">
       2023a
      </a>
      ); Chen et al. (
      <a class="ltx_ref" href="#bib.bib5" title="">
       2024b
      </a>
      )
     </cite>
     , the major contribution of the work lies in the newly proposed visual feedback mechanism, expected to leverage visual signals to enhance the quality of the output figure. To gain a deeper understanding of the impact of the visual feedback mechanism, we conduct both qualitative and quantitative analyses in this section.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS3.p2">
    <p class="ltx_p" id="S5.SS3.p2.1">
     Figure
     <a class="ltx_ref" href="#S5.F4" title="Figure 4 ‣ 5.2 Main Results ‣ 5 Experiments ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     presents examples plotted by LLMs both with and without the visual feedback mechanism. We observe a clear improvement in the quality of the output figure with the visual feedback.
For example, in case C, the text in the figure is jumbled, but this issue is resolved with the assistance of visual feedback.
It is important to note that the visual agent does not reference the ground-truth figure when generating feedback; it only examines the draft plotted by the model.
Table
     <a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ 5.3 Ablation Study ‣ 5 Experiments ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     also presents quantitative results of the visual feedback mechanism, indicating that the absence of visual feedback would result in significantly poorer outcomes for both GPT-4 and GPT-3.5. This reaffirms the importance of visual signals in the task of scientific data visualization.
    </p>
   </div>
   <figure class="ltx_table" id="S5.T2">
    <table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T2.1">
     <tr class="ltx_tr" id="S5.T2.1.1">
      <td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T2.1.1.1">
       <span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1">
        Model
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.1.1.2">
       <span class="ltx_text ltx_font_bold" id="S5.T2.1.1.2.1">
        GPT-4
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.1.1.3">
       <span class="ltx_text ltx_font_bold" id="S5.T2.1.1.3.1">
        GPT-3.5
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T2.1.2">
      <td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.2.1">
       Direct Decod.
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.2">
       48.86
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.3">
       38.03
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T2.1.3">
      <td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.3.1">
       MatPlotAgent
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.2">
       61.16
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.3">
       47.51
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T2.1.4">
      <td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.1.4.1">
       w/o Visual Feedback
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.4.2">
       53.44
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.4.3">
       41.57
      </td>
     </tr>
    </table>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 2:
     </span>
     Effect of the visual feedback mechanism.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S5.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.4
    </span>
    Case Study
   </h3>
   <figure class="ltx_figure" id="S5.F5">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="293" id="S5.F5.g1" src="/html/2402.11453/assets/x5.png" width="456"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 5:
     </span>
     Case study of different models.
    </figcaption>
   </figure>
   <div class="ltx_para" id="S5.SS4.p1">
    <p class="ltx_p" id="S5.SS4.p1.1">
     We present output figures in Figure
     <a class="ltx_ref" href="#S5.F5" title="Figure 5 ‣ 5.4 Case Study ‣ 5 Experiments ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     . The first example is relatively simple, correctly plotted by GPT-4 augmented with MatPlotAgent. The second example is more challenging; while GPT-4 and Magicoder-S-DS-6.7B can generate a draft, both omit some elements. The third example is the most difficult, where none of the three models can produce the correct result.
These results indicate that the proposed MatPlotBench poses a significant challenge for current LLMs. Even the state-of-the-art LLM, GPT-4, equipped with MatPlotAgent, fails in some cases. We believe this benchmark will be effective not only for evaluating AI systems in scientific data visualization but also for assessing general capabilities such as coding and visual perception.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Related Work
  </h2>
  <section class="ltx_paragraph" id="S6.SS0.SSS0.Px1">
   <h4 class="ltx_title ltx_title_paragraph">
    Code LLMs
   </h4>
   <div class="ltx_para" id="S6.SS0.SSS0.Px1.p1">
    <p class="ltx_p" id="S6.SS0.SSS0.Px1.p1.1">
     Since the release of Codex
     <cite class="ltx_cite ltx_citemacro_cite">
      Chen et al. (
      <a class="ltx_ref" href="#bib.bib3" title="">
       2021
      </a>
      )
     </cite>
     , many closed- and open-source code LLMs have been published, pushing the boundaries of LLMs’ capabilities to write functional code. Early open-source efforts include SantaCoder
     <cite class="ltx_cite ltx_citemacro_cite">
      Allal et al. (
      <a class="ltx_ref" href="#bib.bib1" title="">
       2023
      </a>
      )
     </cite>
     and StarCoder
     <cite class="ltx_cite ltx_citemacro_cite">
      Li et al. (
      <a class="ltx_ref" href="#bib.bib13" title="">
       2023b
      </a>
      )
     </cite>
     .
More recently, the Code Llama
     <cite class="ltx_cite ltx_citemacro_cite">
      Rozière et al. (
      <a class="ltx_ref" href="#bib.bib27" title="">
       2024
      </a>
      )
     </cite>
     series is released, including models of varying sizes.
DeepSeekCoder
     <cite class="ltx_cite ltx_citemacro_cite">
      Guo et al. (
      <a class="ltx_ref" href="#bib.bib7" title="">
       2024
      </a>
      )
     </cite>
     , a series of open-source code models ranging in size from 1.3B to 33B, has also garnered significant attention for its impressive performance on general coding benchmarks.
     <cite class="ltx_cite ltx_citemacro_citet">
      Wei et al. (
      <a class="ltx_ref" href="#bib.bib34" title="">
       2023
      </a>
      )
     </cite>
     introduce a novel data augmentation method for automatically creating high-quality fine-tuning data. The resulting Magicoder model surpasses a wide array of open-source code LLMs in performance.
    </p>
   </div>
  </section>
  <section class="ltx_paragraph" id="S6.SS0.SSS0.Px2">
   <h4 class="ltx_title ltx_title_paragraph">
    LLM Agents
   </h4>
   <div class="ltx_para" id="S6.SS0.SSS0.Px2.p1">
    <p class="ltx_p" id="S6.SS0.SSS0.Px2.p1.1">
     Recently, a wide range of LLM-based agent frameworks is proposed to explore LLMs’ potential in real-world scenarios
     <cite class="ltx_cite ltx_citemacro_cite">
      Nakano et al. (
      <a class="ltx_ref" href="#bib.bib18" title="">
       2021
      </a>
      ); Yao et al. (
      <a class="ltx_ref" href="#bib.bib37" title="">
       2022
      </a>
      ); Qin et al. (
      <a class="ltx_ref" href="#bib.bib23" title="">
       2023
      </a>
      ); Zhou et al. (
      <a class="ltx_ref" href="#bib.bib41" title="">
       2023
      </a>
      )
     </cite>
     .
OpenAgents
     <cite class="ltx_cite ltx_citemacro_cite">
      Xie et al. (
      <a class="ltx_ref" href="#bib.bib35" title="">
       2023
      </a>
      )
     </cite>
     proposed an open platform for using language agents in everyday life, which includes a Data Agent, a Plugins Agent, and a Web Agent.
     <cite class="ltx_cite ltx_citemacro_citet">
      Park et al. (
      <a class="ltx_ref" href="#bib.bib20" title="">
       2023
      </a>
      )
     </cite>
     proposed an interactive simulacra of human behavior where computational software agents simulate believable human actions and interactions. Voyager
     <cite class="ltx_cite ltx_citemacro_cite">
      Wang et al. (
      <a class="ltx_ref" href="#bib.bib32" title="">
       2023
      </a>
      )
     </cite>
     introduced the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention.
ChatDev
     <cite class="ltx_cite ltx_citemacro_cite">
      Qian et al. (
      <a class="ltx_ref" href="#bib.bib21" title="">
       2023a
      </a>
      )
     </cite>
     proposed a virtual chat-powered software development company that mirrors the established waterfall model.
In this study, we explore the capabilities of LLM-based agents in the task of scientific data visualization, a critical and practical area for contemporary researchers.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S7">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    7
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S7.p1">
   <p class="ltx_p" id="S7.p1.1">
    We propose to assess and enhance the capabilities of modern LLMs for scientific data visualization, a multifaceted task demanding coding and visual skills. We begin with the creation of MatPlotBench, a rigorous benchmark supporting automated quantitative evaluation that strongly aligns with human assessment. Additionally, we introduce MatPlotAgent, a model-agnostic mechanism employing visual feedback to enhance LLMs’ plotting abilities. Experimental results demonstrate that MatPlotAgent enhances the performance of various LLMs.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S8">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    8
   </span>
   Limitations
  </h2>
  <div class="ltx_para" id="S8.p1">
   <p class="ltx_p" id="S8.p1.1">
    In this paper, we introduce MatPlotBench, a benchmark designed for scientific data visualization. However, the demands of scientific data visualization can vary significantly across disciplines. Since MatPlotBench is developed for general scientific data visualization, it may not encompass all domain-specific requirements, potentially restricting its applicability to certain fields. In the future, the data construction and evaluation approaches can be customized for specific domains if necessary.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Allal et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, and Leandro von Werra. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2301.03988" target="_blank" title="">
      Santacoder: don’t reach for the stars!
     </a>
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Azerbayev et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=4WnqRR915j" target="_blank" title="">
      Llemma: An open language model for mathematics
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">
      The Twelfth International Conference on Learning Representations
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2107.03374" target="_blank" title="">
      Evaluating large language models trained on code
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2024a)
    </span>
    <span class="ltx_bibblock">
     Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. 2024a.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=EHg5GDnyq1" target="_blank" title="">
      Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      The Twelfth International Conference on Learning Representations
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2024b)
    </span>
    <span class="ltx_bibblock">
     Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2024b.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=KuPixIqPiq" target="_blank" title="">
      Teaching large language models to self-debug
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">
      The Twelfth International Conference on Learning Representations
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Deng et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=kiYqbO3wqw" target="_blank" title="">
      Mind2web: Towards a generalist agent for the web
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">
      Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Guo et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2401.14196" target="_blank" title="">
      Deepseek-coder: When the large language model meets programming – the rise of code intelligence
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kojima et al. (2022a)
    </span>
    <span class="ltx_bibblock">
     Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022a.
    </span>
    <span class="ltx_bibblock">
     Large language models are zero-shot reasoners.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , volume 35, pages 22199–22213.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kojima et al. (2022b)
    </span>
    <span class="ltx_bibblock">
     Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022b.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf" target="_blank" title="">
      Large language models are zero-shot reasoners
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">
      Advances in Neural Information Processing Systems
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kwon et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023.
    </span>
    <span class="ltx_bibblock">
     Efficient memory management for large language model serving with pagedattention.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lai et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-Tau Yih, Daniel Fried, Sida Wang, and Tao Yu. 2023.
    </span>
    <span class="ltx_bibblock">
     DS-1000: A natural and reliable benchmark for data science code generation.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">
      Proceedings of the 40th International Conference on Machine Learning
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023a.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=3IyL2XWDkG" target="_blank" title="">
      CAMEL: Communicative agents for ”mind” exploration of large language model society
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">
      Thirty-seventh Conference on Neural Information Processing Systems
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Raymond Li, Loubna Ben allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia LI, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Joel Lamy-Poirier, Joao Monteiro, Nicolas Gontier, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Ben Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason T Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Urvashi Bhattacharyya, Wenhao Yu, Sasha Luccioni, Paulo Villegas, Fedor Zhdanov, Tony Lee, Nadav Timor, Jennifer Ding, Claire S Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro Von Werra, and Harm de Vries. 2023b.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=KoFOg41haE" target="_blank" title="">
      Starcoder: may the source be with you!
     </a>
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">
      Transactions on Machine Learning Research
     </em>
     .
    </span>
    <span class="ltx_bibblock">
     Reproducibility Certification.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=zAdUB0aCTQ" target="_blank" title="">
      Agentbench: Evaluating LLMs as agents
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">
      The Twelfth International Conference on Learning Representations
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Lu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=HtqnVSCj3q" target="_blank" title="">
      Chameleon: Plug-and-play compositional reasoning with large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">
      Thirty-seventh Conference on Neural Information Processing Systems
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Luo et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023a.
    </span>
    <span class="ltx_bibblock">
     Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">
      arXiv preprint arXiv:2308.09583
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Luo et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023b.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2306.08568" target="_blank" title="">
      Wizardcoder: Empowering code large language models with evol-instruct
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nakano et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Ouyang Long, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:245329531" target="_blank" title="">
      Webgpt: Browser-assisted question-answering with human feedback
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">
      ArXiv
     </em>
     , abs/2112.09332.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     OpenAI (2023)
    </span>
    <span class="ltx_bibblock">
     OpenAI. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:257532815" target="_blank" title="">
      Gpt-4 technical report
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Park et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3586183.3606763" target="_blank" title="">
      Generative agents: Interactive simulacra of human behavior
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology
     </em>
     , UIST ’23, New York, NY, USA. Association for Computing Machinery.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qian et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023a.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2307.07924" target="_blank" title="">
      Communicative agents for software development
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qian et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Cheng Qian, Chi Han, Yi Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. 2023b.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-emnlp.462" target="_blank" title="">
      CREATOR: Tool creation for disentangling abstract and concrete reasoning of large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">
      Findings of the Association for Computational Linguistics: EMNLP 2023
     </em>
     , pages 6922–6939, Singapore. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qin et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, and Jie Zhou. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.499" target="_blank" title="">
      WebCPM: Interactive web search for Chinese long-form question answering
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">
      Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)
     </em>
     , pages 8968–8988, Toronto, Canada. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qin et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, dahai li, Zhiyuan Liu, and Maosong Sun. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=dHng2O0Jjr" target="_blank" title="">
      ToolLLM: Facilitating large language models to master 16000+ real-world APIs
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">
      The Twelfth International Conference on Learning Representations
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Ramesh et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2102.12092" target="_blank" title="">
      Zero-shot text-to-image generation
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rombach et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2112.10752" target="_blank" title="">
      High-resolution image synthesis with latent diffusion models
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rozière et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2308.12950" target="_blank" title="">
      Code llama: Open foundation models for code
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Saharia et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. 2022.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2205.11487" target="_blank" title="">
      Photorealistic text-to-image diffusion models with deep language understanding
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schick et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=Yacmpz84TH" target="_blank" title="">
      Toolformer: Language models can teach themselves to use tools
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">
      Thirty-seventh Conference on Neural Information Processing Systems
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shao et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2402.03300" target="_blank" title="">
      Deepseekmath: Pushing the limits of mathematical reasoning in open language models
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shinn et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=vAElhFcKW6" target="_blank" title="">
      Reflexion: language agents with verbal reinforcement learning
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">
      Thirty-seventh Conference on Neural Information Processing Systems
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi (Jim) Fan, and Anima Anandkumar. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:258887849" target="_blank" title="">
      Voyager: An open-ended embodied agent with large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">
      ArXiv
     </em>
     , abs/2305.16291.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf" target="_blank" title="">
      Chain-of-thought prompting elicits reasoning in large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , volume 35, pages 24824–24837. Curran Associates, Inc.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2312.02120" target="_blank" title="">
      Magicoder: Source code is all you need
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xie et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan Shin, Caiming Xiong, and Tao Yu. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.10634" target="_blank" title="">
      Openagents: An open platform for language agents in the wild
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2309.04658" target="_blank" title="">
      Exploring large language models for communication games: An empirical study on werewolf
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/82ad13ec01f9fe44c01cb91814fd7b8c-Paper-Conference.pdf" target="_blank" title="">
      WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , volume 35, pages 20744–20757. Curran Associates, Inc.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik R Narasimhan. 2023a.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=5Xc1ecxO1h" target="_blank" title="">
      Tree of thoughts: Deliberate problem solving with large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">
      Thirty-seventh Conference on Neural Information Processing Systems
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023b.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=WE_vluYUL-X" target="_blank" title="">
      React: Synergizing reasoning and acting in language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">
      The Eleventh International Conference on Learning Representations
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yu et al. (2024)
    </span>
    <span class="ltx_bibblock">
     Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2024.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=N8N0hgNDRt" target="_blank" title="">
      Metamath: Bootstrap your own mathematical questions for large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">
      The Twelfth International Conference on Learning Representations
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhou et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=rmiwIL98uQ" target="_blank" title="">
      Webarena: A realistic web environment for building autonomous agents
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">
      Second Agent Learning in Open-Endedness Workshop
     </em>
     .
    </span>
   </li>
  </ul>
 </section>
 <section class="ltx_appendix" id="A1">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix A
   </span>
   Detailed Prompts
  </h2>
  <div class="ltx_para" id="A1.p1">
   <p class="ltx_p" id="A1.p1.1">
    To better understand MatPlotBench and MatPlotAgent, we list the prompts for automatic evaluation and the three modules in MatPlotAgent, including the query expansion module, the code agent, and the visual agent.
   </p>
  </div>
  <section class="ltx_subsection" id="A1.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     A.1
    </span>
    Evaluation Prompts
   </h3>
   <div class="ltx_para" id="A1.SS1.p1">
    <p class="ltx_p" id="A1.SS1.p1.1">
     The automatic evaluation prompt primarily requires GPT-4V to provide a score between 0 and 100 for the model-generated plot, with reference to the ground truth plot.
    </p>
   </div>
   <figure class="ltx_figure" id="A1.F6">
    <svg class="ltx_picture" height="390.09" id="A1.F6.pic1" overflow="visible" version="1.1" width="600">
     <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,390.09) matrix(1 0 0 -1 0 0)">
      <g fill="#000080" fill-opacity="1.0">
       <path d="M 0 5.91 L 0 384.18 C 0 387.44 2.64 390.09 5.91 390.09 L 594.09 390.09 C 597.36 390.09 600 387.44 600 384.18 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none">
       </path>
      </g>
      <g fill="#FAFAFF" fill-opacity="1.0">
       <path d="M 1.97 5.91 L 1.97 384.18 C 1.97 386.36 3.73 388.12 5.91 388.12 L 594.09 388.12 C 596.27 388.12 598.03 386.36 598.03 384.18 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none">
       </path>
      </g>
      <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)">
       <foreignobject color="#000000" height="362.53" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
        <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.F6.pic1.1.1.1.1.1" style="width:402.3pt;">
         <span class="ltx_p" id="A1.F6.pic1.1.1.1.1.1.1">
          <span class="ltx_text ltx_font_typewriter" id="A1.F6.pic1.1.1.1.1.1.1.1">
           You are an excellent judge at evaluating visualization plots between a model-generated plot and the ground truth. You will be giving scores on how well it matches the ground truth plot.
           <br class="ltx_break"/>
           The generated plot will be given to you as the first figure. If the first figure is blank, that means the code failed to generate a figure.
           <br class="ltx_break"/>
           Another plot will be given to you as the second figure, which is the desired outcome of the user query, meaning it is the ground truth for you to reference.
           <br class="ltx_break"/>
           Please compare the two figures head to head and rate them.
Suppose the second figure has a score of 100, rate the first figure on a scale from 0 to 100.
           <br class="ltx_break"/>
           Scoring should be carried out regarding the plot correctness:
Compare closely between the generated plot and the ground truth, the more resemblance the generated plot has compared to the ground truth, the higher the score. The score should be proportionate to the resemblance between the two plots.
           <br class="ltx_break"/>
           In some rare occurrences, see if the data points are generated randomly according to the query, if so, the generated plot may not perfectly match the ground truth, but it is correct nonetheless.
           <br class="ltx_break"/>
           Only rate the first figure, the second figure is only for reference.
           <br class="ltx_break"/>
           If the first figure is blank, that means the code failed to generate a figure. Give a score of 0 on the Plot correctness.
           <br class="ltx_break"/>
           After scoring from the above aspect, please give a final score. The final score is preceded by the [FINAL SCORE] token.
           <br class="ltx_break"/>
           For example [FINAL SCORE]: 40.
          </span>
         </span>
        </span>
       </foreignobject>
      </g>
     </g>
    </svg>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      Figure 6:
     </span>
     Automatic evaluation prompt for GPT-4V.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="A1.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     A.2
    </span>
    Prompts for MatPlotAgent
   </h3>
   <div class="ltx_para" id="A1.SS2.p1">
    <p class="ltx_p" id="A1.SS2.p1.1">
     The query expansion prompt mainly requires LLMs to generate step-by-step, detailed instructions on how to use Python code to fulfill the requirements specified by users, as shown in Figure
     <a class="ltx_ref" href="#A1.F7" title="Figure 7 ‣ A.2 Prompts for MatPlotAgent ‣ Appendix A Detailed Prompts ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization">
      <span class="ltx_text ltx_ref_tag">
       7
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para" id="A1.SS2.p2">
    <p class="ltx_p" id="A1.SS2.p2.1">
     For the code agent, there are two prompts for the code generation process and the self-debugging mechanism. The code generation prompt mainly requires LLMs to generate executable code according to the user query to plot and save the output figure, as shown in Figure
     <a class="ltx_ref" href="#A1.F8" title="Figure 8 ‣ A.2 Prompts for MatPlotAgent ‣ Appendix A Detailed Prompts ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization">
      <span class="ltx_text ltx_ref_tag">
       8
      </span>
     </a>
     . The self-debugging prompt mainly requires LLMs to correct the buggy code according to the error message from a Python interpreter, as displayed in Figure
     <a class="ltx_ref" href="#A1.F9" title="Figure 9 ‣ A.2 Prompts for MatPlotAgent ‣ Appendix A Detailed Prompts ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization">
      <span class="ltx_text ltx_ref_tag">
       9
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para" id="A1.SS2.p3">
    <p class="ltx_p" id="A1.SS2.p3.1">
     The visual agent prompt mainly requires multi-modal LLMs to firstly understand the user query and analyze the draft plot, then generate the visual feedback to refine the draft, as shown in Figure
     <a class="ltx_ref" href="#A1.F10" title="Figure 10 ‣ A.2 Prompts for MatPlotAgent ‣ Appendix A Detailed Prompts ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization">
      <span class="ltx_text ltx_ref_tag">
       10
      </span>
     </a>
     .
    </p>
   </div>
   <figure class="ltx_figure" id="A1.F7">
    <svg class="ltx_picture" height="205.9" id="A1.F7.pic1" overflow="visible" version="1.1" width="600">
     <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,205.9) matrix(1 0 0 -1 0 0)">
      <g fill="#000080" fill-opacity="1.0">
       <path d="M 0 5.91 L 0 200 C 0 203.26 2.64 205.9 5.91 205.9 L 594.09 205.9 C 597.36 205.9 600 203.26 600 200 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none">
       </path>
      </g>
      <g fill="#FAFAFF" fill-opacity="1.0">
       <path d="M 1.97 5.91 L 1.97 200 C 1.97 202.17 3.73 203.93 5.91 203.93 L 594.09 203.93 C 596.27 203.93 598.03 202.17 598.03 200 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none">
       </path>
      </g>
      <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)">
       <foreignobject color="#000000" height="178.34" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
        <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.F7.pic1.1.1.1.1.1" style="width:402.3pt;">
         <span class="ltx_p" id="A1.F7.pic1.1.1.1.1.1.1">
          <span class="ltx_text ltx_font_bold" id="A1.F7.pic1.1.1.1.1.1.1.1">
           SYSTEM PROMPT:
          </span>
          <span class="ltx_text ltx_font_typewriter" id="A1.F7.pic1.1.1.1.1.1.1.2">
           According to the user query, expand and solidify the query into a step by step detailed instruction (or comment) on how to write python code to fulfill the user query’s requirements. Import the appropriate libraries. Pinpoint the correct library functions to call and set each parameter in every function call accordingly.
          </span>
         </span>
         <span class="ltx_p" id="A1.F7.pic1.1.1.1.1.1.2">
          <span class="ltx_text ltx_font_bold" id="A1.F7.pic1.1.1.1.1.1.2.1">
           USER PROMPT:
          </span>
          <span class="ltx_text ltx_font_typewriter" id="A1.F7.pic1.1.1.1.1.1.2.2">
           Here is the user query: [User Query]:
"""
{{query}}
"""
You should understand what the query’s requirements are, and output step by step, detailed instructions on how to use python code to fulfill these requirements. Include what libraries to import, what library functions to call, how to set the parameters in each function correctly, how to prepare the data, how to manipulate the data so that it becomes appropriate for later functions to call etc,. Make sure the code to be executable and correctly generate the desired output in the user query.
          </span>
         </span>
        </span>
       </foreignobject>
      </g>
     </g>
    </svg>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      Figure 7:
     </span>
     The query expansion prompt in MatPlotAgent.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A1.F8">
    <svg class="ltx_picture" height="157.63" id="A1.F8.pic1" overflow="visible" version="1.1" width="600">
     <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,157.63) matrix(1 0 0 -1 0 0)">
      <g fill="#000080" fill-opacity="1.0">
       <path d="M 0 5.91 L 0 151.72 C 0 154.98 2.64 157.63 5.91 157.63 L 594.09 157.63 C 597.36 157.63 600 154.98 600 151.72 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none">
       </path>
      </g>
      <g fill="#FAFAFF" fill-opacity="1.0">
       <path d="M 1.97 5.91 L 1.97 151.72 C 1.97 153.9 3.73 155.66 5.91 155.66 L 594.09 155.66 C 596.27 155.66 598.03 153.9 598.03 151.72 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none">
       </path>
      </g>
      <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)">
       <foreignobject color="#000000" height="130.07" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
        <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.F8.pic1.1.1.1.1.1" style="width:402.3pt;">
         <span class="ltx_p" id="A1.F8.pic1.1.1.1.1.1.1">
          <span class="ltx_text ltx_font_bold" id="A1.F8.pic1.1.1.1.1.1.1.1">
           SYSTEM PROMPT:
          </span>
          <span class="ltx_text ltx_font_typewriter" id="A1.F8.pic1.1.1.1.1.1.1.2">
           You are a cutting-edge super capable code generation LLM. You will be given a natural language query, generate a runnable python code to satisfy all the requirements in the query. You can use any python library you want. When you complete a plot, remember to save it to a png file.
          </span>
         </span>
         <span class="ltx_p" id="A1.F8.pic1.1.1.1.1.1.2">
          <span class="ltx_text ltx_font_bold" id="A1.F8.pic1.1.1.1.1.1.2.1">
           USER PROMPT:
          </span>
          <span class="ltx_text ltx_font_typewriter" id="A1.F8.pic1.1.1.1.1.1.2.2">
           Here is the query:
"""
{{query}}
"""
If the query requires data manipulation from a csv file, process the data from the csv file and draw the plot in one piece of code. When you complete a plot, remember to save it to a png file. The file name should be """{{file_name}}""".
          </span>
         </span>
        </span>
       </foreignobject>
      </g>
     </g>
    </svg>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      Figure 8:
     </span>
     The code generation prompt in MatPlotAgent.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A1.F9">
    <svg class="ltx_picture" height="73.07" id="A1.F9.pic1" overflow="visible" version="1.1" width="600">
     <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,73.07) matrix(1 0 0 -1 0 0)">
      <g fill="#000080" fill-opacity="1.0">
       <path d="M 0 5.91 L 0 67.16 C 0 70.42 2.64 73.07 5.91 73.07 L 594.09 73.07 C 597.36 73.07 600 70.42 600 67.16 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none">
       </path>
      </g>
      <g fill="#FAFAFF" fill-opacity="1.0">
       <path d="M 1.97 5.91 L 1.97 67.16 C 1.97 69.34 3.73 71.1 5.91 71.1 L 594.09 71.1 C 596.27 71.1 598.03 69.34 598.03 67.16 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none">
       </path>
      </g>
      <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)">
       <foreignobject color="#000000" height="45.51" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
        <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.F9.pic1.1.1.1.1.1" style="width:402.3pt;">
         <span class="ltx_p" id="A1.F9.pic1.1.1.1.1.1.1">
          <span class="ltx_text ltx_font_bold" id="A1.F9.pic1.1.1.1.1.1.1.1">
           USER PROMPT:
          </span>
          <span class="ltx_text ltx_font_typewriter" id="A1.F9.pic1.1.1.1.1.1.1.2">
           There are some errors in the code you gave:
{{error_message}}
please correct the errors.
Then give the complete code and don’t omit anything even though you have given it in the above code.
          </span>
         </span>
        </span>
       </foreignobject>
      </g>
     </g>
    </svg>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      Figure 9:
     </span>
     The self-debugging prompt in MatPlotAgent.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A1.F10">
    <svg class="ltx_picture" height="273.86" id="A1.F10.pic1" overflow="visible" version="1.1" width="600">
     <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,273.86) matrix(1 0 0 -1 0 0)">
      <g fill="#000080" fill-opacity="1.0">
       <path d="M 0 5.91 L 0 267.95 C 0 271.21 2.64 273.86 5.91 273.86 L 594.09 273.86 C 597.36 273.86 600 271.21 600 267.95 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none">
       </path>
      </g>
      <g fill="#FAFAFF" fill-opacity="1.0">
       <path d="M 1.97 5.91 L 1.97 267.95 C 1.97 270.13 3.73 271.89 5.91 271.89 L 594.09 271.89 C 596.27 271.89 598.03 270.13 598.03 267.95 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none">
       </path>
      </g>
      <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)">
       <foreignobject color="#000000" height="246.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
        <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.F10.pic1.1.1.1.1.1" style="width:402.3pt;">
         <span class="ltx_p" id="A1.F10.pic1.1.1.1.1.1.1">
          <span class="ltx_text ltx_font_bold" id="A1.F10.pic1.1.1.1.1.1.1.1">
           SYSTEM PROMPT:
          </span>
          <span class="ltx_text ltx_font_typewriter" id="A1.F10.pic1.1.1.1.1.1.1.2">
           Given a user query and an image of the current plot, please determine whether the plot has faithfully followed the user query. Your task is to provide instruction to make sure the plot has strictly completed the requirements of the query. Please output a detailed step by step instruction on how to use python code to enhance the plot.
          </span>
         </span>
         <span class="ltx_p" id="A1.F10.pic1.1.1.1.1.1.2">
          <span class="ltx_text ltx_font_bold" id="A1.F10.pic1.1.1.1.1.1.2.1">
           USER PROMPT:
          </span>
          <span class="ltx_text ltx_font_typewriter" id="A1.F10.pic1.1.1.1.1.1.2.2">
           Here is the user query: [Query]:
"""
{{query}}
"""
Carefully read and analyze the user query to understand the specific requirements. Check if the plot aligns with the user query in terms of data selection, plot type, and any specific customization. Look at the provided image of the plot. Assess the plot type, the data it represents, labels, titles, colors, and any other visual elements. Compare these elements with the requirements specified in the user query. Note any differences between the user query requirements and the current plot. Based on the identified discrepancies, provide step-by-step instructions on how to modify the Python code to meet the user query requirements. Suggest improvements for better visualization practices, such as clarity, readability, and aesthetics, while ensuring the primary focus is on meeting the user’s specified requirements.
Remember to save the plot to a png file. The file name should be """{{file_name}}"""
          </span>
         </span>
        </span>
       </foreignobject>
      </g>
     </g>
    </svg>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      Figure 10:
     </span>
     Prompt for the visual agent.
    </figcaption>
   </figure>
  </section>
 </section>
 <section class="ltx_appendix" id="A2">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix B
   </span>
   Human Evaluation Details
  </h2>
  <div class="ltx_para" id="A2.p1">
   <p class="ltx_p" id="A2.p1.1">
    We engage human annotators from computer science departments at various universities via social media. They are compensated for their work at a rate slightly higher than the prevailing market rate. All human annotators involved are informed that the collected data will be used solely for academic research purposes, and their personal information will not be disclosed.
   </p>
  </div>
  <section class="ltx_subsection" id="A2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.1
    </span>
    Evaluation Guide for Human Annotators
   </h3>
   <div class="ltx_para" id="A2.SS1.p1">
    <p class="ltx_p" id="A2.SS1.p1.1">
     Figure
     <a class="ltx_ref" href="#A2.F11" title="Figure 11 ‣ B.1 Evaluation Guide for Human Annotators ‣ Appendix B Human Evaluation Details ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization">
      <span class="ltx_text ltx_ref_tag">
       11
      </span>
     </a>
     gives detailed instructions for human annotators when scoring the model-generated plots.
    </p>
   </div>
   <figure class="ltx_figure" id="A2.F11">
    <svg class="ltx_picture" height="287.47" id="A2.F11.pic1" overflow="visible" version="1.1" width="600">
     <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,287.47) matrix(1 0 0 -1 0 0)">
      <g fill="#800000" fill-opacity="1.0">
       <path d="M 0 5.91 L 0 281.56 C 0 284.82 2.64 287.47 5.91 287.47 L 594.09 287.47 C 597.36 287.47 600 284.82 600 281.56 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none">
       </path>
      </g>
      <g fill="#FFFAFA" fill-opacity="1.0">
       <path d="M 1.97 5.91 L 1.97 266.05 L 598.03 266.05 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none">
       </path>
      </g>
      <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 271.95)">
       <foreignobject color="#FFFFFF" height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
        <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.F11.pic1.1.1.1.1.1" style="width:402.3pt;">
         <span class="ltx_p" id="A2.F11.pic1.1.1.1.1.1.1">
          Evaluation Guide
         </span>
        </span>
       </foreignobject>
      </g>
      <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)">
       <foreignobject color="#000000" height="240.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
        <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.F11.pic1.2.2.2.1.1" style="width:402.3pt;">
         <span class="ltx_p" id="A2.F11.pic1.2.2.2.1.1.1">
          <span class="ltx_text ltx_font_bold" id="A2.F11.pic1.2.2.2.1.1.1.1">
           Plot Correctness (0-100 points)
          </span>
         </span>
         <span class="ltx_itemize" id="A2.I1">
          <span class="ltx_item" id="A2.I1.i1" style="list-style-type:none;">
           <span class="ltx_tag ltx_tag_item">
            •
           </span>
           <span class="ltx_para" id="A2.I1.i1.p1">
            <span class="ltx_p" id="A2.I1.i1.p1.1">
             <span class="ltx_text ltx_font_bold" id="A2.I1.i1.p1.1.1">
              Exact Match (90-100 points)
             </span>
             : The generated plot is nearly identical to the ground truth, with only minor, negligible differences.
            </span>
           </span>
          </span>
          <span class="ltx_item" id="A2.I1.i2" style="list-style-type:none;">
           <span class="ltx_tag ltx_tag_item">
            •
           </span>
           <span class="ltx_para" id="A2.I1.i2.p1">
            <span class="ltx_p" id="A2.I1.i2.p1.1">
             <span class="ltx_text ltx_font_bold" id="A2.I1.i2.p1.1.1">
              High Resemblance (70-89 points)
             </span>
             : The generated plot closely resembles the ground truth with some small but noticeable differences in data representation or styling.
            </span>
           </span>
          </span>
          <span class="ltx_item" id="A2.I1.i3" style="list-style-type:none;">
           <span class="ltx_tag ltx_tag_item">
            •
           </span>
           <span class="ltx_para" id="A2.I1.i3.p1">
            <span class="ltx_p" id="A2.I1.i3.p1.1">
             <span class="ltx_text ltx_font_bold" id="A2.I1.i3.p1.1.1">
              Moderate Resemblance (50-69 points)
             </span>
             : The generated plot has a moderate level of similarity to the ground truth, but there are several noticeable differences that impact the plot’s accuracy or interpretation.
            </span>
           </span>
          </span>
          <span class="ltx_item" id="A2.I1.i4" style="list-style-type:none;">
           <span class="ltx_tag ltx_tag_item">
            •
           </span>
           <span class="ltx_para" id="A2.I1.i4.p1">
            <span class="ltx_p" id="A2.I1.i4.p1.1">
             <span class="ltx_text ltx_font_bold" id="A2.I1.i4.p1.1.1">
              Low Resemblance (30-49 points)
             </span>
             : The generated plot shares some similarities with the ground truth but has significant differences that change the overall message or interpretation of the data.
            </span>
           </span>
          </span>
          <span class="ltx_item" id="A2.I1.i5" style="list-style-type:none;">
           <span class="ltx_tag ltx_tag_item">
            •
           </span>
           <span class="ltx_para" id="A2.I1.i5.p1">
            <span class="ltx_p" id="A2.I1.i5.p1.1">
             <span class="ltx_text ltx_font_bold" id="A2.I1.i5.p1.1.1">
              Poor Match (10-29 points)
             </span>
             : The generated plot has very little in common with the ground truth, with major discrepancies in data representation.
            </span>
           </span>
          </span>
          <span class="ltx_item" id="A2.I1.i6" style="list-style-type:none;">
           <span class="ltx_tag ltx_tag_item">
            •
           </span>
           <span class="ltx_para" id="A2.I1.i6.p1">
            <span class="ltx_p" id="A2.I1.i6.p1.1">
             <span class="ltx_text ltx_font_bold" id="A2.I1.i6.p1.1.1">
              No Resemblance (1-9 points)
             </span>
             : The generated plot is completely different from the ground truth, with no discernible similarities in data representation.
            </span>
           </span>
          </span>
          <span class="ltx_item" id="A2.I1.i7" style="list-style-type:none;">
           <span class="ltx_tag ltx_tag_item">
            •
           </span>
           <span class="ltx_para" id="A2.I1.i7.p1">
            <span class="ltx_p" id="A2.I1.i7.p1.1">
             <span class="ltx_text ltx_font_bold" id="A2.I1.i7.p1.1.1">
              Failure to Generate (0 points)
             </span>
             : The first figure is blank, indicating a failure to generate any plot.
            </span>
           </span>
          </span>
         </span>
         <span class="ltx_p" id="A2.F11.pic1.2.2.2.1.1.2">
          <span class="ltx_text ltx_font_bold" id="A2.F11.pic1.2.2.2.1.1.2.1">
           Special Considerations
          </span>
         </span>
         <span class="ltx_itemize" id="A2.I2">
          <span class="ltx_item" id="A2.I2.i1" style="list-style-type:none;">
           <span class="ltx_tag ltx_tag_item">
            •
           </span>
           <span class="ltx_para" id="A2.I2.i1.p1">
            <span class="ltx_p" id="A2.I2.i1.p1.1">
             In cases where the generated plot includes random data points that are correct in the context of the query, the plot should be evaluated for its correctness based on the query’s intent, not solely on its visual match to the ground truth.
            </span>
           </span>
          </span>
         </span>
         <span class="ltx_p" id="A2.F11.pic1.2.2.2.1.1.3">
          <span class="ltx_text ltx_font_bold" id="A2.F11.pic1.2.2.2.1.1.3.1">
           [FINAL SCORE]: XX
          </span>
         </span>
        </span>
       </foreignobject>
      </g>
     </g>
    </svg>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      Figure 11:
     </span>
     Evaluation guide for human annotators when scoring the model-generated plots.
    </figcaption>
   </figure>
   <div class="ltx_pagination ltx_role_newpage">
   </div>
  </section>
 </section>
</article>
