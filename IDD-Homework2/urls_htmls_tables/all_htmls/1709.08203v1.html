<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1709.08203] Survey of Recent Advances in Visual Question Answering</title><meta property="og:description" content="Visual Question Answering (VQA) presents a unique challenge as it requires the ability to understand and encode the multi-modal inputs - in terms of image processing and natural language processing. The algorithm furth‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Survey of Recent Advances in Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Survey of Recent Advances in Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1709.08203">

<!--Generated on Fri Mar 15 22:13:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Survey of Recent Advances in Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Supriya Pandhre
<br class="ltx_break">Indian Institute of Technology Hyderabad
<br class="ltx_break">Hyderabad, India
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">cs15mtech11016@iith.ac.in</span>
</span><span class="ltx_author_notes">Work completed during an internship at Adobe Systems</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shagun Sodhani
<br class="ltx_break">Adobe Systems 
<br class="ltx_break">Noida, India
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">sshagunsodhani@gmail.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">Visual Question Answering (VQA) presents a unique challenge as it requires the ability to understand and encode the multi-modal inputs - in terms of image processing and natural language processing. The algorithm further needs to learn how to perform reasoning over this multi-modal representation so it can answer the questions correctly. This paper presents a survey of different approaches proposed to solve the problem of Visual Question Answering. We also describe the current state of the art model in later part of paper. In particular, the paper describes the approaches taken by various algorithms to extract image features, text features and the way these are employed to predict answers. We also briefly discuss the experiments performed to evaluate the VQA models and report their performances on diverse datasets including newly released VQA2.0<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The task of Visual Question Answering (VQA) requires answering a natural language question using information from the accompanying image. To achieve excellence in VQA task, it requires more than just processing images and text individually. The model also needs to learn how to jointly reason over the two representations so that it can effectively answer the questions. For example, given a picture of cat, the question ‚ÄùIs the cat black in color?‚Äù can be answered using the visual modality once the model understands that it is to check the color of cat. The information about what to look for comes from the text modality. So in the general setting, the VQA model has to combine the information from both the modalities and reason over this combined representation.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Use of deep learning in the area of computer vision and natural language processing, when considered separately, has achieved outstanding results. However, for the VQA task, the algorithm needs to decide what is the relevant information, fetch that information from the image, and use that to answer the questions. In this paper, we present a survey of the recent advancements in the domain of VQA with focus on some of the papers presented in CVPR 2016. These papers have experiemented with both the visual and textual modality to improve the state of the art for VQA task.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We have described each algorithm in a separate section. The first part of each section briefly explains the algorithm from the perspective of methods used to obtain the image representation and the text representation and the methodology for combining the two knowledge representations to determine the correct answer. The second part focuses on the performance of the algorithm of the algorithm on different datasets. Table <a href="#S2.T1" title="Table 1 ‚Ä£ 2.1.2 Experiments ‚Ä£ 2.1 Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources [21] ‚Ä£ 2 Approaches ‚Ä£ Survey of Recent Advances in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes performance of all the algorithms on VQA dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, COCO-QA dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and DAQUAR dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and table <a href="#S2.T2" title="Table 2 ‚Ä£ 2.7.2 Experiments ‚Ä£ 2.7 Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering[10] ‚Ä£ 2 Approaches ‚Ä£ Survey of Recent Advances in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the accuracy on VQA2.0 dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Malinowski et al<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> proposed the first real-world image based dataset for VQA task called DAQUAR. It contains 1,449 images and a total of 12,468 questions with 2,483 unique questions. In COCO-QA dataset, the images are taken from MSCOCO dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and the questions are automatically generated from the captions of the images. However, in case of DAQUAR and VQA dataset, different people are asked to generate the questions and answers related to given images. COCO-QA dataset has 123,287 images with 78,736 training questions and 38,948 test questions. The questions are categorized into four types object, number, color and location-based on type of information it needs and each question has a one word answer. VQA ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> dataset is the largest dataset of real-world images with 82,783 training images, 40,504 validation images and 81,434 test images. There are 248,349 training questions in VQA dataset with over 20 different types of questions and each question has 10 crowd-sourced answers.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In VQA2.0<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> dataset, the images are same as VQA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, however, the number of questions are almost doubled to 443,757 training question. So now, the same question is posed with at least 2 different images, resulting in different answers for image-question pair. This forces the network to use the information from both the image and the text to answer the question and allows for correction of biases. For example, in VQA, if a question started with the n-gram ‚ÄùDo you see a . . .‚Äù, then answer was ‚Äùyes‚Äù for as many as 87% of the questions regardless of the complete question or the accompanying image.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Approaches</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</h3>

<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Algorithm details</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">In the usual setting, Visual Question Answering (VQA) aims to answer questions using only the information captured in the image without using external knowledge sources. For example, shown the image of an umbrella on a beach, the system may be able to answer ‚Äùwhat is the color of the umbrella?‚Äù but may not be able to answer ‚Äùwhy is the umbrella open‚Äù.</p>
</div>
<div id="S2.SS1.SSS1.p2" class="ltx_para">
<p id="S2.SS1.SSS1.p2.1" class="ltx_p">To answer this more general category of questions (which would need some kind of world-knowledge), Wu et al<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> proposed the use of external knowledge bases. The core idea is to get textual information in the form of captions and attributes from the image and encode them into vectors. These textual information vectors are combined with an external knowledge base. The paper uses LSTM based encoder-decoder architecture to generate answers from the combined information of captions, attributes and external knowledge.</p>
</div>
<div id="S2.SS1.SSS1.p3" class="ltx_para">
<p id="S2.SS1.SSS1.p3.1" class="ltx_p">The proposed method consists of three main parts:</p>
</div>
<div id="S2.SS1.SSS1.p4" class="ltx_para">
<p id="S2.SS1.SSS1.p4.1" class="ltx_p">Attribute based image representation: For this part, ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> dataset was used to get the attributes of image, nouns, verbs and adjectives from MSCOCO captions. VGG16¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> model, pretrained for Imagenet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> classification task, was fine tuned for multi-label attribute prediction problem. Finally the paper used max-pooling to get attribute based image representation.</p>
</div>
<div id="S2.SS1.SSS1.p5" class="ltx_para">
<p id="S2.SS1.SSS1.p5.1" class="ltx_p">Caption based image representation: For this part, the attributes predicted in the previous part were fed to a LSTM model to generate 5 captions. The LSTM was trained on MSCOCO captions. Average pooling was applied over hidden state vectors of LSTM, to get image representations in the form of captions.</p>
</div>
<div id="S2.SS1.SSS1.p6" class="ltx_para">
<p id="S2.SS1.SSS1.p6.1" class="ltx_p">Use of an external knowledge base(KB) to imbibe world knowledge into the system so that it can answer free-form questions that need more information than what is available in the question. The paper uses DBpedia¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, a structured database of information extracted from Wikipedia, as the external KB. The predicted attributes from the first part were queried on DBpedia to extract more information about the attributes. This extracted information is encoded into a knowledge vector using Doc2Vec<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> algorithm.</p>
</div>
<div id="S2.SS1.SSS1.p7" class="ltx_para">
<p id="S2.SS1.SSS1.p7.1" class="ltx_p">A weighted combination of the three inputs (predicted attributes of images, generated captions and external knowledge) is fed to an encoder LSTM and a decoder LSTM is used to generate an answer.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Experiments</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">The VQA model was evaluated on Toronto COCO-QA dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and VQA dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. VQA dataset is larger, more complex and challenging than the Toronto QA dataset and has more than 20 types of questions and multi-word answers.</p>
</div>
<div id="S2.SS1.SSS2.p2" class="ltx_para">
<p id="S2.SS1.SSS2.p2.1" class="ltx_p">Toronto COCO-QA dataset has 4 types of questions: object, number, color, location. The model gives highest accuracy on number questions with 75.33% and lowest on location based questions with 60.98% accuracy. When all three types of input: attributes, captions and knowledge, are used, overall performance of model on Toronto COCO-QA dataset is 69.73% which is only ¬†0.7% more than performance achieved by model that uses only attributes and captions, indicating that external knowledge is not that critical for Toronto COCO-QA. Similar pattern of results are obtained in the case of VQA dataset. Overall accuracy on VQA dataset is 55.96%, it is ¬†0.9% more than model that uses only attributes and captions, and not external knowledge. However, when the model uses external knowledge base, it performance ¬†4% better for ‚Äùwhy‚Äù type of questions in VQA dataset.</p>
</div>
<div id="S2.SS1.SSS2.p3" class="ltx_para">
<p id="S2.SS1.SSS2.p3.1" class="ltx_p">In summary, Wu et al¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> proposed method that uses external knowledge base to answer free form question on image. However the approach of converting image information into text information assumes that this conversion process will capture all intricacies of image.A feedback loop from question to all three types of inputs: CNN for predicting attributes, caption-LSTM to generate captions and external knowledge base could be next step towards improving the performance.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Algorithm</span></td>
<td id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">VQA Open-ended test-dev</span></td>
<td id="S2.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">VQA multiple-choice</span></td>
<td id="S2.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">COCO-QA</span></td>
<td id="S2.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">DAQUAR all-single answer</span></td>
<td id="S2.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.6.1" class="ltx_text ltx_font_bold" style="font-size:70%;">DAQUAR reduced-single answer</span></td>
</tr>
<tr id="S2.T1.1.2.2" class="ltx_tr">
<td id="S2.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.1.2.2.1.1" class="ltx_text" style="font-size:70%;">MCB</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.2.2.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S2.T1.1.2.2.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.2.2.2.1" class="ltx_text" style="font-size:70%;">66.7</span></td>
<td id="S2.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.2.2.3.1" class="ltx_text" style="font-size:70%;">70.2</span></td>
<td id="S2.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.2.2.4.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S2.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.2.2.5.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S2.T1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.2.2.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S2.T1.1.3.3" class="ltx_tr">
<td id="S2.T1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.1.3.3.1.1" class="ltx_text" style="font-size:70%;">Show,ask,attend,ans</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.3.3.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S2.T1.1.3.3.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.3.3.2.1" class="ltx_text" style="font-size:70%;">64.5</span></td>
<td id="S2.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.3.3.3.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S2.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.3.3.4.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S2.T1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.3.3.5.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S2.T1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.3.3.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S2.T1.1.4.4" class="ltx_tr">
<td id="S2.T1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.1.4.4.1.1" class="ltx_text" style="font-size:70%;">DAN</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.4.4.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S2.T1.1.4.4.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.4.4.2.1" class="ltx_text" style="font-size:70%;">64.3</span></td>
<td id="S2.T1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.4.4.3.1" class="ltx_text" style="font-size:70%;">69.1</span></td>
<td id="S2.T1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.4.4.4.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S2.T1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.4.4.5.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S2.T1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.4.4.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S2.T1.1.5.5" class="ltx_tr">
<td id="S2.T1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.1.5.5.1.1" class="ltx_text" style="font-size:70%;">ACK</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.5.5.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S2.T1.1.5.5.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.5.5.2.1" class="ltx_text" style="font-size:70%;">59.17</span></td>
<td id="S2.T1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.5.5.3.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S2.T1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.5.5.4.1" class="ltx_text" style="font-size:70%;">69.73</span></td>
<td id="S2.T1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.5.5.5.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S2.T1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.5.5.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S2.T1.1.6.6" class="ltx_tr">
<td id="S2.T1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.1.6.6.1.1" class="ltx_text" style="font-size:70%;">SAN</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.6.6.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S2.T1.1.6.6.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.6.6.2.1" class="ltx_text" style="font-size:70%;">58.7</span></td>
<td id="S2.T1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.6.6.3.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S2.T1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.6.6.4.1" class="ltx_text" style="font-size:70%;">61.6</span></td>
<td id="S2.T1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.6.6.5.1" class="ltx_text" style="font-size:70%;">29.3</span></td>
<td id="S2.T1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.6.6.6.1" class="ltx_text" style="font-size:70%;">45.5</span></td>
</tr>
<tr id="S2.T1.1.7.7" class="ltx_tr">
<td id="S2.T1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.1.7.7.1.1" class="ltx_text" style="font-size:70%;">NMN</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.7.7.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S2.T1.1.7.7.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.7.7.2.1" class="ltx_text" style="font-size:70%;">58.6</span></td>
<td id="S2.T1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.7.7.3.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S2.T1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.7.7.4.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S2.T1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.7.7.5.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S2.T1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.1.7.7.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S2.T1.1.8.8" class="ltx_tr">
<td id="S2.T1.1.8.8.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.1.8.8.1.1" class="ltx_text" style="font-size:70%;">DPPnet</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.T1.1.8.8.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S2.T1.1.8.8.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="S2.T1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.1.8.8.2.1" class="ltx_text" style="font-size:70%;">57.22</span></td>
<td id="S2.T1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.1.8.8.3.1" class="ltx_text" style="font-size:70%;">62.48</span></td>
<td id="S2.T1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.1.8.8.4.1" class="ltx_text" style="font-size:70%;">61.19</span></td>
<td id="S2.T1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.1.8.8.5.1" class="ltx_text" style="font-size:70%;">28.98</span></td>
<td id="S2.T1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.1.8.8.6.1" class="ltx_text" style="font-size:70%;">44.48</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Accuracy of algorithms on VQA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, COCO-QA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and DAQUAR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> datasets</figcaption>
</figure>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</h3>

<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Algorithm details</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">It is a common practice to fine tune the last few layers of pretrained convolutional network (trained for some classification or regression task), so that it may adapt to a new task. However, Visual Question Answering requires the model to capture different kind of information depending on the question that is posed. To resolve this problem, Noh et al¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> proposed DPPnet architecture that learns to dynamically change the network parameter based on the question asked. The architecture consist of two networks: Parameter Prediction Network that predicts the parameters of the answer network and classification network that chooses the answer from a list of candidate answers.</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.1" class="ltx_p">The authors proposed the addition of an extra branch to the fully connected layers of VGG16¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> network and called it as the Parameter Prediction Network. This network dynamically changes the parameters of classification network. The Parameter Prediction Network uses GRU cells and weights of this network depends on question. Output of this parameter prediction network is a weight matrix that is used by the classification network.</p>
</div>
<div id="S2.SS2.SSS1.p3" class="ltx_para">
<p id="S2.SS2.SSS1.p3.1" class="ltx_p">The classification network is modified to incorporate the dynamic parameter layer. It is the second last fully connected layer in network. The final layer is the softmax layer. The weight matrix determined by the dynamic parameter layers has very large size which makes it difficult to generate the complete matrix. To tackle this problem, authors used weight sharing technique. The output of the dynamic parameter layer is now a set of candidate weights instead of whole weight matrix. A hashing function is used to map this small set of weights to the complete weight matrix.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Experiments</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">The Dynamic Parameter Network was tested on three datasets: DAQUAR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, COCO-QA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and VQA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">DPPnet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> achieves accuracy of 57.22% on VQA Open-ended questions and 62.48% on Multiple-choice question of VQA test-dev dataset. The model gives 61.19% accuracy on COCO-QA dataset and 28.98% accuracy on DAQUAR-all dataset for single answer.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Stacked Attention Networks for Image Question Answering¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</h3>

<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Algorithm details</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para">
<p id="S2.SS3.SSS1.p1.1" class="ltx_p">To answer an image based question, the human brain processes the image and the question through multiple steps. To adapt a similar line of reasoning, Yang et al<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> proposed stacked attention network. This network consist of three models: image model, question model and attention network.</p>
</div>
<div id="S2.SS3.SSS1.p2" class="ltx_para">
<p id="S2.SS3.SSS1.p2.1" class="ltx_p">Image model uses VGGnet-16<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> network to obtain image features. Features from the last pooling layer were extracted (to maintain the spatial information) and fed through a single layer perceptron so as to to embed the visual features into a vector space which is compatible with textual features.</p>
</div>
<div id="S2.SS3.SSS1.p3" class="ltx_para">
<p id="S2.SS3.SSS1.p3.1" class="ltx_p">Question model uses two approaches to extract semantic meaning of the questions - LSTM model and CNN model. In the LSTM model, word embeddings of the question are fed into a LSTM and the hidden state vector, corresponding to the last word, is used as the vector representation of the question. The CNN based approach for text representation uses three types of convolutional filters: unigram, bigram and trigram. After applying these filters, three convolutional feature maps are obtained, one from each filter. Max-pooling is used to obtain a single vector from each feature map and these vectors are then concatenated to obtain the vector representation of the question.</p>
</div>
<div id="S2.SS3.SSS1.p4" class="ltx_para">
<p id="S2.SS3.SSS1.p4.1" class="ltx_p">The third model is the stacked attention network that takes as input the image feature vector and question feature vector and narrows down to the most relevant region in image that will help the network to answer the question correctly. The idea behind multi-step reasoning is to recursively refine the image information needed to answer the question. This is achieved by combining the image vector and question vector to generate an attention map. The weighted sum of the image regions and the generated attention distribution is then used as the new image vector. It is again combined with question vector to generate a more focused attention map which attends to the important regions in the image. The paper uses two layers of attention map and combines the final image vector and question vector to predict the answer.</p>
</div>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Experiments</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para">
<p id="S2.SS3.SSS2.p1.1" class="ltx_p">The Stacked Attention Network(SAN)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> model was evaluated on four datasets: DAQUAR-all<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, DAQUAR-reduced, COCO-QA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and VQA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. For DAQUAR and COCO-QA dataset, the question was encoded into a vector of 500 dimensions for the LSTM based model and into a vector of 640 dimensions for the CNN based model. However, the vector size was doubled for the VQA dataset as it is large dataset. The model was trained using SGD with momentum 0.9 with batch size of 100.</p>
</div>
<div id="S2.SS3.SSS2.p2" class="ltx_para">
<p id="S2.SS3.SSS2.p2.1" class="ltx_p">The accuracy on the four datasets is given in table<a href="#S2.T1" title="Table 1 ‚Ä£ 2.1.2 Experiments ‚Ä£ 2.1 Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources [21] ‚Ä£ 2 Approaches ‚Ä£ Survey of Recent Advances in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> under the row header SAN. The results shown in the table<a href="#S2.T1" title="Table 1 ‚Ä£ 2.1.2 Experiments ‚Ä£ 2.1 Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources [21] ‚Ä£ 2 Approaches ‚Ä£ Survey of Recent Advances in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> is for SAN(2,CNN) i.e. Stacked Attention Network with 2 layers of attention and CNN model for question model. The model was trained and tested only on single word answers for DAQUAR-all and DAQUAR-reduced dataset.</p>
</div>
<div id="S2.SS3.SSS2.p3" class="ltx_para">
<p id="S2.SS3.SSS2.p3.1" class="ltx_p">Yang et al also performed qualitative error analysis for SAN. 100 images were randomly picked from the set of wrongly answered test-set images. The errors were categorized into 4 classes: attention over wrong region, attention over correct region but incorrect answer prediction, ambiguous answers, viz. predicted answer is a synonym of the correct label, and wrong answers, viz the labels given in dataset are incorrect. Based on number of incorrect images falling into different error categories mentioned above, highest percentage of error was reported for the case when the network attended to the correct region but predicted incorrect answer. The lowest error percentage was reported from wrong answers.</p>
</div>
<div id="S2.SS3.SSS2.p4" class="ltx_para">
<p id="S2.SS3.SSS2.p4.1" class="ltx_p">Thus concisely saying, Yang et al<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> proposed an attention network to achieve better performance on visual question answering task by iteratively refining relevant image regions. However, based on the error analysis given in the paper, the error due to correct attention but wrong answer prediction, might be reduced by adding few fully connected layers before softmax layer. The model considers the question vector only once to refine the image information, however the important image regions change on different words in question, e.g. ‚Äùon the table‚Äù and ‚Äùunder the table‚Äù changes the relevant region in image. Hence, using the question vector at each stage of attention map would help in improving performance.</p>
</div>
</section>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Neural Module Network<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</h3>

<section id="S2.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.1 </span>Algorithm details</h4>

<div id="S2.SS4.SSS1.p1" class="ltx_para">
<p id="S2.SS4.SSS1.p1.1" class="ltx_p">Neural Module Network(NMN) introduces unique way of solving the visual question answering task. The general trend is to train a single, end-to-end network that extracts image features and question features, and combines them to predict the answers. However, Andreas et al<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> proposed that, to answer different questions, a network needs to perform different kind of processing. Thus, instead of trying to incorporate all reasoning capacity into a single large network, the paper introduces an algorithm that uses compositional structure of question to build and train a smaller network at training-time. In other words, the algorithm looks at question answering task as a function of many simple operations and these operations on a given image and these are determined by the question asked. The paper refers to these operations as modules and are implemented using simple neural networks.</p>
</div>
<div id="S2.SS4.SSS1.p2" class="ltx_para">
<p id="S2.SS4.SSS1.p2.1" class="ltx_p">The paper describes a finite set of modules (i.e. smaller neural networks) that are capable of doing primitive tasks. These networks are then combined with other modules and jointly trained to predict the answers. Based on different tasks, 5 types of modules are described. <span id="S2.SS4.SSS1.p2.1.1" class="ltx_text ltx_font_italic">Find</span> module performs convolution over image and gives attention map for the most relevant region. <span id="S2.SS4.SSS1.p2.1.2" class="ltx_text ltx_font_italic">Transform</span> module is a multilayer perceptron that changes the region of attention as required by the input. <span id="S2.SS4.SSS1.p2.1.3" class="ltx_text ltx_font_italic">Combine</span> module is a convolution layer with nonlinear output layer and it merges the given two attention maps. <span id="S2.SS4.SSS1.p2.1.4" class="ltx_text ltx_font_italic">Describe</span> module takes an image and an attention map and predicts the answer. <span id="S2.SS4.SSS1.p2.1.5" class="ltx_text ltx_font_italic">Measure</span> module parses the attention map alone and is helpful for answering questions related to the existence or count of objects.</p>
</div>
<div id="S2.SS4.SSS1.p3" class="ltx_para">
<p id="S2.SS4.SSS1.p3.1" class="ltx_p">A network graph is created using these modules and the structure of this graph is given by the parse tree of the question. The paper uses Stanford parser<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> to get the parse tree of the question. However, as the parse tree gives a very abstract representation for the question, it might lead to general answers. For example, Q1:Is there red circle to left of blue square and Q2: what is the color of circle to left of blue square, can have same compositional structure. To tackle this problem, the paper further uses LSTM encoded question to capture the syntactic regularities. Hence the final predicted answer uses the output of neural module network and LSTM.</p>
</div>
</section>
<section id="S2.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.2 </span>Experiments</h4>

<div id="S2.SS4.SSS2.p1" class="ltx_para">
<p id="S2.SS4.SSS2.p1.1" class="ltx_p">The paper introduces new synthetic dataset called SHAPES and evaluates the algorithm on it. It is also evaluated on VQA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> dataset. Output of max-pool of conv5 layer from pre-trained VGG-16<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> on ImageNet is used as input to NMN. The algorithm was also tested with image features from fine-tuned VGG-16 on MSCOCO captions. The table<a href="#S2.T1" title="Table 1 ‚Ä£ 2.1.2 Experiments ‚Ä£ 2.1 Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources [21] ‚Ä£ 2 Approaches ‚Ä£ Survey of Recent Advances in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the performance of model with NMN, LSTM and fine-tuned network on MSCOCO captions.</p>
</div>
</section>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</h3>

<section id="S2.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.5.1 </span>Algorithm details</h4>

<div id="S2.SS5.SSS1.p1" class="ltx_para">
<p id="S2.SS5.SSS1.p1.1" class="ltx_p">MCB<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> model was the winner of VQA challenge 2016. Usual approach taken by VQA algorithms is to do a simple concatenation, sum or dot product of image vector and text vector. However Fukui et al<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> hypothesize that capturing relation between every element of two vectors would give richer combined representation and help perform better at VQA task. Hence, the paper uses Bilinear pooling models<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> that computes outer product between image and text vector. However, as the dimension of input vectors increases, using this method becomes intractable due to explosion in number of learnable parameters.</p>
</div>
<div id="S2.SS5.SSS1.p2" class="ltx_para">
<p id="S2.SS5.SSS1.p2.2" class="ltx_p">The approximate solution of performing outer product in higher dimension was proposed by Charikar et al<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> that takes <math id="S2.SS5.SSS1.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS5.SSS1.p2.1.m1.1a"><mi id="S2.SS5.SSS1.p2.1.m1.1.1" xref="S2.SS5.SSS1.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.SSS1.p2.1.m1.1b"><ci id="S2.SS5.SSS1.p2.1.m1.1.1.cmml" xref="S2.SS5.SSS1.p2.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.SSS1.p2.1.m1.1c">n</annotation></semantics></math> dimensional vector and projects it to <math id="S2.SS5.SSS1.p2.2.m2.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S2.SS5.SSS1.p2.2.m2.1a"><mi id="S2.SS5.SSS1.p2.2.m2.1.1" xref="S2.SS5.SSS1.p2.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.SSS1.p2.2.m2.1b"><ci id="S2.SS5.SSS1.p2.2.m2.1.1.cmml" xref="S2.SS5.SSS1.p2.2.m2.1.1">ùëë</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.SSS1.p2.2.m2.1c">d</annotation></semantics></math> dimension using Count Sktech projection function. By this process, the outer product is now performed in lower dimensions. Further, the work by Pham et al<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> proved that count sketch projection of outer product of two vectors is equivalent to the convolution of the count sketches which can easily be performed by doing multiplication of their Fourier transforms.</p>
</div>
<div id="S2.SS5.SSS1.p3" class="ltx_para">
<p id="S2.SS5.SSS1.p3.1" class="ltx_p">MCB model for VQA task considers the image features from ‚Äùpool5‚Äù layer of ResNet-152<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. The output tensor of size 2048x14x14 was then L2 normalized for each of 196(i.e. 14x14) location giving image vector of size 2048. The words in questions are encoded in one-hot encoding and given to an embedding layer and then it fed to LSTM. The output from each LSTM layer is 1024 dimension which is concatenated to get question vector of 2048. These image vector and text vector are given as input to the MCB. The paper also shows that including the attention map in MCB pooling improves the performance. To get this attention map, spatial features from last convolution layer of ResNet or VGGnet are used. In general, top-1000 answers are used as classes in softmax layer. However, here, the softmax with top-3000 answers is applied.</p>
</div>
</section>
<section id="S2.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.5.2 </span>Experiments</h4>

<div id="S2.SS5.SSS2.p1" class="ltx_para">
<p id="S2.SS5.SSS2.p1.1" class="ltx_p">The MCB model was evaluated on visual question answering task as well as visual grounding task. For visual question answering task, MCB was evaluated on VQA dataset and Visual7W<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> dataset. It achieves accuracy of 62.2% on Visual7W dataset. The performance on VQA dataset, given in table <a href="#S2.T1" title="Table 1 ‚Ä£ 2.1.2 Experiments ‚Ä£ 2.1 Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources [21] ‚Ä£ 2 Approaches ‚Ä£ Survey of Recent Advances in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, is for MCB model with attention map, Glove word vector embeddings and Genome.</p>
</div>
<div id="S2.SS5.SSS2.p2" class="ltx_para">
<p id="S2.SS5.SSS2.p2.1" class="ltx_p">To summarize, VQA model by Fukui et al‚Äôs<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> has shown the best performance in VQA challenge. However, the image and text features used by MCB pooling are independent of each other, i.e. the image feature generated by ResNet-152 or VGG-16 are generic and can be made specific to question asked by passing some leaky information into image model. Similar concept can be used while getting features for question.</p>
</div>
</section>
</section>
<section id="S2.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.6 </span>Dual attention networks for multimodal reasoning and matching<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</h3>

<section id="S2.SS6.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.6.1 </span>Algorithm Details</h4>

<div id="S2.SS6.SSS1.p1" class="ltx_para">
<p id="S2.SS6.SSS1.p1.1" class="ltx_p">DAN (Dual Attention Network) employs textual attention along with visual attention in multiple steps. The paper proposes two variants of the algorithm: reasoning-DAN (r-DAN) and matching-DAN (m-DAN). Both algorithms use image features from either pool5 of VGGnet-19 or res5c of ResNet-152. The image is represented as a set of image regions where each image region is a vector of 512 dimensions (if VGG-19 is used) or 2048 dimensions(if ResNet-152 is used). The words in the question are represented are one-hot encoded and then embedded into a vector space. These embeddings are then passed through a bidirectional-LSTM. Sum of hidden states of forward and backward LSTM at each time step represents context vector of the word. The embedding layer and LSTM are trained end-to-end.</p>
</div>
<div id="S2.SS6.SSS1.p2" class="ltx_para">
<p id="S2.SS6.SSS1.p2.1" class="ltx_p">Nam et al<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> use the attention mechanism for both the visual and the textual data. It helps in focusing on different regions of image and question at different time steps. Attention model consists of a two-layer feed forward network with softmax output. The input to the network is the memory vector that contains the information seen so far and the vector corresponding to the visual region of image extracted from VGGnet or ResNet. The output gives a soft attention over the image regions. A similar neural network is employed for textual attention where the input is memory vector and sum of hidden states of bidirectional LSTM. However the only difference is that, in visual attention model, the paper uses an extra layer to get image context vectors into the same dimension as text context vector. Element-wise multiplication of visual context vector and textual context vector is added with previous step‚Äôs memory vector to get current step memory vector. It is then used to predict answers.</p>
</div>
<div id="S2.SS6.SSS1.p3" class="ltx_para">
<p id="S2.SS6.SSS1.p3.1" class="ltx_p">By using attention on image and text, the proposed model is not only helpful in task of combined reasoning over different types of input, but also in cross-domain information retrieval task e.g. finding images depicting same concept as a given text paragraph. The paper calls the first task as reasoning-DAN and second task as matching-DAN. The model for both algorithms is almost same except that, matching-DAN learns a joint embedding and maintains separate image and text memory vector to ease the comparison with arbitrary image text vectors.</p>
</div>
</section>
<section id="S2.SS6.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.6.2 </span>Experiments</h4>

<div id="S2.SS6.SSS2.p1" class="ltx_para">
<p id="S2.SS6.SSS2.p1.1" class="ltx_p">The r-DAN is evaluated on VQA dataset. Using two iterations to refine attention over relatively important region in image has shown the best result empirically. The word embedding size, LSTM and the attention model size are all set to 512. The model is trained on train-set + val-set ans validated on test-dev. It is trained for 60 epochs. The result of the DAN with ResNet image features is given in table<a href="#S2.T1" title="Table 1 ‚Ä£ 2.1.2 Experiments ‚Ä£ 2.1 Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources [21] ‚Ä£ 2 Approaches ‚Ä£ Survey of Recent Advances in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
</section>
<section id="S2.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.7 </span>Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</h3>

<section id="S2.SS7.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.7.1 </span>Algorithm Details</h4>

<div id="S2.SS7.SSS1.p1" class="ltx_para">
<p id="S2.SS7.SSS1.p1.1" class="ltx_p">Kazemi et al<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> proposes an attention based model for VQA task. The paper uses stacked attention which is very similar to the approach proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. The visual question answering task is modeled as a classification problem with question features extracted from LSTM and image features extracted from ResNet-152<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Word embeddings of the questions are fed to the LSTM and the state vector corresponding to the last word are used as the question feature. Image features are extracted from the layer before the final pooling layer of ResNet and <math id="S2.SS7.SSS1.p1.1.m1.1" class="ltx_Math" alttext="l_{2}" display="inline"><semantics id="S2.SS7.SSS1.p1.1.m1.1a"><msub id="S2.SS7.SSS1.p1.1.m1.1.1" xref="S2.SS7.SSS1.p1.1.m1.1.1.cmml"><mi id="S2.SS7.SSS1.p1.1.m1.1.1.2" xref="S2.SS7.SSS1.p1.1.m1.1.1.2.cmml">l</mi><mn id="S2.SS7.SSS1.p1.1.m1.1.1.3" xref="S2.SS7.SSS1.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS7.SSS1.p1.1.m1.1b"><apply id="S2.SS7.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS7.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS7.SSS1.p1.1.m1.1.1.1.cmml" xref="S2.SS7.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS7.SSS1.p1.1.m1.1.1.2.cmml" xref="S2.SS7.SSS1.p1.1.m1.1.1.2">ùëô</ci><cn type="integer" id="S2.SS7.SSS1.p1.1.m1.1.1.3.cmml" xref="S2.SS7.SSS1.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS7.SSS1.p1.1.m1.1c">l_{2}</annotation></semantics></math> normalized to improve performance. The computed image features and question features are concatenated and passed through two convolution layers which produce image feature glimpses. The weighted average of these image feature glimpses and the state vector of LSTM are concatenated and fed to fully connected layer with ReLu non-linearity to get probability distribution over the answers.</p>
</div>
</section>
<section id="S2.SS7.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.7.2 </span>Experiments</h4>

<div id="S2.SS7.SSS2.p1" class="ltx_para">
<p id="S2.SS7.SSS2.p1.1" class="ltx_p">Kazemi et al<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> evaluated the proposed model on VQA1.0<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> as well as on newly released VQA2.0<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. The model achieves accuracy of 64.5% on VQA1.0 test-dev dataset and accuracy of 59.67% on VQA2.0 validation set.</p>
</div>
<div id="S2.SS7.SSS2.p2" class="ltx_para">
<p id="S2.SS7.SSS2.p2.1" class="ltx_p">The paper also analyzes the effect of various factors on the performance of model on validation set of VQA1.0. In the default setting, the model performs <math id="S2.SS7.SSS2.p2.1.m1.1" class="ltx_Math" alttext="l_{2}" display="inline"><semantics id="S2.SS7.SSS2.p2.1.m1.1a"><msub id="S2.SS7.SSS2.p2.1.m1.1.1" xref="S2.SS7.SSS2.p2.1.m1.1.1.cmml"><mi id="S2.SS7.SSS2.p2.1.m1.1.1.2" xref="S2.SS7.SSS2.p2.1.m1.1.1.2.cmml">l</mi><mn id="S2.SS7.SSS2.p2.1.m1.1.1.3" xref="S2.SS7.SSS2.p2.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS7.SSS2.p2.1.m1.1b"><apply id="S2.SS7.SSS2.p2.1.m1.1.1.cmml" xref="S2.SS7.SSS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS7.SSS2.p2.1.m1.1.1.1.cmml" xref="S2.SS7.SSS2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS7.SSS2.p2.1.m1.1.1.2.cmml" xref="S2.SS7.SSS2.p2.1.m1.1.1.2">ùëô</ci><cn type="integer" id="S2.SS7.SSS2.p2.1.m1.1.1.3.cmml" xref="S2.SS7.SSS2.p2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS7.SSS2.p2.1.m1.1c">l_{2}</annotation></semantics></math> normalization along the depth dimension of image features. It employs dropout with probability of 0.5 on fully connected layers, convolutional layers as well as LSTM. The words in question are embedded in 300 dimensions and LSTM state size is set to 1024 dimensions. The model glimpses over image twice and answer is predicted from 3000 class classifier. With this setting, model achieves accuracy of 60.95% when trained with Adam optimizer for 100K steps with batch size of 128.</p>
</div>
<div id="S2.SS7.SSS2.p3" class="ltx_para">
<p id="S2.SS7.SSS2.p3.2" class="ltx_p">The model‚Äôs performance was significantly affected by <math id="S2.SS7.SSS2.p3.1.m1.1" class="ltx_Math" alttext="l_{2}" display="inline"><semantics id="S2.SS7.SSS2.p3.1.m1.1a"><msub id="S2.SS7.SSS2.p3.1.m1.1.1" xref="S2.SS7.SSS2.p3.1.m1.1.1.cmml"><mi id="S2.SS7.SSS2.p3.1.m1.1.1.2" xref="S2.SS7.SSS2.p3.1.m1.1.1.2.cmml">l</mi><mn id="S2.SS7.SSS2.p3.1.m1.1.1.3" xref="S2.SS7.SSS2.p3.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS7.SSS2.p3.1.m1.1b"><apply id="S2.SS7.SSS2.p3.1.m1.1.1.cmml" xref="S2.SS7.SSS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS7.SSS2.p3.1.m1.1.1.1.cmml" xref="S2.SS7.SSS2.p3.1.m1.1.1">subscript</csymbol><ci id="S2.SS7.SSS2.p3.1.m1.1.1.2.cmml" xref="S2.SS7.SSS2.p3.1.m1.1.1.2">ùëô</ci><cn type="integer" id="S2.SS7.SSS2.p3.1.m1.1.1.3.cmml" xref="S2.SS7.SSS2.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS7.SSS2.p3.1.m1.1c">l_{2}</annotation></semantics></math> normalization, dropout, number of fully connected layers used before softmax layer and the use of attention. The accuracy for the model, that does not use <math id="S2.SS7.SSS2.p3.2.m2.1" class="ltx_Math" alttext="l_{2}" display="inline"><semantics id="S2.SS7.SSS2.p3.2.m2.1a"><msub id="S2.SS7.SSS2.p3.2.m2.1.1" xref="S2.SS7.SSS2.p3.2.m2.1.1.cmml"><mi id="S2.SS7.SSS2.p3.2.m2.1.1.2" xref="S2.SS7.SSS2.p3.2.m2.1.1.2.cmml">l</mi><mn id="S2.SS7.SSS2.p3.2.m2.1.1.3" xref="S2.SS7.SSS2.p3.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS7.SSS2.p3.2.m2.1b"><apply id="S2.SS7.SSS2.p3.2.m2.1.1.cmml" xref="S2.SS7.SSS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS7.SSS2.p3.2.m2.1.1.1.cmml" xref="S2.SS7.SSS2.p3.2.m2.1.1">subscript</csymbol><ci id="S2.SS7.SSS2.p3.2.m2.1.1.2.cmml" xref="S2.SS7.SSS2.p3.2.m2.1.1.2">ùëô</ci><cn type="integer" id="S2.SS7.SSS2.p3.2.m2.1.1.3.cmml" xref="S2.SS7.SSS2.p3.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS7.SSS2.p3.2.m2.1c">l_{2}</annotation></semantics></math> normalization, drops to 54.69% and model that does not use dropout in fully connected and convolution layers could achieve accuracy of only 56.98, showing that use of dropout helps in avoiding over-fitting. The result from model with no attention could only give an accuracy of 57.72%, confirming the use of soft attention improves the performance. However, using stacked attention in model shows minimal improvement in accuracy.</p>
</div>
<div id="S2.SS7.SSS2.p4" class="ltx_para">
<p id="S2.SS7.SSS2.p4.1" class="ltx_p">The model was also trained on VQA2.0 training set and evaluated on validation set and reports accuracy of 59.67%. Table<a href="#S2.T1" title="Table 1 ‚Ä£ 2.1.2 Experiments ‚Ä£ 2.1 Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources [21] ‚Ä£ 2 Approaches ‚Ä£ Survey of Recent Advances in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and table <a href="#S2.T2" title="Table 2 ‚Ä£ 2.7.2 Experiments ‚Ä£ 2.7 Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering[10] ‚Ä£ 2 Approaches ‚Ä£ Survey of Recent Advances in Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> show the performance of model on VQA1.0 and VQA2.0 respectively.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<table id="S2.T2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.1.1.1" class="ltx_tr">
<th id="S2.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Algorithm</th>
<th id="S2.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">VQA open ended</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.1.2.1" class="ltx_tr">
<td id="S2.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Show,ask,attend,ans<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</td>
<td id="S2.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">59.67</td>
</tr>
<tr id="S2.T2.1.3.2" class="ltx_tr">
<td id="S2.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">MCB<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S2.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">59.14</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Accuracy of algorithms on VQA2.0<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> validation set</figcaption>
</figure>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Conclusion</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We present an overview of the diverse set of algorithms employed for the visual question answering task. We have compared these algorithms on the basis of approach used for extracting image and textual features. We also discussed how these VQA models perform on a variety of datasets - VQA, VQA2.0, COCO-QA, DAQUAR datasets. On VQA dataset, MCB performance best with accuracy of 66.7%, however Show, Ask, Attend and Answer has shown state of the art result of 59.67% on VQA2.0. The use of attention mechanism in the VQA model has shown significant improvement and using attention based models is becoming a common trend. Attention module is helpful in understanding, how the VQA model has arrived to the answer for a given question and we can accordingly make changes to our architecture so that it can capture more of relevant information. Along with use of attention over image as well as text, adding a feedback loop between image module and text module, and generating answers instead of predicting from finite set of words could be the next step towards achieving better performance in visual question answering task.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
J.¬†Andreas, M.¬†Rohrbach, T.¬†Darrell, and D.¬†Klein.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Neural module networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages 39‚Äì48, 2016.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
S.¬†Antol, A.¬†Agrawal, J.¬†Lu, M.¬†Mitchell, D.¬†Batra, C.¬†Lawrence¬†Zitnick, and
D.¬†Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Vqa: Visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, pages 2425‚Äì2433, 2015.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
S.¬†Auer, C.¬†Bizer, G.¬†Kobilarov, J.¬†Lehmann, R.¬†Cyganiak, and Z.¬†Ives.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Dbpedia: A nucleus for a web of open data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">The semantic web</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, pages 722‚Äì735, 2007.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
M.¬†Charikar, K.¬†Chen, and M.¬†Farach-Colton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Finding frequent items in data streams.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Automata, languages and programming</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, pages 784‚Äì784, 2002.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
X.¬†Chen, H.¬†Fang, T.-Y. Lin, R.¬†Vedantam, S.¬†Gupta, P.¬†Doll√°r, and C.¬†L.
Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco captions: Data collection and evaluation server.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1504.00325</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
J.¬†Deng, W.¬†Dong, R.¬†Socher, L.-J. Li, K.¬†Li, and L.¬†Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision and Pattern Recognition, 2009. CVPR 2009.
IEEE Conference on</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 248‚Äì255. IEEE, 2009.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
A.¬†Fukui, D.¬†H. Park, D.¬†Yang, A.¬†Rohrbach, T.¬†Darrell, and M.¬†Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Multimodal compact bilinear pooling for visual question answering and
visual grounding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1606.01847</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Y.¬†Goyal, T.¬†Khot, D.¬†Summers-Stay, D.¬†Batra, and D.¬†Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1612.00837</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
K.¬†He, X.¬†Zhang, S.¬†Ren, and J.¬†Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 770‚Äì778, 2016.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
V.¬†Kazemi and A.¬†Elqursh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Show, ask, attend, and answer: A strong baseline for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1704.03162</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
D.¬†Klein and C.¬†D. Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Accurate unlexicalized parsing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 41st Annual Meeting on Association for
Computational Linguistics-Volume 1</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 423‚Äì430. Association for
Computational Linguistics, 2003.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Q.¬†Le and T.¬†Mikolov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Distributed representations of sentences and documents.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 31st International Conference on Machine
Learning (ICML-14)</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 1188‚Äì1196, 2014.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
T.-Y. Lin, M.¬†Maire, S.¬†Belongie, J.¬†Hays, P.¬†Perona, D.¬†Ramanan,
P.¬†Doll√°r, and C.¬†L. Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 740‚Äì755.
Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
M.¬†Malinowski and M.¬†Fritz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">A multi-world approach to question answering about real-world scenes
based on uncertain input.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages
1682‚Äì1690, 2014.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
H.¬†Nam, J.-W. Ha, and J.¬†Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Dual attention networks for multimodal reasoning and matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1611.00471</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
H.¬†Noh, P.¬†Hongsuck¬†Seo, and B.¬†Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Image question answering using convolutional neural network with
dynamic parameter prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 30‚Äì38, 2016.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
N.¬†Pham and R.¬†Pagh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Fast and scalable polynomial kernels via explicit feature maps.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 19th ACM SIGKDD international conference
on Knowledge discovery and data mining</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 239‚Äì247. ACM, 2013.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
M.¬†Ren, R.¬†Kiros, and R.¬†Zemel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Image question answering: A visual semantic embedding model and a new
dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Advances in Neural Inf. Process. Syst</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 1(2):5, 2015.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
K.¬†Simonyan and A.¬†Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Very deep convolutional networks for large-scale image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1409.1556</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
J.¬†B. Tenenbaum and W.¬†T. Freeman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Separating style and content.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages
662‚Äì668, 1997.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Q.¬†Wu, P.¬†Wang, C.¬†Shen, A.¬†Dick, and A.¬†van¬†den Hengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Ask me anything: Free-form visual question answering based on
knowledge from external sources.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 4622‚Äì4630, 2016.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Z.¬†Yang, X.¬†He, J.¬†Gao, L.¬†Deng, and A.¬†Smola.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Stacked attention networks for image question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 21‚Äì29, 2016.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Y.¬†Zhu, O.¬†Groth, M.¬†Bernstein, and L.¬†Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Visual7w: Grounded question answering in images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 4995‚Äì5004, 2016.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1709.08202" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1709.08203" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1709.08203">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1709.08203" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1709.08204" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar 15 22:13:13 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
