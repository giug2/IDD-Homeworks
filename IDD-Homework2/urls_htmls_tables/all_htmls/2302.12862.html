<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2302.12862] 1 Introduction</title><meta property="og:description" content="Cross-device federated learning (FL) has been well-studied from algorithmic, system scalability, and training speed perspectives.
Nonetheless, moving from centralized training to cross-device FL for millions or billion…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="1 Introduction">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="1 Introduction">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2302.12862">

<!--Generated on Thu Feb 29 22:44:53 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">marginparsep has been altered.
<br class="ltx_break">topmargin has been altered.
<br class="ltx_break">marginparwidth has been altered.
<br class="ltx_break">marginparpush has been altered.
<br class="ltx_break"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">The page layout violates the ICML style.</span>
Please do not change the page layout, or include packages like geometry,
savetrees, or fullpage, which change it for you.
We’re not able to reliably undo arbitrary changes to the style. Please remove
the offending package(s), or layout-changing commands and try again.</p>
</div>
<div id="p3" class="ltx_para ltx_align_center">
<p id="p3.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:2.0pt;background:black;display:inline-block;"> </span></p>
</div>
<div id="p4" class="ltx_para ltx_align_center">
<p id="p4.1" class="ltx_p"><span id="p4.1.1" class="ltx_text ltx_font_smallcaps">FLINT: A Platform for Federated Learning Integration</span></p>
</div>
<div id="p5" class="ltx_para ltx_align_center">
<p id="p5.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span></p>
</div>
<div id="id1" class="ltx_logical-block">
<div id="id1.p1" class="ltx_para">
<p id="id1.p1.1" class="ltx_p ltx_align_center"><span id="id1.p1.1.1" class="ltx_text ltx_font_bold">Ewen Wang</span> <sup id="id1.p1.1.2" class="ltx_sup">* </sup><sup id="id1.p1.1.3" class="ltx_sup">1 </sup> 
<span id="id1.p1.1.4" class="ltx_text ltx_font_bold">Ajay Kannan</span> <sup id="id1.p1.1.5" class="ltx_sup">1 </sup> 
<span id="id1.p1.1.6" class="ltx_text ltx_font_bold">Yuefeng Liang</span> <sup id="id1.p1.1.7" class="ltx_sup">1 </sup></p>
<p id="id1.p1.2" class="ltx_p ltx_align_center"><span id="id1.p1.2.1" class="ltx_text ltx_font_bold">Boyi Chen</span> <sup id="id1.p1.2.2" class="ltx_sup">* </sup><sup id="id1.p1.2.3" class="ltx_sup">1 </sup> 
<span id="id1.p1.2.4" class="ltx_text ltx_font_bold">Mosharaf Chowdhury</span> <sup id="id1.p1.2.5" class="ltx_sup">* </sup><sup id="id1.p1.2.6" class="ltx_sup">2 </sup><sup id="id1.p1.2.7" class="ltx_sup">3 </sup></p>
</div>
</div>
<div id="p6" class="ltx_para ltx_noindent">
<br class="ltx_break">
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Cross-device federated learning (FL) has been well-studied from algorithmic, system scalability, and training speed perspectives.
Nonetheless, moving from centralized training to cross-device FL for millions or billions of devices presents many risks, including performance loss, developer inertia, poor user experience, and unexpected application failures.
In addition, the corresponding infrastructure, development costs, and return on investment are difficult to estimate.
In this paper, we present a device-cloud collaborative FL platform that integrates with an existing machine learning platform, providing tools to measure real-world constraints, assess infrastructure capabilities, evaluate model training performance, and estimate system resource requirements to responsibly bring FL into production.
We also present a <em id="id2.id1.1" class="ltx_emph ltx_font_italic">decision workflow</em> that leverages the FL-integrated platform to comprehensively evaluate the trade-offs of cross-device FL and share our empirical evaluations of business-critical machine learning applications that impact hundreds of millions of users.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span><sup id="footnotex1.1" class="ltx_sup">*</sup>Equal contribution 
<sup id="footnotex1.2" class="ltx_sup">1</sup>LinkedIn Corporation
<sup id="footnotex1.3" class="ltx_sup">2</sup>University of Michigan and RightScope Inc.
<sup id="footnotex1.4" class="ltx_sup">3</sup>Work done at LinkedIn.
Correspondence to: Ewen Wang &lt;yuxwang@linkedin.com&gt;, Boyi Chen &lt;bochen@linkedin.com&gt;.
 
<br class="ltx_break">Preliminary work. Under review by the
Machine Learning and Systems (MLSys) Conference. Do not distribute.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">With increasing computation power and storage capacity in end-user devices, there is a rising trend to move machine learning (ML) toward the edge where data is generated.
One incentive behind this trend is latency reduction in moving computation to the device.
For instance, for real-time CV and NLP tasks in search and content understanding, sending data in the form of video, audio, or text between user devices and the server is a major bottleneck <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Lv et al.</span> (<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a>)</cite>.
Additionally, increasing demands for data protection and privacy in the forms of government regulations (e.g., <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">GDPR</span></a></cite>) and platform restrictions (e.g., App Tracking Transparency from <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">Apple</span></a></cite>) introduce challenges in performing traditional centralized ML on sensitive data.
These motivate applications like messaging, content recommendation, and advertising, which often rely on potentially sensitive user data to achieve high accuracy, to move ML tasks to the device.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Cross-device federated learning (FL) has captured the zeitgeist as an effective mechanism to address the aforementioned challenges both in industry and academia <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Kairouz et al.</span> (<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2021</span></a>)</cite>.
Federated learning allows distributed ML training on user data on their own devices.
Indeed, federated learning has been successfully deployed on a <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">case-by-case</em> basis throughout the industry.
Examples include query suggestions on Google Keyboard <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Hard et al.</span> (<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2018a</span></a>); <span class="ltx_text" style="color:#000000;">Yang et al.</span> (<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2018</span></a>); <span class="ltx_text" style="color:#000000;">Chen et al.</span> (<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2019</span></a>); <span class="ltx_text" style="color:#000000;">Ramaswamy et al.</span> (<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2019</span></a>)</cite>, Android smart text selection <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Hartmann</span> (<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2021</span></a>)</cite>, applications at Meta <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Nguyen et al.</span> (<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a>); <span class="ltx_text" style="color:#000000;">Wu et al.</span> (<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a>)</cite>, and several ML tasks on Apple’s iOS devices <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Paulik et al.</span> (<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2021b</span></a>)</cite>.
Prior work in cross-device FL has mainly focused on algorithmic improvements <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Li et al.</span> (<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2020</span></a>); <span class="ltx_text" style="color:#000000;">Horvath et al.</span> (<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2021</span></a>)</cite>, system scalability <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Bonawitz et al.</span> (<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2019</span></a>); <span class="ltx_text" style="color:#000000;">Huba et al.</span> (<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a>)</cite>, secure aggregation techniques <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">So et al.</span> (<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a>)</cite>, and model convergence speeds <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Yu et al.</span> (<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2019</span></a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">However, unlike centralized machine learning, where model architecture and parameters can be tuned and tested in an offline setting, cross-device FL relies on online training systems that require a large population of users to produce utility.
At LinkedIn, close to 8,000 types of user devices with more than 150 different OS versions have been observed from use cases in its mobile application.
Having a comprehensive understanding of the impact of different model architectures and hyperparameters on all user devices before deployment is crucial to successful FL training and user satisfaction.
Running resource-intensive ML tasks on user devices can negatively impact user experience and degrade user trust and product experiences.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">Building a cross-device FL system and migrating centralized training to that FL system is non-trivial, especially with millions of users.
Forecasting and optimizing infrastructure requirements like resource consumption, data payload restrictions, and model complexity limits for different design choices (e.g., how to collaboratively manage features from cloud and devices, whether to use synchronous or asynchronous training modes and what hyperparameters to use for training) are critical to production success.
A systematic decision workflow to empirically evaluate production design choices is missing in the existing literature.
Many companies have established ML platforms for centralized training, yet do not have the platform and process to estimate the benefits, constraints, and implications of FL. Vice-versa, existing FL platforms today are independent platforms without a close integration with their centralized counterparts.
</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text ltx_font_bold">Contributions.</span> This paper describes the architecture of a novel device-cloud collaborative platform for FL integration, “FLINT,” that augments LinkedIn’s well-established centralized ML platform. Moreover, it presents a cost-effective decision workflow that leverages the platform to practically assess cross-device FL in LinkedIn’s context.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">In Section <a href="#S2" title="2 Background" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we describe the traditional ML systems at LinkedIn, followed by motivations and challenges of cross-device FL.
Then, Section <a href="#S3" title="3 System Design" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> describes a detailed FL platform design that closely integrates with the centralized components. This includes an experimental framework that extends the simulation capabilities of FedScale <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Lai et al.</span> (<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a>)</cite>, which uses device profiles, traces and a virtual clock to provide realistic FL simulations. We have contributed some of our innovations back upstream to the open-source repository.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/SymbioticLab/FedScale</span></span></span><span id="S1.p6.1.1" class="ltx_text" style="color:#000000;">
In Section <a href="#S4" title="4 Case Studies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we apply the decision workflow on real use cases at LinkedIn that could benefit from FL, demonstrating how the FLINT platform can provide ML practitioners with the tools to estimate impact, de-risk projects, and clarify modeling assumptions using a combination of cloud and device resources.
We show how a close integration with LinkedIn’s centralized ML platform can help modeling teams evaluate FL in a familiar environment.</span></p>
</div>
<div id="S1.p7" class="ltx_para ltx_noindent">
<p id="S1.p7.1" class="ltx_p"><span id="S1.p7.1.1" class="ltx_text" style="color:#000000;">Throughout the sections, we share real-world measurements produced by the platform tools, providing insights into an FL system’s constraints such as device availability and data/compute heterogeneity. This includes on-device benchmarks of critical, low-latency models on popular device hardware. We demonstrate how these measurements can help forecast model performance under observed real-world constraints and estimate cloud and device resource costs.</span></p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Centralized ML at LinkedIn</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p"><span id="S2.SS1.p1.1.1" class="ltx_text" style="color:#000000;">LinkedIn applies ML to tackle business-critical problems in numerous domains, including advertising, search, messaging, news feed, notifications, and more. Like many traditional ML platforms, a typical ML workflow at LinkedIn consists of data generation, model training and inference.
In data generation, data collected in the cloud from multiple sources are anonymized, analyzed, sanity-checked and conflated to extract features and labels for training.
The data is then used as input for the model training step to perform offline model training and testing.
The resulting model is deployed by production systems to serve users (both consumer and enterprise).
At the end, the impressions and actions from users are then logged via tracking events for future data analysis and model iterations.
For each of these steps, LinkedIn’s platform uses a combination of reputable open-source and bespoke tools to meet business needs.</span></p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Motivations for Cross-Device FL</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p"><span id="S2.SS2.p1.1.1" class="ltx_text" style="color:#000000;">Increasing demand for privacy and performance is driving the industry to consider moving away from centralized-only ML solutions and instead incorporating computations at the data sources. Similarly, LinkedIn is considering using cross-device FL for some of its applications.</span></p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Privacy and Security.</span><span id="S2.SS2.p2.1.2" class="ltx_text" style="color:#000000;">
The centralized raw data collection and data mixing between users needed to generate training data introduce privacy and security risks.
With user privacy a priority and a key consideration in LinkedIn’s product design, there are strong incentives to explore moving some business-critical model training (such as those in the advertising and messaging domains) to user devices to reduce tracking and merging sensitive data.</span></p>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Performance.</span><span id="S2.SS2.p3.1.2" class="ltx_text" style="color:#000000;">
Moving ML computations closer to their data sources provides better user experiences via improved systems performance.
Many applications (such as search) at LinkedIn require low latency and high adaptability for recency.
Models for these applications need to be constantly retrained to adopt the most recent user actions; model inference needs to spend minimal time to deliver predictions.
Under the centralized model training paradigm, large payloads of user events and inference results have to be transmitted back and forth between devices and the centralized server, which can sometimes introduce significant delays in model freshness and inference speed.</span></p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Challenges of Cross-Device FL</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p"><span id="S2.SS3.p1.1.1" class="ltx_text" style="color:#000000;">Despite the benefits, cross-device FL comes with unique challenges and isn’t an end-all solution for all scenarios.
In centralized ML systems, parameter tuning can optimize the model performance assessed by offline evaluations on centralized testing data; system performance can be examined by running trained models using designated hardware; centralized data can be validated, sampled, and shuffled in scalable data pipelines </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Baylor et al.</span> <span id="S2.SS3.p1.1.2.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2017</span></a><span id="S2.SS3.p1.1.3.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S2.SS3.p1.1.4" class="ltx_text" style="color:#000000;">.
In contrast, on-device data processing, training, and inference require significantly more careful offline evaluations and system design to responsibly leverage user hardware.
Parameter tuning using user devices can be resource-consuming and deploying faulty or resource-hogging jobs to user devices can harm user trust and negatively impact product reputation and business metrics.</span></p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p"><span id="S2.SS3.p2.1.1" class="ltx_text" style="color:#000000;">Systems and data heterogeneity present another major challenge. User device heterogeneity (Figure </span><a href="#S2.F1" title="Figure 1 ‣ 2.3 Challenges of Cross-Device FL ‣ 2 Background" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S2.SS3.p2.1.2" class="ltx_text" style="color:#000000;">) results in significant differences in computation power across various training tasks (Figure </span><a href="#S3.F4" title="Figure 4 ‣ 3.2 Real World Measurements ‣ 3 System Design" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S2.SS3.p2.1.3" class="ltx_text" style="color:#000000;">). Moreover, user behavior differences between devices lead to uneven data availability (Figure </span><a href="#S2.F2" title="Figure 2 ‣ 2.3 Challenges of Cross-Device FL ‣ 2 Background" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S2.SS3.p2.1.4" class="ltx_text" style="color:#000000;">), diverse data distribution, and violations of feature independence.</span></p>
</div>
<div id="S2.SS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.p3.1" class="ltx_p"><span id="S2.SS3.p3.1.1" class="ltx_text" style="color:#000000;">These practical challenges put constraints on key aspects of machine learning like model complexity and convergence speed. Ensuring high-quality user experience – for both the model engineers of the ML platform and users whose devices would participate in training and inference – requires a comprehensive evaluation framework that considers these system constraints.</span></p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2302.12862/assets/graphics/devices.png" id="S2.F1.g1" class="ltx_graphics ltx_img_landscape" width="330" height="125" alt="Refer to caption">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Distribution of iOS-based (<span id="S2.F1.7.1" class="ltx_text ltx_font_italic">left</span>) and Android-based (<span id="S2.F1.8.2" class="ltx_text ltx_font_italic">right</span>) mobile devices in the user base of an example application at LinkedIn. The gray regions contain device models outside of the legend. Note that Android hardware is much more diverse than iOS hardware, making compute capability challenging to estimate.</figcaption>
</figure>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2302.12862/assets/graphics/device_avail.png" id="S2.F2.g1" class="ltx_graphics ltx_img_landscape" width="314" height="159" alt="Refer to caption">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Normalized device availability of an example application at LinkedIn over a one-week period, demonstrating the high fluctuation in client availability. The predominant trend is that the number of available devices peaks each day and drops to 15% of the weekly peak at the troughs.</figcaption>
</figure>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2302.12862/assets/graphics/workflow.png" id="S2.F3.g1" class="ltx_graphics ltx_img_landscape" width="673" height="160" alt="Refer to caption">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>A device-cloud collaborative ML platform with FL integration.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">3 </span>System Design</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>The Integrated FL Platform at LinkedIn</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text" style="color:#000000;">We propose an FL system that works in collaboration with the centralized ML platform (Figure </span><a href="#S2.F3" title="Figure 3 ‣ 2.3 Challenges of Cross-Device FL ‣ 2 Background" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.SS1.p1.1.2" class="ltx_text" style="color:#000000;">). It shares common components like model stores, job scheduling, monitoring, and visualization tools with the centralized ML platform described by </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Baylor et al.</span> <span id="S3.SS1.p1.1.3.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2017</span></a><span id="S3.SS1.p1.1.4.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S3.SS1.p1.1.5" class="ltx_text" style="color:#000000;">, and introduces FL-specific components to enable cross-device FL.</span></p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text" style="color:#000000;">On the device side, an on-device runtime library encodes the FL training and inference tasks and is consumed by 1st party or 3rd party applications.
On the server side, there is an FL server that performs model parameter aggregation and client coordination. The model store, which is shared by centralized training, can store and retrieve versioned parameters during FL training.
The overall mechanism of the FL server and the device side library works in similar fashions as discussed in other FL system literature </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Bonawitz et al.</span> <span id="S3.SS1.p2.1.2.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2019</span></a><span id="S3.SS1.p2.1.3.2.2.1" class="ltx_text" style="color:#000000;">)</span>; <span class="ltx_text" style="color:#000000;">Paulik et al.</span> <span id="S3.SS1.p2.1.2.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2021a</span></a><span id="S3.SS1.p2.1.3.2.2.1" class="ltx_text" style="color:#000000;">)</span>; <span class="ltx_text" style="color:#000000;">Huba et al.</span> <span id="S3.SS1.p2.1.2.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a><span id="S3.SS1.p2.1.3.2.2.1" class="ltx_text" style="color:#000000;">)</span>; <span class="ltx_text" style="color:#000000;">Lv et al.</span> <span id="S3.SS1.p2.1.2.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a><span id="S3.SS1.p2.1.3.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S3.SS1.p2.1.4" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text" style="color:#000000;">Our proposed FL integration platform, FLINT, builds on top of these well-known FL and centralized ML platforms. This section focuses on 1) tools to leverage centralized data and resources for analyzing FL’s impact and viability, 2) a feature catalog that manages both cloud and device-based data, 3) an experimental framework to optimize model performance and system requirements, and 4) a decision workflow that enables decision-makers to understand the constraints, costs, and effectiveness of FL for their business needs.</span></p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Real World Measurements</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text" style="color:#000000;">Measuring system constraints from different perspectives helps provide realistic evaluation contexts and guides the design of production systems.
Running on-device benchmarks before deployment enables engineers to ensure viability of models embedded in heterogeneous software/hardware stacks.
Most existing web services log session metrics and device information during user requests. Our platform tools can analyze this data to produce metrics and visualizations around user device availability patterns and device computation capabilities.</span></p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">On-Device Benchmarks.</span><span id="S3.SS2.p2.1.2" class="ltx_text" style="color:#000000;">
In cross-device FL, the bulk of the computation is offloaded to the clients. Edge devices act as worker nodes in a large computing cluster.
Importantly, each worker’s underlying CPU, GPU, storage, memory, and OS are heterogeneous and could consume vastly different resources to achieve the same task (Figure </span><a href="#S3.F4" title="Figure 4 ‣ 3.2 Real World Measurements ‣ 3 System Design" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S3.SS2.p2.1.3" class="ltx_text" style="color:#000000;">).
This makes the runtime of an FL task difficult to estimate and could lead to inconsistent user experiences.
Before allocating such workloads to a heterogeneous population of mobile workers, FLINT’s device benchmark step packages models into a benchmark app and deploys it to a pool of test-purposed mobile devices in the cloud, including older and newer generations of popular phones and tablets from Figure </span><a href="#S2.F1" title="Figure 1 ‣ 2.3 Challenges of Cross-Device FL ‣ 2 Background" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S3.SS2.p2.1.4" class="ltx_text" style="color:#000000;">. The collected results (Table </span><a href="#S4.T5" title="Table 5 ‣ 4 Case Studies" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="S3.SS2.p2.1.5" class="ltx_text" style="color:#000000;">) help modelers understand their FL model’s worst-case impact on users to derive compatible device models and OS versions for FL participation.</span></p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2302.12862/assets/graphics/modeltimes.png" id="S3.F4.g1" class="ltx_graphics ltx_figure_panel ltx_img_square" width="157" height="157" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2302.12862/assets/graphics/modelcpus.png" id="S3.F4.g2" class="ltx_graphics ltx_figure_panel ltx_img_square" width="157" height="160" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>A comparison of two business-critical models’ on-device training times and max compute usage percentage over 5,000 examples. This benchmark on 27 device models shows the effects of diverse hardware, and how devices that are optimized for one task might be worse for another. Note the magnitudes difference in training time between FL tasks A and B.</figcaption>
</figure>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">User Device Availability.</span><span id="S3.SS2.p3.1.2" class="ltx_text" style="color:#000000;">
We define device availability as pairs of start and end times during which a device can participate in FL training.
This availability, which fluctuates widely over time (Figure </span><a href="#S2.F2" title="Figure 2 ‣ 2.3 Challenges of Cross-Device FL ‣ 2 Background" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.SS2.p3.1.3" class="ltx_text" style="color:#000000;">), can affect client selection, model fairness, and convergence.
Our tool helps modelers generate device availability from existing session logs by specifying a set of availability criteria.
The criteria can include conditions from three categories;
1) compute capability: based on the device benchmark results, the modeler can generate a list of devices and OS versions that have acceptable worst-case device impact and are compatible with the model architecture;
2) device state: WiFi connection, battery level, and whether the app is open in the foreground;
3) user attributes: account reputation, account age, and last participation time, etc.
These criteria should be iteratively refined to meet the desired model, system, and security needs while ensuring that the model performance is fair among different sub-populations of clients.
For instance, if a device hardware criterion introduces biased model performance on users of older phones, then the hardware requirement needs to be relaxed. And while device charging isn’t required for smaller models, a CPU-intensive model (such as Model E in Table </span><a href="#S4.T5" title="Table 5 ‣ 4 Case Studies" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="S3.SS2.p3.1.4" class="ltx_text" style="color:#000000;">), should require a higher battery level (</span><math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mo mathcolor="#000000" id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><gt id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">&gt;</annotation></semantics></math><span id="S3.SS2.p3.1.5" class="ltx_text" style="color:#000000;">80%) for participation.</span></p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Mobile device availability of an example mobile use case at LinkedIn after applying each participation criteria, showing that only a subset of all users is FL-eligible in practice.</figcaption>
<br class="ltx_break">
<div id="S3.T1.6.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:204.5pt;height:81pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-11.4pt,4.5pt) scale(0.9,0.9) ;">
<table id="S3.T1.6.6.6" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.6.6.6.7.1" class="ltx_tr">
<th id="S3.T1.6.6.6.7.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S3.T1.6.6.6.7.1.1.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Training Criteria</span></th>
<th id="S3.T1.6.6.6.7.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.6.6.6.7.1.2.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Devices Available</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<math id="S3.T1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.T1.1.1.1.1.1.m1.1a"><mi mathcolor="#000000" id="S3.T1.1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.1.m1.1c">A</annotation></semantics></math><span id="S3.T1.1.1.1.1.1.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">: Connected to WiFi</span>
</th>
<td id="S3.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.1.1.2.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">70%</span></td>
</tr>
<tr id="S3.T1.3.3.3.3" class="ltx_tr">
<th id="S3.T1.3.3.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S3.T1.2.2.2.2.1.m1.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.T1.2.2.2.2.1.m1.1a"><mi mathcolor="#000000" id="S3.T1.2.2.2.2.1.m1.1.1" xref="S3.T1.2.2.2.2.1.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.1.m1.1b"><ci id="S3.T1.2.2.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.2.2.1.m1.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.1.m1.1c">B</annotation></semantics></math><span id="S3.T1.3.3.3.3.2.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">: Battery Level </span><math id="S3.T1.3.3.3.3.2.m2.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="S3.T1.3.3.3.3.2.m2.1a"><mo mathcolor="#000000" id="S3.T1.3.3.3.3.2.m2.1.1" xref="S3.T1.3.3.3.3.2.m2.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.2.m2.1b"><geq id="S3.T1.3.3.3.3.2.m2.1.1.cmml" xref="S3.T1.3.3.3.3.2.m2.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.2.m2.1c">\geq</annotation></semantics></math><span id="S3.T1.3.3.3.3.2.2" class="ltx_text ltx_font_smallcaps" style="color:#000000;"> 80%</span>
</th>
<td id="S3.T1.3.3.3.3.3" class="ltx_td ltx_align_center"><span id="S3.T1.3.3.3.3.3.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">34%</span></td>
</tr>
<tr id="S3.T1.5.5.5.5" class="ltx_tr">
<th id="S3.T1.5.5.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<math id="S3.T1.4.4.4.4.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.T1.4.4.4.4.1.m1.1a"><mi mathcolor="#000000" id="S3.T1.4.4.4.4.1.m1.1.1" xref="S3.T1.4.4.4.4.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.4.1.m1.1b"><ci id="S3.T1.4.4.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.4.4.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.4.1.m1.1c">C</annotation></semantics></math><span id="S3.T1.5.5.5.5.2.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">: OS Release </span><math id="S3.T1.5.5.5.5.2.m2.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="S3.T1.5.5.5.5.2.m2.1a"><mo mathcolor="#000000" id="S3.T1.5.5.5.5.2.m2.1.1" xref="S3.T1.5.5.5.5.2.m2.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.5.2.m2.1b"><geq id="S3.T1.5.5.5.5.2.m2.1.1.cmml" xref="S3.T1.5.5.5.5.2.m2.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.5.2.m2.1c">\geq</annotation></semantics></math><span id="S3.T1.5.5.5.5.2.2" class="ltx_text ltx_font_smallcaps" style="color:#000000;"> Sept. 2019</span>
</th>
<td id="S3.T1.5.5.5.5.3" class="ltx_td ltx_align_center"><span id="S3.T1.5.5.5.5.3.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">93%</span></td>
</tr>
<tr id="S3.T1.6.6.6.6" class="ltx_tr">
<th id="S3.T1.6.6.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><math id="S3.T1.6.6.6.6.1.m1.1" class="ltx_Math" alttext="A\cap B\cap C" display="inline"><semantics id="S3.T1.6.6.6.6.1.m1.1a"><mrow id="S3.T1.6.6.6.6.1.m1.1.1" xref="S3.T1.6.6.6.6.1.m1.1.1.cmml"><mi mathcolor="#000000" id="S3.T1.6.6.6.6.1.m1.1.1.2" xref="S3.T1.6.6.6.6.1.m1.1.1.2.cmml">A</mi><mo mathcolor="#000000" id="S3.T1.6.6.6.6.1.m1.1.1.1" xref="S3.T1.6.6.6.6.1.m1.1.1.1.cmml">∩</mo><mi mathcolor="#000000" id="S3.T1.6.6.6.6.1.m1.1.1.3" xref="S3.T1.6.6.6.6.1.m1.1.1.3.cmml">B</mi><mo mathcolor="#000000" id="S3.T1.6.6.6.6.1.m1.1.1.1a" xref="S3.T1.6.6.6.6.1.m1.1.1.1.cmml">∩</mo><mi mathcolor="#000000" id="S3.T1.6.6.6.6.1.m1.1.1.4" xref="S3.T1.6.6.6.6.1.m1.1.1.4.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.6.1.m1.1b"><apply id="S3.T1.6.6.6.6.1.m1.1.1.cmml" xref="S3.T1.6.6.6.6.1.m1.1.1"><intersect id="S3.T1.6.6.6.6.1.m1.1.1.1.cmml" xref="S3.T1.6.6.6.6.1.m1.1.1.1"></intersect><ci id="S3.T1.6.6.6.6.1.m1.1.1.2.cmml" xref="S3.T1.6.6.6.6.1.m1.1.1.2">𝐴</ci><ci id="S3.T1.6.6.6.6.1.m1.1.1.3.cmml" xref="S3.T1.6.6.6.6.1.m1.1.1.3">𝐵</ci><ci id="S3.T1.6.6.6.6.1.m1.1.1.4.cmml" xref="S3.T1.6.6.6.6.1.m1.1.1.4">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.6.1.m1.1c">A\cap B\cap C</annotation></semantics></math></th>
<td id="S3.T1.6.6.6.6.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T1.6.6.6.6.2.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">22%</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.3" class="ltx_p"><span id="S3.SS2.p4.3.1" class="ltx_text" style="color:#000000;">In Table </span><a href="#S3.T1" title="Table 1 ‣ 3.2 Real World Measurements ‣ 3 System Design" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S3.SS2.p4.3.2" class="ltx_text" style="color:#000000;">, we specify a restrictive scenario where conditions </span><math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mi mathcolor="#000000" id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">A</annotation></semantics></math><span id="S3.SS2.p4.3.3" class="ltx_text" style="color:#000000;">, </span><math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><mi mathcolor="#000000" id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><ci id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">B</annotation></semantics></math><span id="S3.SS2.p4.3.4" class="ltx_text" style="color:#000000;">, and </span><math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><mi mathcolor="#000000" id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><ci id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">C</annotation></semantics></math><span id="S3.SS2.p4.3.5" class="ltx_text" style="color:#000000;"> must all be met, leaving only 22% of the clients available for FL participation.
In this scenario, the training task may require various permissions from the device that may not be available in the background to complete all the sub-tasks (model download/upload, data processing, model evaluation, metrics reporting, etc.).
This worst-case assumption helps to de-risk potential platform-specific changes to background task permissions. Naturally, app usage duration is tail-heavy and poses a challenge in completing training during short durations of availability.</span></p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Proxy Data Generator</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text" style="color:#000000;">To benchmark existing models in FL under realistic heterogeneous conditions, we provide the modeler with a tool to generate per-device proxy datasets from training data in our centralized catalog. In our experiments, the proxy datasets need to be no bigger than centralized training to achieve similar performances. When available, the modeler selects a partitioning field such as obfuscated member or device identifier. The generator uses this field to map records to FL clients.
When privacy is a concern, the centralized dataset’s client-level identifier is discarded.
In these cases, synthetic partitioning strategies </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Li et al.</span> <span id="S3.SS3.p1.1.2.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a><span id="S3.SS3.p1.1.3.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S3.SS3.p1.1.4" class="ltx_text" style="color:#000000;"> can inject label and data quantity skew between the partitions modeled by a Dirichlet distribution. To evaluate the model under varying data heterogeneity, developers can generate multiple versions of a synthetically-split proxy dataset. After generating a proxy, the tool stores it back to the data catalog, adding FL-specific metadata describing feature distributions, client data quantity, label distribution, and client population. These characteristics provide an important understanding of the data heterogeneity between clients (Figure </span><a href="#S3.F5" title="Figure 5 ‣ 3.3 Proxy Data Generator ‣ 3 System Design" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="S3.SS3.p1.1.5" class="ltx_text" style="color:#000000;"> and Table </span><a href="#S3.T2" title="Table 2 ‣ 3.3 Proxy Data Generator ‣ 3 System Design" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.SS3.p1.1.6" class="ltx_text" style="color:#000000;">).</span></p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2302.12862/assets/graphics/datasizes.png" id="S3.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="263" height="141" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The quantity distribution of key proxy datasets from different domains used in the evaluation, showing that the data sizes between clients in different domains can greatly vary.</figcaption>
</figure>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Data Locality.</span><span id="S3.SS3.p2.1.2" class="ltx_text" style="color:#000000;"> Though FL can effectively move compute to the data source, the device runtime should still be able to access cloud-based data through network communication when doing so provides systems and model performance benefits. For some tasks, it may be optimal to pull ready-to-use features from the cloud on-demand and join them with device-based contextual features. This reduces the storage and compute footprint of storing and processing large features like embeddings on the device.
Meanwhile, inference records containing smaller cloud-based features can be cached on the device to reduce network-induced latency during training data processing.
Additionally, many models require vocabulary files, which contain a set of string to integer ID mappings for features in a dataset, to encode strings into vector values during data processing. The device runtime can pull or cache these files depending on storage usage, WiFi connectivity, and the resource and latency requirements of the task.
To allow experimenting with combinations of feature management strategies for various applications, FLINT provides a feature catalog (Figure </span><a href="#S3.F6" title="Figure 6 ‣ 3.3 Proxy Data Generator ‣ 3 System Design" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">6</span></a><span id="S3.SS3.p2.1.3" class="ltx_text" style="color:#000000;">) that manages 1) the device-based features’ retention policies and data size limits through cloud-based metadata, 2) the caching strategy of cloud-based features on user devices, and 3) where feature transformations happen.
The device feature management and caching also allow multiple applications to use overlapping features without duplicated work; when a feature value is created for one task, the runtime can cache it for reuse to reduce latency.</span></p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Characteristics of sample proxy datasets that are heavily down-sampled on a client level. The max/avg/std values are calculated from client data quantity.</figcaption>
<br class="ltx_break">
<div id="S3.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:225.4pt;height:113.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-12.5pt,6.3pt) scale(0.9,0.9) ;">
<table id="S3.T2.3.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.3.1.1.1" class="ltx_tr">
<td id="S3.T2.3.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S3.T2.3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.3.1.1.1.2.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Dataset A</span></td>
<td id="S3.T2.3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.3.1.1.1.3.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Dataset B</span></td>
<td id="S3.T2.3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.3.1.1.1.4.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Dataset C</span></td>
</tr>
<tr id="S3.T2.3.1.2.2" class="ltx_tr">
<td id="S3.T2.3.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.3.1.2.2.1.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Client pop.</span></td>
<td id="S3.T2.3.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.3.1.2.2.2.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">700,000</span></td>
<td id="S3.T2.3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.3.1.2.2.3.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">1,024,950</span></td>
<td id="S3.T2.3.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.3.1.2.2.4.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">16,422,290</span></td>
</tr>
<tr id="S3.T2.3.1.3.3" class="ltx_tr">
<td id="S3.T2.3.1.3.3.1" class="ltx_td ltx_align_left"><span id="S3.T2.3.1.3.3.1.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Max records</span></td>
<td id="S3.T2.3.1.3.3.2" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.3.3.2.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">39,731</span></td>
<td id="S3.T2.3.1.3.3.3" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.3.3.3.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">103,471</span></td>
<td id="S3.T2.3.1.3.3.4" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.3.3.4.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">406</span></td>
</tr>
<tr id="S3.T2.3.1.4.4" class="ltx_tr">
<td id="S3.T2.3.1.4.4.1" class="ltx_td ltx_align_left"><span id="S3.T2.3.1.4.4.1.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Avg records</span></td>
<td id="S3.T2.3.1.4.4.2" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.4.4.2.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">99</span></td>
<td id="S3.T2.3.1.4.4.3" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.4.4.3.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">184</span></td>
<td id="S3.T2.3.1.4.4.4" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.4.4.4.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">1.53</span></td>
</tr>
<tr id="S3.T2.3.1.5.5" class="ltx_tr">
<td id="S3.T2.3.1.5.5.1" class="ltx_td ltx_align_left"><span id="S3.T2.3.1.5.5.1.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Std records</span></td>
<td id="S3.T2.3.1.5.5.2" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.5.5.2.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">667</span></td>
<td id="S3.T2.3.1.5.5.3" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.5.5.3.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">374</span></td>
<td id="S3.T2.3.1.5.5.4" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.5.5.4.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">1.47</span></td>
</tr>
<tr id="S3.T2.3.1.6.6" class="ltx_tr">
<td id="S3.T2.3.1.6.6.1" class="ltx_td ltx_align_left"><span id="S3.T2.3.1.6.6.1.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Label ratio</span></td>
<td id="S3.T2.3.1.6.6.2" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.6.6.2.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">0.28</span></td>
<td id="S3.T2.3.1.6.6.3" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.6.6.3.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">0.05</span></td>
<td id="S3.T2.3.1.6.6.4" class="ltx_td ltx_align_center"><span id="S3.T2.3.1.6.6.4.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">0.06</span></td>
</tr>
<tr id="S3.T2.3.1.7.7" class="ltx_tr">
<td id="S3.T2.3.1.7.7.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S3.T2.3.1.7.7.1.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Lookback days</span></td>
<td id="S3.T2.3.1.7.7.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.3.1.7.7.2.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">90</span></td>
<td id="S3.T2.3.1.7.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.3.1.7.7.3.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">28</span></td>
<td id="S3.T2.3.1.7.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.3.1.7.7.4.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">61</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2302.12862/assets/graphics/featureCatalog.png" id="S3.F6.g1" class="ltx_graphics ltx_img_landscape" width="314" height="133" alt="Refer to caption">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The architecture of a device-cloud feature catalog that manages both device-side and cloud-side features. Certain features and mapping vocabulary can be pulled from the cloud and cached during inference and training. Processed features can also be cached for reuse.</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Experimental Framework</h3>

<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.1" class="ltx_p"><span id="S3.SS4.p1.1.1" class="ltx_text" style="color:#000000;">A holistic experimental framework for FL should not only produce model metrics, but also system metrics under realistic constraints.
One goal is to understand the return on investment of FL applications under measured system constraints.
Another is to predict the infrastructure needs of such a system.
An added benefit is that modelers can better understand and tune the FL parameters before deploying jobs to devices because offloading all the hyper-parameter tuning workloads to production leads to wasted user resources.
Our framework builds on top of and significantly extends an open-source FL benchmarking platform to fit our requirements. Deployed on centralized ML clusters, a group of executors poll tasks to run from a leader node, which manages client selection, tracks virtual time, and calculates systems metrics.</span></p>
</div>
<div id="S3.SS4.p2" class="ltx_para ltx_noindent">
<p id="S3.SS4.p2.1" class="ltx_p"><span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Inputs and Assumptions</span><span id="S3.SS4.p2.1.2" class="ltx_text" style="color:#000000;">.
In practice, the systems constraints discussed earlier all affect FL’s training performance.
As such, our framework takes multiple real-world inputs to incorporate the complex interactions among these factors in its simulations.
First, each executor loads a partition of the proxy dataset and maps its records to clients.
Then, the leader loads and uses device availability records for client selection and task completion decisions.
It then consumes the model’s on-device benchmarks (model footprint, processing-time, network usage, etc.), along with the hardware/OS distribution of the users. With these detailed inputs, our framework can report model and system metrics over both virtual clock time and communication rounds to account for data heterogeneity, model complexity, device availability, and hardware capability.</span></p>
</div>
<div id="S3.SS4.p3" class="ltx_para ltx_noindent">
<p id="S3.SS4.p3.8" class="ltx_p"><span id="S3.SS4.p3.8.1" class="ltx_text ltx_font_bold" style="color:#000000;">Synchronous and Asynchronous Training.</span><span id="S3.SS4.p3.8.2" class="ltx_text" style="color:#000000;">
Our framework supports synchronous FedAvg </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">McMahan et al.</span> <span id="S3.SS4.p3.8.3.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2017</span></a><span id="S3.SS4.p3.8.4.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S3.SS4.p3.8.5" class="ltx_text" style="color:#000000;"> and asynchronous FedBuff </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Nguyen et al.</span> <span id="S3.SS4.p3.8.6.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a><span id="S3.SS4.p3.8.7.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S3.SS4.p3.8.8" class="ltx_text" style="color:#000000;"> training modes.
In practice, client selection is largely dictated by client arrival and availability. Hence, our framework directly selects the next available device from the input sessions at a given virtual time and dispatches a task to an executor. The framework reports results over a virtual time that’s calculated independently of the underlying hardware clock. This allows for a better representation of the system in practice when estimating how long a job needs to run, or how much compute time needs to be spent on the device. Even before a client task is dispatched to an executor, the task’s duration is calculated using the inputs provided.
To estimate client </span><math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><mi mathcolor="#000000" id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><ci id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">k</annotation></semantics></math><span id="S3.SS4.p3.8.9" class="ltx_text" style="color:#000000;">’s task duration, we sample </span><math id="S3.SS4.p3.2.m2.1" class="ltx_Math" alttext="t\leftarrow T" display="inline"><semantics id="S3.SS4.p3.2.m2.1a"><mrow id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml"><mi mathcolor="#000000" id="S3.SS4.p3.2.m2.1.1.2" xref="S3.SS4.p3.2.m2.1.1.2.cmml">t</mi><mo mathcolor="#000000" stretchy="false" id="S3.SS4.p3.2.m2.1.1.1" xref="S3.SS4.p3.2.m2.1.1.1.cmml">←</mo><mi mathcolor="#000000" id="S3.SS4.p3.2.m2.1.1.3" xref="S3.SS4.p3.2.m2.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><apply id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1"><ci id="S3.SS4.p3.2.m2.1.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1.1">←</ci><ci id="S3.SS4.p3.2.m2.1.1.2.cmml" xref="S3.SS4.p3.2.m2.1.1.2">𝑡</ci><ci id="S3.SS4.p3.2.m2.1.1.3.cmml" xref="S3.SS4.p3.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">t\leftarrow T</annotation></semantics></math><span id="S3.SS4.p3.8.10" class="ltx_text" style="color:#000000;">, the distribution of time to train a single example from on-device benchmarks; we also sample a network bandwidth </span><math id="S3.SS4.p3.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS4.p3.3.m3.1a"><mi mathcolor="#000000" id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><ci id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">N</annotation></semantics></math><span id="S3.SS4.p3.8.11" class="ltx_text" style="color:#000000;"> from Puffer </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Yan et al.</span> <span id="S3.SS4.p3.8.12.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2020</span></a><span id="S3.SS4.p3.8.13.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S3.SS4.p3.8.14" class="ltx_text" style="color:#000000;">, an open-source dataset containing edge device network speeds.
Let </span><math id="S3.SS4.p3.4.m4.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.SS4.p3.4.m4.1a"><mi mathcolor="#000000" id="S3.SS4.p3.4.m4.1.1" xref="S3.SS4.p3.4.m4.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.1b"><ci id="S3.SS4.p3.4.m4.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.1c">E</annotation></semantics></math><span id="S3.SS4.p3.8.15" class="ltx_text" style="color:#000000;"> be the number of local epochs, </span><math id="S3.SS4.p3.5.m5.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS4.p3.5.m5.1a"><mi mathcolor="#000000" id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.1b"><ci id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.1c">M</annotation></semantics></math><span id="S3.SS4.p3.8.16" class="ltx_text" style="color:#000000;"> be the size of a gradient update, and </span><math id="S3.SS4.p3.6.m6.1" class="ltx_Math" alttext="|D_{k}|" display="inline"><semantics id="S3.SS4.p3.6.m6.1a"><mrow id="S3.SS4.p3.6.m6.1.1.1" xref="S3.SS4.p3.6.m6.1.1.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.SS4.p3.6.m6.1.1.1.2" xref="S3.SS4.p3.6.m6.1.1.2.1.cmml">|</mo><msub id="S3.SS4.p3.6.m6.1.1.1.1" xref="S3.SS4.p3.6.m6.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.SS4.p3.6.m6.1.1.1.1.2" xref="S3.SS4.p3.6.m6.1.1.1.1.2.cmml">D</mi><mi mathcolor="#000000" id="S3.SS4.p3.6.m6.1.1.1.1.3" xref="S3.SS4.p3.6.m6.1.1.1.1.3.cmml">k</mi></msub><mo mathcolor="#000000" stretchy="false" id="S3.SS4.p3.6.m6.1.1.1.3" xref="S3.SS4.p3.6.m6.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.6.m6.1b"><apply id="S3.SS4.p3.6.m6.1.1.2.cmml" xref="S3.SS4.p3.6.m6.1.1.1"><abs id="S3.SS4.p3.6.m6.1.1.2.1.cmml" xref="S3.SS4.p3.6.m6.1.1.1.2"></abs><apply id="S3.SS4.p3.6.m6.1.1.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.6.m6.1.1.1.1.1.cmml" xref="S3.SS4.p3.6.m6.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p3.6.m6.1.1.1.1.2.cmml" xref="S3.SS4.p3.6.m6.1.1.1.1.2">𝐷</ci><ci id="S3.SS4.p3.6.m6.1.1.1.1.3.cmml" xref="S3.SS4.p3.6.m6.1.1.1.1.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.6.m6.1c">|D_{k}|</annotation></semantics></math><span id="S3.SS4.p3.8.17" class="ltx_text" style="color:#000000;"> be client </span><math id="S3.SS4.p3.7.m7.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS4.p3.7.m7.1a"><mi mathcolor="#000000" id="S3.SS4.p3.7.m7.1.1" xref="S3.SS4.p3.7.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.7.m7.1b"><ci id="S3.SS4.p3.7.m7.1.1.cmml" xref="S3.SS4.p3.7.m7.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.7.m7.1c">k</annotation></semantics></math><span id="S3.SS4.p3.8.18" class="ltx_text" style="color:#000000;">’s partition size, </span><math id="S3.SS4.p3.8.m8.2" class="ltx_Math" alttext="taskDuration(k)=t*E*|D_{k}|+\frac{2*M}{N}" display="inline"><semantics id="S3.SS4.p3.8.m8.2a"><mrow id="S3.SS4.p3.8.m8.2.2" xref="S3.SS4.p3.8.m8.2.2.cmml"><mrow id="S3.SS4.p3.8.m8.2.2.3" xref="S3.SS4.p3.8.m8.2.2.3.cmml"><mi mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.3.2" xref="S3.SS4.p3.8.m8.2.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.8.m8.2.2.3.1" xref="S3.SS4.p3.8.m8.2.2.3.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.3.3" xref="S3.SS4.p3.8.m8.2.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.8.m8.2.2.3.1a" xref="S3.SS4.p3.8.m8.2.2.3.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.3.4" xref="S3.SS4.p3.8.m8.2.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.8.m8.2.2.3.1b" xref="S3.SS4.p3.8.m8.2.2.3.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.3.5" xref="S3.SS4.p3.8.m8.2.2.3.5.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.8.m8.2.2.3.1c" xref="S3.SS4.p3.8.m8.2.2.3.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.3.6" xref="S3.SS4.p3.8.m8.2.2.3.6.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.8.m8.2.2.3.1d" xref="S3.SS4.p3.8.m8.2.2.3.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.3.7" xref="S3.SS4.p3.8.m8.2.2.3.7.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.8.m8.2.2.3.1e" xref="S3.SS4.p3.8.m8.2.2.3.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.3.8" xref="S3.SS4.p3.8.m8.2.2.3.8.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.8.m8.2.2.3.1f" xref="S3.SS4.p3.8.m8.2.2.3.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.3.9" xref="S3.SS4.p3.8.m8.2.2.3.9.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.8.m8.2.2.3.1g" xref="S3.SS4.p3.8.m8.2.2.3.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.3.10" xref="S3.SS4.p3.8.m8.2.2.3.10.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.8.m8.2.2.3.1h" xref="S3.SS4.p3.8.m8.2.2.3.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.3.11" xref="S3.SS4.p3.8.m8.2.2.3.11.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.8.m8.2.2.3.1i" xref="S3.SS4.p3.8.m8.2.2.3.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.3.12" xref="S3.SS4.p3.8.m8.2.2.3.12.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.8.m8.2.2.3.1j" xref="S3.SS4.p3.8.m8.2.2.3.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.3.13" xref="S3.SS4.p3.8.m8.2.2.3.13.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p3.8.m8.2.2.3.1k" xref="S3.SS4.p3.8.m8.2.2.3.1.cmml">​</mo><mrow id="S3.SS4.p3.8.m8.2.2.3.14.2" xref="S3.SS4.p3.8.m8.2.2.3.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.SS4.p3.8.m8.2.2.3.14.2.1" xref="S3.SS4.p3.8.m8.2.2.3.cmml">(</mo><mi mathcolor="#000000" id="S3.SS4.p3.8.m8.1.1" xref="S3.SS4.p3.8.m8.1.1.cmml">k</mi><mo mathcolor="#000000" stretchy="false" id="S3.SS4.p3.8.m8.2.2.3.14.2.2" xref="S3.SS4.p3.8.m8.2.2.3.cmml">)</mo></mrow></mrow><mo mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.2" xref="S3.SS4.p3.8.m8.2.2.2.cmml">=</mo><mrow id="S3.SS4.p3.8.m8.2.2.1" xref="S3.SS4.p3.8.m8.2.2.1.cmml"><mrow id="S3.SS4.p3.8.m8.2.2.1.1" xref="S3.SS4.p3.8.m8.2.2.1.1.cmml"><mi mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.1.1.3" xref="S3.SS4.p3.8.m8.2.2.1.1.3.cmml">t</mi><mo lspace="0.222em" mathcolor="#000000" rspace="0.222em" id="S3.SS4.p3.8.m8.2.2.1.1.2" xref="S3.SS4.p3.8.m8.2.2.1.1.2.cmml">∗</mo><mi mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.1.1.4" xref="S3.SS4.p3.8.m8.2.2.1.1.4.cmml">E</mi><mo lspace="0.222em" mathcolor="#000000" rspace="0.222em" id="S3.SS4.p3.8.m8.2.2.1.1.2a" xref="S3.SS4.p3.8.m8.2.2.1.1.2.cmml">∗</mo><mrow id="S3.SS4.p3.8.m8.2.2.1.1.1.1" xref="S3.SS4.p3.8.m8.2.2.1.1.1.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.SS4.p3.8.m8.2.2.1.1.1.1.2" xref="S3.SS4.p3.8.m8.2.2.1.1.1.2.1.cmml">|</mo><msub id="S3.SS4.p3.8.m8.2.2.1.1.1.1.1" xref="S3.SS4.p3.8.m8.2.2.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.1.1.1.1.1.2" xref="S3.SS4.p3.8.m8.2.2.1.1.1.1.1.2.cmml">D</mi><mi mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.1.1.1.1.1.3" xref="S3.SS4.p3.8.m8.2.2.1.1.1.1.1.3.cmml">k</mi></msub><mo mathcolor="#000000" stretchy="false" id="S3.SS4.p3.8.m8.2.2.1.1.1.1.3" xref="S3.SS4.p3.8.m8.2.2.1.1.1.2.1.cmml">|</mo></mrow></mrow><mo mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.1.2" xref="S3.SS4.p3.8.m8.2.2.1.2.cmml">+</mo><mfrac mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.1.3" xref="S3.SS4.p3.8.m8.2.2.1.3.cmml"><mrow id="S3.SS4.p3.8.m8.2.2.1.3.2" xref="S3.SS4.p3.8.m8.2.2.1.3.2.cmml"><mn mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.1.3.2.2" xref="S3.SS4.p3.8.m8.2.2.1.3.2.2.cmml">2</mn><mo lspace="0.222em" mathcolor="#000000" rspace="0.222em" id="S3.SS4.p3.8.m8.2.2.1.3.2.1" xref="S3.SS4.p3.8.m8.2.2.1.3.2.1.cmml">∗</mo><mi mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.1.3.2.3" xref="S3.SS4.p3.8.m8.2.2.1.3.2.3.cmml">M</mi></mrow><mi mathcolor="#000000" id="S3.SS4.p3.8.m8.2.2.1.3.3" xref="S3.SS4.p3.8.m8.2.2.1.3.3.cmml">N</mi></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.8.m8.2b"><apply id="S3.SS4.p3.8.m8.2.2.cmml" xref="S3.SS4.p3.8.m8.2.2"><eq id="S3.SS4.p3.8.m8.2.2.2.cmml" xref="S3.SS4.p3.8.m8.2.2.2"></eq><apply id="S3.SS4.p3.8.m8.2.2.3.cmml" xref="S3.SS4.p3.8.m8.2.2.3"><times id="S3.SS4.p3.8.m8.2.2.3.1.cmml" xref="S3.SS4.p3.8.m8.2.2.3.1"></times><ci id="S3.SS4.p3.8.m8.2.2.3.2.cmml" xref="S3.SS4.p3.8.m8.2.2.3.2">𝑡</ci><ci id="S3.SS4.p3.8.m8.2.2.3.3.cmml" xref="S3.SS4.p3.8.m8.2.2.3.3">𝑎</ci><ci id="S3.SS4.p3.8.m8.2.2.3.4.cmml" xref="S3.SS4.p3.8.m8.2.2.3.4">𝑠</ci><ci id="S3.SS4.p3.8.m8.2.2.3.5.cmml" xref="S3.SS4.p3.8.m8.2.2.3.5">𝑘</ci><ci id="S3.SS4.p3.8.m8.2.2.3.6.cmml" xref="S3.SS4.p3.8.m8.2.2.3.6">𝐷</ci><ci id="S3.SS4.p3.8.m8.2.2.3.7.cmml" xref="S3.SS4.p3.8.m8.2.2.3.7">𝑢</ci><ci id="S3.SS4.p3.8.m8.2.2.3.8.cmml" xref="S3.SS4.p3.8.m8.2.2.3.8">𝑟</ci><ci id="S3.SS4.p3.8.m8.2.2.3.9.cmml" xref="S3.SS4.p3.8.m8.2.2.3.9">𝑎</ci><ci id="S3.SS4.p3.8.m8.2.2.3.10.cmml" xref="S3.SS4.p3.8.m8.2.2.3.10">𝑡</ci><ci id="S3.SS4.p3.8.m8.2.2.3.11.cmml" xref="S3.SS4.p3.8.m8.2.2.3.11">𝑖</ci><ci id="S3.SS4.p3.8.m8.2.2.3.12.cmml" xref="S3.SS4.p3.8.m8.2.2.3.12">𝑜</ci><ci id="S3.SS4.p3.8.m8.2.2.3.13.cmml" xref="S3.SS4.p3.8.m8.2.2.3.13">𝑛</ci><ci id="S3.SS4.p3.8.m8.1.1.cmml" xref="S3.SS4.p3.8.m8.1.1">𝑘</ci></apply><apply id="S3.SS4.p3.8.m8.2.2.1.cmml" xref="S3.SS4.p3.8.m8.2.2.1"><plus id="S3.SS4.p3.8.m8.2.2.1.2.cmml" xref="S3.SS4.p3.8.m8.2.2.1.2"></plus><apply id="S3.SS4.p3.8.m8.2.2.1.1.cmml" xref="S3.SS4.p3.8.m8.2.2.1.1"><times id="S3.SS4.p3.8.m8.2.2.1.1.2.cmml" xref="S3.SS4.p3.8.m8.2.2.1.1.2"></times><ci id="S3.SS4.p3.8.m8.2.2.1.1.3.cmml" xref="S3.SS4.p3.8.m8.2.2.1.1.3">𝑡</ci><ci id="S3.SS4.p3.8.m8.2.2.1.1.4.cmml" xref="S3.SS4.p3.8.m8.2.2.1.1.4">𝐸</ci><apply id="S3.SS4.p3.8.m8.2.2.1.1.1.2.cmml" xref="S3.SS4.p3.8.m8.2.2.1.1.1.1"><abs id="S3.SS4.p3.8.m8.2.2.1.1.1.2.1.cmml" xref="S3.SS4.p3.8.m8.2.2.1.1.1.1.2"></abs><apply id="S3.SS4.p3.8.m8.2.2.1.1.1.1.1.cmml" xref="S3.SS4.p3.8.m8.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.8.m8.2.2.1.1.1.1.1.1.cmml" xref="S3.SS4.p3.8.m8.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.p3.8.m8.2.2.1.1.1.1.1.2.cmml" xref="S3.SS4.p3.8.m8.2.2.1.1.1.1.1.2">𝐷</ci><ci id="S3.SS4.p3.8.m8.2.2.1.1.1.1.1.3.cmml" xref="S3.SS4.p3.8.m8.2.2.1.1.1.1.1.3">𝑘</ci></apply></apply></apply><apply id="S3.SS4.p3.8.m8.2.2.1.3.cmml" xref="S3.SS4.p3.8.m8.2.2.1.3"><divide id="S3.SS4.p3.8.m8.2.2.1.3.1.cmml" xref="S3.SS4.p3.8.m8.2.2.1.3"></divide><apply id="S3.SS4.p3.8.m8.2.2.1.3.2.cmml" xref="S3.SS4.p3.8.m8.2.2.1.3.2"><times id="S3.SS4.p3.8.m8.2.2.1.3.2.1.cmml" xref="S3.SS4.p3.8.m8.2.2.1.3.2.1"></times><cn type="integer" id="S3.SS4.p3.8.m8.2.2.1.3.2.2.cmml" xref="S3.SS4.p3.8.m8.2.2.1.3.2.2">2</cn><ci id="S3.SS4.p3.8.m8.2.2.1.3.2.3.cmml" xref="S3.SS4.p3.8.m8.2.2.1.3.2.3">𝑀</ci></apply><ci id="S3.SS4.p3.8.m8.2.2.1.3.3.cmml" xref="S3.SS4.p3.8.m8.2.2.1.3.3">𝑁</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.8.m8.2c">taskDuration(k)=t*E*|D_{k}|+\frac{2*M}{N}</annotation></semantics></math><span id="S3.SS4.p3.8.19" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="S3.SS4.p4" class="ltx_para ltx_noindent">
<p id="S3.SS4.p4.1" class="ltx_p"><span id="S3.SS4.p4.1.1" class="ltx_text" style="color:#000000;">While the asynchronous mode is simpler to implement in a real-time system, it is more difficult to schedule tasks in the right order in a fast-forwarded and distributed simulation. To resolve this, the leader node uses a priority queue-based task scheduler to generate tasks in a streaming fashion and dispatch them to workers in the correct order. From evaluations of different models, we observe that the benefits of an asynchronous system depend on the spread of the client task durations.
We offer two explanations on why FedBuff </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Nguyen et al.</span> <span id="S3.SS4.p4.1.2.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a><span id="S3.SS4.p4.1.3.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S3.SS4.p4.1.4" class="ltx_text" style="color:#000000;"> offers faster convergence (Table </span><a href="#S3.T3" title="Table 3 ‣ 3.4 Experimental Framework ‣ 3 System Design" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.SS4.p4.1.5" class="ltx_text" style="color:#000000;">):
1) fewer client tasks have to be started because the aggregation tolerates stale updates, while FedAvg </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">McMahan et al.</span> <span id="S3.SS4.p4.1.6.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2017</span></a><span id="S3.SS4.p4.1.7.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S3.SS4.p4.1.8" class="ltx_text" style="color:#000000;"> throws away all stragglers;
2) more client tasks can be started due to the asynchronous task scheduling.
The effects of 1) and 2) are greater when the client task durations are heavy-tailed and the staleness limit is higher.</span></p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Projected training time speedup of FedBuff over FedAvg. The “client tasks started” statistic includes failed and stale tasks which are not aggregated. Client computation is the projected sum of processing time on all devices.</figcaption>
<br class="ltx_break">
<table id="S3.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.3.1.1" class="ltx_tr">
<th id="S3.T3.3.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"></th>
<th id="S3.T3.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S3.T3.3.1.1.2.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Task A</span></th>
<th id="S3.T3.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S3.T3.3.1.1.3.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Task B</span></th>
<th id="S3.T3.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S3.T3.3.1.1.4.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Task C</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.3.2.1" class="ltx_tr">
<th id="S3.T3.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S3.T3.3.2.1.1.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">FedBuff Speed-up</span></th>
<td id="S3.T3.3.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S3.T3.3.2.1.2.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">1.2x</span></td>
<td id="S3.T3.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S3.T3.3.2.1.3.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">6x</span></td>
<td id="S3.T3.3.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S3.T3.3.2.1.4.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">2x</span></td>
</tr>
<tr id="S3.T3.3.3.2" class="ltx_tr">
<th id="S3.T3.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S3.T3.3.3.2.1.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Client Tasks Started</span></th>
<td id="S3.T3.3.3.2.2" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S3.T3.3.3.2.2.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">48.8k</span></td>
<td id="S3.T3.3.3.2.3" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S3.T3.3.3.2.3.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">32.3k</span></td>
<td id="S3.T3.3.3.2.4" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S3.T3.3.3.2.4.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">610k</span></td>
</tr>
<tr id="S3.T3.3.4.3" class="ltx_tr">
<th id="S3.T3.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S3.T3.3.4.3.1.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Client Computation</span></th>
<td id="S3.T3.3.4.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S3.T3.3.4.3.2.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">7.5 hrs</span></td>
<td id="S3.T3.3.4.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S3.T3.3.4.3.3.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">6.8 days</span></td>
<td id="S3.T3.3.4.3.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S3.T3.3.4.3.4.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">25.9 days</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS4.p5" class="ltx_para ltx_noindent">
<p id="S3.SS4.p5.1" class="ltx_p"><span id="S3.SS4.p5.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Scalability and Fault Tolerance.</span><span id="S3.SS4.p5.1.2" class="ltx_text" style="color:#000000;"> Using large existing ML clusters and a familiar job management tool, developers can easily simulate millions of clients with our framework. To increase parallelism, a nuance of our proxy data generator tool from Section </span><a href="#S3.SS3" title="3.3 Proxy Data Generator ‣ 3 System Design" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">3.3</span></a><span id="S3.SS4.p5.1.3" class="ltx_text" style="color:#000000;"> is that it outputs one partition per </span><span id="S3.SS4.p5.1.4" class="ltx_text ltx_font_italic" style="color:#000000;">executor</span><span id="S3.SS4.p5.1.5" class="ltx_text" style="color:#000000;"> rather than one file per FL client; each partition contains a set of unique clients for an executor to load into memory, which speeds up the random access of client records during training. To support multi-versioned proxies with millions of clients, this strategy prevents an explosion of namespaces on the pipeline storage, which is typically HDFS or cloud blob stores. Furthermore, storing many clients’ records together in a file improves the compression ratio.
If each partition still exceeds the memory of the executor, the data can be additionally split by timestamp and swapped in and out during the simulation. This allows a cluster of 20 executors to process over 60,000 client tasks per hour for </span><span id="S3.SS4.p5.1.6" class="ltx_text ltx_font_italic" style="color:#000000;">Task C</span><span id="S3.SS4.p5.1.7" class="ltx_text" style="color:#000000;"> in Table </span><a href="#S3.T3" title="Table 3 ‣ 3.4 Experimental Framework ‣ 3 System Design" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.SS4.p5.1.8" class="ltx_text" style="color:#000000;">; the system scales horizontally and can gracefully handle millions of clients (Table </span><a href="#S3.T2" title="Table 2 ‣ 3.3 Proxy Data Generator ‣ 3 System Design" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.SS4.p5.1.9" class="ltx_text" style="color:#000000;">).</span></p>
</div>
<div id="S3.SS4.p6" class="ltx_para ltx_noindent">
<p id="S3.SS4.p6.1" class="ltx_p"><span id="S3.SS4.p6.1.1" class="ltx_text" style="color:#000000;">For very large experiments, a job could run for days on more than 100 machines. At this scale, the job needs to be fault-tolerant and self-healing. To recover from executor failures, the leader node halts dispatching tasks until all executors have pinged it with a healthy status-code. If a leader node fails, all the executors wait until it is back online to proceed polling for tasks. Since the leader frequently checkpoints the virtual time and recent model weights to the pipeline storage, any restarted leader and executor can resume from the checkpoints without losing more than one round of work.</span></p>
</div>
<div id="S3.SS4.p7" class="ltx_para ltx_noindent">
<p id="S3.SS4.p7.1" class="ltx_p"><span id="S3.SS4.p7.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Parameter Tuning.</span><span id="S3.SS4.p7.1.2" class="ltx_text" style="color:#000000;">
An FL system introduces many more parameters to tune, e.g. cohort size, asynchronous buffer size and staleness limits. For example, cohort size is a key parameter that can determine data efficiency and model convergence </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Charles et al.</span> <span id="S3.SS4.p7.1.3.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2021</span></a><span id="S3.SS4.p7.1.4.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S3.SS4.p7.1.5" class="ltx_text" style="color:#000000;">, but may have a different optimal value for each application.
However, once a model is deployed, parameter tuning should be done sparingly to responsibly leverage users’ device resources.
Additionally, our empirical results (Figure </span><a href="#S4.F10" title="Figure 10 ‣ 4.1 Advertising ‣ 4 Case Studies" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">10</span></a><span id="S3.SS4.p7.1.6" class="ltx_text" style="color:#000000;">) show that model performance under random client sampling can be unstable because clients selected in earlier rounds heavily impact a model’s final performance.
Our experimental framework runs multiple trials of each configuration to report error-bounded metrics. Though such noise can still complicate parameter tuning, parameters selected from proxy datasets can often effectively translate to real FL tasks </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Kuo et al.</span> <span id="S3.SS4.p7.1.7.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a><span id="S3.SS4.p7.1.8.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S3.SS4.p7.1.9" class="ltx_text" style="color:#000000;">.
Our framework also provides users with an understanding of the relationship between different parameters and model/system metrics.
Figure </span><a href="#S3.F7" title="Figure 7 ‣ 3.5 Forecasting System Resource ‣ 3 System Design" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">7</span></a><span id="S3.SS4.p7.1.10" class="ltx_text" style="color:#000000;"> shows the relationship between FedBuff’s buffer size parameter and estimated round duration.
Figure </span><a href="#S4.F10" title="Figure 10 ‣ 4.1 Advertising ‣ 4 Case Studies" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">10</span></a><span id="S3.SS4.p7.1.11" class="ltx_text" style="color:#000000;"> shows how learning rate schedules can affect training stability.</span></p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Forecasting System Resource</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p"><span id="S3.SS5.p1.1.1" class="ltx_text" style="color:#000000;">Besides model performance, the FL platform should forecast the overall resource needs from the cloud and user devices, helping engineers optimize the resource efficiency of the system and prevent overloading the finite device and infrastructure capacity.
This can help manage the carbon footprint of edge training jobs, since they can be less energy efficient than centralized training.
Moreover, renewable energy access at the edge is much more limited due to geographical diversity </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Wu et al.</span> <span id="S3.SS5.p1.1.2.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a><span id="S3.SS5.p1.1.3.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S3.SS5.p1.1.4" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2302.12862/assets/graphics/buffer_size_round_duration.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="236" height="148" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Buffer size settings vs time duration to populate the buffer during a sample model’s FL training with max concurrency = 180; having a realistic estimation of time during offline evaluation help modelers understand the impact of different parameters.</figcaption>
</figure>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.2" class="ltx_p"><span id="S3.SS5.p2.2.1" class="ltx_text ltx_font_bold" style="color:#000000;">Reducing Device Resource Consumption.</span><span id="S3.SS5.p2.2.2" class="ltx_text" style="color:#000000;">
In addition to cloud infrastructure costs, a device-cloud platform should account for total edge resource utilization in its notion of budget. As more FL-enabled apps begin sharing the same finite amount of device resources, imposing such a budget can incentivize teams to reduce both cloud and device resource footprint.
Centralized ML jobs typically specify the workers needed to complete the workload in a reasonable amount of time. While more workers may increase parallelism, it could reduce per-worker utilization, resulting in wasted budget. Similarly in FL, if concurrency is too high, more updates become stale and discarded (see Figure </span><a href="#S3.F8" title="Figure 8 ‣ 3.5 Forecasting System Resource ‣ 3 System Design" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">8</span></a><span id="S3.SS5.p2.2.3" class="ltx_text" style="color:#000000;">). The efficiency of an FL system can be measured with task completion, stragglers, and total device computation time. Our framework reports model performance over these variables so that parameters can be adjusted to reduce the overall user resource footprint. Due to differences in model and data complexity discussed earlier, the sample model for Task C from Table </span><a href="#S3.T3" title="Table 3 ‣ 3.4 Experimental Framework ‣ 3 System Design" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.SS5.p2.2.4" class="ltx_text" style="color:#000000;"> consumes 620 hours (25.9 days) of client compute time to converge, while the sample model from Task A only takes less than 8 hours.
The total device time is calculated as
</span><math id="S3.SS5.p2.1.m1.1" class="ltx_Math" alttext="\sum_{k}^{K}taskDuration(k)" display="inline"><semantics id="S3.SS5.p2.1.m1.1a"><mrow id="S3.SS5.p2.1.m1.1.2" xref="S3.SS5.p2.1.m1.1.2.cmml"><msubsup id="S3.SS5.p2.1.m1.1.2.1" xref="S3.SS5.p2.1.m1.1.2.1.cmml"><mo mathcolor="#000000" id="S3.SS5.p2.1.m1.1.2.1.2.2" xref="S3.SS5.p2.1.m1.1.2.1.2.2.cmml">∑</mo><mi mathcolor="#000000" id="S3.SS5.p2.1.m1.1.2.1.2.3" xref="S3.SS5.p2.1.m1.1.2.1.2.3.cmml">k</mi><mi mathcolor="#000000" id="S3.SS5.p2.1.m1.1.2.1.3" xref="S3.SS5.p2.1.m1.1.2.1.3.cmml">K</mi></msubsup><mrow id="S3.SS5.p2.1.m1.1.2.2" xref="S3.SS5.p2.1.m1.1.2.2.cmml"><mi mathcolor="#000000" id="S3.SS5.p2.1.m1.1.2.2.2" xref="S3.SS5.p2.1.m1.1.2.2.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.2.2.1" xref="S3.SS5.p2.1.m1.1.2.2.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS5.p2.1.m1.1.2.2.3" xref="S3.SS5.p2.1.m1.1.2.2.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.2.2.1a" xref="S3.SS5.p2.1.m1.1.2.2.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS5.p2.1.m1.1.2.2.4" xref="S3.SS5.p2.1.m1.1.2.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.2.2.1b" xref="S3.SS5.p2.1.m1.1.2.2.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS5.p2.1.m1.1.2.2.5" xref="S3.SS5.p2.1.m1.1.2.2.5.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.2.2.1c" xref="S3.SS5.p2.1.m1.1.2.2.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS5.p2.1.m1.1.2.2.6" xref="S3.SS5.p2.1.m1.1.2.2.6.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.2.2.1d" xref="S3.SS5.p2.1.m1.1.2.2.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS5.p2.1.m1.1.2.2.7" xref="S3.SS5.p2.1.m1.1.2.2.7.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.2.2.1e" xref="S3.SS5.p2.1.m1.1.2.2.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS5.p2.1.m1.1.2.2.8" xref="S3.SS5.p2.1.m1.1.2.2.8.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.2.2.1f" xref="S3.SS5.p2.1.m1.1.2.2.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS5.p2.1.m1.1.2.2.9" xref="S3.SS5.p2.1.m1.1.2.2.9.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.2.2.1g" xref="S3.SS5.p2.1.m1.1.2.2.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS5.p2.1.m1.1.2.2.10" xref="S3.SS5.p2.1.m1.1.2.2.10.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.2.2.1h" xref="S3.SS5.p2.1.m1.1.2.2.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS5.p2.1.m1.1.2.2.11" xref="S3.SS5.p2.1.m1.1.2.2.11.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.2.2.1i" xref="S3.SS5.p2.1.m1.1.2.2.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS5.p2.1.m1.1.2.2.12" xref="S3.SS5.p2.1.m1.1.2.2.12.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.2.2.1j" xref="S3.SS5.p2.1.m1.1.2.2.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS5.p2.1.m1.1.2.2.13" xref="S3.SS5.p2.1.m1.1.2.2.13.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p2.1.m1.1.2.2.1k" xref="S3.SS5.p2.1.m1.1.2.2.1.cmml">​</mo><mrow id="S3.SS5.p2.1.m1.1.2.2.14.2" xref="S3.SS5.p2.1.m1.1.2.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.SS5.p2.1.m1.1.2.2.14.2.1" xref="S3.SS5.p2.1.m1.1.2.2.cmml">(</mo><mi mathcolor="#000000" id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml">k</mi><mo mathcolor="#000000" stretchy="false" id="S3.SS5.p2.1.m1.1.2.2.14.2.2" xref="S3.SS5.p2.1.m1.1.2.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><apply id="S3.SS5.p2.1.m1.1.2.cmml" xref="S3.SS5.p2.1.m1.1.2"><apply id="S3.SS5.p2.1.m1.1.2.1.cmml" xref="S3.SS5.p2.1.m1.1.2.1"><csymbol cd="ambiguous" id="S3.SS5.p2.1.m1.1.2.1.1.cmml" xref="S3.SS5.p2.1.m1.1.2.1">superscript</csymbol><apply id="S3.SS5.p2.1.m1.1.2.1.2.cmml" xref="S3.SS5.p2.1.m1.1.2.1"><csymbol cd="ambiguous" id="S3.SS5.p2.1.m1.1.2.1.2.1.cmml" xref="S3.SS5.p2.1.m1.1.2.1">subscript</csymbol><sum id="S3.SS5.p2.1.m1.1.2.1.2.2.cmml" xref="S3.SS5.p2.1.m1.1.2.1.2.2"></sum><ci id="S3.SS5.p2.1.m1.1.2.1.2.3.cmml" xref="S3.SS5.p2.1.m1.1.2.1.2.3">𝑘</ci></apply><ci id="S3.SS5.p2.1.m1.1.2.1.3.cmml" xref="S3.SS5.p2.1.m1.1.2.1.3">𝐾</ci></apply><apply id="S3.SS5.p2.1.m1.1.2.2.cmml" xref="S3.SS5.p2.1.m1.1.2.2"><times id="S3.SS5.p2.1.m1.1.2.2.1.cmml" xref="S3.SS5.p2.1.m1.1.2.2.1"></times><ci id="S3.SS5.p2.1.m1.1.2.2.2.cmml" xref="S3.SS5.p2.1.m1.1.2.2.2">𝑡</ci><ci id="S3.SS5.p2.1.m1.1.2.2.3.cmml" xref="S3.SS5.p2.1.m1.1.2.2.3">𝑎</ci><ci id="S3.SS5.p2.1.m1.1.2.2.4.cmml" xref="S3.SS5.p2.1.m1.1.2.2.4">𝑠</ci><ci id="S3.SS5.p2.1.m1.1.2.2.5.cmml" xref="S3.SS5.p2.1.m1.1.2.2.5">𝑘</ci><ci id="S3.SS5.p2.1.m1.1.2.2.6.cmml" xref="S3.SS5.p2.1.m1.1.2.2.6">𝐷</ci><ci id="S3.SS5.p2.1.m1.1.2.2.7.cmml" xref="S3.SS5.p2.1.m1.1.2.2.7">𝑢</ci><ci id="S3.SS5.p2.1.m1.1.2.2.8.cmml" xref="S3.SS5.p2.1.m1.1.2.2.8">𝑟</ci><ci id="S3.SS5.p2.1.m1.1.2.2.9.cmml" xref="S3.SS5.p2.1.m1.1.2.2.9">𝑎</ci><ci id="S3.SS5.p2.1.m1.1.2.2.10.cmml" xref="S3.SS5.p2.1.m1.1.2.2.10">𝑡</ci><ci id="S3.SS5.p2.1.m1.1.2.2.11.cmml" xref="S3.SS5.p2.1.m1.1.2.2.11">𝑖</ci><ci id="S3.SS5.p2.1.m1.1.2.2.12.cmml" xref="S3.SS5.p2.1.m1.1.2.2.12">𝑜</ci><ci id="S3.SS5.p2.1.m1.1.2.2.13.cmml" xref="S3.SS5.p2.1.m1.1.2.2.13">𝑛</ci><ci id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">\sum_{k}^{K}taskDuration(k)</annotation></semantics></math><span id="S3.SS5.p2.2.5" class="ltx_text" style="color:#000000;">, where </span><math id="S3.SS5.p2.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS5.p2.2.m2.1a"><mi mathcolor="#000000" id="S3.SS5.p2.2.m2.1.1" xref="S3.SS5.p2.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.2.m2.1b"><ci id="S3.SS5.p2.2.m2.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.2.m2.1c">K</annotation></semantics></math><span id="S3.SS5.p2.2.6" class="ltx_text" style="color:#000000;"> is the sequence of clients that had performed training.
</span>
<br class="ltx_break"><span id="S3.SS5.p2.2.7" class="ltx_text" style="color:#000000;">
</span><span id="S3.SS5.p2.2.8" class="ltx_text ltx_font_bold" style="color:#000000;">Infrastructure Requirements.</span><span id="S3.SS5.p2.2.9" class="ltx_text" style="color:#000000;">
Since the trainer in a cross-device FL pipeline is online by nature and handles requests in real-time, a projection of each training job’s infrastructure needs is necessary for the modeler to ensure there are enough resources to handle their FL job throughout heavy load swings (Figure </span><a href="#S2.F2" title="Figure 2 ‣ 2.3 Challenges of Cross-Device FL ‣ 2 Background" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.SS5.p2.2.10" class="ltx_text" style="color:#000000;">).
When multiple FL applications coexist, it is likely for resource contention to occur if they share the same pool of workers for aggregation and coordination.
The training duration projected by the experimental framework helps to schedule FL workloads efficiently and prevent overloading the service workers due to task overlaps, especially when Trusted Execution Environments (TEE) with limited bandwidth </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Huba et al.</span> <span id="S3.SS5.p2.2.11.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a><span id="S3.SS5.p2.2.12.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S3.SS5.p2.2.13" class="ltx_text" style="color:#000000;"> are used for secure aggregation.
In Task C in Table </span><a href="#S3.T3" title="Table 3 ‣ 3.4 Experimental Framework ‣ 3 System Design" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.SS5.p2.2.14" class="ltx_text" style="color:#000000;">, an asynchronous setting where we assume client arrival is uniform, the model takes 48 hours to aggregate 610k tasks (3.53 updates/s).
Multiplied by the size of each gradient update (Table </span><a href="#S4.T5" title="Table 5 ‣ 4 Case Studies" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="S3.SS5.p2.2.15" class="ltx_text" style="color:#000000;">), a TEE needs to receive and aggregate only 2.68MB/second of updates. This demonstrates the framework’s ability to project cloud resource needs ahead of deployment based on factors like model size and concurrency.</span></p>
</div>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2302.12862/assets/graphics/sysmetrics.png" id="S3.F8.g1" class="ltx_graphics ltx_img_landscape" width="285" height="189" alt="Refer to caption">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Succeeded, interrupted, and stale client tasks under different concurrency and max staleness settings in FedBuff. Higher concurrency can increase both client tasks started and the amount of wasted tasks. Higher staleness tolerance can decrease stale tasks but could slow down learning with older gradients.</figcaption>
</figure>
<figure id="S3.F9" class="ltx_figure"><img src="/html/2302.12862/assets/graphics/checklist.png" id="S3.F9.g1" class="ltx_graphics ltx_img_landscape" width="673" height="154" alt="Refer to caption">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>The proposed decision workflow to analyze and bring cross-device FL into production.</figcaption>
</figure>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Privacy and Security</h3>

<div id="S3.SS6.p1" class="ltx_para ltx_noindent">
<p id="S3.SS6.p1.1" class="ltx_p"><span id="S3.SS6.p1.1.1" class="ltx_text" style="color:#000000;">Although FL greatly improves user privacy and security by leaving sensitive data on the device, achieving desired privacy properties may still require introducing additional privacy enhancing technologies (PETs) into the system </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Kairouz et al.</span> <span id="S3.SS6.p1.1.2.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2021</span></a><span id="S3.SS6.p1.1.3.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S3.SS6.p1.1.4" class="ltx_text" style="color:#000000;">.
Currently, developers/security engineers audit the system on a case-by-case basis, since each project has different risk tolerance and privacy budgets.
Our experimental framework can help developers and security experts evaluate the model and resource trade-offs of techniques like FL with differential privacy (FL-DP) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Kairouz et al.</span> <span id="S3.SS6.p1.1.5.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2021</span></a><span id="S3.SS6.p1.1.6.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S3.SS6.p1.1.7" class="ltx_text" style="color:#000000;">, secure aggregation (SecAgg) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Mo et al.</span> <span id="S3.SS6.p1.1.8.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2021</span></a><span id="S3.SS6.p1.1.9.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S3.SS6.p1.1.10" class="ltx_text" style="color:#000000;">, and robust training </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Wong et al.</span> <span id="S3.SS6.p1.1.11.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2020</span></a><span id="S3.SS6.p1.1.12.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S3.SS6.p1.1.13" class="ltx_text" style="color:#000000;"> against adversarial attacks </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Sun et al.</span> <span id="S3.SS6.p1.1.14.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2019</span></a><span id="S3.SS6.p1.1.15.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S3.SS6.p1.1.16" class="ltx_text" style="color:#000000;">. Our SecAgg uses TEEs for remote attestation </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Huba et al.</span> <span id="S3.SS6.p1.1.17.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a><span id="S3.SS6.p1.1.18.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S3.SS6.p1.1.19" class="ltx_text" style="color:#000000;">, making it compatible with async FL.</span></p>
</div>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>Decision Workflow</h3>

<div id="S3.SS7.p1" class="ltx_para ltx_noindent">
<p id="S3.SS7.p1.1" class="ltx_p"><span id="S3.SS7.p1.1.1" class="ltx_text" style="color:#000000;">A standard process to bring FL projects to life at LinkedIn simplifies many of the production ML operations (MLOps) complexities introduced by FL.
We propose a decision workflow in Figure </span><a href="#S3.F9" title="Figure 9 ‣ 3.5 Forecasting System Resource ‣ 3 System Design" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">9</span></a><span id="S3.SS7.p1.1.2" class="ltx_text" style="color:#000000;"> that uses the components of the hybrid FL platform to ensure that the important risks and challenges of each FL project are practically assessed before deployment reaches the users. This covers all aspects of the system, from understanding the client data, compute, and availability, to estimating resource impact, model performance, and privacy/security risks. The process complements the proposed FL/ML platform, leveraging the platform’s tools in each of the steps.</span></p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">4 </span>Case Studies</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text" style="color:#000000;">We apply our decision workflow on three business-critical domains: advertising, messaging, and search. We present the empirical results (Tables </span><a href="#S4.T4" title="Table 4 ‣ 4 Case Studies" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S4.p1.1.2" class="ltx_text" style="color:#000000;"> and </span><a href="#S4.T5" title="Table 5 ‣ 4 Case Studies" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="S4.p1.1.3" class="ltx_text" style="color:#000000;">), discussing the benefits, systems/performance trade-offs and newfound challenges in the context of the evaluations. Each model is at parity with the centralized model or suffers slight performance loss due to 1) FL’s constraints and 2) proxy datasets exclude some features that are only available on-device.</span></p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Projected FL training time to reach convergence for each domain’s representative model. The performance difference is the median of the FL model’s offline metric over N=15 trials compared to the centralized model. We measure ads and messaging performance with Area Under Precision-Recall Curve (AUPR), and search with Normalized Discounted Cumulative Gain (NDCG). In all cases, performance can reach an acceptable range under FL’s constraints when compared with centralized training.</figcaption>
<br class="ltx_break">
<table id="S4.T4.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.3.1.1" class="ltx_tr">
<td id="S4.T4.3.1.1.1" class="ltx_td ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"></td>
<th id="S4.T4.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S4.T4.3.1.1.2.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Ads</span></th>
<th id="S4.T4.3.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S4.T4.3.1.1.3.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Messaging</span></th>
<th id="S4.T4.3.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S4.T4.3.1.1.4.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Search</span></th>
</tr>
<tr id="S4.T4.3.2.2" class="ltx_tr">
<td id="S4.T4.3.2.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S4.T4.3.2.2.1.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Training Time</span></td>
<td id="S4.T4.3.2.2.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S4.T4.3.2.2.2.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">4.2 days</span></td>
<td id="S4.T4.3.2.2.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S4.T4.3.2.2.3.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">18.9 hrs</span></td>
<td id="S4.T4.3.2.2.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S4.T4.3.2.2.4.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">2.58 hrs</span></td>
</tr>
<tr id="S4.T4.3.3.3" class="ltx_tr">
<td id="S4.T4.3.3.3.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S4.T4.3.3.3.1.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">Performance Diff.</span></td>
<td id="S4.T4.3.3.3.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S4.T4.3.3.3.2.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">-1.85%</span></td>
<td id="S4.T4.3.3.3.3" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S4.T4.3.3.3.3.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">-0.18%</span></td>
<td id="S4.T4.3.3.3.4" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S4.T4.3.3.3.4.1" class="ltx_text ltx_font_smallcaps" style="color:#000000;">-1.64%</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption" style="color:#000000;"><span class="ltx_tag ltx_tag_table">Table 5: </span>On-device evaluation of device-capable model architectures selected to represent common ML tasks at LinkedIn. We report mean training times and CPU utilization % for each model over 5,000 records, aggregated across 27 devices with diverse hardware.</figcaption>
<table id="S4.T5.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.3.1.1" class="ltx_tr">
<th id="S4.T5.3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.1.1.1.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T5.3.1.1.1.1.1.1" class="ltx_text" style="color:#000000;">Model</span></span>
</span>
</th>
<th id="S4.T5.3.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.1.1.2.1.1" class="ltx_p" style="width:125.2pt;"><span id="S4.T5.3.1.1.2.1.1.1" class="ltx_text" style="color:#000000;">Description</span></span>
</span>
</th>
<th id="S4.T5.3.1.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.1.1.3.1.1" class="ltx_p"><span id="S4.T5.3.1.1.3.1.1.1" class="ltx_text" style="color:#000000;">Trainable Params</span></span>
</span>
</th>
<th id="S4.T5.3.1.1.4" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.1.1.4.1.1" class="ltx_p"><span id="S4.T5.3.1.1.4.1.1.1" class="ltx_text" style="color:#000000;">Storage (MB)</span></span>
</span>
</th>
<th id="S4.T5.3.1.1.5" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.1.1.5.1.1" class="ltx_p"><span id="S4.T5.3.1.1.5.1.1.1" class="ltx_text" style="color:#000000;">Network (MB)</span></span>
</span>
</th>
<th id="S4.T5.3.1.1.6" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.1.1.6.1.1" class="ltx_p"><span id="S4.T5.3.1.1.6.1.1.1" class="ltx_text" style="color:#000000;">Memory (MB)</span></span>
</span>
</th>
<th id="S4.T5.3.1.1.7" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.1.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.1.1.7.1.1" class="ltx_p"><span id="S4.T5.3.1.1.7.1.1.1" class="ltx_text" style="color:#000000;">Mean Time (s)</span></span>
</span>
</th>
<th id="S4.T5.3.1.1.8" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.1.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.1.1.8.1.1" class="ltx_p"><span id="S4.T5.3.1.1.8.1.1.1" class="ltx_text" style="color:#000000;">Stdev Time (s)</span></span>
</span>
</th>
<th id="S4.T5.3.1.1.9" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.1.1.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.1.1.9.1.1" class="ltx_p"><span id="S4.T5.3.1.1.9.1.1.1" class="ltx_text" style="color:#000000;">Mean CPU (%)</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.3.2.1" class="ltx_tr">
<td id="S4.T5.3.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.2.1.1.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T5.3.2.1.1.1.1.1" class="ltx_text ltx_font_italic" style="color:#000000;">A</span></span>
</span>
</td>
<td id="S4.T5.3.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.2.1.2.1.1" class="ltx_p" style="width:125.2pt;"><span id="S4.T5.3.2.1.2.1.1.1" class="ltx_text" style="color:#000000;">Tiny Neural Net</span></span>
</span>
</td>
<td id="S4.T5.3.2.1.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.2.1.3.1.1" class="ltx_p"><span id="S4.T5.3.2.1.3.1.1.1" class="ltx_text" style="color:#000000;">1.51k</span></span>
</span>
</td>
<td id="S4.T5.3.2.1.4" class="ltx_td ltx_align_justify ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.2.1.4.1.1" class="ltx_p"><span id="S4.T5.3.2.1.4.1.1.1" class="ltx_text" style="color:#000000;">0.057</span></span>
</span>
</td>
<td id="S4.T5.3.2.1.5" class="ltx_td ltx_align_justify ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.2.1.5.1.1" class="ltx_p"><span id="S4.T5.3.2.1.5.1.1.1" class="ltx_text" style="color:#000000;">0.11</span></span>
</span>
</td>
<td id="S4.T5.3.2.1.6" class="ltx_td ltx_align_justify ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.2.1.6.1.1" class="ltx_p"><span id="S4.T5.3.2.1.6.1.1.1" class="ltx_text" style="color:#000000;">3.08</span></span>
</span>
</td>
<td id="S4.T5.3.2.1.7" class="ltx_td ltx_align_justify ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.2.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.2.1.7.1.1" class="ltx_p"><span id="S4.T5.3.2.1.7.1.1.1" class="ltx_text" style="color:#000000;">4.98</span></span>
</span>
</td>
<td id="S4.T5.3.2.1.8" class="ltx_td ltx_align_justify ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.2.1.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.2.1.8.1.1" class="ltx_p"><span id="S4.T5.3.2.1.8.1.1.1" class="ltx_text" style="color:#000000;">3.37</span></span>
</span>
</td>
<td id="S4.T5.3.2.1.9" class="ltx_td ltx_align_justify ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.2.1.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.2.1.9.1.1" class="ltx_p"><span id="S4.T5.3.2.1.9.1.1.1" class="ltx_text" style="color:#000000;">1.63</span></span>
</span>
</td>
</tr>
<tr id="S4.T5.3.3.2" class="ltx_tr">
<td id="S4.T5.3.3.2.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.2.1.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T5.3.3.2.1.1.1.1" class="ltx_text ltx_font_italic" style="color:#000000;">B</span></span>
</span>
</td>
<td id="S4.T5.3.3.2.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.2.2.1.1" class="ltx_p" style="width:125.2pt;"><span id="S4.T5.3.3.2.2.1.1.1" class="ltx_text" style="color:#000000;">MLP w/ sparse features</span></span>
</span>
</td>
<td id="S4.T5.3.3.2.3" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.2.3.1.1" class="ltx_p"><span id="S4.T5.3.3.2.3.1.1.1" class="ltx_text" style="color:#000000;">189k</span></span>
</span>
</td>
<td id="S4.T5.3.3.2.4" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.2.4.1.1" class="ltx_p"><span id="S4.T5.3.3.2.4.1.1.1" class="ltx_text" style="color:#000000;">0.76</span></span>
</span>
</td>
<td id="S4.T5.3.3.2.5" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.2.5.1.1" class="ltx_p"><span id="S4.T5.3.3.2.5.1.1.1" class="ltx_text" style="color:#000000;">1.52</span></span>
</span>
</td>
<td id="S4.T5.3.3.2.6" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.2.6.1.1" class="ltx_p"><span id="S4.T5.3.3.2.6.1.1.1" class="ltx_text" style="color:#000000;">10.64</span></span>
</span>
</td>
<td id="S4.T5.3.3.2.7" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.3.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.2.7.1.1" class="ltx_p"><span id="S4.T5.3.3.2.7.1.1.1" class="ltx_text" style="color:#000000;">61.81</span></span>
</span>
</td>
<td id="S4.T5.3.3.2.8" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.3.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.2.8.1.1" class="ltx_p"><span id="S4.T5.3.3.2.8.1.1.1" class="ltx_text" style="color:#000000;">44.17</span></span>
</span>
</td>
<td id="S4.T5.3.3.2.9" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.3.2.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.3.2.9.1.1" class="ltx_p"><span id="S4.T5.3.3.2.9.1.1.1" class="ltx_text" style="color:#000000;">3.91</span></span>
</span>
</td>
</tr>
<tr id="S4.T5.3.4.3" class="ltx_tr">
<td id="S4.T5.3.4.3.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.4.3.1.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T5.3.4.3.1.1.1.1" class="ltx_text ltx_font_italic" style="color:#000000;">C</span></span>
</span>
</td>
<td id="S4.T5.3.4.3.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.4.3.2.1.1" class="ltx_p" style="width:125.2pt;"><span id="S4.T5.3.4.3.2.1.1.1" class="ltx_text" style="color:#000000;">MLP w/ medium embedding</span></span>
</span>
</td>
<td id="S4.T5.3.4.3.3" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.4.3.3.1.1" class="ltx_p"><span id="S4.T5.3.4.3.3.1.1.1" class="ltx_text" style="color:#000000;">208k</span></span>
</span>
</td>
<td id="S4.T5.3.4.3.4" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.4.3.4.1.1" class="ltx_p"><span id="S4.T5.3.4.3.4.1.1.1" class="ltx_text" style="color:#000000;">0.85</span></span>
</span>
</td>
<td id="S4.T5.3.4.3.5" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.4.3.5.1.1" class="ltx_p"><span id="S4.T5.3.4.3.5.1.1.1" class="ltx_text" style="color:#000000;">1.88</span></span>
</span>
</td>
<td id="S4.T5.3.4.3.6" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.4.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.4.3.6.1.1" class="ltx_p"><span id="S4.T5.3.4.3.6.1.1.1" class="ltx_text" style="color:#000000;">0.85</span></span>
</span>
</td>
<td id="S4.T5.3.4.3.7" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.4.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.4.3.7.1.1" class="ltx_p"><span id="S4.T5.3.4.3.7.1.1.1" class="ltx_text" style="color:#000000;">3.26</span></span>
</span>
</td>
<td id="S4.T5.3.4.3.8" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.4.3.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.4.3.8.1.1" class="ltx_p"><span id="S4.T5.3.4.3.8.1.1.1" class="ltx_text" style="color:#000000;">2.23</span></span>
</span>
</td>
<td id="S4.T5.3.4.3.9" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.4.3.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.4.3.9.1.1" class="ltx_p"><span id="S4.T5.3.4.3.9.1.1.1" class="ltx_text" style="color:#000000;">5.29</span></span>
</span>
</td>
</tr>
<tr id="S4.T5.3.5.4" class="ltx_tr">
<td id="S4.T5.3.5.4.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.5.4.1.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T5.3.5.4.1.1.1.1" class="ltx_text ltx_font_italic" style="color:#000000;">D</span></span>
</span>
</td>
<td id="S4.T5.3.5.4.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.5.4.2.1.1" class="ltx_p" style="width:125.2pt;"><span id="S4.T5.3.5.4.2.1.1.1" class="ltx_text" style="color:#000000;">CNN w/ large embedding</span></span>
</span>
</td>
<td id="S4.T5.3.5.4.3" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.5.4.3.1.1" class="ltx_p"><span id="S4.T5.3.5.4.3.1.1.1" class="ltx_text" style="color:#000000;">390k</span></span>
</span>
</td>
<td id="S4.T5.3.5.4.4" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.5.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.5.4.4.1.1" class="ltx_p"><span id="S4.T5.3.5.4.4.1.1.1" class="ltx_text" style="color:#000000;">10.79</span></span>
</span>
</td>
<td id="S4.T5.3.5.4.5" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.5.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.5.4.5.1.1" class="ltx_p"><span id="S4.T5.3.5.4.5.1.1.1" class="ltx_text" style="color:#000000;">3.12</span></span>
</span>
</td>
<td id="S4.T5.3.5.4.6" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.5.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.5.4.6.1.1" class="ltx_p"><span id="S4.T5.3.5.4.6.1.1.1" class="ltx_text" style="color:#000000;">8.37</span></span>
</span>
</td>
<td id="S4.T5.3.5.4.7" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.5.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.5.4.7.1.1" class="ltx_p"><span id="S4.T5.3.5.4.7.1.1.1" class="ltx_text" style="color:#000000;">70.13</span></span>
</span>
</td>
<td id="S4.T5.3.5.4.8" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.5.4.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.5.4.8.1.1" class="ltx_p"><span id="S4.T5.3.5.4.8.1.1.1" class="ltx_text" style="color:#000000;">50.82</span></span>
</span>
</td>
<td id="S4.T5.3.5.4.9" class="ltx_td ltx_align_justify" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.5.4.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.5.4.9.1.1" class="ltx_p"><span id="S4.T5.3.5.4.9.1.1.1" class="ltx_text" style="color:#000000;">4.72</span></span>
</span>
</td>
</tr>
<tr id="S4.T5.3.6.5" class="ltx_tr">
<td id="S4.T5.3.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.6.5.1.1.1" class="ltx_p" style="width:34.1pt;"><span id="S4.T5.3.6.5.1.1.1.1" class="ltx_text ltx_font_italic" style="color:#000000;">E</span></span>
</span>
</td>
<td id="S4.T5.3.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.6.5.2.1.1" class="ltx_p" style="width:125.2pt;"><span id="S4.T5.3.6.5.2.1.1.1" class="ltx_text" style="color:#000000;">Multi-task MLP</span></span>
</span>
</td>
<td id="S4.T5.3.6.5.3" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.6.5.3.1.1" class="ltx_p"><span id="S4.T5.3.6.5.3.1.1.1" class="ltx_text" style="color:#000000;">922k</span></span>
</span>
</td>
<td id="S4.T5.3.6.5.4" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.6.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.6.5.4.1.1" class="ltx_p"><span id="S4.T5.3.6.5.4.1.1.1" class="ltx_text" style="color:#000000;">7.52</span></span>
</span>
</td>
<td id="S4.T5.3.6.5.5" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.6.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.6.5.5.1.1" class="ltx_p"><span id="S4.T5.3.6.5.5.1.1.1" class="ltx_text" style="color:#000000;">7.38</span></span>
</span>
</td>
<td id="S4.T5.3.6.5.6" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.6.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.6.5.6.1.1" class="ltx_p"><span id="S4.T5.3.6.5.6.1.1.1" class="ltx_text" style="color:#000000;">43.14</span></span>
</span>
</td>
<td id="S4.T5.3.6.5.7" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.6.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.6.5.7.1.1" class="ltx_p"><span id="S4.T5.3.6.5.7.1.1.1" class="ltx_text" style="color:#000000;">238.38</span></span>
</span>
</td>
<td id="S4.T5.3.6.5.8" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.6.5.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.6.5.8.1.1" class="ltx_p"><span id="S4.T5.3.6.5.8.1.1.1" class="ltx_text" style="color:#000000;">178.13</span></span>
</span>
</td>
<td id="S4.T5.3.6.5.9" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-left:3.1pt;padding-right:3.1pt;">
<span id="S4.T5.3.6.5.9.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.3.6.5.9.1.1" class="ltx_p"><span id="S4.T5.3.6.5.9.1.1.1" class="ltx_text" style="color:#000000;">6.43</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Advertising</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text" style="color:#000000;">Privacy in machine learning has received significant attention in recent years.
Traditional machine learning in the digital advertising industry relies on collecting user data for measurement, targeting, and click/conversion predictions.
By reducing sensitive data tracking, cross-device FL enables private model training that can improve advertising quality, member trust and safety.
In this section, we describe the detailed steps we took to evaluate FL on an advertising use case, and the results that demonstrate the potential of moving to cross-device FL while revealing several practical challenges.
</span></p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Client Participation and Availability</span><span id="S4.SS1.p2.1.2" class="ltx_text" style="color:#000000;">. First, we define our client participation criteria: (a) app is open in the foreground, (b) battery level </span><math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mo mathcolor="#000000" id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><gt id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">&gt;</annotation></semantics></math><span id="S4.SS1.p2.1.3" class="ltx_text" style="color:#000000;"> 80%, and (c) connected to WiFi.
Our criteria are designed to be conservative, choosing to err on the side of classifying a device as not FL-ready when in doubt.
For example, we require (a) because if for some reason CPU or battery usage spikes when the app is in the background, a phone OS could choose to kill our training process. To eliminate this possibility altogether, we do not count any app background time as time we can use for FL. We then use these filters to generate device availability traces.
We query for two weeks of anonymized session data from the LinkedIn app, since usage tends to exhibit weekly periodicity.
Short gaps where the app is in the background are subtracted from the availability session duration, whereas longer gaps split a session into two.
Since we only have battery level and WiFi connectivity data for a smaller subset of mobile usage, we calculate empirical probabilities of WiFi connection and high battery level over time (Table </span><a href="#S3.T1" title="Table 1 ‣ 3.2 Real World Measurements ‣ 3 System Design" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.SS1.p2.1.4" class="ltx_text" style="color:#000000;">). For each session from our query, we perform a weighted coin-flip based on the session’s start time to decide whether to include or exclude it from the output device traces.</span></p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Building a Proxy Dataset.</span><span id="S4.SS1.p3.1.2" class="ltx_text" style="color:#000000;"> Next, we use a centralized dataset in advertising that is down-sampled on a client level to preserve the natural quantity and label skew. We then analyze the feature locality of the data to move it into an on-device setting. In this domain, a candidate is typically a potential advertisement to display or an targeting-segment that is scored in the context of the user. This application retrieves 184 candidates in a single request on average from the server, which includes some server-side features. Afterwards, each candidate is decorated with client-side features and similarity scores are calculated. To create a proxy dataset, we create a client id field based on the member id, and map each unique id to an integer for further anonymization.
Then, we run a Spark job that groups the examples by client ids and computes inter-client statistics. Through this analysis, we find that client data is non-IID and extremely tail-heavy due to users engaging disproportionately more on the app (std. of 667, and max of 39,731 records).</span></p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.2" class="ltx_p"><span id="S4.SS1.p4.2.1" class="ltx_text ltx_font_bold" style="color:#000000;">Selecting a Mobile-ready Model.</span><span id="S4.SS1.p4.2.2" class="ltx_text" style="color:#000000;"> Next, we analyze three model architectures that are tested in a centralized setting. Models that need to be deployed in third-party apps via an SDK have stricter size requirements (</span><math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mo mathcolor="#000000" id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><lt id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">&lt;</annotation></semantics></math><span id="S4.SS1.p4.2.3" class="ltx_text" style="color:#000000;">1MB), while critical models in the first-party app have looser storage constraints (</span><math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><mo mathcolor="#000000" id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><lt id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">&lt;</annotation></semantics></math><span id="S4.SS1.p4.2.4" class="ltx_text" style="color:#000000;">10MB). Thoroughly evaluating resource footprint requires taking measurements on device, since model complexity alone is not a good predictor (compare the resource footprints between Models </span><span id="S4.SS1.p4.2.5" class="ltx_text ltx_font_italic" style="color:#000000;">A</span><span id="S4.SS1.p4.2.6" class="ltx_text" style="color:#000000;">, </span><span id="S4.SS1.p4.2.7" class="ltx_text ltx_font_italic" style="color:#000000;">B</span><span id="S4.SS1.p4.2.8" class="ltx_text" style="color:#000000;">, and </span><span id="S4.SS1.p4.2.9" class="ltx_text ltx_font_italic" style="color:#000000;">C</span><span id="S4.SS1.p4.2.10" class="ltx_text" style="color:#000000;"> in Table </span><a href="#S4.T5" title="Table 5 ‣ 4 Case Studies" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="S4.SS1.p4.2.11" class="ltx_text" style="color:#000000;">). We convert our three candidate models to a TFLite format, and deploy them for training on dummy data to 27 different devices on AWS Device Farm in our benchmarking app. Out of the three architectures, we picked the model that satisfies the size requirement mentioned earlier at 0.76MB and consumed the least network and memory. This also helps us validate that the ops bundled with the ML runtime are sufficient to execute the model training.</span></p>
</div>
<div id="S4.SS1.p5" class="ltx_para ltx_noindent">
<p id="S4.SS1.p5.1" class="ltx_p"><span id="S4.SS1.p5.1.1" class="ltx_text" style="color:#000000;">While we observe that the model training footprint is acceptable, model assets may pose a challenge. During data processing, the feature transformer must map more than 70% of its features from categorical values to unique indices through vocabulary files during the data pre-processing step.
Though these mappings work well in a centralized setting, the device must refresh and store vocab files as assets, which could be as big as 1.28MB for high-cardinality variables.
To overcome the memory and disk constraints, feature hashing </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Weinberger et al.</span> <span id="S4.SS1.p5.1.2.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2009</span></a><span id="S4.SS1.p5.1.3.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S4.SS1.p5.1.4" class="ltx_text" style="color:#000000;"> can perform the mapping through a hash function, trading less storage space with lower predictive power (due to hash collisions).</span></p>
</div>
<div id="S4.SS1.p6" class="ltx_para ltx_noindent">
<p id="S4.SS1.p6.1" class="ltx_p"><span id="S4.SS1.p6.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Systems and Model Performance.</span><span id="S4.SS1.p6.1.2" class="ltx_text" style="color:#000000;"> Next, we partition the proxy dataset for 20 workers by client id in a round-robin fashion to enable faster job execution time in a cluster. This number is picked roughly based on the total data size divided by the memory available per worker. Our job config specifies the device traces, on-device performance distributions produced earlier, and other hyper-parameters to realistically evaluate the FL training. Shown in Figure </span><a href="#S4.F10" title="Figure 10 ‣ 4.1 Advertising ‣ 4 Case Studies" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">10</span></a><span id="S4.SS1.p6.1.3" class="ltx_text" style="color:#000000;">, model performance under random client sampling can be highly variable due to data heterogeneity, as the clients selected in the earlier rounds can determine the model’s convergence. We use such experiments to tune parameters (such as the learning rate schedule in Figure </span><a href="#S4.F10" title="Figure 10 ‣ 4.1 Advertising ‣ 4 Case Studies" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">10</span></a><span id="S4.SS1.p6.1.4" class="ltx_text" style="color:#000000;">) before production deployment. With the scalability of the framework, we repeat each trial 5 times to estimate an error bound. We decide that the projected training time of 4.2 days is an acceptable SLA when FedBuff async training is enabled, as the centralized counterpart only needs to be retrained weekly. The performance difference in Table </span><a href="#S4.T4" title="Table 4 ‣ 4 Case Studies" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S4.SS1.p6.1.5" class="ltx_text" style="color:#000000;"> is also acceptable; since this on-device deployment helps meet critical compliance and regulation requirements in the ads industry, there is a higher tolerance for accuracy degradation (up to 5%). Moreover, the proxy dataset is only a subset of all the signals that can be consumed on device; hence it’s a worst-case estimation.</span></p>
</div>
<figure id="S4.F10" class="ltx_figure"><img src="/html/2302.12862/assets/graphics/lan_aupr.png" id="S4.F10.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="275" height="150" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>AUPR of an example model trained using two different exponential decay LR schedules on N=5 trials each. This shows a good learning rate schedule can improve training stability.</figcaption>
</figure>
<div id="S4.SS1.p7" class="ltx_para ltx_noindent">
<p id="S4.SS1.p7.1" class="ltx_p"><span id="S4.SS1.p7.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Security and Privacy.</span><span id="S4.SS1.p7.1.2" class="ltx_text" style="color:#000000;"> Transitioning from a centralized setting where signals are collected, the data minimization already greatly improves the product’s privacy budget without any additional PETs. Nonetheless, we project the data transfer bandwidth needed from a TEE is under 3MB/s, which is sufficiently within the limit. From the security evaluation of this case study, in which the model is distributed via an SDK, we identify a new attack scenario if it is possible for the SDK’s host application to control a significant portion of the FL participants, hence poisoning the data or updates of a group of clients. This unique hub-and-spoke setup prompts further security research on detection and defenses.</span></p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Messaging</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text" style="color:#000000;">Consumer messaging applications often contain highly confidential data and are encrypted end-to-end. This poses restrictions on the data that on-premise ML tasks like abuse detection and smart-inbox features could use. Cross-device FL enables message data to be used for training in its original state on the device. To create a proxy dataset without data decryption, we partition a dataset of synthetic messages used for centralized training. The FL training achieves a promising performance compared to the centralized training, with only a 0.18% difference in the test metrics. This difference is negligible given the improved freshness of the training data, which helps the global model quickly adapt to user feedback. Lastly, the evaluation process helps us identify practical on-device challenges in this domain.</span></p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Size of Text Embeddings.</span><span id="S4.SS2.p2.1.2" class="ltx_text" style="color:#000000;">
Large mobile apps can discourage downloads and increase uninstalls.
Many deep NLP models contain word embedding tables to map text tokens into fixed-size embeddings that are fed into the rest of the layers. One of our centralized models in the messaging domain initially has a 150 million parameter embedding layer greater than 500MB, prohibiting on-device deployment.
Reducing the vocabulary from 500K words to 50k and the embedding dimension from 300 to 50 leads to a 60-fold size decrease, fitting the 10MB size constraint. Other solutions include embedding compression methods like TT-Rec </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Yin et al.</span> <span id="S4.SS2.p2.1.3.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2021</span></a><span id="S4.SS2.p2.1.4.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S4.SS2.p2.1.5" class="ltx_text" style="color:#000000;"> or MEmCom </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Pansare et al.</span> <span id="S4.SS2.p2.1.6.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a><span id="S4.SS2.p2.1.7.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S4.SS2.p2.1.8" class="ltx_text" style="color:#000000;">. Finally, the application can bundle a text embedding that’s shared by NLP models in different domains (search, recommendations, etc.), and download a smaller language-specific subset of the corpus based on the user’s language.</span></p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Security.</span><span id="S4.SS2.p3.1.2" class="ltx_text" style="color:#000000;">
Evasion attacks involve adversaries carefully crafting samples fed into the model to change the inference result, presenting a practical concern for message abuse and scam detection models during inference time.
Access to the model is especially a concern if a bad actor could decrypt the weights stored on the device. Existing defenses involve robust training, but generating adversarial examples during training can be expensive, </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Wong et al.</span> <span id="S4.SS2.p3.1.3.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2020</span></a><span id="S4.SS2.p3.1.4.2.2.1" class="ltx_text" style="color:#000000;">)</span>; <span class="ltx_text" style="color:#000000;">Hao et al.</span> <span id="S4.SS2.p3.1.3.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a><span id="S4.SS2.p3.1.4.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S4.SS2.p3.1.5" class="ltx_text" style="color:#000000;"> even more so on the device. This introduces a tradeoff between model robustness and resource consumption.
Data poisoning attacks are another concern when enough users coordinate to generate fake messages and corresponding actions, though this usually requires adversaries controlling an impractical portion of the population </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Shejwalkar et al.</span> <span id="S4.SS2.p3.1.6.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a><span id="S4.SS2.p3.1.7.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S4.SS2.p3.1.8" class="ltx_text" style="color:#000000;">.
Using the FLINT platform, our decision workflow enables evaluating new mitigation strategies; for instance, a more robust client selection criteria that incorporates the user’s reputation score and account age, or continuous FL training to adapt recent user feedback.</span></p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Search</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text" style="color:#000000;">In industry, FL has been used in browser URL bar suggestions by locally training on private browsing history </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Hartmann et al.</span> <span id="S4.SS3.p1.1.2.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2019</span></a><span id="S4.SS3.p1.1.3.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S4.SS3.p1.1.4" class="ltx_text" style="color:#000000;"> and ranking keyboard suggestions </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Hard et al.</span> <span id="S4.SS3.p1.1.5.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2018a</span></a><span id="S4.SS3.p1.1.6.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S4.SS3.p1.1.7" class="ltx_text" style="color:#000000;">.
Naturally, training ranking tasks on device allows directly using the displayed candidates and user feedback to generate training data directly on the device.
At LinkedIn, most production search workflows are bounded by strict latency budgets in the sub-100ms range </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Guo et al.</span> <span id="S4.SS3.p1.1.8.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2021</span></a><span id="S4.SS3.p1.1.9.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S4.SS3.p1.1.10" class="ltx_text" style="color:#000000;">. Query autosuggestion and completion require instant predictions to feel responsive; search ranking models need regular retraining to reflect search trends.
On-device ML has the potential to improve model freshness and reduce inference latency.
In ranking tasks, the application can locally cache, retrieve, and rank frequent documents without any network communication. For language generation tasks like query completion, locally-trained LSTMs can generate more personalized search suggestions using partial queries.</span></p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text" style="color:#000000;">Our evaluation of a low-latency model in the search domain shows a performance difference of only 1.64% (Table </span><a href="#S4.T4" title="Table 4 ‣ 4 Case Studies" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S4.SS3.p2.1.2" class="ltx_text" style="color:#000000;">) when trained on FL under realistic system constraints, with minimal device resource usage.
Moreover, FL training can reduce the resources needed to store/ETL data and regularly retrain models in data centers.
However, similar to advertising, training data in search can have a very high quantity skew because of “superusers”.</span></p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Systems for Federated Learning.</span><span id="S5.p1.1.2" class="ltx_text" style="color:#000000;">
Several large-scale cross-device FL systems have been proposed in recent years, most notably Google’s GFL system </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Bonawitz et al.</span> <span id="S5.p1.1.3.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2019</span></a><span id="S5.p1.1.4.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S5.p1.1.5" class="ltx_text" style="color:#000000;">, Apple’s FL system </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Paulik et al.</span> <span id="S5.p1.1.6.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2021a</span></a><span id="S5.p1.1.7.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S5.p1.1.8" class="ltx_text" style="color:#000000;">, and Meta’s PAPAYA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Huba et al.</span> <span id="S5.p1.1.9.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a><span id="S5.p1.1.10.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S5.p1.1.11" class="ltx_text" style="color:#000000;">.
The designs of our service and client run-time draw inspiration from all of them. Our design echoes PAPAYA by supporting both sync and async, selecting clients based on demand by active tasks.
Our sync mode is similar to GFL’s round-based design and uses client over-commitment to handle dropouts.</span></p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Evaluation Frameworks.</span><span id="S5.p2.1.2" class="ltx_text" style="color:#000000;">
To build the experimental framework, which to the best of our knowledge is first described in this paper, we considered many existing open-source FL toolkits that provide simulation capability, e.g., TFF </span><cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">Bonawitz et al.</span> </a></cite><span id="S5.p2.1.5" class="ltx_text" style="color:#000000;">, FLUTE </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Dimitriadis et al.</span> <span id="S5.p2.1.6.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a><span id="S5.p2.1.7.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S5.p2.1.8" class="ltx_text" style="color:#000000;">, Flower </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Beutel et al.</span> <span id="S5.p2.1.9.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a><span id="S5.p2.1.10.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S5.p2.1.11" class="ltx_text" style="color:#000000;">, and FedML </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">He et al.</span> <span id="S5.p2.1.12.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2020</span></a><span id="S5.p2.1.13.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S5.p2.1.14" class="ltx_text" style="color:#000000;">.
While they provide a variety of models, datasets, and algorithms for benchmarks, they report results over communication rounds.
Our design expands on FedScale </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Lai et al.</span> <span id="S5.p2.1.15.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a><span id="S5.p2.1.16.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S5.p2.1.17" class="ltx_text" style="color:#000000;">, reporting both model and system metrics reported over a virtual time and communication rounds to account for model complexity, device availability, compute and network, etc.</span></p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">FL Benchmarks.</span><span id="S5.p3.1.2" class="ltx_text" style="color:#000000;">
Many popular cross-device FL benchmarks </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Caldas et al.</span> <span id="S5.p3.1.3.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2018</span></a><span id="S5.p3.1.4.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S5.p3.1.5" class="ltx_text" style="color:#000000;"> are focused on CV and NLP tasks (FEMNIST, CIFAR10, Reddit, Shakespeare etc.), and have helped drive FL research and algorithmic improvements in the recent years. In general, our work prompts the design of more tabular FL datasets with sparse features, noisy and imbalanced labels, and heavy data quantity skew.
This is representative of in-app user behavior in the wild, where data is often scarce and noisy, and superusers dominate. An equally important consideration for realistic benchmarks is whether the models that are benchmarked can be deployed to lower-end user devices (and be small enough to co-exist with other models in a mobile app).
We suggest researchers report a measure of model size and on-device resource usage with the benchmarks of FL models.</span></p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text" style="color:#000000;">The open-source benchmarks implemented in FedScale </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Lai et al.</span> <span id="S5.p4.1.2.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a><span id="S5.p4.1.3.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S5.p4.1.4" class="ltx_text" style="color:#000000;"> are close proxies for our case studies given the naturally-partitioned datasets. The Taobao Ad Display / Click dataset (label ratio: 5.4%, clients: 1.1 mil, mean: 23, std: 65) is a good proxy for our advertising scenario because it captures the scarcity of user response. Models B and C from Table </span><a href="#S4.T5" title="Table 5 ‣ 4 Case Studies" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="S5.p4.1.5" class="ltx_text" style="color:#000000;"> fit in the ballpark in terms of architecture, size and performance requirements. For message classification tasks, the Amazon Review dataset is a good proxy (clients: 256,059, mean: 2.2, std: 4.4). Section 4.2 describes the model architecture deployed in a typical message classification task. As for next-word query prediction in search, we believe there are already mature benchmarks such as Stackoverflow and Reddit, modeled by on-device LSTMs </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Hard et al.</span> <span id="S5.p4.1.6.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2018b</span></a><span id="S5.p4.1.7.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S5.p4.1.8" class="ltx_text" style="color:#000000;">. In search ranking, there is a gap for a federated learning-to-rank dataset with a natural partitioning. Lastly, FedScale incorporates device availability traces from </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Yang et al.</span> <span id="S5.p4.1.9.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2021</span></a><span id="S5.p4.1.10.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S5.p4.1.11" class="ltx_text" style="color:#000000;">, which captures a similar weekly fluctuation pattern with a difference of 4x between peak and low, given the device is plugged-in and idle. Our availability in Figure </span><a href="#S2.F2" title="Figure 2 ‣ 2.3 Challenges of Cross-Device FL ‣ 2 Background" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S5.p4.1.12" class="ltx_text" style="color:#000000;"> fluctuates by a factor of 14x due to strict participation requirements and geographically-based usage patterns, serving as an upper-bound. The device traces an be re-sampled and adjusted based on the deployment scenario.</span></p>
</div>
<div id="S5.p5" class="ltx_para ltx_noindent">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">ML Platforms.</span><span id="S5.p5.1.2" class="ltx_text" style="color:#000000;">
The learning algorithm itself is only one component of an ML platform </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Sparks et al.</span> <span id="S5.p5.1.3.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2017</span></a><span id="S5.p5.1.4.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S5.p5.1.5" class="ltx_text" style="color:#000000;">. Systematically deploying ML in production has received large attention over the past decade with many available MLOps and platform solutions </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Baylor et al.</span> <span id="S5.p5.1.6.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2017</span></a><span id="S5.p5.1.7.2.2.1" class="ltx_text" style="color:#000000;">)</span>; <span class="ltx_text" style="color:#000000;">Zaharia et al.</span> <span id="S5.p5.1.6.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2018</span></a><span id="S5.p5.1.7.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S5.p5.1.8" class="ltx_text" style="color:#000000;"> because gluing together disjoint components may do a job once, but often leads to significant technical debt </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Sculley et al.</span> <span id="S5.p5.1.9.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2015</span></a><span id="S5.p5.1.10.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S5.p5.1.11" class="ltx_text" style="color:#000000;">. With increasing data and model sizes, many parallelism techniques require the orchestration of distributed systems </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Sergeev &amp; Del Balso</span> <span id="S5.p5.1.12.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2018</span></a><span id="S5.p5.1.13.2.2.1" class="ltx_text" style="color:#000000;">)</span>; <span class="ltx_text" style="color:#000000;">Moritz et al.</span> <span id="S5.p5.1.12.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2018</span></a><span id="S5.p5.1.13.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S5.p5.1.14" class="ltx_text" style="color:#000000;">.
Lastly, ML platforms need to be user-friendly via automation and declarative solutions so that even non-experts can leverage ML </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Kraska et al.</span> <span id="S5.p5.1.15.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2013</span></a><span id="S5.p5.1.16.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S5.p5.1.17" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="S5.p6" class="ltx_para ltx_noindent">
<p id="S5.p6.1" class="ltx_p"><span id="S5.p6.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">FL Platforms.</span><span id="S5.p6.1.2" class="ltx_text" style="color:#000000;"> The concept of device-cloud collaborative ML platforms is not new. Alibaba’s Wall-E </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" style="color:#000000;">Lv et al.</span> <span id="S5.p6.1.3.1.1.1" class="ltx_text" style="color:#000000;">(</span><a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="color:#000000;">2022</span></a><span id="S5.p6.1.4.2.2.1" class="ltx_text" style="color:#000000;">)</span></cite><span id="S5.p6.1.5" class="ltx_text" style="color:#000000;"> provides a deployment platform and high-performance mobile compute runtime for on-device tasks. To the best of our knowledge, FLINT is the first to fill the important gaps to allow an effective coexistence of centralized and on-device ML applications. We believe such a platform should perform all the tasks of an ML platform while providing the tools to analyze and make decisions based on the systems and data challenges inherent to FL.</span></p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">
<span class="ltx_tag ltx_tag_section">6 </span>Concluding Remarks</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p"><span id="S6.p1.1.1" class="ltx_text" style="color:#000000;">As shown by our evaluations of three business-critical ML applications, a cloud-device collaborative FL platform can help ML developers and decision makers practically assess the systems constraints, costs, and benefits of production FL projects.
Leveraging the platform, a systematic decision workflow can help teams responsibly bring FL projects to hundreds of millions of users at LinkedIn.
Our results also confirm that in industry scenarios where users could benefit from improved system performance and data privacy, FL has the potential to replace centralized training.</span></p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text" style="color:#000000;">In literature, most cross-device FL benchmarks and systems are designed to process purely device-generated data (text, voice, image), and their components operate in standalone FL platforms.
As shown in our practical scenarios, the model/system performance and user experience in FL can greatly benefit from a collaboration of device-side and cloud-side data and systems.
Hence we emphasize further innovations in the device-cloud platform space.</span></p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="color:#000000;">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para ltx_noindent">
<p id="Sx1.p1.1" class="ltx_p"><span id="Sx1.p1.1.1" class="ltx_text" style="color:#000000;">We would like to thank
Lu An,
Sudhanshu Arora,
Oscar Bonilla,
Ting Chen,
Alexey Dubovkin,
Ebrahim Emami,
Humberto Gonzalez,
Ankit Goyal,
Mingyang Hu,
Abelino Jimenez,
Raghavan Muthuregunathan,
Haowen Ning,
Ray Ortigas,
Yafei Wang,
YuanKun Xue,
Hao Yu,
Leighton Zhang,
Haifeng Zhao,
and Tong Zhou for their valuable feedback on this work. Further, we thank Siyao Sun, Rahul Tandra, Zheng Li and Souvik Ghosh for their continuous support throughout this project.</span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="color:#000000;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.3.3.1" class="ltx_text" style="color:#000000;">(1)</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.5.1" class="ltx_text" style="color:#000000;">
Apple.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.6.1" class="ltx_text" style="color:#000000;">App tracking transparency.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="color:#000000;">URL
</span><a target="_blank" href="https://developer.apple.com/documentation/apptrackingtransparency" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="color:#000000;">https://developer.apple.com/documentation/apptrackingtransparency</a><span id="bib.bib1.8.2" class="ltx_text" style="color:#000000;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.9.1" class="ltx_text" style="color:#000000;">Accessed: 2022-10-20.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="color:#000000;">Baylor et al. (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="color:#000000;">
Baylor, D., Breck, E., Cheng, H.-T., Fiedel, N., Foo, C. Y., Haque, Z., Haykal,
S., Ispir, M., Jain, V., Koc, L., et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="color:#000000;">Tfx: A tensorflow-based production-scale machine learning platform.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.9.1" class="ltx_text" style="color:#000000;">In </span><em id="bib.bib2.10.2" class="ltx_emph ltx_font_italic" style="color:#000000;">Proceedings of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining</em><span id="bib.bib2.11.3" class="ltx_text" style="color:#000000;">, pp.  1387–1395, 2017.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="color:#000000;">Beutel et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="color:#000000;">
Beutel, D. J., Topal, T., Mathur, A., Qiu, X., Fernandez-Marques, J., Gao, Y.,
Sani, L., Li, K. H., Parcollet, T., de Gusmão, P. P. B., et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="color:#000000;">Flower: A friendly federated learning framework.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.9.1" class="ltx_text" style="color:#000000;">2022.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.4.4.1" class="ltx_text" style="color:#000000;">(4)</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.6.1" class="ltx_text" style="color:#000000;">
Bonawitz, K., Eichner, H., Grieskamp, W., et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="color:#000000;">Tensorflow federated: machine learning on decentralized data.(2020).
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="color:#000000;">Bonawitz et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="color:#000000;">
Bonawitz, K., Eichner, H., Grieskamp, W., Huba, D., Ingerman, A., Ivanov, V.,
Kiddon, C., Konečnỳ, J., Mazzocchi, S., McMahan, B., et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="color:#000000;">Towards federated learning at scale: System design.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib5.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">Proceedings of Machine Learning and Systems</em><span id="bib.bib5.10.2" class="ltx_text" style="color:#000000;">, 1:374–388, 2019.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="color:#000000;">Caldas et al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="color:#000000;">
Caldas, S., Duddu, S. M. K., Wu, P., Li, T., Konečnỳ, J., McMahan,
H. B., Smith, V., and Talwalkar, A.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="color:#000000;">Leaf: A benchmark for federated settings.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib6.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">arXiv preprint arXiv:1812.01097</em><span id="bib.bib6.10.2" class="ltx_text" style="color:#000000;">, 2018.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="color:#000000;">Charles et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="color:#000000;">
Charles, Z., Garrett, Z., Huo, Z., Shmulyian, S., and Smith, V.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="color:#000000;">On large-cohort training for federated learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib7.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">Advances in neural information processing systems</em><span id="bib.bib7.10.2" class="ltx_text" style="color:#000000;">,
34:20461–20475, 2021.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="color:#000000;">Chen et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="color:#000000;">
Chen, M., Mathews, R., Ouyang, T., and Beaufays, F.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="color:#000000;">Federated learning of out-of-vocabulary words.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib8.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">arXiv preprint arXiv:1903.10635</em><span id="bib.bib8.10.2" class="ltx_text" style="color:#000000;">, 2019.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="color:#000000;">Dimitriadis et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="color:#000000;">
Dimitriadis, D., Garcia, M. H., Diaz, D. M., Manoel, A., and Sim, R.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="color:#000000;">Flute: A scalable, extensible framework for high-performance
federated learning simulations.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib9.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">arXiv preprint arXiv:2203.13789</em><span id="bib.bib9.10.2" class="ltx_text" style="color:#000000;">, 2022.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.3.3.1" class="ltx_text" style="color:#000000;">(10)</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.5.1" class="ltx_text" style="color:#000000;">
GDPR.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.6.1" class="ltx_text" style="color:#000000;">General data protection regulation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="color:#000000;">URL </span><a target="_blank" href="https://gdpr-info.eu/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="color:#000000;">https://gdpr-info.eu/</a><span id="bib.bib10.8.2" class="ltx_text" style="color:#000000;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.9.1" class="ltx_text" style="color:#000000;">Accessed: 2022-10-25.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="color:#000000;">Guo et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="color:#000000;">
Guo, W., Liu, X., Wang, S., Kazi, M., Wang, Z., Fu, Z., Jia, J., Zhang, L.,
Gao, H., and Long, B.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="color:#000000;">Deep natural language processing for linkedin search.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib11.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">arXiv preprint arXiv:2108.13300</em><span id="bib.bib11.10.2" class="ltx_text" style="color:#000000;">, 2021.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="color:#000000;">Hao et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="color:#000000;">
Hao, W., Awatramani, A., Hu, J., Mao, C., Chen, P.-C., Cidon, E., Cidon, A.,
and Yang, J.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="color:#000000;">A tale of two models: Constructing evasive attacks on edge models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib12.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">Proceedings of Machine Learning and Systems</em><span id="bib.bib12.10.2" class="ltx_text" style="color:#000000;">, 4:414–429, 2022.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="color:#000000;">Hard et al. (2018a)</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="color:#000000;">
Hard, A., Rao, K., Mathews, R., Ramaswamy, S., Beaufays, F., Augenstein, S.,
Eichner, H., Kiddon, C., and Ramage, D.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="color:#000000;">Federated learning for mobile keyboard prediction.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib13.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">arXiv preprint arXiv:1811.03604</em><span id="bib.bib13.10.2" class="ltx_text" style="color:#000000;">, 2018a.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.5.5.1" class="ltx_text" style="color:#000000;">Hard et al. (2018b)</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="color:#000000;">
Hard, A., Rao, K., Mathews, R., Ramaswamy, S., Beaufays, F., Augenstein, S.,
Eichner, H., Kiddon, C., and Ramage, D.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="color:#000000;">Federated learning for mobile keyboard prediction.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib14.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">arXiv preprint arXiv:1811.03604</em><span id="bib.bib14.10.2" class="ltx_text" style="color:#000000;">, 2018b.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.4.4.1" class="ltx_text" style="color:#000000;">Hartmann (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.6.1" class="ltx_text" style="color:#000000;">
Hartmann, F.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="color:#000000;">Predicting text selections with federated learning, 2021.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="color:#000000;">URL
</span><a target="_blank" href="https://ai.googleblog.com/2021/11/predicting-text-selections-with.html" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="color:#000000;">https://ai.googleblog.com/2021/11/predicting-text-selections-with.html</a><span id="bib.bib15.9.2" class="ltx_text" style="color:#000000;">.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="color:#000000;">Hartmann et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="color:#000000;">
Hartmann, F., Suh, S., Komarzewski, A., Smith, T. D., and Segall, I.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="color:#000000;">Federated learning for ranking browser history suggestions.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib16.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">arXiv preprint arXiv:1911.11807</em><span id="bib.bib16.10.2" class="ltx_text" style="color:#000000;">, 2019.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="color:#000000;">He et al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="color:#000000;">
He, C., Li, S., So, J., Zeng, X., Zhang, M., Wang, H., Wang, X., Vepakomma, P.,
Singh, A., Qiu, H., et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="color:#000000;">Fedml: A research library and benchmark for federated machine
learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib17.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">arXiv preprint arXiv:2007.13518</em><span id="bib.bib17.10.2" class="ltx_text" style="color:#000000;">, 2020.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="color:#000000;">Horvath et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="color:#000000;">
Horvath, S., Laskaridis, S., Almeida, M., Leontiadis, I., Venieris, S., and
Lane, N.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="color:#000000;">Fjord: Fair and accurate federated learning under heterogeneous
targets with ordered dropout.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib18.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">Advances in Neural Information Processing Systems</em><span id="bib.bib18.10.2" class="ltx_text" style="color:#000000;">,
34:12876–12889, 2021.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="color:#000000;">Huba et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="color:#000000;">
Huba, D., Nguyen, J., Malik, K., Zhu, R., Rabbat, M., Yousefpour, A., Wu,
C.-J., Zhan, H., Ustinov, P., Srinivas, H., et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="color:#000000;">Papaya: Practical, private, and scalable federated learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib19.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">Proceedings of Machine Learning and Systems</em><span id="bib.bib19.10.2" class="ltx_text" style="color:#000000;">, 4:814–832, 2022.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="color:#000000;">Kairouz et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="color:#000000;">
Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N.,
Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="color:#000000;">Advances and open problems in federated learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib20.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">Foundations and Trends® in Machine Learning</em><span id="bib.bib20.10.2" class="ltx_text" style="color:#000000;">,
14(1–2):1–210, 2021.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="color:#000000;">Kraska et al. (2013)</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="color:#000000;">
Kraska, T., Talwalkar, A., Duchi, J. C., Griffith, R., Franklin, M. J., and
Jordan, M. I.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="color:#000000;">Mlbase: A distributed machine-learning system.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.9.1" class="ltx_text" style="color:#000000;">In </span><em id="bib.bib21.10.2" class="ltx_emph ltx_font_italic" style="color:#000000;">Cidr</em><span id="bib.bib21.11.3" class="ltx_text" style="color:#000000;">, volume 1, pp.  2–1, 2013.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="color:#000000;">Kuo et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="color:#000000;">
Kuo, K., Thaker, P., Khodak, M., Ngyuen, J., Jiang, D., Talwalkar, A., and
Smith, V.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="color:#000000;">On noisy evaluation in federated hyperparameter tuning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib22.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">arXiv preprint arXiv:2212.08930</em><span id="bib.bib22.10.2" class="ltx_text" style="color:#000000;">, 2022.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="color:#000000;">Lai et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="color:#000000;">
Lai, F., Dai, Y., Singapuram, S., Liu, J., Zhu, X., Madhyastha, H., and
Chowdhury, M.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="color:#000000;">FedScale: Benchmarking model and system performance of federated
learning at scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.9.1" class="ltx_text" style="color:#000000;">In </span><em id="bib.bib23.10.2" class="ltx_emph ltx_font_italic" style="color:#000000;">International Conference on Machine Learning</em><span id="bib.bib23.11.3" class="ltx_text" style="color:#000000;">, pp. 11814–11827. PMLR, 2022.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="color:#000000;">Li et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="color:#000000;">
Li, Q., Diao, Y., Chen, Q., and He, B.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="color:#000000;">Federated learning on non-iid data silos: An experimental study.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.9.1" class="ltx_text" style="color:#000000;">In </span><em id="bib.bib24.10.2" class="ltx_emph ltx_font_italic" style="color:#000000;">2022 IEEE 38th International Conference on Data Engineering
(ICDE)</em><span id="bib.bib24.11.3" class="ltx_text" style="color:#000000;">, pp.  965–978. IEEE, 2022.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text" style="color:#000000;">Li et al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="color:#000000;">
Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="color:#000000;">Federated optimization in heterogeneous networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib25.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">Proceedings of Machine Learning and Systems</em><span id="bib.bib25.10.2" class="ltx_text" style="color:#000000;">, 2:429–450, 2020.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="color:#000000;">Lv et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="color:#000000;">
Lv, C., Niu, C., Gu, R., Jiang, X., Wang, Z., Liu, B., Wu, Z., Yao, Q., Huang,
C., Huang, P., et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="color:#000000;">Walle: An end-to-end, general-purpose, and large-scale production
system for device-cloud collaborative machine learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib26.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">arXiv preprint arXiv:2205.14833</em><span id="bib.bib26.10.2" class="ltx_text" style="color:#000000;">, 2022.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="color:#000000;">McMahan et al. (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="color:#000000;">
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="color:#000000;">Communication-efficient learning of deep networks from decentralized
data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.9.1" class="ltx_text" style="color:#000000;">In </span><em id="bib.bib27.10.2" class="ltx_emph ltx_font_italic" style="color:#000000;">Artificial intelligence and statistics</em><span id="bib.bib27.11.3" class="ltx_text" style="color:#000000;">, pp.  1273–1282.
PMLR, 2017.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="color:#000000;">Mo et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="color:#000000;">
Mo, F., Haddadi, H., Katevas, K., Marin, E., Perino, D., and Kourtellis, N.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="color:#000000;">Ppfl: privacy-preserving federated learning with trusted execution
environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.9.1" class="ltx_text" style="color:#000000;">In </span><em id="bib.bib28.10.2" class="ltx_emph ltx_font_italic" style="color:#000000;">Proceedings of the 19th Annual International Conference on
Mobile Systems, Applications, and Services</em><span id="bib.bib28.11.3" class="ltx_text" style="color:#000000;">, pp.  94–108, 2021.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.7.5.1" class="ltx_text" style="color:#000000;">Moritz et al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.9.1" class="ltx_text" style="color:#000000;">
Moritz, P., Nishihara, R., Wang, S., Tumanov, A., Liaw, R., Liang, E., Elibol,
M., Yang, Z., Paul, W., Jordan, M. I., et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.10.1" class="ltx_text" style="color:#000000;">Ray: A distributed framework for emerging </span><math id="bib.bib29.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib29.1.m1.1a"><mo mathcolor="#000000" stretchy="false" id="bib.bib29.1.m1.1.1" xref="bib.bib29.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib29.1.m1.1b"><ci id="bib.bib29.1.m1.1.1.cmml" xref="bib.bib29.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib29.1.m1.1c">\{</annotation></semantics></math><span id="bib.bib29.11.2" class="ltx_text" style="color:#000000;">AI</span><math id="bib.bib29.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib29.2.m2.1a"><mo mathcolor="#000000" stretchy="false" id="bib.bib29.2.m2.1.1" xref="bib.bib29.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib29.2.m2.1b"><ci id="bib.bib29.2.m2.1.1.cmml" xref="bib.bib29.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib29.2.m2.1c">\}</annotation></semantics></math><span id="bib.bib29.12.3" class="ltx_text" style="color:#000000;"> applications.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.13.1" class="ltx_text" style="color:#000000;">In </span><em id="bib.bib29.14.2" class="ltx_emph ltx_font_italic" style="color:#000000;">13th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 18)</em><span id="bib.bib29.15.3" class="ltx_text" style="color:#000000;">, pp.  561–577, 2018.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.5.5.1" class="ltx_text" style="color:#000000;">Nguyen et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="color:#000000;">
Nguyen, J., Malik, K., Zhan, H., Yousefpour, A., Rabbat, M., Malek, M., and
Huba, D.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="color:#000000;">Federated learning with buffered asynchronous aggregation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.9.1" class="ltx_text" style="color:#000000;">In </span><em id="bib.bib30.10.2" class="ltx_emph ltx_font_italic" style="color:#000000;">International Conference on Artificial Intelligence and
Statistics</em><span id="bib.bib30.11.3" class="ltx_text" style="color:#000000;">, pp.  3581–3607. PMLR, 2022.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.5.5.1" class="ltx_text" style="color:#000000;">Pansare et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="color:#000000;">
Pansare, N., Katukuri, J., Arora, A., Cipollone, F., Shaik, R., Tokgozoglu, N.,
and Venkataraman, C.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text" style="color:#000000;">Learning compressed embeddings for on-device inference.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib31.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">Proceedings of Machine Learning and Systems</em><span id="bib.bib31.10.2" class="ltx_text" style="color:#000000;">, 4:382–397, 2022.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.5.5.1" class="ltx_text" style="color:#000000;">Paulik et al. (2021a)</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="color:#000000;">
Paulik, M., Seigel, M., Mason, H., Telaar, D., Kluivers, J., van Dalen, R.,
Lau, C. W., Carlson, L., Granqvist, F., Vandevelde, C., et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="color:#000000;">Federated evaluation and tuning for on-device personalization: System
design &amp; applications.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib32.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">arXiv preprint arXiv:2102.08503</em><span id="bib.bib32.10.2" class="ltx_text" style="color:#000000;">, 2021a.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.5.5.1" class="ltx_text" style="color:#000000;">Paulik et al. (2021b)</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="color:#000000;">
Paulik, M., Seigel, M., Mason, H., Telaar, D., Kluivers, J., van Dalen, R.,
Lau, C. W., Carlson, L., Granqvist, F., Vandevelde, C., et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="color:#000000;">Federated evaluation and tuning for on-device personalization: System
design &amp; applications.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib33.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">arXiv preprint arXiv:2102.08503</em><span id="bib.bib33.10.2" class="ltx_text" style="color:#000000;">, 2021b.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="color:#000000;">Ramaswamy et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="color:#000000;">
Ramaswamy, S., Mathews, R., Rao, K., and Beaufays, F.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="color:#000000;">Federated learning for emoji prediction in a mobile keyboard.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib34.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">arXiv preprint arXiv:1906.04329</em><span id="bib.bib34.10.2" class="ltx_text" style="color:#000000;">, 2019.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text" style="color:#000000;">Sculley et al. (2015)</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="color:#000000;">
Sculley, D., Holt, G., Golovin, D., Davydov, E., Phillips, T., Ebner, D.,
Chaudhary, V., Young, M., Crespo, J.-F., and Dennison, D.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="color:#000000;">Hidden technical debt in machine learning systems.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.9.1" class="ltx_text" style="color:#000000;">In </span><em id="bib.bib35.10.2" class="ltx_emph ltx_font_italic" style="color:#000000;">NIPS</em><span id="bib.bib35.11.3" class="ltx_text" style="color:#000000;">, 2015.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.5.5.1" class="ltx_text" style="color:#000000;">Sergeev &amp; Del Balso (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="color:#000000;">
Sergeev, A. and Del Balso, M.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text" style="color:#000000;">Horovod: fast and easy distributed deep learning in tensorflow.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib36.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">arXiv preprint arXiv:1802.05799</em><span id="bib.bib36.10.2" class="ltx_text" style="color:#000000;">, 2018.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.5.5.1" class="ltx_text" style="color:#000000;">Shejwalkar et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="color:#000000;">
Shejwalkar, V., Houmansadr, A., Kairouz, P., and Ramage, D.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="color:#000000;">Back to the drawing board: A critical evaluation of poisoning attacks
on production federated learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.9.1" class="ltx_text" style="color:#000000;">pp.  1354–1371, 2022.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text" style="color:#000000;">So et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="color:#000000;">
So, J., Nolet, C. J., Yang, C.-S., Li, S., Yu, Q., E Ali, R., Guler, B., and
Avestimehr, S.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text" style="color:#000000;">Lightsecagg: a lightweight and versatile design for secure
aggregation in federated learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib38.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">Proceedings of Machine Learning and Systems</em><span id="bib.bib38.10.2" class="ltx_text" style="color:#000000;">, 4:694–720, 2022.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.5.5.1" class="ltx_text" style="color:#000000;">Sparks et al. (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="color:#000000;">
Sparks, E. R., Venkataraman, S., Kaftan, T., Franklin, M. J., and Recht, B.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text" style="color:#000000;">Keystoneml: Optimizing pipelines for large-scale advanced analytics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.9.1" class="ltx_text" style="color:#000000;">In </span><em id="bib.bib39.10.2" class="ltx_emph ltx_font_italic" style="color:#000000;">2017 IEEE 33rd international conference on data engineering
(ICDE)</em><span id="bib.bib39.11.3" class="ltx_text" style="color:#000000;">, pp.  535–546. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text" style="color:#000000;">Sun et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="color:#000000;">
Sun, Z., Kairouz, P., Suresh, A. T., and McMahan, H. B.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="color:#000000;">Can you really backdoor federated learning?
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib40.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">arXiv preprint arXiv:1911.07963</em><span id="bib.bib40.10.2" class="ltx_text" style="color:#000000;">, 2019.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="color:#000000;">Weinberger et al. (2009)</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="color:#000000;">
Weinberger, K., Dasgupta, A., Langford, J., Smola, A., and Attenberg, J.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="color:#000000;">Feature hashing for large scale multitask learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.9.1" class="ltx_text" style="color:#000000;">In </span><em id="bib.bib41.10.2" class="ltx_emph ltx_font_italic" style="color:#000000;">Proceedings of the 26th annual international conference on
machine learning</em><span id="bib.bib41.11.3" class="ltx_text" style="color:#000000;">, pp.  1113–1120, 2009.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="color:#000000;">Wong et al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="color:#000000;">
Wong, E., Rice, L., and Kolter, J. Z.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="color:#000000;">Fast is better than free: Revisiting adversarial training.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib42.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">arXiv preprint arXiv:2001.03994</em><span id="bib.bib42.10.2" class="ltx_text" style="color:#000000;">, 2020.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text" style="color:#000000;">Wu et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="color:#000000;">
Wu, C.-J., Raghavendra, R., Gupta, U., Acun, B., Ardalani, N., Maeng, K.,
Chang, G., Aga, F., Huang, J., Bai, C., et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text" style="color:#000000;">Sustainable ai: Environmental implications, challenges and
opportunities.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib43.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">Proceedings of Machine Learning and Systems</em><span id="bib.bib43.10.2" class="ltx_text" style="color:#000000;">, 4:795–813, 2022.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text" style="color:#000000;">Yan et al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="color:#000000;">
Yan, F. Y., Ayers, H., Zhu, C., Fouladi, S., Hong, J., Zhang, K., Levis, P.,
and Winstein, K.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text" style="color:#000000;">Learning in situ: a randomized experiment in video streaming.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.9.1" class="ltx_text" style="color:#000000;">In </span><em id="bib.bib44.10.2" class="ltx_emph ltx_font_italic" style="color:#000000;">17th USENIX Symposium on Networked Systems Design and
Implementation (NSDI 20)</em><span id="bib.bib44.11.3" class="ltx_text" style="color:#000000;">, pp.  495–511, 2020.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text" style="color:#000000;">Yang et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text" style="color:#000000;">
Yang, C., Wang, Q., Xu, M., Chen, Z., Bian, K., Liu, Y., and Liu, X.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text" style="color:#000000;">Characterizing impacts of heterogeneity in federated learning upon
large-scale smartphone data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.9.1" class="ltx_text" style="color:#000000;">In </span><em id="bib.bib45.10.2" class="ltx_emph ltx_font_italic" style="color:#000000;">Proceedings of the Web Conference 2021</em><span id="bib.bib45.11.3" class="ltx_text" style="color:#000000;">, pp.  935–946,
2021.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.5.5.1" class="ltx_text" style="color:#000000;">Yang et al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text" style="color:#000000;">
Yang, T., Andrew, G., Eichner, H., Sun, H., Li, W., Kong, N., Ramage, D., and
Beaufays, F.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.8.1" class="ltx_text" style="color:#000000;">Applied federated learning: Improving google keyboard query
suggestions.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib46.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">arXiv preprint arXiv:1812.02903</em><span id="bib.bib46.10.2" class="ltx_text" style="color:#000000;">, 2018.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.5.5.1" class="ltx_text" style="color:#000000;">Yin et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text" style="color:#000000;">
Yin, C., Acun, B., Wu, C.-J., and Liu, X.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.8.1" class="ltx_text" style="color:#000000;">Tt-rec: Tensor train compression for deep learning recommendation
models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib47.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">Proceedings of Machine Learning and Systems</em><span id="bib.bib47.10.2" class="ltx_text" style="color:#000000;">, 3:448–462, 2021.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.5.5.1" class="ltx_text" style="color:#000000;">Yu et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.7.1" class="ltx_text" style="color:#000000;">
Yu, H., Yang, S., and Zhu, S.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.8.1" class="ltx_text" style="color:#000000;">Parallel restarted sgd with faster convergence and less
communication: Demystifying why model averaging works for deep learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib48.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">Proceedings of the AAAI Conference on Artificial Intelligence</em><span id="bib.bib48.10.2" class="ltx_text" style="color:#000000;">,
33:5693–5700, 2019.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.5.5.1" class="ltx_text" style="color:#000000;">Zaharia et al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.7.1" class="ltx_text" style="color:#000000;">
Zaharia, M., Chen, A., Davidson, A., Ghodsi, A., Hong, S. A., Konwinski, A.,
Murching, S., Nykodym, T., Ogilvie, P., Parkhe, M., et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.8.1" class="ltx_text" style="color:#000000;">Accelerating the machine learning lifecycle with mlflow.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib49.9.1" class="ltx_emph ltx_font_italic" style="color:#000000;">IEEE Data Eng. Bull.</em><span id="bib.bib49.10.2" class="ltx_text" style="color:#000000;">, 41(4):39–45, 2018.
</span>
</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:creator" content="Ewen Wang, Ajay Kannan, Yuefeng Liang, Boyi Chen, Mosharaf Chowdhury"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="Machine Learning, MLSys"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="mlsys 2022"></div>
<div class="ltx_rdf" about="" property="dcterms:title" content="FLINT: A Platform for Federated Learning Integration"></div>

<div id="p7" class="ltx_para ltx_noindent">
<p id="p7.1" class="ltx_p"></p>
</div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2302.12861" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2302.12862" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2302.12862">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2302.12862" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2302.12863" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 22:44:53 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
