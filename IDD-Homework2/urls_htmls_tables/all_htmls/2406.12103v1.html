<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper.</title>
<!--Generated on Mon Jun 17 21:31:02 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.12103v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S1" title="In Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S2" title="In Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background and Overview</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S2.SS1" title="In 2 Background and Overview ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Overview of findings</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S3" title="In Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Considering when to use DP</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S3.SS1" title="In 3 Considering when to use DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Assessing risk and privacy threats</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S3.SS2" title="In 3 Considering when to use DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Understanding contextual needs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S3.SS3" title="In 3 Considering when to use DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Aligning DP and institutional incentives</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S3.SS4" title="In 3 Considering when to use DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Integrating DP with other privacy practices</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S4" title="In Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Communicating around DP</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S4.SS1" title="In 4 Communicating around DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Tailoring communications for different stakeholders</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S4.SS2" title="In 4 Communicating around DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Avoiding transparency traps</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S4.SS3" title="In 4 Communicating around DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Engaging beyond epsilon</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S5" title="In Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Design and policy choices</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S5.SS1" title="In 5 Design and policy choices ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Choosing the right definition and unit of privacy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S5.SS2" title="In 5 Design and policy choices ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Selecting and allocating privacy-loss parameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S5.SS3" title="In 5 Design and policy choices ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Designing entire pipelines for DP</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S6" title="In Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Practice of DP</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S6.SS1" title="In 6 Practice of DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Facilitating data processing and exploration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S6.SS2" title="In 6 Practice of DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Choosing metadata parameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S6.SS3" title="In 6 Practice of DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Tools for using and evaluating data products with DP</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S7" title="In Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Trust and Governance of DP</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S7.SS1" title="In 7 Trust and Governance of DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Stakeholder engagement and oversight</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S7.SS2" title="In 7 Trust and Governance of DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Documenting decisions, justification, and guidelines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S7.SS3" title="In 7 Trust and Governance of DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Auditing DP deployments</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S8" title="In Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Centering Policy and Practice: Research Gaps around Usable Differential Privacy<span class="ltx_note ltx_role_thanks" id="id1.id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>R.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA.
<br class="ltx_break"/>The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper.</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rachel Cummings
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Industrial Engineering and Operations Research,
Columbia University
</span>
<span class="ltx_contact ltx_role_affiliation">Data Science Institute, Columbia University
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jayshree Sarathy
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Data Science Institute, Columbia University
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">As a mathematically rigorous framework that has amassed a rich theoretical literature, differential privacy is considered by many experts to be the ‘gold standard’ for privacy-preserving data analysis. Others argue that while differential privacy is a clean formulation in theory, it poses significant challenges in practice. Both perspectives are, in our view, valid and important. To bridge the gaps between differential privacy’s promises and its real-world usability, researchers and practitioners must work together to advance policy and practice of this technology. In this paper, we outline pressing open questions towards building usable differential privacy and offer recommendations for the field, such as developing risk frameworks to align with user needs, tailoring communications for different stakeholders, modeling the impact of privacy-loss parameters, investing in effective user interfaces, and facilitating algorithmic and procedural audits of differential privacy systems.</p>
</div>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="p1.1.1">Keywords —</span> differential privacy, usable security &amp; privacy, technology policy</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Statistical disclosure limitation has been an active area of research and practice for several decades <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib98" title="">98</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib75" title="">75</a>]</cite>. With the introduction of <em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">differential privacy</em> (DP) in 2006 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib41" title="">41</a>]</cite>, however, the field has undergone an invigorating renewal. Differential privacy is a framework for adding noise to statistical releases in order to protect the privacy of individual data subjects. It provides a formal characterization of the tradeoffs between privacy and accuracy of statistical releases. Since its inception, differential privacy has been studied deeply in the theoretical computer science literature as well as in areas as diverse as statistics, demography, and law. DP has been deployed across government <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib2" title="">2</a>]</cite> and industry <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib74" title="">74</a>]</cite>, changing the way that organizations protect privacy of data subjects while enabling transparency and data sharing.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">With these deployments have come significant challenges. The use of differential privacy has triggered confusion and contestation across nearly all of its deployments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib109" title="">109</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib107" title="">107</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib100" title="">100</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib64" title="">64</a>]</cite>.
Computer scientists have largely built consensus around the basic definitions and assumptions of this framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib8" title="">8</a>]</cite>, but as DP spills out into other fields and engages a wide variety of stakeholders, its motivations, communications, design &amp; policy choices, and practical usage are questioned and negotiated anew by parties who understand data privacy from conflicting perspectives. In our view, these frictions signal a need for researchers—both within and beyond computer science—to center policy and practice of differential privacy.
By policy, we are primarily concerned with <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">organizational policy</em> rather than <em class="ltx_emph ltx_font_italic" id="S1.p2.1.2">public policy</em>, although we believe that making progress on the former will help to inform the latter. We believe researchers should design guidelines around both technical and social factors of DP systems within organizational contexts. By practice, we are referring broadly to challenges related to the design, deployment, and use of DP systems.
This paper provides a roadmap of the open challenges and starting points for solutions to make DP significantly more usable in practice—and in some cases to develop alternatives that leverage the insights underlying DP’s rigorous foundations yet are relaxed or tailored to meet the needs of stakeholders.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">The goal of this paper is not just to outline the known challenges, but also to try to scope out the potential unknown hurdles to deploying DP. These unknowns are particularly important to address in order to regulate the use of DP and integrate the principles of DP into law and policy, as well as to prepare for the engineering challenges of large-scale DP deployments.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background and Overview</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we provide a brief background on DP and an overview of the paper. We refer the curious reader to other surveys and textbooks for a more comprehensive overview of DP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib114" title="">114</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib115" title="">115</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib88" title="">88</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib26" title="">26</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.2">Differential privacy (DP), introduced by Dwork, McSherry, Nissim, and Smith in 2006 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib41" title="">41</a>]</cite>, is a mathematical definition of privacy that characterizes how much information a statistical analysis reveals about any one <em class="ltx_emph ltx_font_italic" id="S2.p2.2.1">privacy unit</em> (typically, an individual or a small group of individuals) in the dataset.
The standard definition is parametrized by two quantities—<math alttext="\varepsilon" class="ltx_Math" display="inline" id="S2.p2.1.m1.1"><semantics id="S2.p2.1.m1.1a"><mi id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><ci id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">\varepsilon</annotation><annotation encoding="application/x-llamapun" id="S2.p2.1.m1.1d">italic_ε</annotation></semantics></math> and <math alttext="\delta" class="ltx_Math" display="inline" id="S2.p2.2.m2.1"><semantics id="S2.p2.2.m2.1a"><mi id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><ci id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">\delta</annotation><annotation encoding="application/x-llamapun" id="S2.p2.2.m2.1d">italic_δ</annotation></semantics></math>—that denote the <em class="ltx_emph ltx_font_italic" id="S2.p2.2.2">privacy loss</em> incurred by running a given set of analyses on the data.
A mechanism that limits privacy loss must introduce carefully calibrated noise to any computation over the data.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.7">We provide the formal definition of DP below. Let <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S2.p3.1.m1.1"><semantics id="S2.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><ci id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.1.m1.1d">caligraphic_D</annotation></semantics></math> be a data universe and <math alttext="\mathcal{D}^{n}" class="ltx_Math" display="inline" id="S2.p3.2.m2.1"><semantics id="S2.p3.2.m2.1a"><msup id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p3.2.m2.1.1.2" xref="S2.p3.2.m2.1.1.2.cmml">𝒟</mi><mi id="S2.p3.2.m2.1.1.3" xref="S2.p3.2.m2.1.1.3.cmml">n</mi></msup><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><apply id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p3.2.m2.1.1.1.cmml" xref="S2.p3.2.m2.1.1">superscript</csymbol><ci id="S2.p3.2.m2.1.1.2.cmml" xref="S2.p3.2.m2.1.1.2">𝒟</ci><ci id="S2.p3.2.m2.1.1.3.cmml" xref="S2.p3.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">\mathcal{D}^{n}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.2.m2.1d">caligraphic_D start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> be the space of datasets of size <math alttext="n" class="ltx_Math" display="inline" id="S2.p3.3.m3.1"><semantics id="S2.p3.3.m3.1a"><mi id="S2.p3.3.m3.1.1" xref="S2.p3.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.1b"><ci id="S2.p3.3.m3.1.1.cmml" xref="S2.p3.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.p3.3.m3.1d">italic_n</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>This is called the <em class="ltx_emph ltx_font_italic" id="footnote1.1">bounded</em> DP definition because the size of the dataset is fixed and public. One could also consider the <em class="ltx_emph ltx_font_italic" id="footnote1.2">unbounded</em> definition for settings where the dataset size is itself sensitive <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib70" title="">70</a>]</cite>.</span></span></span>
Two datasets <math alttext="d,d^{\prime}\in\mathcal{D}^{n}" class="ltx_Math" display="inline" id="S2.p3.4.m4.2"><semantics id="S2.p3.4.m4.2a"><mrow id="S2.p3.4.m4.2.2" xref="S2.p3.4.m4.2.2.cmml"><mrow id="S2.p3.4.m4.2.2.1.1" xref="S2.p3.4.m4.2.2.1.2.cmml"><mi id="S2.p3.4.m4.1.1" xref="S2.p3.4.m4.1.1.cmml">d</mi><mo id="S2.p3.4.m4.2.2.1.1.2" xref="S2.p3.4.m4.2.2.1.2.cmml">,</mo><msup id="S2.p3.4.m4.2.2.1.1.1" xref="S2.p3.4.m4.2.2.1.1.1.cmml"><mi id="S2.p3.4.m4.2.2.1.1.1.2" xref="S2.p3.4.m4.2.2.1.1.1.2.cmml">d</mi><mo id="S2.p3.4.m4.2.2.1.1.1.3" xref="S2.p3.4.m4.2.2.1.1.1.3.cmml">′</mo></msup></mrow><mo id="S2.p3.4.m4.2.2.2" xref="S2.p3.4.m4.2.2.2.cmml">∈</mo><msup id="S2.p3.4.m4.2.2.3" xref="S2.p3.4.m4.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p3.4.m4.2.2.3.2" xref="S2.p3.4.m4.2.2.3.2.cmml">𝒟</mi><mi id="S2.p3.4.m4.2.2.3.3" xref="S2.p3.4.m4.2.2.3.3.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.4.m4.2b"><apply id="S2.p3.4.m4.2.2.cmml" xref="S2.p3.4.m4.2.2"><in id="S2.p3.4.m4.2.2.2.cmml" xref="S2.p3.4.m4.2.2.2"></in><list id="S2.p3.4.m4.2.2.1.2.cmml" xref="S2.p3.4.m4.2.2.1.1"><ci id="S2.p3.4.m4.1.1.cmml" xref="S2.p3.4.m4.1.1">𝑑</ci><apply id="S2.p3.4.m4.2.2.1.1.1.cmml" xref="S2.p3.4.m4.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.p3.4.m4.2.2.1.1.1.1.cmml" xref="S2.p3.4.m4.2.2.1.1.1">superscript</csymbol><ci id="S2.p3.4.m4.2.2.1.1.1.2.cmml" xref="S2.p3.4.m4.2.2.1.1.1.2">𝑑</ci><ci id="S2.p3.4.m4.2.2.1.1.1.3.cmml" xref="S2.p3.4.m4.2.2.1.1.1.3">′</ci></apply></list><apply id="S2.p3.4.m4.2.2.3.cmml" xref="S2.p3.4.m4.2.2.3"><csymbol cd="ambiguous" id="S2.p3.4.m4.2.2.3.1.cmml" xref="S2.p3.4.m4.2.2.3">superscript</csymbol><ci id="S2.p3.4.m4.2.2.3.2.cmml" xref="S2.p3.4.m4.2.2.3.2">𝒟</ci><ci id="S2.p3.4.m4.2.2.3.3.cmml" xref="S2.p3.4.m4.2.2.3.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m4.2c">d,d^{\prime}\in\mathcal{D}^{n}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.4.m4.2d">italic_d , italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∈ caligraphic_D start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> are neighboring, denoted <math alttext="d\sim d^{\prime}" class="ltx_Math" display="inline" id="S2.p3.5.m5.1"><semantics id="S2.p3.5.m5.1a"><mrow id="S2.p3.5.m5.1.1" xref="S2.p3.5.m5.1.1.cmml"><mi id="S2.p3.5.m5.1.1.2" xref="S2.p3.5.m5.1.1.2.cmml">d</mi><mo id="S2.p3.5.m5.1.1.1" xref="S2.p3.5.m5.1.1.1.cmml">∼</mo><msup id="S2.p3.5.m5.1.1.3" xref="S2.p3.5.m5.1.1.3.cmml"><mi id="S2.p3.5.m5.1.1.3.2" xref="S2.p3.5.m5.1.1.3.2.cmml">d</mi><mo id="S2.p3.5.m5.1.1.3.3" xref="S2.p3.5.m5.1.1.3.3.cmml">′</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.5.m5.1b"><apply id="S2.p3.5.m5.1.1.cmml" xref="S2.p3.5.m5.1.1"><csymbol cd="latexml" id="S2.p3.5.m5.1.1.1.cmml" xref="S2.p3.5.m5.1.1.1">similar-to</csymbol><ci id="S2.p3.5.m5.1.1.2.cmml" xref="S2.p3.5.m5.1.1.2">𝑑</ci><apply id="S2.p3.5.m5.1.1.3.cmml" xref="S2.p3.5.m5.1.1.3"><csymbol cd="ambiguous" id="S2.p3.5.m5.1.1.3.1.cmml" xref="S2.p3.5.m5.1.1.3">superscript</csymbol><ci id="S2.p3.5.m5.1.1.3.2.cmml" xref="S2.p3.5.m5.1.1.3.2">𝑑</ci><ci id="S2.p3.5.m5.1.1.3.3.cmml" xref="S2.p3.5.m5.1.1.3.3">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.5.m5.1c">d\sim d^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.5.m5.1d">italic_d ∼ italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>, if they differ in a single record.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Another common formalization is that <math alttext="d,d^{\prime}" class="ltx_Math" display="inline" id="footnote2.m1.2"><semantics id="footnote2.m1.2b"><mrow id="footnote2.m1.2.2.1" xref="footnote2.m1.2.2.2.cmml"><mi id="footnote2.m1.1.1" xref="footnote2.m1.1.1.cmml">d</mi><mo id="footnote2.m1.2.2.1.2" xref="footnote2.m1.2.2.2.cmml">,</mo><msup id="footnote2.m1.2.2.1.1" xref="footnote2.m1.2.2.1.1.cmml"><mi id="footnote2.m1.2.2.1.1.2" xref="footnote2.m1.2.2.1.1.2.cmml">d</mi><mo id="footnote2.m1.2.2.1.1.3" xref="footnote2.m1.2.2.1.1.3.cmml">′</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="footnote2.m1.2c"><list id="footnote2.m1.2.2.2.cmml" xref="footnote2.m1.2.2.1"><ci id="footnote2.m1.1.1.cmml" xref="footnote2.m1.1.1">𝑑</ci><apply id="footnote2.m1.2.2.1.1.cmml" xref="footnote2.m1.2.2.1.1"><csymbol cd="ambiguous" id="footnote2.m1.2.2.1.1.1.cmml" xref="footnote2.m1.2.2.1.1">superscript</csymbol><ci id="footnote2.m1.2.2.1.1.2.cmml" xref="footnote2.m1.2.2.1.1.2">𝑑</ci><ci id="footnote2.m1.2.2.1.1.3.cmml" xref="footnote2.m1.2.2.1.1.3">′</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m1.2d">d,d^{\prime}</annotation><annotation encoding="application/x-llamapun" id="footnote2.m1.2e">italic_d , italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> are neighboring if <math alttext="d^{\prime}" class="ltx_Math" display="inline" id="footnote2.m2.1"><semantics id="footnote2.m2.1b"><msup id="footnote2.m2.1.1" xref="footnote2.m2.1.1.cmml"><mi id="footnote2.m2.1.1.2" xref="footnote2.m2.1.1.2.cmml">d</mi><mo id="footnote2.m2.1.1.3" xref="footnote2.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="footnote2.m2.1c"><apply id="footnote2.m2.1.1.cmml" xref="footnote2.m2.1.1"><csymbol cd="ambiguous" id="footnote2.m2.1.1.1.cmml" xref="footnote2.m2.1.1">superscript</csymbol><ci id="footnote2.m2.1.1.2.cmml" xref="footnote2.m2.1.1.2">𝑑</ci><ci id="footnote2.m2.1.1.3.cmml" xref="footnote2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m2.1d">d^{\prime}</annotation><annotation encoding="application/x-llamapun" id="footnote2.m2.1e">italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> can be created from <math alttext="d" class="ltx_Math" display="inline" id="footnote2.m3.1"><semantics id="footnote2.m3.1b"><mi id="footnote2.m3.1.1" xref="footnote2.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="footnote2.m3.1c"><ci id="footnote2.m3.1.1.cmml" xref="footnote2.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m3.1d">d</annotation><annotation encoding="application/x-llamapun" id="footnote2.m3.1e">italic_d</annotation></semantics></math> by adding or removing a single data record.</span></span></span>
Let <math alttext="\mathcal{H}" class="ltx_Math" display="inline" id="S2.p3.6.m6.1"><semantics id="S2.p3.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p3.6.m6.1.1" xref="S2.p3.6.m6.1.1.cmml">ℋ</mi><annotation-xml encoding="MathML-Content" id="S2.p3.6.m6.1b"><ci id="S2.p3.6.m6.1.1.cmml" xref="S2.p3.6.m6.1.1">ℋ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.6.m6.1c">\mathcal{H}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.6.m6.1d">caligraphic_H</annotation></semantics></math> be a hyperparameter space and <math alttext="\mathcal{Y}" class="ltx_Math" display="inline" id="S2.p3.7.m7.1"><semantics id="S2.p3.7.m7.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p3.7.m7.1.1" xref="S2.p3.7.m7.1.1.cmml">𝒴</mi><annotation-xml encoding="MathML-Content" id="S2.p3.7.m7.1b"><ci id="S2.p3.7.m7.1.1.cmml" xref="S2.p3.7.m7.1.1">𝒴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.7.m7.1c">\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.7.m7.1d">caligraphic_Y</annotation></semantics></math> be an output space.</p>
</div>
<div class="ltx_theorem ltx_theorem_definition" id="Thmdefinition1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="Thmdefinition1.1.1.1">Definition 1</span></span><span class="ltx_text ltx_font_bold" id="Thmdefinition1.2.2"> </span>(Differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib41" title="">41</a>]</cite>)<span class="ltx_text ltx_font_bold" id="Thmdefinition1.3.3">.</span>
</h6>
<div class="ltx_para" id="Thmdefinition1.p1">
<p class="ltx_p" id="Thmdefinition1.p1.7"><span class="ltx_text ltx_font_italic" id="Thmdefinition1.p1.7.7">A randomized mechanism <math alttext="\mathcal{M}:\mathcal{D}^{n}\times\mathbb{R}_{\geq 0}\times[0,1]\times\mathcal{%
H}\rightarrow\mathcal{Y}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.1.1.m1.2"><semantics id="Thmdefinition1.p1.1.1.m1.2a"><mrow id="Thmdefinition1.p1.1.1.m1.2.3" xref="Thmdefinition1.p1.1.1.m1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.1.1.m1.2.3.2" xref="Thmdefinition1.p1.1.1.m1.2.3.2.cmml">ℳ</mi><mo id="Thmdefinition1.p1.1.1.m1.2.3.1" lspace="0.278em" rspace="0.278em" xref="Thmdefinition1.p1.1.1.m1.2.3.1.cmml">:</mo><mrow id="Thmdefinition1.p1.1.1.m1.2.3.3" xref="Thmdefinition1.p1.1.1.m1.2.3.3.cmml"><mrow id="Thmdefinition1.p1.1.1.m1.2.3.3.2" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.cmml"><msup id="Thmdefinition1.p1.1.1.m1.2.3.3.2.2" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.1.1.m1.2.3.3.2.2.2" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.2.2.cmml">𝒟</mi><mi id="Thmdefinition1.p1.1.1.m1.2.3.3.2.2.3" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.2.3.cmml">n</mi></msup><mo id="Thmdefinition1.p1.1.1.m1.2.3.3.2.1" lspace="0.222em" rspace="0.222em" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.1.cmml">×</mo><msub id="Thmdefinition1.p1.1.1.m1.2.3.3.2.3" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.cmml"><mi id="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.2" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.2.cmml">ℝ</mi><mrow id="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.3" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.3.cmml"><mi id="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.3.2" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.3.2.cmml"></mi><mo id="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.3.1" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.3.1.cmml">≥</mo><mn id="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.3.3" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.3.3.cmml">0</mn></mrow></msub><mo id="Thmdefinition1.p1.1.1.m1.2.3.3.2.1a" lspace="0.222em" rspace="0.222em" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.1.cmml">×</mo><mrow id="Thmdefinition1.p1.1.1.m1.2.3.3.2.4.2" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.4.1.cmml"><mo id="Thmdefinition1.p1.1.1.m1.2.3.3.2.4.2.1" stretchy="false" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.4.1.cmml">[</mo><mn id="Thmdefinition1.p1.1.1.m1.1.1" xref="Thmdefinition1.p1.1.1.m1.1.1.cmml">0</mn><mo id="Thmdefinition1.p1.1.1.m1.2.3.3.2.4.2.2" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.4.1.cmml">,</mo><mn id="Thmdefinition1.p1.1.1.m1.2.2" xref="Thmdefinition1.p1.1.1.m1.2.2.cmml">1</mn><mo id="Thmdefinition1.p1.1.1.m1.2.3.3.2.4.2.3" rspace="0.055em" stretchy="false" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.4.1.cmml">]</mo></mrow><mo id="Thmdefinition1.p1.1.1.m1.2.3.3.2.1b" rspace="0.222em" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.1.cmml">×</mo><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.1.1.m1.2.3.3.2.5" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.5.cmml">ℋ</mi></mrow><mo id="Thmdefinition1.p1.1.1.m1.2.3.3.1" stretchy="false" xref="Thmdefinition1.p1.1.1.m1.2.3.3.1.cmml">→</mo><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.1.1.m1.2.3.3.3" xref="Thmdefinition1.p1.1.1.m1.2.3.3.3.cmml">𝒴</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.1.1.m1.2b"><apply id="Thmdefinition1.p1.1.1.m1.2.3.cmml" xref="Thmdefinition1.p1.1.1.m1.2.3"><ci id="Thmdefinition1.p1.1.1.m1.2.3.1.cmml" xref="Thmdefinition1.p1.1.1.m1.2.3.1">:</ci><ci id="Thmdefinition1.p1.1.1.m1.2.3.2.cmml" xref="Thmdefinition1.p1.1.1.m1.2.3.2">ℳ</ci><apply id="Thmdefinition1.p1.1.1.m1.2.3.3.cmml" xref="Thmdefinition1.p1.1.1.m1.2.3.3"><ci id="Thmdefinition1.p1.1.1.m1.2.3.3.1.cmml" xref="Thmdefinition1.p1.1.1.m1.2.3.3.1">→</ci><apply id="Thmdefinition1.p1.1.1.m1.2.3.3.2.cmml" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2"><times id="Thmdefinition1.p1.1.1.m1.2.3.3.2.1.cmml" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.1"></times><apply id="Thmdefinition1.p1.1.1.m1.2.3.3.2.2.cmml" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.2"><csymbol cd="ambiguous" id="Thmdefinition1.p1.1.1.m1.2.3.3.2.2.1.cmml" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.2">superscript</csymbol><ci id="Thmdefinition1.p1.1.1.m1.2.3.3.2.2.2.cmml" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.2.2">𝒟</ci><ci id="Thmdefinition1.p1.1.1.m1.2.3.3.2.2.3.cmml" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.2.3">𝑛</ci></apply><apply id="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.cmml" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.3"><csymbol cd="ambiguous" id="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.1.cmml" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.3">subscript</csymbol><ci id="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.2.cmml" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.2">ℝ</ci><apply id="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.3.cmml" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.3"><geq id="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.3.1.cmml" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.3.1"></geq><csymbol cd="latexml" id="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.3.2.cmml" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.3.2">absent</csymbol><cn id="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.3.3.cmml" type="integer" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.3.3.3">0</cn></apply></apply><interval closure="closed" id="Thmdefinition1.p1.1.1.m1.2.3.3.2.4.1.cmml" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.4.2"><cn id="Thmdefinition1.p1.1.1.m1.1.1.cmml" type="integer" xref="Thmdefinition1.p1.1.1.m1.1.1">0</cn><cn id="Thmdefinition1.p1.1.1.m1.2.2.cmml" type="integer" xref="Thmdefinition1.p1.1.1.m1.2.2">1</cn></interval><ci id="Thmdefinition1.p1.1.1.m1.2.3.3.2.5.cmml" xref="Thmdefinition1.p1.1.1.m1.2.3.3.2.5">ℋ</ci></apply><ci id="Thmdefinition1.p1.1.1.m1.2.3.3.3.cmml" xref="Thmdefinition1.p1.1.1.m1.2.3.3.3">𝒴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.1.1.m1.2c">\mathcal{M}:\mathcal{D}^{n}\times\mathbb{R}_{\geq 0}\times[0,1]\times\mathcal{%
H}\rightarrow\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="Thmdefinition1.p1.1.1.m1.2d">caligraphic_M : caligraphic_D start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT × blackboard_R start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT × [ 0 , 1 ] × caligraphic_H → caligraphic_Y</annotation></semantics></math> is <em class="ltx_emph ltx_font_upright" id="Thmdefinition1.p1.2.2.1"><math alttext="(\varepsilon,\delta)" class="ltx_Math" display="inline" id="Thmdefinition1.p1.2.2.1.m1.2"><semantics id="Thmdefinition1.p1.2.2.1.m1.2a"><mrow id="Thmdefinition1.p1.2.2.1.m1.2.3.2" xref="Thmdefinition1.p1.2.2.1.m1.2.3.1.cmml"><mo id="Thmdefinition1.p1.2.2.1.m1.2.3.2.1" stretchy="false" xref="Thmdefinition1.p1.2.2.1.m1.2.3.1.cmml">(</mo><mi id="Thmdefinition1.p1.2.2.1.m1.1.1" xref="Thmdefinition1.p1.2.2.1.m1.1.1.cmml">ε</mi><mo id="Thmdefinition1.p1.2.2.1.m1.2.3.2.2" xref="Thmdefinition1.p1.2.2.1.m1.2.3.1.cmml">,</mo><mi id="Thmdefinition1.p1.2.2.1.m1.2.2" xref="Thmdefinition1.p1.2.2.1.m1.2.2.cmml">δ</mi><mo id="Thmdefinition1.p1.2.2.1.m1.2.3.2.3" stretchy="false" xref="Thmdefinition1.p1.2.2.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.2.2.1.m1.2b"><interval closure="open" id="Thmdefinition1.p1.2.2.1.m1.2.3.1.cmml" xref="Thmdefinition1.p1.2.2.1.m1.2.3.2"><ci id="Thmdefinition1.p1.2.2.1.m1.1.1.cmml" xref="Thmdefinition1.p1.2.2.1.m1.1.1">𝜀</ci><ci id="Thmdefinition1.p1.2.2.1.m1.2.2.cmml" xref="Thmdefinition1.p1.2.2.1.m1.2.2">𝛿</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.2.2.1.m1.2c">(\varepsilon,\delta)</annotation><annotation encoding="application/x-llamapun" id="Thmdefinition1.p1.2.2.1.m1.2d">( italic_ε , italic_δ )</annotation></semantics></math>-differentially private</em> if for all datasets <math alttext="d\sim d^{\prime}\in\mathcal{D}^{n}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.3.3.m2.1"><semantics id="Thmdefinition1.p1.3.3.m2.1a"><mrow id="Thmdefinition1.p1.3.3.m2.1.1" xref="Thmdefinition1.p1.3.3.m2.1.1.cmml"><mi id="Thmdefinition1.p1.3.3.m2.1.1.2" xref="Thmdefinition1.p1.3.3.m2.1.1.2.cmml">d</mi><mo id="Thmdefinition1.p1.3.3.m2.1.1.3" xref="Thmdefinition1.p1.3.3.m2.1.1.3.cmml">∼</mo><msup id="Thmdefinition1.p1.3.3.m2.1.1.4" xref="Thmdefinition1.p1.3.3.m2.1.1.4.cmml"><mi id="Thmdefinition1.p1.3.3.m2.1.1.4.2" xref="Thmdefinition1.p1.3.3.m2.1.1.4.2.cmml">d</mi><mo id="Thmdefinition1.p1.3.3.m2.1.1.4.3" xref="Thmdefinition1.p1.3.3.m2.1.1.4.3.cmml">′</mo></msup><mo id="Thmdefinition1.p1.3.3.m2.1.1.5" xref="Thmdefinition1.p1.3.3.m2.1.1.5.cmml">∈</mo><msup id="Thmdefinition1.p1.3.3.m2.1.1.6" xref="Thmdefinition1.p1.3.3.m2.1.1.6.cmml"><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.3.3.m2.1.1.6.2" xref="Thmdefinition1.p1.3.3.m2.1.1.6.2.cmml">𝒟</mi><mi id="Thmdefinition1.p1.3.3.m2.1.1.6.3" xref="Thmdefinition1.p1.3.3.m2.1.1.6.3.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.3.3.m2.1b"><apply id="Thmdefinition1.p1.3.3.m2.1.1.cmml" xref="Thmdefinition1.p1.3.3.m2.1.1"><and id="Thmdefinition1.p1.3.3.m2.1.1a.cmml" xref="Thmdefinition1.p1.3.3.m2.1.1"></and><apply id="Thmdefinition1.p1.3.3.m2.1.1b.cmml" xref="Thmdefinition1.p1.3.3.m2.1.1"><csymbol cd="latexml" id="Thmdefinition1.p1.3.3.m2.1.1.3.cmml" xref="Thmdefinition1.p1.3.3.m2.1.1.3">similar-to</csymbol><ci id="Thmdefinition1.p1.3.3.m2.1.1.2.cmml" xref="Thmdefinition1.p1.3.3.m2.1.1.2">𝑑</ci><apply id="Thmdefinition1.p1.3.3.m2.1.1.4.cmml" xref="Thmdefinition1.p1.3.3.m2.1.1.4"><csymbol cd="ambiguous" id="Thmdefinition1.p1.3.3.m2.1.1.4.1.cmml" xref="Thmdefinition1.p1.3.3.m2.1.1.4">superscript</csymbol><ci id="Thmdefinition1.p1.3.3.m2.1.1.4.2.cmml" xref="Thmdefinition1.p1.3.3.m2.1.1.4.2">𝑑</ci><ci id="Thmdefinition1.p1.3.3.m2.1.1.4.3.cmml" xref="Thmdefinition1.p1.3.3.m2.1.1.4.3">′</ci></apply></apply><apply id="Thmdefinition1.p1.3.3.m2.1.1c.cmml" xref="Thmdefinition1.p1.3.3.m2.1.1"><in id="Thmdefinition1.p1.3.3.m2.1.1.5.cmml" xref="Thmdefinition1.p1.3.3.m2.1.1.5"></in><share href="https://arxiv.org/html/2406.12103v1#Thmdefinition1.p1.3.3.m2.1.1.4.cmml" id="Thmdefinition1.p1.3.3.m2.1.1d.cmml" xref="Thmdefinition1.p1.3.3.m2.1.1"></share><apply id="Thmdefinition1.p1.3.3.m2.1.1.6.cmml" xref="Thmdefinition1.p1.3.3.m2.1.1.6"><csymbol cd="ambiguous" id="Thmdefinition1.p1.3.3.m2.1.1.6.1.cmml" xref="Thmdefinition1.p1.3.3.m2.1.1.6">superscript</csymbol><ci id="Thmdefinition1.p1.3.3.m2.1.1.6.2.cmml" xref="Thmdefinition1.p1.3.3.m2.1.1.6.2">𝒟</ci><ci id="Thmdefinition1.p1.3.3.m2.1.1.6.3.cmml" xref="Thmdefinition1.p1.3.3.m2.1.1.6.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.3.3.m2.1c">d\sim d^{\prime}\in\mathcal{D}^{n}</annotation><annotation encoding="application/x-llamapun" id="Thmdefinition1.p1.3.3.m2.1d">italic_d ∼ italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∈ caligraphic_D start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT</annotation></semantics></math>, privacy loss parameters <math alttext="\varepsilon\geq 0" class="ltx_Math" display="inline" id="Thmdefinition1.p1.4.4.m3.1"><semantics id="Thmdefinition1.p1.4.4.m3.1a"><mrow id="Thmdefinition1.p1.4.4.m3.1.1" xref="Thmdefinition1.p1.4.4.m3.1.1.cmml"><mi id="Thmdefinition1.p1.4.4.m3.1.1.2" xref="Thmdefinition1.p1.4.4.m3.1.1.2.cmml">ε</mi><mo id="Thmdefinition1.p1.4.4.m3.1.1.1" xref="Thmdefinition1.p1.4.4.m3.1.1.1.cmml">≥</mo><mn id="Thmdefinition1.p1.4.4.m3.1.1.3" xref="Thmdefinition1.p1.4.4.m3.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.4.4.m3.1b"><apply id="Thmdefinition1.p1.4.4.m3.1.1.cmml" xref="Thmdefinition1.p1.4.4.m3.1.1"><geq id="Thmdefinition1.p1.4.4.m3.1.1.1.cmml" xref="Thmdefinition1.p1.4.4.m3.1.1.1"></geq><ci id="Thmdefinition1.p1.4.4.m3.1.1.2.cmml" xref="Thmdefinition1.p1.4.4.m3.1.1.2">𝜀</ci><cn id="Thmdefinition1.p1.4.4.m3.1.1.3.cmml" type="integer" xref="Thmdefinition1.p1.4.4.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.4.4.m3.1c">\varepsilon\geq 0</annotation><annotation encoding="application/x-llamapun" id="Thmdefinition1.p1.4.4.m3.1d">italic_ε ≥ 0</annotation></semantics></math> and <math alttext="\delta\in[0,1]" class="ltx_Math" display="inline" id="Thmdefinition1.p1.5.5.m4.2"><semantics id="Thmdefinition1.p1.5.5.m4.2a"><mrow id="Thmdefinition1.p1.5.5.m4.2.3" xref="Thmdefinition1.p1.5.5.m4.2.3.cmml"><mi id="Thmdefinition1.p1.5.5.m4.2.3.2" xref="Thmdefinition1.p1.5.5.m4.2.3.2.cmml">δ</mi><mo id="Thmdefinition1.p1.5.5.m4.2.3.1" xref="Thmdefinition1.p1.5.5.m4.2.3.1.cmml">∈</mo><mrow id="Thmdefinition1.p1.5.5.m4.2.3.3.2" xref="Thmdefinition1.p1.5.5.m4.2.3.3.1.cmml"><mo id="Thmdefinition1.p1.5.5.m4.2.3.3.2.1" stretchy="false" xref="Thmdefinition1.p1.5.5.m4.2.3.3.1.cmml">[</mo><mn id="Thmdefinition1.p1.5.5.m4.1.1" xref="Thmdefinition1.p1.5.5.m4.1.1.cmml">0</mn><mo id="Thmdefinition1.p1.5.5.m4.2.3.3.2.2" xref="Thmdefinition1.p1.5.5.m4.2.3.3.1.cmml">,</mo><mn id="Thmdefinition1.p1.5.5.m4.2.2" xref="Thmdefinition1.p1.5.5.m4.2.2.cmml">1</mn><mo id="Thmdefinition1.p1.5.5.m4.2.3.3.2.3" stretchy="false" xref="Thmdefinition1.p1.5.5.m4.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.5.5.m4.2b"><apply id="Thmdefinition1.p1.5.5.m4.2.3.cmml" xref="Thmdefinition1.p1.5.5.m4.2.3"><in id="Thmdefinition1.p1.5.5.m4.2.3.1.cmml" xref="Thmdefinition1.p1.5.5.m4.2.3.1"></in><ci id="Thmdefinition1.p1.5.5.m4.2.3.2.cmml" xref="Thmdefinition1.p1.5.5.m4.2.3.2">𝛿</ci><interval closure="closed" id="Thmdefinition1.p1.5.5.m4.2.3.3.1.cmml" xref="Thmdefinition1.p1.5.5.m4.2.3.3.2"><cn id="Thmdefinition1.p1.5.5.m4.1.1.cmml" type="integer" xref="Thmdefinition1.p1.5.5.m4.1.1">0</cn><cn id="Thmdefinition1.p1.5.5.m4.2.2.cmml" type="integer" xref="Thmdefinition1.p1.5.5.m4.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.5.5.m4.2c">\delta\in[0,1]</annotation><annotation encoding="application/x-llamapun" id="Thmdefinition1.p1.5.5.m4.2d">italic_δ ∈ [ 0 , 1 ]</annotation></semantics></math>, hyperparameters <math alttext="hp\in\mathcal{H}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.6.6.m5.1"><semantics id="Thmdefinition1.p1.6.6.m5.1a"><mrow id="Thmdefinition1.p1.6.6.m5.1.1" xref="Thmdefinition1.p1.6.6.m5.1.1.cmml"><mrow id="Thmdefinition1.p1.6.6.m5.1.1.2" xref="Thmdefinition1.p1.6.6.m5.1.1.2.cmml"><mi id="Thmdefinition1.p1.6.6.m5.1.1.2.2" xref="Thmdefinition1.p1.6.6.m5.1.1.2.2.cmml">h</mi><mo id="Thmdefinition1.p1.6.6.m5.1.1.2.1" xref="Thmdefinition1.p1.6.6.m5.1.1.2.1.cmml">⁢</mo><mi id="Thmdefinition1.p1.6.6.m5.1.1.2.3" xref="Thmdefinition1.p1.6.6.m5.1.1.2.3.cmml">p</mi></mrow><mo id="Thmdefinition1.p1.6.6.m5.1.1.1" xref="Thmdefinition1.p1.6.6.m5.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.6.6.m5.1.1.3" xref="Thmdefinition1.p1.6.6.m5.1.1.3.cmml">ℋ</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.6.6.m5.1b"><apply id="Thmdefinition1.p1.6.6.m5.1.1.cmml" xref="Thmdefinition1.p1.6.6.m5.1.1"><in id="Thmdefinition1.p1.6.6.m5.1.1.1.cmml" xref="Thmdefinition1.p1.6.6.m5.1.1.1"></in><apply id="Thmdefinition1.p1.6.6.m5.1.1.2.cmml" xref="Thmdefinition1.p1.6.6.m5.1.1.2"><times id="Thmdefinition1.p1.6.6.m5.1.1.2.1.cmml" xref="Thmdefinition1.p1.6.6.m5.1.1.2.1"></times><ci id="Thmdefinition1.p1.6.6.m5.1.1.2.2.cmml" xref="Thmdefinition1.p1.6.6.m5.1.1.2.2">ℎ</ci><ci id="Thmdefinition1.p1.6.6.m5.1.1.2.3.cmml" xref="Thmdefinition1.p1.6.6.m5.1.1.2.3">𝑝</ci></apply><ci id="Thmdefinition1.p1.6.6.m5.1.1.3.cmml" xref="Thmdefinition1.p1.6.6.m5.1.1.3">ℋ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.6.6.m5.1c">hp\in\mathcal{H}</annotation><annotation encoding="application/x-llamapun" id="Thmdefinition1.p1.6.6.m5.1d">italic_h italic_p ∈ caligraphic_H</annotation></semantics></math>, and events <math alttext="E\subseteq\mathcal{Y}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.7.7.m6.1"><semantics id="Thmdefinition1.p1.7.7.m6.1a"><mrow id="Thmdefinition1.p1.7.7.m6.1.1" xref="Thmdefinition1.p1.7.7.m6.1.1.cmml"><mi id="Thmdefinition1.p1.7.7.m6.1.1.2" xref="Thmdefinition1.p1.7.7.m6.1.1.2.cmml">E</mi><mo id="Thmdefinition1.p1.7.7.m6.1.1.1" xref="Thmdefinition1.p1.7.7.m6.1.1.1.cmml">⊆</mo><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.7.7.m6.1.1.3" xref="Thmdefinition1.p1.7.7.m6.1.1.3.cmml">𝒴</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.7.7.m6.1b"><apply id="Thmdefinition1.p1.7.7.m6.1.1.cmml" xref="Thmdefinition1.p1.7.7.m6.1.1"><subset id="Thmdefinition1.p1.7.7.m6.1.1.1.cmml" xref="Thmdefinition1.p1.7.7.m6.1.1.1"></subset><ci id="Thmdefinition1.p1.7.7.m6.1.1.2.cmml" xref="Thmdefinition1.p1.7.7.m6.1.1.2">𝐸</ci><ci id="Thmdefinition1.p1.7.7.m6.1.1.3.cmml" xref="Thmdefinition1.p1.7.7.m6.1.1.3">𝒴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.7.7.m6.1c">E\subseteq\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="Thmdefinition1.p1.7.7.m6.1d">italic_E ⊆ caligraphic_Y</annotation></semantics></math>,</span></p>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\Pr[\mathcal{M}(d,\varepsilon,\delta,hp)\in E]\leq e^{\varepsilon}\cdot\Pr[%
\mathcal{M}(d^{\prime},\varepsilon,\delta,hp)\in E]+\delta," class="ltx_Math" display="block" id="S2.Ex1.m1.8"><semantics id="S2.Ex1.m1.8a"><mrow id="S2.Ex1.m1.8.8.1" xref="S2.Ex1.m1.8.8.1.1.cmml"><mrow id="S2.Ex1.m1.8.8.1.1" xref="S2.Ex1.m1.8.8.1.1.cmml"><mrow id="S2.Ex1.m1.8.8.1.1.1.1" xref="S2.Ex1.m1.8.8.1.1.1.2.cmml"><mi id="S2.Ex1.m1.4.4" xref="S2.Ex1.m1.4.4.cmml">Pr</mi><mo id="S2.Ex1.m1.8.8.1.1.1.1a" xref="S2.Ex1.m1.8.8.1.1.1.2.cmml">⁡</mo><mrow id="S2.Ex1.m1.8.8.1.1.1.1.1" xref="S2.Ex1.m1.8.8.1.1.1.2.cmml"><mo id="S2.Ex1.m1.8.8.1.1.1.1.1.2" stretchy="false" xref="S2.Ex1.m1.8.8.1.1.1.2.cmml">[</mo><mrow id="S2.Ex1.m1.8.8.1.1.1.1.1.1" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.cmml"><mrow id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.3.cmml">ℳ</mi><mo id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.2" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.2.cmml"><mo id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.2.cmml">(</mo><mi id="S2.Ex1.m1.1.1" xref="S2.Ex1.m1.1.1.cmml">d</mi><mo id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S2.Ex1.m1.2.2" xref="S2.Ex1.m1.2.2.cmml">ε</mi><mo id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.4" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S2.Ex1.m1.3.3" xref="S2.Ex1.m1.3.3.cmml">δ</mi><mo id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.5" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.2.cmml">,</mo><mrow id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.1" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.1.2" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.cmml">h</mi><mo id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.1.3.cmml">p</mi></mrow><mo id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.6" stretchy="false" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.Ex1.m1.8.8.1.1.1.1.1.1.2" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.2.cmml">∈</mo><mi id="S2.Ex1.m1.8.8.1.1.1.1.1.1.3" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.3.cmml">E</mi></mrow><mo id="S2.Ex1.m1.8.8.1.1.1.1.1.3" stretchy="false" xref="S2.Ex1.m1.8.8.1.1.1.2.cmml">]</mo></mrow></mrow><mo id="S2.Ex1.m1.8.8.1.1.3" xref="S2.Ex1.m1.8.8.1.1.3.cmml">≤</mo><mrow id="S2.Ex1.m1.8.8.1.1.2" xref="S2.Ex1.m1.8.8.1.1.2.cmml"><mrow id="S2.Ex1.m1.8.8.1.1.2.1" xref="S2.Ex1.m1.8.8.1.1.2.1.cmml"><msup id="S2.Ex1.m1.8.8.1.1.2.1.3" xref="S2.Ex1.m1.8.8.1.1.2.1.3.cmml"><mi id="S2.Ex1.m1.8.8.1.1.2.1.3.2" xref="S2.Ex1.m1.8.8.1.1.2.1.3.2.cmml">e</mi><mi id="S2.Ex1.m1.8.8.1.1.2.1.3.3" xref="S2.Ex1.m1.8.8.1.1.2.1.3.3.cmml">ε</mi></msup><mo id="S2.Ex1.m1.8.8.1.1.2.1.2" lspace="0.222em" rspace="0.222em" xref="S2.Ex1.m1.8.8.1.1.2.1.2.cmml">⋅</mo><mrow id="S2.Ex1.m1.8.8.1.1.2.1.1.1" xref="S2.Ex1.m1.8.8.1.1.2.1.1.2.cmml"><mi id="S2.Ex1.m1.7.7" xref="S2.Ex1.m1.7.7.cmml">Pr</mi><mo id="S2.Ex1.m1.8.8.1.1.2.1.1.1a" xref="S2.Ex1.m1.8.8.1.1.2.1.1.2.cmml">⁡</mo><mrow id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1" xref="S2.Ex1.m1.8.8.1.1.2.1.1.2.cmml"><mo id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.2" stretchy="false" xref="S2.Ex1.m1.8.8.1.1.2.1.1.2.cmml">[</mo><mrow id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.cmml"><mrow id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.4" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.4.cmml">ℳ</mi><mo id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.3" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.3.cmml">⁢</mo><mrow id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.3.cmml"><mo id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.3" stretchy="false" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.3.cmml">(</mo><msup id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.2" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.2.cmml">d</mi><mo id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.3.cmml">′</mo></msup><mo id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.4" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.3.cmml">,</mo><mi id="S2.Ex1.m1.5.5" xref="S2.Ex1.m1.5.5.cmml">ε</mi><mo id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.5" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.3.cmml">,</mo><mi id="S2.Ex1.m1.6.6" xref="S2.Ex1.m1.6.6.cmml">δ</mi><mo id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.6" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.3.cmml">,</mo><mrow id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.2" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.2.cmml"><mi id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.2.2" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.2.2.cmml">h</mi><mo id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.2.1" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.2.1.cmml">⁢</mo><mi id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.2.3" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.2.3.cmml">p</mi></mrow><mo id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.7" stretchy="false" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.3" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.3.cmml">∈</mo><mi id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.4" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.4.cmml">E</mi></mrow><mo id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.3" stretchy="false" xref="S2.Ex1.m1.8.8.1.1.2.1.1.2.cmml">]</mo></mrow></mrow></mrow><mo id="S2.Ex1.m1.8.8.1.1.2.2" xref="S2.Ex1.m1.8.8.1.1.2.2.cmml">+</mo><mi id="S2.Ex1.m1.8.8.1.1.2.3" xref="S2.Ex1.m1.8.8.1.1.2.3.cmml">δ</mi></mrow></mrow><mo id="S2.Ex1.m1.8.8.1.2" xref="S2.Ex1.m1.8.8.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.8b"><apply id="S2.Ex1.m1.8.8.1.1.cmml" xref="S2.Ex1.m1.8.8.1"><leq id="S2.Ex1.m1.8.8.1.1.3.cmml" xref="S2.Ex1.m1.8.8.1.1.3"></leq><apply id="S2.Ex1.m1.8.8.1.1.1.2.cmml" xref="S2.Ex1.m1.8.8.1.1.1.1"><ci id="S2.Ex1.m1.4.4.cmml" xref="S2.Ex1.m1.4.4">Pr</ci><apply id="S2.Ex1.m1.8.8.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1"><in id="S2.Ex1.m1.8.8.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.2"></in><apply id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1"><times id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.2"></times><ci id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.3">ℳ</ci><vector id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1"><ci id="S2.Ex1.m1.1.1.cmml" xref="S2.Ex1.m1.1.1">𝑑</ci><ci id="S2.Ex1.m1.2.2.cmml" xref="S2.Ex1.m1.2.2">𝜀</ci><ci id="S2.Ex1.m1.3.3.cmml" xref="S2.Ex1.m1.3.3">𝛿</ci><apply id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.1"><times id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1"></times><ci id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.1.2">ℎ</ci><ci id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.1.1.3">𝑝</ci></apply></vector></apply><ci id="S2.Ex1.m1.8.8.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.3">𝐸</ci></apply></apply><apply id="S2.Ex1.m1.8.8.1.1.2.cmml" xref="S2.Ex1.m1.8.8.1.1.2"><plus id="S2.Ex1.m1.8.8.1.1.2.2.cmml" xref="S2.Ex1.m1.8.8.1.1.2.2"></plus><apply id="S2.Ex1.m1.8.8.1.1.2.1.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1"><ci id="S2.Ex1.m1.8.8.1.1.2.1.2.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.2">⋅</ci><apply id="S2.Ex1.m1.8.8.1.1.2.1.3.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.8.8.1.1.2.1.3.1.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.3">superscript</csymbol><ci id="S2.Ex1.m1.8.8.1.1.2.1.3.2.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.3.2">𝑒</ci><ci id="S2.Ex1.m1.8.8.1.1.2.1.3.3.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.3.3">𝜀</ci></apply><apply id="S2.Ex1.m1.8.8.1.1.2.1.1.2.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1"><ci id="S2.Ex1.m1.7.7.cmml" xref="S2.Ex1.m1.7.7">Pr</ci><apply id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1"><in id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.3"></in><apply id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2"><times id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.3.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.3"></times><ci id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.4.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.4">ℳ</ci><vector id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.3.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2"><apply id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.2">𝑑</ci><ci id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.1.1.1.1.3">′</ci></apply><ci id="S2.Ex1.m1.5.5.cmml" xref="S2.Ex1.m1.5.5">𝜀</ci><ci id="S2.Ex1.m1.6.6.cmml" xref="S2.Ex1.m1.6.6">𝛿</ci><apply id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.2.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.2"><times id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.2.1.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.2.1"></times><ci id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.2.2.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.2.2">ℎ</ci><ci id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.2.3.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.2.2.2.2.3">𝑝</ci></apply></vector></apply><ci id="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.4.cmml" xref="S2.Ex1.m1.8.8.1.1.2.1.1.1.1.1.4">𝐸</ci></apply></apply></apply><ci id="S2.Ex1.m1.8.8.1.1.2.3.cmml" xref="S2.Ex1.m1.8.8.1.1.2.3">𝛿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.8c">\Pr[\mathcal{M}(d,\varepsilon,\delta,hp)\in E]\leq e^{\varepsilon}\cdot\Pr[%
\mathcal{M}(d^{\prime},\varepsilon,\delta,hp)\in E]+\delta,</annotation><annotation encoding="application/x-llamapun" id="S2.Ex1.m1.8d">roman_Pr [ caligraphic_M ( italic_d , italic_ε , italic_δ , italic_h italic_p ) ∈ italic_E ] ≤ italic_e start_POSTSUPERSCRIPT italic_ε end_POSTSUPERSCRIPT ⋅ roman_Pr [ caligraphic_M ( italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , italic_ε , italic_δ , italic_h italic_p ) ∈ italic_E ] + italic_δ ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="Thmdefinition1.p1.8"><span class="ltx_text ltx_font_italic" id="Thmdefinition1.p1.8.1">where all probabilities are taken over the random coins of <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="Thmdefinition1.p1.8.1.m1.1"><semantics id="Thmdefinition1.p1.8.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="Thmdefinition1.p1.8.1.m1.1.1" xref="Thmdefinition1.p1.8.1.m1.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="Thmdefinition1.p1.8.1.m1.1b"><ci id="Thmdefinition1.p1.8.1.m1.1.1.cmml" xref="Thmdefinition1.p1.8.1.m1.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdefinition1.p1.8.1.m1.1c">\mathcal{M}</annotation><annotation encoding="application/x-llamapun" id="Thmdefinition1.p1.8.1.m1.1d">caligraphic_M</annotation></semantics></math>.</span></p>
</div>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">DP is an elegant theoretical formulation of privacy. It can account for current and future attacks, measure compositions of privacy loss over multiple data releases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib42" title="">42</a>]</cite>, and enable third-party scrutiny of the algorithm. This makes it possible for a data analyst to take into account the noise introduced when performing inference and estimating uncertainty through confidence intervals.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">Over the past two decades, disclosure methods that satisfy DP have been adopted by a variety of data-collection agencies and institutions, including Apple <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib60" title="">60</a>]</cite>, Google <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib46" title="">46</a>]</cite>, Uber <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib87" title="">87</a>]</cite>, the U.S. Census Bureau <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib74" title="">74</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib2" title="">2</a>]</cite>, GovTech Singapore <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib108" title="">108</a>]</cite>, the Wikimedia Foundation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib6" title="">6</a>]</cite>, and the Israeli Ministry of Health <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib64" title="">64</a>]</cite>.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>For a more comprehensive list of deployments, see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib34" title="">34</a>]</cite>.</span></span></span> In the last several years, we have also seen an emergence of open-source software libraries for DP data analysis, including DiffPrivLib <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib65" title="">65</a>]</cite>, OpenDP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib52" title="">52</a>]</cite>, and Tumult Analytics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib14" title="">14</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">While these tools and deployments are promising, they have also made clear the difficulties of using DP in practice. We detail these challenges throughout the rest of the paper.
DP is no longer a new field—there has been a tremendous amount of work around DP algorithm design and statistical inference—but most of the literature has been focused on theoretical aspects. Only recently have scholars turned their attention to practical dimensions of using DP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib102" title="">102</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib83" title="">83</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib116" title="">116</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib88" title="">88</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib89" title="">89</a>]</cite>, and research on these translational aspects are still in early stages.
In particular, there is a lack of consensus around policies and best practices of using DP across areas such as assessment of risk and incentives, communication, design, use, evaluation, and governance.
This paper aims to provide an overview of these understudied areas and develop a roadmap towards improving the practice of DP.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Overview of findings</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">We approach this paper as two computer scientists with over 15 years of experience, collectively, with the research and practice of DP. Our findings draw heavily on our professional experiences and expertise, as well as a review of the literature on making DP usable in practice. Given our background, our findings are oriented towards the advancement of DP systems; however, we recognize that DP is not always the best approach, and we advocate for understanding the limits of DP and researching alternatives when relevant.
Our overarching commitment is to privacy as a holistic, multifaceted concept. Our findings are also colored by our position as computer scientists; we regularly interact with data subjects, engineers, privacy officers, data curators, data analysts, executives, lawyers, and policymakers, but we are not experts in these areas.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">In this paper, we identify five important categories of concerns we must address to make DP usable in practice, now and in the future. These are: considerations of use; communication; design and policy; practice; and trust and governance. For each of these categories, we discuss challenges for researchers and practitioners, as well as some suggested directions for further inquiry and practical adoption. A summary of our findings are outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S2.T1" title="Table 1 ‣ 2.1 Overview of findings ‣ 2 Background and Overview ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_tag">1</span></a>.
The last column of the table provides a brief assessment of how far along the community is in each of these categories. These categorizations are not meant to be exhaustive, but rather to serve as a starting point for future work in usable DP.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_ll ltx_border_r ltx_border_t" id="S2.T1.1.2.1.1" style="padding-bottom:2.15277pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.1.1.1">
<span class="ltx_p" id="S2.T1.1.2.1.1.1.1" style="width:68.3pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.2.1.1.1.1.1">Category</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S2.T1.1.2.1.2" style="padding-bottom:2.15277pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.1.2.1">
<span class="ltx_p" id="S2.T1.1.2.1.2.1.1" style="width:128.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.2.1.2.1.1.1">Challenges</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.1.2.1.3" style="padding-bottom:2.15277pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.1.3.1">
<span class="ltx_p" id="S2.T1.1.2.1.3.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.2.1.3.1.1.1">Recommendations</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_rr ltx_border_t" id="S2.T1.1.2.1.4" style="padding-bottom:2.15277pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.2.1.4.1">
<span class="ltx_p" id="S2.T1.1.2.1.4.1.1" style="width:76.8pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.2.1.4.1.1.1">State of the Field</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.3.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_ll ltx_border_r ltx_border_tt" id="S2.T1.1.3.1.1" rowspan="4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.3.1.1.1">
<span class="ltx_p" id="S2.T1.1.3.1.1.1.1" style="width:68.3pt;"><span class="ltx_text" id="S2.T1.1.3.1.1.1.1.1">Considerations of Use (§ <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S3" title="3 Considering when to use DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_tag">3</span></a>)</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S2.T1.1.3.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.3.1.2.1">
<span class="ltx_p" id="S2.T1.1.3.1.2.1.1" style="width:128.0pt;">Assessing risk and privacy threats</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" id="S2.T1.1.3.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.3.1.3.1">
<span class="ltx_p" id="S2.T1.1.3.1.3.1.1" style="width:199.2pt;">Developing risk frameworks that align mathematical guarantees and user preferences</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_rr ltx_border_tt" id="S2.T1.1.3.1.4" rowspan="4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.3.1.4.1">
<span class="ltx_p" id="S2.T1.1.3.1.4.1.1" style="width:76.8pt;"><span class="ltx_text" id="S2.T1.1.3.1.4.1.1.1">Requires shift in research agendas towards centering contextual analyses and stakeholder needs</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.4.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.4.2.1.1">
<span class="ltx_p" id="S2.T1.1.4.2.1.1.1" style="width:128.0pt;">Understanding contextual needs</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T1.1.4.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.4.2.2.1">
<span class="ltx_p" id="S2.T1.1.4.2.2.1.1" style="width:199.2pt;">Assessing informational norms and using DP attentively to context</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.5.3.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.5.3.1.1">
<span class="ltx_p" id="S2.T1.1.5.3.1.1.1" style="width:128.0pt;">Aligning DP and institutional incentives</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T1.1.5.3.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.5.3.2.1">
<span class="ltx_p" id="S2.T1.1.5.3.2.1.1" style="width:199.2pt;">Positioning privacy as a positive good, while resisting privacy-washing</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.6.4.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.6.4.1.1">
<span class="ltx_p" id="S2.T1.1.6.4.1.1.1" style="width:128.0pt;">Integrating DP with other privacy practices</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T1.1.6.4.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.6.4.2.1">
<span class="ltx_p" id="S2.T1.1.6.4.2.1.1" style="width:199.2pt;">Combining DP with cryptographic primitives and privacy assessments</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.7.5">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_ll ltx_border_r ltx_border_t" id="S2.T1.1.7.5.1" rowspan="3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.7.5.1.1">
<span class="ltx_p" id="S2.T1.1.7.5.1.1.1" style="width:68.3pt;"><span class="ltx_text" id="S2.T1.1.7.5.1.1.1.1">Communication (§ <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S4" title="4 Communicating around DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_tag">4</span></a>)</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.7.5.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.7.5.2.1">
<span class="ltx_p" id="S2.T1.1.7.5.2.1.1" style="width:128.0pt;">Tailoring communications for different parties</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.7.5.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.7.5.3.1">
<span class="ltx_p" id="S2.T1.1.7.5.3.1.1" style="width:199.2pt;">Understanding and addressing the needs of: data subjects, engineers, privacy officers, data curators, data analysts, lawyers, policymakers, and executives</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_rr ltx_border_t" id="S2.T1.1.7.5.4" rowspan="3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.7.5.4.1">
<span class="ltx_p" id="S2.T1.1.7.5.4.1.1" style="width:76.8pt;"><span class="ltx_text" id="S2.T1.1.7.5.4.1.1.1">Promising initial research; future work should dive deeper into these questions</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.8.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.8.6.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.8.6.1.1">
<span class="ltx_p" id="S2.T1.1.8.6.1.1.1" style="width:128.0pt;">Transparency traps</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T1.1.8.6.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.8.6.2.1">
<span class="ltx_p" id="S2.T1.1.8.6.2.1.1" style="width:199.2pt;">Creating structures for making sense of technical transparency</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.1">
<span class="ltx_p" id="S2.T1.1.1.1.1.1" style="width:128.0pt;">Engaging beyond <math alttext="\epsilon" class="ltx_align_left" display="inline" id="S2.T1.1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.1.m1.1a"><mi id="S2.T1.1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.1.m1.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.1.m1.1d">italic_ϵ</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T1.1.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.2.1">
<span class="ltx_p" id="S2.T1.1.1.2.1.1" style="width:199.2pt;">Communicating about <em class="ltx_emph ltx_font_italic" id="S2.T1.1.1.2.1.1.1">all</em> the parameters and implementation choices involved in deploying DP</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.9.7">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_ll ltx_border_r ltx_border_t" id="S2.T1.1.9.7.1" rowspan="3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.9.7.1.1">
<span class="ltx_p" id="S2.T1.1.9.7.1.1.1" style="width:68.3pt;"><span class="ltx_text" id="S2.T1.1.9.7.1.1.1.1">Design and Policy (§ <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S5" title="5 Design and policy choices ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_tag">5</span></a>)</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.9.7.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.9.7.2.1">
<span class="ltx_p" id="S2.T1.1.9.7.2.1.1" style="width:128.0pt;">Choosing definition and units</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.9.7.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.9.7.3.1">
<span class="ltx_p" id="S2.T1.1.9.7.3.1.1" style="width:199.2pt;">Developing guidance on translating risk assessments to these choices</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_rr ltx_border_t" id="S2.T1.1.9.7.4" rowspan="3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.9.7.4.1">
<span class="ltx_p" id="S2.T1.1.9.7.4.1.1" style="width:76.8pt;"><span class="ltx_text" id="S2.T1.1.9.7.4.1.1.1">Need to translate theoretical research into practical, actionable guidance</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.10.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.10.8.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.10.8.1.1">
<span class="ltx_p" id="S2.T1.1.10.8.1.1.1" style="width:128.0pt;">Privacy-loss parameters</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T1.1.10.8.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.10.8.2.1">
<span class="ltx_p" id="S2.T1.1.10.8.2.1.1" style="width:199.2pt;">Modeling impact of parameters, testing explanations, creating databases, and translating to real-world risk</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.11.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.11.9.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.11.9.1.1">
<span class="ltx_p" id="S2.T1.1.11.9.1.1.1" style="width:128.0pt;">Designing entire pipelines for DP</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T1.1.11.9.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.11.9.2.1">
<span class="ltx_p" id="S2.T1.1.11.9.2.1.1" style="width:199.2pt;">Considering impact of choices for sampling and editing data</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.12.10">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_ll ltx_border_r ltx_border_t" id="S2.T1.1.12.10.1" rowspan="3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.12.10.1.1">
<span class="ltx_p" id="S2.T1.1.12.10.1.1.1" style="width:68.3pt;"><span class="ltx_text" id="S2.T1.1.12.10.1.1.1.1">Practice (§ <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S6" title="6 Practice of DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_tag">6</span></a>)</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.12.10.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.12.10.2.1">
<span class="ltx_p" id="S2.T1.1.12.10.2.1.1" style="width:128.0pt;">Facilitating data processing and exploration</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.12.10.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.12.10.3.1">
<span class="ltx_p" id="S2.T1.1.12.10.3.1.1" style="width:199.2pt;">Creating best practices for dataset annotation and researching strategies for exploration</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_rr ltx_border_t" id="S2.T1.1.12.10.4" rowspan="3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.12.10.4.1">
<span class="ltx_p" id="S2.T1.1.12.10.4.1.1" style="width:76.8pt;"><span class="ltx_text" id="S2.T1.1.12.10.4.1.1.1">Promising set of tools to build on, but requires more attention and investment</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.13.11">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.13.11.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.13.11.1.1">
<span class="ltx_p" id="S2.T1.1.13.11.1.1.1" style="width:128.0pt;">Choosing metadata parameters</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T1.1.13.11.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.13.11.2.1">
<span class="ltx_p" id="S2.T1.1.13.11.2.1.1" style="width:199.2pt;">Building algorithmic and procedural toolkit for choosing parameters with limited domain knowledge</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.14.12">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.14.12.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.14.12.1.1">
<span class="ltx_p" id="S2.T1.1.14.12.1.1.1" style="width:128.0pt;">Tools for using and evaluating data products</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T1.1.14.12.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.14.12.2.1">
<span class="ltx_p" id="S2.T1.1.14.12.2.1.1" style="width:199.2pt;">Investing in user interfaces, software libraries, and visualization tools</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.15.13">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_ll ltx_border_r ltx_border_t" id="S2.T1.1.15.13.1" rowspan="3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.15.13.1.1">
<span class="ltx_p" id="S2.T1.1.15.13.1.1.1" style="width:68.3pt;"><span class="ltx_text" id="S2.T1.1.15.13.1.1.1.1">Trust and Governance (§ <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S7" title="7 Trust and Governance of DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_tag">7</span></a>)</span></span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.15.13.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.15.13.2.1">
<span class="ltx_p" id="S2.T1.1.15.13.2.1.1" style="width:128.0pt;">Stakeholder engagement and oversight</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.15.13.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.15.13.3.1">
<span class="ltx_p" id="S2.T1.1.15.13.3.1.1" style="width:199.2pt;">Understanding how to meaningfully engage with stakeholders while lowering barriers for deployment</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_rr ltx_border_t" id="S2.T1.1.15.13.4" rowspan="3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.15.13.4.1">
<span class="ltx_p" id="S2.T1.1.15.13.4.1.1" style="width:76.8pt;"><span class="ltx_text" id="S2.T1.1.15.13.4.1.1.1">In very early stages; requires collaborative efforts and collective commitments</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.16.14">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S2.T1.1.16.14.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.16.14.1.1">
<span class="ltx_p" id="S2.T1.1.16.14.1.1.1" style="width:128.0pt;">Documenting decisions</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S2.T1.1.16.14.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.16.14.2.1">
<span class="ltx_p" id="S2.T1.1.16.14.2.1.1" style="width:199.2pt;">Building transparency and competitive pressure regarding deployment choices</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.17.15">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S2.T1.1.17.15.1" style="padding-bottom:4.30554pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.17.15.1.1">
<span class="ltx_p" id="S2.T1.1.17.15.1.1.1" style="width:128.0pt;">Auditing deployments</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S2.T1.1.17.15.2" style="padding-bottom:4.30554pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.17.15.2.1">
<span class="ltx_p" id="S2.T1.1.17.15.2.1.1" style="width:199.2pt;">Enabling algorithmic and procedural audits</span>
</span>
</td>
</tr>
</tbody>
</table>
<br class="ltx_break"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Summary of challenges, recommendations, and state of the field for using DP in practice</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Considering when to use DP</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Before designing deployments of DP, organizations must first consider whether this technology is warranted and beneficial for their specific contexts and use-cases. This step is fraught with complexities around assessing risk, understanding practical privacy needs, considering institutional incentives, and integrating DP with other privacy tools.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Assessing risk and privacy threats</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Building meaningful privacy solutions start with assessing privacy risks, but different communities understand the risks from releasing statistical information in vastly different ways. As researchers, we need to begin by understanding the types of risks and attacks that are important to stakeholders, and then analyze whether DP addresses these threats. This is a fundamental challenge because the framing of differential privacy corresponds to a particular understanding of privacy attacks as <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.1">relative</em>, <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.2">individual-focused</em>, and <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.3">worst-case</em>, an understanding that may not be shared by all stakeholders, and one that represents a fairly recent shift in computer scientists’ understanding of privacy attacks on statistical releases.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">First, the DP guarantee considers <em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.1.1">relative privacy risk</em>, or the probability that an adversary can guess an individual’s sensitive information based on a statistical release compared to their chance of doing so if that individual was not included in the dataset at all.
This baseline helps distinguish the informational impact of a given release from population-level inferences that could be made without access to the data, which is important for risk assessment both in computer science and law <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib8" title="">8</a>]</cite>.
Stakeholders, however, may be more concerned with <em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.1.2">absolute privacy risk</em>, or the probability that the adversary can guess an individual’s sensitive information regardless of the baseline success of the adversary when the individual is not included in the dataset. This discrepancy leads to disagreement about what constitutes effective privacy protection. For example, stakeholders have debated whether the use of aggregated census data to accurately infer an individual’s race given their surname constitutes a privacy violation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib69" title="">69</a>]</cite>.
DP advocates would say no, as this inference could be made based on public knowledge of the statistical relationship between race and surnames.
Others might say that because of the potentially sensitive information that can be inferred about individuals, this is still a privacy violation—albeit one that is hard to protect against. Bridging these perspectives is critical for moving forward.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Second, the DP guarantee primarily considers the privacy of individuals or small groups.
Some DP experts have stated that population-level statistical inferences are categorically <em class="ltx_emph ltx_font_italic" id="S3.SS1.p3.1.1">not</em> privacy violations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib19" title="">19</a>]</cite>.
However, this does not take into account that privacy is also relational and collective, as described by Viljoen as “horizontal data relations” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib110" title="">110</a>]</cite> or by Barocas and Levy as “privacy dependencies” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib12" title="">12</a>]</cite>. Extensions to the DP definition such as <em class="ltx_emph ltx_font_italic" id="S3.SS1.p3.1.2">attribute privacy</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib119" title="">119</a>]</cite> have started to explore these directions; attribute privacy protects ‘global properties’ of a dataset, such as the mortality rate of an entire hospital, as well as parameters of the distribution from which the dataset is drawn <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib119" title="">119</a>]</cite>.
Other DP experts take a more moderate view, maintaining that while population inferences can be significant privacy risks, DP is still useful in that it allows us to <em class="ltx_emph ltx_font_italic" id="S3.SS1.p3.1.3">separately</em> analyze individual and collective privacy risks and provide adequate protection to the former <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib115" title="">115</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">Third, DP provides guarantees of protection against a <em class="ltx_emph ltx_font_italic" id="S3.SS1.p4.1.1">worst-case</em> adversary—one that can have complete knowledge of the dataset except for one individual’s contribution. The DP guarantee ensures that even an adversary with this high level of knowledge about the dataset can gain only limited knowledge about the unknown individual.
However, this guarantee is often too strong for the needs of stakeholders. Having such a strong guarantee comes at a cost; it is harder to satisfy while providing good accuracy. Stakeholders of DP deployments have argued that this theoretical worst-case notion of risk does not comport with their experience of real-world risks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib99" title="">99</a>]</cite>. Stakeholders may be more interested in the protections offered to each individual if we assume the adversary only has limited knowledge of the dataset.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">Advocates of DP point out that given the rise in availability of data sources and computational power, any assumptions about limits to the adversary’s knowledge may not stand the test of time <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib114" title="">114</a>]</cite>.
But framing threats, risks, and motivation in these ways does not seem to connect with stakeholders who are used to thinking about privacy threats differently <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib99" title="">99</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib69" title="">69</a>]</cite>.
In addition, the conservative guarantee of DP does not mean that it is a guarantee of the right ‘type’; using DP does not always provide insight into how well protected the dataset will be against the type of attacks that stakeholders <em class="ltx_emph ltx_font_italic" id="S3.SS1.p5.1.1">are</em> worried about. These concerns will be specific to the application and context, but could include: absolute risk, group privacy, or even privacy concerns outside the scope of DP such as sharing data with third parties. Conversely, the protections that are within the scope of stakeholders’ toolkits (e.g., heuristic approaches such as data suppression or rounding) are often too narrow and do not subsume the rigorous protections promised by DP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib8" title="">8</a>]</cite>.
This is a major gap that needs to be addressed.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1">Given the fundamental differences in how DP frames risks relative to other understandings of privacy threats, a critical area for future research is around bridging these gaps. One avenue is to develop frameworks for assessing privacy threats and attacks that are quantifiable, rigorous, and attentive to future risks, but that are also meaningful to the particular context and goals of stakeholders.
This is a challenging because of the space of parameters and choices specifying the risks that come from attacks will depend on the information and resources available to the adversary, the goals of the adversary, and other application-specific details <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib29" title="">29</a>]</cite>.
Collapsing these parameters into a single quantitative measure may not be informative, desirable, or even possible. Yet, the field would benefit from outlining the types of attacks that are important to stakeholders in each context and determine how DP addresses, or fails to addresses, these risks.</p>
</div>
<div class="ltx_para" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.1">Additionally, research around formal privacy should give more attention to collective notions of privacy and the threats of inference beyond risks to individual privacy. As mentioned above, recent work has focused on applying a DP-like approach to different privacy units beyond individuals <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib119" title="">119</a>]</cite>, and we believe such work is extremely important and fruitful.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Understanding contextual needs</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Privacy is a broad, essentially contested concept <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib82" title="">82</a>]</cite>. It is only meaningful when considered in its social, legal, technical, and political context <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib90" title="">90</a>]</cite>. DP, on the other hand, is defined in a way that is agnostic to social context <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib13" title="">13</a>]</cite>. It it is well-aligned with the aims of statistical disclosure limitation and enables concrete conversations around limiting privacy risks, but
its algorithmic formalisms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib59" title="">59</a>]</cite> are not grounded in or attentive to its sociotechnical environment.
While the abstract nature of DP facilitates theoretical analysis, it also creates a significant barrier to the deployment of DP in practice.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">An open question is whether DP can be made more attentive to social and sociotechnical contexts. Regardless, it is clear that
privacy harms need to be evaluated in a holistic manner and placed into broader contexts around legal standards and other forms of governance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib91" title="">91</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib8" title="">8</a>]</cite>.
Using analytical approaches from contextual integrity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib90" title="">90</a>]</cite> may be fruitful <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib13" title="">13</a>]</cite>; for example, these methods can help elucidate why heuristic approaches to privacy, such as using methods of swapping or blank-and-impute, may not satisfy contextual obligations to protect against re-identification of data subjects in settings such as the U.S. census <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib2" title="">2</a>]</cite>. In particular, the growing possibility for attacks against these heuristic approaches violate the contextual norms, as characterized by laws such as Title 13, around providing robust privacy protection to census respondents.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">To be able to make these determinations, data owners need more guidance on assessing user expectations around privacy in a given context. Data curators and regulators need more detailed frameworks to understand what would need to change about existing data flows in order to conform to the contextual norms, and whether whether DP would allow data systems to conform to these norms or further undermine them <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib13" title="">13</a>]</cite>. For example, suppose a company that collects user health data wants to improve its privacy practices. It currently shares aggregated statistics about its users’ health information with third parties such as insurance firms, and is considering using DP when sharing these statistics to better protect its users. To understand the impacts of DP on improving its privacy practices, the company should clarify the implications of its current data flows. What are the informational norms of this context and the current tensions with respect to (1) sharing aggregate statistics with third parties, (2) sharing aggregate statistics specifically with insurance companies, and/or (3) sharing aggregate statistics that can lead to individual harms for data subjects? And in which cases would using DP be beneficial?</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">In addition to understanding privacy and data sharing preferences, which are two aspects that have been explored in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib84" title="">84</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib116" title="">116</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib18" title="">18</a>]</cite>, we need better ways to understand other contextual needs, such as data utility, data accuracy, trust, explainability, and so on. In some cases, these other needs may indicate that DP is not appropriate for use. For example, a company may want to use DP to share data with third-party advertisers, but it may not be feasible for these advertisers to use noised estimates of market share or ad revenue. Similarly, if data for public use requires simple, explainable methods, DP may not be the right approach until improved explanations are developed. An important research direction is understand <em class="ltx_emph ltx_font_italic" id="S3.SS2.p4.1.1">when</em> it is appropriate to use DP and what are the alternatives if not.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">Finally, it is important to go beyond simply aligning DP with current norms or privacy expectations. Data practitioners should be equipped to make ethical judgments about what the norms <em class="ltx_emph ltx_font_italic" id="S3.SS2.p5.1.1">should</em> be. Case studies that highlight the values embedded within modernizing privacy protections, such as Abdu et al.’s study of the U.S. census deployment of DP, are a valuable starting point <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib1" title="">1</a>]</cite>. Normative thinking around DP includes identifying moral obligations to protect vulnerable stakeholders, and considering how DP would affect the ethical landscape of these norms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib51" title="">51</a>]</cite>. In our experience running working group sessions on responsible use of DP, we find that it can sometimes be hard for practitioners, especially technically focused ones, to engage in big-picture thinking around ethics and governance, but ethical guidelines such as the NIST Privacy Framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib93" title="">93</a>]</cite>, ASA Ethical Guidelines <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib95" title="">95</a>]</cite>, and UN Fundamental Principles of Official Statistics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib86" title="">86</a>]</cite> are valuable tools to help practitioners reason about the use of DP. Equally important are consent and oversight by ethical review boards that are well-equipped, and should be further trained, to make such judgments.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Aligning DP and institutional incentives</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Contextual norms are one important avenue for evaluating privacy needs, but another approach is to consider the landscapes of institutional incentives for deploying DP. We identify three main challenges in this arena: perceived tensions, real incompatibilities, and practical barriers.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">The first challenge is around the perceived tensions between privacy and corporate metrics. Privacy is often positioned as oppositional to data accuracy, innovation, and profitability. But as Julie Cohen argues, privacy is a positive good <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib25" title="">25</a>]</cite>; it is critical for providing trust that leads to good data accuracy, space that enables innovation, and safety that protects good business. For example, officials at government agencies such as the U.S. Census Bureau understand that protecting privacy of data subjects <em class="ltx_emph ltx_font_italic" id="S3.SS3.p2.1.1">now</em> is critical to maintaining data accuracy both <em class="ltx_emph ltx_font_italic" id="S3.SS3.p2.1.2">now and later</em>. This is because without the promises and protections of privacy, communities—particularly those that are vulnerable or marginalized, such as non-citizens, victims of domestic violence, transgender youth, and tenants who could be evicted for not following occupancy mandates—are reluctant to provide their sensitive information to the Census Bureau. When the public lacks confidence in privacy protections, response rates go down, leading to differential undercounts and exacerbated errors for certain populations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib76" title="">76</a>]</cite>. In other words, there is little hope of accuracy without first having privacy.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Positioning privacy as something that works in concert with other goals is a challenging endeavor. Research needs to explore how we can frame privacy as something that adds value, and something that is important to invest in, even if it comes at an initial cost. Privacy will almost always have some immediate benefits, and even more rewards in the future, such as safety, longevity, innovation, and utility <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib76" title="">76</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">Of course, there <em class="ltx_emph ltx_font_italic" id="S3.SS3.p4.1.1">are</em> some settings in which institutional logics are fundamentally in conflict with the goals of privacy.
While using DP in these settings may protect data subjects from some privacy threats, it can also be used to hinder privacy goals more broadly <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib105" title="">105</a>]</cite>. For example, DP can be used to shut down stakeholders’ privacy concerns (such as concerns about agency and consent) that cannot be translated into technical language, justify adjacent privacy harms such as extractive data collection, and reinforce centralized power <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib101" title="">101</a>]</cite>. Recent work has also highlighted the <em class="ltx_emph ltx_font_italic" id="S3.SS3.p4.1.2">framing effects</em> of DP that can prioritize certain forms of risk management over others <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib104" title="">104</a>]</cite>. For example, when using the local model of DP, the numerical measure of privacy loss cannot speak to the level of agency provided to data subjects in making data-sharing decisions and determining the privacy protections applied to their data. Consider a messaging application’s use of local DP to collect statistics on what GIFs are most popular with users <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib109" title="">109</a>]</cite>. Data subjects’ may be told that their sensitive data that captures keystrokes and GIF selections will be noised before being aggregated, but ultimately the application has control over how the protections are implemented and what parameters are used. Thus, the privacy-loss parameter leaves out important information about agency and control, which are crucial for understanding the privacy protections actually provided.</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1">With this in mind, we urge researchers to explore the following question: When do business incentives clash with the goals of privacy-enhancing mechanisms, and can DP really bridge the gaps? By identifying the tensions between corporate incentives and privacy tools, research can help us consider whether implementing DP would promote democratic engagement around data practices or, instead, be a tool for ‘privacy washing’ or ‘privacy theater’ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib105" title="">105</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1">Finally, there are many ways in which DP can be made more easily adoptable for engineering teams. DP primers and trainings that are catered not only to academics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib114" title="">114</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib63" title="">63</a>]</cite> but also engineers (such as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib87" title="">87</a>]</cite>), are important for practical adoption. Equally important is the continued development of open source software libraries for DP data analysis, such as DiffPrivLib <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib65" title="">65</a>]</cite>, OpenDP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib52" title="">52</a>]</cite>, and Tumult Analytics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib14" title="">14</a>]</cite>. These and other libraries have already been influential in bringing together theory and practice of DP, and we need more such efforts towards building software that serves both as educational resources and trusted tools for development.</p>
</div>
<div class="ltx_para" id="S3.SS3.p7">
<p class="ltx_p" id="S3.SS3.p7.1">Research should continue to focus on gaps for industry deployments and creating tools and algorithms to address real-world challenges.
One of these challenges is around the many parameters that practitioners need to set in order to deploy DP successfully. As we discuss more in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S5" title="5 Design and policy choices ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_tag">5</span></a>, this continues to be a hard problem.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Integrating DP with other privacy practices</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">The popular discourse around DP can sometimes seem to suggest that DP is a comprehensive privacy solution.
As most practitioners and experts know, however, DP is just one part of a suite of privacy and security tools. When considering the use of DP, organizations should also make sure to combine DP with best practices in security such as encrypted data storage, secure communication channels, and access control mechanisms.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">Beyond this, there is promising research on DP primitives that offer a wider set of protections or more favorable tradeoffs. For example, DP was first studied in terms of two models: the <em class="ltx_emph ltx_font_italic" id="S3.SS4.p2.1.1">central model</em>, where noise is applied by a trusted data aggregator after they have collected sensitive information from data subjects, and the <em class="ltx_emph ltx_font_italic" id="S3.SS4.p2.1.2">local model</em>, where data subjects add noise to their own individual bits of information before sending these to a potentially untrusted aggregator. While the local model requires much less trust, it does not allow for as much accuracy as in the central model.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>For more details on the local vs. central model, see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib35" title="">35</a>]</cite>.</span></span></span> Recent work has put forth the <em class="ltx_emph ltx_font_italic" id="S3.SS4.p2.1.3">shuffle model</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib23" title="">23</a>]</cite>, where the data subjects’ information is randomly permuted (but not noised) before reaching the aggregator. The shuffle model is innovative and valuable because it allows for accuracy that matches the central model with trust assumptions that are similar to the local model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib45" title="">45</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">The shuffle model was made possible by bringing tools and insights from cryptography to bear on DP algorithms. The success of this model—which uses just one of many cryptography primitives—suggests that researchers should be drawing more heavily on the intersection between DP and cryptography. As Wagh et al. write, such research is mutually beneficial to both communities, as cryptographic primitives can help bridge the utility gaps for DP, while DP relaxations of cryptographic primitives can yield implementations that are orders of magnitude faster than typical primitives <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib111" title="">111</a>]</cite>. Beyond shuffling, DP researchers can also look to multiparty computation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib97" title="">97</a>]</cite>, zero-knowledge proofs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib48" title="">48</a>]</cite>, and fully homomorphic encryption <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib72" title="">72</a>]</cite>; each of these points of intersection are vibrant areas of current research. We should continue to focus on how DP can interact with other privacy techniques to provide a more robust set of privacy protections.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Communicating around DP</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Once organizations have decided to use DP, they face the challenge of communicating about their deployment to relevant parties.
In recent years, there has been a surge of research around explaining the protections of DP to end users <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib116" title="">116</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib105" title="">105</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib117" title="">117</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib67" title="">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib113" title="">113</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib84" title="">84</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib9" title="">9</a>]</cite>, but there is much more work needed in this arena, including: addressing the needs of different communities, avoiding transparency traps, and engaging with stakeholders on the many different deployment choices involved in DP.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Tailoring communications for different stakeholders</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Communication around DP is not a monolithic endeavor. There are several parties involved in the implementation of DP—including data subjects, engineers, privacy officers, data curators, data analysts, executives, lawyers, and policymakers—and each require different types of tailored communications. Below, drawing on the suggestions from Cummings et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib28" title="">28</a>]</cite>, we describe communication challenges and areas for further research for each of these parties.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Data subjects who decide to share personal data.</span></p>
</div>
<div class="ltx_para" id="S4.I1.i1.p2">
<p class="ltx_p" id="S4.I1.i1.p2.1">Data subjects are tasked with the decision of whether or not to contribute their sensitive data into an aggregated dataset or DP analysis. Making this decision in an informed manner requires understanding the protections afforded by DP in general, as well as the protections afforded by the specific privacy units, parameters, and model used by the system in question.
Recent work along these lines has aimed to evaluate data subjects’ understanding of DP and data sharing preferences, given different types of explanations. For example, Cummings et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib30" title="">30</a>]</cite> find that the way DP is typically described is insufficient to help data subjects make informed decisions, and they recommend either training subjects to understand DP descriptions (shown by Xiong et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib116" title="">116</a>]</cite> to be challenging) or to rely on approaches similar to privacy nutrition labels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib68" title="">68</a>]</cite>. Nanayakkara et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib84" title="">84</a>]</cite> look more deeply into how data subjects understand privacy loss parameters (i.e., epsilon), offering suggestions for explaining these parameters in ways that maximize data subjects’ risk comprehension.
Further research on explanations—such as extending these findings to new data contexts and data uses—is critical for designing effective communications for data subjects.</p>
</div>
<div class="ltx_para" id="S4.I1.i1.p3">
<p class="ltx_p" id="S4.I1.i1.p3.1">Beyond understanding privacy risks and protections, data subjects also need to understand the upsides of contributing their data. Communications around DP should focus not just on the harms but also on the social and relational benefits of data sharing, as discussed by Viljoen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib110" title="">110</a>]</cite>. These themes have been touched on in prior work (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib18" title="">18</a>]</cite>), but explanations of the individual and social utility from DP releases have been under-explored in the literature thus far.</p>
</div>
<div class="ltx_para" id="S4.I1.i1.p4">
<p class="ltx_p" id="S4.I1.i1.p4.1">Finally, data subjects typically have the least agency over the DP system itself, compared to the many other stakeholders involved. It is important to find modes of communication—as well as concrete actions—that afford more power to data subjects over the use of their data in DP systems.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Engineers who build and manage DP systems.</span></p>
</div>
<div class="ltx_para" id="S4.I1.i2.p2">
<p class="ltx_p" id="S4.I1.i2.p2.1">Engineers and managers (including technical and product managers) are at the core of DP deployments. They are involved at a hands-on level in ensuring that the system is reliable and that the privacy guarantees are correct. They are in charge of managing privacy composition over multiple statistical releases and verifying
privacy-accuracy tradeoffs. Communicating with this set of users should be technically oriented, but must go beyond creating academic materials.</p>
</div>
<div class="ltx_para" id="S4.I1.i2.p3">
<p class="ltx_p" id="S4.I1.i2.p3.1">Current efforts to develop open-source DP libraries and repositories can help engineers understand what goes on under the hood in order to adapt methodologies for their own deployments. Even so, it can be challenging to get up to speed with a whole new paradigm under tight deadlines. In addition, recent user studies of open-source libraries for DP have demonstrated challenges around balancing ease-of-use and safe handling of data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib89" title="">89</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib106" title="">106</a>]</cite>. Therefore, libraries should focus both on creating sound, vetted algorithms (e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib53" title="">53</a>]</cite>) and providing higher-level APIs that mimic the workflow of common data analysis libraries, such as numpy or scikitlearn
 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib16" title="">16</a>]</cite> (e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib65" title="">65</a>]</cite>).</p>
</div>
<div class="ltx_para" id="S4.I1.i2.p4">
<p class="ltx_p" id="S4.I1.i2.p4.1">In addition, these APIs should incorporate visualization tools. Recent work on such tools <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib83" title="">83</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib17" title="">17</a>]</cite> is promising and calls attention to the many avenues for future work, such as providing visualizations for different types of queries, and enabling engineers to visually understand the impact on error of choosing metadata parameters, such as ranges and categories of data values.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">Privacy officers who are responsible for safe releases.</span></p>
</div>
<div class="ltx_para" id="S4.I1.i3.p2">
<p class="ltx_p" id="S4.I1.i3.p2.1">Privacy officers (including privacy teams and disclosure review boards) are well-versed in traditional privacy protections, such as removing PII <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib77" title="">77</a>]</cite>, swapping outliers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib31" title="">31</a>]</cite>, and suppressing small counts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib118" title="">118</a>]</cite>. They have likely heard of DP but may not be sure it is right for the organization’s context. Therefore, communicating with privacy officers should begin with explaining the particular situations in which DP is useful, as well as the situations that are outside of the scope of DP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib114" title="">114</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib115" title="">115</a>]</cite>. Communication should also emphasize the new risks that have been identified under traditional (or existing) privacy protections <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib85" title="">85</a>]</cite>. Finally, the research community should create trainings geared specifically towards privacy officers, empowering them to make decisions about parameter selection that are right for their organization, and giving them the tools to communicate with the other stakeholders in this list.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i4.p1.1.1">Data curators who manage access to sensitive data.</span></p>
</div>
<div class="ltx_para" id="S4.I1.i4.p2">
<p class="ltx_p" id="S4.I1.i4.p2.1">Data curators, who may be data scientists, researchers, statisticians, or administrators, have either collected or overseen the collection of the sensitive data, and are responsible for sharing it in an appropriate manner. Due to their detailed knowledge of the data domain and collection processes, they are likely in charge of setting global privacy-loss parameters and allocating privacy-loss budget to interested parties. Recent work studying data curators’ decision-making around DP, however, has shown that making these decisions can be incredibly challenging <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib101" title="">101</a>]</cite>. The research community should create trainings and establish best practices to help curators make these important decisions.</p>
</div>
<div class="ltx_para" id="S4.I1.i4.p3">
<p class="ltx_p" id="S4.I1.i4.p3.1">Data curators are also involved with other data management practices, such as data annotation, archiving, and application/review/approval processes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib78" title="">78</a>]</cite>.
All of these practices may need to be modified when integrated with DP.
Dataset annotation, in particular, has been shown to be very important for releasing DP statistics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib102" title="">102</a>]</cite>. Creating detailed annotations and codebooks is an important duty of the data curator in order to increase usability of the released statistics. For example, the curator should provide data bounds for numerical variables and a list of categories for categorical variables. They should also provide pointers to similar data that are publicly available. Sarathy et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib102" title="">102</a>]</cite> also recommend that data curators set aside a portion of the privacy-loss budget for default releases of summary statistics, such as CDFs and histograms of key variables. However, more research is needed in order to provide guidelines for data curators on how to make such default releases and what portion of the privacy budget should be set aside for this.</p>
</div>
<div class="ltx_para" id="S4.I1.i4.p4">
<p class="ltx_p" id="S4.I1.i4.p4.1">The data curator must also fully contend with the downstream uses of data—not just what statistics are required, but <em class="ltx_emph ltx_font_italic" id="S4.I1.i4.p4.1.1">how</em> the data are used. Data production is not a neutral endeavor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib80" title="">80</a>]</cite>; each method will entrench certain norms. For example, privacy protections used in the U.S. census prior to 2020—such as <em class="ltx_emph ltx_font_italic" id="S4.I1.i4.p4.1.2">swapping</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib118" title="">118</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib31" title="">31</a>]</cite>—added noise to the data but did not allow the bureau to reveal how the noise was added, thus entrenching the norm of treating census data as fact without accounting for error due to privacy protections <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib15" title="">15</a>]</cite>. When DP was introduced and data users were asked to take privacy error into account, this disrupted many downstream uses of census data.
DP may disrupt other norms around data use beyond accounting for error, such as accessing data via a query interface rather than direct exploration, and having a limited number of attempts to analyze the data due to a finite privacy loss budget versus unlimited data exploration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib102" title="">102</a>]</cite>.
The data curator is responsible for explaining the limits of the data to stakeholders, especially when insights are shared using DP, so research needs to focus on best practices for communication along these lines.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i5.p1.1.1">Data analysts who use DP releases for business, research and policymaking.</span></p>
</div>
<div class="ltx_para" id="S4.I1.i5.p2">
<p class="ltx_p" id="S4.I1.i5.p2.1">Data analysts range from academic economists to grade school students. Communications with data analysts must be attuned to and matched with the level and type of technical detail that are used in their daily work. This suggests a strategy of layered communications, including: visual explanations of DP concepts to be used a high level, code snippets that demonstrate how to work with DP releases, and mathematical explanations that can be referenced if a data analyst needs to dig into the system processes.</p>
</div>
<div class="ltx_para" id="S4.I1.i5.p3">
<p class="ltx_p" id="S4.I1.i5.p3.1">Communicating error is an important piece of these communications. It is natural for data users to want to overlook the error inherent in any data product, especially when they are tasked with using it to communicate with decision-makers. DP, however, forces data users to take error into account. A DP release (whether in the form of query-release outputs or tabular synthetic data) is not a deterministic function of the collected data. This is sometimes readily apparent—if the data contain implausible values such as negative counts—but it can also take less obvious forms. Either way, data users must take into account the noise from DP releases when using them for decision-making. It is therefore important to find ways to communicate error to data analysts, and to allow them to remain cognizant about this error in their downstream uses of the data.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S4.I1.i6.p1">
<p class="ltx_p" id="S4.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i6.p1.1.1">Executives who must decide on the use of DP and defend its impacts.</span></p>
</div>
<div class="ltx_para" id="S4.I1.i6.p2">
<p class="ltx_p" id="S4.I1.i6.p2.1">The goals and incentives of executives may differ from those of engineers, privacy officers, and data curators. Executives are typically more focused on improving organizational performance along
metrics such as revenue, market share, or consumer satisfaction. They are unlikely to adopt DP purely for safety or ethical concerns.
Therefore, privacy needs to be communicated as a benefit rather than a moral imperative or a tradeoff with key performance metrics of the organization. This might look like highlighting demand for privacy protections from users, possibilities for higher quality and quantity of data collection, protection against real attacks from adversaries, avoidance of liability or subpoenas, compliance with privacy laws, and competitive advantages.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para" id="S4.I1.i7.p1">
<p class="ltx_p" id="S4.I1.i7.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i7.p1.1.1">Lawyers who must understand how DP systems fit with existing laws and regulations.</span></p>
</div>
<div class="ltx_para" id="S4.I1.i7.p2">
<p class="ltx_p" id="S4.I1.i7.p2.1">Lawyers are responsible for understanding whether DP is necessary and sufficient to meet the requirements of existing privacy and data processing laws.
They often must determine whether a move to DP is justified by privacy regulations, and whether a new DP system is compliant with privacy laws. There are a few challenges of making these determinations. First, lawyers must translate between the mathematical language of DP and the legal formalizations of privacy, which can be misaligned in several ways, such as having continuous versus binary notions of risk; being agnostic to context versus specifically pertaining to data type, domain, and geography; and envisioning worst-case attacks versus attending to reasonable adversaries. There has been an innovative line of research in the past several years that translates between legal and mathematical notions of privacy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib91" title="">91</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib24" title="">24</a>]</cite>; this needs to be expanded to keep up with the growing adoption of DP across sectors and industries.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span>
<div class="ltx_para" id="S4.I1.i8.p1">
<p class="ltx_p" id="S4.I1.i8.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i8.p1.1.1">Policymakers who develop new regulations around data and privacy.</span></p>
</div>
<div class="ltx_para" id="S4.I1.i8.p2">
<p class="ltx_p" id="S4.I1.i8.p2.1">Policymakers need improved communication around the tensions between computational understandings of risk and current privacy regulations. With guidance from both lawyers and computer scientists, they must understand the underspecifications in current privacy laws and how these can be redeveloped to remain in-step with technological advances and new understandings of privacy risks. Legal guidance will be especially important here to develop regulations that are contextually specific, yet also broad enough to remain relevant in the years ahead. In particular, experts in law and computer science must work together to design regulations and policy guidance based on concepts that are both legally and mathematically sound <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib92" title="">92</a>]</cite>.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Avoiding transparency traps</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Organizations often equate effective communication with transparency. This is especially true with DP. Unlike other privacy techniques like swapping that rely on ’security through obscurity,’ DP enables organizations to provide transparency about implementation details.
Therefore, with DP it is not only <em class="ltx_emph ltx_font_italic" id="S4.SS2.p1.1.1">possible</em>, but also <em class="ltx_emph ltx_font_italic" id="S4.SS2.p1.1.2">encouraged</em>, to be fully transparent about the technical details of a privacy-preserving system to stakeholders such as data subjects, data users, and auditors.
However, this is not as straightforward as it seems. Prior deployments of DP have demonstrated that technical transparency requires supplementary tools and structures in order to be effective <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib15" title="">15</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Transparency can pose difficulties for communication in several ways.
First, transparency around DP often requires justifying the <em class="ltx_emph ltx_font_italic" id="S4.SS2.p2.1.1">need</em> for DP. As we discuss in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S3" title="3 Considering when to use DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_tag">3</span></a>, this is a complicated conversation, as stakeholders and organizations can have very different perspectives about what constitutes a privacy risk, what is a meaningful privacy attack, and whether DP is an appropriate intervention to address these threats.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Second, transparency around DP can hinder consensus across stakeholders. For example, highlighting the privacy-utility tradeoff, and the zero-sum nature<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>This can be a challenge with privacy design in general, see, e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib22" title="">22</a>]</cite>.</span></span></span> of choosing privacy-loss parameters and allocating them across data users, puts communication with stakeholders on a contentious playing field.
In particular, relevant parties are more aware of how increasing accuracy of a particular statistic for one set of data subjects can decrease accuracy for another set of data subjects, since the privacy-loss budget is a finite resource.
This happened with the communications around the U.S. census, when considering how much privacy-loss budget would be given to tribal communities, for example, where the push towards transparency created unexpected challenges for getting meaningful engagement from stakeholders about their privacy and utility needs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib62" title="">62</a>]</cite>.
Second, communication via technical transparency requires shared background and perspectives around statistical concepts. For example, assurances to stakeholders that the scale of noise being added by DP is smaller than the scale of noise due to other sources (such as sampling, editing, and privacy mechanisms used prior to DP) requires first ensuring that stakeholders are aware of
the noise inherent in all data processes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib15" title="">15</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S4.SS1" title="4.1 Tailoring communications for different stakeholders ‣ 4 Communicating around DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_tag">4.1</span></a> regarding data curators, the failure to account for stakeholders’ comfort towards noise in data was one of the key communication hurdles during the U.S. census deployment of DP. When being transparent about the technicalities of DP, it is important to consider what was previously <em class="ltx_emph ltx_font_italic" id="S4.SS2.p4.1.1">not</em> made transparent (e.g., the noise that was always included in census data due to sampling or data processing errors), the norms and data practices that were built upon faulty assumptions (e.g., policies around political redistricting in the U.S. that assume census data can be treated as accurate down to the individual respondent), and how transparency around DP upends these practices.</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">Third, transparency can create undue focus on some parameters over others. While there are many different design choices around DP, some of these choices (such as the choice of privacy-loss parameters) are more visible to stakeholders and easier to communicate about than others (such as sampling method or the choice of hyperparameters) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib102" title="">102</a>]</cite>. We detail some of these design choices below in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S5" title="5 Design and policy choices ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_tag">5</span></a>. It is important to consider which parameters are most important to get feedback on from stakeholders, while also not letting other impactful decisions get hidden in the process.</p>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.1">Transparency without adequate accompaniments to make sense of information, such as trusted expertise <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib1" title="">1</a>]</cite>, can actively damage trust and communications. Therefore, it is important for researchers to understand what tools, practices, relationships, and supports are necessary for enabling stakeholders to benefit from transparency rather than become overwhelmed by inaccessible details. These are not new questions; they are studied heavily in the fields of science communication, design, journalism, and technology studies. Collaborations with researchers in these areas – particularly researchers who study techniques for communicating uncertainty – will greatly improve our own understandings of how to communicate effectively about DP.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Engaging beyond epsilon</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">While technical transparency comes with its challenges, it is still essential for organizations to be open and precise about the details of their DP systems. As Dwork, Mulligan and Kohli write, “unless pertinent information is made available, assessment and comparison of DP systems is infeasible.” They recommend creating an <em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.1.1">Epsilon Registry</em> for documenting and comparing technical implementations of different deployments. The goals of such a registry are to support shared learning across organizations, enable oversight, and exert pressure on organizations to provide meaningful privacy protections rather than using DP simply as privacy theater.
Registries of this sort have been used by the Census Bureau <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib54" title="">54</a>]</cite> and are called for by the broader DP community <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib34" title="">34</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Despite the name of the registry, the authors note that it is critical to look beyond just privacy-loss parameters such as epsilon. As we have also discussed earlier in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S4.SS2" title="4.2 Avoiding transparency traps ‣ 4 Communicating around DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_tag">4.2</span></a>, there are several parameters that are critical for characterizing privacy and utility that should also be made available to the public, such as sampling methods, composition of algorithms, DP model (e.g., local, central, shuffle), algorithms used, and algorithm-specific hyperparameters. Communications regarding all of these implementation details—and the justifications for the choices—are critical for advancing the state of privacy protections via DP.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Design and policy choices</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">There are many design and policy decisions involved in using DP in practice. Below, we discuss these choices in more detail, and offer suggestions for easier decision-making.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Choosing the right definition and unit of privacy</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.6">The initial steps of a DP deployment include choosing a definition and unit of privacy. There are various definitions of DP, each with their own parameters that capture different aspects of privacy risk, including: pure <math alttext="\varepsilon" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><mi id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\varepsilon</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">italic_ε</annotation></semantics></math>-DP, approximate <math alttext="(\varepsilon,\delta)" class="ltx_Math" display="inline" id="S5.SS1.p1.2.m2.2"><semantics id="S5.SS1.p1.2.m2.2a"><mrow id="S5.SS1.p1.2.m2.2.3.2" xref="S5.SS1.p1.2.m2.2.3.1.cmml"><mo id="S5.SS1.p1.2.m2.2.3.2.1" stretchy="false" xref="S5.SS1.p1.2.m2.2.3.1.cmml">(</mo><mi id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">ε</mi><mo id="S5.SS1.p1.2.m2.2.3.2.2" xref="S5.SS1.p1.2.m2.2.3.1.cmml">,</mo><mi id="S5.SS1.p1.2.m2.2.2" xref="S5.SS1.p1.2.m2.2.2.cmml">δ</mi><mo id="S5.SS1.p1.2.m2.2.3.2.3" stretchy="false" xref="S5.SS1.p1.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.2b"><interval closure="open" id="S5.SS1.p1.2.m2.2.3.1.cmml" xref="S5.SS1.p1.2.m2.2.3.2"><ci id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">𝜀</ci><ci id="S5.SS1.p1.2.m2.2.2.cmml" xref="S5.SS1.p1.2.m2.2.2">𝛿</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.2c">(\varepsilon,\delta)</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.2.m2.2d">( italic_ε , italic_δ )</annotation></semantics></math>-DP, <math alttext="(\mu,\tau)" class="ltx_Math" display="inline" id="S5.SS1.p1.3.m3.2"><semantics id="S5.SS1.p1.3.m3.2a"><mrow id="S5.SS1.p1.3.m3.2.3.2" xref="S5.SS1.p1.3.m3.2.3.1.cmml"><mo id="S5.SS1.p1.3.m3.2.3.2.1" stretchy="false" xref="S5.SS1.p1.3.m3.2.3.1.cmml">(</mo><mi id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml">μ</mi><mo id="S5.SS1.p1.3.m3.2.3.2.2" xref="S5.SS1.p1.3.m3.2.3.1.cmml">,</mo><mi id="S5.SS1.p1.3.m3.2.2" xref="S5.SS1.p1.3.m3.2.2.cmml">τ</mi><mo id="S5.SS1.p1.3.m3.2.3.2.3" stretchy="false" xref="S5.SS1.p1.3.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.2b"><interval closure="open" id="S5.SS1.p1.3.m3.2.3.1.cmml" xref="S5.SS1.p1.3.m3.2.3.2"><ci id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1">𝜇</ci><ci id="S5.SS1.p1.3.m3.2.2.cmml" xref="S5.SS1.p1.3.m3.2.2">𝜏</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.2c">(\mu,\tau)</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.3.m3.2d">( italic_μ , italic_τ )</annotation></semantics></math>-concentrated DP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib43" title="">43</a>]</cite>, <math alttext="(\rho)" class="ltx_Math" display="inline" id="S5.SS1.p1.4.m4.1"><semantics id="S5.SS1.p1.4.m4.1a"><mrow id="S5.SS1.p1.4.m4.1.2.2"><mo id="S5.SS1.p1.4.m4.1.2.2.1" stretchy="false">(</mo><mi id="S5.SS1.p1.4.m4.1.1" xref="S5.SS1.p1.4.m4.1.1.cmml">ρ</mi><mo id="S5.SS1.p1.4.m4.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.4.m4.1b"><ci id="S5.SS1.p1.4.m4.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.4.m4.1c">(\rho)</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.4.m4.1d">( italic_ρ )</annotation></semantics></math>-zero-concentrated DP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib21" title="">21</a>]</cite>, <math alttext="(\varepsilon,\alpha)" class="ltx_Math" display="inline" id="S5.SS1.p1.5.m5.2"><semantics id="S5.SS1.p1.5.m5.2a"><mrow id="S5.SS1.p1.5.m5.2.3.2" xref="S5.SS1.p1.5.m5.2.3.1.cmml"><mo id="S5.SS1.p1.5.m5.2.3.2.1" stretchy="false" xref="S5.SS1.p1.5.m5.2.3.1.cmml">(</mo><mi id="S5.SS1.p1.5.m5.1.1" xref="S5.SS1.p1.5.m5.1.1.cmml">ε</mi><mo id="S5.SS1.p1.5.m5.2.3.2.2" xref="S5.SS1.p1.5.m5.2.3.1.cmml">,</mo><mi id="S5.SS1.p1.5.m5.2.2" xref="S5.SS1.p1.5.m5.2.2.cmml">α</mi><mo id="S5.SS1.p1.5.m5.2.3.2.3" stretchy="false" xref="S5.SS1.p1.5.m5.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.5.m5.2b"><interval closure="open" id="S5.SS1.p1.5.m5.2.3.1.cmml" xref="S5.SS1.p1.5.m5.2.3.2"><ci id="S5.SS1.p1.5.m5.1.1.cmml" xref="S5.SS1.p1.5.m5.1.1">𝜀</ci><ci id="S5.SS1.p1.5.m5.2.2.cmml" xref="S5.SS1.p1.5.m5.2.2">𝛼</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.5.m5.2c">(\varepsilon,\alpha)</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.5.m5.2d">( italic_ε , italic_α )</annotation></semantics></math>-Renyi DP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib81" title="">81</a>]</cite>, <math alttext="\mu" class="ltx_Math" display="inline" id="S5.SS1.p1.6.m6.1"><semantics id="S5.SS1.p1.6.m6.1a"><mi id="S5.SS1.p1.6.m6.1.1" xref="S5.SS1.p1.6.m6.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.6.m6.1b"><ci id="S5.SS1.p1.6.m6.1.1.cmml" xref="S5.SS1.p1.6.m6.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.6.m6.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.6.m6.1d">italic_μ</annotation></semantics></math>-Gaussian DP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib36" title="">36</a>]</cite>, and more. There are also various units of privacy, which refers to what DP will protect and what is considered a neighboring database. For example, the typical unit of privacy is for an individual, which means that a function run on a dataset <em class="ltx_emph ltx_font_italic" id="S5.SS1.p1.6.1">with</em> this individual’s data and a neighboring dataset <em class="ltx_emph ltx_font_italic" id="S5.SS1.p1.6.2">without</em> this individual’s data will result in similar (distributions of) outputs. In other words, DP will limit the influence of that individual’s data on the final output.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">When an individual contributes multiple data points—such as a user interacting repeatedly with a platform—there is also a distinction between obscuring one single action of a user (<em class="ltx_emph ltx_font_italic" id="S5.SS1.p2.1.1">event-level privacy</em>) or their entire history of actions (<em class="ltx_emph ltx_font_italic" id="S5.SS1.p2.1.2">user-level privacy</em>). Other privacy units include small groups of individuals (<em class="ltx_emph ltx_font_italic" id="S5.SS1.p2.1.3">group privacy</em>), data attributes when each individual’s data have multiple features (<em class="ltx_emph ltx_font_italic" id="S5.SS1.p2.1.4">attribute privacy</em>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib119" title="">119</a>]</cite>).</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">Making these decisions is foundational to the nature of the privacy protections. However, it is challenging to understand what is the right definition and unit to choose for a given deployment. System designers require more detailed guidance on how to translate privacy risk considerations into privacy definitions and units, how to understand the conversions from one set of definitions to another, and how to consider tradeoffs in making these decisions.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Selecting and allocating privacy-loss parameters</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">One of the main decisions the data curator must make is around privacy loss parameters. The leading privacy loss parameter in Definition <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#Thmdefinition1" title="Definition 1 (Differential privacy [41]). ‣ 2 Background and Overview ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_tag">1</span></a> is epsilon (<math alttext="\varepsilon" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><mi id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><ci id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">\varepsilon</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">italic_ε</annotation></semantics></math>). Privacy theorists previously recommended that epsilon be set to be a small value between 0.01 and 1, although some practical deployments have necessitated using a much higher value <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib109" title="">109</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib62" title="">62</a>]</cite>, which corresponds to weaker privacy protections.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">The DP literature often approaches the problem of selecting the privacy-loss parameter in terms of optimizing the privacy-accuracy tradeoff <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib57" title="">57</a>]</cite>, which is a tricky problem itself. However, the job at hand is even more complex. To make this decision, data owners must integrate many factors including the needs of different stakeholders, prioritization of vulnerable stakeholders and scenarios, moral and ethical standpoints around privacy protections, risk calculations, changing needs over time as data systems scale, and so on. In addition, they must understand how these different factors translate to quantitative parameters. There is a considerable lack of guidance available to practitioners at the moment around making this choice.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">In addition to selecting the global privacy-loss budget, data curators must also allocate privacy-loss budget across different data analysts.
For example, research data repositories may want to allocate budget across different institutions with tiered levels of access, and companies may want to split the budget across internal analytics teams and third parties such as researchers or advertisers. Practitioners would benefit from principled methods and frameworks for conducting such allocations.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1">Further research is required into how data owners can choose both the global privacy-loss budget and its allocation across stakeholders and use-cases. Such research can build upon existing work on modeling the impact of privacy-loss parameters using economic and social choice theory <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib71" title="">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib33" title="">33</a>]</cite>, testing explanations of epsilon <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib79" title="">79</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib84" title="">84</a>]</cite>, creating databases that document parameters used across different deployments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib39" title="">39</a>]</cite>, and translating parameters to real-world risk <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib96" title="">96</a>]</cite>. Beyond these approaches, it is also critical to bring in perspectives from social science and humanities to bear on this question. In particular, it will be important to understand how to integrate social and technical analyses when making decisions about a privacy-loss parameter that can impact a range of stakeholders in different ways.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Designing entire pipelines for DP</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Much of the DP literature has focused on privacy loss starting at the point when data are shared (or sent by individuals to an aggregator), but it is increasingly important to understand how steps such as selecting the sample frame, sampling, weighting responses, imputing, and editing the collected data all affect the privacy guarantee. Researchers are starting to explore the design of entire data pipelines with DP in mind.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">Recent work has highlighted, for example, the ways in which sampling designs can both help and hinder the final privacy guarantee <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib20" title="">20</a>]</cite>. In particular, random sampling data points before a DP release can “amplify” the privacy guarantee, allowing one to gain more utility for the same privacy loss. Other types of sampling methods, such as proportional or cluster sampling, can have no effect on or even degrade the privacy guarantee. Those working with data would benefit from research that designs statistical methods to enhance, rather than degrade, the privacy protections offered by DP when considering the entire pipeline of creation and use.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">DP pipelines should also extend well beyond the point of computation. DP outputs can take on a life of their own once they are released, and more research is needed to understand what constitutes a successful release <em class="ltx_emph ltx_font_italic" id="S5.SS3.p3.1.1">after</em> the outputs are out in the wild. To ensure effective usage of DP statistics, we will likely need to build robust sociotechnical systems for citing, trusting, and explaining the outputs of DP mechanisms.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Practice of DP</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The next set of challenges we address concerns the practice of DP, which is only starting to be explored in the DP literature.
Practices of using DP include both what data users empirically do and what they <em class="ltx_emph ltx_font_italic" id="S6.p1.1.1">should</em> do in order to create safe, usable releases.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Facilitating data processing and exploration</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">Data processing and exploration are important steps in data analysis that are understudied in the DP literature.
Experts estimate that over 80% of data analysis consists of data wrangling, cleaning, and editing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib32" title="">32</a>]</cite>. However, these steps are difficult under DP constraints because it is unclear how they affect the privacy guarantee, and it is often challenging for data analysts to have enough context to perform these tasks when asked to do so via a privacy-preserving interface <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib102" title="">102</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">Therefore, a critical area for future work is to build tools and strategies for processing and exploration under DP constraints. There are two main challenges to overcome: doing these processes without direct access the raw data, and conducting explorations with only a finite privacy loss budget.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1">One important strategy for mitigating these challenges is through detailed, thorough codebooks and data annotations. As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S4.I1.i4" title="item 4 ‣ 4.1 Tailoring communications for different stakeholders ‣ 4 Communicating around DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_tag">4</span></a>,
data curators should provide information that enables data analysts to understand basic features of the data without spending additional privacy-loss budget on these steps.
Further research is needed to understand what features are important to include in annotations, how annotations can be (semi)-automated, and best practices for creating and using codebooks during a DP release.</p>
</div>
<div class="ltx_para" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1">A second important direction is regarding DP strategies for data cleaning and exploration. What parts of these processes can be systematically privatized, and what cannot? Many practitioners claim that these processes cannot be done with DP because they are not single algorithmic procedures, but rather a collection of tools, many of which involve looking at the data directly or searching for individual points (such as outlier detection) instead of global properties of the dataset. However, as datasets grow larger and contain millions of rows or features, practitioners are no longer able to just “look at the data” to guide their analyses. As a starting point, some DP tools exist for data exploration tasks, such as privately computing marginal distributions of features, which can provide a picture of the dataset for analysts. Further research is needed to understand how to do other important data pre-processing tasks in a DP manner, such as checking for outliers that may represent miscoded data or dropping missing values.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Choosing metadata parameters</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">Datasets in the wild may have an infinite range or set of categories; this means the <em class="ltx_emph ltx_font_italic" id="S6.SS2.p1.1.1">sensitivity</em> of analyses to the input data points could be unbounded, which can pose a problem for maintaining the privacy guarantee. Therefore, many DP algorithms require the inputs to be bounded, or for the curator/analyst to set a bound to which the input values can be clipped, in order to limit the privacy loss of the release <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib42" title="">42</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">However, these bounds and categories must be data-independent; they cannot be taken straight from the dataset, but rather should come from knowledge about the data domain and its collection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib88" title="">88</a>]</cite>. Selecting these parameters independently from the dataset is critical for maintaining the desired privacy guarantee. For example, consider a dataset containing the incomes of individuals in a company, and query for the mean income. Using the max income directly from the dataset for the upper bound of the ‘income’ variable will directly reveal the sensitive income of this individual.
However, these parameters also impact data utility. Ranges or categories that are too limited may introduce more bias into statistics computed, but parameters that are too broad may introduce more variance. Typically, the <em class="ltx_emph ltx_font_italic" id="S6.SS2.p2.1.1">data curator</em> is responsible for choosing these parameters using their high knowledge of the data domain, but <em class="ltx_emph ltx_font_italic" id="S6.SS2.p2.1.2">data analysts</em> are also accustomed to fine-tuning these parameters to achieve, e.g., their bias-variance goals.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>For more details, see <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://programming-dp.com/ch5.html" title="">https://programming-dp.com/ch5.html</a>.</span></span></span></p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">Across the board, practitioners of DP find that choosing metadata parameters in a way that preserves both privacy and utility is extremely challenging. Strategies do exist to mitigate this challenge, such as choosing these parameters based on publicly available data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib103" title="">103</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib73" title="">73</a>]</cite>, or spending a portion of the privacy loss budget on estimating these parameters privately (e.g., using DP binary search to estimate the range of a variable, as shown in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib38" title="">38</a>]</cite>)—although this may lead to other decision points, such as what portion of the privacy loss budget should be used. A goal of future work should be to create algorithmic and procedural strategies to guide setting of such parameters.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Tools for using and evaluating data products with DP</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">As mentioned in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#S4" title="4 Communicating around DP ‣ Centering Policy and Practice: Research Gaps around Usable Differential PrivacyR.C. supported in part by NSF grant CNS-1942772 (CAREER), DARPA contract number W911NF-21-1-0371, and an Early Career Faculty Impact Fellowship from Columbia University. J.S. supported in part by the Columbia Data Science Institute and DARPA contract number W911NF-21-1-0371. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Government or DARPA. The authors are grateful to Philip Leclerc, Priyanka Nanayakkara, Salil Vadhan, and Alex Wood for their comments on an earlier draft of this paper."><span class="ltx_text ltx_ref_tag">4</span></a>, transparency without adequate infrastructure can be counterproductive. We need pipelines for creating and evaluating DP data products (existing communities are starting to work on this, e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib14" title="">14</a>]</cite>). This includes algorithmic research, such as providing DP confidence intervals and uncertainty measures that require minimal assumptions about the data (e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib112" title="">112</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib38" title="">38</a>]</cite>), but it also includes effective user interfaces, comprehensive software libraries <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib65" title="">65</a>]</cite>, visualization tools <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib83" title="">83</a>]</cite>, and guidelines for citing and explaining the DP results to data users and policymakers.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Trust and Governance of DP</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Trust and governance of DP deployments requires bringing together all the components already discussed, from having reliable tools to regulating details of the deployment and considering legal and contextual alignments. This section will compile the different suggestions in prior sections toward creating more trusted—and trustworthy—DP deployments.</p>
</div>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Stakeholder engagement and oversight</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">Organizations are increasingly interested in equity and participatory engagement <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib94" title="">94</a>]</cite>. However, it not always clear how to implement these ideals in practice, particularly when making decisions about technically complex choices such as those found within DP. Privacy co-design is challenging and depends heavily on context <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib22" title="">22</a>]</cite>. Researchers have observed that—particularly with privacy, which is used as a catch-all concern for greater questions around data collection and governance—stakeholder concerns can either get lost in the design process or stall the deployment of the privacy technology completely <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib7" title="">7</a>]</cite>. We need more research and practice around what it means to meaningfully engage with stakeholders and offer vulnerable parties both power and oversight, while still lowering barriers for important privacy projects to reach the stage of deployment.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Documenting decisions, justification, and guidelines</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">Documenting DP deployments is important for building trust.
Just as data curators should annotate datasets (e.g., using the framework of Gebru et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib56" title="">56</a>]</cite>) prior to a DP release, organizations should also make public the justifications for their use of DP, alternative options that were considered and discarded, any relevant risks regarding privacy or data utility, level of protection against these risks that would be considered adequate for use case, whether DP addresses these risks, whether these risks and protections were discussed with stakeholders, and how each of these risks will be addressed in the deployment. These justifications should also bring together moral, legal, and technical guidelines around privacy in the organization’s domain. The documentation should clearly outline all the decisions made in the design and implementation processes, compare these decisions with similar deployments, and provide explanations for the decisions (as recommended by Dwork et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib39" title="">39</a>]</cite>).</p>
</div>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1">Finally, organizations should provide clear guidelines for use of the releases. What uses does the organization consider appropriate and aligned with the goals of the release? For example, the organization may encourage the use of DP statistics to learn about a group of consumers’ behaviors, but not to target this group of consumers in harmful ways. Specifying appropriate use is also important for utility of DP releases, since some DP algorithms only provide strong accuracy guarantees on a pre-specified set of queries.
While such guidelines will not always stop users from abusing the release, documenting the expectations of system designers is important for learning from the mistakes of each deployment and developing more robust guardrails over time.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Auditing DP deployments</h3>
<div class="ltx_para" id="S7.SS3.p1">
<p class="ltx_p" id="S7.SS3.p1.1">As the use of DP becomes more mainstream, it will be increasingly important to have practices in place to audit these deployments. Such audits should start with the questions posed by Dwork, Kohli and Mulligan in their proposal of an <em class="ltx_emph ltx_font_italic" id="S7.SS3.p1.1.1">epsilon registry</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.12103v1#bib.bib40" title="">40</a>]</cite>, including: paths of privacy loss, granularity, epsilon per datum, burn rate, privacy loss allowed before retirement, variant of DP used, and justification for all of these implementation choices.
The responses to these questions should be compared with expert advice and across a range of deployments.</p>
</div>
<div class="ltx_para" id="S7.SS3.p2">
<p class="ltx_p" id="S7.SS3.p2.1">One challenge for trust and governance of DP comes from its inherent nature as a randomized algorithm, as well as the fact that multiple releases will continue to degrade privacy loss. When data users are only given one (or a few) releases from a single dataset, how can they verify that the algorithm was run correctly? Beyond institutional mechanisms such as regulations and audits, there is also room for technical methods that can provide trust in the DP release. This might look like zero-knowledge proofs of correctness, or mathematical proofs of privacy and utility.</p>
</div>
<div class="ltx_para" id="S7.SS3.p3">
<p class="ltx_p" id="S7.SS3.p3.1">Finally, we need to answer the question: what recourse do data subjects (and/or data users) have if something goes wrong with a DP deployment? Answering this question requires dialogue between technical and legal stakeholders to understand what can and should be done in these cases.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">DP is crossing the line from an emerging technology to an established one. With this shift comes new challenges and responsibilities for researchers and practitioners. Recent efforts have only scratched the surface. In this paper, we have outlined the next frontier of research around making DP usable. We center critical concerns around policy and practice, from assessing risks in a contextual manner to documenting and auditing deployments. We hope that the challenges and solutions identified in this paper can be a guide for researchers to focus their efforts, for policymakers to build frameworks to regulate DP, and for stakeholders to know where we must set our sights in order to align DP research with the problems it aims to solve.</p>
<div class="ltx_pagination ltx_role_newpage"></div>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. A. Abdu, L. M. Chambers, D. K. Mulligan, and A. Z. Jacobs.

</span>
<span class="ltx_bibblock">Algorithmic transparency and participation through the handoff lens:
Lessons learned from the US Census Bureau’s adoption of differential
privacy.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">The 2024 ACM Conference on Fairness, Accountability, and
Transparency</span>, pages 1150–1162, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. M. Abowd.

</span>
<span class="ltx_bibblock">The US Census Bureau adopts differential privacy.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 24th ACM SIGKDD International Conference
on Knowledge Discovery &amp; Data Mining</span>, KDD ‘18, pages 2867–2867, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. M. Abowd and I. M. Schmutte.

</span>
<span class="ltx_bibblock">An economic analysis of privacy protection and statistical accuracy
as social choices.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">American Economic Review</span>, 109(1):171–202, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
M. Ackerman, T. Darrell, and D. J. Weitzner.

</span>
<span class="ltx_bibblock">Privacy in context.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Human–Computer Interaction</span>, 16(2-4):167–176, 2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. Acquisti and R. Steed.

</span>
<span class="ltx_bibblock">Learning to live with privacy-preserving analytics.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Communications of the ACM</span>, 66(7):24–27, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
T. Adeleye, S. Berghel, D. Desfontaines, M. Hay, I. Johnson, C. Lemoisson,
A. Machanavajjhala, T. Magerlein, G. Modena, D. Pujol, et al.

</span>
<span class="ltx_bibblock">Publishing wikipedia usage data with strong privacy guarantees.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2308.16298</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
N. Agrawal, R. Binns, M. Van Kleek, K. Laine, and N. Shadbolt.

</span>
<span class="ltx_bibblock">Exploring design and governance challenges in the development of
privacy-preserving computation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 2021 CHI Conference on Human Factors in
Computing Systems</span>, CHI ‘21, pages 1–13, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
M. Altman, A. Cohen, K. Nissim, and A. Wood.

</span>
<span class="ltx_bibblock">What a hybrid legal-technical analysis teaches us about privacy
regulation: The case of singling out.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Boston University Journal of Science &amp; Technology Law</span>,
27(1):1–63, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
N. Ashena, O. Inel, B. L. Persaud, and A. Bernstein.

</span>
<span class="ltx_bibblock">Casual users and rational choices within differential privacy.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">2024 IEEE Symposium on Security and Privacy (SP)</span>, pages
88–88. IEEE Computer Society, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
B. Balle, G. Barthe, and M. Gaboardi.

</span>
<span class="ltx_bibblock">Privacy amplification by subsampling: Tight analyses via couplings
and divergences.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Advances in Neural Information Processing Systems</span>, 31:1–9,
2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
B. Balle, J. Bell, A. Gascón, and K. Nissim.

</span>
<span class="ltx_bibblock">The privacy blanket of the shuffle model.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 39th Annual International Cryptology
Conference</span>, CRYPTO ‘19, pages 638–667, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S. Barocas and K. Levy.

</span>
<span class="ltx_bibblock">Privacy dependencies.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Washington Law Review</span>, 95:555–616, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
S. Benthall and R. Cummings.

</span>
<span class="ltx_bibblock">Integrating differential privacy and contextual integrity.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Proceedings of the ACM Symposium on Computer Science and
Law</span>, CSLAW ’24, page 9–15, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
S. Berghel, P. Bohannon, D. Desfontaines, C. Estes, S. Haney, L. Hartman,
M. Hay, A. Machanavajjhala, T. Magerlein, G. Miklau, A. Pai, W. Sexton, and
R. Shrestha.

</span>
<span class="ltx_bibblock">Tumult analytics: a robust, easy-to-use, scalable, and expressive
framework for differential privacy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2212.04133</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
d. boyd and J. Sarathy.

</span>
<span class="ltx_bibblock">Differential perspectives: Epistemic disconnects surrounding the US
Census Bureau’s use of differential privacy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Harvard Data Science Review</span>, Special Issue 2:172–187, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
E. Bressert.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">SciPy and NumPy: An Overview for Developers</span>.

</span>
<span class="ltx_bibblock">O’Reilly Media, Inc., 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
M. Budiu, P. Thaker, P. Gopalan, U. Wieder, and M. Zaharia.

</span>
<span class="ltx_bibblock">Overlook: Differentially private exploratory visualization for big
data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Journal of Privacy and Confidentiality</span>, 12(1):1–33, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
B. Bullek, S. Garboski, D. J. Mir, and E. M. Peck.

</span>
<span class="ltx_bibblock">Towards understanding differential privacy: When do people trust
randomized response technique?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 2017 CHI Conference on Human Factors in
Computing Systems</span>, CHI ‘17, pages 3833–3837, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
M. Bun, D. Desfontaines, C. Dwork, M. Noar, K. Nissim, A. Roth, A. Smith,
T. Steinke, J. Ullman, and S. Vadhan.

</span>
<span class="ltx_bibblock">Statistical inference is not a privacy violation, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M. Bun, J. Drechsler, M. Gaboardi, A. McMillan, and J. Sarathy.

</span>
<span class="ltx_bibblock">Controlling privacy loss in sampling schemes: An analysis of
stratified and cluster sampling.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Proceedings of 3rd Symposium on Foundations of Responsible
Computing</span>, FORC ‘22, pages 1–24, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
M. Bun and T. Steinke.

</span>
<span class="ltx_bibblock">Concentrated differential privacy: Simplifications, extensions, and
lower bounds.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Proceedings of the Theory of Cryptography Conference</span>, TCC
‘16, pages 635–658, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
A. Cavoukian.

</span>
<span class="ltx_bibblock">Privacy by design: The 7 foundational principles.

</span>
<span class="ltx_bibblock">2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
A. Cheu.

</span>
<span class="ltx_bibblock">Differential privacy in the shuffle model: A survey of separations.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2107.11839</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
A. Cohen and K. Nissim.

</span>
<span class="ltx_bibblock">Towards formalizing the gdpr’s notion of singling out.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">Proceedings of the National Academy of Sciences</span>,
117(15):8344–8352, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
J. E. Cohen.

</span>
<span class="ltx_bibblock">What privacy is for.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Harvard Law Review</span>, 126(7):1904–1933, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
E. Cowan, M. Shoemate, and M. Pereira.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Hands-on Differential Privacy</span>.

</span>
<span class="ltx_bibblock">O’Reilly Media, Inc., 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
R. Cramer, I. B. Damgård, and J. B. Nielsen.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Secure multiparty computation and secret sharing</span>.

</span>
<span class="ltx_bibblock">Cambridge University Press, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
R. Cummings, D. Desfontaines, D. Evans, R. Geambasu, M. Jagielski, Y. Huang,
P. Kairouz, G. Kamath, S. Oh, O. Ohrimenko, N. Papernot, R. Rogers, M. Shen,
S. Song, W. Su, A. Terzis, A. Thakurta, S. Vassilvitskii, Y.-X. Wang,
L. Xiong, S. Yekhanin, D. Yu, H. Zhang, and W. Zhang.

</span>
<span class="ltx_bibblock">Challenges towards the next frontier in privacy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">Harvard Data Science Review</span>, 6, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
R. Cummings, S. Hod, J. Sarathy, and M. Swanberg.

</span>
<span class="ltx_bibblock">Attaxonomy: Unpacking differential privacy guarantees against
practical adversaries.

</span>
<span class="ltx_bibblock">2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
R. Cummings, G. Kaptchuk, and E. M. Redmiles.

</span>
<span class="ltx_bibblock">“I need a better description”’: An investigation into user
expectations for differential privacy.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 2021 ACM SIGSAC Conference on Computer and
Communications Security</span>, CCS ‘21, pages 3037–3052, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
T. Dalenius and S. P. Reiss.

</span>
<span class="ltx_bibblock">Data-swapping: A technique for disclosure control.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">Journal of Statistical Planning and Inference</span>, 6(1):73–85,
1982.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
T. Dasu and T. Johnson.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Exploratory data mining and data cleaning</span>.

</span>
<span class="ltx_bibblock">John Wiley &amp; Sons, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
I. Dekel, R. Cummings, O. Heffetz, and K. Ligett.

</span>
<span class="ltx_bibblock">The privacy elasticity of behavior: Conceptualization and
application.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">Proceedings of the 24th ACM Conference on Economics and
Computation</span>, EC ’23, page 516, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
D. Desfontaines.

</span>
<span class="ltx_bibblock">A list of real-world uses of differential privacy.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://desfontain.es/blog/real-world-differential-privacy.html" title="">https://desfontain.es/blog/real-world-differential-privacy.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
D. Desfontaines.

</span>
<span class="ltx_bibblock">Local vs. central differential privacy.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://desfontain.es/blog/local-global-differential-privacy.html" title="">https://desfontain.es/blog/local-global-differential-privacy.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
J. Dong, A. Roth, and W. J. Su.

</span>
<span class="ltx_bibblock">Gaussian differential privacy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">Journal of the Royal Statistical Society Series B: Statistical
Methodology</span>, 84(1):3–37, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
J. Drechsler.

</span>
<span class="ltx_bibblock">Differential privacy for government agencies—are we there yet?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">Journal of the American Statistical Association</span>,
118(541):761–773, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
J. Drechsler, I. Globus-Harris, A. Mcmillan, J. Sarathy, and A. Smith.

</span>
<span class="ltx_bibblock">Nonparametric differentially private confidence intervals for the
median.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">Journal of Survey Statistics and Methodology</span>, 10(3):804–829,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
C. Dwork, N. Kohli, and D. Mulligan.

</span>
<span class="ltx_bibblock">Differential privacy in practice: Expose your epsilons!

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">Journal of Privacy and Confidentiality</span>, 9(2):1–22, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
C. Dwork and J. Lei.

</span>
<span class="ltx_bibblock">Differential privacy and robust statistics.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 41st Annual ACM Symposium on Theory of
computing</span>, STOC ‘09, pages 371–380, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
C. Dwork, F. McSherry, K. Nissim, and A. Smith.

</span>
<span class="ltx_bibblock">Calibrating noise to sensitivity in private data analysis.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Proceedings of the Theory of Cryptography Conference</span>, TCC
‘06, pages 265–284, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
C. Dwork and A. Roth.

</span>
<span class="ltx_bibblock">The algorithmic foundations of differential privacy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">Foundations and Trends in Theoretical Computer Science</span>,
9(3–4):211–407, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
C. Dwork and G. N. Rothblum.

</span>
<span class="ltx_bibblock">Concentrated differential privacy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:1603.01887</span>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
C. Dwork, A. Smith, T. Steinke, and J. Ullman.

</span>
<span class="ltx_bibblock">Exposed! a survey of attacks on private data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">Annual Review of Statistics and Its Application</span>, 4:61–84,
2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Ú. Erlingsson, V. Feldman, I. Mironov, A. Raghunathan, K. Talwar, and
A. Thakurta.

</span>
<span class="ltx_bibblock">Amplification by shuffling: From local to central differential
privacy via anonymity.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Proceedings of the Thirtieth Annual ACM-SIAM Symposium on
Discrete Algorithms</span>, SODA ‘19, pages 2468–2479, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Ú. Erlingsson, V. Pihur, and A. Korolova.

</span>
<span class="ltx_bibblock">RAPPOR: Randomized aggregatable privacy-preserving ordinal
response.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 2014 ACM SIGSAC Conference on Computer and
Communications Security</span>, CCS ‘14, pages 1054–1067, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
C. Ferrando, S. Wang, and D. Sheldon.

</span>
<span class="ltx_bibblock">Parametric bootstrap for differentially private confidence intervals.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">Proceedings of the International Conference on Artificial
Intelligence and Statistics</span>, AISTATS ‘22, pages 1598–1618, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
U. Fiege, A. Fiat, and A. Shamir.

</span>
<span class="ltx_bibblock">Zero knowledge proofs of identity.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">Proceedings of the nineteenth Annual ACM Symposium on Theory
of Computing</span>, STOC ‘87, pages 210–217, 1987.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
D. Franzen, S. Nuñez von Voigt, P. Sörries, F. Tschorsch, and
C. Müller-Birn.

</span>
<span class="ltx_bibblock">Am I private and if so, how many? Communicating privacy
guarantees of differential privacy with risk communication formats.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">Proceedings of the 2022 ACM SIGSAC Conference on Computer and
Communications Security</span>, CCS ’22, page 1125–1139, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
B. Friedman.

</span>
<span class="ltx_bibblock">Value-sensitive design.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">ACM Interactions</span>, 3(6):16–23, 1996.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
B. M. Frischmann, M. J. Madison, and K. J. Strandburg.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">Governing knowledge commons</span>.

</span>
<span class="ltx_bibblock">Oxford University Press, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
M. Gaboardi, M. Hay, and S. Vadhan.

</span>
<span class="ltx_bibblock">A programming framework for OpenDP, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
M. Gaboardi, J. Honaker, G. King, J. Murtagh, K. Nissim, J. Ullman, and
S. Vadhan.

</span>
<span class="ltx_bibblock">Psi: a private data sharing interface.

</span>
<span class="ltx_bibblock">2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
S. L. Garfinkel, R. Rodriguez, and P. Leclerc.

</span>
<span class="ltx_bibblock">Differential privacy at the US Census Bureau: Status report.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://csrc.nist.gov/CSRC/media/Projects/pec/documents/stppa-01-20200127-talk03-Garfinkel-diff-priv-census.pdf" title="">https://csrc.nist.gov/CSRC/media/Projects/pec/documents/stppa-01-20200127-talk03-Garfinkel-diff-priv-census.pdf</a>,
2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
G. M. Garrido, X. Liu, F. Matthes, and D. Song.

</span>
<span class="ltx_bibblock">Lessons learned: Surveying the practicality of differential privacy
in the industry.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">Proceedings of Privacy Enhancing Technologies Symposium</span>,
2023:151–170, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. D. Iii,
and K. Crawford.

</span>
<span class="ltx_bibblock">Datasheets for datasets.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">Communications of the ACM</span>, 64(12):86–92, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Q. Geng, W. Ding, R. Guo, and S. Kumar.

</span>
<span class="ltx_bibblock">Tight analysis of privacy and utility tradeoff in approximate
differential privacy.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">International Conference on Artificial Intelligence and
Statistics</span>, AISTATS ‘20, pages 89–99, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
C. Gentry.

</span>
<span class="ltx_bibblock">Fully homomorphic encryption using ideal lattices.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">Proceedings of the 41st Annual ACM Symposium on Theory of
Computing</span>, pages 169–178, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
B. Green and S. Viljoen.

</span>
<span class="ltx_bibblock">Algorithmic realism: expanding the boundaries of algorithmic thought.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">Proceedings of the 2020 Conference on Fairness,
Accountability, and Transparency</span>, FAccT ‘20, pages 19–31, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
A. Greenberg.

</span>
<span class="ltx_bibblock">Apple’s’ differential privacy is about collecting your data –
but not your data, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
S. Gürses, C. Troncoso, and C. Diaz.

</span>
<span class="ltx_bibblock">Engineering privacy by design.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">Computers, Privacy &amp; Data Protection</span>, 14(3):25, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
M. B. Hawes.

</span>
<span class="ltx_bibblock">Implementing differential privacy: Seven lessons from the 2020
United States census.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib62.1.1">Harvard Data Science Review</span>, 2(2), 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
O. Heffetz and K. Ligett.

</span>
<span class="ltx_bibblock">Privacy and data-based research.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib63.1.1">Journal of Economic Perspectives</span>, 28(2):75–98, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
S. Hod and R. Canetti.

</span>
<span class="ltx_bibblock">Differentially private release of israel’s national registry of live
births.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib64.1.1">arXiv preprint arXiv:2405.00267</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
N. Holohan, S. Braghin, P. Mac Aonghusa, and K. Levacher.

</span>
<span class="ltx_bibblock">Diffprivlib: the IBM differential privacy library.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib65.1.1">arXiv e-prints</span>, 1907.02444, July 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
J. Hsu, M. Gaboardi, A. Haeberlen, S. Khanna, A. Narayan, B. C. Pierce, and
A. Roth.

</span>
<span class="ltx_bibblock">Differential privacy: An economic method for choosing epsilon.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib66.1.1">Proceedings of the 27th IEEE Computer Security Foundations
Symposium</span>, CSF ‘14, pages 398–410, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
F. Karegar, A. S. Alaqra, and S. Fischer-Hübner.

</span>
<span class="ltx_bibblock">Exploring <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib67.1.m1.1"><semantics id="bib.bib67.1.m1.1a"><mo id="bib.bib67.1.m1.1.1" stretchy="false" xref="bib.bib67.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib67.1.m1.1b"><ci id="bib.bib67.1.m1.1.1.cmml" xref="bib.bib67.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib67.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib67.1.m1.1d">{</annotation></semantics></math>User-Suitable<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib67.2.m2.1"><semantics id="bib.bib67.2.m2.1a"><mo id="bib.bib67.2.m2.1.1" stretchy="false" xref="bib.bib67.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib67.2.m2.1b"><ci id="bib.bib67.2.m2.1.1.cmml" xref="bib.bib67.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib67.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib67.2.m2.1d">}</annotation></semantics></math> metaphors for differentially private
data analyses.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib67.3.1">Eighteenth Symposium on Usable Privacy and Security (SOUPS
2022)</span>, pages 175–193, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
P. G. Kelley, J. Bresee, L. F. Cranor, and R. W. Reeder.

</span>
<span class="ltx_bibblock">A “nutrition label” for privacy.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib68.1.1">Proceedings of the 5th Symposium on Usable Privacy and
Security</span>, SOUPS ’09, pages 1–12, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
C. T. Kenny, S. Kuriwaki, C. McCartan, E. T. Rosenman, T. Simko, and K. Imai.

</span>
<span class="ltx_bibblock">The use of differential privacy for census data and its impact on
redistricting: The case of the 2020 US census.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib69.1.1">Science Advances</span>, 7(41):eabk3283, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
D. Kifer and A. Machanavajjhala.

</span>
<span class="ltx_bibblock">No free lunch in data privacy.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib70.1.1">Proceedings of the 2011 ACM SIGMOD International Conference
on Management of data</span>, pages 193–204, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
N. Kohli and P. Laskowski.

</span>
<span class="ltx_bibblock">Epsilon voting: Mechanism design for parameter selection in
differential privacy.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib71.1.1">Proceedings of the 2018 IEEE Symposium on Privacy-Aware
Computing</span>, PAC ‘18, pages 19–30, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
B. Li, D. Micciancio, M. Schultz, and J. Sorrell.

</span>
<span class="ltx_bibblock">Securing approximate homomorphic encryption using differential
privacy.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib72.1.1">Proceedings of the Annual International Cryptology
Conference</span>, CRYPTO ‘22, pages 560–589, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
T. Liu, G. Vietri, T. Steinke, J. Ullman, and S. Wu.

</span>
<span class="ltx_bibblock">Leveraging public data for practical private query release.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib73.1.1">Proceedings of the International Conference on Machine
Learning</span>, ICML ‘21, pages 6968–6977, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
A. Machanavajjhala, D. Kifer, J. Abowd, J. Gehrke, and L. Vilhuber.

</span>
<span class="ltx_bibblock">Privacy: Theory meets practice on the map.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib74.1.1">Proceedings of the 2008 IEEE 24th International Conference on
Data Engineering</span>, ICDE ‘08, pages 277–286, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
G. J. Matthews and O. Harel.

</span>
<span class="ltx_bibblock">Data confidentiality: A review of methods for statistical disclosure
limitation and methods for assessing privacy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib75.1.1">Statistics Surveys</span>, 5(1):1–29, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
T. S. Mayer.

</span>
<span class="ltx_bibblock">Privacy and confidentiality research and the US Census Bureau
recommendations based on a review of the literature.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib76.1.1">Survey Methodology</span>, (1):1–51, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
E. McCallister, T. Grance, and K. A. Scarfone.

</span>
<span class="ltx_bibblock">SP 800-122. Guide to protecting the confidentiality of personally
identifiable information (PII).

</span>
<span class="ltx_bibblock">Technical report, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
M. McLure, A. V. Level, C. L. Cranston, B. Oehlerts, and M. Culbertson.

</span>
<span class="ltx_bibblock">Data curation: A study of researcher practices and needs.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib78.1.1">portal: Libraries and the Academy</span>, 14(2):139–164, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
L. Mehner, S. N. von Voigt, and F. Tschorsch.

</span>
<span class="ltx_bibblock">Towards explaining epsilon: A worst-case study of differential
privacy risks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib79.1.1">Proceedings of the 2021 IEEE European Symposium on Security
and Privacy Workshops</span>, pages 328–331, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
M. Miceli and J. Posada.

</span>
<span class="ltx_bibblock">The data-production dispositif.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib80.1.1">Proceedings of the ACM on Human-Computer Interaction</span>,
6(CSCW2):1–37, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
I. Mironov.

</span>
<span class="ltx_bibblock">Rényi differential privacy.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib81.1.1">Proceedings of the 2017 IEEE 30th Computer Security
Foundations Symposium</span>, CSF ‘17, pages 263–275, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
D. K. Mulligan, C. Koopman, and N. Doty.

</span>
<span class="ltx_bibblock">Privacy is an essentially contested concept: a multi-dimensional
analytic for mapping privacy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib82.1.1">Philosophical Transactions of the Royal Society A: Mathematical,
Physical and Engineering Sciences</span>, 374(2083):20160118, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
P. Nanayakkara, J. Bater, X. He, J. Hullman, and J. Rogers.

</span>
<span class="ltx_bibblock">Visualizing privacy-utility trade-offs in differentially private data
releases.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib83.1.1">Proceedings on Privacy Enhancing Technologies Symposium</span>,
PETS ‘22, pages 601–618, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
P. Nanayakkara, M. A. Smart, R. Cummings, G. Kaptchuk, and E. M. Redmiles.

</span>
<span class="ltx_bibblock">What are the chances? Explaining the epsilon parameter in
differential privacy.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib84.1.1">Proceedings of the 32nd USENIX Security Symposium</span>, USENIX
Security ‘23, pages 1613–1630, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
A. Narayanan and V. Shmatikov.

</span>
<span class="ltx_bibblock">Robust de-anonymization of large sparse datasets.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib85.1.1">2008 IEEE Symposium on Security and Privacy (sp 2008)</span>, pages
111–125, USA, 2008. IEEE Computer Society.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
U. Nations.

</span>
<span class="ltx_bibblock">U.N. fundamental principles of official statistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://unstats.un.org/unsd/dnss/gp/Implementation_Guidelines_FINAL_without_edit.pdf" title="">https://unstats.un.org/unsd/dnss/gp/Implementation_Guidelines_FINAL_without_edit.pdf</a>,
2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
J. Near.

</span>
<span class="ltx_bibblock">Differential privacy at scale: Uber and Berkeley collaboration,
2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
J. P. Near and C. Abuah.

</span>
<span class="ltx_bibblock">Programming differential privacy, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
I. C. Ngong, B. Stenger, J. P. Near, and Y. Feng.

</span>
<span class="ltx_bibblock">Evaluating the usability of differential privacy tools with data
practitioners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib89.1.1">arXiv preprint arXiv:2309.13506</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
H. Nissenbaum.

</span>
<span class="ltx_bibblock">Privacy as contextual integrity.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib90.1.1">Washinton Law Review</span>, 79:119–158, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
K. Nissim, A. Bembenek, A. Wood, M. Bun, M. Gaboardi, U. Gasser, D. R. O’Brien,
T. Steinke, and S. Vadhan.

</span>
<span class="ltx_bibblock">Bridging the gap between computer science and legal approaches to
privacy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib91.1.1">Harvard Journal of Law &amp; Technology</span>, 31:687–780, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
K. Nissim and A. Wood.

</span>
<span class="ltx_bibblock">Foundations for robust data protection: Co-designing law and computer
science.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib92.1.1">2021 Third IEEE International Conference on Trust, Privacy
and Security in Intelligent Systems and Applications (TPS-ISA)</span>, pages
235–242. IEEE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
N. I. of Standards and Technology.

</span>
<span class="ltx_bibblock">NIST privacy framework.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nist.gov/privacy-framework/" title="">https://www.nist.gov/privacy-framework/</a>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
E. O. of the President.

</span>
<span class="ltx_bibblock">Memorandum for the heads of executive departments and agencies:
Broadening public participation and community engagement in the regulatory
process, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
C. on Professional Ethics of the American Statistical Association.

</span>
<span class="ltx_bibblock">Ethical guidelines for statistical practice.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.amstat.org/your-career/ethical-guidelines-for-statistical-practice" title="">https://www.amstat.org/your-career/ethical-guidelines-for-statistical-practice</a>,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
A. Pankova and P. Laud.

</span>
<span class="ltx_bibblock">Interpreting epsilon of differential privacy in terms of advantage in
guessing or approximating sensitive attributes.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib96.1.1">Proceedings of the 2022 IEEE 35th Computer Security
Foundations Symposium</span>, CSF ‘22, pages 96–111, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
M. Pettai and P. Laud.

</span>
<span class="ltx_bibblock">Combining differential privacy and secure multiparty computation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib97.1.1">Proceedings of the 31st Annual Computer Security Applications
Conference</span>, pages 421–430, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
D. B. Rubin.

</span>
<span class="ltx_bibblock">Statistical disclosure limitation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib98.1.1">Journal of Official Statistics</span>, 9(2):461–468, 1993.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
S. Ruggles, C. Fitch, D. Magnuson, and J. Schroeder.

</span>
<span class="ltx_bibblock">Differential privacy and census data: Implications for social and
economic research.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib99.1.1">AEA Papers and Proceedings</span>, volume 109, pages 403–408,
2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
A. R. Santos-Lozada, J. T. Howard, and A. M. Verdery.

</span>
<span class="ltx_bibblock">How differential privacy will affect our understanding of health
disparities in the United States.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib100.1.1">Proceedings of the National Academy of Sciences</span>,
117(24):13405–13412, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
J. Sarathy.

</span>
<span class="ltx_bibblock">From algorithmic to institutional logics: the politics of
differential privacy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib101.1.1">SSRN pre-print 4079222</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
J. Sarathy, S. Song, A. Haque, T. Schlatter, and S. Vadhan.

</span>
<span class="ltx_bibblock">Don’t look at the data! How differential privacy reconfigures the
practices of data science.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib102.1.1">Proceedings of the 2023 CHI Conference on Human Factors in
Computing Systems</span>, CHI ‘23, pages 1–19, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
J. Seeman, A. Slavkovic, and M. Reimherr.

</span>
<span class="ltx_bibblock">Private posterior inference consistent with public information: A
case study in small area estimation from synthetic census data.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib103.1.1">Proceedings of the International Conference on Privacy in
Statistical Databases</span>, pages 323–336, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
J. Seeman and D. Susser.

</span>
<span class="ltx_bibblock">Between privacy and utility: On differential privacy in theory and
practice.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib104.1.1">ACM Journal of Responsible Computing</span>, 1(1), 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
M. A. Smart, D. Sood, and K. Vaccaro.

</span>
<span class="ltx_bibblock">Understanding risks of privacy theater with differential privacy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib105.1.1">Proceedings of the ACM on Human-Computer Interaction</span>,
6(CSCW2):1–24, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
P. Song, J. Sarathy, M. Shoemate, and S. Vadhan.

</span>
<span class="ltx_bibblock">“I inherently just trust that it works”’: Investigating mental
models of open-source libraries for differential privacy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib106.1.1">Proceedings of the ACM on Human-computer Interaction</span>, (CSCW2),
2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
R. Steed, T. Liu, Z. S. Wu, and A. Acquisti.

</span>
<span class="ltx_bibblock">Policy impacts of statistical uncertainty and privacy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib107.1.1">Science</span>, 377(6609):928–931, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
A. Tang and A. Singh.

</span>
<span class="ltx_bibblock">Privacy in the public sector: Lessons learned and strategies for
success.

</span>
<span class="ltx_bibblock">Santa Clara, CA, Sept. 2023. USENIX Association.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
J. Tang, A. Korolova, X. Bai, X. Wang, and X. Wang.

</span>
<span class="ltx_bibblock">Privacy loss in Apple’s implementation of differential privacy on
macOS 10.12.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib109.1.1">arXiv preprint arXiv:1709.02753</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
S. Viljoen.

</span>
<span class="ltx_bibblock">A relational theory of data governance.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib110.1.1">The Yale Law Journal</span>, 131:573–654, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
S. Wagh, X. He, A. Machanavajjhala, and P. Mittal.

</span>
<span class="ltx_bibblock">DP-cryptography: marrying differential privacy and cryptography in
emerging applications.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib111.1.1">Communications of the ACM</span>, 64(2):84–93, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
Y. Wang, D. Kifer, and J. Lee.

</span>
<span class="ltx_bibblock">Differentially private confidence intervals for empirical risk
minimization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib112.1.1">Journal of Privacy and Confidentiality</span>, 9(1), 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
Z. A. Wen, J. Jia, H. Yan, Y. Yao, Z. Liu, and C. Dong.

</span>
<span class="ltx_bibblock">The influence of explanation designs on user understanding
differential privacy and making data-sharing decision.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib113.1.1">Information Sciences</span>, 642:118799, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
A. Wood, M. Altman, A. Bembenek, M. Bun, M. Gaboardi, J. Honaker, K. Nissim,
D. R. O’Brien, T. Steinke, and S. Vadhan.

</span>
<span class="ltx_bibblock">Differential privacy: A primer for a non-technical audience.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib114.1.1">Vanderbilt Journal of Entertainment &amp; Technology Law</span>,
21:209–276, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
A. Wood, M. Altman, K. Nissim, and S. Vadhan.

</span>
<span class="ltx_bibblock">Designing access with differential privacy.

</span>
<span class="ltx_bibblock">In S. Cole, I. Dhaliwal, A. Sautmann, and L. Vilhuber, editors, <span class="ltx_text ltx_font_italic" id="bib.bib115.1.1">Handbook on Using Administrative Data for Research and Evidence-based
Policy</span>. 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
A. Xiong, T. Wang, N. Li, and S. Jha.

</span>
<span class="ltx_bibblock">Towards effective differential privacy communication for users’
data sharing decision and comprehension.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib116.1.1">Proceedings of the 2020 IEEE Symposium on Security and
Privacy</span>, S&amp;P ‘20, pages 392–410, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
A. Xiong, C. Wu, T. Wang, R. W. Proctor, J. Blocki, N. Li, and S. Jha.

</span>
<span class="ltx_bibblock">Exploring use of explanative illustrations to communicate
differential privacy models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib117.1.1">Proceedings of the Human Factors and Ergonomics Society
Annual Meeting</span>, volume 67, pages 226–232, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
L. Zayatz.

</span>
<span class="ltx_bibblock">Disclosure avoidance practices and research at the US Census
Bureau: An update.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib118.1.1">Journal of Official Statistics</span>, 23(2):253, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
W. Zhang, O. Ohrimenko, and R. Cummings.

</span>
<span class="ltx_bibblock">Attribute privacy: Framework and mechanisms.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib119.1.1">Proceedings of the 2022 ACM Conference on Fairness,
Accountability, and Transparency</span>, FAccT ‘22, pages 757–766, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Jun 17 21:31:02 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
