<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2311.14764] SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions</title><meta property="og:description" content="High-quality training data is essential for enhancing the robustness of object detection models. Within the maritime domain, obtaining a diverse real image dataset is particularly challenging due to the difficulty of c‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2311.14764">

<!--Generated on Tue Feb 27 17:29:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Martin Tran
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text" style="font-size:90%;">Signal Processing, Artificial Intelligence and Vision Technologies (SAIVT), Queensland University of Technology, Australia</span>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jordan Shipard
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.1.id1" class="ltx_text" style="font-size:90%;">Signal Processing, Artificial Intelligence and Vision Technologies (SAIVT), Queensland University of Technology, Australia</span>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hermawan Mulyono
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Sentient Vision Systems, Australia
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Arnold Wiliem
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text" style="font-size:90%;">Signal Processing, Artificial Intelligence and Vision Technologies (SAIVT), Queensland University of Technology, Australia</span>
</span>
<span class="ltx_contact ltx_role_affiliation">Sentient Vision Systems, Australia
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Clinton Fookes
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text" style="font-size:90%;">Signal Processing, Artificial Intelligence and Vision Technologies (SAIVT), Queensland University of Technology, Australia</span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p"><span id="id5.id1.1" class="ltx_text" style="font-size:90%;">High-quality training data is essential for enhancing the robustness of object detection models. Within the maritime domain, obtaining a diverse real image dataset is particularly challenging due to the difficulty of capturing sea images with the presence of maritime objects , especially in stormy conditions. These challenges arise due to resource limitations, in addition to the unpredictable appearance of maritime objects. Nevertheless, acquiring data from stormy conditions is essential for training effective maritime detection models, particularly for search and rescue, where real-world conditions can be unpredictable. In this work, we introduce SafeSea, which is a stepping stone towards transforming actual sea images with various Sea State backgrounds while retaining maritime objects. Compared to existing generative methods such as Stable Diffusion Inpainting¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, this approach reduces the time and effort required to create synthetic datasets for training maritime object detection models. The proposed method uses two automated filters to only pass generated images that meet the criteria. In particular, these filters will first classify the sea condition according to its Sea State level and then it will check whether the objects from the input image are still preserved. This method enabled the creation of the SafeSea dataset, offering diverse weather condition backgrounds to supplement the training of maritime models. Lastly, we observed that a maritime object detection model faced challenges in detecting objects in stormy sea backgrounds, emphasizing the impact of weather conditions on detection accuracy. The code, and dataset are available at <a target="_blank" href="https://github.com/martin-3240/SafeSea" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/martin-3240/SafeSea</a>.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2311.14764/assets/sections/images/method.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="401" height="430" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Diagram summarizing our method (SafeSea). Original maritime images‚Äô sea background is edited with a mask and text description using Blended Latent Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>. The edited images are then classified into 4 Sea State categories before their marine objects (boats) are cropped according to the ground truth bounding box and checked for preservation.</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span id="S1.p1.1.1" class="ltx_text" style="font-size:90%;">Climate change is increasing the likelihood of extreme weather events, such as large storms </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a><span id="S1.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p1.1.4" class="ltx_text" style="font-size:90%;">, which can be destructive, especially for humans and boats in stormy waters. Quick and accurate rescue efforts are crucial in such situations as death can occur in as little as 50 minutes¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a><span id="S1.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p1.1.7" class="ltx_text" style="font-size:90%;">. Automated disaster response in the ocean is a growing area that uses advanced technology to help search and rescue in extreme weather events. Leveraging autonomous systems, remote sensing technologies, artificial intelligence, and real-time data analysis, automated disaster response aims to minimise risks, and accelerate response times. However, detecting entities in need of rescue during severe conditions in the ocean is often challenging. Therefore, there is a need for automated disaster responses that can operate effectively in these harsh conditions and can precisely locate objects, vessels, and people requiring assistance.</span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p"><span id="S1.p2.1.1" class="ltx_text" style="font-size:90%;">Developing robust models to automatically detect objects in the sea during extreme weather conditions is a challenging task. The challenge primarily arises from the difficulty of obtaining high-quality data in such chaotic and unpredictable situations. Adverse weather conditions, particularly during storms, pose a safety risk, preventing the deployment of cameras in critical areas for data collection. Furthermore, the wild nature of disaster scenes often stretches already limited resources, including personnel, equipment, and communication channels, thus constraining the capacity for data acquisition </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a><span id="S1.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.4" class="ltx_text" style="font-size:90%;">. Due to the lack of datasets for training models to effectively detect maritime objects in the ocean during severe weather conditions, generating synthetic datasets to train and test object detection models can be highly beneficial.</span></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p"><span id="S1.p3.1.1" class="ltx_text" style="font-size:90%;">By harnessing recent generative models such as Stable Diffusion (SD)¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a><span id="S1.p3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.4" class="ltx_text" style="font-size:90%;">, and DALL-E¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a><span id="S1.p3.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.7" class="ltx_text" style="font-size:90%;">, it is now possible to generate realistic images based on textual descriptions. However, generating synthetic images solely based on textual descriptions experiences limitations in specifying object location, size, and type within the ocean scene, restricting the replication of realistic scenarios. To overcome this challenge, a more practical approach is to utilize real sea images with marine objects and performing transformations on the sea background.</span></p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p"><span id="S1.p4.1.1" class="ltx_text" style="font-size:90%;">Unfortunately, when applying the Stable Diffusion model to edit the real sea image‚Äôs backgrounds, our experiments reveal a significant limitation in using text prompts to precisely control the desired Sea State. The edited background randomly appears not according to the description, so it would require checking manually to ensure the image‚Äôs quality. Consequently, this process becomes time-consuming as it requires manual control to achieve a visually satisfactory Sea State. Furthermore, we found that existing image editing methods often replace or excessively modify the objects in the edited images, making the images unusable. Therefore, relying solely on synthetic image generation techniques to create a dataset with diverse Sea State Levels proves to be challenging, as it demands a substantial amount of time for generation and manual verification.</span></p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text" style="font-size:90%;">To tackle this issue, we propose a method for editing images of calm ocean scenes into stormy ocean scenes, focusing on UAV-view images. We replace the original calm ocean with an ocean environment corresponding to standard definitions from a Bureau of Meteorology as depicted in Table </span><a href="#S2.T1" title="Table 1 ‚Ä£ 2 Related Works ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S1.p5.1.2" class="ltx_text" style="font-size:90%;">, all while attempting to retain the maritime objects in the images¬†</span><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>In this work we confine ourselves to only retaining the boats and reserving the other marine objects for future work.</span></span></span><span id="S1.p5.1.3" class="ltx_text" style="font-size:90%;">.
This method shows as a proof-of-concept to develop a simple-yet-effective approach, capable of automatically screening out poor-quality edited images with overly edited preserved objects.</span></p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text" style="font-size:90%;">Our work takes an input image and use an image generation method to perform background transformation.
Recent work proposed the Blended Latent Diffusion ¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a><span id="S1.p6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p6.1.4" class="ltx_text" style="font-size:90%;"> which demonstrates significantly improved results over the Stable Diffusion model in preserving foreground objects while also translating the background. Therefore, we employed Blended Latent Diffusion to modify images with object masking. Due to the stochastic nature of diffusion¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p6.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a><span id="S1.p6.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p6.1.7" class="ltx_text" style="font-size:90%;">, we find that prompts alone can be unreliable for producing images according to the sea state definitions. Thus we apply the Sea State Classifier which classifies the transformed image into one of the sea state definitions. Then, the Object Preservation Checker is applied to evaluate whether the transformed image still preserve the objects from the input image.</span></p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p"><span id="S1.p7.1.1" class="ltx_text" style="font-size:90%;">We leverage the SeaDroneSee dataset¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a><span id="S1.p7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p7.1.4" class="ltx_text" style="font-size:90%;"> for experimentation and validation. To quantitatively evaluate the effectiveness of our approach, we compare it against other image editing methods, such as Stable Diffusion Inpainting </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p7.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a><span id="S1.p7.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p7.1.7" class="ltx_text" style="font-size:90%;">. Furthermore, we conducted object detection tests using YoloV5 model pre-trained on calm sea state images </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p7.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a><span id="S1.p7.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p7.1.10" class="ltx_text" style="font-size:90%;"> to assess the impact of various sea states on object recognisability in edited images. Our findings reveal that the pre-trained object detection model struggles to identify objects in increasingly stormy conditions. This suggests that the model‚Äôs performance is less effective when it encounters previously unseen stormy sea backgrounds, making it more challenging to detect objects in rough sea surface conditions.</span></p>
</div>
<div id="S1.p8" class="ltx_para ltx_noindent">
<p id="S1.p8.1" class="ltx_p"><span id="S1.p8.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Contributions - </span><span id="S1.p8.1.2" class="ltx_text" style="font-size:90%;"> We list our contributions as follows:</span></p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text" style="font-size:90%;">We propose a simple-yet-effective method for modifying the sea state level of the ocean in maritime images while preserving the objects of interest, enhancing their utility for various applications;</span></p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text" style="font-size:90%;">We construct and propose a synthetic dataset, the SafeSea dataset, enabling the training of models to accommodate diverse weather conditions;</span></p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text" style="font-size:90%;">We conduct an evaluation of the SafeSea dataset with YoloV5 to assess the model‚Äôs performance under varying weather conditions.</span></p>
</div>
</li>
</ol>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p"><span id="S1.p9.1.1" class="ltx_text" style="font-size:90%;">We continue our paper as follows. Section¬†</span><a href="#S2" title="2 Related Works ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S1.p9.1.2" class="ltx_text" style="font-size:90%;"> presents an overview of prior work including maritime datasets and diffusion models. In Section¬†</span><a href="#S3" title="3 Problem Definition ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S1.p9.1.3" class="ltx_text" style="font-size:90%;">, we define our problem and the goal we want to achieve before Section¬†</span><a href="#S4" title="4 Proposed SafeSea method ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S1.p9.1.4" class="ltx_text" style="font-size:90%;"> outlines our proposed SafeSea method designed to accomplish this goal. The description of the SafeSea dataset are presented in Section¬†</span><a href="#S5" title="5 SafeSea dataset ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="S1.p9.1.5" class="ltx_text" style="font-size:90%;">. Section¬†</span><a href="#S6" title="6 Experiment ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">6</span></a><span id="S1.p9.1.6" class="ltx_text" style="font-size:90%;"> presents our experiment and discussion. We conclude the paper and discuss about limitations and future work in Sections¬†</span><a href="#S7" title="7 Conclusion ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">7</span></a><span id="S1.p9.1.7" class="ltx_text" style="font-size:90%;"> and Section¬†</span><a href="#S8" title="8 Limitation and Future Work ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">8</span></a><span id="S1.p9.1.8" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Sea Datasets - </span><span id="S2.p1.1.2" class="ltx_text" style="font-size:90%;"> Numerous public maritime datasets facilitate the training of marine object detection models. Notable examples include the VAIS dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a><span id="S2.p1.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.5" class="ltx_text" style="font-size:90%;">, offering over 1,000 RGB and infrared image pairs, showcasing various ship types. The IPATCH dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a><span id="S2.p1.1.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.8" class="ltx_text" style="font-size:90%;"> records realistic maritime piracy scenarios, while the SeaShips dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.9.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a><span id="S2.p1.1.10.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.11" class="ltx_text" style="font-size:90%;"> features ship images from inland waterways. The ‚ÄôSingapore Maritime Dataset‚Äô </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.12.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a><span id="S2.p1.1.13.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.14" class="ltx_text" style="font-size:90%;"> captures marine objects using onshore platforms and vessels, providing diverse perspectives. Additionally, the Seagull dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.15.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a><span id="S2.p1.1.16.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.17" class="ltx_text" style="font-size:90%;"> and Maritime SATellite Imagery (MASATI) dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.18.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a><span id="S2.p1.1.19.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.20" class="ltx_text" style="font-size:90%;"> offer aerial images. The SeaDronesSee dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.21.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a><span id="S2.p1.1.22.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.23" class="ltx_text" style="font-size:90%;"> contains over 54,000 frames with around 400,000 instances. Despite their strengths, these datasets often focus on specific objects like ships and boats, and often the objects of interest are relatively large compared to the image size in certain datasets. Furthermore, most of the images are taken under good weather conditions.</span></p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Synthetic Datasets - </span><span id="S2.p2.1.2" class="ltx_text" style="font-size:90%;"> Extensive research explores methodologies employing synthetic images for training object detection models. Noteworthy contributions include Peng et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a><span id="S2.p2.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p2.1.5" class="ltx_text" style="font-size:90%;"> emphasizing the refinement of synthetic object backgrounds for improved detection reliability. The use of powerful game engines like Unity¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a><span id="S2.p2.1.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p2.1.8" class="ltx_text" style="font-size:90%;"> and Unreal Engine¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.9.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a><span id="S2.p2.1.10.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p2.1.11" class="ltx_text" style="font-size:90%;"> is prominent, as demonstrated by Becktor et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.12.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a><span id="S2.p2.1.13.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p2.1.14" class="ltx_text" style="font-size:90%;"> in the context of spacecraft guidance and control. Unreal Engine 4 ¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.15.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a><span id="S2.p2.1.16.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p2.1.17" class="ltx_text" style="font-size:90%;"> has proven valuable in autonomous driving, Dosovitskiy et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.18.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a><span id="S2.p2.1.19.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p2.1.20" class="ltx_text" style="font-size:90%;">, and maritime image generation, as utilized by Becktor et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.21.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a><span id="S2.p2.1.22.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p2.1.23" class="ltx_text" style="font-size:90%;">. Kiefer et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.24.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a><span id="S2.p2.1.25.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p2.1.26" class="ltx_text" style="font-size:90%;"> analyzed maritime and terrestrial images, incorporating real and synthetic data from the Grand Theft Auto V (GTAV) simulation platform </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.27.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a><span id="S2.p2.1.28.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p2.1.29" class="ltx_text" style="font-size:90%;">. Xiaomin et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.30.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a><span id="S2.p2.1.31.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p2.1.32" class="ltx_text" style="font-size:90%;"> introduced SeeDroneSim, utilizing the Blender game engine </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.33.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a><span id="S2.p2.1.34.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p2.1.35" class="ltx_text" style="font-size:90%;"> for UAV-based maritime images. Airbus </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p2.1.36.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a><span id="S2.p2.1.37.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p2.1.38" class="ltx_text" style="font-size:90%;"> released a 2018 dataset of 40,000 satellite images designed for ship detection using synthetic aperture radar (SAR) technology. While existing datasets focus on synthetic objects of interest, our work concentrates on generating diverse environmental conditions based on real data.</span></p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Diffusion Models - </span><span id="S2.p3.1.2" class="ltx_text" style="font-size:90%;"> Diffusion models have been widely employed for image transformation in various contexts. Trabucco, Doherty et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a><span id="S2.p3.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.5" class="ltx_text" style="font-size:90%;"> employed pre-trained text-to-image diffusion models for semantic-based image modification. Shin et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a><span id="S2.p3.1.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.8" class="ltx_text" style="font-size:90%;"> utilized Stable Diffusion for image generation using the Textual Inversion </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.9.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a><span id="S2.p3.1.10.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.11" class="ltx_text" style="font-size:90%;"> method. Ron et al. proposed the Null-text Inversion method </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.12.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a><span id="S2.p3.1.13.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.14" class="ltx_text" style="font-size:90%;">, employing prompt-to-prompt text editing for image denoising. Rombach et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.15.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a><span id="S2.p3.1.16.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.17" class="ltx_text" style="font-size:90%;"> introduced Stable Diffusion Inpainting for image inpainting using masks and a latent text-to-image diffusion model. Omri et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.18.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a><span id="S2.p3.1.19.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.20" class="ltx_text" style="font-size:90%;"> accelerated the transformation process with a lower-dimensional latent space. Our selection of the Blended Latent Diffusion approach </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.21.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a><span id="S2.p3.1.22.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.23" class="ltx_text" style="font-size:90%;"> is based on its demonstrated superior results in editing image backgrounds.</span></p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text" style="font-size:90%;">Presently available real-image maritime datasets have limitations, including fixed camera positions and a restricted variety of marine objects. Furthermore, they often lack diversity in representing various weather conditions reflecting on the sea background. This lack of diversity can restrain the development of a high-quality dataset for training deep-learning models. To address this challenge, numerous studies have been conducted to generate high-quality synthetic images capable of replicating real-world scenarios. These synthetic images can be produced by leveraging game engines such as Unity </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a><span id="S2.p4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p4.1.4" class="ltx_text" style="font-size:90%;"> and Unreal Engine </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p4.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a><span id="S2.p4.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p4.1.7" class="ltx_text" style="font-size:90%;"> or by employing diffusion models to modify real images</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p4.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a><span id="S2.p4.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p4.1.10" class="ltx_text" style="font-size:90%;">. While using game engines can be resource-intensive, editing images with diffusion models offers a simpler approach. Unfortunately, the editing process via diffusion models is still time consuming and labour intensive due to imprecise control that specifies which part of the image that needs to be edited.
This works proposes a proof-of-concept to enable automation in the editing process which significantly reduces the time and labour whilst maintaining the quality of the edited images.</span></p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.2.1.1" class="ltx_tr">
<th id="S2.T1.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row"><span id="S2.T1.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Sea State</span></th>
<th id="S2.T1.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S2.T1.2.1.1.2.1" class="ltx_text" style="font-size:90%;">Definition</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.2.2.1" class="ltx_tr">
<th id="S2.T1.2.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S2.T1.2.2.1.1.1" class="ltx_text" style="font-size:90%;">1</span></th>
<td id="S2.T1.2.2.1.2" class="ltx_td ltx_align_left ltx_border_tt">
<span id="S2.T1.2.2.1.2.1" class="ltx_text" style="font-size:90%;"></span>
<span id="S2.T1.2.2.1.2.2" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:176.4pt;">
<span id="S2.T1.2.2.1.2.2.1" class="ltx_p"><span id="S2.T1.2.2.1.2.2.1.1" class="ltx_text" style="font-size:90%;">The water exhibits a gentle ripple, devoid of breaking waves, featuring a low swell of short to average length occasionally.</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.2.3.2" class="ltx_tr">
<th id="S2.T1.2.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S2.T1.2.3.2.1.1" class="ltx_text" style="font-size:90%;">2</span></th>
<td id="S2.T1.2.3.2.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S2.T1.2.3.2.2.1" class="ltx_text" style="font-size:90%;"></span>
<span id="S2.T1.2.3.2.2.2" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:176.4pt;">
<span id="S2.T1.2.3.2.2.2.1" class="ltx_p"><span id="S2.T1.2.3.2.2.2.1.1" class="ltx_text" style="font-size:90%;">Slight waves breaking, with smooth waves on the water surface</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.2.4.3" class="ltx_tr">
<th id="S2.T1.2.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S2.T1.2.4.3.1.1" class="ltx_text" style="font-size:90%;">3</span></th>
<td id="S2.T1.2.4.3.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S2.T1.2.4.3.2.1" class="ltx_text" style="font-size:90%;"></span>
<span id="S2.T1.2.4.3.2.2" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:176.4pt;">
<span id="S2.T1.2.4.3.2.2.1" class="ltx_p"><span id="S2.T1.2.4.3.2.2.1.1" class="ltx_text" style="font-size:90%;">Mildly increased waves, leading to some rock buoys and causing minor disturbances for small craft</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.2.5.4" class="ltx_tr">
<th id="S2.T1.2.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t"><span id="S2.T1.2.5.4.1.1" class="ltx_text" style="font-size:90%;">4</span></th>
<td id="S2.T1.2.5.4.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_t">
<span id="S2.T1.2.5.4.2.1" class="ltx_text" style="font-size:90%;"></span>
<span id="S2.T1.2.5.4.2.2" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:176.4pt;">
<span id="S2.T1.2.5.4.2.2.1" class="ltx_p"><span id="S2.T1.2.5.4.2.2.1.1" class="ltx_text" style="font-size:90%;">The sea takes on a furrowed appearance, characterized by moderate waves</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Sea States descriptions as defined by the ABS</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">3 </span>Problem Definition</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.24" class="ltx_p"><span id="S3.p1.24.1" class="ltx_text" style="font-size:90%;">Here we technically define our problem. Starting from an image </span><math id="S3.p1.1.m1.1" class="ltx_Math" alttext="{\boldsymbol{X}}" display="inline"><semantics id="S3.p1.1.m1.1a"><mi mathsize="90%" id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">ùëø</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">ùëø</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">{\boldsymbol{X}}</annotation></semantics></math><span id="S3.p1.24.2" class="ltx_text" style="font-size:90%;"> containing a set of objects, </span><math id="S3.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{B}=\{{\boldsymbol{b}}_{i}\}^{N}_{i=1}" display="inline"><semantics id="S3.p1.2.m2.1a"><mrow id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml">‚Ñ¨</mi><mo mathsize="90%" id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml">=</mo><msubsup id="S3.p1.2.m2.1.1.1" xref="S3.p1.2.m2.1.1.1.cmml"><mrow id="S3.p1.2.m2.1.1.1.1.1.1" xref="S3.p1.2.m2.1.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S3.p1.2.m2.1.1.1.1.1.1.2" xref="S3.p1.2.m2.1.1.1.1.1.2.cmml">{</mo><msub id="S3.p1.2.m2.1.1.1.1.1.1.1" xref="S3.p1.2.m2.1.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S3.p1.2.m2.1.1.1.1.1.1.1.2" xref="S3.p1.2.m2.1.1.1.1.1.1.1.2.cmml">ùíÉ</mi><mi mathsize="90%" id="S3.p1.2.m2.1.1.1.1.1.1.1.3" xref="S3.p1.2.m2.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo maxsize="90%" minsize="90%" id="S3.p1.2.m2.1.1.1.1.1.1.3" xref="S3.p1.2.m2.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.p1.2.m2.1.1.1.3" xref="S3.p1.2.m2.1.1.1.3.cmml"><mi mathsize="90%" id="S3.p1.2.m2.1.1.1.3.2" xref="S3.p1.2.m2.1.1.1.3.2.cmml">i</mi><mo mathsize="90%" id="S3.p1.2.m2.1.1.1.3.1" xref="S3.p1.2.m2.1.1.1.3.1.cmml">=</mo><mn mathsize="90%" id="S3.p1.2.m2.1.1.1.3.3" xref="S3.p1.2.m2.1.1.1.3.3.cmml">1</mn></mrow><mi mathsize="90%" id="S3.p1.2.m2.1.1.1.1.3" xref="S3.p1.2.m2.1.1.1.1.3.cmml">N</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><eq id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2"></eq><ci id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3">‚Ñ¨</ci><apply id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.2.cmml" xref="S3.p1.2.m2.1.1.1">subscript</csymbol><apply id="S3.p1.2.m2.1.1.1.1.cmml" xref="S3.p1.2.m2.1.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.1.2.cmml" xref="S3.p1.2.m2.1.1.1">superscript</csymbol><set id="S3.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.p1.2.m2.1.1.1.1.1.1"><apply id="S3.p1.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.p1.2.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.p1.2.m2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.p1.2.m2.1.1.1.1.1.1.1.2.cmml" xref="S3.p1.2.m2.1.1.1.1.1.1.1.2">ùíÉ</ci><ci id="S3.p1.2.m2.1.1.1.1.1.1.1.3.cmml" xref="S3.p1.2.m2.1.1.1.1.1.1.1.3">ùëñ</ci></apply></set><ci id="S3.p1.2.m2.1.1.1.1.3.cmml" xref="S3.p1.2.m2.1.1.1.1.3">ùëÅ</ci></apply><apply id="S3.p1.2.m2.1.1.1.3.cmml" xref="S3.p1.2.m2.1.1.1.3"><eq id="S3.p1.2.m2.1.1.1.3.1.cmml" xref="S3.p1.2.m2.1.1.1.3.1"></eq><ci id="S3.p1.2.m2.1.1.1.3.2.cmml" xref="S3.p1.2.m2.1.1.1.3.2">ùëñ</ci><cn type="integer" id="S3.p1.2.m2.1.1.1.3.3.cmml" xref="S3.p1.2.m2.1.1.1.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">\mathcal{B}=\{{\boldsymbol{b}}_{i}\}^{N}_{i=1}</annotation></semantics></math><span id="S3.p1.24.3" class="ltx_text" style="font-size:90%;">, where </span><math id="S3.p1.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.p1.3.m3.1a"><mi mathsize="90%" id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">N</annotation></semantics></math><span id="S3.p1.24.4" class="ltx_text" style="font-size:90%;"> is the number of objects, and </span><math id="S3.p1.4.m4.1" class="ltx_Math" alttext="{\boldsymbol{b}}_{i}" display="inline"><semantics id="S3.p1.4.m4.1a"><msub id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml"><mi mathsize="90%" id="S3.p1.4.m4.1.1.2" xref="S3.p1.4.m4.1.1.2.cmml">ùíÉ</mi><mi mathsize="90%" id="S3.p1.4.m4.1.1.3" xref="S3.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.1.cmml" xref="S3.p1.4.m4.1.1">subscript</csymbol><ci id="S3.p1.4.m4.1.1.2.cmml" xref="S3.p1.4.m4.1.1.2">ùíÉ</ci><ci id="S3.p1.4.m4.1.1.3.cmml" xref="S3.p1.4.m4.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">{\boldsymbol{b}}_{i}</annotation></semantics></math><span id="S3.p1.24.5" class="ltx_text" style="font-size:90%;"> is the </span><math id="S3.p1.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.p1.5.m5.1a"><mi mathsize="90%" id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><ci id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">i</annotation></semantics></math><span id="S3.p1.24.6" class="ltx_text" style="font-size:90%;">-th object bounding box.
We first define the foreground </span><math id="S3.p1.6.m6.1" class="ltx_Math" alttext="f_{x}" display="inline"><semantics id="S3.p1.6.m6.1a"><msub id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml"><mi mathsize="90%" id="S3.p1.6.m6.1.1.2" xref="S3.p1.6.m6.1.1.2.cmml">f</mi><mi mathsize="90%" id="S3.p1.6.m6.1.1.3" xref="S3.p1.6.m6.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.1b"><apply id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p1.6.m6.1.1.1.cmml" xref="S3.p1.6.m6.1.1">subscript</csymbol><ci id="S3.p1.6.m6.1.1.2.cmml" xref="S3.p1.6.m6.1.1.2">ùëì</ci><ci id="S3.p1.6.m6.1.1.3.cmml" xref="S3.p1.6.m6.1.1.3">ùë•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.1c">f_{x}</annotation></semantics></math><span id="S3.p1.24.7" class="ltx_text" style="font-size:90%;"> of image </span><math id="S3.p1.7.m7.1" class="ltx_Math" alttext="{\boldsymbol{X}}" display="inline"><semantics id="S3.p1.7.m7.1a"><mi mathsize="90%" id="S3.p1.7.m7.1.1" xref="S3.p1.7.m7.1.1.cmml">ùëø</mi><annotation-xml encoding="MathML-Content" id="S3.p1.7.m7.1b"><ci id="S3.p1.7.m7.1.1.cmml" xref="S3.p1.7.m7.1.1">ùëø</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.7.m7.1c">{\boldsymbol{X}}</annotation></semantics></math><span id="S3.p1.24.8" class="ltx_text" style="font-size:90%;"> as all areas of </span><math id="S3.p1.8.m8.1" class="ltx_Math" alttext="\mathcal{B}" display="inline"><semantics id="S3.p1.8.m8.1a"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S3.p1.8.m8.1.1" xref="S3.p1.8.m8.1.1.cmml">‚Ñ¨</mi><annotation-xml encoding="MathML-Content" id="S3.p1.8.m8.1b"><ci id="S3.p1.8.m8.1.1.cmml" xref="S3.p1.8.m8.1.1">‚Ñ¨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.8.m8.1c">\mathcal{B}</annotation></semantics></math><span id="S3.p1.24.9" class="ltx_text" style="font-size:90%;">, subsequently, we define the background </span><math id="S3.p1.9.m9.1" class="ltx_Math" alttext="bg_{x}" display="inline"><semantics id="S3.p1.9.m9.1a"><mrow id="S3.p1.9.m9.1.1" xref="S3.p1.9.m9.1.1.cmml"><mi mathsize="90%" id="S3.p1.9.m9.1.1.2" xref="S3.p1.9.m9.1.1.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.p1.9.m9.1.1.1" xref="S3.p1.9.m9.1.1.1.cmml">‚Äã</mo><msub id="S3.p1.9.m9.1.1.3" xref="S3.p1.9.m9.1.1.3.cmml"><mi mathsize="90%" id="S3.p1.9.m9.1.1.3.2" xref="S3.p1.9.m9.1.1.3.2.cmml">g</mi><mi mathsize="90%" id="S3.p1.9.m9.1.1.3.3" xref="S3.p1.9.m9.1.1.3.3.cmml">x</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.9.m9.1b"><apply id="S3.p1.9.m9.1.1.cmml" xref="S3.p1.9.m9.1.1"><times id="S3.p1.9.m9.1.1.1.cmml" xref="S3.p1.9.m9.1.1.1"></times><ci id="S3.p1.9.m9.1.1.2.cmml" xref="S3.p1.9.m9.1.1.2">ùëè</ci><apply id="S3.p1.9.m9.1.1.3.cmml" xref="S3.p1.9.m9.1.1.3"><csymbol cd="ambiguous" id="S3.p1.9.m9.1.1.3.1.cmml" xref="S3.p1.9.m9.1.1.3">subscript</csymbol><ci id="S3.p1.9.m9.1.1.3.2.cmml" xref="S3.p1.9.m9.1.1.3.2">ùëî</ci><ci id="S3.p1.9.m9.1.1.3.3.cmml" xref="S3.p1.9.m9.1.1.3.3">ùë•</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.9.m9.1c">bg_{x}</annotation></semantics></math><span id="S3.p1.24.10" class="ltx_text" style="font-size:90%;"> as the inverse of all areas of </span><math id="S3.p1.10.m10.1" class="ltx_Math" alttext="\mathcal{B}" display="inline"><semantics id="S3.p1.10.m10.1a"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S3.p1.10.m10.1.1" xref="S3.p1.10.m10.1.1.cmml">‚Ñ¨</mi><annotation-xml encoding="MathML-Content" id="S3.p1.10.m10.1b"><ci id="S3.p1.10.m10.1.1.cmml" xref="S3.p1.10.m10.1.1">‚Ñ¨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.10.m10.1c">\mathcal{B}</annotation></semantics></math><span id="S3.p1.24.11" class="ltx_text" style="font-size:90%;"> in </span><math id="S3.p1.11.m11.1" class="ltx_Math" alttext="{\boldsymbol{X}}" display="inline"><semantics id="S3.p1.11.m11.1a"><mi mathsize="90%" id="S3.p1.11.m11.1.1" xref="S3.p1.11.m11.1.1.cmml">ùëø</mi><annotation-xml encoding="MathML-Content" id="S3.p1.11.m11.1b"><ci id="S3.p1.11.m11.1.1.cmml" xref="S3.p1.11.m11.1.1">ùëø</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.11.m11.1c">{\boldsymbol{X}}</annotation></semantics></math><span id="S3.p1.24.12" class="ltx_text" style="font-size:90%;">. Initially, </span><math id="S3.p1.12.m12.1" class="ltx_Math" alttext="bg_{x}" display="inline"><semantics id="S3.p1.12.m12.1a"><mrow id="S3.p1.12.m12.1.1" xref="S3.p1.12.m12.1.1.cmml"><mi mathsize="90%" id="S3.p1.12.m12.1.1.2" xref="S3.p1.12.m12.1.1.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.p1.12.m12.1.1.1" xref="S3.p1.12.m12.1.1.1.cmml">‚Äã</mo><msub id="S3.p1.12.m12.1.1.3" xref="S3.p1.12.m12.1.1.3.cmml"><mi mathsize="90%" id="S3.p1.12.m12.1.1.3.2" xref="S3.p1.12.m12.1.1.3.2.cmml">g</mi><mi mathsize="90%" id="S3.p1.12.m12.1.1.3.3" xref="S3.p1.12.m12.1.1.3.3.cmml">x</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.12.m12.1b"><apply id="S3.p1.12.m12.1.1.cmml" xref="S3.p1.12.m12.1.1"><times id="S3.p1.12.m12.1.1.1.cmml" xref="S3.p1.12.m12.1.1.1"></times><ci id="S3.p1.12.m12.1.1.2.cmml" xref="S3.p1.12.m12.1.1.2">ùëè</ci><apply id="S3.p1.12.m12.1.1.3.cmml" xref="S3.p1.12.m12.1.1.3"><csymbol cd="ambiguous" id="S3.p1.12.m12.1.1.3.1.cmml" xref="S3.p1.12.m12.1.1.3">subscript</csymbol><ci id="S3.p1.12.m12.1.1.3.2.cmml" xref="S3.p1.12.m12.1.1.3.2">ùëî</ci><ci id="S3.p1.12.m12.1.1.3.3.cmml" xref="S3.p1.12.m12.1.1.3.3">ùë•</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.12.m12.1c">bg_{x}</annotation></semantics></math><span id="S3.p1.24.13" class="ltx_text" style="font-size:90%;"> is a calm ocean environment. We wish to replace </span><math id="S3.p1.13.m13.1" class="ltx_Math" alttext="bg_{x}" display="inline"><semantics id="S3.p1.13.m13.1a"><mrow id="S3.p1.13.m13.1.1" xref="S3.p1.13.m13.1.1.cmml"><mi mathsize="90%" id="S3.p1.13.m13.1.1.2" xref="S3.p1.13.m13.1.1.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.p1.13.m13.1.1.1" xref="S3.p1.13.m13.1.1.1.cmml">‚Äã</mo><msub id="S3.p1.13.m13.1.1.3" xref="S3.p1.13.m13.1.1.3.cmml"><mi mathsize="90%" id="S3.p1.13.m13.1.1.3.2" xref="S3.p1.13.m13.1.1.3.2.cmml">g</mi><mi mathsize="90%" id="S3.p1.13.m13.1.1.3.3" xref="S3.p1.13.m13.1.1.3.3.cmml">x</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.13.m13.1b"><apply id="S3.p1.13.m13.1.1.cmml" xref="S3.p1.13.m13.1.1"><times id="S3.p1.13.m13.1.1.1.cmml" xref="S3.p1.13.m13.1.1.1"></times><ci id="S3.p1.13.m13.1.1.2.cmml" xref="S3.p1.13.m13.1.1.2">ùëè</ci><apply id="S3.p1.13.m13.1.1.3.cmml" xref="S3.p1.13.m13.1.1.3"><csymbol cd="ambiguous" id="S3.p1.13.m13.1.1.3.1.cmml" xref="S3.p1.13.m13.1.1.3">subscript</csymbol><ci id="S3.p1.13.m13.1.1.3.2.cmml" xref="S3.p1.13.m13.1.1.3.2">ùëî</ci><ci id="S3.p1.13.m13.1.1.3.3.cmml" xref="S3.p1.13.m13.1.1.3.3">ùë•</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.13.m13.1c">bg_{x}</annotation></semantics></math><span id="S3.p1.24.14" class="ltx_text" style="font-size:90%;"> such that it corresponds to a particular sea state, </span><math id="S3.p1.14.m14.1" class="ltx_Math" alttext="SS" display="inline"><semantics id="S3.p1.14.m14.1a"><mrow id="S3.p1.14.m14.1.1" xref="S3.p1.14.m14.1.1.cmml"><mi mathsize="90%" id="S3.p1.14.m14.1.1.2" xref="S3.p1.14.m14.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.p1.14.m14.1.1.1" xref="S3.p1.14.m14.1.1.1.cmml">‚Äã</mo><mi mathsize="90%" id="S3.p1.14.m14.1.1.3" xref="S3.p1.14.m14.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.14.m14.1b"><apply id="S3.p1.14.m14.1.1.cmml" xref="S3.p1.14.m14.1.1"><times id="S3.p1.14.m14.1.1.1.cmml" xref="S3.p1.14.m14.1.1.1"></times><ci id="S3.p1.14.m14.1.1.2.cmml" xref="S3.p1.14.m14.1.1.2">ùëÜ</ci><ci id="S3.p1.14.m14.1.1.3.cmml" xref="S3.p1.14.m14.1.1.3">ùëÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.14.m14.1c">SS</annotation></semantics></math><span id="S3.p1.24.15" class="ltx_text" style="font-size:90%;">, where </span><math id="S3.p1.15.m15.1" class="ltx_Math" alttext="SS" display="inline"><semantics id="S3.p1.15.m15.1a"><mrow id="S3.p1.15.m15.1.1" xref="S3.p1.15.m15.1.1.cmml"><mi mathsize="90%" id="S3.p1.15.m15.1.1.2" xref="S3.p1.15.m15.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.p1.15.m15.1.1.1" xref="S3.p1.15.m15.1.1.1.cmml">‚Äã</mo><mi mathsize="90%" id="S3.p1.15.m15.1.1.3" xref="S3.p1.15.m15.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.15.m15.1b"><apply id="S3.p1.15.m15.1.1.cmml" xref="S3.p1.15.m15.1.1"><times id="S3.p1.15.m15.1.1.1.cmml" xref="S3.p1.15.m15.1.1.1"></times><ci id="S3.p1.15.m15.1.1.2.cmml" xref="S3.p1.15.m15.1.1.2">ùëÜ</ci><ci id="S3.p1.15.m15.1.1.3.cmml" xref="S3.p1.15.m15.1.1.3">ùëÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.15.m15.1c">SS</annotation></semantics></math><span id="S3.p1.24.16" class="ltx_text" style="font-size:90%;"> ranges from 1 to 4 and are defined in Table¬†</span><a href="#S2.T1" title="Table 1 ‚Ä£ 2 Related Works ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S3.p1.24.17" class="ltx_text" style="font-size:90%;">, with examples shown in Figure¬†</span><a href="#S6.F6" title="Figure 6 ‚Ä£ 6.1.2 Results ‚Ä£ 6.1 SafeSea method evaluation ‚Ä£ 6 Experiment ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">6</span></a><span id="S3.p1.24.18" class="ltx_text" style="font-size:90%;">. We only aim to replace </span><math id="S3.p1.16.m16.1" class="ltx_Math" alttext="bg_{x}" display="inline"><semantics id="S3.p1.16.m16.1a"><mrow id="S3.p1.16.m16.1.1" xref="S3.p1.16.m16.1.1.cmml"><mi mathsize="90%" id="S3.p1.16.m16.1.1.2" xref="S3.p1.16.m16.1.1.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.p1.16.m16.1.1.1" xref="S3.p1.16.m16.1.1.1.cmml">‚Äã</mo><msub id="S3.p1.16.m16.1.1.3" xref="S3.p1.16.m16.1.1.3.cmml"><mi mathsize="90%" id="S3.p1.16.m16.1.1.3.2" xref="S3.p1.16.m16.1.1.3.2.cmml">g</mi><mi mathsize="90%" id="S3.p1.16.m16.1.1.3.3" xref="S3.p1.16.m16.1.1.3.3.cmml">x</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.16.m16.1b"><apply id="S3.p1.16.m16.1.1.cmml" xref="S3.p1.16.m16.1.1"><times id="S3.p1.16.m16.1.1.1.cmml" xref="S3.p1.16.m16.1.1.1"></times><ci id="S3.p1.16.m16.1.1.2.cmml" xref="S3.p1.16.m16.1.1.2">ùëè</ci><apply id="S3.p1.16.m16.1.1.3.cmml" xref="S3.p1.16.m16.1.1.3"><csymbol cd="ambiguous" id="S3.p1.16.m16.1.1.3.1.cmml" xref="S3.p1.16.m16.1.1.3">subscript</csymbol><ci id="S3.p1.16.m16.1.1.3.2.cmml" xref="S3.p1.16.m16.1.1.3.2">ùëî</ci><ci id="S3.p1.16.m16.1.1.3.3.cmml" xref="S3.p1.16.m16.1.1.3.3">ùë•</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.16.m16.1c">bg_{x}</annotation></semantics></math><span id="S3.p1.24.19" class="ltx_text" style="font-size:90%;">, and aim to retain the number of </span><math id="S3.p1.17.m17.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.p1.17.m17.1a"><mi mathsize="90%" id="S3.p1.17.m17.1.1" xref="S3.p1.17.m17.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p1.17.m17.1b"><ci id="S3.p1.17.m17.1.1.cmml" xref="S3.p1.17.m17.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.17.m17.1c">N</annotation></semantics></math><span id="S3.p1.24.20" class="ltx_text" style="font-size:90%;"> as was present in </span><math id="S3.p1.18.m18.1" class="ltx_Math" alttext="{\boldsymbol{X}}" display="inline"><semantics id="S3.p1.18.m18.1a"><mi mathsize="90%" id="S3.p1.18.m18.1.1" xref="S3.p1.18.m18.1.1.cmml">ùëø</mi><annotation-xml encoding="MathML-Content" id="S3.p1.18.m18.1b"><ci id="S3.p1.18.m18.1.1.cmml" xref="S3.p1.18.m18.1.1">ùëø</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.18.m18.1c">{\boldsymbol{X}}</annotation></semantics></math><span id="S3.p1.24.21" class="ltx_text" style="font-size:90%;">. We refer to the resulting image as </span><math id="S3.p1.19.m19.1" class="ltx_Math" alttext="{\boldsymbol{Y}}" display="inline"><semantics id="S3.p1.19.m19.1a"><mi mathsize="90%" id="S3.p1.19.m19.1.1" xref="S3.p1.19.m19.1.1.cmml">ùíÄ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.19.m19.1b"><ci id="S3.p1.19.m19.1.1.cmml" xref="S3.p1.19.m19.1.1">ùíÄ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.19.m19.1c">{\boldsymbol{Y}}</annotation></semantics></math><span id="S3.p1.24.22" class="ltx_text" style="font-size:90%;">. Specifically, let </span><math id="S3.p1.20.m20.1" class="ltx_Math" alttext="f_{y}" display="inline"><semantics id="S3.p1.20.m20.1a"><msub id="S3.p1.20.m20.1.1" xref="S3.p1.20.m20.1.1.cmml"><mi mathsize="90%" id="S3.p1.20.m20.1.1.2" xref="S3.p1.20.m20.1.1.2.cmml">f</mi><mi mathsize="90%" id="S3.p1.20.m20.1.1.3" xref="S3.p1.20.m20.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.20.m20.1b"><apply id="S3.p1.20.m20.1.1.cmml" xref="S3.p1.20.m20.1.1"><csymbol cd="ambiguous" id="S3.p1.20.m20.1.1.1.cmml" xref="S3.p1.20.m20.1.1">subscript</csymbol><ci id="S3.p1.20.m20.1.1.2.cmml" xref="S3.p1.20.m20.1.1.2">ùëì</ci><ci id="S3.p1.20.m20.1.1.3.cmml" xref="S3.p1.20.m20.1.1.3">ùë¶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.20.m20.1c">f_{y}</annotation></semantics></math><span id="S3.p1.24.23" class="ltx_text" style="font-size:90%;"> and </span><math id="S3.p1.21.m21.1" class="ltx_Math" alttext="bg_{y}" display="inline"><semantics id="S3.p1.21.m21.1a"><mrow id="S3.p1.21.m21.1.1" xref="S3.p1.21.m21.1.1.cmml"><mi mathsize="90%" id="S3.p1.21.m21.1.1.2" xref="S3.p1.21.m21.1.1.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.p1.21.m21.1.1.1" xref="S3.p1.21.m21.1.1.1.cmml">‚Äã</mo><msub id="S3.p1.21.m21.1.1.3" xref="S3.p1.21.m21.1.1.3.cmml"><mi mathsize="90%" id="S3.p1.21.m21.1.1.3.2" xref="S3.p1.21.m21.1.1.3.2.cmml">g</mi><mi mathsize="90%" id="S3.p1.21.m21.1.1.3.3" xref="S3.p1.21.m21.1.1.3.3.cmml">y</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.21.m21.1b"><apply id="S3.p1.21.m21.1.1.cmml" xref="S3.p1.21.m21.1.1"><times id="S3.p1.21.m21.1.1.1.cmml" xref="S3.p1.21.m21.1.1.1"></times><ci id="S3.p1.21.m21.1.1.2.cmml" xref="S3.p1.21.m21.1.1.2">ùëè</ci><apply id="S3.p1.21.m21.1.1.3.cmml" xref="S3.p1.21.m21.1.1.3"><csymbol cd="ambiguous" id="S3.p1.21.m21.1.1.3.1.cmml" xref="S3.p1.21.m21.1.1.3">subscript</csymbol><ci id="S3.p1.21.m21.1.1.3.2.cmml" xref="S3.p1.21.m21.1.1.3.2">ùëî</ci><ci id="S3.p1.21.m21.1.1.3.3.cmml" xref="S3.p1.21.m21.1.1.3.3">ùë¶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.21.m21.1c">bg_{y}</annotation></semantics></math><span id="S3.p1.24.24" class="ltx_text" style="font-size:90%;"> as the foreground, and the background of </span><math id="S3.p1.22.m22.1" class="ltx_Math" alttext="{\boldsymbol{Y}}" display="inline"><semantics id="S3.p1.22.m22.1a"><mi mathsize="90%" id="S3.p1.22.m22.1.1" xref="S3.p1.22.m22.1.1.cmml">ùíÄ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.22.m22.1b"><ci id="S3.p1.22.m22.1.1.cmml" xref="S3.p1.22.m22.1.1">ùíÄ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.22.m22.1c">{\boldsymbol{Y}}</annotation></semantics></math><span id="S3.p1.24.25" class="ltx_text" style="font-size:90%;">, respectively. The goal is to have </span><math id="S3.p1.23.m23.1" class="ltx_Math" alttext="bg_{y}\neq bg_{x}" display="inline"><semantics id="S3.p1.23.m23.1a"><mrow id="S3.p1.23.m23.1.1" xref="S3.p1.23.m23.1.1.cmml"><mrow id="S3.p1.23.m23.1.1.2" xref="S3.p1.23.m23.1.1.2.cmml"><mi mathsize="90%" id="S3.p1.23.m23.1.1.2.2" xref="S3.p1.23.m23.1.1.2.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.p1.23.m23.1.1.2.1" xref="S3.p1.23.m23.1.1.2.1.cmml">‚Äã</mo><msub id="S3.p1.23.m23.1.1.2.3" xref="S3.p1.23.m23.1.1.2.3.cmml"><mi mathsize="90%" id="S3.p1.23.m23.1.1.2.3.2" xref="S3.p1.23.m23.1.1.2.3.2.cmml">g</mi><mi mathsize="90%" id="S3.p1.23.m23.1.1.2.3.3" xref="S3.p1.23.m23.1.1.2.3.3.cmml">y</mi></msub></mrow><mo mathsize="90%" id="S3.p1.23.m23.1.1.1" xref="S3.p1.23.m23.1.1.1.cmml">‚â†</mo><mrow id="S3.p1.23.m23.1.1.3" xref="S3.p1.23.m23.1.1.3.cmml"><mi mathsize="90%" id="S3.p1.23.m23.1.1.3.2" xref="S3.p1.23.m23.1.1.3.2.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.p1.23.m23.1.1.3.1" xref="S3.p1.23.m23.1.1.3.1.cmml">‚Äã</mo><msub id="S3.p1.23.m23.1.1.3.3" xref="S3.p1.23.m23.1.1.3.3.cmml"><mi mathsize="90%" id="S3.p1.23.m23.1.1.3.3.2" xref="S3.p1.23.m23.1.1.3.3.2.cmml">g</mi><mi mathsize="90%" id="S3.p1.23.m23.1.1.3.3.3" xref="S3.p1.23.m23.1.1.3.3.3.cmml">x</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.23.m23.1b"><apply id="S3.p1.23.m23.1.1.cmml" xref="S3.p1.23.m23.1.1"><neq id="S3.p1.23.m23.1.1.1.cmml" xref="S3.p1.23.m23.1.1.1"></neq><apply id="S3.p1.23.m23.1.1.2.cmml" xref="S3.p1.23.m23.1.1.2"><times id="S3.p1.23.m23.1.1.2.1.cmml" xref="S3.p1.23.m23.1.1.2.1"></times><ci id="S3.p1.23.m23.1.1.2.2.cmml" xref="S3.p1.23.m23.1.1.2.2">ùëè</ci><apply id="S3.p1.23.m23.1.1.2.3.cmml" xref="S3.p1.23.m23.1.1.2.3"><csymbol cd="ambiguous" id="S3.p1.23.m23.1.1.2.3.1.cmml" xref="S3.p1.23.m23.1.1.2.3">subscript</csymbol><ci id="S3.p1.23.m23.1.1.2.3.2.cmml" xref="S3.p1.23.m23.1.1.2.3.2">ùëî</ci><ci id="S3.p1.23.m23.1.1.2.3.3.cmml" xref="S3.p1.23.m23.1.1.2.3.3">ùë¶</ci></apply></apply><apply id="S3.p1.23.m23.1.1.3.cmml" xref="S3.p1.23.m23.1.1.3"><times id="S3.p1.23.m23.1.1.3.1.cmml" xref="S3.p1.23.m23.1.1.3.1"></times><ci id="S3.p1.23.m23.1.1.3.2.cmml" xref="S3.p1.23.m23.1.1.3.2">ùëè</ci><apply id="S3.p1.23.m23.1.1.3.3.cmml" xref="S3.p1.23.m23.1.1.3.3"><csymbol cd="ambiguous" id="S3.p1.23.m23.1.1.3.3.1.cmml" xref="S3.p1.23.m23.1.1.3.3">subscript</csymbol><ci id="S3.p1.23.m23.1.1.3.3.2.cmml" xref="S3.p1.23.m23.1.1.3.3.2">ùëî</ci><ci id="S3.p1.23.m23.1.1.3.3.3.cmml" xref="S3.p1.23.m23.1.1.3.3.3">ùë•</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.23.m23.1c">bg_{y}\neq bg_{x}</annotation></semantics></math><span id="S3.p1.24.26" class="ltx_text" style="font-size:90%;">, and </span><math id="S3.p1.24.m24.1" class="ltx_Math" alttext="f_{y}\approx f_{x}" display="inline"><semantics id="S3.p1.24.m24.1a"><mrow id="S3.p1.24.m24.1.1" xref="S3.p1.24.m24.1.1.cmml"><msub id="S3.p1.24.m24.1.1.2" xref="S3.p1.24.m24.1.1.2.cmml"><mi mathsize="90%" id="S3.p1.24.m24.1.1.2.2" xref="S3.p1.24.m24.1.1.2.2.cmml">f</mi><mi mathsize="90%" id="S3.p1.24.m24.1.1.2.3" xref="S3.p1.24.m24.1.1.2.3.cmml">y</mi></msub><mo mathsize="90%" id="S3.p1.24.m24.1.1.1" xref="S3.p1.24.m24.1.1.1.cmml">‚âà</mo><msub id="S3.p1.24.m24.1.1.3" xref="S3.p1.24.m24.1.1.3.cmml"><mi mathsize="90%" id="S3.p1.24.m24.1.1.3.2" xref="S3.p1.24.m24.1.1.3.2.cmml">f</mi><mi mathsize="90%" id="S3.p1.24.m24.1.1.3.3" xref="S3.p1.24.m24.1.1.3.3.cmml">x</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.24.m24.1b"><apply id="S3.p1.24.m24.1.1.cmml" xref="S3.p1.24.m24.1.1"><approx id="S3.p1.24.m24.1.1.1.cmml" xref="S3.p1.24.m24.1.1.1"></approx><apply id="S3.p1.24.m24.1.1.2.cmml" xref="S3.p1.24.m24.1.1.2"><csymbol cd="ambiguous" id="S3.p1.24.m24.1.1.2.1.cmml" xref="S3.p1.24.m24.1.1.2">subscript</csymbol><ci id="S3.p1.24.m24.1.1.2.2.cmml" xref="S3.p1.24.m24.1.1.2.2">ùëì</ci><ci id="S3.p1.24.m24.1.1.2.3.cmml" xref="S3.p1.24.m24.1.1.2.3">ùë¶</ci></apply><apply id="S3.p1.24.m24.1.1.3.cmml" xref="S3.p1.24.m24.1.1.3"><csymbol cd="ambiguous" id="S3.p1.24.m24.1.1.3.1.cmml" xref="S3.p1.24.m24.1.1.3">subscript</csymbol><ci id="S3.p1.24.m24.1.1.3.2.cmml" xref="S3.p1.24.m24.1.1.3.2">ùëì</ci><ci id="S3.p1.24.m24.1.1.3.3.cmml" xref="S3.p1.24.m24.1.1.3.3">ùë•</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.24.m24.1c">f_{y}\approx f_{x}</annotation></semantics></math><span id="S3.p1.24.27" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">4 </span>Proposed SafeSea method</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text" style="font-size:90%;">The overall diagram of the propose method is depicted in Figure¬†</span><a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.p1.1.2" class="ltx_text" style="font-size:90%;"> and the pseudo code is presented in Algorithm¬†</span><a href="#alg1" title="Algorithm 1 ‚Ä£ 4 Proposed SafeSea method ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.p1.1.3" class="ltx_text" style="font-size:90%;">. The method has three main components: (1) the image generation module which transform the background of an input image; (2) Sea State Level classifier; and (3) the Object Preservation Checker. Once an image is generated, its quality is automatically assessed by using (2) and (3). Specifically, the Sea State Level classifier determines the sea state level of the generated image, and the Object Preservation Checker ensures the generated image still contains the objects from the input image.</span></p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text" style="font-size:90%;">The following section first discusses the image generation module which is powered by a diffusion model. Then, the Sea State Level classifier and the Object Preservation Checker modules are presented.</span></p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">

<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_float"><span id="alg1.10.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Pseudocode for generating synthetic images of the sea with marine objects from real images as proposed in SafeSea method. The real image has its sea background edited using Blended Latent Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>, then its Sea State level is provided by the Sea State Classifier. Finally, the original objects are checked if they are retained in the edited image, which lead to the decision of saving the image if at least one object is preserved.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="alg1.3" class="ltx_p ltx_figure_panel"><span id="alg1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Input:</span><span id="alg1.3.2" class="ltx_text" style="font-size:90%;"> Real sea images with marine objects (</span><math id="alg1.1.m1.1" class="ltx_Math" alttext="Rs" display="inline"><semantics id="alg1.1.m1.1a"><mrow id="alg1.1.m1.1.1" xref="alg1.1.m1.1.1.cmml"><mi mathsize="90%" id="alg1.1.m1.1.1.2" xref="alg1.1.m1.1.1.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="alg1.1.m1.1.1.1" xref="alg1.1.m1.1.1.1.cmml">‚Äã</mo><mi mathsize="90%" id="alg1.1.m1.1.1.3" xref="alg1.1.m1.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.1.m1.1b"><apply id="alg1.1.m1.1.1.cmml" xref="alg1.1.m1.1.1"><times id="alg1.1.m1.1.1.1.cmml" xref="alg1.1.m1.1.1.1"></times><ci id="alg1.1.m1.1.1.2.cmml" xref="alg1.1.m1.1.1.2">ùëÖ</ci><ci id="alg1.1.m1.1.1.3.cmml" xref="alg1.1.m1.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.1.m1.1c">Rs</annotation></semantics></math><span id="alg1.3.3" class="ltx_text" style="font-size:90%;">), matching mask images (</span><math id="alg1.2.m2.1" class="ltx_Math" alttext="Ms" display="inline"><semantics id="alg1.2.m2.1a"><mrow id="alg1.2.m2.1.1" xref="alg1.2.m2.1.1.cmml"><mi mathsize="90%" id="alg1.2.m2.1.1.2" xref="alg1.2.m2.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="alg1.2.m2.1.1.1" xref="alg1.2.m2.1.1.1.cmml">‚Äã</mo><mi mathsize="90%" id="alg1.2.m2.1.1.3" xref="alg1.2.m2.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.2.m2.1b"><apply id="alg1.2.m2.1.1.cmml" xref="alg1.2.m2.1.1"><times id="alg1.2.m2.1.1.1.cmml" xref="alg1.2.m2.1.1.1"></times><ci id="alg1.2.m2.1.1.2.cmml" xref="alg1.2.m2.1.1.2">ùëÄ</ci><ci id="alg1.2.m2.1.1.3.cmml" xref="alg1.2.m2.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.2.m2.1c">Ms</annotation></semantics></math><span id="alg1.3.4" class="ltx_text" style="font-size:90%;">) with ground true bounding boxes of marine objects are masked as black, text description of background (</span><math id="alg1.3.m3.1" class="ltx_Math" alttext="P" display="inline"><semantics id="alg1.3.m3.1a"><mi mathsize="90%" id="alg1.3.m3.1.1" xref="alg1.3.m3.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="alg1.3.m3.1b"><ci id="alg1.3.m3.1.1.cmml" xref="alg1.3.m3.1.1">ùëÉ</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.3.m3.1c">P</annotation></semantics></math><span id="alg1.3.5" class="ltx_text" style="font-size:90%;">)</span>
<br class="ltx_break"><span id="alg1.3.6" class="ltx_text" style="font-size:90%;">¬†¬†¬†¬†¬† </span><span id="alg1.3.7" class="ltx_text ltx_font_bold" style="font-size:90%;">Output:</span><span id="alg1.3.8" class="ltx_text" style="font-size:90%;"> Transformed images with edited background reflected different sea condition and the marine objects are preserved</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="alg1.11" class="ltx_listing ltx_figure_panel ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l1.1.1.1" class="ltx_text" style="font-size:80%;">1:</span></span><span id="alg1.l1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span id="alg1.l1.3" class="ltx_text" style="font-size:90%;">¬†each </span><math id="alg1.l1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="alg1.l1.m1.1a"><mi mathsize="90%" id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">ùëã</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">X</annotation></semantics></math><span id="alg1.l1.4" class="ltx_text" style="font-size:90%;"> in </span><math id="alg1.l1.m2.1" class="ltx_Math" alttext="Rs" display="inline"><semantics id="alg1.l1.m2.1a"><mrow id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml"><mi mathsize="90%" id="alg1.l1.m2.1.1.2" xref="alg1.l1.m2.1.1.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="alg1.l1.m2.1.1.1" xref="alg1.l1.m2.1.1.1.cmml">‚Äã</mo><mi mathsize="90%" id="alg1.l1.m2.1.1.3" xref="alg1.l1.m2.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><apply id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1"><times id="alg1.l1.m2.1.1.1.cmml" xref="alg1.l1.m2.1.1.1"></times><ci id="alg1.l1.m2.1.1.2.cmml" xref="alg1.l1.m2.1.1.2">ùëÖ</ci><ci id="alg1.l1.m2.1.1.3.cmml" xref="alg1.l1.m2.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">Rs</annotation></semantics></math><span id="alg1.l1.5" class="ltx_text" style="font-size:90%;">¬†</span><span id="alg1.l1.6" class="ltx_text ltx_font_bold" style="font-size:90%;">do</span><span id="alg1.l1.7" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l2.1.1.1" class="ltx_text" style="font-size:80%;">2:</span></span><span id="alg1.l2.2" class="ltx_text" style="font-size:90%;">¬†¬†¬†¬†</span><math id="alg1.l2.m1.1" class="ltx_Math" alttext="M\leftarrow" display="inline"><semantics id="alg1.l2.m1.1a"><mrow id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml"><mi mathsize="90%" id="alg1.l2.m1.1.1.2" xref="alg1.l2.m1.1.1.2.cmml">M</mi><mo mathsize="90%" stretchy="false" id="alg1.l2.m1.1.1.1" xref="alg1.l2.m1.1.1.1.cmml">‚Üê</mo><mi id="alg1.l2.m1.1.1.3" xref="alg1.l2.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><apply id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1"><ci id="alg1.l2.m1.1.1.1.cmml" xref="alg1.l2.m1.1.1.1">‚Üê</ci><ci id="alg1.l2.m1.1.1.2.cmml" xref="alg1.l2.m1.1.1.2">ùëÄ</ci><csymbol cd="latexml" id="alg1.l2.m1.1.1.3.cmml" xref="alg1.l2.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">M\leftarrow</annotation></semantics></math><span id="alg1.l2.3" class="ltx_text" style="font-size:90%;"> Corresponding mask image from </span><math id="alg1.l2.m2.1" class="ltx_Math" alttext="Ms" display="inline"><semantics id="alg1.l2.m2.1a"><mrow id="alg1.l2.m2.1.1" xref="alg1.l2.m2.1.1.cmml"><mi mathsize="90%" id="alg1.l2.m2.1.1.2" xref="alg1.l2.m2.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="alg1.l2.m2.1.1.1" xref="alg1.l2.m2.1.1.1.cmml">‚Äã</mo><mi mathsize="90%" id="alg1.l2.m2.1.1.3" xref="alg1.l2.m2.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l2.m2.1b"><apply id="alg1.l2.m2.1.1.cmml" xref="alg1.l2.m2.1.1"><times id="alg1.l2.m2.1.1.1.cmml" xref="alg1.l2.m2.1.1.1"></times><ci id="alg1.l2.m2.1.1.2.cmml" xref="alg1.l2.m2.1.1.2">ùëÄ</ci><ci id="alg1.l2.m2.1.1.3.cmml" xref="alg1.l2.m2.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m2.1c">Ms</annotation></semantics></math><span id="alg1.l2.4" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l3.2.1.1" class="ltx_text" style="font-size:80%;">3:</span></span><span id="alg1.l3.3" class="ltx_text" style="font-size:90%;">¬†¬†¬†¬†</span><math id="alg1.l3.m1.1" class="ltx_Math" alttext="Y\leftarrow" display="inline"><semantics id="alg1.l3.m1.1a"><mrow id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml"><mi mathsize="90%" id="alg1.l3.m1.1.1.2" xref="alg1.l3.m1.1.1.2.cmml">Y</mi><mo mathsize="90%" stretchy="false" id="alg1.l3.m1.1.1.1" xref="alg1.l3.m1.1.1.1.cmml">‚Üê</mo><mi id="alg1.l3.m1.1.1.3" xref="alg1.l3.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"><ci id="alg1.l3.m1.1.1.1.cmml" xref="alg1.l3.m1.1.1.1">‚Üê</ci><ci id="alg1.l3.m1.1.1.2.cmml" xref="alg1.l3.m1.1.1.2">ùëå</ci><csymbol cd="latexml" id="alg1.l3.m1.1.1.3.cmml" xref="alg1.l3.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">Y\leftarrow</annotation></semantics></math><span id="alg1.l3.4" class="ltx_text" style="font-size:90%;"> Blended_Latent_Diffusion(</span><math id="alg1.l3.m2.3" class="ltx_Math" alttext="X,M,P" display="inline"><semantics id="alg1.l3.m2.3a"><mrow id="alg1.l3.m2.3.4.2" xref="alg1.l3.m2.3.4.1.cmml"><mi mathsize="90%" id="alg1.l3.m2.1.1" xref="alg1.l3.m2.1.1.cmml">X</mi><mo mathsize="90%" id="alg1.l3.m2.3.4.2.1" xref="alg1.l3.m2.3.4.1.cmml">,</mo><mi mathsize="90%" id="alg1.l3.m2.2.2" xref="alg1.l3.m2.2.2.cmml">M</mi><mo mathsize="90%" id="alg1.l3.m2.3.4.2.2" xref="alg1.l3.m2.3.4.1.cmml">,</mo><mi mathsize="90%" id="alg1.l3.m2.3.3" xref="alg1.l3.m2.3.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l3.m2.3b"><list id="alg1.l3.m2.3.4.1.cmml" xref="alg1.l3.m2.3.4.2"><ci id="alg1.l3.m2.1.1.cmml" xref="alg1.l3.m2.1.1">ùëã</ci><ci id="alg1.l3.m2.2.2.cmml" xref="alg1.l3.m2.2.2">ùëÄ</ci><ci id="alg1.l3.m2.3.3.cmml" xref="alg1.l3.m2.3.3">ùëÉ</ci></list></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m2.3c">X,M,P</annotation></semantics></math><span id="alg1.l3.5" class="ltx_text" style="font-size:90%;">) </span><span id="alg1.l3.1" class="ltx_text" style="font-size:90%;float:right;"><math id="alg1.l3.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg1.l3.1.m1.1a"><mo id="alg1.l3.1.m1.1.1" xref="alg1.l3.1.m1.1.1.cmml">‚ñ∑</mo><annotation-xml encoding="MathML-Content" id="alg1.l3.1.m1.1b"><ci id="alg1.l3.1.m1.1.1.cmml" xref="alg1.l3.1.m1.1.1">‚ñ∑</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.1.m1.1c">\triangleright</annotation></semantics></math> Edited image
</span>
</div>
<div id="alg1.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l4.2.1.1" class="ltx_text" style="font-size:80%;">4:</span></span><span id="alg1.l4.3" class="ltx_text" style="font-size:90%;">¬†¬†¬†¬†</span><math id="alg1.l4.m1.1" class="ltx_Math" alttext="SS_{E}\leftarrow" display="inline"><semantics id="alg1.l4.m1.1a"><mrow id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml"><mrow id="alg1.l4.m1.1.1.2" xref="alg1.l4.m1.1.1.2.cmml"><mi mathsize="90%" id="alg1.l4.m1.1.1.2.2" xref="alg1.l4.m1.1.1.2.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="alg1.l4.m1.1.1.2.1" xref="alg1.l4.m1.1.1.2.1.cmml">‚Äã</mo><msub id="alg1.l4.m1.1.1.2.3" xref="alg1.l4.m1.1.1.2.3.cmml"><mi mathsize="90%" id="alg1.l4.m1.1.1.2.3.2" xref="alg1.l4.m1.1.1.2.3.2.cmml">S</mi><mi mathsize="90%" id="alg1.l4.m1.1.1.2.3.3" xref="alg1.l4.m1.1.1.2.3.3.cmml">E</mi></msub></mrow><mo mathsize="90%" stretchy="false" id="alg1.l4.m1.1.1.1" xref="alg1.l4.m1.1.1.1.cmml">‚Üê</mo><mi id="alg1.l4.m1.1.1.3" xref="alg1.l4.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><apply id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1"><ci id="alg1.l4.m1.1.1.1.cmml" xref="alg1.l4.m1.1.1.1">‚Üê</ci><apply id="alg1.l4.m1.1.1.2.cmml" xref="alg1.l4.m1.1.1.2"><times id="alg1.l4.m1.1.1.2.1.cmml" xref="alg1.l4.m1.1.1.2.1"></times><ci id="alg1.l4.m1.1.1.2.2.cmml" xref="alg1.l4.m1.1.1.2.2">ùëÜ</ci><apply id="alg1.l4.m1.1.1.2.3.cmml" xref="alg1.l4.m1.1.1.2.3"><csymbol cd="ambiguous" id="alg1.l4.m1.1.1.2.3.1.cmml" xref="alg1.l4.m1.1.1.2.3">subscript</csymbol><ci id="alg1.l4.m1.1.1.2.3.2.cmml" xref="alg1.l4.m1.1.1.2.3.2">ùëÜ</ci><ci id="alg1.l4.m1.1.1.2.3.3.cmml" xref="alg1.l4.m1.1.1.2.3.3">ùê∏</ci></apply></apply><csymbol cd="latexml" id="alg1.l4.m1.1.1.3.cmml" xref="alg1.l4.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">SS_{E}\leftarrow</annotation></semantics></math><span id="alg1.l4.4" class="ltx_text" style="font-size:90%;"> Sea_State_Classifier(</span><math id="alg1.l4.m2.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="alg1.l4.m2.1a"><mi mathsize="90%" id="alg1.l4.m2.1.1" xref="alg1.l4.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="alg1.l4.m2.1b"><ci id="alg1.l4.m2.1.1.cmml" xref="alg1.l4.m2.1.1">ùëå</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m2.1c">Y</annotation></semantics></math><span id="alg1.l4.5" class="ltx_text" style="font-size:90%;">) </span><span id="alg1.l4.1" class="ltx_text" style="font-size:90%;float:right;"><math id="alg1.l4.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg1.l4.1.m1.1a"><mo id="alg1.l4.1.m1.1.1" xref="alg1.l4.1.m1.1.1.cmml">‚ñ∑</mo><annotation-xml encoding="MathML-Content" id="alg1.l4.1.m1.1b"><ci id="alg1.l4.1.m1.1.1.cmml" xref="alg1.l4.1.m1.1.1">‚ñ∑</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.1.m1.1c">\triangleright</annotation></semantics></math> Find the edited image‚Äôs Sea State
</span>
</div>
<div id="alg1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l5.2.1.1" class="ltx_text" style="font-size:80%;">5:</span></span><span id="alg1.l5.3" class="ltx_text" style="font-size:90%;">¬†¬†¬†¬†</span><math id="alg1.l5.m1.1" class="ltx_Math" alttext="E_{R}\leftarrow" display="inline"><semantics id="alg1.l5.m1.1a"><mrow id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml"><msub id="alg1.l5.m1.1.1.2" xref="alg1.l5.m1.1.1.2.cmml"><mi mathsize="90%" id="alg1.l5.m1.1.1.2.2" xref="alg1.l5.m1.1.1.2.2.cmml">E</mi><mi mathsize="90%" id="alg1.l5.m1.1.1.2.3" xref="alg1.l5.m1.1.1.2.3.cmml">R</mi></msub><mo mathsize="90%" stretchy="false" id="alg1.l5.m1.1.1.1" xref="alg1.l5.m1.1.1.1.cmml">‚Üê</mo><mi id="alg1.l5.m1.1.1.3" xref="alg1.l5.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><apply id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1"><ci id="alg1.l5.m1.1.1.1.cmml" xref="alg1.l5.m1.1.1.1">‚Üê</ci><apply id="alg1.l5.m1.1.1.2.cmml" xref="alg1.l5.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l5.m1.1.1.2.1.cmml" xref="alg1.l5.m1.1.1.2">subscript</csymbol><ci id="alg1.l5.m1.1.1.2.2.cmml" xref="alg1.l5.m1.1.1.2.2">ùê∏</ci><ci id="alg1.l5.m1.1.1.2.3.cmml" xref="alg1.l5.m1.1.1.2.3">ùëÖ</ci></apply><csymbol cd="latexml" id="alg1.l5.m1.1.1.3.cmml" xref="alg1.l5.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">E_{R}\leftarrow</annotation></semantics></math><span id="alg1.l5.4" class="ltx_text" style="font-size:90%;"> </span><math id="alg1.l5.m2.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="alg1.l5.m2.1a"><mi mathsize="90%" id="alg1.l5.m2.1.1" xref="alg1.l5.m2.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="alg1.l5.m2.1b"><ci id="alg1.l5.m2.1.1.cmml" xref="alg1.l5.m2.1.1">ùëå</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m2.1c">Y</annotation></semantics></math><span id="alg1.l5.5" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l5.1" class="ltx_text" style="font-size:90%;float:right;"><math id="alg1.l5.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg1.l5.1.m1.1a"><mo id="alg1.l5.1.m1.1.1" xref="alg1.l5.1.m1.1.1.cmml">‚ñ∑</mo><annotation-xml encoding="MathML-Content" id="alg1.l5.1.m1.1b"><ci id="alg1.l5.1.m1.1.1.cmml" xref="alg1.l5.1.m1.1.1">‚ñ∑</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.1.m1.1c">\triangleright</annotation></semantics></math> Resize the edited image
</span>
</div>
<div id="alg1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l6.2.1.1" class="ltx_text" style="font-size:80%;">6:</span></span><span id="alg1.l6.3" class="ltx_text" style="font-size:90%;">¬†¬†¬†¬†</span><math id="alg1.l6.m1.1" class="ltx_Math" alttext="Cs\leftarrow" display="inline"><semantics id="alg1.l6.m1.1a"><mrow id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml"><mrow id="alg1.l6.m1.1.1.2" xref="alg1.l6.m1.1.1.2.cmml"><mi mathsize="90%" id="alg1.l6.m1.1.1.2.2" xref="alg1.l6.m1.1.1.2.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="alg1.l6.m1.1.1.2.1" xref="alg1.l6.m1.1.1.2.1.cmml">‚Äã</mo><mi mathsize="90%" id="alg1.l6.m1.1.1.2.3" xref="alg1.l6.m1.1.1.2.3.cmml">s</mi></mrow><mo mathsize="90%" stretchy="false" id="alg1.l6.m1.1.1.1" xref="alg1.l6.m1.1.1.1.cmml">‚Üê</mo><mi id="alg1.l6.m1.1.1.3" xref="alg1.l6.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><apply id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1"><ci id="alg1.l6.m1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1">‚Üê</ci><apply id="alg1.l6.m1.1.1.2.cmml" xref="alg1.l6.m1.1.1.2"><times id="alg1.l6.m1.1.1.2.1.cmml" xref="alg1.l6.m1.1.1.2.1"></times><ci id="alg1.l6.m1.1.1.2.2.cmml" xref="alg1.l6.m1.1.1.2.2">ùê∂</ci><ci id="alg1.l6.m1.1.1.2.3.cmml" xref="alg1.l6.m1.1.1.2.3">ùë†</ci></apply><csymbol cd="latexml" id="alg1.l6.m1.1.1.3.cmml" xref="alg1.l6.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">Cs\leftarrow</annotation></semantics></math><span id="alg1.l6.4" class="ltx_text" style="font-size:90%;"> </span><math id="alg1.l6.m2.1" class="ltx_Math" alttext="E_{R}" display="inline"><semantics id="alg1.l6.m2.1a"><msub id="alg1.l6.m2.1.1" xref="alg1.l6.m2.1.1.cmml"><mi mathsize="90%" id="alg1.l6.m2.1.1.2" xref="alg1.l6.m2.1.1.2.cmml">E</mi><mi mathsize="90%" id="alg1.l6.m2.1.1.3" xref="alg1.l6.m2.1.1.3.cmml">R</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l6.m2.1b"><apply id="alg1.l6.m2.1.1.cmml" xref="alg1.l6.m2.1.1"><csymbol cd="ambiguous" id="alg1.l6.m2.1.1.1.cmml" xref="alg1.l6.m2.1.1">subscript</csymbol><ci id="alg1.l6.m2.1.1.2.cmml" xref="alg1.l6.m2.1.1.2">ùê∏</ci><ci id="alg1.l6.m2.1.1.3.cmml" xref="alg1.l6.m2.1.1.3">ùëÖ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m2.1c">E_{R}</annotation></semantics></math><span id="alg1.l6.5" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l6.1" class="ltx_text" style="font-size:90%;float:right;"><math id="alg1.l6.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg1.l6.1.m1.1a"><mo id="alg1.l6.1.m1.1.1" xref="alg1.l6.1.m1.1.1.cmml">‚ñ∑</mo><annotation-xml encoding="MathML-Content" id="alg1.l6.1.m1.1b"><ci id="alg1.l6.1.m1.1.1.cmml" xref="alg1.l6.1.m1.1.1">‚ñ∑</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.1.m1.1c">\triangleright</annotation></semantics></math> Crop objects from the edited image
</span>
</div>
<div id="alg1.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l7.1.1.1" class="ltx_text" style="font-size:80%;">7:</span></span><span id="alg1.l7.2" class="ltx_text" style="font-size:90%;">¬†¬†¬†¬†</span><span id="alg1.l7.3" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span><span id="alg1.l7.4" class="ltx_text" style="font-size:90%;">¬†each </span><math id="alg1.l7.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="alg1.l7.m1.1a"><mi mathsize="90%" id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><ci id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1">ùê∂</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">C</annotation></semantics></math><span id="alg1.l7.5" class="ltx_text" style="font-size:90%;"> in </span><math id="alg1.l7.m2.1" class="ltx_Math" alttext="Cs" display="inline"><semantics id="alg1.l7.m2.1a"><mrow id="alg1.l7.m2.1.1" xref="alg1.l7.m2.1.1.cmml"><mi mathsize="90%" id="alg1.l7.m2.1.1.2" xref="alg1.l7.m2.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="alg1.l7.m2.1.1.1" xref="alg1.l7.m2.1.1.1.cmml">‚Äã</mo><mi mathsize="90%" id="alg1.l7.m2.1.1.3" xref="alg1.l7.m2.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l7.m2.1b"><apply id="alg1.l7.m2.1.1.cmml" xref="alg1.l7.m2.1.1"><times id="alg1.l7.m2.1.1.1.cmml" xref="alg1.l7.m2.1.1.1"></times><ci id="alg1.l7.m2.1.1.2.cmml" xref="alg1.l7.m2.1.1.2">ùê∂</ci><ci id="alg1.l7.m2.1.1.3.cmml" xref="alg1.l7.m2.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m2.1c">Cs</annotation></semantics></math><span id="alg1.l7.6" class="ltx_text" style="font-size:90%;">¬†</span><span id="alg1.l7.7" class="ltx_text ltx_font_bold" style="font-size:90%;">do</span><span id="alg1.l7.8" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l8.1.1.1" class="ltx_text" style="font-size:80%;">8:</span></span><span id="alg1.l8.2" class="ltx_text" style="font-size:90%;">¬†¬†¬†¬†¬†¬†¬†¬†¬†</span><span id="alg1.l8.3" class="ltx_text ltx_font_bold" style="font-size:90%;">if</span><span id="alg1.l8.4" class="ltx_text" style="font-size:90%;">¬†Object_Preservation_Checker(</span><math id="alg1.l8.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="alg1.l8.m1.1a"><mi mathsize="90%" id="alg1.l8.m1.1.1" xref="alg1.l8.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="alg1.l8.m1.1b"><ci id="alg1.l8.m1.1.1.cmml" xref="alg1.l8.m1.1.1">ùê∂</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m1.1c">C</annotation></semantics></math><span id="alg1.l8.5" class="ltx_text" style="font-size:90%;">) = boat¬†</span><span id="alg1.l8.6" class="ltx_text ltx_font_bold" style="font-size:90%;">then</span><span id="alg1.l8.7" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l9.1.1.1" class="ltx_text" style="font-size:80%;">9:</span></span><span id="alg1.l9.2" class="ltx_text" style="font-size:90%;">¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†Save </span><math id="alg1.l9.m1.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="alg1.l9.m1.1a"><mi mathsize="90%" id="alg1.l9.m1.1.1" xref="alg1.l9.m1.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.1b"><ci id="alg1.l9.m1.1.1.cmml" xref="alg1.l9.m1.1.1">ùëå</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.1c">Y</annotation></semantics></math><span id="alg1.l9.3" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l10.1.1.1" class="ltx_text" style="font-size:80%;">10:</span></span><span id="alg1.l10.2" class="ltx_text" style="font-size:90%;">¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†</span><span id="alg1.l10.3" class="ltx_text ltx_font_bold" style="font-size:90%;">break</span><span id="alg1.l10.4" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l11.1.1.1" class="ltx_text" style="font-size:80%;">11:</span></span><span id="alg1.l11.2" class="ltx_text" style="font-size:90%;">¬†¬†¬†¬†¬†¬†¬†¬†¬†</span><span id="alg1.l11.3" class="ltx_text ltx_font_bold" style="font-size:90%;">end</span><span id="alg1.l11.4" class="ltx_text" style="font-size:90%;">¬†</span><span id="alg1.l11.5" class="ltx_text ltx_font_bold" style="font-size:90%;">if</span>
</div>
<div id="alg1.l12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l12.1.1.1" class="ltx_text" style="font-size:80%;">12:</span></span><span id="alg1.l12.2" class="ltx_text" style="font-size:90%;">¬†¬†¬†¬†</span><span id="alg1.l12.3" class="ltx_text ltx_font_bold" style="font-size:90%;">end</span><span id="alg1.l12.4" class="ltx_text" style="font-size:90%;">¬†</span><span id="alg1.l12.5" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span>
</div>
<div id="alg1.l13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l13.1.1.1" class="ltx_text" style="font-size:80%;">13:</span></span><span id="alg1.l13.2" class="ltx_text ltx_font_bold" style="font-size:90%;">end</span><span id="alg1.l13.3" class="ltx_text" style="font-size:90%;">¬†</span><span id="alg1.l13.4" class="ltx_text ltx_font_bold" style="font-size:90%;">for</span>
</div>
</div>
</div>
</div>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Image Generation Module</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">Diffusion models¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a><span id="S4.SS1.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p1.1.4" class="ltx_text" style="font-size:90%;"> are capable of being trained with guiding input channels, enabling them to perform conditional image generation such as creating synthesizing visual content based on textual descriptions¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a><span id="S4.SS1.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p1.1.7" class="ltx_text" style="font-size:90%;">. Beyond synthesis, diffusion models are versatile image editors, allowing for targeted modifications. In this process, noise is added to the original image and subsequently denoised in response to an optional prompt, which describes the new image. Using masking techniques, these models can carry out semantic editing, selectively altering specific regions within the images while leaving others intact. Various approaches exist for utilizing Stable Diffusion in image editing, which can be broadly categorized into two main groups: those that employ pre-specified masks and those that mask images based on provided text descriptions.</span></p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text" style="font-size:90%;">In this work, we applied Blended Latent Diffusion method </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a><span id="S4.SS1.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p2.1.4" class="ltx_text" style="font-size:90%;"> to edit original sea images.
Blended Latent Diffusion provides a versatile approach to image editing, utilizing masks alongside guided text prompts. In a nutshell, it provides a versatile approach to image editing, utilizing masks alongside guided text prompts. To alter an image, a mask of identical dimension to that image is applied to designate the region to be modified, while guided text instructions define how the edited area should appear. The intended outcome is an image in which the masked region undergoes alterations while the unmasked portion remains unaltered. In the transformation process, the original image is encoded into a latent space, introducing a controlled level of noise. Additionally, the mask is downsampled within this latent space. At each stage of this process, the noisy latent image is subjected to denoising and is regarded as the foreground to be blended with the background elements. We observe that the method demonstrates promise in generating images with different ocean backgrounds whilst maintaining a considerable level of object preservation, in which the object can be recognized visually after the background edition process.</span></p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Sea State Classifier</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text" style="font-size:90%;">To provide information about Sea State level in the edited image‚Äôs background, we employed a Sea State classifier. We selected four distinct Sea States for our study, namely Sea States 1, 2, 3, and 4, as these are the only Sea States for which datasets are publicly available. The Sea State definitions are shown in Table </span><a href="#S2.T1" title="Table 1 ‚Ä£ 2 Related Works ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.SS2.p1.1.2" class="ltx_text" style="font-size:90%;"> .It is important to note that without recorded weather conditions at the time of image capture, visually classifying sea images into different Sea States is a challenging task. Leveraging the Manzoor-Umair Sea State Image Dataset (MU-SSiD) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p1.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a><span id="S4.SS2.p1.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS2.p1.1.5" class="ltx_text" style="font-size:90%;">, we trained a DenseNet¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p1.1.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a><span id="S4.SS2.p1.1.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS2.p1.1.8" class="ltx_text" style="font-size:90%;"> to categorize images into the four Sea State categories. The trained model achieved an accuracy rate of 71% against the testing dataset.</span></p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Object Preservation</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text" style="font-size:90%;">To evaluate whether an object remains preserved following image transformation, we developed an object preservation checker. This checker‚Äôs primary function is to identify objects that are no longer recognizable within the provided ground truth bounding box. To do this, we train a binary classifier on a dataset consiting of two classes: ‚Äùboat‚Äù and ‚Äùnot boat.‚Äù The ‚Äùboat‚Äù class contains images of cropped boats sourced from the ground truth SeaDroneSee dataset¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a><span id="S4.SS3.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS3.p1.1.4" class="ltx_text" style="font-size:90%;">, complemented by their augmented versions, which include flipping and blurring. In contrast, the ‚Äônot boat‚Äô class comprises images from the negative class extracted from the Boat-MNIST dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a><span id="S4.SS3.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS3.p1.1.7" class="ltx_text" style="font-size:90%;">, as well as crops from randomly selected backgrounds from synthetic images. Additionally, it includes crops intentionally containing small portions of boat objects from the edited images.These crops are generated by following the object ground truth bounding box, ensuring that they contain only one-fourth of the bounding box‚Äôs area with the rest outside. Using the dataset with only horizontal flip data augmentation for the boat class. We trained a DenseNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p1.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a><span id="S4.SS3.p1.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS3.p1.1.10" class="ltx_text" style="font-size:90%;"> model with a training batch size of 32, a fixed learning rate of 1e-5 without decay, and utilized the Adam optimization algorithm during the training process. Overall, the model achieves the accuracy of 74.86% against testing dataset, including crops of boat objects in real images and non-boat crops from real and edited images.</span></p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text" style="font-size:90%;">Subsequently, based on the given ground truth bounding box information, we extracted boat objects from the edited images for evaluation using the trained checker. We conducted random visual checks on the preserved boat objects to evaluate the model‚Äôs performance. While there are occasional misclassifications of objects as boats that do not visually resemble boats, the model generally achieves satisfactory accuracy in detecting non-boat objects, in which it can pick up cropped objects that resemble boats and filter out non-boat crops according to the visual checks with the accuracy of 69.45%. Examples of the cropped boat objects are illustrated in Figure </span><a href="#S4.F2" title="Figure 2 ‚Ä£ 4.3 Object Preservation ‚Ä£ 4 Proposed SafeSea method ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S4.SS3.p2.1.2" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<figure id="S4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F2.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/SS/crop_1.jpg" id="S4.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="685" height="398" alt="Refer to caption">
<figcaption class="ltx_caption"></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F2.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/SS/crop_3.jpg" id="S4.F2.2.g1" class="ltx_graphics ltx_img_landscape" width="685" height="420" alt="Refer to caption">
<figcaption class="ltx_caption"></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F2.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/SS/crop_4.jpg" id="S4.F2.3.g1" class="ltx_graphics ltx_img_landscape" width="685" height="357" alt="Refer to caption">
<figcaption class="ltx_caption"></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F2.4" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/SS/crop_6.jpg" id="S4.F2.4.g1" class="ltx_graphics ltx_img_landscape" width="685" height="349" alt="Refer to caption">
<figcaption class="ltx_caption"></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples of boat object crops from edited images. The crops are taken based on the ground truth bounding boxes provided from the source images in the ‚ÄôSeaDroneSea‚Äô dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">5 </span>SafeSea dataset</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text" style="font-size:90%;">The SafeSea dataset is created using the SafeSea method, involving the transformation of 300 calm ocean background images originally sourced from the ‚ÄôSeaDroneSee‚Äô dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a><span id="S5.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S5.p1.1.4" class="ltx_text" style="font-size:90%;">. All edited images were resized to match the dimensions of their respective originals.
The SafeSea method produces 69,694 images images. These are then classified into one of the sea state levels. The distribution of these images across different sea state levels is detailed in Table </span><a href="#S6.T3" title="Table 3 ‚Ä£ 6.1.2 Results ‚Ä£ 6.1 SafeSea method evaluation ‚Ä£ 6 Experiment ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S5.p1.1.5" class="ltx_text" style="font-size:90%;">.
Notably, Sea State levels 3 and 1 exhibit the most and the least number of images, boasting 45,066 and 2,087 images respectively. In the intermediate Sea State 2 category, there are 19,390 images, while Sea State 4 includes 3,151 images. For a visual representation, a selection of dataset examples is provided in Figure¬†</span><a href="#S5.F3" title="Figure 3 ‚Ä£ 5 SafeSea dataset ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S5.p1.1.6" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<figure id="S5.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F3.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/PerfectStorm/122_1_10818.jpg" id="S5.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="685" height="386" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;">Sea State Level 1</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F3.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/PerfectStorm/102_1_11233.jpg" id="S5.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="685" height="386" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;">Sea State Level 2</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F3.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/PerfectStorm/111_7_5565.jpg" id="S5.F3.3.g1" class="ltx_graphics ltx_img_landscape" width="685" height="386" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;">Sea State Level 3</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F3.4" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/PerfectStorm/122_4_5562.jpg" id="S5.F3.4.g1" class="ltx_graphics ltx_img_landscape" width="685" height="386" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;">Sea State Level 4</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Examples of images in the SafeSea dataset. There is one example representing each Sea State level in the dataset.</figcaption>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">6 </span>Experiment</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p"><span id="S6.p1.1.1" class="ltx_text" style="font-size:90%;">This section is divided into two parts. The first part evaluates the proposed SafeSea method efficacy in filtering compared with the other methods such as the vanilla Stable Diffusion Inpainting. Then, we study the performance of YoloV5 detection model on the proposed SafeSea dataset.</span></p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>SafeSea method evaluation</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p"><span id="S6.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">We first describe the experiment setup and then present the results afterwards.</span></p>
</div>
<section id="S6.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1 </span>Experiment setup</h4>

<div id="S6.SS1.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S6.SS1.SSS1.p1.1" class="ltx_p"><span id="S6.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Evaluation protocol - </span><span id="S6.SS1.SSS1.p1.1.2" class="ltx_text" style="font-size:90%;"> We generate 100 images from each evaluated method. Then, each image is manually checked by humans and categorized into good quality of bad quality group. A good quality image is defined as the following rules:</span></p>
<ul id="S6.I1" class="ltx_itemize">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p"><span id="S6.I1.i1.p1.1.1" class="ltx_text" style="font-size:90%;">The background should contain either island, ocean or cloud. For instance if the background contains unexpected objects such as a fridge, then the image is deemed low quality.</span></p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p"><span id="S6.I1.i2.p1.1.1" class="ltx_text" style="font-size:90%;">The background should look realistic</span></p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p"><span id="S6.I1.i3.p1.1.1" class="ltx_text" style="font-size:90%;">All the objects should look visually acceptable. At least one boat is preserved in the image.</span></p>
</div>
</li>
</ul>
<p id="S6.SS1.SSS1.p1.2" class="ltx_p"><span id="S6.SS1.SSS1.p1.2.1" class="ltx_text" style="font-size:90%;">Each method is then compared by looking at its percentage of generating good quality images.</span></p>
</div>
<div id="S6.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S6.SS1.SSS1.p2.1" class="ltx_p"><span id="S6.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Methods - </span><span id="S6.SS1.SSS1.p2.1.2" class="ltx_text" style="font-size:90%;"> The proposed SafeSea method is compared against two baselines: (1) the vanilla Stable Diffusion inpainting¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.SS1.SSS1.p2.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a><span id="S6.SS1.SSS1.p2.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S6.SS1.SSS1.p2.1.5" class="ltx_text" style="font-size:90%;">; and (2) the vanilla Blended Latent Diffusion¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.SS1.SSS1.p2.1.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a><span id="S6.SS1.SSS1.p2.1.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S6.SS1.SSS1.p2.1.8" class="ltx_text" style="font-size:90%;">. Details for each baseline parameters is presented as follow.</span></p>
</div>
<div id="S6.SS1.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S6.SS1.SSS1.p3.1" class="ltx_p"><span id="S6.SS1.SSS1.p3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Stable Diffusion inpainting (SD Inpainting) - </span><span id="S6.SS1.SSS1.p3.1.2" class="ltx_text" style="font-size:90%;"> We use Stable Diffusion v1-4¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.SS1.SSS1.p3.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a><span id="S6.SS1.SSS1.p3.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S6.SS1.SSS1.p3.1.5" class="ltx_text" style="font-size:90%;"> with batch size of one. The masked image is derived from the groundtruth bounding boxes. The method is then fed with the following prompts to generate the images.</span></p>
<ul id="S6.I2" class="ltx_itemize">
<li id="S6.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S6.I2.i1.p1" class="ltx_para">
<p id="S6.I2.i1.p1.1" class="ltx_p"><span id="S6.I2.i1.p1.1.1" class="ltx_text" style="font-size:90%;">Sea State 1: Aerial image of the sea‚Äôs surface. The water is gently rippled with no waves breaking. Canon EOS R3, Nikon d850 400mm, Canon DSLR, lens 300mm, 4K, HD.</span></p>
</div>
</li>
<li id="S6.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S6.I2.i2.p1" class="ltx_para">
<p id="S6.I2.i2.p1.1" class="ltx_p"><span id="S6.I2.i2.p1.1.1" class="ltx_text" style="font-size:90%;">Sea State 2: Aerial image of the sea‚Äôs surface. There are slight waves breaking with smooth wave on surface. Canon EOS R3, Nikon d850 400mm, Canon DSLR, lens 300mm, 4K, HD.</span></p>
</div>
</li>
<li id="S6.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S6.I2.i3.p1" class="ltx_para">
<p id="S6.I2.i3.p1.1" class="ltx_p"><span id="S6.I2.i3.p1.1.1" class="ltx_text" style="font-size:90%;">Sea State 3: Aerial image of the sea‚Äôs surface. Mild Waves are slight causing rock buoys and small craft. Canon EOS R3, Nikon d850 400mm, Canon DSLR, lens 300mm, 4K, HD.</span></p>
</div>
</li>
<li id="S6.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S6.I2.i4.p1" class="ltx_para">
<p id="S6.I2.i4.p1.1" class="ltx_p"><span id="S6.I2.i4.p1.1.1" class="ltx_text" style="font-size:90%;">Sea State 4: Aerial image of the sea‚Äôs surface. The water has furrowed appearance with moderate waves. Canon EOS R3, Nikon d850 400mm, Canon DSLR, lens 300mm, 4K, HD.</span></p>
</div>
</li>
</ul>
</div>
<div id="S6.SS1.SSS1.p4" class="ltx_para ltx_noindent">
<p id="S6.SS1.SSS1.p4.1" class="ltx_p"><span id="S6.SS1.SSS1.p4.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Blended Latent Diffusion (BLD) - </span><span id="S6.SS1.SSS1.p4.1.2" class="ltx_text" style="font-size:90%;">
We use Stable Diffusion v2-1-base </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.SS1.SSS1.p4.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a><span id="S6.SS1.SSS1.p4.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S6.SS1.SSS1.p4.1.5" class="ltx_text" style="font-size:90%;"> with batch size of ten. Similar to the method above, we use the derived mask image from ground truth bounding boxes. We observe that when editing the sea background of an image to various Sea State Level, the reliance on prompts alone is insufficient. Therefore, to simplify the generation issue with prompt we only use one prompt in our experiments, which is ‚ÄúAerial image of sea‚Äôs surface. Canon EOS R3, Nikon d850 400mm, Canon DSLR, lens 300mm, 4K, HD‚Äù. The prompt allows generation of images with varying sea state levels with different generation seeds.</span></p>
</div>
<div id="S6.SS1.SSS1.p5" class="ltx_para ltx_noindent">
<p id="S6.SS1.SSS1.p5.1" class="ltx_p"><span id="S6.SS1.SSS1.p5.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">SafeSea (proposed) - </span><span id="S6.SS1.SSS1.p5.1.2" class="ltx_text" style="font-size:90%;"> We utilize Blended Latent Diffusion with the same parameters as above to edit original images. Then we use the sea state level classifier to determine the sea state level of the generated images. The Object Preservation Checker ensures objects are preserved. The generated image will be preserved if it preserves at least one object.
Same as the other methods, the mask image is derived from the groundtruth bounding boxes provided by the SeaDroneSee dataset¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.SS1.SSS1.p5.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a><span id="S6.SS1.SSS1.p5.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S6.SS1.SSS1.p5.1.5" class="ltx_text" style="font-size:90%;">. Note that this dataset contains several classes, but in this experiment we only aim to preserve the boat as boats have much larger object size.</span></p>
</div>
</section>
<section id="S6.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2 </span>Results</h4>

<div id="S6.SS1.SSS2.p1" class="ltx_para">
<p id="S6.SS1.SSS2.p1.1" class="ltx_p"><span id="S6.SS1.SSS2.p1.1.1" class="ltx_text" style="font-size:90%;">Table¬†</span><a href="#S6.T2" title="Table 2 ‚Ä£ 6.1.2 Results ‚Ä£ 6.1 SafeSea method evaluation ‚Ä£ 6 Experiment ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S6.SS1.SSS2.p1.1.2" class="ltx_text" style="font-size:90%;"> presents the comparison results. The results suggest that the proposed SafeSea outperforms the baselines. As expected the BLD produces better image quality than the SD Inpainting as also shown in¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.SS1.SSS2.p1.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a><span id="S6.SS1.SSS2.p1.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S6.SS1.SSS2.p1.1.5" class="ltx_text" style="font-size:90%;"></span></p>
</div>
<div id="S6.SS1.SSS2.p2" class="ltx_para">
<p id="S6.SS1.SSS2.p2.1" class="ltx_p"><span id="S6.SS1.SSS2.p2.1.1" class="ltx_text" style="font-size:90%;">It is clear that SafeSea outperforms the other methods, as it generates more good images with a realistic sea background and maintains object integrity more effectively.</span></p>
</div>
<figure id="S6.T2" class="ltx_table">
<table id="S6.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T2.3.4.1" class="ltx_tr">
<th id="S6.T2.3.4.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S6.T2.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T2.3.4.1.2.1" class="ltx_text" style="font-size:90%;">Good images (in %)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T2.1.1" class="ltx_tr">
<th id="S6.T2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S6.T2.1.1.2.1" class="ltx_text" style="font-size:90%;">SD Inpainting¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T2.1.1.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a><span id="S6.T2.1.1.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S6.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S6.T2.1.1.1.1" class="ltx_text" style="font-size:90%;">26.5% </span><math id="S6.T2.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T2.1.1.1.m1.1a"><mo mathsize="90%" id="S6.T2.1.1.1.m1.1.1" xref="S6.T2.1.1.1.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S6.T2.1.1.1.m1.1b"><csymbol cd="latexml" id="S6.T2.1.1.1.m1.1.1.cmml" xref="S6.T2.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.1.1.1.m1.1c">\pm</annotation></semantics></math><span id="S6.T2.1.1.1.2" class="ltx_text" style="font-size:90%;"> 5.94</span>
</td>
</tr>
<tr id="S6.T2.2.2" class="ltx_tr">
<th id="S6.T2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S6.T2.2.2.2.1" class="ltx_text" style="font-size:90%;">BLD¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T2.2.2.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a><span id="S6.T2.2.2.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S6.T2.2.2.1" class="ltx_td ltx_align_center">
<span id="S6.T2.2.2.1.1" class="ltx_text" style="font-size:90%;">52.94% </span><math id="S6.T2.2.2.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T2.2.2.1.m1.1a"><mo mathsize="90%" id="S6.T2.2.2.1.m1.1.1" xref="S6.T2.2.2.1.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S6.T2.2.2.1.m1.1b"><csymbol cd="latexml" id="S6.T2.2.2.1.m1.1.1.cmml" xref="S6.T2.2.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.2.2.1.m1.1c">\pm</annotation></semantics></math><span id="S6.T2.2.2.1.2" class="ltx_text" style="font-size:90%;"> 8.25</span>
</td>
</tr>
<tr id="S6.T2.3.3" class="ltx_tr">
<th id="S6.T2.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S6.T2.3.3.2.1" class="ltx_text" style="font-size:90%;">SafeSea (proposed)</span></th>
<td id="S6.T2.3.3.1" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S6.T2.3.3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">63.59%</span><span id="S6.T2.3.3.1.2" class="ltx_text" style="font-size:90%;"> </span><math id="S6.T2.3.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S6.T2.3.3.1.m1.1a"><mo mathsize="90%" id="S6.T2.3.3.1.m1.1.1" xref="S6.T2.3.3.1.m1.1.1.cmml">¬±</mo><annotation-xml encoding="MathML-Content" id="S6.T2.3.3.1.m1.1b"><csymbol cd="latexml" id="S6.T2.3.3.1.m1.1.1.cmml" xref="S6.T2.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.3.3.1.m1.1c">\pm</annotation></semantics></math><span id="S6.T2.3.3.1.3" class="ltx_text" style="font-size:90%;"> 2.76</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison between SD Inpainting, BLD and SafeSea when we manually check quality of the 100 generated images from each method. The good image rate computes the percentage of good images passed by manually check by humans. High good image rate suggests the method works better for the task.</figcaption>
</figure>
<div id="S6.SS1.SSS2.p3" class="ltx_para">
<p id="S6.SS1.SSS2.p3.1" class="ltx_p"><span id="S6.SS1.SSS2.p3.1.1" class="ltx_text" style="font-size:90%;">While the result of Stable Diffusion Inpainting appear to achieve realism, many objects are blended into the background, or overly edited. Additionally, there are other inherent issues, such as other objects being inserted into the edited image. It appears as though the objects of interest may be inadvertently duplicated and dispersed into the background, as shown in figure </span><a href="#S6.F4" title="Figure 4 ‚Ä£ 6.1.2 Results ‚Ä£ 6.1 SafeSea method evaluation ‚Ä£ 6 Experiment ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S6.SS1.SSS2.p3.1.2" class="ltx_text" style="font-size:90%;">. Although some edited background is of considerable quality, many of objects of interest are not retained. Additionally, the edited images may not be suitable for use due to the introduction of irrelevant modified objects and unexpected background.</span></p>
</div>
<div id="S6.SS1.SSS2.p4" class="ltx_para">
<p id="S6.SS1.SSS2.p4.1" class="ltx_p"><span id="S6.SS1.SSS2.p4.1.1" class="ltx_text" style="font-size:90%;">When employing Blended Latent Diffusion, the issue of objects of interest being uncontrollably spread is mitigated, as demonstrated in Figure </span><a href="#S6.F5" title="Figure 5 ‚Ä£ 6.1.2 Results ‚Ä£ 6.1 SafeSea method evaluation ‚Ä£ 6 Experiment ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">5</span></a><span id="S6.SS1.SSS2.p4.1.2" class="ltx_text" style="font-size:90%;">. Notably, the boat objects are effectively preserved, and the edited background exhibits acceptable quality. Nevertheless, it is worth mentioning that the generated sea background does not possess the same vividness observed in Stable Diffusion Inpainting. This disparity may be affected by the utilisation of different trained Diffusion models in these approaches. Future work will investigate other image generation methods that have similar preservation properties to the Blended Latent Diffusion whilst producing more vivid background.</span></p>
</div>
<figure id="S6.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F4.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/original/1309.jpg" id="S6.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="685" height="456" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;">Original</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F4.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/inpainting/resized_1309.png" id="S6.F4.2.g1" class="ltx_graphics ltx_img_landscape" width="685" height="456" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;">Edited</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F4.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/original/10774.jpg" id="S6.F4.3.g1" class="ltx_graphics ltx_img_landscape" width="685" height="386" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;">Original</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F4.4" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/inpainting/resized_10774.png" id="S6.F4.4.g1" class="ltx_graphics ltx_img_landscape" width="685" height="386" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;">Edited</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F4.5" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/original/10818.jpg" id="S6.F4.5.g1" class="ltx_graphics ltx_img_landscape" width="685" height="386" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;">Original</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F4.6" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/inpainting/10818_inpaint.png" id="S6.F4.6.g1" class="ltx_graphics ltx_img_landscape" width="685" height="386" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;">Edited</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Failure edited images using SD Inpainting. It suggests that this method can edit the image‚Äôs sea background, however, the objects are not retained well, and several irrelevant objects are introduced unintentionally.</figcaption>
</figure>
<figure id="S6.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F5.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/original/1309.jpg" id="S6.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="685" height="456" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;">Original</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F5.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/BLD/resized_102_8_1309.png" id="S6.F5.2.g1" class="ltx_graphics ltx_img_landscape" width="685" height="456" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;">Edited</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F5.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/original/10774.jpg" id="S6.F5.3.g1" class="ltx_graphics ltx_img_landscape" width="685" height="386" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;">Original</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F5.4" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/BLD/resized_103_8_10774.png" id="S6.F5.4.g1" class="ltx_graphics ltx_img_landscape" width="685" height="386" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;">Edited</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Example of good edited images using Blended Latent Diffusion. The image‚Äôs sea background is edited while the objects are preserved.</figcaption>
</figure>
<figure id="S6.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F6.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/SS/SS1_122_1_5576.jpg" id="S6.F6.1.g1" class="ltx_graphics ltx_img_square" width="685" height="685" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;">Sea State Level 1</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F6.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/SS/SS2_130_4_9273.jpg" id="S6.F6.2.g1" class="ltx_graphics ltx_img_square" width="685" height="685" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;">Sea State Level 2</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F6.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/SS/SS3_104_8_5545.jpg" id="S6.F6.3.g1" class="ltx_graphics ltx_img_square" width="685" height="685" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;">Sea State Level 3</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S6.F6.4" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:114.3pt;"><img src="/html/2311.14764/assets/sections/images/SS/SS4_106_3_5146.jpg" id="S6.F6.4.g1" class="ltx_graphics ltx_img_square" width="685" height="685" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;">Sea State Level 4</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Examples of edited images belong to different classified sea state levels. The Sea State level information is provided to these images after the editing process. The sea surface becomes more dynamic with waves and whitecaps as the Sea State Level increases.</figcaption>
</figure>
<div id="S6.SS1.SSS2.p5" class="ltx_para">
<p id="S6.SS1.SSS2.p5.1" class="ltx_p"><span id="S6.SS1.SSS2.p5.1.1" class="ltx_text" style="font-size:90%;">To further confirm the above result, we present the percentage of images produced by SD Inpainting and BLD retained after applying our SafeSea filter. Results in Table¬†</span><a href="#S6.T4" title="Table 4 ‚Ä£ 6.1.2 Results ‚Ä£ 6.1 SafeSea method evaluation ‚Ä£ 6 Experiment ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S6.SS1.SSS2.p5.1.2" class="ltx_text" style="font-size:90%;"> suggest that SD Inpainting produces much lower quality than BLD as significant amount of images are filtered.</span></p>
</div>
<figure id="S6.T3" class="ltx_table">
<table id="S6.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T3.2.1.1" class="ltx_tr">
<th id="S6.T3.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S6.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T3.2.1.1.2.1" class="ltx_text" style="font-size:90%;">SS1</span></th>
<th id="S6.T3.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T3.2.1.1.3.1" class="ltx_text" style="font-size:90%;">SS2</span></th>
<th id="S6.T3.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T3.2.1.1.4.1" class="ltx_text" style="font-size:90%;">SS3</span></th>
<th id="S6.T3.2.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T3.2.1.1.5.1" class="ltx_text" style="font-size:90%;">SS4</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T3.2.2.1" class="ltx_tr">
<th id="S6.T3.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S6.T3.2.2.1.1.1" class="ltx_text" style="font-size:90%;">Generated</span></th>
<td id="S6.T3.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T3.2.2.1.2.1" class="ltx_text" style="font-size:90%;">2,336</span></td>
<td id="S6.T3.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T3.2.2.1.3.1" class="ltx_text" style="font-size:90%;">25,114</span></td>
<td id="S6.T3.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T3.2.2.1.4.1" class="ltx_text" style="font-size:90%;">65,275</span></td>
<td id="S6.T3.2.2.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S6.T3.2.2.1.5.1" class="ltx_text" style="font-size:90%;">4,275</span></td>
</tr>
<tr id="S6.T3.2.3.2" class="ltx_tr">
<th id="S6.T3.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S6.T3.2.3.2.1.1" class="ltx_text" style="font-size:90%;">Filtered</span></th>
<td id="S6.T3.2.3.2.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T3.2.3.2.2.1" class="ltx_text" style="font-size:90%;">2,087</span></td>
<td id="S6.T3.2.3.2.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T3.2.3.2.3.1" class="ltx_text" style="font-size:90%;">19,390</span></td>
<td id="S6.T3.2.3.2.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T3.2.3.2.4.1" class="ltx_text" style="font-size:90%;">45,066</span></td>
<td id="S6.T3.2.3.2.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S6.T3.2.3.2.5.1" class="ltx_text" style="font-size:90%;">3,151</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Number of images generated before and after applying object preservation checker. 97,000 images are generated from 300 sourced images and then filter out the ones that do not retain any object.</figcaption>
</figure>
<figure id="S6.T4" class="ltx_table">
<table id="S6.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T4.2.1.1" class="ltx_tr">
<th id="S6.T4.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="S6.T4.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S6.T4.2.1.1.2.1" class="ltx_text" style="font-size:90%;">SD Inpainting¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T4.2.1.1.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a><span id="S6.T4.2.1.1.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S6.T4.2.1.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt">
<span id="S6.T4.2.1.1.3.1" class="ltx_text" style="font-size:90%;">BLD¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T4.2.1.1.3.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a><span id="S6.T4.2.1.1.3.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S6.T4.2.2.2" class="ltx_tr">
<th id="S6.T4.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="S6.T4.2.2.2.1.1" class="ltx_text" style="font-size:90%;">Image passing rate (in %)</span></th>
<td id="S6.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S6.T4.2.2.2.2.1" class="ltx_text" style="font-size:90%;">8.16%</span></td>
<td id="S6.T4.2.2.2.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t"><span id="S6.T4.2.2.2.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">71.85%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison between SD Inpainting and BLD when we apply our SafeSea filters. The Image passing rate computes the percentage of images passed by our filters. High passing rate suggests better quality images according to our filters.</figcaption>
</figure>
</section>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Applying YoloV5 on the SafeSea dataset</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p"><span id="S6.SS2.p1.1.1" class="ltx_text" style="font-size:90%;">We employed the SafeSea dataset for assessing a pre-trained YoloV5 object detection model‚Äôs ability in detecting ‚Äôboat‚Äô objects across various Sea State levels.</span></p>
</div>
<section id="S6.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1 </span>Experiment setup</h4>

<div id="S6.SS2.SSS1.p1" class="ltx_para">
<p id="S6.SS2.SSS1.p1.1" class="ltx_p"><span id="S6.SS2.SSS1.p1.1.1" class="ltx_text" style="font-size:90%;">We run the YoloV5 with default parameters on the SafeSea dataset images. the Mean Average Precision (mAP) for the four sea state level is then calculated.</span></p>
</div>
</section>
<section id="S6.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2 </span>Results</h4>

<div id="S6.SS2.SSS2.p1" class="ltx_para">
<p id="S6.SS2.SSS2.p1.1" class="ltx_p"><span id="S6.SS2.SSS2.p1.1.1" class="ltx_text" style="font-size:90%;">Figure </span><a href="#S6.F7" title="Figure 7 ‚Ä£ 6.2.2 Results ‚Ä£ 6.2 Applying YoloV5 on the SafeSea dataset ‚Ä£ 6 Experiment ‚Ä£ SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">7</span></a><span id="S6.SS2.SSS2.p1.1.2" class="ltx_text" style="font-size:90%;"> presents the result. Notably, there is a noticeable decrease in mAP values from Sea State 1 to Sea State 4, both for IoU of 0.5 and the range of 0.5 to 0.95.</span></p>
</div>
<figure id="S6.F7" class="ltx_figure"><img src="/html/2311.14764/assets/x1.png" id="S6.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>mAP Comparison for Different Sea State Levels. It shows that the mAP values decrease from Sea State 1 to Sea State 4 for both IoU of 0.5 and from 0.5 to 0.95.</figcaption>
</figure>
<div id="S6.SS2.SSS2.p2" class="ltx_para">
<p id="S6.SS2.SSS2.p2.1" class="ltx_p"><span id="S6.SS2.SSS2.p2.1.1" class="ltx_text" style="font-size:90%;">Given that the Sea State Level background is not entirely controlled within the transformation process, the distribution of images among the sea state categories is determined by the Sea State Classifier. The results indicate a significantly higher number of images classified as Sea State 3 of 45,066; while Sea State 1 comprises the lowest number with 2,087 images. It is important to note that each image originally contains a varying number of objects, both before and after the editing process. Additionally, object size plays a considerable role in the editing process; we have observed that larger objects tend to be better preserved in comparison to smaller ones. This, in turn, has a direct impact on the object detection confidence scores, which subsequently affect the mAP scores. In general, we observe that the pre-trained model tends to be more struggle when detecting objects in images classified with higher sea state levels. However, it is crucial to acknowledge that several other factors also influence the results, such as the number of objects being preserved and the sizes of the original objects.</span></p>
</div>
</section>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p"><span id="S7.p1.1.1" class="ltx_text" style="font-size:90%;">In this work, we introduce SafeSea, a proof-of-concept for generating synthetic maritime images by modifying real images, where the original sea background is transformed to simulate various sea surface conditions, corresponding to different Sea States. Our method capitalizes on the capabilities of Blended Latent Diffusion </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a><span id="S7.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S7.p1.1.4" class="ltx_text" style="font-size:90%;"> to manipulate images. Subsequently, these modified images are categorized into distinct Sea State Levels, ranging from 1 to 4. Moreover, the original objects within these images are scrutinized to ensure their preservation throughout the editing process. Employing this technique, we have created the SafeSea dataset, which includes maritime images featuring marine objects set against diverse Sea State backgrounds by utilizing the ‚ÄôSeaDroneSea‚Äô dataset¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S7.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a><span id="S7.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S7.p1.1.7" class="ltx_text" style="font-size:90%;">. Additionally, we have observed that stormy sea backgrounds can impact the performance of the YoloV5 object detection model.</span></p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">8 </span>Limitation and Future Work</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p"><span id="S8.p1.1.1" class="ltx_text" style="font-size:90%;">The current SeaSafe method, as proposed, exhibits certain limitations that necessitate attention in our future work. Primarily, it lacks control over the generated sea background during the editing process, limiting the diversity of realistic backgrounds. Furthermore, the diffusion model employed by BLD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S8.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a><span id="S8.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S8.p1.1.4" class="ltx_text" style="font-size:90%;"> has constraints in generating realistic wave and whitecap patterns. Additionally, smaller objects such as swimmers are ignored in the image quality evaluation. Our forthcoming efforts will concentrate on optimizing the image editing process to elevate the overall quality of generated images. Exploring alternative image editing methods is also on the agenda to enhance the image generation module. Additionally, addressing the control over the insertion of irrelevant objects during editing is crucial, as it can significantly impact object detection models. Future work will specifically tackle the introduction of unexpected objects, mitigating their potential impact on object detection models. Simultaneously, improvements are planned for both the Sea State Classifier and Object Detection Checker to elevate their performance. Furthermore, we aim to implement an additional filter to exclude generated images that do not align with the desired Sea State Level criteria. These enhancements collectively constitute our roadmap for refining the SeaSafe method in subsequent stages of development. Future work will also delve into the scalability of SafeSea on larger datasets and in real-world scenarios beyond the SeaDronesSee dataset with the exploration of how the object detector performs against the unedited SeaDronesSee dataset.</span></p>
</div>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">9 </span>Acknowledgement</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p"><span id="S9.p1.1.1" class="ltx_text" style="font-size:90%;">This work has been supported by Sentient Vision Systems. Sentient Vision Systems is one of the leading Australian developers of computer vision and artificial intelligence software solutions for defence and civilian applications.</span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.2.2.1" class="ltx_text" style="font-size:90%;">[1]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.4.1" class="ltx_text" style="font-size:90%;">
Omri Avrahami, Ohad Fried, and Dani Lischinski.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.5.1" class="ltx_text" style="font-size:90%;">Blended latent diffusion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM Transactions on Graphics</span><span id="bib.bib1.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.2.2.1" class="ltx_text" style="font-size:90%;">[2]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.4.1" class="ltx_text" style="font-size:90%;">
Jonathan Binner Becktor, Frederik Emil Thorsson Saabye Sch√∂ller,
Evangelos Boukas, Mogens Blanke, and Lazaros Nalpantidis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.5.1" class="ltx_text" style="font-size:90%;">Bolstering maritime object detection with synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IFAC</span><span id="bib.bib2.8.3" class="ltx_text" style="font-size:90%;">. Elsevier, 2022.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.2.2.1" class="ltx_text" style="font-size:90%;">[3]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.4.1" class="ltx_text" style="font-size:90%;">
Jonathan Becktor, William Seto, Aditya Deole, Saptarshi Bandyopadhyay, Niyousha
Rahimi, Shahriar Talebi, Mehran Mesbahi, and Amir Rahmani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.5.1" class="ltx_text" style="font-size:90%;">Robust vision-based multi-spacecraft guidance navigation and control
using cnn-based pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AERO</span><span id="bib.bib3.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.2.2.1" class="ltx_text" style="font-size:90%;">[4]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.4.1" class="ltx_text" style="font-size:90%;">
Blender¬†Online Community.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.5.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Blender - a 3D modelling and rendering package</span><span id="bib.bib4.6.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.2.2.1" class="ltx_text" style="font-size:90%;">[5]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.4.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen
Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.5.1" class="ltx_text" style="font-size:90%;">CARLA: An open urban driving simulator.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.6.1" class="ltx_text" style="font-size:90%;">In Sergey Levine, Vincent Vanhoucke, and Ken Goldberg, editors, </span><span id="bib.bib5.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib5.8.3" class="ltx_text" style="font-size:90%;">. PMLR, 2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.2.2.1" class="ltx_text" style="font-size:90%;">[6]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.4.1" class="ltx_text" style="font-size:90%;">
US¬†‚Äî EPA.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.5.1" class="ltx_text" style="font-size:90%;">Climate change indicators: Weather and climate.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.epa.gov/climate-indicators/weather-climate" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.epa.gov/climate-indicators/weather-climate</a><span id="bib.bib6.6.1" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">[Online; accessed 06-October-2023].
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.2.2.1" class="ltx_text" style="font-size:90%;">[7]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.4.1" class="ltx_text" style="font-size:90%;">
Epic Games.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.5.1" class="ltx_text" style="font-size:90%;">Unreal engine.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.2.2.1" class="ltx_text" style="font-size:90%;">[8]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.4.1" class="ltx_text" style="font-size:90%;">
Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit¬†H. Bermano, Gal
Chechik, and Daniel Cohen-Or.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.5.1" class="ltx_text" style="font-size:90%;">An image is worth one word: Personalizing text-to-image generation
using textual inversion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib8.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.2.2.1" class="ltx_text" style="font-size:90%;">[9]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.4.1" class="ltx_text" style="font-size:90%;">
Antonio-Javier Gallego, Antonio Pertusa, and Pablo Gil.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.5.1" class="ltx_text" style="font-size:90%;">Automatic ship classification from optical aerial images with
convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Remote Sensing</span><span id="bib.bib9.7.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.2.2.1" class="ltx_text" style="font-size:90%;">[10]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.4.1" class="ltx_text" style="font-size:90%;">
Rockstar Games.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.5.1" class="ltx_text" style="font-size:90%;">Grand theft auto v.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.2.2.1" class="ltx_text" style="font-size:90%;">[11]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.4.1" class="ltx_text" style="font-size:90%;">
Emily Heaslip.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.5.1" class="ltx_text" style="font-size:90%;">Why marine weather forecasts are so inaccurate - and how to improve
them.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.2.2.1" class="ltx_text" style="font-size:90%;">[12]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.4.1" class="ltx_text" style="font-size:90%;">
Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel
Cohen-or.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.5.1" class="ltx_text" style="font-size:90%;">Prompt-to-prompt image editing with cross-attention control.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib12.8.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.2.2.1" class="ltx_text" style="font-size:90%;">[13]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.4.1" class="ltx_text" style="font-size:90%;">
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.5.1" class="ltx_text" style="font-size:90%;">Denoising diffusion probabilistic models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib13.8.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.2.2.1" class="ltx_text" style="font-size:90%;">[14]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.4.1" class="ltx_text" style="font-size:90%;">
Gao Huang, Zhuang Liu, Laurens van¬†der Maaten, and Kilian¬†Q. Weinberger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.5.1" class="ltx_text" style="font-size:90%;">Densely connected convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib14.8.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.2.2.1" class="ltx_text" style="font-size:90%;">[15]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.4.1" class="ltx_text" style="font-size:90%;">
inversion Jeff¬†Faudi, Martin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.5.1" class="ltx_text" style="font-size:90%;">Airbus ship detection challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.6.1" class="ltx_text" style="font-size:90%;">Kaggle, 2018.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.2.2.1" class="ltx_text" style="font-size:90%;">[16]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.4.1" class="ltx_text" style="font-size:90%;">
Arthur Juliani, Vincent-Pierre Berges, Ervin Teng, Andrew Cohen, Jonathan
Harper, Chris Elion, Chris Goy, Yuan Gao, Hunter Henry, Marwan Mattar, and
Danny Lange.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.5.1" class="ltx_text" style="font-size:90%;">Unity: A general platform for intelligent agents.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.6.1" class="ltx_text" style="font-size:90%;">ArXiv, 2020.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.2.2.1" class="ltx_text" style="font-size:90%;">[17]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.4.1" class="ltx_text" style="font-size:90%;">
Benjamin Kiefer, David Ott, and Andreas Zell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.5.1" class="ltx_text" style="font-size:90%;">Leveraging synthetic data in object detection on unmanned aerial
vehicles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICPR</span><span id="bib.bib17.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.2.2.1" class="ltx_text" style="font-size:90%;">[18]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.4.1" class="ltx_text" style="font-size:90%;">
Xiaomin Lin, Cheng Liu, Allen Pattillo, Miao Yu, and Yiannis Aloimonous.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.5.1" class="ltx_text" style="font-size:90%;">Seadronesim: Simulation of aerial images for detection of objects
above water.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">WACV</span><span id="bib.bib18.8.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.2.2.1" class="ltx_text" style="font-size:90%;">[19]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.4.1" class="ltx_text" style="font-size:90%;">
Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.5.1" class="ltx_text" style="font-size:90%;">Null-text inversion for editing real images using guided diffusion
models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib19.8.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.2.2.1" class="ltx_text" style="font-size:90%;">[20]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.4.1" class="ltx_text" style="font-size:90%;">
ntnu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.5.1" class="ltx_text" style="font-size:90%;">seadronessee-odv2-rebalanced 2x2-train-val-1x1-test dataset.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://universe.roboflow.com/ntnu-2wibj/seadronessee-odv2-rebalanced-2x2-train-val-1x1-test" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://universe.roboflow.com/ntnu-2wibj/seadronessee-odv2-rebalanced-2x2-train-val-1x1-test</a><span id="bib.bib20.6.1" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">visited on 2023-10-06.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.2.2.1" class="ltx_text" style="font-size:90%;">[21]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.4.1" class="ltx_text" style="font-size:90%;">
Luis Patino, Tom Cane, Alain Vallee, and James Ferryman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.5.1" class="ltx_text" style="font-size:90%;">Pets 2016: Dataset and challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPRW</span><span id="bib.bib21.8.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.2.2.1" class="ltx_text" style="font-size:90%;">[22]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.4.1" class="ltx_text" style="font-size:90%;">
Xingchao Peng, Baochen Sun, Karim Ali, and Kate Saenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.5.1" class="ltx_text" style="font-size:90%;">Learning deep object detectors from 3d models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib22.8.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.2.2.1" class="ltx_text" style="font-size:90%;">[23]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.4.1" class="ltx_text" style="font-size:90%;">
Dilip¬†K. Prasad, Deepu Rajan, Lily Rachmawati, Eshan Rajabally, and Chai Quek.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.5.1" class="ltx_text" style="font-size:90%;">Video processing from electro-optical sensors for object detection
and tracking in a maritime environment: A survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Intelligent Transportation Systems</span><span id="bib.bib23.7.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.2.2.1" class="ltx_text" style="font-size:90%;">[24]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.4.1" class="ltx_text" style="font-size:90%;">
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.5.1" class="ltx_text" style="font-size:90%;">Hierarchical text-conditional image generation with clip latents.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.6.1" class="ltx_text" style="font-size:90%;">ArXiv, 2022.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.2.2.1" class="ltx_text" style="font-size:90%;">[25]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.4.1" class="ltx_text" style="font-size:90%;">
Aditya Ramesh, Mikhail Pavlov, Gabriel Soh, Scott Gray, Chelsea Voss, Alec
Radford, Mark Chen, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.5.1" class="ltx_text" style="font-size:90%;">Zero-shot text-to-image generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib25.8.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.2.2.1" class="ltx_text" style="font-size:90%;">[26]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.4.1" class="ltx_text" style="font-size:90%;">
Ricardo Ribeiro, Gon√ßalo Cruz, Jorge Matos, and Alexandre Bernardino.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.5.1" class="ltx_text" style="font-size:90%;">A data set for airborne maritime surveillance environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Circuits and Systems for Video Technology</span><span id="bib.bib26.7.2" class="ltx_text" style="font-size:90%;">,
2019.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.2.2.1" class="ltx_text" style="font-size:90%;">[27]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.4.1" class="ltx_text" style="font-size:90%;">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn
Ommer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.5.1" class="ltx_text" style="font-size:90%;">High-resolution image synthesis with latent diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib27.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.2.2.1" class="ltx_text" style="font-size:90%;">[28]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.4.1" class="ltx_text" style="font-size:90%;">
Zhenfeng Shao, Wenjing Wu, Zhongyuan Wang, Wan Du, and Chengyuan Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.5.1" class="ltx_text" style="font-size:90%;">Seaships: A large-scale precisely annotated dataset for ship
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Multimedia</span><span id="bib.bib28.7.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.2.2.1" class="ltx_text" style="font-size:90%;">[29]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.4.1" class="ltx_text" style="font-size:90%;">
Joonghyuk Shin, Minguk Kang, and Jaesik Park.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.5.1" class="ltx_text" style="font-size:90%;">Fill-up: Balancing long-tailed data with generative models, 2023.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.2.2.1" class="ltx_text" style="font-size:90%;">[30]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.4.1" class="ltx_text" style="font-size:90%;">
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.5.1" class="ltx_text" style="font-size:90%;">Deep unsupervised learning using nonequilibrium thermodynamics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib30.8.3" class="ltx_text" style="font-size:90%;">. PMLR, 2015.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.2.2.1" class="ltx_text" style="font-size:90%;">[31]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.4.1" class="ltx_text" style="font-size:90%;">
Michael Tipton, Elizabeth McCormack, Graham Elliott, Monica Cisternelli, Arthur
Allen, and Arden¬†C. Turner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.5.1" class="ltx_text" style="font-size:90%;">Survival time and search time in water: Past, present and future.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Journal of Thermal Biology</span><span id="bib.bib31.7.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.2.2.1" class="ltx_text" style="font-size:90%;">[32]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.4.1" class="ltx_text" style="font-size:90%;">
Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.5.1" class="ltx_text" style="font-size:90%;">Effective data augmentation with diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.6.1" class="ltx_text" style="font-size:90%;">ArXiv, 2023.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.2.2.1" class="ltx_text" style="font-size:90%;">[33]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.4.1" class="ltx_text" style="font-size:90%;">
Muhammad Umair, Manzoor¬†Ahmed Hashmani, Syed¬†Sajjad Hussain¬†Rizvi, Hasmi Taib,
Mohd¬†Nasir Abdullah, and Mehak¬†Maqbool Memon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.5.1" class="ltx_text" style="font-size:90%;">A novel deep learning model for sea state classification using
visual-range sea images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Symmetry</span><span id="bib.bib33.7.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.2.2.1" class="ltx_text" style="font-size:90%;">[34]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.4.1" class="ltx_text" style="font-size:90%;">
Leon¬†Amadeus Varga, Benjamin Kiefer, Martin Messmer, and Andreas Zell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.5.1" class="ltx_text" style="font-size:90%;">Seadronessee: A maritime benchmark for detecting humans in open
water.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">WACV</span><span id="bib.bib34.8.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.2.2.1" class="ltx_text" style="font-size:90%;">[35]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.4.1" class="ltx_text" style="font-size:90%;">
Mabel¬†M. Zhang, Jean Choi, Kostas Daniilidis, Michael¬†T. Wolf, and Christopher
Kanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.5.1" class="ltx_text" style="font-size:90%;">Vais: A dataset for recognizing maritime imagery in the visible and
infrared spectrums.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.6.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPRW</span><span id="bib.bib35.8.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2311.14763" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2311.14764" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2311.14764">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2311.14764" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2311.14765" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 17:29:01 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
