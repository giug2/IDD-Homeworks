<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2303.17590] Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data</title><meta property="og:description" content="Large-scale pre-trained Vision &amp; Language (VL) models have shown remarkable performance in many applications, enabling replacing a fixed set of supported classes with zero-shot open vocabulary reasoning over (almos…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2303.17590">

<!--Generated on Thu Feb 29 17:45:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id5.5.5" class="ltx_text ltx_font_bold">Paola Cascante-Bonilla    <sup id="id5.5.5.2" class="ltx_sup"><span id="id5.5.5.2.1" class="ltx_text ltx_font_medium ltx_font_italic">1,2</span></sup>
 Khaled Shehada<sup id="id2.2.2.1" class="ltx_sup"><math id="id2.2.2.1.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="id2.2.2.1.m1.1a"><mo id="id2.2.2.1.m1.1.1" xref="id2.2.2.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="id2.2.2.1.m1.1b"><times id="id2.2.2.1.m1.1.1.cmml" xref="id2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id2.2.2.1.m1.1c">*</annotation></semantics></math></sup><sup id="id5.5.5.3" class="ltx_sup"><span id="id5.5.5.3.1" class="ltx_text ltx_font_medium ltx_font_italic">2,3</span></sup>
 James Seale Smith<sup id="id5.5.5.4" class="ltx_sup"><span id="id5.5.5.4.1" class="ltx_text ltx_font_medium ltx_font_italic">2,4</span></sup>
 Sivan Doveh<sup id="id5.5.5.5" class="ltx_sup"><span id="id5.5.5.5.1" class="ltx_text ltx_font_medium ltx_font_italic">6,7</span></sup></span> 
<br class="ltx_break"><span id="id9.9.9" class="ltx_text ltx_font_bold">Donghyun Kim<sup id="id9.9.9.1" class="ltx_sup"><span id="id9.9.9.1.1" class="ltx_text ltx_font_medium ltx_font_italic">2,7</span></sup>
 Rameswar Panda<sup id="id9.9.9.2" class="ltx_sup"><span id="id9.9.9.2.1" class="ltx_text ltx_font_medium ltx_font_italic">2,7</span></sup>
 Gül Varol<sup id="id9.9.9.3" class="ltx_sup"><span id="id9.9.9.3.1" class="ltx_text ltx_font_medium ltx_font_italic">5</span></sup>
 Aude Oliva<sup id="id9.9.9.4" class="ltx_sup"><span id="id9.9.9.4.1" class="ltx_text ltx_font_medium ltx_font_italic">2,3</span></sup></span> 
<br class="ltx_break"><span id="id12.12.12" class="ltx_text ltx_font_bold">Vicente Ordonez<sup id="id12.12.12.1" class="ltx_sup"><span id="id12.12.12.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup>
 Rogerio Feris<sup id="id12.12.12.2" class="ltx_sup"><span id="id12.12.12.2.1" class="ltx_text ltx_font_medium ltx_font_italic">2,7</span></sup>
 Leonid Karlinsky<sup id="id12.12.12.3" class="ltx_sup"><span id="id12.12.12.3.1" class="ltx_text ltx_font_medium ltx_font_italic">2,7</span></sup></span> 
<br class="ltx_break"><sup id="id21.20.id1" class="ltx_sup"><span id="id21.20.id1.1" class="ltx_text ltx_font_italic">1</span></sup>Rice University <sup id="id22.21.id2" class="ltx_sup"><span id="id22.21.id2.1" class="ltx_text ltx_font_italic">2</span></sup>MIT-IBM Watson AI Lab <sup id="id23.22.id3" class="ltx_sup"><span id="id23.22.id3.1" class="ltx_text ltx_font_italic">3</span></sup>MIT
 <sup id="id24.23.id4" class="ltx_sup"><span id="id24.23.id4.1" class="ltx_text ltx_font_italic">4</span></sup>Georgia Institute of Technology
<br class="ltx_break"><sup id="id25.24.id5" class="ltx_sup"><span id="id25.24.id5.1" class="ltx_text ltx_font_italic">5</span></sup>LIGM, École des Ponts <sup id="id26.25.id6" class="ltx_sup"><span id="id26.25.id6.1" class="ltx_text ltx_font_italic">6</span></sup> Weizmann Institute of Science  <sup id="id27.26.id7" class="ltx_sup"><span id="id27.26.id7.1" class="ltx_text ltx_font_italic">7</span></sup>IBM Research
<br class="ltx_break">
</span><span class="ltx_author_notes"><span id="id28.27.id1" class="ltx_text ltx_font_bold">Equal contribution. Project page: </span><a target="_blank" href="https://synthetic-vic.github.io/" title="" class="ltx_ref ltx_href ltx_font_bold">https://synthetic-vic.github.io/</a><span id="id29.28.id1" class="ltx_text ltx_font_bold">Work partially done while interning at the MIT-IBM Watson AI Lab.</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id20.1" class="ltx_p">Large-scale pre-trained Vision &amp; Language (VL) models have shown remarkable performance in many applications, enabling replacing a fixed set of supported classes with zero-shot open vocabulary reasoning over (almost arbitrary) natural language prompts. However, recent works have uncovered a fundamental weakness of these models. For example, their difficulty to understand Visual Language Concepts (VLC) that go ‘beyond nouns’ such as the meaning of non-object words (e.g., attributes, actions, relations, states, etc.), or difficulty in performing compositional reasoning such as understanding the significance of the order of the words in a sentence. In this work, we investigate to which extent purely synthetic data could be leveraged to teach these models to overcome such shortcomings without compromising their zero-shot capabilities.
We contribute <span id="id20.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Sy</span>nthetic <span id="id20.1.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">Vi</span>sual <span id="id20.1.3" class="ltx_text ltx_font_bold" style="color:#0000FF;">C</span>oncepts (<span id="id20.1.4" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span>) - a million-scale synthetic dataset and data generation codebase allowing to generate additional suitable data to improve VLC understanding and compositional reasoning of VL models. Additionally, we propose a general VL finetuning strategy for effectively leveraging <span id="id20.1.5" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> towards achieving these improvements.
Our extensive experiments and ablations on VL-Checklist, Winoground, and ARO benchmarks demonstrate that it is possible to adapt strong pre-trained VL models with synthetic data significantly enhancing their VLC understanding (e.g. by 9.9% on ARO and 4.3% on VL-Checklist) with under <math id="id20.1.m1.1" class="ltx_Math" alttext="1\%" display="inline"><semantics id="id20.1.m1.1a"><mrow id="id20.1.m1.1.1" xref="id20.1.m1.1.1.cmml"><mn id="id20.1.m1.1.1.2" xref="id20.1.m1.1.1.2.cmml">1</mn><mo id="id20.1.m1.1.1.1" xref="id20.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id20.1.m1.1b"><apply id="id20.1.m1.1.1.cmml" xref="id20.1.m1.1.1"><csymbol cd="latexml" id="id20.1.m1.1.1.1.cmml" xref="id20.1.m1.1.1.1">percent</csymbol><cn type="integer" id="id20.1.m1.1.1.2.cmml" xref="id20.1.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id20.1.m1.1c">1\%</annotation></semantics></math> drop in their zero-shot accuracy.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2303.17590/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="389" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of our proposed synthetic set: we place different objects in a scene and change their position, color, size, and material. We further emphasize on human-level interactions, sampling a wide set of body poses and behaviors to cover transitive and intransitive human actions. </figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">There have been impressive advances in the performance of zero-shot recognition through the use of large-scale pre-trained Vision &amp; Language (VL) models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. However, these VL models still face some important challenges in understanding Visual Language Concepts (VLC) beyond object nouns (e.g., recognizing attributes, relations, states) and in terms of compositional reasoning capabilities (i.e.., understanding subtle changes in meaning due to small changes in word order).
Recently, several benchmark tests have been devised to demonstrate the extent to which these models lack these capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Please also see supplementary material for the expanded set of results of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> including results for all the most recent open-sourced VL models, all exhibiting poor VLC understanding performance.</span></span></span>.
As noted in several recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, this behavior of VL models is likely due to the contrastive pre-training prevalent for all of them and likely inducing ‘bag-of-objects’ kind of representations (for both images and text alike). Indeed, for (even large) random batches of paired image-text samples, the collection of objects (nouns) in the image (or text) is likely to uniquely determine the image (or text) in the batch, making contrastive batch losses focus on the objects (nouns) while regarding other details (attributes, relations, states, word order, etc.) as unnecessary. Intuitively, this impairs VLC understanding and compositional reasoning of the resulting model.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Given the above, a natural question to ask is what is the most effective way to ‘fix’ the VL models to improve their VLC understanding and compositional reasoning performance? An approach proposed in concurrent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, advocates for the use of text augmentation, using language tools to teach a model the importance of non-noun words by manipulating them (e.g., replacing them with incorrect alternatives) and adding the resulting texts to the same batch. Although effective, such augmentation techniques are only easy on the text side and are much harder and prohibitively expensive on the image side. Indeed, finding, collecting, or generating real image samples sharing the objects but differing in their composition, attributes, relations, or states is very difficult. Although significant progress has been achieved with text-based editing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, these methods are relatively slow (leveraging diffusion) and not sufficiently stable to allow effective use for augmentation in training pipelines. In this work, therefore, we propose an orthogonal route – VL
data synthesis for fixing VL models by targeted demonstration. Specifically, we propose enhancing the VLC and compositionality aspects of the generated <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">visual and text</span> data, in turn using this data for finetuning VL models teaching them to pay closer attention to these aspects. Moreover, besides being largely free and infinitely scalable, synthetic data has an additional advantage – it can also be free from privacy concerns always accompanying real data.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Besides the inherent challenges of realistic data simulation, building synthetic data that can be effectively used to improve VLC and compositionality aspects of VL models pre-trained on massive real data poses additional technical challenges.
Unlike the majority of prior work focusing on synthetic visual data generation, we need not only to generate images, but also the text that describes compositional items in a scene.
We generate synthetic videos that leverage realistic physical 3D simulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> including diverse 3D environments and different 3D objects, human motions, and actions assets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, added interaction with objects, and different camera viewpoints.
Every frame of these videos is accompanied by rich metadata, allowing using language grammar for generating detailed descriptive captions of any instantaneous scene in each video. These captions, in turn, allow collecting diverse image-text pairs samples contrasting which one to another highlights to the model the importance of the compositional items in the text captions (e.g. different viewpoints or different frames in the same video share objects but may strongly differ in the VLC and other compositional items).
While motion assets were used by previous works to generate synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>, the visual data was not accompanied by textual captions and was not designed with the need to highlight compositionality in mind.
We contribute <span id="S1.p3.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Sy</span>nthetic <span id="S1.p3.1.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">Vi</span>sual <span id="S1.p3.1.3" class="ltx_text ltx_font_bold" style="color:#0000FF;">C</span>oncepts (<span id="S1.p3.1.4" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span>) – a large (million-scale) generated synthetic VL dataset with rich textual captions, easily extensible through our data synthesis code
together with all the already generated million-scale synthetic data used in this paper (Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In addition to the data synthesis pipeline, we also offer a strategy for effectively leveraging the generated synthetic data, while avoiding forgetting real data alignment and losing the strong a-priori zero-shot capabilities of the model. We propose and extensively ablate a combination of domain adaptation by stylization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>, parameter efficient fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, long captions handling, and model averaging methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> to reduce forgetting, as well as examine the effect of different aspects of data synthesis and finetuning choices on the gains in VLC and compositionality understanding.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our contributions can be summarized as follows: (i) we contribute <span id="S1.p5.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> – a million-scale synthetic dataset with rich textual captions, intended for improving VLC understanding and compositional reasoning in VL models, as well as the methodology and the generation codebase
<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We release our code together with all million-scale synthetic data used in this paper here: <a target="_blank" href="https://github.com/uvavision/SyViC" title="" class="ltx_ref ltx_href">https://github.com/uvavision/SyViC</a></span></span></span> 
for its synthesis and potentially extensibility; (ii) an effective general VL model finetuning strategy
enabling effective leveraging of <span id="S1.p5.1.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> data for enhancing the aforementioned aspects of strong pre-trained VL models without sacrificing their zero-shot capabilities; (iii) experimental results and extensive ablation study showing significant (over 10% in some cases) improvement in VLC understanding and compositional reasoning respectively, measured on all the recent VL-Checklist, ARO, and Winoground benchmarks and validated on the most popular CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> model and its derivatives (e.g. the most recent CyCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>).</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">For supplemental materials, readers are referred to the associated arXiv document at [arXiv:2303.17590].</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Large-scale Vision&amp;Language (VL) Models:</span>
Large-scale pre-trained VL models such as CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> or ALIGN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> show remarkable success in many zero-shot recognition tasks such as image classification or detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>. Despite the continued advancements made in this direction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, recent studies (<em id="S2.p1.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p1.1.3" class="ltx_text"></span>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>) show that existing VL models exhibit limited comprehension of structured vision language concepts (VLC). Yuksekgonul <em id="S2.p1.1.4" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.5" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> argue that contrastive learning for image-retrieval learns shortcuts and does not learn compositional information. To address this limitation, some approaches investigate how to augment the text captions or images in contrastive learning to enhance the ability of VLC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. Smith <em id="S2.p1.1.6" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p1.1.7" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> learn VLC concepts with additional supervised datasets in a continual learning setup. In contrast, we use 3D graphic engines to generate realistic synthetic videos with different compositions and generate corresponding text captions, which allows a VL model to learn compositionality and non-object words such as attributes, actions, relations, etc.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Learning from Synthetic Data.</span> There has been a lot of work on learning from synthetic data in image classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, human pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, action recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>, etc. Synthetic data is easy to generate and particularly useful for providing dense annotation such as semantic segmentation and depth estimation
since these are prohibitively expensive to annotate manually.
Some of the work relies on graphics engines to generate realistic data. Mishra <em id="S2.p2.1.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.p2.1.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> propose a method to learn how to generate task-adaptive synthetic data with the 3D simulation engine. For human-related problems, parametric body models (<em id="S2.p2.1.4" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p2.1.5" class="ltx_text"></span>, SMPL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>) can be leveraged, along with motion assets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>, to generate synthetic human videos for low-level body analysis tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> or action recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>. Similar to our work, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> seek to associate semantic labels to synthetic images, but different from symbolic action categories, our focus is to assign rich textual descriptions to our generated images.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Since synthetic data suffers from a domain gap such as textures, visual styles, or colors from real images, domain adaptation, and generalization have been proposed to address this issue. Adversarial learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> can be used to generate real-like images or feature alignment between synthetic and real data. Additionally, stylization methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> are proposed as a style augmentation to make a model robust to diverse styles. In contrast, we manually randomize the visual content including different 3D objects, materials, and color attributes in graphics engines. Then we generate realistic synthetic videos from different domains with corresponding text captions. The generated data can be served as a hard negative augmentation and enhance the ability of VLC.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We first present our synthetic data generation pipeline (Sec. <a href="#S3.SS1" title="3.1 Synthetic Data Generation ‣ 3 Method ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>), then describe how we leverage it for significant gains in VLC understanding and compositional reasoning capabilities of strong pre-trained VL models (Sec. <a href="#S3.SS2" title="3.2 Finetuning large-scale pre-trained VL models using synthetic data ‣ 3 Method ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>). Our entire approach is illustrated in detail in Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.1 Synthetic Data Generation ‣ 3 Method ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Synthetic Data Generation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In this section, we outline the components and the pipeline of our approach used to generate the proposed <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Sy</span>nthetic <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">Vi</span>sual <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_bold" style="color:#0000FF;">C</span>oncepts (<span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span>) synthetic VL dataset for improving VLC understanding and compositional reasoning of VL models. Our contributed dataset includes 767,738 image-text pairs, 598K sampled from 1,680 diverse synthetic videos, and the remaining 169K generated as individual static synthetic scenes. Example samples from <span id="S3.SS1.p1.1.5" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> are provided in Supplementary.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.3" class="ltx_p"><span id="S3.SS1.p2.3.1" class="ltx_text ltx_font_bold">3D physics-based simulation platform:</span> ThreeDWorld (TDW) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, which is built on top of Unity3D, is a multi-modal simulation platform that enables realistic physical interactions.
TDW contains <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="2304" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mn id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">2304</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><cn type="integer" id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">2304</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">2304</annotation></semantics></math> objects, 585 unique materials subdivided in metal, cardboard, wood, ceramic, and glass, and over <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mn id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><cn type="integer" id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">30</annotation></semantics></math> indoor and <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="9" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mn id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><cn type="integer" id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">9</annotation></semantics></math> outdoor scenes (3D environments).
For generating synthetic VL data, we start with placing random objects in a scene following the workflow proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. We also use their camera positions and configurations to place objects visible inside good empty room perspectives. We group the available 3D object models by assigning dimension-related labels to each object and use the ImageNet category labels available for each object model as its associated text for later caption synthesis.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.3" class="ltx_p"><span id="S3.SS1.p3.3.1" class="ltx_text ltx_font_bold">Camera Viewpoints</span>: To further augment the set of plausible object placements and relations,
we simultaneously place <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mn id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><cn type="integer" id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">4</annotation></semantics></math> to <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mn id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><cn type="integer" id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">12</annotation></semantics></math> cameras around a specific point of an empty room, and randomly place <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="n\geq 1" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mrow id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml">n</mi><mo id="S3.SS1.p3.3.m3.1.1.1" xref="S3.SS1.p3.3.m3.1.1.1.cmml">≥</mo><mn id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><geq id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1.1"></geq><ci id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">𝑛</ci><cn type="integer" id="S3.SS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">n\geq 1</annotation></semantics></math> objects in the scene, allowing us to render images from different views of the same scene further strengthening the compositional aspects of the data as discussed in the introduction (Sec. <a href="#S1" title="1 Introduction ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). For each scene (frame), TDW cameras are able to capture RGB images, the corresponding object instance and category semantic segmentation masks, and a depth map. We use these, as well as a range of sensor and physics data representing the state of the world returned by TDW’s API, to enable dense annotations and supervision for each scene (frame) as part of our metadata generation process. We collect all of this information in our metadata and use it to estimate the position of the objects in the scene instead of relying on the 3D coordinates of each object and the camera position.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">Digital humans</span>: As we focus on compositionality aspects of images and text pairs, having people in our images is important. However, people models (especially animatable ones) are usually not present in common collections of 3D assets. Existing large-scale synthetic datasets often focus on realistically placing objects in a scene, but typically humans and animals are not included.
We first inspected what libraries were available for realistic human synthesis. PeopleSansPeople <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, a library with 28 human 3D models and 39 unique actions, allows only random human placement, not allowing for humanoid customization or integration of human-object interactions.
We leverage TDW support for Skinned Multi-Person Linear Model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> (SMPL) humanoids. SMPL is a parametric body model that enables a realistic representation of the shape and pose of arbitrary (non-clothed) 3D human bodies with diverse genders and shapes. SMPL models can be easily animated using motion capture data. The pose of the SMPL model is defined by a set of joint angles that determine the position of the corresponding body parts in the 3D space. The extended SMPL-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> additionally allows controlling hand articulation and face expressions. Given the available library asset in TDW that enables placing these SMPLs in a scene, we create a stand-alone module to automatically incorporate arbitrary custom animations and <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="514" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mn id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">514</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><cn type="integer" id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">514</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">514</annotation></semantics></math> unique human textures from the SURREAL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> and Multi-Garment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> datasets for clothing the synthetic human models for further enhancing the diversity and compositional features of our data.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2303.17590/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="248" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Summarizing the entire flow of the proposed approach including components and choices of <span id="S3.F2.4.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> data synthesis pipeline <span id="S3.F2.5.2" class="ltx_text ltx_font_bold">(left)</span> and proposed effective finetuning technique <span id="S3.F2.6.3" class="ltx_text ltx_font_bold">(right)</span> attaining significant gains detailed below. </figcaption>
</figure>
<div id="S3.SS1.p5" class="ltx_para ltx_noindent">
<p id="S3.SS1.p5.1" class="ltx_p"><span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_bold">Human motion synthesis and handling interactions:</span> In Unity, SMPLs have skeletons and can be driven by motion-capture animations but they don’t have mass or colliders, this means that they are not physics assets since they can walk through other objects without interacting with them.
To solve this issue, we add colliders to each body part of the SMPL model and create three asset templates (i.e., male, female, neutral) that contain all mesh configurations. Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Synthetic Data Generation ‣ 3 Method ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows some examples of our rigged asset templates with colliders.
All other existing 3D models in TDW have colliders and track collisions at runtime through TDW physics-based simulation. Therefore, our collider-enhanced SMPLs are pulled downward by gravity, as well as simulate interaction by reacting naturally to collisions with other objects in the scene during motion simulation.
For human actions, we first synthesize a diverse set of human actions from random language descriptions using TEACH <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, a Transformer-based model that generates a continuous sequence of SMPL motions from a sequence of action text labels. TEACH was trained on AMASS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, a large-scale motion-capture (mocap) collection, and BABEL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, a dataset that provides textual descriptions for AMASS, including per-frame unique actions annotations. Second, we extend our set of SMPL motions by directly sampling unique human motions from BABEL and AMASS. We export the corresponding mocaps to FBX files, and extract the animations in Unity, enabling them as asset bundles for use with TDW. FBX is a common format that facilitates data exchange between different 3D simulation platforms.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para ltx_noindent">
<p id="S3.SS1.p6.8" class="ltx_p"><span id="S3.SS1.p6.8.1" class="ltx_text ltx_font_bold">Domain randomization:</span> One of the key qualities of the generated synthetic data, is its ability to highlight the importance of VLCs and compositional properties of the scene (e.g., objects attributes, relations, and more) in the contrastive learning objectives guiding the VL model finetuning. As opposed to methods based on text augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> that can only enhance those on the text part of the VL image-text pairs, in <span id="S3.SS1.p6.8.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> construction we can easily manipulate also the visual content. We randomly place <math id="S3.SS1.p6.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS1.p6.1.m1.1a"><mn id="S3.SS1.p6.1.m1.1.1" xref="S3.SS1.p6.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.1b"><cn type="integer" id="S3.SS1.p6.1.m1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.1c">1</annotation></semantics></math> to <math id="S3.SS1.p6.2.m2.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S3.SS1.p6.2.m2.1a"><mn id="S3.SS1.p6.2.m2.1.1" xref="S3.SS1.p6.2.m2.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.2.m2.1b"><cn type="integer" id="S3.SS1.p6.2.m2.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.2.m2.1c">8</annotation></semantics></math> 3D object models in the scene, randomizing their material and color attributes (<math id="S3.SS1.p6.3.m3.1" class="ltx_Math" alttext="2304\times 585\times 551" display="inline"><semantics id="S3.SS1.p6.3.m3.1a"><mrow id="S3.SS1.p6.3.m3.1.1" xref="S3.SS1.p6.3.m3.1.1.cmml"><mn id="S3.SS1.p6.3.m3.1.1.2" xref="S3.SS1.p6.3.m3.1.1.2.cmml">2304</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p6.3.m3.1.1.1" xref="S3.SS1.p6.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS1.p6.3.m3.1.1.3" xref="S3.SS1.p6.3.m3.1.1.3.cmml">585</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p6.3.m3.1.1.1a" xref="S3.SS1.p6.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS1.p6.3.m3.1.1.4" xref="S3.SS1.p6.3.m3.1.1.4.cmml">551</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.3.m3.1b"><apply id="S3.SS1.p6.3.m3.1.1.cmml" xref="S3.SS1.p6.3.m3.1.1"><times id="S3.SS1.p6.3.m3.1.1.1.cmml" xref="S3.SS1.p6.3.m3.1.1.1"></times><cn type="integer" id="S3.SS1.p6.3.m3.1.1.2.cmml" xref="S3.SS1.p6.3.m3.1.1.2">2304</cn><cn type="integer" id="S3.SS1.p6.3.m3.1.1.3.cmml" xref="S3.SS1.p6.3.m3.1.1.3">585</cn><cn type="integer" id="S3.SS1.p6.3.m3.1.1.4.cmml" xref="S3.SS1.p6.3.m3.1.1.4">551</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.3.m3.1c">2304\times 585\times 551</annotation></semantics></math> choices for each placed object). We randomly place <math id="S3.SS1.p6.4.m4.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S3.SS1.p6.4.m4.1a"><mn id="S3.SS1.p6.4.m4.1.1" xref="S3.SS1.p6.4.m4.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.4.m4.1b"><cn type="integer" id="S3.SS1.p6.4.m4.1.1.cmml" xref="S3.SS1.p6.4.m4.1.1">0</cn></annotation-xml></semantics></math> to <math id="S3.SS1.p6.5.m5.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS1.p6.5.m5.1a"><mn id="S3.SS1.p6.5.m5.1.1" xref="S3.SS1.p6.5.m5.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.5.m5.1b"><cn type="integer" id="S3.SS1.p6.5.m5.1.1.cmml" xref="S3.SS1.p6.5.m5.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.5.m5.1c">4</annotation></semantics></math> human avatars in the scene, randomizing their gender and clothing. We set camera poses as explained above, keeping scene ID shared for all the cameras of the same scene, and randomly sample <math id="S3.SS1.p6.6.m6.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS1.p6.6.m6.1a"><mn id="S3.SS1.p6.6.m6.1.1" xref="S3.SS1.p6.6.m6.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.6.m6.1b"><cn type="integer" id="S3.SS1.p6.6.m6.1.1.cmml" xref="S3.SS1.p6.6.m6.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.6.m6.1c">4</annotation></semantics></math> viewpoints of each scene. We use human motion assets (as explained above) and randomly sample a motion sequence for each human avatar out of <math id="S3.SS1.p6.7.m7.1" class="ltx_Math" alttext="1898" display="inline"><semantics id="S3.SS1.p6.7.m7.1a"><mn id="S3.SS1.p6.7.m7.1.1" xref="S3.SS1.p6.7.m7.1.1.cmml">1898</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.7.m7.1b"><cn type="integer" id="S3.SS1.p6.7.m7.1.1.cmml" xref="S3.SS1.p6.7.m7.1.1">1898</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.7.m7.1c">1898</annotation></semantics></math> imported or generated mocaps. Finally, we sample an average of <math id="S3.SS1.p6.8.m8.1" class="ltx_Math" alttext="1500" display="inline"><semantics id="S3.SS1.p6.8.m8.1a"><mn id="S3.SS1.p6.8.m8.1.1" xref="S3.SS1.p6.8.m8.1.1.cmml">1500</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.8.m8.1b"><cn type="integer" id="S3.SS1.p6.8.m8.1.1.cmml" xref="S3.SS1.p6.8.m8.1.1">1500</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.8.m8.1c">1500</annotation></semantics></math> frames from each resulting synthetic video sequence generating image-text pairs describing a scene with the same objects and attributes, but in different arrangements and different corresponding captions thus enhancing the importance of the compositional aspects of the scene in the contrastive loss. We explore the importance of different simulation aspects in our ablation studies in Sec. <a href="#S4.SS4" title="4.4 Ablations ‣ 4 Experiments ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para ltx_noindent">
<p id="S3.SS1.p7.1" class="ltx_p"><span id="S3.SS1.p7.1.1" class="ltx_text ltx_font_bold">Metadata-driven caption text synthesis:</span>
In addition to RGB frames, we obtain a large collection of rich metadata from the simulation platform, containing information on objects, humanoids, and the scene setting. For each frame, the metadata includes: (i) The world coordinates of each object and humanoid in the scene, including the camera position and viewing direction. (ii) The physical attributes of each object and humanoid in the scene (object physical attributes include color, size, and material; human attributes include the per-frame action label that changes over time and clothing description). (iii) Rendered depth images, instance segmentation masks, and category segmentation masks.
Using the metadata, we compute the positional relations between each pair of objects and/or humans by comparing the pixels covered by their segmentation masks as well as their camera coordinates. Then, we use a simple grammar that deterministically maps positional relationships, object attributes, human attributes and action descriptions, and scene descriptions to a well-formed caption. More details on the grammar are provided in Supplementary.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Finetuning large-scale pre-trained VL models using synthetic data</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.3" class="ltx_p">In this section, we propose a methodology for effectively leveraging the <span id="S3.SS2.p1.3.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> synthetic VL data produced as explained in Sec. <a href="#S3.SS1" title="3.1 Synthetic Data Generation ‣ 3 Method ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. We will use the following notation. Let <math id="S3.SS2.p1.1.m1.2" class="ltx_Math" alttext="(T,I)" display="inline"><semantics id="S3.SS2.p1.1.m1.2a"><mrow id="S3.SS2.p1.1.m1.2.3.2" xref="S3.SS2.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.p1.1.m1.2.3.2.1" xref="S3.SS2.p1.1.m1.2.3.1.cmml">(</mo><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">T</mi><mo id="S3.SS2.p1.1.m1.2.3.2.2" xref="S3.SS2.p1.1.m1.2.3.1.cmml">,</mo><mi id="S3.SS2.p1.1.m1.2.2" xref="S3.SS2.p1.1.m1.2.2.cmml">I</mi><mo stretchy="false" id="S3.SS2.p1.1.m1.2.3.2.3" xref="S3.SS2.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.2b"><interval closure="open" id="S3.SS2.p1.1.m1.2.3.1.cmml" xref="S3.SS2.p1.1.m1.2.3.2"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑇</ci><ci id="S3.SS2.p1.1.m1.2.2.cmml" xref="S3.SS2.p1.1.m1.2.2">𝐼</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.2c">(T,I)</annotation></semantics></math> be the text &amp; image pair admitted by a VL model. The model (e.g., CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, CyCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>)
components are denoted as: (i) image encoder <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="e_{I}=\mathcal{E}_{I}(I)" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.2" xref="S3.SS2.p1.2.m2.1.2.cmml"><msub id="S3.SS2.p1.2.m2.1.2.2" xref="S3.SS2.p1.2.m2.1.2.2.cmml"><mi id="S3.SS2.p1.2.m2.1.2.2.2" xref="S3.SS2.p1.2.m2.1.2.2.2.cmml">e</mi><mi id="S3.SS2.p1.2.m2.1.2.2.3" xref="S3.SS2.p1.2.m2.1.2.2.3.cmml">I</mi></msub><mo id="S3.SS2.p1.2.m2.1.2.1" xref="S3.SS2.p1.2.m2.1.2.1.cmml">=</mo><mrow id="S3.SS2.p1.2.m2.1.2.3" xref="S3.SS2.p1.2.m2.1.2.3.cmml"><msub id="S3.SS2.p1.2.m2.1.2.3.2" xref="S3.SS2.p1.2.m2.1.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.2.m2.1.2.3.2.2" xref="S3.SS2.p1.2.m2.1.2.3.2.2.cmml">ℰ</mi><mi id="S3.SS2.p1.2.m2.1.2.3.2.3" xref="S3.SS2.p1.2.m2.1.2.3.2.3.cmml">I</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.2.3.1" xref="S3.SS2.p1.2.m2.1.2.3.1.cmml">​</mo><mrow id="S3.SS2.p1.2.m2.1.2.3.3.2" xref="S3.SS2.p1.2.m2.1.2.3.cmml"><mo stretchy="false" id="S3.SS2.p1.2.m2.1.2.3.3.2.1" xref="S3.SS2.p1.2.m2.1.2.3.cmml">(</mo><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">I</mi><mo stretchy="false" id="S3.SS2.p1.2.m2.1.2.3.3.2.2" xref="S3.SS2.p1.2.m2.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.2.cmml" xref="S3.SS2.p1.2.m2.1.2"><eq id="S3.SS2.p1.2.m2.1.2.1.cmml" xref="S3.SS2.p1.2.m2.1.2.1"></eq><apply id="S3.SS2.p1.2.m2.1.2.2.cmml" xref="S3.SS2.p1.2.m2.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.2.2.1.cmml" xref="S3.SS2.p1.2.m2.1.2.2">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.2.2.2.cmml" xref="S3.SS2.p1.2.m2.1.2.2.2">𝑒</ci><ci id="S3.SS2.p1.2.m2.1.2.2.3.cmml" xref="S3.SS2.p1.2.m2.1.2.2.3">𝐼</ci></apply><apply id="S3.SS2.p1.2.m2.1.2.3.cmml" xref="S3.SS2.p1.2.m2.1.2.3"><times id="S3.SS2.p1.2.m2.1.2.3.1.cmml" xref="S3.SS2.p1.2.m2.1.2.3.1"></times><apply id="S3.SS2.p1.2.m2.1.2.3.2.cmml" xref="S3.SS2.p1.2.m2.1.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.2.3.2.1.cmml" xref="S3.SS2.p1.2.m2.1.2.3.2">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.2.3.2.2.cmml" xref="S3.SS2.p1.2.m2.1.2.3.2.2">ℰ</ci><ci id="S3.SS2.p1.2.m2.1.2.3.2.3.cmml" xref="S3.SS2.p1.2.m2.1.2.3.2.3">𝐼</ci></apply><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝐼</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">e_{I}=\mathcal{E}_{I}(I)</annotation></semantics></math>; (ii) text encoder <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="e_{T}=\mathcal{E}_{T}(T)" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.2" xref="S3.SS2.p1.3.m3.1.2.cmml"><msub id="S3.SS2.p1.3.m3.1.2.2" xref="S3.SS2.p1.3.m3.1.2.2.cmml"><mi id="S3.SS2.p1.3.m3.1.2.2.2" xref="S3.SS2.p1.3.m3.1.2.2.2.cmml">e</mi><mi id="S3.SS2.p1.3.m3.1.2.2.3" xref="S3.SS2.p1.3.m3.1.2.2.3.cmml">T</mi></msub><mo id="S3.SS2.p1.3.m3.1.2.1" xref="S3.SS2.p1.3.m3.1.2.1.cmml">=</mo><mrow id="S3.SS2.p1.3.m3.1.2.3" xref="S3.SS2.p1.3.m3.1.2.3.cmml"><msub id="S3.SS2.p1.3.m3.1.2.3.2" xref="S3.SS2.p1.3.m3.1.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.3.m3.1.2.3.2.2" xref="S3.SS2.p1.3.m3.1.2.3.2.2.cmml">ℰ</mi><mi id="S3.SS2.p1.3.m3.1.2.3.2.3" xref="S3.SS2.p1.3.m3.1.2.3.2.3.cmml">T</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.2.3.1" xref="S3.SS2.p1.3.m3.1.2.3.1.cmml">​</mo><mrow id="S3.SS2.p1.3.m3.1.2.3.3.2" xref="S3.SS2.p1.3.m3.1.2.3.cmml"><mo stretchy="false" id="S3.SS2.p1.3.m3.1.2.3.3.2.1" xref="S3.SS2.p1.3.m3.1.2.3.cmml">(</mo><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">T</mi><mo stretchy="false" id="S3.SS2.p1.3.m3.1.2.3.3.2.2" xref="S3.SS2.p1.3.m3.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.2.cmml" xref="S3.SS2.p1.3.m3.1.2"><eq id="S3.SS2.p1.3.m3.1.2.1.cmml" xref="S3.SS2.p1.3.m3.1.2.1"></eq><apply id="S3.SS2.p1.3.m3.1.2.2.cmml" xref="S3.SS2.p1.3.m3.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.2.2.1.cmml" xref="S3.SS2.p1.3.m3.1.2.2">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.2.2.2.cmml" xref="S3.SS2.p1.3.m3.1.2.2.2">𝑒</ci><ci id="S3.SS2.p1.3.m3.1.2.2.3.cmml" xref="S3.SS2.p1.3.m3.1.2.2.3">𝑇</ci></apply><apply id="S3.SS2.p1.3.m3.1.2.3.cmml" xref="S3.SS2.p1.3.m3.1.2.3"><times id="S3.SS2.p1.3.m3.1.2.3.1.cmml" xref="S3.SS2.p1.3.m3.1.2.3.1"></times><apply id="S3.SS2.p1.3.m3.1.2.3.2.cmml" xref="S3.SS2.p1.3.m3.1.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.2.3.2.1.cmml" xref="S3.SS2.p1.3.m3.1.2.3.2">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.2.3.2.2.cmml" xref="S3.SS2.p1.3.m3.1.2.3.2.2">ℰ</ci><ci id="S3.SS2.p1.3.m3.1.2.3.2.3.cmml" xref="S3.SS2.p1.3.m3.1.2.3.2.3">𝑇</ci></apply><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">e_{T}=\mathcal{E}_{T}(T)</annotation></semantics></math>.
In this notation,
the text-to-image similarity score is computed as:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.4" class="ltx_math_unparsed" alttext="\mathcal{S}(T,I)=cos(\mathcal{E}_{T}(T),\mathcal{E}_{I}(I)))=cos(e_{T},e_{I})," display="block"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4b"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.4.5">𝒮</mi><mrow id="S3.E1.m1.4.6"><mo stretchy="false" id="S3.E1.m1.4.6.1">(</mo><mi id="S3.E1.m1.1.1">T</mi><mo id="S3.E1.m1.4.6.2">,</mo><mi id="S3.E1.m1.2.2">I</mi><mo stretchy="false" id="S3.E1.m1.4.6.3">)</mo></mrow><mo id="S3.E1.m1.4.7">=</mo><mi id="S3.E1.m1.4.8">c</mi><mi id="S3.E1.m1.4.9">o</mi><mi id="S3.E1.m1.4.10">s</mi><mrow id="S3.E1.m1.4.11"><mo stretchy="false" id="S3.E1.m1.4.11.1">(</mo><msub id="S3.E1.m1.4.11.2"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.4.11.2.2">ℰ</mi><mi id="S3.E1.m1.4.11.2.3">T</mi></msub><mrow id="S3.E1.m1.4.11.3"><mo stretchy="false" id="S3.E1.m1.4.11.3.1">(</mo><mi id="S3.E1.m1.3.3">T</mi><mo stretchy="false" id="S3.E1.m1.4.11.3.2">)</mo></mrow><mo id="S3.E1.m1.4.11.4">,</mo><msub id="S3.E1.m1.4.11.5"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.4.11.5.2">ℰ</mi><mi id="S3.E1.m1.4.11.5.3">I</mi></msub><mrow id="S3.E1.m1.4.11.6"><mo stretchy="false" id="S3.E1.m1.4.11.6.1">(</mo><mi id="S3.E1.m1.4.4">I</mi><mo stretchy="false" id="S3.E1.m1.4.11.6.2">)</mo></mrow><mo stretchy="false" id="S3.E1.m1.4.11.7">)</mo></mrow><mo stretchy="false" id="S3.E1.m1.4.12">)</mo><mo id="S3.E1.m1.4.13">=</mo><mi id="S3.E1.m1.4.14">c</mi><mi id="S3.E1.m1.4.15">o</mi><mi id="S3.E1.m1.4.16">s</mi><mo stretchy="false" id="S3.E1.m1.4.17">(</mo><msub id="S3.E1.m1.4.18"><mi id="S3.E1.m1.4.18.2">e</mi><mi id="S3.E1.m1.4.18.3">T</mi></msub><mo id="S3.E1.m1.4.19">,</mo><msub id="S3.E1.m1.4.20"><mi id="S3.E1.m1.4.20.2">e</mi><mi id="S3.E1.m1.4.20.3">I</mi></msub><mo stretchy="false" id="S3.E1.m1.4.21">)</mo><mo id="S3.E1.m1.4.22">,</mo></mrow><annotation encoding="application/x-tex" id="S3.E1.m1.4c">\mathcal{S}(T,I)=cos(\mathcal{E}_{T}(T),\mathcal{E}_{I}(I)))=cos(e_{T},e_{I}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.4" class="ltx_p">where <math id="S3.SS2.p1.4.m1.1" class="ltx_Math" alttext="cos" display="inline"><semantics id="S3.SS2.p1.4.m1.1a"><mrow id="S3.SS2.p1.4.m1.1.1" xref="S3.SS2.p1.4.m1.1.1.cmml"><mi id="S3.SS2.p1.4.m1.1.1.2" xref="S3.SS2.p1.4.m1.1.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.4.m1.1.1.1" xref="S3.SS2.p1.4.m1.1.1.1.cmml">​</mo><mi id="S3.SS2.p1.4.m1.1.1.3" xref="S3.SS2.p1.4.m1.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.4.m1.1.1.1a" xref="S3.SS2.p1.4.m1.1.1.1.cmml">​</mo><mi id="S3.SS2.p1.4.m1.1.1.4" xref="S3.SS2.p1.4.m1.1.1.4.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m1.1b"><apply id="S3.SS2.p1.4.m1.1.1.cmml" xref="S3.SS2.p1.4.m1.1.1"><times id="S3.SS2.p1.4.m1.1.1.1.cmml" xref="S3.SS2.p1.4.m1.1.1.1"></times><ci id="S3.SS2.p1.4.m1.1.1.2.cmml" xref="S3.SS2.p1.4.m1.1.1.2">𝑐</ci><ci id="S3.SS2.p1.4.m1.1.1.3.cmml" xref="S3.SS2.p1.4.m1.1.1.3">𝑜</ci><ci id="S3.SS2.p1.4.m1.1.1.4.cmml" xref="S3.SS2.p1.4.m1.1.1.4">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m1.1c">cos</annotation></semantics></math> is the cosine similarity (inner product of normalized vectors).
We next describe in detail the components of our finetuning strategy. Their merit and tradeoffs are thoroughly investigated in Sec. <a href="#S4.SS4" title="4.4 Ablations ‣ 4 Experiments ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>, arriving at the conclusion that parameter efficient finetuning + domain adaptive stylization + proposed caption splitting technique are the most effective combination. We also confirm in Sec. <a href="#S4.SS4" title="4.4 Ablations ‣ 4 Experiments ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>, that model averaging can provide expected trade-offs between VLC understanding and compositional reasoning gains and maintaining zero-shot performance.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.5" class="ltx_p"><span id="S3.SS2.p2.5.1" class="ltx_text ltx_font_bold">Avoiding forgetting through parameter efficient fine-tuning:</span>
Inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, we use LoRA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> for VL fine-tuning with reduced forgetting of base model performance.
We apply LoRA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> to adapt the encoders (<math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{E}_{T}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">ℰ</mi><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">ℰ</ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\mathcal{E}_{T}</annotation></semantics></math>, <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{E}_{I}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><msub id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">ℰ</mi><mi id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">ℰ</ci><ci id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\mathcal{E}_{I}</annotation></semantics></math>)
of a pre-trained VL model
by parameterizing the adapted weights <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{W}_{k}^{*}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><msubsup id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.3.m3.1.1.2.2" xref="S3.SS2.p2.3.m3.1.1.2.2.cmml">𝒲</mi><mi id="S3.SS2.p2.3.m3.1.1.2.3" xref="S3.SS2.p2.3.m3.1.1.2.3.cmml">k</mi><mo id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">∗</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">superscript</csymbol><apply id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.2.1.cmml" xref="S3.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.2.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2.2">𝒲</ci><ci id="S3.SS2.p2.3.m3.1.1.2.3.cmml" xref="S3.SS2.p2.3.m3.1.1.2.3">𝑘</ci></apply><times id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\mathcal{W}_{k}^{*}</annotation></semantics></math>
corresponding to the original model weights <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{W}_{k}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><msub id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">𝒲</mi><mi id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">𝒲</ci><ci id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\mathcal{W}_{k}</annotation></semantics></math> for each layer <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mi id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">k</annotation></semantics></math> as:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\mathcal{W}_{k}^{*}=\mathcal{W}_{k}+\mathcal{A}_{k}\cdot\mathcal{B}_{k}" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msubsup id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.2.2.2" xref="S3.E2.m1.1.1.2.2.2.cmml">𝒲</mi><mi id="S3.E2.m1.1.1.2.2.3" xref="S3.E2.m1.1.1.2.2.3.cmml">k</mi><mo id="S3.E2.m1.1.1.2.3" xref="S3.E2.m1.1.1.2.3.cmml">∗</mo></msubsup><mo id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><msub id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.3.2.2" xref="S3.E2.m1.1.1.3.2.2.cmml">𝒲</mi><mi id="S3.E2.m1.1.1.3.2.3" xref="S3.E2.m1.1.1.3.2.3.cmml">k</mi></msub><mo id="S3.E2.m1.1.1.3.1" xref="S3.E2.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml"><msub id="S3.E2.m1.1.1.3.3.2" xref="S3.E2.m1.1.1.3.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.3.3.2.2" xref="S3.E2.m1.1.1.3.3.2.2.cmml">𝒜</mi><mi id="S3.E2.m1.1.1.3.3.2.3" xref="S3.E2.m1.1.1.3.3.2.3.cmml">k</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.1.1.3.3.1" xref="S3.E2.m1.1.1.3.3.1.cmml">⋅</mo><msub id="S3.E2.m1.1.1.3.3.3" xref="S3.E2.m1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.3.3.3.2" xref="S3.E2.m1.1.1.3.3.3.2.cmml">ℬ</mi><mi id="S3.E2.m1.1.1.3.3.3.3" xref="S3.E2.m1.1.1.3.3.3.3.cmml">k</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"></eq><apply id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.2.2.1.cmml" xref="S3.E2.m1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.2.2.2">𝒲</ci><ci id="S3.E2.m1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.2.2.3">𝑘</ci></apply><times id="S3.E2.m1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.2.3"></times></apply><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><plus id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3.1"></plus><apply id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.3.2.2">𝒲</ci><ci id="S3.E2.m1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.3.2.3">𝑘</ci></apply><apply id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3"><ci id="S3.E2.m1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3.1">⋅</ci><apply id="S3.E2.m1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.3.2.1.cmml" xref="S3.E2.m1.1.1.3.3.2">subscript</csymbol><ci id="S3.E2.m1.1.1.3.3.2.2.cmml" xref="S3.E2.m1.1.1.3.3.2.2">𝒜</ci><ci id="S3.E2.m1.1.1.3.3.2.3.cmml" xref="S3.E2.m1.1.1.3.3.2.3">𝑘</ci></apply><apply id="S3.E2.m1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.E2.m1.1.1.3.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.3.2">ℬ</ci><ci id="S3.E2.m1.1.1.3.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3.3">𝑘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\mathcal{W}_{k}^{*}=\mathcal{W}_{k}+\mathcal{A}_{k}\cdot\mathcal{B}_{k}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p2.15" class="ltx_p">where for <math id="S3.SS2.p2.6.m1.1" class="ltx_Math" alttext="\mathcal{W}_{k}" display="inline"><semantics id="S3.SS2.p2.6.m1.1a"><msub id="S3.SS2.p2.6.m1.1.1" xref="S3.SS2.p2.6.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.6.m1.1.1.2" xref="S3.SS2.p2.6.m1.1.1.2.cmml">𝒲</mi><mi id="S3.SS2.p2.6.m1.1.1.3" xref="S3.SS2.p2.6.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m1.1b"><apply id="S3.SS2.p2.6.m1.1.1.cmml" xref="S3.SS2.p2.6.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m1.1.1.1.cmml" xref="S3.SS2.p2.6.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.6.m1.1.1.2.cmml" xref="S3.SS2.p2.6.m1.1.1.2">𝒲</ci><ci id="S3.SS2.p2.6.m1.1.1.3.cmml" xref="S3.SS2.p2.6.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m1.1c">\mathcal{W}_{k}</annotation></semantics></math> of size <math id="S3.SS2.p2.7.m2.1" class="ltx_Math" alttext="m\times l" display="inline"><semantics id="S3.SS2.p2.7.m2.1a"><mrow id="S3.SS2.p2.7.m2.1.1" xref="S3.SS2.p2.7.m2.1.1.cmml"><mi id="S3.SS2.p2.7.m2.1.1.2" xref="S3.SS2.p2.7.m2.1.1.2.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.7.m2.1.1.1" xref="S3.SS2.p2.7.m2.1.1.1.cmml">×</mo><mi id="S3.SS2.p2.7.m2.1.1.3" xref="S3.SS2.p2.7.m2.1.1.3.cmml">l</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m2.1b"><apply id="S3.SS2.p2.7.m2.1.1.cmml" xref="S3.SS2.p2.7.m2.1.1"><times id="S3.SS2.p2.7.m2.1.1.1.cmml" xref="S3.SS2.p2.7.m2.1.1.1"></times><ci id="S3.SS2.p2.7.m2.1.1.2.cmml" xref="S3.SS2.p2.7.m2.1.1.2">𝑚</ci><ci id="S3.SS2.p2.7.m2.1.1.3.cmml" xref="S3.SS2.p2.7.m2.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m2.1c">m\times l</annotation></semantics></math>, <math id="S3.SS2.p2.8.m3.1" class="ltx_Math" alttext="\mathcal{A}_{k}" display="inline"><semantics id="S3.SS2.p2.8.m3.1a"><msub id="S3.SS2.p2.8.m3.1.1" xref="S3.SS2.p2.8.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.8.m3.1.1.2" xref="S3.SS2.p2.8.m3.1.1.2.cmml">𝒜</mi><mi id="S3.SS2.p2.8.m3.1.1.3" xref="S3.SS2.p2.8.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m3.1b"><apply id="S3.SS2.p2.8.m3.1.1.cmml" xref="S3.SS2.p2.8.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m3.1.1.1.cmml" xref="S3.SS2.p2.8.m3.1.1">subscript</csymbol><ci id="S3.SS2.p2.8.m3.1.1.2.cmml" xref="S3.SS2.p2.8.m3.1.1.2">𝒜</ci><ci id="S3.SS2.p2.8.m3.1.1.3.cmml" xref="S3.SS2.p2.8.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m3.1c">\mathcal{A}_{k}</annotation></semantics></math> and <math id="S3.SS2.p2.9.m4.1" class="ltx_Math" alttext="\mathcal{B}_{k}" display="inline"><semantics id="S3.SS2.p2.9.m4.1a"><msub id="S3.SS2.p2.9.m4.1.1" xref="S3.SS2.p2.9.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.9.m4.1.1.2" xref="S3.SS2.p2.9.m4.1.1.2.cmml">ℬ</mi><mi id="S3.SS2.p2.9.m4.1.1.3" xref="S3.SS2.p2.9.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m4.1b"><apply id="S3.SS2.p2.9.m4.1.1.cmml" xref="S3.SS2.p2.9.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.9.m4.1.1.1.cmml" xref="S3.SS2.p2.9.m4.1.1">subscript</csymbol><ci id="S3.SS2.p2.9.m4.1.1.2.cmml" xref="S3.SS2.p2.9.m4.1.1.2">ℬ</ci><ci id="S3.SS2.p2.9.m4.1.1.3.cmml" xref="S3.SS2.p2.9.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m4.1c">\mathcal{B}_{k}</annotation></semantics></math> are rank-<math id="S3.SS2.p2.10.m5.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS2.p2.10.m5.1a"><mi id="S3.SS2.p2.10.m5.1.1" xref="S3.SS2.p2.10.m5.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.10.m5.1b"><ci id="S3.SS2.p2.10.m5.1.1.cmml" xref="S3.SS2.p2.10.m5.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.10.m5.1c">r</annotation></semantics></math> matrices of sizes <math id="S3.SS2.p2.11.m6.1" class="ltx_Math" alttext="m\times r" display="inline"><semantics id="S3.SS2.p2.11.m6.1a"><mrow id="S3.SS2.p2.11.m6.1.1" xref="S3.SS2.p2.11.m6.1.1.cmml"><mi id="S3.SS2.p2.11.m6.1.1.2" xref="S3.SS2.p2.11.m6.1.1.2.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.11.m6.1.1.1" xref="S3.SS2.p2.11.m6.1.1.1.cmml">×</mo><mi id="S3.SS2.p2.11.m6.1.1.3" xref="S3.SS2.p2.11.m6.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.11.m6.1b"><apply id="S3.SS2.p2.11.m6.1.1.cmml" xref="S3.SS2.p2.11.m6.1.1"><times id="S3.SS2.p2.11.m6.1.1.1.cmml" xref="S3.SS2.p2.11.m6.1.1.1"></times><ci id="S3.SS2.p2.11.m6.1.1.2.cmml" xref="S3.SS2.p2.11.m6.1.1.2">𝑚</ci><ci id="S3.SS2.p2.11.m6.1.1.3.cmml" xref="S3.SS2.p2.11.m6.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.11.m6.1c">m\times r</annotation></semantics></math> and <math id="S3.SS2.p2.12.m7.1" class="ltx_Math" alttext="r\times l" display="inline"><semantics id="S3.SS2.p2.12.m7.1a"><mrow id="S3.SS2.p2.12.m7.1.1" xref="S3.SS2.p2.12.m7.1.1.cmml"><mi id="S3.SS2.p2.12.m7.1.1.2" xref="S3.SS2.p2.12.m7.1.1.2.cmml">r</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.12.m7.1.1.1" xref="S3.SS2.p2.12.m7.1.1.1.cmml">×</mo><mi id="S3.SS2.p2.12.m7.1.1.3" xref="S3.SS2.p2.12.m7.1.1.3.cmml">l</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.12.m7.1b"><apply id="S3.SS2.p2.12.m7.1.1.cmml" xref="S3.SS2.p2.12.m7.1.1"><times id="S3.SS2.p2.12.m7.1.1.1.cmml" xref="S3.SS2.p2.12.m7.1.1.1"></times><ci id="S3.SS2.p2.12.m7.1.1.2.cmml" xref="S3.SS2.p2.12.m7.1.1.2">𝑟</ci><ci id="S3.SS2.p2.12.m7.1.1.3.cmml" xref="S3.SS2.p2.12.m7.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.12.m7.1c">r\times l</annotation></semantics></math> respectively. These low-rank residual adapters can be applied efficiently during training and collapsed at inference time resulting in zero cost in terms of inference speeds or parameter counts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.
During finetuning
all the base model
parameters <math id="S3.SS2.p2.13.m8.2" class="ltx_Math" alttext="\forall k,\{\mathcal{W}_{k}\}" display="inline"><semantics id="S3.SS2.p2.13.m8.2a"><mrow id="S3.SS2.p2.13.m8.2.2.2" xref="S3.SS2.p2.13.m8.2.2.3.cmml"><mrow id="S3.SS2.p2.13.m8.1.1.1.1" xref="S3.SS2.p2.13.m8.1.1.1.1.cmml"><mo rspace="0.167em" id="S3.SS2.p2.13.m8.1.1.1.1.1" xref="S3.SS2.p2.13.m8.1.1.1.1.1.cmml">∀</mo><mi id="S3.SS2.p2.13.m8.1.1.1.1.2" xref="S3.SS2.p2.13.m8.1.1.1.1.2.cmml">k</mi></mrow><mo id="S3.SS2.p2.13.m8.2.2.2.3" xref="S3.SS2.p2.13.m8.2.2.3.cmml">,</mo><mrow id="S3.SS2.p2.13.m8.2.2.2.2.1" xref="S3.SS2.p2.13.m8.2.2.2.2.2.cmml"><mo stretchy="false" id="S3.SS2.p2.13.m8.2.2.2.2.1.2" xref="S3.SS2.p2.13.m8.2.2.2.2.2.cmml">{</mo><msub id="S3.SS2.p2.13.m8.2.2.2.2.1.1" xref="S3.SS2.p2.13.m8.2.2.2.2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.13.m8.2.2.2.2.1.1.2" xref="S3.SS2.p2.13.m8.2.2.2.2.1.1.2.cmml">𝒲</mi><mi id="S3.SS2.p2.13.m8.2.2.2.2.1.1.3" xref="S3.SS2.p2.13.m8.2.2.2.2.1.1.3.cmml">k</mi></msub><mo stretchy="false" id="S3.SS2.p2.13.m8.2.2.2.2.1.3" xref="S3.SS2.p2.13.m8.2.2.2.2.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.13.m8.2b"><list id="S3.SS2.p2.13.m8.2.2.3.cmml" xref="S3.SS2.p2.13.m8.2.2.2"><apply id="S3.SS2.p2.13.m8.1.1.1.1.cmml" xref="S3.SS2.p2.13.m8.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.p2.13.m8.1.1.1.1.1.cmml" xref="S3.SS2.p2.13.m8.1.1.1.1.1">for-all</csymbol><ci id="S3.SS2.p2.13.m8.1.1.1.1.2.cmml" xref="S3.SS2.p2.13.m8.1.1.1.1.2">𝑘</ci></apply><set id="S3.SS2.p2.13.m8.2.2.2.2.2.cmml" xref="S3.SS2.p2.13.m8.2.2.2.2.1"><apply id="S3.SS2.p2.13.m8.2.2.2.2.1.1.cmml" xref="S3.SS2.p2.13.m8.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.13.m8.2.2.2.2.1.1.1.cmml" xref="S3.SS2.p2.13.m8.2.2.2.2.1.1">subscript</csymbol><ci id="S3.SS2.p2.13.m8.2.2.2.2.1.1.2.cmml" xref="S3.SS2.p2.13.m8.2.2.2.2.1.1.2">𝒲</ci><ci id="S3.SS2.p2.13.m8.2.2.2.2.1.1.3.cmml" xref="S3.SS2.p2.13.m8.2.2.2.2.1.1.3">𝑘</ci></apply></set></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.13.m8.2c">\forall k,\{\mathcal{W}_{k}\}</annotation></semantics></math>
are frozen
and only the LoRA adapters <math id="S3.SS2.p2.14.m9.2" class="ltx_Math" alttext="\forall k,\{(\mathcal{A}_{k},\mathcal{B}_{k})\}" display="inline"><semantics id="S3.SS2.p2.14.m9.2a"><mrow id="S3.SS2.p2.14.m9.2.2.2" xref="S3.SS2.p2.14.m9.2.2.3.cmml"><mrow id="S3.SS2.p2.14.m9.1.1.1.1" xref="S3.SS2.p2.14.m9.1.1.1.1.cmml"><mo rspace="0.167em" id="S3.SS2.p2.14.m9.1.1.1.1.1" xref="S3.SS2.p2.14.m9.1.1.1.1.1.cmml">∀</mo><mi id="S3.SS2.p2.14.m9.1.1.1.1.2" xref="S3.SS2.p2.14.m9.1.1.1.1.2.cmml">k</mi></mrow><mo id="S3.SS2.p2.14.m9.2.2.2.3" xref="S3.SS2.p2.14.m9.2.2.3.cmml">,</mo><mrow id="S3.SS2.p2.14.m9.2.2.2.2.1" xref="S3.SS2.p2.14.m9.2.2.2.2.2.cmml"><mo stretchy="false" id="S3.SS2.p2.14.m9.2.2.2.2.1.2" xref="S3.SS2.p2.14.m9.2.2.2.2.2.cmml">{</mo><mrow id="S3.SS2.p2.14.m9.2.2.2.2.1.1.2" xref="S3.SS2.p2.14.m9.2.2.2.2.1.1.3.cmml"><mo stretchy="false" id="S3.SS2.p2.14.m9.2.2.2.2.1.1.2.3" xref="S3.SS2.p2.14.m9.2.2.2.2.1.1.3.cmml">(</mo><msub id="S3.SS2.p2.14.m9.2.2.2.2.1.1.1.1" xref="S3.SS2.p2.14.m9.2.2.2.2.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.14.m9.2.2.2.2.1.1.1.1.2" xref="S3.SS2.p2.14.m9.2.2.2.2.1.1.1.1.2.cmml">𝒜</mi><mi id="S3.SS2.p2.14.m9.2.2.2.2.1.1.1.1.3" xref="S3.SS2.p2.14.m9.2.2.2.2.1.1.1.1.3.cmml">k</mi></msub><mo id="S3.SS2.p2.14.m9.2.2.2.2.1.1.2.4" xref="S3.SS2.p2.14.m9.2.2.2.2.1.1.3.cmml">,</mo><msub id="S3.SS2.p2.14.m9.2.2.2.2.1.1.2.2" xref="S3.SS2.p2.14.m9.2.2.2.2.1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.14.m9.2.2.2.2.1.1.2.2.2" xref="S3.SS2.p2.14.m9.2.2.2.2.1.1.2.2.2.cmml">ℬ</mi><mi id="S3.SS2.p2.14.m9.2.2.2.2.1.1.2.2.3" xref="S3.SS2.p2.14.m9.2.2.2.2.1.1.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S3.SS2.p2.14.m9.2.2.2.2.1.1.2.5" xref="S3.SS2.p2.14.m9.2.2.2.2.1.1.3.cmml">)</mo></mrow><mo stretchy="false" id="S3.SS2.p2.14.m9.2.2.2.2.1.3" xref="S3.SS2.p2.14.m9.2.2.2.2.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.14.m9.2b"><list id="S3.SS2.p2.14.m9.2.2.3.cmml" xref="S3.SS2.p2.14.m9.2.2.2"><apply id="S3.SS2.p2.14.m9.1.1.1.1.cmml" xref="S3.SS2.p2.14.m9.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.p2.14.m9.1.1.1.1.1.cmml" xref="S3.SS2.p2.14.m9.1.1.1.1.1">for-all</csymbol><ci id="S3.SS2.p2.14.m9.1.1.1.1.2.cmml" xref="S3.SS2.p2.14.m9.1.1.1.1.2">𝑘</ci></apply><set id="S3.SS2.p2.14.m9.2.2.2.2.2.cmml" xref="S3.SS2.p2.14.m9.2.2.2.2.1"><interval closure="open" id="S3.SS2.p2.14.m9.2.2.2.2.1.1.3.cmml" xref="S3.SS2.p2.14.m9.2.2.2.2.1.1.2"><apply id="S3.SS2.p2.14.m9.2.2.2.2.1.1.1.1.cmml" xref="S3.SS2.p2.14.m9.2.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.14.m9.2.2.2.2.1.1.1.1.1.cmml" xref="S3.SS2.p2.14.m9.2.2.2.2.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.14.m9.2.2.2.2.1.1.1.1.2.cmml" xref="S3.SS2.p2.14.m9.2.2.2.2.1.1.1.1.2">𝒜</ci><ci id="S3.SS2.p2.14.m9.2.2.2.2.1.1.1.1.3.cmml" xref="S3.SS2.p2.14.m9.2.2.2.2.1.1.1.1.3">𝑘</ci></apply><apply id="S3.SS2.p2.14.m9.2.2.2.2.1.1.2.2.cmml" xref="S3.SS2.p2.14.m9.2.2.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.14.m9.2.2.2.2.1.1.2.2.1.cmml" xref="S3.SS2.p2.14.m9.2.2.2.2.1.1.2.2">subscript</csymbol><ci id="S3.SS2.p2.14.m9.2.2.2.2.1.1.2.2.2.cmml" xref="S3.SS2.p2.14.m9.2.2.2.2.1.1.2.2.2">ℬ</ci><ci id="S3.SS2.p2.14.m9.2.2.2.2.1.1.2.2.3.cmml" xref="S3.SS2.p2.14.m9.2.2.2.2.1.1.2.2.3">𝑘</ci></apply></interval></set></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.14.m9.2c">\forall k,\{(\mathcal{A}_{k},\mathcal{B}_{k})\}</annotation></semantics></math> are being learned.
Keeping rank <math id="S3.SS2.p2.15.m10.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS2.p2.15.m10.1a"><mi id="S3.SS2.p2.15.m10.1.1" xref="S3.SS2.p2.15.m10.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.15.m10.1b"><ci id="S3.SS2.p2.15.m10.1.1.cmml" xref="S3.SS2.p2.15.m10.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.15.m10.1c">r</annotation></semantics></math> low, the number of extra parameters added by all the LoRA adapters is low, consequently leading to significantly reduced forgetting in terms of largely maintaining the zero-shot performance of the original VL model.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Further reducing forgetting via model averaging:</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> introduced an elegant technique to mitigate forgetting in finetuned models. All the parameters of the source model (before finetune) and the final model (after finetune) are averaged between the two models (typically with <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\alpha=0.5" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mrow id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">α</mi><mo id="S3.SS2.p3.1.m1.1.1.1" xref="S3.SS2.p3.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><eq id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1.1"></eq><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝛼</ci><cn type="float" id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\alpha=0.5</annotation></semantics></math> weight). We evaluate the effect of this on <span id="S3.SS2.p3.1.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> finetuned models in our ablation Sec. <a href="#S4.SS4" title="4.4 Ablations ‣ 4 Experiments ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Domain adaption using style transfer:</span> In addition, to mitigate the domain gap introduced by the use of synthetic data, we experiment with two style transfer techniques that align the content and feature statistics of the input frames with randomly-selected real-life images. A pre-trained Adaptive Instance Normalization (AdaIN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> enabled encoder-decoder model was used to align the channel-wise statistics of each synthetic frame with a randomly-sampled image from the Human Motion Database (HMDB51) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> dataset thus generating a stylized synthetic image. We use AdaIN with an interpolation factor <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="\alpha=0.5" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mrow id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">α</mi><mo id="S3.SS2.p4.1.m1.1.1.1" xref="S3.SS2.p4.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><eq id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1.1"></eq><ci id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">𝛼</ci><cn type="float" id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\alpha=0.5</annotation></semantics></math>. In addition, in order to preserve the color information in the synthetic frames, we first match the color distribution of the sampled style image to that of the synthetic frame <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
We additionally experimented with MixStyle <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> (using ImageNet as a source of real style images) as an extension of the DA stylization pipeline without observing significant gains over AdaIN.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.2" class="ltx_p"><span id="S3.SS2.p5.2.1" class="ltx_text ltx_font_bold">Handling arbitrary caption length with caption splitting:</span> The captions generated for SyViC are comprehensive: they contain descriptions of every object and/or humanoid visible in the frame as well as the pairwise positional relationship between objects. Intuitively, including these more elaborate (dense) descriptions in our captions gives a clear advantage in terms of promoting VLC understanding and compositionality following the finetuning of a VL model on <span id="S3.SS2.p5.2.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span>.
Hence, captions need to be sufficiently long texts that cannot be fully processed by common VL models (e.g. CLIP) text encoders (<math id="S3.SS2.p5.1.m1.1" class="ltx_Math" alttext="\mathcal{E}_{T}" display="inline"><semantics id="S3.SS2.p5.1.m1.1a"><msub id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.1.m1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.2.cmml">ℰ</mi><mi id="S3.SS2.p5.1.m1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.2">ℰ</ci><ci id="S3.SS2.p5.1.m1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">\mathcal{E}_{T}</annotation></semantics></math>) during training, as those are caped by relatively short max sequence context length (e.g. 77 for CLIP). Therefore, inspired by CLIP multi-caption strategy for inference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, during training, we handle arbitrary caption lengths by splitting a given caption into sub-captions that can each be encoded separately and averaging the text features obtained from each sub-caption. In particular, the features of a caption of arbitrary length text <math id="S3.SS2.p5.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.p5.2.m2.1a"><mi id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><ci id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">T</annotation></semantics></math> is:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.2" class="ltx_Math" alttext="\mathcal{E}_{T}(T)=\frac{1}{n}\sum_{i}^{n}\mathcal{E}_{T}(T_{i})" display="block"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml"><mrow id="S3.E3.m1.2.2.3" xref="S3.E3.m1.2.2.3.cmml"><msub id="S3.E3.m1.2.2.3.2" xref="S3.E3.m1.2.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.2.2.3.2.2" xref="S3.E3.m1.2.2.3.2.2.cmml">ℰ</mi><mi id="S3.E3.m1.2.2.3.2.3" xref="S3.E3.m1.2.2.3.2.3.cmml">T</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.3.1" xref="S3.E3.m1.2.2.3.1.cmml">​</mo><mrow id="S3.E3.m1.2.2.3.3.2" xref="S3.E3.m1.2.2.3.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.3.3.2.1" xref="S3.E3.m1.2.2.3.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">T</mi><mo stretchy="false" id="S3.E3.m1.2.2.3.3.2.2" xref="S3.E3.m1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml">=</mo><mrow id="S3.E3.m1.2.2.1" xref="S3.E3.m1.2.2.1.cmml"><mfrac id="S3.E3.m1.2.2.1.3" xref="S3.E3.m1.2.2.1.3.cmml"><mn id="S3.E3.m1.2.2.1.3.2" xref="S3.E3.m1.2.2.1.3.2.cmml">1</mn><mi id="S3.E3.m1.2.2.1.3.3" xref="S3.E3.m1.2.2.1.3.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.2" xref="S3.E3.m1.2.2.1.2.cmml">​</mo><mrow id="S3.E3.m1.2.2.1.1" xref="S3.E3.m1.2.2.1.1.cmml"><munderover id="S3.E3.m1.2.2.1.1.2" xref="S3.E3.m1.2.2.1.1.2.cmml"><mo movablelimits="false" id="S3.E3.m1.2.2.1.1.2.2.2" xref="S3.E3.m1.2.2.1.1.2.2.2.cmml">∑</mo><mi id="S3.E3.m1.2.2.1.1.2.2.3" xref="S3.E3.m1.2.2.1.1.2.2.3.cmml">i</mi><mi id="S3.E3.m1.2.2.1.1.2.3" xref="S3.E3.m1.2.2.1.1.2.3.cmml">n</mi></munderover><mrow id="S3.E3.m1.2.2.1.1.1" xref="S3.E3.m1.2.2.1.1.1.cmml"><msub id="S3.E3.m1.2.2.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.2.2.1.1.1.3.2" xref="S3.E3.m1.2.2.1.1.1.3.2.cmml">ℰ</mi><mi id="S3.E3.m1.2.2.1.1.1.3.3" xref="S3.E3.m1.2.2.1.1.1.3.3.cmml">T</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.2.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.2.2.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E3.m1.2.2.1.1.1.1.1.1" xref="S3.E3.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.2" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2.cmml">T</mi><mi id="S3.E3.m1.2.2.1.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E3.m1.2.2.1.1.1.1.1.3" xref="S3.E3.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2"><eq id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2"></eq><apply id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2.3"><times id="S3.E3.m1.2.2.3.1.cmml" xref="S3.E3.m1.2.2.3.1"></times><apply id="S3.E3.m1.2.2.3.2.cmml" xref="S3.E3.m1.2.2.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.3.2.1.cmml" xref="S3.E3.m1.2.2.3.2">subscript</csymbol><ci id="S3.E3.m1.2.2.3.2.2.cmml" xref="S3.E3.m1.2.2.3.2.2">ℰ</ci><ci id="S3.E3.m1.2.2.3.2.3.cmml" xref="S3.E3.m1.2.2.3.2.3">𝑇</ci></apply><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝑇</ci></apply><apply id="S3.E3.m1.2.2.1.cmml" xref="S3.E3.m1.2.2.1"><times id="S3.E3.m1.2.2.1.2.cmml" xref="S3.E3.m1.2.2.1.2"></times><apply id="S3.E3.m1.2.2.1.3.cmml" xref="S3.E3.m1.2.2.1.3"><divide id="S3.E3.m1.2.2.1.3.1.cmml" xref="S3.E3.m1.2.2.1.3"></divide><cn type="integer" id="S3.E3.m1.2.2.1.3.2.cmml" xref="S3.E3.m1.2.2.1.3.2">1</cn><ci id="S3.E3.m1.2.2.1.3.3.cmml" xref="S3.E3.m1.2.2.1.3.3">𝑛</ci></apply><apply id="S3.E3.m1.2.2.1.1.cmml" xref="S3.E3.m1.2.2.1.1"><apply id="S3.E3.m1.2.2.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.2.1.cmml" xref="S3.E3.m1.2.2.1.1.2">superscript</csymbol><apply id="S3.E3.m1.2.2.1.1.2.2.cmml" xref="S3.E3.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.2.2.1.cmml" xref="S3.E3.m1.2.2.1.1.2">subscript</csymbol><sum id="S3.E3.m1.2.2.1.1.2.2.2.cmml" xref="S3.E3.m1.2.2.1.1.2.2.2"></sum><ci id="S3.E3.m1.2.2.1.1.2.2.3.cmml" xref="S3.E3.m1.2.2.1.1.2.2.3">𝑖</ci></apply><ci id="S3.E3.m1.2.2.1.1.2.3.cmml" xref="S3.E3.m1.2.2.1.1.2.3">𝑛</ci></apply><apply id="S3.E3.m1.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1"><times id="S3.E3.m1.2.2.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.2"></times><apply id="S3.E3.m1.2.2.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.3.1.cmml" xref="S3.E3.m1.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.3.2.cmml" xref="S3.E3.m1.2.2.1.1.1.3.2">ℰ</ci><ci id="S3.E3.m1.2.2.1.1.1.3.3.cmml" xref="S3.E3.m1.2.2.1.1.1.3.3">𝑇</ci></apply><apply id="S3.E3.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.2">𝑇</ci><ci id="S3.E3.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">\mathcal{E}_{T}(T)=\frac{1}{n}\sum_{i}^{n}\mathcal{E}_{T}(T_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p5.3" class="ltx_p">where <math id="S3.SS2.p5.3.m1.1" class="ltx_Math" alttext="T_{i}" display="inline"><semantics id="S3.SS2.p5.3.m1.1a"><msub id="S3.SS2.p5.3.m1.1.1" xref="S3.SS2.p5.3.m1.1.1.cmml"><mi id="S3.SS2.p5.3.m1.1.1.2" xref="S3.SS2.p5.3.m1.1.1.2.cmml">T</mi><mi id="S3.SS2.p5.3.m1.1.1.3" xref="S3.SS2.p5.3.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m1.1b"><apply id="S3.SS2.p5.3.m1.1.1.cmml" xref="S3.SS2.p5.3.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.3.m1.1.1.1.cmml" xref="S3.SS2.p5.3.m1.1.1">subscript</csymbol><ci id="S3.SS2.p5.3.m1.1.1.2.cmml" xref="S3.SS2.p5.3.m1.1.1.2">𝑇</ci><ci id="S3.SS2.p5.3.m1.1.1.3.cmml" xref="S3.SS2.p5.3.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m1.1c">T_{i}</annotation></semantics></math> is a sub-caption comprised of one or more sentences that fit into the text encoder max context size.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para ltx_noindent">
<p id="S3.SS2.p6.1" class="ltx_p"><span id="S3.SS2.p6.1.1" class="ltx_text ltx_font_bold">Losses:</span> We employ the original models (e.g. CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> and CyCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>)
contrastive and other losses when training on <span id="S3.SS2.p6.1.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> with the aforementioned architectural and training protocol changes as explained above.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.5.1.1" class="ltx_tr">
<th id="S3.T1.5.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt"></th>
<th id="S3.T1.5.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="3"><span id="S3.T1.5.1.1.2.1" class="ltx_text" style="font-size:70%;">VL Checklist</span></th>
<th id="S3.T1.5.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="5"><span id="S3.T1.5.1.1.3.1" class="ltx_text" style="font-size:70%;">ARO</span></th>
<th id="S3.T1.5.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.5.1.1.4.1" class="ltx_text" style="font-size:70%;">Zero-Short</span></th>
</tr>
<tr id="S3.T1.5.2.2" class="ltx_tr">
<th id="S3.T1.5.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_border_r"></th>
<th id="S3.T1.5.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S3.T1.5.2.2.2.1" class="ltx_text" style="font-size:70%;">Relation</span></th>
<th id="S3.T1.5.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r"><span id="S3.T1.5.2.2.3.1" class="ltx_text" style="font-size:70%;">Attribute</span></th>
<th id="S3.T1.5.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r"><span id="S3.T1.5.2.2.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Average</span></th>
<th id="S3.T1.5.2.2.5" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S3.T1.5.2.2.5.1" class="ltx_text" style="font-size:70%;">VG-Rel.</span></th>
<th id="S3.T1.5.2.2.6" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S3.T1.5.2.2.6.1" class="ltx_text" style="font-size:70%;">VG-Att.</span></th>
<th id="S3.T1.5.2.2.7" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S3.T1.5.2.2.7.1" class="ltx_text" style="font-size:70%;">Flickr30k</span></th>
<th id="S3.T1.5.2.2.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r"><span id="S3.T1.5.2.2.8.1" class="ltx_text" style="font-size:70%;">COCO</span></th>
<th id="S3.T1.5.2.2.9" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r"><span id="S3.T1.5.2.2.9.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Average</span></th>
<th id="S3.T1.5.2.2.10" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S3.T1.5.2.2.10.1" class="ltx_text" style="font-size:70%;">(21 tasks)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.5.3.1" class="ltx_tr">
<td id="S3.T1.5.3.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T1.5.3.1.1.1" class="ltx_text" style="font-size:70%;">CLIP</span></td>
<td id="S3.T1.5.3.1.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.5.3.1.2.1" class="ltx_text" style="font-size:70%;">63.57</span></td>
<td id="S3.T1.5.3.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T1.5.3.1.3.1" class="ltx_text" style="font-size:70%;">67.51</span></td>
<td id="S3.T1.5.3.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T1.5.3.1.4.1" class="ltx_text" style="font-size:70%;">65.54</span></td>
<td id="S3.T1.5.3.1.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.5.3.1.5.1" class="ltx_text" style="font-size:70%;">58.84</span></td>
<td id="S3.T1.5.3.1.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.5.3.1.6.1" class="ltx_text" style="font-size:70%;">63.19</span></td>
<td id="S3.T1.5.3.1.7" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.5.3.1.7.1" class="ltx_text" style="font-size:70%;">47.20</span></td>
<td id="S3.T1.5.3.1.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T1.5.3.1.8.1" class="ltx_text" style="font-size:70%;">59.46</span></td>
<td id="S3.T1.5.3.1.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T1.5.3.1.9.1" class="ltx_text" style="font-size:70%;">57.17</span></td>
<td id="S3.T1.5.3.1.10" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.5.3.1.10.1" class="ltx_text" style="font-size:70%;">56.07</span></td>
</tr>
<tr id="S3.T1.5.4.2" class="ltx_tr">
<td id="S3.T1.5.4.2.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S3.T1.5.4.2.1.1" class="ltx_text" style="font-size:70%;">CyCLIP</span></td>
<td id="S3.T1.5.4.2.2" class="ltx_td ltx_align_left"><span id="S3.T1.5.4.2.2.1" class="ltx_text" style="font-size:70%;">61.15</span></td>
<td id="S3.T1.5.4.2.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S3.T1.5.4.2.3.1" class="ltx_text" style="font-size:70%;">66.96</span></td>
<td id="S3.T1.5.4.2.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S3.T1.5.4.2.4.1" class="ltx_text" style="font-size:70%;">64.06</span></td>
<td id="S3.T1.5.4.2.5" class="ltx_td ltx_align_left"><span id="S3.T1.5.4.2.5.1" class="ltx_text" style="font-size:70%;">59.12</span></td>
<td id="S3.T1.5.4.2.6" class="ltx_td ltx_align_left"><span id="S3.T1.5.4.2.6.1" class="ltx_text" style="font-size:70%;">65.41</span></td>
<td id="S3.T1.5.4.2.7" class="ltx_td ltx_align_left"><span id="S3.T1.5.4.2.7.1" class="ltx_text" style="font-size:70%;">20.82</span></td>
<td id="S3.T1.5.4.2.8" class="ltx_td ltx_align_left ltx_border_r"><span id="S3.T1.5.4.2.8.1" class="ltx_text" style="font-size:70%;">29.54</span></td>
<td id="S3.T1.5.4.2.9" class="ltx_td ltx_align_left ltx_border_r"><span id="S3.T1.5.4.2.9.1" class="ltx_text" style="font-size:70%;">43.72</span></td>
<td id="S3.T1.5.4.2.10" class="ltx_td ltx_align_left"><span id="S3.T1.5.4.2.10.1" class="ltx_text" style="font-size:70%;">55.99</span></td>
</tr>
<tr id="S3.T1.5.5.3" class="ltx_tr">
<td id="S3.T1.5.5.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T1.5.5.3.1.1" class="ltx_text" style="font-size:70%;">syn-CLIP</span></td>
<td id="S3.T1.5.5.3.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S3.T1.5.5.3.2.1" class="ltx_text" style="font-size:70%;">69.39</span><span id="S3.T1.5.5.3.2.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+5.82)</span>
</td>
<td id="S3.T1.5.5.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T1.5.5.3.3.1" class="ltx_text" style="font-size:70%;">70.37</span><span id="S3.T1.5.5.3.3.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+2.86)</span>
</td>
<td id="S3.T1.5.5.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T1.5.5.3.4.1" class="ltx_text" style="font-size:70%;">69.88</span><span id="S3.T1.5.5.3.4.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+4.34)</span>
</td>
<td id="S3.T1.5.5.3.5" class="ltx_td ltx_align_left ltx_border_t">
<span id="S3.T1.5.5.3.5.1" class="ltx_text" style="font-size:70%;">71.40</span><span id="S3.T1.5.5.3.5.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+12.56)</span>
</td>
<td id="S3.T1.5.5.3.6" class="ltx_td ltx_align_left ltx_border_t">
<span id="S3.T1.5.5.3.6.1" class="ltx_text" style="font-size:70%;">66.94</span><span id="S3.T1.5.5.3.6.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+3.75)</span>
</td>
<td id="S3.T1.5.5.3.7" class="ltx_td ltx_align_left ltx_border_t">
<span id="S3.T1.5.5.3.7.1" class="ltx_text" style="font-size:70%;">59.06</span><span id="S3.T1.5.5.3.7.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+11.86)</span>
</td>
<td id="S3.T1.5.5.3.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T1.5.5.3.8.1" class="ltx_text" style="font-size:70%;">70.96</span><span id="S3.T1.5.5.3.8.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+11.5)</span>
</td>
<td id="S3.T1.5.5.3.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="S3.T1.5.5.3.9.1" class="ltx_text" style="font-size:70%;">67.09</span><span id="S3.T1.5.5.3.9.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+9.9)</span>
</td>
<td id="S3.T1.5.5.3.10" class="ltx_td ltx_align_left ltx_border_t">
<span id="S3.T1.5.5.3.10.1" class="ltx_text" style="font-size:70%;">55.27</span><span id="S3.T1.5.5.3.10.2" class="ltx_text" style="font-size:70%;color:#F18C8E;"> (-0.8)</span>
</td>
</tr>
<tr id="S3.T1.5.6.4" class="ltx_tr">
<td id="S3.T1.5.6.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S3.T1.5.6.4.1.1" class="ltx_text" style="font-size:70%;">syn-CyCLIP</span></td>
<td id="S3.T1.5.6.4.2" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S3.T1.5.6.4.2.1" class="ltx_text" style="font-size:70%;">65.73</span><span id="S3.T1.5.6.4.2.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+4.58)</span>
</td>
<td id="S3.T1.5.6.4.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">
<span id="S3.T1.5.6.4.3.1" class="ltx_text" style="font-size:70%;">68.06</span><span id="S3.T1.5.6.4.3.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+1.1)</span>
</td>
<td id="S3.T1.5.6.4.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">
<span id="S3.T1.5.6.4.4.1" class="ltx_text" style="font-size:70%;">66.89</span><span id="S3.T1.5.6.4.4.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+2.83)</span>
</td>
<td id="S3.T1.5.6.4.5" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S3.T1.5.6.4.5.1" class="ltx_text" style="font-size:70%;">69.02</span><span id="S3.T1.5.6.4.5.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+9.9)</span>
</td>
<td id="S3.T1.5.6.4.6" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S3.T1.5.6.4.6.1" class="ltx_text" style="font-size:70%;">63.65</span><span id="S3.T1.5.6.4.6.2" class="ltx_text" style="font-size:70%;color:#F18C8E;"> (-1.76)</span>
</td>
<td id="S3.T1.5.6.4.7" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S3.T1.5.6.4.7.1" class="ltx_text" style="font-size:70%;">49.17</span><span id="S3.T1.5.6.4.7.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+28.35)</span>
</td>
<td id="S3.T1.5.6.4.8" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">
<span id="S3.T1.5.6.4.8.1" class="ltx_text" style="font-size:70%;">59.36</span><span id="S3.T1.5.6.4.8.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+29.82)</span>
</td>
<td id="S3.T1.5.6.4.9" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">
<span id="S3.T1.5.6.4.9.1" class="ltx_text" style="font-size:70%;">60.30</span><span id="S3.T1.5.6.4.9.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+16.58)</span>
</td>
<td id="S3.T1.5.6.4.10" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S3.T1.5.6.4.10.1" class="ltx_text" style="font-size:70%;">55.40 </span><span id="S3.T1.5.6.4.10.2" class="ltx_text" style="font-size:70%;color:#F18C8E;"> (-0.6)</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance of syn-<math id="S3.T1.3.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.T1.3.m1.1b"><mo id="S3.T1.3.m1.1.1" xref="S3.T1.3.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.m1.1c"><lt id="S3.T1.3.m1.1.1.cmml" xref="S3.T1.3.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.m1.1d">&lt;</annotation></semantics></math><span id="S3.T1.4.1" class="ltx_text ltx_font_italic">model<math id="S3.T1.4.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.T1.4.1.m1.1b"><mo id="S3.T1.4.1.m1.1.1" xref="S3.T1.4.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.1.m1.1c"><gt id="S3.T1.4.1.m1.1.1.cmml" xref="S3.T1.4.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.1.m1.1d">&gt;</annotation></semantics></math></span>s – finetuned on <span id="S3.T1.21.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> using our proposed recipe, measured on VL-Checklist <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> and ARO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. Gains and losses are highlighted in <span id="S3.T1.22.3" class="ltx_text" style="color:#4EA34E;">green</span> and <span id="S3.T1.23.4" class="ltx_text" style="color:#FF0000;">red</span> respectively.</figcaption>
</figure>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1" class="ltx_tr">
<th id="S3.T2.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S3.T2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="3"><span id="S3.T2.1.1.3.1" class="ltx_text" style="font-size:70%;">Winoground</span></th>
<th id="S3.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">
<span id="S3.T2.1.1.1.1" class="ltx_text" style="font-size:70%;">Winoground</span><sup id="S3.T2.1.1.1.2" class="ltx_sup"><span id="S3.T2.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">†</span></sup>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<th id="S3.T2.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S3.T2.1.2.1.2" class="ltx_td ltx_align_left"><span id="S3.T2.1.2.1.2.1" class="ltx_text" style="font-size:70%;">Text</span></td>
<td id="S3.T2.1.2.1.3" class="ltx_td ltx_align_left"><span id="S3.T2.1.2.1.3.1" class="ltx_text" style="font-size:70%;">Image</span></td>
<td id="S3.T2.1.2.1.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S3.T2.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Group</span></td>
<td id="S3.T2.1.2.1.5" class="ltx_td ltx_align_left"><span id="S3.T2.1.2.1.5.1" class="ltx_text" style="font-size:70%;">Text</span></td>
<td id="S3.T2.1.2.1.6" class="ltx_td ltx_align_left"><span id="S3.T2.1.2.1.6.1" class="ltx_text" style="font-size:70%;">Image</span></td>
<td id="S3.T2.1.2.1.7" class="ltx_td ltx_align_left"><span id="S3.T2.1.2.1.7.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Group</span></td>
</tr>
<tr id="S3.T2.1.3.2" class="ltx_tr">
<th id="S3.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T2.1.3.2.1.1" class="ltx_text" style="font-size:70%;">CLIP</span></th>
<td id="S3.T2.1.3.2.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.3.2.2.1" class="ltx_text" style="font-size:70%;">31.25</span></td>
<td id="S3.T2.1.3.2.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.3.2.3.1" class="ltx_text" style="font-size:70%;">10.50</span></td>
<td id="S3.T2.1.3.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S3.T2.1.3.2.4.1" class="ltx_text" style="font-size:70%;">8.00</span></td>
<td id="S3.T2.1.3.2.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.3.2.5.1" class="ltx_text" style="font-size:70%;">31.58</span></td>
<td id="S3.T2.1.3.2.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.3.2.6.1" class="ltx_text" style="font-size:70%;">10.53</span></td>
<td id="S3.T2.1.3.2.7" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.3.2.7.1" class="ltx_text" style="font-size:70%;">8.19</span></td>
</tr>
<tr id="S3.T2.1.4.3" class="ltx_tr">
<th id="S3.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t"><span id="S3.T2.1.4.3.1.1" class="ltx_text" style="font-size:70%;">syn-CLIP</span></th>
<td id="S3.T2.1.4.3.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="S3.T2.1.4.3.2.1" class="ltx_text" style="font-size:70%;">30.00</span></td>
<td id="S3.T2.1.4.3.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="S3.T2.1.4.3.3.1" class="ltx_text" style="font-size:70%;">11.50</span></td>
<td id="S3.T2.1.4.3.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t">
<span id="S3.T2.1.4.3.4.1" class="ltx_text" style="font-size:70%;">9.50</span><span id="S3.T2.1.4.3.4.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+1.50)</span>
</td>
<td id="S3.T2.1.4.3.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="S3.T2.1.4.3.5.1" class="ltx_text" style="font-size:70%;">29.82</span></td>
<td id="S3.T2.1.4.3.6" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="S3.T2.1.4.3.6.1" class="ltx_text" style="font-size:70%;">12.28</span></td>
<td id="S3.T2.1.4.3.7" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="S3.T2.1.4.3.7.1" class="ltx_text" style="font-size:70%;">9.94</span><span id="S3.T2.1.4.3.7.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+1.75)</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Winoground <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> performance of syn-CLIP – finetuned on <span id="S3.T2.15.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span>.
The syn-CyCLIP results on Winoground are provided in the Supplementary.
<sup id="S3.T2.16.2" class="ltx_sup"><span id="S3.T2.16.2.1" class="ltx_text ltx_font_italic">†</span></sup> ‘clean’ (no-tag) subset of valid Winoground samples from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Implementation details</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">For CLIP, we use the original OpenAI CLIP implementation and checkpoints. We modify their codebase to include LoRA adapters (Sec. 3.2), and use rank 16 in all our experiments. For CyCLIP, we adapt the implementation used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Code and checkpoints kindly shared by the authors.</span></span></span>. For both CLIP and CyCLIP, we use a 5e-7 initial learning rate for finetuning and follow a cosine annealing learning rate schedule <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> using an Adam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> optimizer. For all experiments, we use ViT/32-B as the model architecture and fine-tune it for six epochs on one A100 GPU with a total batch size of <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="400" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">400</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><cn type="integer" id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">400</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">400</annotation></semantics></math> image-caption pairs. In addition to the original CLIP data augmentation transforms, we apply heavy random augmentation policies including manipulations in image inversion, contrast, sharpness, equalization, posterization, colorization, brightness, and solarization.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Datasets</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To test the effectiveness of our proposed <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> synthetic dataset and the accompanying finetuning approach for improving VL models’ VLC understanding and compositional reasoning capabilities we have evaluated on 3 benchmarks (Winoground <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, VL-Checklist <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>, and ARO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>) consisted of 7 datasets total.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">VL-Checklist <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite></span> – is a large-scale dataset comprised of: Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, SWiG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, VAW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, and HAKE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Each image of these datasets is associated with two captions, a positive and a negative. The positive caption corresponds to the image and is taken from the source dataset. The negative caption is made from the positive caption by changing one word, so the resulting sentence no longer corresponds to the image. Depending on the word that was changed, VL-Checklist evaluates 7 types of VLC that can be divided into two main groups: (1) Attributes – color, material, size, state, and action, and (2) Relations – spatial or action relation between two objects and/or humans. In the following, we report average results for each of the main (Rel. and Attr.) groups on the combined VL-Checklist dataset. We also detail the individual improvements on all 7 VLC types in Fig. <a href="#S4.F3" title="Figure 3 ‣ 4.4 Ablations ‣ 4 Experiments ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (left).</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Winoground <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite></span> – is a small dataset that evaluates the ability of VL models for compositional reasoning, specifically understanding the meaning of the sentence after changing the order of its words. The dataset has 400 samples, each comprised of two images and two texts. The texts have the same words in a different order, each text corresponding to one image in the sample. The Winoground metrics include (a) image score - percent of samples where the model picks the correct text for each image; (b) text score - percent of samples where the model picks the correct image for each text; (c) group score - percent of samples where both text and image score conditions are satisfied jointly. Recently, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> has analyzed Winoground for the source of its difficulty and found that only <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="171" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mn id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">171</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><cn type="integer" id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">171</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">171</annotation></semantics></math> of its 400 samples are a valid subset. Other samples are not compositional, ambiguous, related to invisible details, have highly uncommon images or text, or require complex reasoning beyond compositionality. We report results on both the full Winoground and the ‘clean’ 171 images subset from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.6" class="ltx_p"><span id="S4.SS2.p4.6.1" class="ltx_text ltx_font_bold">ARO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite></span> – or the Attribution, Relation, and Order benchmark, is a large dataset designed to evaluate the ability of VL models to understand four different types of skills. It consists of Visual Genome Attribution and Visual Genome Relation, which leverages the Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> dataset along with the GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> annotations to test the understanding of properties and relational understanding of objects in complex natural scenes. VG-Relation includes <math id="S4.SS2.p4.1.m1.1" class="ltx_Math" alttext="48" display="inline"><semantics id="S4.SS2.p4.1.m1.1a"><mn id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml">48</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><cn type="integer" id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">48</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">48</annotation></semantics></math> distinct relations with <math id="S4.SS2.p4.2.m2.1" class="ltx_Math" alttext="23937" display="inline"><semantics id="S4.SS2.p4.2.m2.1a"><mn id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml">23937</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><cn type="integer" id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1">23937</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">23937</annotation></semantics></math> test cases, and VG-Attribution includes <math id="S4.SS2.p4.3.m3.1" class="ltx_Math" alttext="117" display="inline"><semantics id="S4.SS2.p4.3.m3.1a"><mn id="S4.SS2.p4.3.m3.1.1" xref="S4.SS2.p4.3.m3.1.1.cmml">117</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.3.m3.1b"><cn type="integer" id="S4.SS2.p4.3.m3.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1">117</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.3.m3.1c">117</annotation></semantics></math> unique attribute pairs with <math id="S4.SS2.p4.4.m4.1" class="ltx_Math" alttext="28748" display="inline"><semantics id="S4.SS2.p4.4.m4.1a"><mn id="S4.SS2.p4.4.m4.1.1" xref="S4.SS2.p4.4.m4.1.1.cmml">28748</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.4.m4.1b"><cn type="integer" id="S4.SS2.p4.4.m4.1.1.cmml" xref="S4.SS2.p4.4.m4.1.1">28748</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.4.m4.1c">28748</annotation></semantics></math> test cases. It also leverages the COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and Flickr30k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> datasets to evaluate the model sensitivity to select the right caption after applying four different shuffling perturbations (e.g., exchanging nouns and adjectives, or by shuffling trigrams). These tests are performed on the <math id="S4.SS2.p4.5.m5.1" class="ltx_Math" alttext="5000" display="inline"><semantics id="S4.SS2.p4.5.m5.1a"><mn id="S4.SS2.p4.5.m5.1.1" xref="S4.SS2.p4.5.m5.1.1.cmml">5000</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.5.m5.1b"><cn type="integer" id="S4.SS2.p4.5.m5.1.1.cmml" xref="S4.SS2.p4.5.m5.1.1">5000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.5.m5.1c">5000</annotation></semantics></math> and the <math id="S4.SS2.p4.6.m6.1" class="ltx_Math" alttext="1000" display="inline"><semantics id="S4.SS2.p4.6.m6.1a"><mn id="S4.SS2.p4.6.m6.1.1" xref="S4.SS2.p4.6.m6.1.1.cmml">1000</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.6.m6.1b"><cn type="integer" id="S4.SS2.p4.6.m6.1.1.cmml" xref="S4.SS2.p4.6.m6.1.1">1000</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.6.m6.1c">1000</annotation></semantics></math> images from the respective COCO and Flickr30k test splits.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.7" class="ltx_p">The main results of finetuning CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, CyCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> – one of CLIP’s most recent improvements
are summarized in Tables <a href="#S3.T1" title="Table 1 ‣ 3.2 Finetuning large-scale pre-trained VL models using synthetic data ‣ 3 Method ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S3.T2" title="Table 2 ‣ 3.2 Finetuning large-scale pre-trained VL models using synthetic data ‣ 3 Method ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. All models were finetuned using our proposed approach and <span id="S4.SS3.p1.7.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> synthetic data to obtain their syn-<math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mo id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><lt id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">&lt;</annotation></semantics></math><span id="S4.SS3.p1.2.1" class="ltx_text ltx_font_italic">model<math id="S4.SS3.p1.2.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.SS3.p1.2.1.m1.1a"><mo id="S4.SS3.p1.2.1.m1.1.1" xref="S4.SS3.p1.2.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.1.m1.1b"><gt id="S4.SS3.p1.2.1.m1.1.1.cmml" xref="S4.SS3.p1.2.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.1.m1.1c">&gt;</annotation></semantics></math></span> variants. Each model is compared to its respective source model pre-trained on large-scale real data before finetuning on <span id="S4.SS3.p1.7.3" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span>. As we can observe, our <span id="S4.SS3.p1.7.4" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> synthetic data and the proposed finetuning recipe on this data demonstrate significant improvements over their source baselines. E.g. for CLIP obtaining <math id="S4.SS3.p1.3.m2.1" class="ltx_Math" alttext="1.75\%" display="inline"><semantics id="S4.SS3.p1.3.m2.1a"><mrow id="S4.SS3.p1.3.m2.1.1" xref="S4.SS3.p1.3.m2.1.1.cmml"><mn id="S4.SS3.p1.3.m2.1.1.2" xref="S4.SS3.p1.3.m2.1.1.2.cmml">1.75</mn><mo id="S4.SS3.p1.3.m2.1.1.1" xref="S4.SS3.p1.3.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m2.1b"><apply id="S4.SS3.p1.3.m2.1.1.cmml" xref="S4.SS3.p1.3.m2.1.1"><csymbol cd="latexml" id="S4.SS3.p1.3.m2.1.1.1.cmml" xref="S4.SS3.p1.3.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p1.3.m2.1.1.2.cmml" xref="S4.SS3.p1.3.m2.1.1.2">1.75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m2.1c">1.75\%</annotation></semantics></math>, <math id="S4.SS3.p1.4.m3.1" class="ltx_Math" alttext="4.34\%" display="inline"><semantics id="S4.SS3.p1.4.m3.1a"><mrow id="S4.SS3.p1.4.m3.1.1" xref="S4.SS3.p1.4.m3.1.1.cmml"><mn id="S4.SS3.p1.4.m3.1.1.2" xref="S4.SS3.p1.4.m3.1.1.2.cmml">4.34</mn><mo id="S4.SS3.p1.4.m3.1.1.1" xref="S4.SS3.p1.4.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m3.1b"><apply id="S4.SS3.p1.4.m3.1.1.cmml" xref="S4.SS3.p1.4.m3.1.1"><csymbol cd="latexml" id="S4.SS3.p1.4.m3.1.1.1.cmml" xref="S4.SS3.p1.4.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p1.4.m3.1.1.2.cmml" xref="S4.SS3.p1.4.m3.1.1.2">4.34</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m3.1c">4.34\%</annotation></semantics></math>, and <math id="S4.SS3.p1.5.m4.1" class="ltx_Math" alttext="9.9\%" display="inline"><semantics id="S4.SS3.p1.5.m4.1a"><mrow id="S4.SS3.p1.5.m4.1.1" xref="S4.SS3.p1.5.m4.1.1.cmml"><mn id="S4.SS3.p1.5.m4.1.1.2" xref="S4.SS3.p1.5.m4.1.1.2.cmml">9.9</mn><mo id="S4.SS3.p1.5.m4.1.1.1" xref="S4.SS3.p1.5.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.5.m4.1b"><apply id="S4.SS3.p1.5.m4.1.1.cmml" xref="S4.SS3.p1.5.m4.1.1"><csymbol cd="latexml" id="S4.SS3.p1.5.m4.1.1.1.cmml" xref="S4.SS3.p1.5.m4.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p1.5.m4.1.1.2.cmml" xref="S4.SS3.p1.5.m4.1.1.2">9.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.5.m4.1c">9.9\%</annotation></semantics></math> average absolute improvement in Winoground group score (most difficult average metric), VL-Checklist and ARO respectively. In addition, we illustrate the individual VLC metrics improvements obtained for CLIP in VL-Checklist and ARO benchmarks in Fig. <a href="#S4.F3" title="Figure 3 ‣ 4.4 Ablations ‣ 4 Experiments ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> showing up to <math id="S4.SS3.p1.6.m5.1" class="ltx_Math" alttext="9.1\%" display="inline"><semantics id="S4.SS3.p1.6.m5.1a"><mrow id="S4.SS3.p1.6.m5.1.1" xref="S4.SS3.p1.6.m5.1.1.cmml"><mn id="S4.SS3.p1.6.m5.1.1.2" xref="S4.SS3.p1.6.m5.1.1.2.cmml">9.1</mn><mo id="S4.SS3.p1.6.m5.1.1.1" xref="S4.SS3.p1.6.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.6.m5.1b"><apply id="S4.SS3.p1.6.m5.1.1.cmml" xref="S4.SS3.p1.6.m5.1.1"><csymbol cd="latexml" id="S4.SS3.p1.6.m5.1.1.1.cmml" xref="S4.SS3.p1.6.m5.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p1.6.m5.1.1.2.cmml" xref="S4.SS3.p1.6.m5.1.1.2">9.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.6.m5.1c">9.1\%</annotation></semantics></math> and <math id="S4.SS3.p1.7.m6.1" class="ltx_Math" alttext="12.6\%" display="inline"><semantics id="S4.SS3.p1.7.m6.1a"><mrow id="S4.SS3.p1.7.m6.1.1" xref="S4.SS3.p1.7.m6.1.1.cmml"><mn id="S4.SS3.p1.7.m6.1.1.2" xref="S4.SS3.p1.7.m6.1.1.2.cmml">12.6</mn><mo id="S4.SS3.p1.7.m6.1.1.1" xref="S4.SS3.p1.7.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.7.m6.1b"><apply id="S4.SS3.p1.7.m6.1.1.cmml" xref="S4.SS3.p1.7.m6.1.1"><csymbol cd="latexml" id="S4.SS3.p1.7.m6.1.1.1.cmml" xref="S4.SS3.p1.7.m6.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p1.7.m6.1.1.2.cmml" xref="S4.SS3.p1.7.m6.1.1.2">12.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.7.m6.1c">12.6\%</annotation></semantics></math> respective absolute improvements. This underlines the effectiveness and promise of our method and <span id="S4.SS3.p1.7.5" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> synthetic data towards improving VLC understanding and compositional reasoning in VL models. Importantly, as we can see from Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Finetuning large-scale pre-trained VL models using synthetic data ‣ 3 Method ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, these strong gains come at a very small (under 1%) cost in the zero-shot performance of the respective VL models measured using the standard Elevater <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> benchmark using 21 diverse zero-shot tasks.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S4.T3.1.1.1.1.1" class="ltx_text" style="font-size:70%;">objects with attr.</span></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T3.1.1.1.2.1" class="ltx_text" style="font-size:70%;">humans</span></th>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="S4.T3.1.1.1.3.1" class="ltx_text" style="font-size:70%;">VL Checklist</span></td>
</tr>
<tr id="S4.T3.1.2.2" class="ltx_tr">
<th id="S4.T3.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.1.2.2.1.1" class="ltx_text" style="font-size:70%;">randomization</span></th>
<th id="S4.T3.1.2.2.2" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S4.T3.1.2.2.3" class="ltx_td ltx_align_left"><span id="S4.T3.1.2.2.3.1" class="ltx_text" style="font-size:70%;">Relation</span></td>
<td id="S4.T3.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T3.1.2.2.4.1" class="ltx_text" style="font-size:70%;">Attribute</span></td>
<td id="S4.T3.1.2.2.5" class="ltx_td ltx_align_left"><span id="S4.T3.1.2.2.5.1" class="ltx_text" style="font-size:70%;">Average</span></td>
</tr>
<tr id="S4.T3.1.3.3" class="ltx_tr">
<th id="S4.T3.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" colspan="2"><span id="S4.T3.1.3.3.1.1" class="ltx_text" style="font-size:70%;">CLIP</span></th>
<td id="S4.T3.1.3.3.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T3.1.3.3.2.1" class="ltx_text" style="font-size:70%;">63.57</span></td>
<td id="S4.T3.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T3.1.3.3.3.1" class="ltx_text" style="font-size:70%;">67.51</span></td>
<td id="S4.T3.1.3.3.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T3.1.3.3.4.1" class="ltx_text" style="font-size:70%;">65.54</span></td>
</tr>
<tr id="S4.T3.1.4.4" class="ltx_tr">
<th id="S4.T3.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.1.4.4.1.1" class="ltx_text" style="font-size:70%;">✗</span></th>
<th id="S4.T3.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T3.1.4.4.2.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></th>
<td id="S4.T3.1.4.4.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T3.1.4.4.3.1" class="ltx_text" style="font-size:70%;">64.03</span></td>
<td id="S4.T3.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T3.1.4.4.4.1" class="ltx_text" style="font-size:70%;">67.09</span></td>
<td id="S4.T3.1.4.4.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T3.1.4.4.5.1" class="ltx_text" style="font-size:70%;">65.56</span></td>
</tr>
<tr id="S4.T3.1.5.5" class="ltx_tr">
<th id="S4.T3.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.1.5.5.1.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></th>
<th id="S4.T3.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T3.1.5.5.2.1" class="ltx_text" style="font-size:70%;">✗</span></th>
<td id="S4.T3.1.5.5.3" class="ltx_td ltx_align_left"><span id="S4.T3.1.5.5.3.1" class="ltx_text" style="font-size:70%;">65.00</span></td>
<td id="S4.T3.1.5.5.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T3.1.5.5.4.1" class="ltx_text" style="font-size:70%;">68.15</span></td>
<td id="S4.T3.1.5.5.5" class="ltx_td ltx_align_left"><span id="S4.T3.1.5.5.5.1" class="ltx_text" style="font-size:70%;">65.95</span></td>
</tr>
<tr id="S4.T3.1.6.6" class="ltx_tr">
<th id="S4.T3.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.T3.1.6.6.1.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></th>
<th id="S4.T3.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T3.1.6.6.2.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></th>
<td id="S4.T3.1.6.6.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T3.1.6.6.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">69.39</span></td>
<td id="S4.T3.1.6.6.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S4.T3.1.6.6.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">70.37</span></td>
<td id="S4.T3.1.6.6.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T3.1.6.6.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">69.88</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Importance of human avatars, objects, and object attribute variations, evaluated on VL-Checklist and CLIP
</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablations</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We extensively ablate our <span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> synthetic data and the proposed VL models finetuning approach on this data according to the following points. We use the most popular CLIP model finetuned on our <span id="S4.SS4.p1.1.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> synthetic dataset evaluated on the largest of the benchmarks - the VL-Checklist to perform our ablations.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.1" class="ltx_p"><span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC<span id="S4.SS4.p2.1.1.1" class="ltx_text" style="color:#000000;"> - objects, humans, object attribute randomization</span></span> – we evaluate the major components that comprise our <span id="S4.SS4.p2.1.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> synthetic data, namely the importance of the synthetic data to contain humans performing various motions and actions, the importance of having objects with randomized attributes (Sec. <a href="#S3.SS1" title="3.1 Synthetic Data Generation ‣ 3 Method ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>), and the final result of having all types of data combined. The results of this ablation are summarized in Tab. <a href="#S4.T3" title="Table 3 ‣ 4.3 Results ‣ 4 Experiments ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. As expected, humans alone cannot teach the model the needed skills only improving relations VLC by a small margin. Additionally, having only objects with randomized attributes improves attribute VLC, yet only improves relations by <math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="1.4\%" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><mrow id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml"><mn id="S4.SS4.p2.1.m1.1.1.2" xref="S4.SS4.p2.1.m1.1.1.2.cmml">1.4</mn><mo id="S4.SS4.p2.1.m1.1.1.1" xref="S4.SS4.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><apply id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS4.p2.1.m1.1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p2.1.m1.1.1.2.cmml" xref="S4.SS4.p2.1.m1.1.1.2">1.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">1.4\%</annotation></semantics></math> which is also expected, as many of the relations involve human actions. The best result is observed on the combined dataset with all the components.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S4.T4.1.1.1.1.1" class="ltx_text" style="font-size:70%;">SURREAL</span></th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T4.1.1.1.2.1" class="ltx_text" style="font-size:70%;">Multi-Garment</span></th>
<td id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="S4.T4.1.1.1.3.1" class="ltx_text" style="font-size:70%;">VL Checklist</span></td>
</tr>
<tr id="S4.T4.1.2.2" class="ltx_tr">
<th id="S4.T4.1.2.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T4.1.2.2.2" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S4.T4.1.2.2.3" class="ltx_td ltx_align_left"><span id="S4.T4.1.2.2.3.1" class="ltx_text" style="font-size:70%;">Relation</span></td>
<td id="S4.T4.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T4.1.2.2.4.1" class="ltx_text" style="font-size:70%;">Attribute</span></td>
<td id="S4.T4.1.2.2.5" class="ltx_td ltx_align_left"><span id="S4.T4.1.2.2.5.1" class="ltx_text" style="font-size:70%;">Average</span></td>
</tr>
<tr id="S4.T4.1.3.3" class="ltx_tr">
<th id="S4.T4.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" colspan="2"><span id="S4.T4.1.3.3.1.1" class="ltx_text" style="font-size:70%;">CLIP</span></th>
<td id="S4.T4.1.3.3.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T4.1.3.3.2.1" class="ltx_text" style="font-size:70%;">63.57</span></td>
<td id="S4.T4.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T4.1.3.3.3.1" class="ltx_text" style="font-size:70%;">67.51</span></td>
<td id="S4.T4.1.3.3.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T4.1.3.3.4.1" class="ltx_text" style="font-size:70%;">65.54</span></td>
</tr>
<tr id="S4.T4.1.4.4" class="ltx_tr">
<th id="S4.T4.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.1.4.4.1.1" class="ltx_text" style="font-size:70%;">✗</span></th>
<th id="S4.T4.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T4.1.4.4.2.1" class="ltx_text" style="font-size:70%;">✗</span></th>
<td id="S4.T4.1.4.4.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T4.1.4.4.3.1" class="ltx_text" style="font-size:70%;">67.56</span></td>
<td id="S4.T4.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T4.1.4.4.4.1" class="ltx_text" style="font-size:70%;">68.73</span></td>
<td id="S4.T4.1.4.4.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T4.1.4.4.5.1" class="ltx_text" style="font-size:70%;">68.15</span></td>
</tr>
<tr id="S4.T4.1.5.5" class="ltx_tr">
<th id="S4.T4.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T4.1.5.5.1.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></th>
<th id="S4.T4.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T4.1.5.5.2.1" class="ltx_text" style="font-size:70%;">✗</span></th>
<td id="S4.T4.1.5.5.3" class="ltx_td ltx_align_left"><span id="S4.T4.1.5.5.3.1" class="ltx_text" style="font-size:70%;">67.56</span></td>
<td id="S4.T4.1.5.5.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T4.1.5.5.4.1" class="ltx_text" style="font-size:70%;">67.26</span></td>
<td id="S4.T4.1.5.5.5" class="ltx_td ltx_align_left"><span id="S4.T4.1.5.5.5.1" class="ltx_text" style="font-size:70%;">67.41</span></td>
</tr>
<tr id="S4.T4.1.6.6" class="ltx_tr">
<th id="S4.T4.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.T4.1.6.6.1.1" class="ltx_text" style="font-size:70%;">✗</span></th>
<th id="S4.T4.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T4.1.6.6.2.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></th>
<td id="S4.T4.1.6.6.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T4.1.6.6.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">69.39</span></td>
<td id="S4.T4.1.6.6.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S4.T4.1.6.6.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">70.37</span></td>
<td id="S4.T4.1.6.6.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T4.1.6.6.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">69.88</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Importance of human avatar clothing choices between SURREAL, Multi-Garment, and simple color textures (corresponding to none), evaluated on VL-Checklist and CLIP</figcaption>
</figure>
<div id="S4.SS4.p3" class="ltx_para ltx_noindent">
<p id="S4.SS4.p3.1" class="ltx_p"><span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC<span id="S4.SS4.p3.1.1.1" class="ltx_text" style="color:#000000;"> - human clothing</span></span> – we evaluate the diversity of human clothing comparing 3 levels of diversity: (i) none - using only a uniform color for human models; (ii) basic - using less diverse texture maps from SURREAL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>; and (iii) most diverse - using texture maps from Multi-Garment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, enriched with clothing colors, human age, and hair color annotations (manually done by us for the textures) which increase captions’ expressivity. Results are presented in Table <a href="#S4.T4" title="Table 4 ‣ 4.4 Ablations ‣ 4 Experiments ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. As expected, the most diverse human textures deliver the best result underlining the importance of this factor. Surprisingly, better human textures improve VL-Checklist Relations metric performance, likely due to the significantly better realism of the Multi-Garment textures.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para ltx_noindent">
<p id="S4.SS4.p4.1" class="ltx_p"><span id="S4.SS4.p4.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC<span id="S4.SS4.p4.1.1.1" class="ltx_text" style="color:#000000;"> - types of object attributes to randomize</span></span> – Table <a href="#S4.T5" title="Table 5 ‣ 4.4 Ablations ‣ 4 Experiments ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> examines how randomizing different object attributes affects performance. Specifically, we evaluate the randomization of size, material, and color. Interestingly, we find that the best performance is achieved without color randomization. We suspect it is due to unnatural color-object combinations that arise under such randomization, which teach the model wrong beliefs on real objects’ color distributions and go against true object-color associations existing in the VL model following pre-training on the original VL data.</p>
</div>
<div id="S4.SS4.p5" class="ltx_para ltx_noindent">
<p id="S4.SS4.p5.1" class="ltx_p"><span id="S4.SS4.p5.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC<span id="S4.SS4.p5.1.1.1" class="ltx_text" style="color:#000000;"> - types of captioning</span></span> – we have investigated several variants of ways to obtain textual captions from <span id="S4.SS4.p5.1.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> metadata (Sec. <a href="#S3.SS1" title="3.1 Synthetic Data Generation ‣ 3 Method ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>). Results are summarized
the Supplementary.
We compared our proposed metadata grammar-based approach to two cascade methods that paraphrase the captions resulting from the grammar using zero-shot LLM inference (in-context learning). The paraphrased caption is then appended to the original grammar-based caption and consumed through our caption-splitting module (as standalone, open LLM-based paraphrasing is not very high quality). As can be seen, currently paraphrasing has minimal effect, but we posit it will become an important tool as stronger LLMs will become openly available.</p>
</div>
<div id="S4.SS4.p6" class="ltx_para ltx_noindent">
<p id="S4.SS4.p6.1" class="ltx_p"><span id="S4.SS4.p6.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC<span id="S4.SS4.p6.1.1.1" class="ltx_text" style="color:#000000;"> - importance of physics and number of humans in the scene</span></span> – we also looked into to which extent reliable physical simulation (made available in <span id="S4.SS4.p6.1.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> through TDW and Unity capabilities) and human-human positional and other relations are important for the observed VL model improvements. In Table <a href="#S4.T6" title="Table 6 ‣ 4.4 Ablations ‣ 4 Experiments ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> we evaluate the effects of removing the physics (resulting in humans or objects floating in space) or removing the multi-human scenes (thus preventing all human-human relations from appearing in the data). As expected, both reliable physics simulation and human-human relations (interactions of sorts) are mostly important to the gains in the Relations metric.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S4.T5.1.1.1.1.1" class="ltx_text" style="font-size:70%;">color</span></th>
<th id="S4.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S4.T5.1.1.1.2.1" class="ltx_text" style="font-size:70%;">size</span></th>
<th id="S4.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T5.1.1.1.3.1" class="ltx_text" style="font-size:70%;">material</span></th>
<td id="S4.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="S4.T5.1.1.1.4.1" class="ltx_text" style="font-size:70%;">VL Checklist</span></td>
</tr>
<tr id="S4.T5.1.2.2" class="ltx_tr">
<th id="S4.T5.1.2.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T5.1.2.2.2" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T5.1.2.2.3" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S4.T5.1.2.2.4" class="ltx_td ltx_align_left"><span id="S4.T5.1.2.2.4.1" class="ltx_text" style="font-size:70%;">Relation</span></td>
<td id="S4.T5.1.2.2.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T5.1.2.2.5.1" class="ltx_text" style="font-size:70%;">Attribute</span></td>
<td id="S4.T5.1.2.2.6" class="ltx_td ltx_align_left"><span id="S4.T5.1.2.2.6.1" class="ltx_text" style="font-size:70%;">Average</span></td>
</tr>
<tr id="S4.T5.1.3.3" class="ltx_tr">
<th id="S4.T5.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" colspan="3"><span id="S4.T5.1.3.3.1.1" class="ltx_text" style="font-size:70%;">CLIP</span></th>
<td id="S4.T5.1.3.3.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T5.1.3.3.2.1" class="ltx_text" style="font-size:70%;">63.57</span></td>
<td id="S4.T5.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T5.1.3.3.3.1" class="ltx_text" style="font-size:70%;">67.51</span></td>
<td id="S4.T5.1.3.3.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T5.1.3.3.4.1" class="ltx_text" style="font-size:70%;">65.54</span></td>
</tr>
<tr id="S4.T5.1.4.4" class="ltx_tr">
<th id="S4.T5.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T5.1.4.4.1.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></th>
<th id="S4.T5.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T5.1.4.4.2.1" class="ltx_text" style="font-size:70%;">✗</span></th>
<th id="S4.T5.1.4.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T5.1.4.4.3.1" class="ltx_text" style="font-size:70%;">✗</span></th>
<td id="S4.T5.1.4.4.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T5.1.4.4.4.1" class="ltx_text" style="font-size:70%;">67.71</span></td>
<td id="S4.T5.1.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T5.1.4.4.5.1" class="ltx_text" style="font-size:70%;">64.61</span></td>
<td id="S4.T5.1.4.4.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T5.1.4.4.6.1" class="ltx_text" style="font-size:70%;">66.16</span></td>
</tr>
<tr id="S4.T5.1.5.5" class="ltx_tr">
<th id="S4.T5.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T5.1.5.5.1.1" class="ltx_text" style="font-size:70%;">✗</span></th>
<th id="S4.T5.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T5.1.5.5.2.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></th>
<th id="S4.T5.1.5.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T5.1.5.5.3.1" class="ltx_text" style="font-size:70%;">✗</span></th>
<td id="S4.T5.1.5.5.4" class="ltx_td ltx_align_left"><span id="S4.T5.1.5.5.4.1" class="ltx_text" style="font-size:70%;">68.58</span></td>
<td id="S4.T5.1.5.5.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T5.1.5.5.5.1" class="ltx_text" style="font-size:70%;">68.23</span></td>
<td id="S4.T5.1.5.5.6" class="ltx_td ltx_align_left"><span id="S4.T5.1.5.5.6.1" class="ltx_text" style="font-size:70%;">68.40</span></td>
</tr>
<tr id="S4.T5.1.6.6" class="ltx_tr">
<th id="S4.T5.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T5.1.6.6.1.1" class="ltx_text" style="font-size:70%;">✗</span></th>
<th id="S4.T5.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T5.1.6.6.2.1" class="ltx_text" style="font-size:70%;">✗</span></th>
<th id="S4.T5.1.6.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T5.1.6.6.3.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></th>
<td id="S4.T5.1.6.6.4" class="ltx_td ltx_align_left"><span id="S4.T5.1.6.6.4.1" class="ltx_text" style="font-size:70%;">65.23</span></td>
<td id="S4.T5.1.6.6.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T5.1.6.6.5.1" class="ltx_text" style="font-size:70%;">67.01</span></td>
<td id="S4.T5.1.6.6.6" class="ltx_td ltx_align_left"><span id="S4.T5.1.6.6.6.1" class="ltx_text" style="font-size:70%;">66.12</span></td>
</tr>
<tr id="S4.T5.1.7.7" class="ltx_tr">
<th id="S4.T5.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T5.1.7.7.1.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></th>
<th id="S4.T5.1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T5.1.7.7.2.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></th>
<th id="S4.T5.1.7.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T5.1.7.7.3.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></th>
<td id="S4.T5.1.7.7.4" class="ltx_td ltx_align_left"><span id="S4.T5.1.7.7.4.1" class="ltx_text" style="font-size:70%;">66.67</span></td>
<td id="S4.T5.1.7.7.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T5.1.7.7.5.1" class="ltx_text" style="font-size:70%;">65.97</span></td>
<td id="S4.T5.1.7.7.6" class="ltx_td ltx_align_left"><span id="S4.T5.1.7.7.6.1" class="ltx_text" style="font-size:70%;">66.32</span></td>
</tr>
<tr id="S4.T5.1.8.8" class="ltx_tr">
<th id="S4.T5.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.T5.1.8.8.1.1" class="ltx_text" style="font-size:70%;">✗</span></th>
<th id="S4.T5.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.T5.1.8.8.2.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></th>
<th id="S4.T5.1.8.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T5.1.8.8.3.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></th>
<td id="S4.T5.1.8.8.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T5.1.8.8.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">69.39</span></td>
<td id="S4.T5.1.8.8.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S4.T5.1.8.8.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">70.37</span></td>
<td id="S4.T5.1.8.8.6" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T5.1.8.8.6.1" class="ltx_text ltx_font_bold" style="font-size:70%;">69.88</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>Importance of different kinds of object attributes randomization, evaluated on VL-Checklist and CLIP</figcaption>
</figure>
<div id="S4.SS4.p7" class="ltx_para ltx_noindent">
<p id="S4.SS4.p7.1" class="ltx_p"><span id="S4.SS4.p7.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC<span id="S4.SS4.p7.1.1.1" class="ltx_text" style="color:#000000;"> - number of models and number of samples</span></span> – for lack of space, this is explored in the Supplementary.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T6.1.1.1" class="ltx_tr">
<th id="S4.T6.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S4.T6.1.1.1.1.1" class="ltx_text" style="font-size:70%;">physics</span></th>
<th id="S4.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T6.1.1.1.2.1" class="ltx_text" style="font-size:70%;">multi-human</span></th>
<td id="S4.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="S4.T6.1.1.1.3.1" class="ltx_text" style="font-size:70%;">VL Checklist</span></td>
</tr>
<tr id="S4.T6.1.2.2" class="ltx_tr">
<th id="S4.T6.1.2.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T6.1.2.2.2" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S4.T6.1.2.2.3" class="ltx_td ltx_align_left"><span id="S4.T6.1.2.2.3.1" class="ltx_text" style="font-size:70%;">Relation</span></td>
<td id="S4.T6.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T6.1.2.2.4.1" class="ltx_text" style="font-size:70%;">Attribute</span></td>
<td id="S4.T6.1.2.2.5" class="ltx_td ltx_align_left"><span id="S4.T6.1.2.2.5.1" class="ltx_text" style="font-size:70%;">Average</span></td>
</tr>
<tr id="S4.T6.1.3.3" class="ltx_tr">
<th id="S4.T6.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" colspan="2"><span id="S4.T6.1.3.3.1.1" class="ltx_text" style="font-size:70%;">CLIP</span></th>
<td id="S4.T6.1.3.3.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T6.1.3.3.2.1" class="ltx_text" style="font-size:70%;">63.57</span></td>
<td id="S4.T6.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T6.1.3.3.3.1" class="ltx_text" style="font-size:70%;">67.51</span></td>
<td id="S4.T6.1.3.3.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T6.1.3.3.4.1" class="ltx_text" style="font-size:70%;">65.54</span></td>
</tr>
<tr id="S4.T6.1.4.4" class="ltx_tr">
<th id="S4.T6.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T6.1.4.4.1.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></th>
<th id="S4.T6.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T6.1.4.4.2.1" class="ltx_text" style="font-size:70%;">✗</span></th>
<td id="S4.T6.1.4.4.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T6.1.4.4.3.1" class="ltx_text" style="font-size:70%;">67.66</span></td>
<td id="S4.T6.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T6.1.4.4.4.1" class="ltx_text" style="font-size:70%;">69.24</span></td>
<td id="S4.T6.1.4.4.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T6.1.4.4.5.1" class="ltx_text" style="font-size:70%;">68.45</span></td>
</tr>
<tr id="S4.T6.1.5.5" class="ltx_tr">
<th id="S4.T6.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T6.1.5.5.1.1" class="ltx_text" style="font-size:70%;">✗</span></th>
<th id="S4.T6.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T6.1.5.5.2.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></th>
<td id="S4.T6.1.5.5.3" class="ltx_td ltx_align_left"><span id="S4.T6.1.5.5.3.1" class="ltx_text" style="font-size:70%;">65.91</span></td>
<td id="S4.T6.1.5.5.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T6.1.5.5.4.1" class="ltx_text" style="font-size:70%;">69.01</span></td>
<td id="S4.T6.1.5.5.5" class="ltx_td ltx_align_left"><span id="S4.T6.1.5.5.5.1" class="ltx_text" style="font-size:70%;">67.46</span></td>
</tr>
<tr id="S4.T6.1.6.6" class="ltx_tr">
<th id="S4.T6.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.T6.1.6.6.1.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></th>
<th id="S4.T6.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T6.1.6.6.2.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></th>
<td id="S4.T6.1.6.6.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T6.1.6.6.3.1" class="ltx_text" style="font-size:70%;">69.39</span></td>
<td id="S4.T6.1.6.6.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S4.T6.1.6.6.4.1" class="ltx_text" style="font-size:70%;">70.37</span></td>
<td id="S4.T6.1.6.6.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T6.1.6.6.5.1" class="ltx_text" style="font-size:70%;">69.88</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 6: </span>Importance of physics simulation and multi-human relations in <span id="S4.T6.6.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span>, evaluated on VL-Checklist and CLIP</figcaption>
</figure>
<figure id="S4.T7" class="ltx_table">
<table id="S4.T7.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T7.1.2.1" class="ltx_tr">
<td id="S4.T7.1.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span id="S4.T7.1.2.1.1.1" class="ltx_text" style="font-size:70%;">#</span></td>
<td id="S4.T7.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T7.1.2.1.2.1" class="ltx_text" style="font-size:70%;">LoRA</span></td>
<td id="S4.T7.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T7.1.2.1.3.1" class="ltx_text" style="font-size:70%;">freezing</span></td>
<td id="S4.T7.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T7.1.2.1.4.1" class="ltx_text" style="font-size:70%;">model</span></td>
<td id="S4.T7.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T7.1.2.1.5.1" class="ltx_text" style="font-size:70%;">DA</span></td>
<td id="S4.T7.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T7.1.2.1.6.1" class="ltx_text" style="font-size:70%;">Caption</span></td>
<td id="S4.T7.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3"><span id="S4.T7.1.2.1.7.1" class="ltx_text" style="font-size:70%;">VL Checklist</span></td>
<td id="S4.T7.1.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T7.1.2.1.8.1" class="ltx_text" style="font-size:70%;">ARO</span></td>
<td id="S4.T7.1.2.1.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T7.1.2.1.9.1" class="ltx_text" style="font-size:70%;">ZS</span></td>
</tr>
<tr id="S4.T7.1.1" class="ltx_tr">
<td id="S4.T7.1.1.2" class="ltx_td ltx_border_r"></td>
<td id="S4.T7.1.1.3" class="ltx_td"></td>
<td id="S4.T7.1.1.1" class="ltx_td ltx_align_center"><math id="S4.T7.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{E}_{I}" display="inline"><semantics id="S4.T7.1.1.1.m1.1a"><msub id="S4.T7.1.1.1.m1.1.1" xref="S4.T7.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="S4.T7.1.1.1.m1.1.1.2" xref="S4.T7.1.1.1.m1.1.1.2.cmml">ℰ</mi><mi mathsize="70%" id="S4.T7.1.1.1.m1.1.1.3" xref="S4.T7.1.1.1.m1.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T7.1.1.1.m1.1b"><apply id="S4.T7.1.1.1.m1.1.1.cmml" xref="S4.T7.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T7.1.1.1.m1.1.1.1.cmml" xref="S4.T7.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T7.1.1.1.m1.1.1.2.cmml" xref="S4.T7.1.1.1.m1.1.1.2">ℰ</ci><ci id="S4.T7.1.1.1.m1.1.1.3.cmml" xref="S4.T7.1.1.1.m1.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.1.1.1.m1.1c">\mathcal{E}_{I}</annotation></semantics></math></td>
<td id="S4.T7.1.1.4" class="ltx_td ltx_align_center"><span id="S4.T7.1.1.4.1" class="ltx_text" style="font-size:70%;">averaging</span></td>
<td id="S4.T7.1.1.5" class="ltx_td ltx_align_center"><span id="S4.T7.1.1.5.1" class="ltx_text" style="font-size:70%;">styl.</span></td>
<td id="S4.T7.1.1.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T7.1.1.6.1" class="ltx_text" style="font-size:70%;">split emb.</span></td>
<td id="S4.T7.1.1.7" class="ltx_td ltx_align_left"><span id="S4.T7.1.1.7.1" class="ltx_text" style="font-size:70%;">Rel.</span></td>
<td id="S4.T7.1.1.8" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T7.1.1.8.1" class="ltx_text" style="font-size:70%;">Attr.</span></td>
<td id="S4.T7.1.1.9" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T7.1.1.9.1" class="ltx_text" style="font-size:70%;">Avg.</span></td>
<td id="S4.T7.1.1.10" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T7.1.1.10.1" class="ltx_text" style="font-size:70%;">Avg.</span></td>
<td id="S4.T7.1.1.11" class="ltx_td ltx_align_center"><span id="S4.T7.1.1.11.1" class="ltx_text" style="font-size:70%;">(21 tasks)</span></td>
</tr>
<tr id="S4.T7.1.3.2" class="ltx_tr">
<td id="S4.T7.1.3.2.1" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T7.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="5"><span id="S4.T7.1.3.2.2.1" class="ltx_text" style="font-size:70%;">CLIP</span></td>
<td id="S4.T7.1.3.2.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T7.1.3.2.3.1" class="ltx_text" style="font-size:70%;">63.57</span></td>
<td id="S4.T7.1.3.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T7.1.3.2.4.1" class="ltx_text" style="font-size:70%;">67.51</span></td>
<td id="S4.T7.1.3.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T7.1.3.2.5.1" class="ltx_text" style="font-size:70%;">65.54</span></td>
<td id="S4.T7.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.1.3.2.6.1" class="ltx_text" style="font-size:70%;">57.17</span></td>
<td id="S4.T7.1.3.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.3.2.7.1" class="ltx_text" style="font-size:70%;">56.07</span></td>
</tr>
<tr id="S4.T7.1.4.3" class="ltx_tr">
<td id="S4.T7.1.4.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T7.1.4.3.1.1" class="ltx_text" style="font-size:70%;">1</span></td>
<td id="S4.T7.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.4.3.2.1" class="ltx_text" style="font-size:70%;">✗</span></td>
<td id="S4.T7.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.4.3.3.1" class="ltx_text" style="font-size:70%;">✗</span></td>
<td id="S4.T7.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.4.3.4.1" class="ltx_text" style="font-size:70%;">✗</span></td>
<td id="S4.T7.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.4.3.5.1" class="ltx_text" style="font-size:70%;">✗</span></td>
<td id="S4.T7.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.1.4.3.6.1" class="ltx_text" style="font-size:70%;">✗</span></td>
<td id="S4.T7.1.4.3.7" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T7.1.4.3.7.1" class="ltx_text" style="font-size:70%;">67.10</span></td>
<td id="S4.T7.1.4.3.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T7.1.4.3.8.1" class="ltx_text" style="font-size:70%;">65.45</span></td>
<td id="S4.T7.1.4.3.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T7.1.4.3.9.1" class="ltx_text" style="font-size:70%;">66.28</span></td>
<td id="S4.T7.1.4.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.1.4.3.10.1" class="ltx_text" style="font-size:70%;">62.83</span></td>
<td id="S4.T7.1.4.3.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.4.3.11.1" class="ltx_text" style="font-size:70%;">53.84</span></td>
</tr>
<tr id="S4.T7.1.5.4" class="ltx_tr">
<td id="S4.T7.1.5.4.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T7.1.5.4.1.1" class="ltx_text" style="font-size:70%;">2</span></td>
<td id="S4.T7.1.5.4.2" class="ltx_td ltx_align_center"><span id="S4.T7.1.5.4.2.1" class="ltx_text" style="font-size:70%;">✗</span></td>
<td id="S4.T7.1.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T7.1.5.4.3.1" class="ltx_text" style="font-size:70%;">✗</span></td>
<td id="S4.T7.1.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T7.1.5.4.4.1" class="ltx_text" style="font-size:70%;">✗</span></td>
<td id="S4.T7.1.5.4.5" class="ltx_td ltx_align_center"><span id="S4.T7.1.5.4.5.1" class="ltx_text" style="font-size:70%;">✗</span></td>
<td id="S4.T7.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T7.1.5.4.6.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></td>
<td id="S4.T7.1.5.4.7" class="ltx_td ltx_align_left"><span id="S4.T7.1.5.4.7.1" class="ltx_text" style="font-size:70%;">67.76</span></td>
<td id="S4.T7.1.5.4.8" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T7.1.5.4.8.1" class="ltx_text" style="font-size:70%;">67.51</span></td>
<td id="S4.T7.1.5.4.9" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T7.1.5.4.9.1" class="ltx_text" style="font-size:70%;">67.64</span></td>
<td id="S4.T7.1.5.4.10" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T7.1.5.4.10.1" class="ltx_text" style="font-size:70%;">59.27</span></td>
<td id="S4.T7.1.5.4.11" class="ltx_td ltx_align_center"><span id="S4.T7.1.5.4.11.1" class="ltx_text" style="font-size:70%;">53.87</span></td>
</tr>
<tr id="S4.T7.1.6.5" class="ltx_tr">
<td id="S4.T7.1.6.5.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T7.1.6.5.1.1" class="ltx_text" style="font-size:70%;">3</span></td>
<td id="S4.T7.1.6.5.2" class="ltx_td ltx_align_center"><span id="S4.T7.1.6.5.2.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></td>
<td id="S4.T7.1.6.5.3" class="ltx_td ltx_align_center"><span id="S4.T7.1.6.5.3.1" class="ltx_text" style="font-size:70%;">✗</span></td>
<td id="S4.T7.1.6.5.4" class="ltx_td ltx_align_center"><span id="S4.T7.1.6.5.4.1" class="ltx_text" style="font-size:70%;">✗</span></td>
<td id="S4.T7.1.6.5.5" class="ltx_td ltx_align_center"><span id="S4.T7.1.6.5.5.1" class="ltx_text" style="font-size:70%;">✗</span></td>
<td id="S4.T7.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T7.1.6.5.6.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></td>
<td id="S4.T7.1.6.5.7" class="ltx_td ltx_align_left"><span id="S4.T7.1.6.5.7.1" class="ltx_text" style="font-size:70%;color:#0000FF;">69.32</span></td>
<td id="S4.T7.1.6.5.8" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T7.1.6.5.8.1" class="ltx_text" style="font-size:70%;color:#0000FF;">69.46</span></td>
<td id="S4.T7.1.6.5.9" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T7.1.6.5.9.1" class="ltx_text" style="font-size:70%;color:#0000FF;">69.39</span></td>
<td id="S4.T7.1.6.5.10" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T7.1.6.5.10.1" class="ltx_text" style="font-size:70%;color:#0000FF;">64.34</span></td>
<td id="S4.T7.1.6.5.11" class="ltx_td ltx_align_center"><span id="S4.T7.1.6.5.11.1" class="ltx_text" style="font-size:70%;">53.16</span></td>
</tr>
<tr id="S4.T7.1.7.6" class="ltx_tr">
<td id="S4.T7.1.7.6.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T7.1.7.6.1.1" class="ltx_text" style="font-size:70%;">4</span></td>
<td id="S4.T7.1.7.6.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.7.6.2.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></td>
<td id="S4.T7.1.7.6.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.7.6.3.1" class="ltx_text" style="font-size:70%;">✗</span></td>
<td id="S4.T7.1.7.6.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.7.6.4.1" class="ltx_text" style="font-size:70%;">✗</span></td>
<td id="S4.T7.1.7.6.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.7.6.5.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></td>
<td id="S4.T7.1.7.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.1.7.6.6.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></td>
<td id="S4.T7.1.7.6.7" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T7.1.7.6.7.1" class="ltx_text ltx_font_bold" style="font-size:70%;">69.39</span></td>
<td id="S4.T7.1.7.6.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T7.1.7.6.8.1" class="ltx_text ltx_font_bold" style="font-size:70%;">70.37</span></td>
<td id="S4.T7.1.7.6.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T7.1.7.6.9.1" class="ltx_text ltx_font_bold" style="font-size:70%;">69.88</span></td>
<td id="S4.T7.1.7.6.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.1.7.6.10.1" class="ltx_text ltx_font_bold" style="font-size:70%;">67.09</span></td>
<td id="S4.T7.1.7.6.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.7.6.11.1" class="ltx_text" style="font-size:70%;color:#0000FF;">55.27</span></td>
</tr>
<tr id="S4.T7.1.8.7" class="ltx_tr">
<td id="S4.T7.1.8.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T7.1.8.7.1.1" class="ltx_text" style="font-size:70%;">5</span></td>
<td id="S4.T7.1.8.7.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.8.7.2.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></td>
<td id="S4.T7.1.8.7.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.8.7.3.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></td>
<td id="S4.T7.1.8.7.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.8.7.4.1" class="ltx_text" style="font-size:70%;">✗</span></td>
<td id="S4.T7.1.8.7.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.8.7.5.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></td>
<td id="S4.T7.1.8.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.1.8.7.6.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></td>
<td id="S4.T7.1.8.7.7" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T7.1.8.7.7.1" class="ltx_text" style="font-size:70%;">63.54</span></td>
<td id="S4.T7.1.8.7.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T7.1.8.7.8.1" class="ltx_text" style="font-size:70%;">68.20</span></td>
<td id="S4.T7.1.8.7.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T7.1.8.7.9.1" class="ltx_text" style="font-size:70%;">65.87</span></td>
<td id="S4.T7.1.8.7.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T7.1.8.7.10.1" class="ltx_text" style="font-size:70%;">60.29</span></td>
<td id="S4.T7.1.8.7.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.1.8.7.11.1" class="ltx_text" style="font-size:70%;">54.54</span></td>
</tr>
<tr id="S4.T7.1.9.8" class="ltx_tr">
<td id="S4.T7.1.9.8.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T7.1.9.8.1.1" class="ltx_text" style="font-size:70%;">6</span></td>
<td id="S4.T7.1.9.8.2" class="ltx_td ltx_align_center"><span id="S4.T7.1.9.8.2.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></td>
<td id="S4.T7.1.9.8.3" class="ltx_td ltx_align_center"><span id="S4.T7.1.9.8.3.1" class="ltx_text" style="font-size:70%;">✗</span></td>
<td id="S4.T7.1.9.8.4" class="ltx_td ltx_align_center"><span id="S4.T7.1.9.8.4.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></td>
<td id="S4.T7.1.9.8.5" class="ltx_td ltx_align_center"><span id="S4.T7.1.9.8.5.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></td>
<td id="S4.T7.1.9.8.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T7.1.9.8.6.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></td>
<td id="S4.T7.1.9.8.7" class="ltx_td ltx_align_left"><span id="S4.T7.1.9.8.7.1" class="ltx_text" style="font-size:70%;">66.70</span></td>
<td id="S4.T7.1.9.8.8" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T7.1.9.8.8.1" class="ltx_text" style="font-size:70%;color:#0000FF;">69.62</span></td>
<td id="S4.T7.1.9.8.9" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T7.1.9.8.9.1" class="ltx_text" style="font-size:70%;">68.16</span></td>
<td id="S4.T7.1.9.8.10" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T7.1.9.8.10.1" class="ltx_text" style="font-size:70%;">63.76</span></td>
<td id="S4.T7.1.9.8.11" class="ltx_td ltx_align_center"><span id="S4.T7.1.9.8.11.1" class="ltx_text ltx_font_bold" style="font-size:70%;">55.72</span></td>
</tr>
<tr id="S4.T7.1.10.9" class="ltx_tr">
<td id="S4.T7.1.10.9.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S4.T7.1.10.9.1.1" class="ltx_text" style="font-size:70%;">7</span></td>
<td id="S4.T7.1.10.9.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.1.10.9.2.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></td>
<td id="S4.T7.1.10.9.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.1.10.9.3.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></td>
<td id="S4.T7.1.10.9.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.1.10.9.4.1" class="ltx_text" style="font-size:70%;">✗</span></td>
<td id="S4.T7.1.10.9.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.1.10.9.5.1" class="ltx_text" style="font-size:70%;color:#0000FF;">✓</span></td>
<td id="S4.T7.1.10.9.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T7.1.10.9.6.1" class="ltx_text" style="font-size:70%;">✗</span></td>
<td id="S4.T7.1.10.9.7" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T7.1.10.9.7.1" class="ltx_text" style="font-size:70%;">65.16</span></td>
<td id="S4.T7.1.10.9.8" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S4.T7.1.10.9.8.1" class="ltx_text" style="font-size:70%;">66.88</span></td>
<td id="S4.T7.1.10.9.9" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="S4.T7.1.10.9.9.1" class="ltx_text" style="font-size:70%;">66.02</span></td>
<td id="S4.T7.1.10.9.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T7.1.10.9.10.1" class="ltx_text" style="font-size:70%;">57.55</span></td>
<td id="S4.T7.1.10.9.11" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T7.1.10.9.11.1" class="ltx_text" style="font-size:70%;">52.69</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 7: </span>Importance of the finetuning recipe components, evaluated on VL-Checklist and CLIP</figcaption>
</figure>
<div id="S4.SS4.p8" class="ltx_para ltx_noindent">
<p id="S4.SS4.p8.1" class="ltx_p"><span id="S4.SS4.p8.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC<span id="S4.SS4.p8.1.1.1" class="ltx_text" style="color:#000000;"> - finetuning recipe components</span></span> – in Table <a href="#S4.T7" title="Table 7 ‣ 4.4 Ablations ‣ 4 Experiments ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> we extensively evaluate the different components of the proposed finetuning approach on <span id="S4.SS4.p8.1.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> that leads to significant improvements on VL-Checklist, ARO, and Winoground VLC and compositional reasoning metrics. We start with vanilla CLIP finetuning on <span id="S4.SS4.p8.1.3" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> (row #1), already showing some improvement in VL-Checklist relations metrics, and on the ARO benchmark, yet losing to base CLIP on VL-Checklist attributes metrics. Adding our caption splitting module (Sec. <a href="#S3.SS2" title="3.2 Finetuning large-scale pre-trained VL models using synthetic data ‣ 3 Method ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>) allows handling long (arbitrary size) texts outputted by our metadata-driven grammar and consequently utilizes all the caption information re-gaining the attributes performance (row #2). Adding parameter-efficient finetuning (LoRA, Sec. <a href="#S3.SS2" title="3.2 Finetuning large-scale pre-trained VL models using synthetic data ‣ 3 Method ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>) regularizes finetuning by forcing smaller (low-rank, low-parameters) updates of the large-scale pre-trained CLIP model, consequently somewhat handling the expected domain gap between the synthetic data of <span id="S4.SS4.p8.1.4" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> and the downstream evaluation (real data) tasks. Notably, LoRA does not add any additional parameters to the model, all LoRA adapters are collapsed into the model weights after finetuning. Consequently, we observed significant improvements from adding LoRA in all metrics (row #3) with only minor degradation (0.7%) in ZS evaluation. With adding domain stylization (Sec. <a href="#S3.SS2" title="3.2 Finetuning large-scale pre-trained VL models using synthetic data ‣ 3 Method ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>) we observe the best results in all VLC and compositional reasoning metrics improving ARO by 2.8% and keeping (even slightly improving) the advantages on VL-Checklist. Next, we investigate the variations of our best approach (LoRA + domain stylization + caption splitting). First, we investigate a strategy inspired by the LiT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite> approach (row #5). Freezing the visual encoder <math id="S4.SS4.p8.1.m1.1" class="ltx_Math" alttext="\mathcal{E}_{I}" display="inline"><semantics id="S4.SS4.p8.1.m1.1a"><msub id="S4.SS4.p8.1.m1.1.1" xref="S4.SS4.p8.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.p8.1.m1.1.1.2" xref="S4.SS4.p8.1.m1.1.1.2.cmml">ℰ</mi><mi id="S4.SS4.p8.1.m1.1.1.3" xref="S4.SS4.p8.1.m1.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p8.1.m1.1b"><apply id="S4.SS4.p8.1.m1.1.1.cmml" xref="S4.SS4.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p8.1.m1.1.1.1.cmml" xref="S4.SS4.p8.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p8.1.m1.1.1.2.cmml" xref="S4.SS4.p8.1.m1.1.1.2">ℰ</ci><ci id="S4.SS4.p8.1.m1.1.1.3.cmml" xref="S4.SS4.p8.1.m1.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p8.1.m1.1c">\mathcal{E}_{I}</annotation></semantics></math> as expected provides a (small) boost in ZS performance, but the reduced plasticity of the model comes at the price of observing smaller (only 3.1%) improvements on ARO and almost no improvements on the VL-Checklist. This leads us to conclude, that freezing the visual encoder is not a good strategy for <span id="S4.SS4.p8.1.5" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> finetuning. Next, we check the model averaging strategy (Sec. <a href="#S3.SS2" title="3.2 Finetuning large-scale pre-trained VL models using synthetic data ‣ 3 Method ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>) inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> (row #6). This does a better job of mitigating ZS forgetting, while at the same time keeping more of the gains on VL-Checklist and ARO. We conclude that model averaging is a good strategy to complement <span id="S4.SS4.p8.1.6" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> finetuning, allowing a soft trade-off between mitigating ZS forgetting and VLC and compositionality metrics gains. Finally, we again explore the importance of caption splitting for the best finetuning configuration of <span id="S4.SS4.p8.1.7" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> (row #7) and re-confirm its significance as performance drops without it.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2303.17590/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_figure_panel ltx_img_square" width="207" height="190" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2303.17590/assets/x4.png" id="S4.F3.g2" class="ltx_graphics ltx_figure_panel ltx_img_square" width="230" height="192" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S4.F3.3.1" class="ltx_text ltx_font_bold">(left)</span> detailed evaluation of syn-CLIP on the 7 separate VL-Checklist <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> metrics; <span id="S4.F3.4.2" class="ltx_text ltx_font_bold">(right)</span> detailed evaluation of syn-CLIP on all the Compositional Tasks proposed in ARO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. </figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Summary &amp; Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Large vision and language models have dictated the status quo in computer vision and multimodal perception, achieving state-of-the-art results in a number of challenging benchmarks. However, existing models struggle with compositional reasoning and understanding concepts beyond object nouns, such as attributes and relationships. Our work has investigated, for the first time, whether synthetic data can be leveraged to mitigate these shortcomings. We proposed a data generation pipeline, used to create a million-scale dataset of synthetic images and accompanying captions, and an effective fine-tuning strategy with comprehensive analysis to enhance the compositional and concept understanding capabilities of multimodal models, without compromising their zero-shot classification performance.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Limitations.</span> While we have achieved quite promising results in three different benchmarks, our work has limitations. As an example, our graphics simulator has a simplified model of lighting, sensor noise, and reflectance functions compared to the real world, which may impact robustness to color constancy. We believe more advanced domain adaptation and rendering techniques are likely needed to further improve our results. We also think a more detailed study of the scaling laws for synthetic data is a great research direction to fully unlock the potential of our work.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Acknowledgements.</span><span id="S5.p3.1.2" class="ltx_text" style="font-size:90%;">
This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. FA8750-19-C-1001. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency (DARPA). This project was also partially supported by NSF Award IIS-2221943, ANR CorVis ANR-21-CE23-0003-01, and the MIT-IBM Watson AI Lab. We thank Jeremy Schwartz and Esther Alter for their helpful discussions and assistance with the TDW package.</span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Nikos Athanasiou, Mathis Petrovich, Michael J. Black, and Gül Varol.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Teach: Temporal action composition for 3d humans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">3DV</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Nikos Athanasiou, Mathis Petrovich, Michael J. Black, and Gül Varol.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Teach: Temporal action composition for 3d humans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2022 International Conference on 3D Vision (3DV)</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, pages
414–423, 2022.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Omri Avrahami, Dani Lischinski, and Ohad Fried.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Blended diffusion for text-driven editing of natural images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 18208–18218, 2022.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti
Aggarwal, Subhojit Som, Songhao Piao, and Furu Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">VLMo: Unified vision-language pre-training with
mixture-of-modality-experts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">2022.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt, and Gerard Pons-Moll.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Multi-garment net: Learning to dress 3d people from images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Computer Vision (ICCV)</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">.
IEEE, oct 2019.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Tim Brooks, Aleksander Holynski, and Alexei A Efros.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Instructpix2pix: Learning to follow image editing instructions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2211.09800</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Paola Cascante-Bonilla, Hui Wu, Letao Wang, Rogerio Feris, and Vicente Ordonez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Sim vqa: Exploring simulated environments for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2022 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 5046–5056, 2022.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson,
Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha
Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping
Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob
Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Scaling instruction-finetuned language models, 2022.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
César Roberto De Souza, Adrien Gaidon, Yohann Cabon, and Antonio M.
López Peña.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Procedural generation of videos to train deep action recognition
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Anuj Diwan, Layne Berry, Eunsol Choi, David Harwath, and Kyle Mahowald.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Why is winoground hard? investigating failures in visuolinguistic
compositionality.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 2236–2250, Abu Dhabi, United Arab
Emirates, Dec. 2022. Association for Computational Linguistics.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang,
Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, Zicheng Liu, and
Michael Zeng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">An empirical study of training end-to-end vision-and-language
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Sivan Doveh, Assaf Arbelle, Sivan Harary, Rameswar Panda, Roei Herzig, Eli
Schwartz, Donghyun Kim, Raja Giryes, Rogerio Feris, Shimon Ullman, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Teaching structured vision&amp;language concepts to vision&amp;language
models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2211.11733</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Salehe Erfanian Ebadi, You-Cyuan Jhang, Alex Zook, Saurav Dhakad, Adam Crespi,
Pete Parisi, Steve Borkman, Jonathan Hogins, and Sujoy Ganguly.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Peoplesanspeople: A synthetic data generator for human-centric
computer vision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">2021.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Chuang Gan, Jeremy Schwartz, Seth Alter, Martin Schrimpf, James Traer,
Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi
Sano, Kuno Kim, Elias Wang, Damian Mrowca, Michael Lingelbach, Aidan Curtis,
Kevin T. Feigelis, Daniel Bear, Dan Gutfreund, David Cox, James J. DiCarlo,
Josh H. McDermott, Joshua B. Tenenbaum, and Daniel L. K. Yamins.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Threedworld: A platform for interactive multi-modal physical
simulation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, abs/2007.04954, 2020.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Domain-adversarial training of neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">The journal of machine learning research</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 17(1):2096–2030,
2016.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Yuting Gao, Jinfeng Liu, Zihan Xu, Jun Zhang, Ke Li, and Chunhua Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Pyramidclip: Hierarchical feature alignment for vision-language model
pretraining.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2204.14095</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
L. A. Gatys, A. S. Ecker, M. Bethge, A. Hertzmann, and E. Shechtman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Controlling perceptual factors in neural style transfer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 3730–3738, Los Alamitos, CA, USA, jul 2017. IEEE
Computer Society.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Shashank Goel, Hritik Bansal, Sumit Bhatia, Ryan A Rossi, Vishwa Vinay, and
Aditya Grover.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Cyclip: Cyclic contrastive language-image pretraining.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2205.14459</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel
Cohen-Or.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Prompt-to-prompt image editing with cross attention control.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2208.01626</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Lora: Low-rank adaptation of large language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2106.09685</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Xun Huang and Serge Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Arbitrary style transfer in real-time with adaptive instance
normalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Drew A Hudson and Christopher D Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Gqa: A new dataset for real-world visual reasoning and compositional
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 6700–6709, 2019.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V.
Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Scaling up visual and vision-language representation learning with
noisy text supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar
Mosseri, and Michal Irani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Imagic: Text-based real image editing with diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2210.09276</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Donghyun Kim, Kaihong Wang, Kate Saenko, Margrit Betke, and Stan Sclaroff.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">A unified framework for domain adaptive pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXIII</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages
603–620. Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Diffusionclip: Text-guided diffusion models for robust image
manipulation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages 2426–2435, 2022.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Diederik P Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1412.6980</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International journal of computer vision</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, 123(1):32–73, 2017.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">HMDB: a large video database for human motion recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Computer
Vision (ICCV)</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Chunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan Zhang, Jyoti Aneja,
Jianwei Yang, Ping Jin, Yong Jae Lee, Houdong Hu, Zicheng Liu, and Jianfeng
Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Elevater: A benchmark and toolkit for evaluating language-augmented
visual models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neural Information Processing Systems</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2201.12086</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao,
Fengwei Yu, and Junjie Yan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Supervision exists everywhere: A data efficient contrastive
language-image pre-training paradigm.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2110.05208</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Yong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang, Yue Xu, Mingyang Chen, Ze Ma,
Shiyi Wang, Hao-Shu Fang, and Cewu Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Hake: Human activity knowledge engine.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1904.06539</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2014: 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings, Part V 13</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 740–755.
Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J
Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Smpl: A skinned multi-person linear model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM transactions on graphics (TOG)</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, 34(6):1–16, 2015.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J.
Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">SMPL: A skinned multi-person linear model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM Trans. Graphics (Proc. SIGGRAPH Asia)</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">, 34(6):248:1–248:16,
Oct. 2015.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Ilya Loshchilov and Frank Hutter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Decoupled weight decay regularization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1711.05101</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Ilya Loshchilov and Frank Hutter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">SGDR: Stochastic gradient descent with warm restarts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and
Michael J. Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">AMASS: Archive of motion capture as surface shapes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Computer Vision</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, pages
5442–5451, Oct. 2019.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Samarth Mishra, Rameswar Panda, Cheng Perng Phoo, Chun-Fu Richard Chen, Leonid
Karlinsky, Kate Saenko, Venkatesh Saligrama, and Rogerio S Feris.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Task2sim: Towards effective pre-training and transfer from synthetic
data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages 9194–9204, 2022.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Null-text inversion for editing real images using guided diffusion
models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2211.09794</span><span id="bib.bib41.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella
Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey
Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak,
Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Crosslingual generalization through multitask finetuning, 2022.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and
Jun-Yan Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Zero-shot image-to-image translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2302.03027</span><span id="bib.bib43.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A.
Osman, Dimitrios Tzionas, and Michael J. Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Expressive body capture: 3D hands, face, and body from a single
image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, pages 10975–10985, 2019.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate
Saenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Visda: The visual domain adaptation challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1710.06924</span><span id="bib.bib45.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Mathis Petrovich, Michael J. Black, and Gül Varol.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">TEMOS: Generating diverse human motions from textual descriptions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">2022.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Khoi Pham, Kushal Kafle, Zhe Lin, Zhihong Ding, Scott Cohen, Quan Tran, and
Abhinav Shrivastava.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Learning to predict visual attributes in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, pages 13018–13028, 2021.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Sarah Pratt, Mark Yatskar, Luca Weihs, Ali Farhadi, and Aniruddha Kembhavi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Grounded situation recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, pages 314–332.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Abhinanda R. Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra
Quiros-Ramirez, and Michael J. Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">BABEL: Bodies, action and behavior with English labels.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">2021.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language
supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, pages
8748–8763. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Playing for data: Ground truth from computer games.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2016: 14th European Conference,
Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">,
pages 102–118. Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M
Lopez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">The synthia dataset: A large collection of synthetic images for
semantic segmentation of urban scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, pages 3234–3243, 2016.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross
Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell
Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig
Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Laion-5b: An open large-scale dataset for training next generation
image-text models, 2022.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
James Seale Smith, Paola Cascante-Bonilla, Assaf Arbelle, Donghyun Kim,
Rameswar Panda, David Cox, Diyi Yang, Zsolt Kira, Rogerio Feris, and Leonid
Karlinsky.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Construct-vl: Data-free continual structured vl concepts learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv e-prints</span><span id="bib.bib54.4.2" class="ltx_text" style="font-size:90%;">, pages arXiv–2211, 2022.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Joshua Susskind, Wenda Wang,
and Russell Webb.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Learning from simulated and unsupervised images through adversarial
training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:90%;">, pages 2107–2116, 2017.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech
Galuba, Marcus Rohrbach, and Douwe Kiela.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Flava: A foundational language and vision alignment model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib56.5.3" class="ltx_text" style="font-size:90%;">, pages 15638–15650, 2022.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe
Kiela, and Candace Ross.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Winoground: Probing vision and language models for visio-linguistic
compositionality.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib57.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib57.5.3" class="ltx_text" style="font-size:90%;">, pages 5238–5248, 2022.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Adversarial discriminative domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib58.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib58.5.3" class="ltx_text" style="font-size:90%;">, pages 7167–7176, 2017.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Carnegie Mellon University.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">CMU graphics lab motion capture database.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://mocap.cs.cmu.edu/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://mocap.cs.cmu.edu/</a><span id="bib.bib59.3.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
Gül Varol, Ivan Laptev, Cordelia Schmid, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">Synthetic humans for action recognition from unseen viewpoints.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCV</span><span id="bib.bib60.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
Gül Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J. Black,
Ivan Laptev, and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">Learning from synthetic humans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib61.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib61.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:90%;">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe
Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
and Alexander M. Rush.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.2.1" class="ltx_text" style="font-size:90%;">Transformers: State-of-the-art natural language processing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib62.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</span><span id="bib.bib62.5.3" class="ltx_text" style="font-size:90%;">, pages 38–45, Online,
Oct. 2020. Association for Computational Linguistics.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text" style="font-size:90%;">
Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith,
Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi,
Hongseok Namkoong, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.2.1" class="ltx_text" style="font-size:90%;">Robust fine-tuning of zero-shot models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib63.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib63.5.3" class="ltx_text" style="font-size:90%;">, pages 7959–7971, 2022.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text" style="font-size:90%;">
Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan
Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.2.1" class="ltx_text" style="font-size:90%;">Filip: Fine-grained interactive language-image pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2111.07783</span><span id="bib.bib64.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text" style="font-size:90%;">
Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.2.1" class="ltx_text" style="font-size:90%;">From image descriptions to visual denotations: New similarity
metrics for semantic inference over event descriptions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Transactions of the Association for Computational Linguistics</span><span id="bib.bib65.4.2" class="ltx_text" style="font-size:90%;">,
2:67–78, 02 2014.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text" style="font-size:90%;">
Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James
Zou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.2.1" class="ltx_text" style="font-size:90%;">When and why vision-language models behave like bags-of-words, and
what to do about it?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib66.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The Eleventh International Conference on Learning
Representations</span><span id="bib.bib66.5.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text" style="font-size:90%;">
Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-Fu Chang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.2.1" class="ltx_text" style="font-size:90%;">Open-vocabulary object detection using captions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib67.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib67.5.3" class="ltx_text" style="font-size:90%;">, pages 14393–14402, 2021.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text" style="font-size:90%;">
Yan Zeng, Xinsong Zhang, and Hang Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.2.1" class="ltx_text" style="font-size:90%;">Multi-grained vision language pre-training: Aligning texts with
visual concepts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2111.08276</span><span id="bib.bib68.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text" style="font-size:90%;">
Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers,
Alexander Kolesnikov, and Lucas Beyer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.2.1" class="ltx_text" style="font-size:90%;">Lit: Zero-shot transfer with locked-image text tuning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib69.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib69.5.3" class="ltx_text" style="font-size:90%;">, pages 18123–18133, 2022.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text" style="font-size:90%;">
Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan Shen, Kyusong Lee, Xiaopeng
Lu, and Jianwei Yin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.2.1" class="ltx_text" style="font-size:90%;">Vl-checklist: Evaluating pre-trained vision-language models with
objects, attributes and relations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2207.00221</span><span id="bib.bib70.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text" style="font-size:90%;">
Yuyang Zhao, Zhun Zhong, Na Zhao, Nicu Sebe, and Gim Hee Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.2.1" class="ltx_text" style="font-size:90%;">Style-hallucinated dual consistency learning: A unified framework for
visual domain generalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2212.09068</span><span id="bib.bib71.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text" style="font-size:90%;">
Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.2.1" class="ltx_text" style="font-size:90%;">Domain generalization with mixstyle.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2104.02008</span><span id="bib.bib72.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock"><span id="bib.bib73.1.1" class="ltx_text" style="font-size:90%;">
Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.2.1" class="ltx_text" style="font-size:90%;">Domain generalization with mixstyle.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib73.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib73.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Ax1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">Appendix</h2>

<div id="Ax1.p1" class="ltx_para">
<p id="Ax1.p1.1" class="ltx_p">In this supplementary material, we share our code and provide additional insights and experimental results that were not included in the main paper due to space constraints. In Section <a href="#A1" title="Appendix A Code ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> we describe the implementation code for generating <span id="Ax1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> and the code for the proposed finetuning approach on <span id="Ax1.p1.1.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span>.
In Section <a href="#A2" title="Appendix B Expanding VL-Checklist [70] analysis to most recent VL models ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>, we analyze the performance of recently open-sourced VL models on VL-Checklist <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> and show they have low performance, demonstrating the need for our improvements.
In Section <a href="#A3" title="Appendix C Winoground Results for CyCLIP [18] ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>, we provide additional results on Winoground <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> using CyCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> (excluded from the main text for space purposes).
Section <a href="#A4" title="Appendix D Improving BLIP [31] using SyViC ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a> demonstrates how we can improve BLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> using <span id="Ax1.p1.1.3" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span>.
In Section <a href="#A5" title="Appendix E Exploring a combination with text augmentation methods ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>, we combine our contributions with those of <span id="Ax1.p1.1.4" class="ltx_text ltx_font_italic">concurrent</span> work of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and demonstrate that our approach for improving VLC using synthetic data is <span id="Ax1.p1.1.5" class="ltx_text ltx_font_italic">orthogonal / complementary</span> to the text-augmentation based methods.
Section <a href="#A6" title="Appendix F Metadata-driven caption text synthesis, more details ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a> provides more dataset details, describing how the metadata from each synthesized scene is used to generate a caption for each image in <span id="Ax1.p1.1.6" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span>.
In Section <a href="#A7" title="Appendix G LLM-Based Caption Paraphrasing ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">G</span></a>, we explore combinations of our metadata-driven grammar-based caption generation with paraphrasing using openly available large language models. Section <a href="#A8" title="Appendix H Exploration into Synthetic Data Diversity ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">H</span></a> provides ”<span id="Ax1.p1.1.7" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> - number of models and number of samples” ablation excluded from the main paper due to lack of space.
Finally, in Section <a href="#A9" title="Appendix I Some Qualitative Examples with Synthetic Humans ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> we provide some randomly sampled examples from <span id="Ax1.p1.1.8" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span>.</p>
</div>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Code</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Our code for both <span id="A1.p1.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> data synthesis and the proposed finetuning approach is included in our project page: <a target="_blank" href="https://synthetic-vic.github.io/" title="" class="ltx_ref ltx_href">https://synthetic-vic.github.io/</a></p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Expanding VL-Checklist <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> analysis to most recent VL models</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">As promised in the footnote in the introduction we have evaluated the very recently released open-source VL models, namely: METER (CVPR 22) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, X-VLM (ICML 22) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, and VLMO (NeurIPS 22) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
on the most extensive VLC understanding benchmark of VL-Checklist <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> observing average performance of 56.8%, 58.9%, and 54.6%
respectively. As noted in the introduction, this relatively low VLC understanding performance (below CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>) of the newest (open) VL models illustrates once again the very much needed improvement in this aspect. Consequently, it also underlines the importance of <span id="A2.p1.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> and the proposed finetuning approach for administering some of this improvement and highlighting the future potential of our approach and synthetic data in general for the VL modeling. We additionally explore the very recent BLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> model and how it could be improved using <span id="A2.p1.1.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> and our approach in Section <a href="#A4" title="Appendix D Improving BLIP [31] using SyViC ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Winoground Results for CyCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">As promised in the main paper (lines 555-556), we include the Winoground <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> results of CyCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> not included in the main paper for lack of space. The results are included in Table <a href="#A3.T8" title="Table 8 ‣ Appendix C Winoground Results for CyCLIP [18] ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> and, compared to the CyCLIP baseline, demonstrate stable improvements of up to <math id="A3.p1.1.m1.1" class="ltx_Math" alttext="1.17\%" display="inline"><semantics id="A3.p1.1.m1.1a"><mrow id="A3.p1.1.m1.1.1" xref="A3.p1.1.m1.1.1.cmml"><mn id="A3.p1.1.m1.1.1.2" xref="A3.p1.1.m1.1.1.2.cmml">1.17</mn><mo id="A3.p1.1.m1.1.1.1" xref="A3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A3.p1.1.m1.1b"><apply id="A3.p1.1.m1.1.1.cmml" xref="A3.p1.1.m1.1.1"><csymbol cd="latexml" id="A3.p1.1.m1.1.1.1.cmml" xref="A3.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="A3.p1.1.m1.1.1.2.cmml" xref="A3.p1.1.m1.1.1.2">1.17</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.1.m1.1c">1.17\%</annotation></semantics></math> group score for syn-CyCLIP finetuned on <span id="A3.p1.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> using our proposed approach.</p>
</div>
<figure id="A3.T8" class="ltx_table">
<table id="A3.T8.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A3.T8.1.1" class="ltx_tr">
<th id="A3.T8.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="A3.T8.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="3"><span id="A3.T8.1.1.3.1" class="ltx_text" style="font-size:90%;">Winoground</span></th>
<th id="A3.T8.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">
<span id="A3.T8.1.1.1.1" class="ltx_text" style="font-size:90%;">Winoground</span><sup id="A3.T8.1.1.1.2" class="ltx_sup"><span id="A3.T8.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">†</span></sup>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A3.T8.1.2.1" class="ltx_tr">
<th id="A3.T8.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="A3.T8.1.2.1.2" class="ltx_td ltx_align_left"><span id="A3.T8.1.2.1.2.1" class="ltx_text" style="font-size:90%;">Text</span></td>
<td id="A3.T8.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r"><span id="A3.T8.1.2.1.3.1" class="ltx_text" style="font-size:90%;">Image</span></td>
<td id="A3.T8.1.2.1.4" class="ltx_td ltx_align_left ltx_border_r"><span id="A3.T8.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Group</span></td>
<td id="A3.T8.1.2.1.5" class="ltx_td ltx_align_left"><span id="A3.T8.1.2.1.5.1" class="ltx_text" style="font-size:90%;">Text</span></td>
<td id="A3.T8.1.2.1.6" class="ltx_td ltx_align_left ltx_border_r"><span id="A3.T8.1.2.1.6.1" class="ltx_text" style="font-size:90%;">Image</span></td>
<td id="A3.T8.1.2.1.7" class="ltx_td ltx_align_left"><span id="A3.T8.1.2.1.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Group</span></td>
</tr>
<tr id="A3.T8.1.3.2" class="ltx_tr">
<th id="A3.T8.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="A3.T8.1.3.2.1.1" class="ltx_text" style="font-size:90%;">CyCLIP</span></th>
<td id="A3.T8.1.3.2.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T8.1.3.2.2.1" class="ltx_text" style="font-size:90%;">28.50</span></td>
<td id="A3.T8.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A3.T8.1.3.2.3.1" class="ltx_text" style="font-size:90%;">9.50</span></td>
<td id="A3.T8.1.3.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A3.T8.1.3.2.4.1" class="ltx_text" style="font-size:90%;">7.25</span></td>
<td id="A3.T8.1.3.2.5" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T8.1.3.2.5.1" class="ltx_text" style="font-size:90%;">32.16</span></td>
<td id="A3.T8.1.3.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A3.T8.1.3.2.6.1" class="ltx_text" style="font-size:90%;">11.11</span></td>
<td id="A3.T8.1.3.2.7" class="ltx_td ltx_align_left ltx_border_t"><span id="A3.T8.1.3.2.7.1" class="ltx_text" style="font-size:90%;">8.19</span></td>
</tr>
<tr id="A3.T8.1.4.3" class="ltx_tr">
<th id="A3.T8.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t"><span id="A3.T8.1.4.3.1.1" class="ltx_text" style="font-size:90%;">syn-CyCLIP</span></th>
<td id="A3.T8.1.4.3.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="A3.T8.1.4.3.2.1" class="ltx_text" style="font-size:90%;">30.00</span><span id="A3.T8.1.4.3.2.2" class="ltx_text" style="font-size:90%;color:#4EA34E;"> (1.5)</span>
</td>
<td id="A3.T8.1.4.3.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t">
<span id="A3.T8.1.4.3.3.1" class="ltx_text" style="font-size:90%;">10.75</span><span id="A3.T8.1.4.3.3.2" class="ltx_text" style="font-size:90%;color:#4EA34E;"> (1.25)</span>
</td>
<td id="A3.T8.1.4.3.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t">
<span id="A3.T8.1.4.3.4.1" class="ltx_text" style="font-size:90%;">8.25</span><span id="A3.T8.1.4.3.4.2" class="ltx_text" style="font-size:90%;color:#4EA34E;"> (1.0)</span>
</td>
<td id="A3.T8.1.4.3.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="A3.T8.1.4.3.5.1" class="ltx_text" style="font-size:90%;">30.99</span><span id="A3.T8.1.4.3.5.2" class="ltx_text" style="font-size:90%;color:#F18C8E;"> (-1.17)</span>
</td>
<td id="A3.T8.1.4.3.6" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t">
<span id="A3.T8.1.4.3.6.1" class="ltx_text" style="font-size:90%;">12.87</span><span id="A3.T8.1.4.3.6.2" class="ltx_text" style="font-size:90%;color:#4EA34E;"> (1.76)</span>
</td>
<td id="A3.T8.1.4.3.7" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="A3.T8.1.4.3.7.1" class="ltx_text" style="font-size:90%;">9.36</span><span id="A3.T8.1.4.3.7.2" class="ltx_text" style="font-size:90%;color:#4EA34E;"> (1.17)</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 8: </span>Winoground <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> performance of syn-CyCLIP – finetuned on <span id="A3.T8.16.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span>.
<sup id="A3.T8.17.2" class="ltx_sup"><span id="A3.T8.17.2.1" class="ltx_text ltx_font_italic">†</span></sup> ‘clean’ (no-tag) subset of valid Winoground samples from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Improving BLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> using <span id="A4.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span>
</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.4" class="ltx_p">BLIP is a recently released VL model achieving better out-of-the-box performance on VL-Checklist <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> and Winoground <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> compared to CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. In Table <a href="#A4.T9" title="Table 9 ‣ Appendix D Improving BLIP [31] using SyViC ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> we show how using our proposed <span id="A4.p1.4.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> dataset and the finetuning approach applied to BLIP, significant additional performance gains (<math id="A4.p1.1.m1.1" class="ltx_Math" alttext="1.48\%" display="inline"><semantics id="A4.p1.1.m1.1a"><mrow id="A4.p1.1.m1.1.1" xref="A4.p1.1.m1.1.1.cmml"><mn id="A4.p1.1.m1.1.1.2" xref="A4.p1.1.m1.1.1.2.cmml">1.48</mn><mo id="A4.p1.1.m1.1.1.1" xref="A4.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A4.p1.1.m1.1b"><apply id="A4.p1.1.m1.1.1.cmml" xref="A4.p1.1.m1.1.1"><csymbol cd="latexml" id="A4.p1.1.m1.1.1.1.cmml" xref="A4.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="A4.p1.1.m1.1.1.2.cmml" xref="A4.p1.1.m1.1.1.2">1.48</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.1.m1.1c">1.48\%</annotation></semantics></math> on VL-Checklist and up to <math id="A4.p1.2.m2.1" class="ltx_Math" alttext="4.67\%" display="inline"><semantics id="A4.p1.2.m2.1a"><mrow id="A4.p1.2.m2.1.1" xref="A4.p1.2.m2.1.1.cmml"><mn id="A4.p1.2.m2.1.1.2" xref="A4.p1.2.m2.1.1.2.cmml">4.67</mn><mo id="A4.p1.2.m2.1.1.1" xref="A4.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A4.p1.2.m2.1b"><apply id="A4.p1.2.m2.1.1.cmml" xref="A4.p1.2.m2.1.1"><csymbol cd="latexml" id="A4.p1.2.m2.1.1.1.cmml" xref="A4.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="A4.p1.2.m2.1.1.2.cmml" xref="A4.p1.2.m2.1.1.2">4.67</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.2.m2.1c">4.67\%</annotation></semantics></math> on Winoground group score) can be achieved.
BLIP is designed and optimized for VL understanding and generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, and has a relatively low zero-shot out-of-the-box performance compared to CLIP (e.g., we observed an over <math id="A4.p1.3.m3.1" class="ltx_Math" alttext="7\%" display="inline"><semantics id="A4.p1.3.m3.1a"><mrow id="A4.p1.3.m3.1.1" xref="A4.p1.3.m3.1.1.cmml"><mn id="A4.p1.3.m3.1.1.2" xref="A4.p1.3.m3.1.1.2.cmml">7</mn><mo id="A4.p1.3.m3.1.1.1" xref="A4.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A4.p1.3.m3.1b"><apply id="A4.p1.3.m3.1.1.cmml" xref="A4.p1.3.m3.1.1"><csymbol cd="latexml" id="A4.p1.3.m3.1.1.1.cmml" xref="A4.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="A4.p1.3.m3.1.1.2.cmml" xref="A4.p1.3.m3.1.1.2">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.3.m3.1c">7\%</annotation></semantics></math> drop in zero-shot comparing baseline BLIP to CLIP and similar drop for syn-BLIP compared to CLIP).
In more detail,
we employ the retrieval flow of BLIP starting from ViT/B and CapFilt-L base model and use it as the BLIP baseline in Table <a href="#A4.T9" title="Table 9 ‣ Appendix D Improving BLIP [31] using SyViC ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. We finetune BLIP on <span id="A4.p1.4.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> following the complete proposed recipe detailed in
Section <a href="#S3.SS2" title="3.2 Finetuning large-scale pre-trained VL models using synthetic data ‣ 3 Method ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>
of the main paper. We add rank-16 LoRa adapters to both BLIP encoders and the decoder (cross-attention layers in the text encoder).
We fine-tune for two epochs with a learning rate of 5e-6 using an Adam optimizer with a weight decay factor of <math id="A4.p1.4.m4.1" class="ltx_Math" alttext="0.05" display="inline"><semantics id="A4.p1.4.m4.1a"><mn id="A4.p1.4.m4.1.1" xref="A4.p1.4.m4.1.1.cmml">0.05</mn><annotation-xml encoding="MathML-Content" id="A4.p1.4.m4.1b"><cn type="float" id="A4.p1.4.m4.1.1.cmml" xref="A4.p1.4.m4.1.1">0.05</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.4.m4.1c">0.05</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
<figure id="A4.T9" class="ltx_table">
<table id="A4.T9.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.T9.1.1" class="ltx_tr">
<th id="A4.T9.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="A4.T9.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="3"><span id="A4.T9.1.1.3.1" class="ltx_text" style="font-size:70%;">VL Checklist</span></th>
<th id="A4.T9.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="3"><span id="A4.T9.1.1.4.1" class="ltx_text" style="font-size:70%;">Winoground</span></th>
<th id="A4.T9.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">
<span id="A4.T9.1.1.1.1" class="ltx_text" style="font-size:70%;">Winoground</span><sup id="A4.T9.1.1.1.2" class="ltx_sup"><span id="A4.T9.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:70%;">†</span></sup>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.T9.1.2.1" class="ltx_tr">
<th id="A4.T9.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="A4.T9.1.2.1.2" class="ltx_td ltx_align_left"><span id="A4.T9.1.2.1.2.1" class="ltx_text" style="font-size:70%;">Relation</span></td>
<td id="A4.T9.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r"><span id="A4.T9.1.2.1.3.1" class="ltx_text" style="font-size:70%;">Attribute</span></td>
<td id="A4.T9.1.2.1.4" class="ltx_td ltx_align_left ltx_border_r"><span id="A4.T9.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Average</span></td>
<td id="A4.T9.1.2.1.5" class="ltx_td ltx_align_left"><span id="A4.T9.1.2.1.5.1" class="ltx_text" style="font-size:70%;">Text</span></td>
<td id="A4.T9.1.2.1.6" class="ltx_td ltx_align_left ltx_border_r"><span id="A4.T9.1.2.1.6.1" class="ltx_text" style="font-size:70%;">Image</span></td>
<td id="A4.T9.1.2.1.7" class="ltx_td ltx_align_left ltx_border_r"><span id="A4.T9.1.2.1.7.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Group</span></td>
<td id="A4.T9.1.2.1.8" class="ltx_td ltx_align_left"><span id="A4.T9.1.2.1.8.1" class="ltx_text" style="font-size:70%;">Text</span></td>
<td id="A4.T9.1.2.1.9" class="ltx_td ltx_align_left ltx_border_r"><span id="A4.T9.1.2.1.9.1" class="ltx_text" style="font-size:70%;">Image</span></td>
<td id="A4.T9.1.2.1.10" class="ltx_td"></td>
</tr>
<tr id="A4.T9.1.3.2" class="ltx_tr">
<th id="A4.T9.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="A4.T9.1.3.2.1.1" class="ltx_text" style="font-size:70%;">BLIP</span></th>
<td id="A4.T9.1.3.2.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A4.T9.1.3.2.2.1" class="ltx_text" style="font-size:70%;">68.45</span></td>
<td id="A4.T9.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A4.T9.1.3.2.3.1" class="ltx_text" style="font-size:70%;">73.11</span></td>
<td id="A4.T9.1.3.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A4.T9.1.3.2.4.1" class="ltx_text" style="font-size:70%;">70.78</span></td>
<td id="A4.T9.1.3.2.5" class="ltx_td ltx_align_left ltx_border_t"><span id="A4.T9.1.3.2.5.1" class="ltx_text" style="font-size:70%;">38.00</span></td>
<td id="A4.T9.1.3.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A4.T9.1.3.2.6.1" class="ltx_text" style="font-size:70%;">18.25</span></td>
<td id="A4.T9.1.3.2.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A4.T9.1.3.2.7.1" class="ltx_text" style="font-size:70%;">14.50</span></td>
<td id="A4.T9.1.3.2.8" class="ltx_td ltx_align_left ltx_border_t"><span id="A4.T9.1.3.2.8.1" class="ltx_text" style="font-size:70%;">43.86</span></td>
<td id="A4.T9.1.3.2.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A4.T9.1.3.2.9.1" class="ltx_text" style="font-size:70%;">25.15</span></td>
<td id="A4.T9.1.3.2.10" class="ltx_td ltx_align_left ltx_border_t"><span id="A4.T9.1.3.2.10.1" class="ltx_text" style="font-size:70%;">21.06</span></td>
</tr>
<tr id="A4.T9.1.4.3" class="ltx_tr">
<th id="A4.T9.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t"><span id="A4.T9.1.4.3.1.1" class="ltx_text" style="font-size:70%;">syn-BLIP</span></th>
<td id="A4.T9.1.4.3.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="A4.T9.1.4.3.2.1" class="ltx_text" style="font-size:70%;">70.18</span><span id="A4.T9.1.4.3.2.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+1.73)</span>
</td>
<td id="A4.T9.1.4.3.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t">
<span id="A4.T9.1.4.3.3.1" class="ltx_text" style="font-size:70%;">75.34</span><span id="A4.T9.1.4.3.3.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+2.23)</span>
</td>
<td id="A4.T9.1.4.3.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t">
<span id="A4.T9.1.4.3.4.1" class="ltx_text" style="font-size:70%;">72.76</span><span id="A4.T9.1.4.3.4.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+1.48)</span>
</td>
<td id="A4.T9.1.4.3.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="A4.T9.1.4.3.5.1" class="ltx_text" style="font-size:70%;">43.25</span><span id="A4.T9.1.4.3.5.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+5.25)</span>
</td>
<td id="A4.T9.1.4.3.6" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t">
<span id="A4.T9.1.4.3.6.1" class="ltx_text" style="font-size:70%;">19.75</span><span id="A4.T9.1.4.3.6.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+1.5)</span>
</td>
<td id="A4.T9.1.4.3.7" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t">
<span id="A4.T9.1.4.3.7.1" class="ltx_text" style="font-size:70%;">16.75</span><span id="A4.T9.1.4.3.7.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+2.25)</span>
</td>
<td id="A4.T9.1.4.3.8" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="A4.T9.1.4.3.8.1" class="ltx_text" style="font-size:70%;">52.63</span><span id="A4.T9.1.4.3.8.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+8.77)</span>
</td>
<td id="A4.T9.1.4.3.9" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t">
<span id="A4.T9.1.4.3.9.1" class="ltx_text" style="font-size:70%;">29.82</span><span id="A4.T9.1.4.3.9.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+4.67)</span>
</td>
<td id="A4.T9.1.4.3.10" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="A4.T9.1.4.3.10.1" class="ltx_text" style="font-size:70%;">25.73</span><span id="A4.T9.1.4.3.10.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+4.67)</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 9: </span>Performance of syn-BLIP – finetuned on <span id="A4.T9.17.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> and evaluated on VL-Checklist and Winoground. <sup id="A4.T9.18.2" class="ltx_sup"><span id="A4.T9.18.2.1" class="ltx_text ltx_font_italic">†</span></sup> ‘clean’ (no-tag) subset of valid Winoground samples from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Gains and losses are highlighted in <span id="A4.T9.19.3" class="ltx_text" style="color:#4EA34E;">green</span> and <span id="A4.T9.20.4" class="ltx_text" style="color:#FF0000;">red</span> respectively.</figcaption>
</figure>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Exploring a combination with text augmentation methods</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.2" class="ltx_p">As discussed in lines 108-112 in the Introduction of the main paper, <span id="A5.p1.2.1" class="ltx_text ltx_font_italic">concurrent</span> works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> propose an <span id="A5.p1.2.2" class="ltx_text ltx_font_italic">orthogonal</span> approach of improving VLC understanding performance via using text augmentation while training on additional real VL paired image+text data. These works use language tools to teach a model the importance of non-noun words by manipulating them (replacing words with incorrect alternatives in the text captions of real image+text pairs) and adding the resulting texts to the same batch. In order to show that our proposed approach of improving the VLC understanding performance of VL models using targeted demonstration on <span id="A5.p1.2.3" class="ltx_text ltx_font_italic">both text and image side</span> via generating synthetic data (our <span id="A5.p1.2.4" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> dataset) is truly orthogonal and complementary to the text augmentation methods, we have conducted the following experiment whose results are summarized in Table <a href="#A5.T10" title="Table 10 ‣ Appendix E Exploring a combination with text augmentation methods ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. Specifically, we used <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> code, kindly shared to us by the authors, to combine our <span id="A5.p1.2.5" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> finetuning (as described in
Section <a href="#S3.SS2" title="3.2 Finetuning large-scale pre-trained VL models using synthetic data ‣ 3 Method ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>
of the main paper) with the LAION <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> experiment of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> using their text augmentation method both for the real data captions as well as for our <span id="A5.p1.2.6" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> synthetic data captions. More specifically, we finetune (using the method described in
Section <a href="#S3.SS2" title="3.2 Finetuning large-scale pre-trained VL models using synthetic data ‣ 3 Method ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>
in the main paper, also including <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> negative text augmentations and their additional losses for the negatives as detailed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>) on combined batches containing both LAION <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> text+image pairs and <span id="A5.p1.2.7" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> text+image pairs. The base model is CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> both for <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and syn-<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. As we can see in <a href="#A5.T10" title="Table 10 ‣ Appendix E Exploring a combination with text augmentation methods ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, syn-<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> significantly (up to <math id="A5.p1.1.m1.1" class="ltx_Math" alttext="12.26\%" display="inline"><semantics id="A5.p1.1.m1.1a"><mrow id="A5.p1.1.m1.1.1" xref="A5.p1.1.m1.1.1.cmml"><mn id="A5.p1.1.m1.1.1.2" xref="A5.p1.1.m1.1.1.2.cmml">12.26</mn><mo id="A5.p1.1.m1.1.1.1" xref="A5.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.1.m1.1b"><apply id="A5.p1.1.m1.1.1.cmml" xref="A5.p1.1.m1.1.1"><csymbol cd="latexml" id="A5.p1.1.m1.1.1.1.cmml" xref="A5.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="A5.p1.1.m1.1.1.2.cmml" xref="A5.p1.1.m1.1.1.2">12.26</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.1.m1.1c">12.26\%</annotation></semantics></math> on Relations and <math id="A5.p1.2.m2.1" class="ltx_Math" alttext="8.46\%" display="inline"><semantics id="A5.p1.2.m2.1a"><mrow id="A5.p1.2.m2.1.1" xref="A5.p1.2.m2.1.1.cmml"><mn id="A5.p1.2.m2.1.1.2" xref="A5.p1.2.m2.1.1.2.cmml">8.46</mn><mo id="A5.p1.2.m2.1.1.1" xref="A5.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.2.m2.1b"><apply id="A5.p1.2.m2.1.1.cmml" xref="A5.p1.2.m2.1.1"><csymbol cd="latexml" id="A5.p1.2.m2.1.1.1.cmml" xref="A5.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="A5.p1.2.m2.1.1.2.cmml" xref="A5.p1.2.m2.1.1.2">8.46</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.2.m2.1c">8.46\%</annotation></semantics></math> on average) improves the base <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> performance on VL-Checklist (trained on the same LAION data without <span id="A5.p1.2.8" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span>) and is roughly matching <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> performance on the ARO and zero-shot evaluations.</p>
</div>
<figure id="A5.T10" class="ltx_table">
<table id="A5.T10.9" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A5.T10.9.1.1" class="ltx_tr">
<th id="A5.T10.9.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt"></th>
<th id="A5.T10.9.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="3"><span id="A5.T10.9.1.1.2.1" class="ltx_text" style="font-size:70%;">VL Checklist</span></th>
<th id="A5.T10.9.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="5"><span id="A5.T10.9.1.1.3.1" class="ltx_text" style="font-size:70%;">ARO</span></th>
<th id="A5.T10.9.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="A5.T10.9.1.1.4.1" class="ltx_text" style="font-size:70%;">Zero-Short</span></th>
</tr>
<tr id="A5.T10.9.2.2" class="ltx_tr">
<th id="A5.T10.9.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_border_r"></th>
<th id="A5.T10.9.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="A5.T10.9.2.2.2.1" class="ltx_text" style="font-size:70%;">Relation</span></th>
<th id="A5.T10.9.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r"><span id="A5.T10.9.2.2.3.1" class="ltx_text" style="font-size:70%;">Attribute</span></th>
<th id="A5.T10.9.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r"><span id="A5.T10.9.2.2.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Average</span></th>
<th id="A5.T10.9.2.2.5" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="A5.T10.9.2.2.5.1" class="ltx_text" style="font-size:70%;">VG-Rel.</span></th>
<th id="A5.T10.9.2.2.6" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="A5.T10.9.2.2.6.1" class="ltx_text" style="font-size:70%;">VG-Att.</span></th>
<th id="A5.T10.9.2.2.7" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="A5.T10.9.2.2.7.1" class="ltx_text" style="font-size:70%;">Flickr30k</span></th>
<th id="A5.T10.9.2.2.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r"><span id="A5.T10.9.2.2.8.1" class="ltx_text" style="font-size:70%;">COCO</span></th>
<th id="A5.T10.9.2.2.9" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r"><span id="A5.T10.9.2.2.9.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Average</span></th>
<th id="A5.T10.9.2.2.10" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="A5.T10.9.2.2.10.1" class="ltx_text" style="font-size:70%;">(21 tasks)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A5.T10.9.3.1" class="ltx_tr">
<td id="A5.T10.9.3.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A5.T10.9.3.1.1.1" class="ltx_text" style="font-size:70%;">CLIP</span></td>
<td id="A5.T10.9.3.1.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T10.9.3.1.2.1" class="ltx_text" style="font-size:70%;">63.57</span></td>
<td id="A5.T10.9.3.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A5.T10.9.3.1.3.1" class="ltx_text" style="font-size:70%;">67.51</span></td>
<td id="A5.T10.9.3.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A5.T10.9.3.1.4.1" class="ltx_text" style="font-size:70%;">65.54</span></td>
<td id="A5.T10.9.3.1.5" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T10.9.3.1.5.1" class="ltx_text" style="font-size:70%;">58.84</span></td>
<td id="A5.T10.9.3.1.6" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T10.9.3.1.6.1" class="ltx_text" style="font-size:70%;">63.19</span></td>
<td id="A5.T10.9.3.1.7" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T10.9.3.1.7.1" class="ltx_text" style="font-size:70%;">47.20</span></td>
<td id="A5.T10.9.3.1.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A5.T10.9.3.1.8.1" class="ltx_text" style="font-size:70%;">59.46</span></td>
<td id="A5.T10.9.3.1.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A5.T10.9.3.1.9.1" class="ltx_text" style="font-size:70%;">57.17</span></td>
<td id="A5.T10.9.3.1.10" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T10.9.3.1.10.1" class="ltx_text" style="font-size:70%;">56.07</span></td>
</tr>
<tr id="A5.T10.9.4.2" class="ltx_tr">
<td id="A5.T10.9.4.2.1" class="ltx_td ltx_align_left ltx_border_r"><cite class="ltx_cite ltx_citemacro_cite"><span id="A5.T10.9.4.2.1.1.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="A5.T10.9.4.2.1.2.2" class="ltx_text" style="font-size:70%;">]</span></cite></td>
<td id="A5.T10.9.4.2.2" class="ltx_td ltx_align_left"><span id="A5.T10.9.4.2.2.1" class="ltx_text" style="font-size:70%;">66.05</span></td>
<td id="A5.T10.9.4.2.3" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T10.9.4.2.3.1" class="ltx_text" style="font-size:70%;">69.64</span></td>
<td id="A5.T10.9.4.2.4" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T10.9.4.2.4.1" class="ltx_text" style="font-size:70%;">67.85</span></td>
<td id="A5.T10.9.4.2.5" class="ltx_td ltx_align_left"><span id="A5.T10.9.4.2.5.1" class="ltx_text" style="font-size:70%;">80.64</span></td>
<td id="A5.T10.9.4.2.6" class="ltx_td ltx_align_left"><span id="A5.T10.9.4.2.6.1" class="ltx_text" style="font-size:70%;">72.81</span></td>
<td id="A5.T10.9.4.2.7" class="ltx_td ltx_align_left"><span id="A5.T10.9.4.2.7.1" class="ltx_text" style="font-size:70%;">92.82</span></td>
<td id="A5.T10.9.4.2.8" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T10.9.4.2.8.1" class="ltx_text" style="font-size:70%;">87.67</span></td>
<td id="A5.T10.9.4.2.9" class="ltx_td ltx_align_left ltx_border_r"><span id="A5.T10.9.4.2.9.1" class="ltx_text" style="font-size:70%;">83.48</span></td>
<td id="A5.T10.9.4.2.10" class="ltx_td ltx_align_left"><span id="A5.T10.9.4.2.10.1" class="ltx_text" style="font-size:70%;">56.71</span></td>
</tr>
<tr id="A5.T10.9.5.3" class="ltx_tr">
<td id="A5.T10.9.5.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A5.T10.9.5.3.1.1" class="ltx_text" style="font-size:70%;">syn-CLIP</span></td>
<td id="A5.T10.9.5.3.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="A5.T10.9.5.3.2.1" class="ltx_text" style="font-size:70%;">69.39</span><span id="A5.T10.9.5.3.2.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+5.82)</span>
</td>
<td id="A5.T10.9.5.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="A5.T10.9.5.3.3.1" class="ltx_text" style="font-size:70%;">70.37</span><span id="A5.T10.9.5.3.3.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+2.86)</span>
</td>
<td id="A5.T10.9.5.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="A5.T10.9.5.3.4.1" class="ltx_text" style="font-size:70%;">69.88</span><span id="A5.T10.9.5.3.4.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+4.34)</span>
</td>
<td id="A5.T10.9.5.3.5" class="ltx_td ltx_align_left ltx_border_t">
<span id="A5.T10.9.5.3.5.1" class="ltx_text" style="font-size:70%;">71.40</span><span id="A5.T10.9.5.3.5.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+12.56)</span>
</td>
<td id="A5.T10.9.5.3.6" class="ltx_td ltx_align_left ltx_border_t">
<span id="A5.T10.9.5.3.6.1" class="ltx_text" style="font-size:70%;">66.94</span><span id="A5.T10.9.5.3.6.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+3.75)</span>
</td>
<td id="A5.T10.9.5.3.7" class="ltx_td ltx_align_left ltx_border_t">
<span id="A5.T10.9.5.3.7.1" class="ltx_text" style="font-size:70%;">59.06</span><span id="A5.T10.9.5.3.7.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+11.86)</span>
</td>
<td id="A5.T10.9.5.3.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="A5.T10.9.5.3.8.1" class="ltx_text" style="font-size:70%;">70.96</span><span id="A5.T10.9.5.3.8.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+11.5)</span>
</td>
<td id="A5.T10.9.5.3.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<span id="A5.T10.9.5.3.9.1" class="ltx_text" style="font-size:70%;">67.09</span><span id="A5.T10.9.5.3.9.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+9.9)</span>
</td>
<td id="A5.T10.9.5.3.10" class="ltx_td ltx_align_left ltx_border_t">
<span id="A5.T10.9.5.3.10.1" class="ltx_text" style="font-size:70%;">55.27</span><span id="A5.T10.9.5.3.10.2" class="ltx_text" style="font-size:70%;color:#F18C8E;"> (-0.8)</span>
</td>
</tr>
<tr id="A5.T10.9.6.4" class="ltx_tr">
<td id="A5.T10.9.6.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">
<span id="A5.T10.9.6.4.1.1" class="ltx_text" style="font-size:70%;">syn-</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A5.T10.9.6.4.1.2.1" class="ltx_text" style="font-size:70%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="A5.T10.9.6.4.1.3.2" class="ltx_text" style="font-size:70%;">]</span></cite>
</td>
<td id="A5.T10.9.6.4.2" class="ltx_td ltx_align_left ltx_border_bb">
<span id="A5.T10.9.6.4.2.1" class="ltx_text" style="font-size:70%;">78.31</span><span id="A5.T10.9.6.4.2.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+12.26)</span>
</td>
<td id="A5.T10.9.6.4.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">
<span id="A5.T10.9.6.4.3.1" class="ltx_text" style="font-size:70%;">74.31</span><span id="A5.T10.9.6.4.3.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+7.67)</span>
</td>
<td id="A5.T10.9.6.4.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">
<span id="A5.T10.9.6.4.4.1" class="ltx_text" style="font-size:70%;">76.31</span><span id="A5.T10.9.6.4.4.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+8.46)</span>
</td>
<td id="A5.T10.9.6.4.5" class="ltx_td ltx_align_left ltx_border_bb">
<span id="A5.T10.9.6.4.5.1" class="ltx_text" style="font-size:70%;">80.79</span><span id="A5.T10.9.6.4.5.2" class="ltx_text" style="font-size:70%;color:#4EA34E;"> (+0.15)</span>
</td>
<td id="A5.T10.9.6.4.6" class="ltx_td ltx_align_left ltx_border_bb">
<span id="A5.T10.9.6.4.6.1" class="ltx_text" style="font-size:70%;">72.37</span><span id="A5.T10.9.6.4.6.2" class="ltx_text" style="font-size:70%;color:#F18C8E;"> (-0.44)</span>
</td>
<td id="A5.T10.9.6.4.7" class="ltx_td ltx_align_left ltx_border_bb">
<span id="A5.T10.9.6.4.7.1" class="ltx_text" style="font-size:70%;">92.44</span><span id="A5.T10.9.6.4.7.2" class="ltx_text" style="font-size:70%;color:#F18C8E;"> (-0.38)</span>
</td>
<td id="A5.T10.9.6.4.8" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">
<span id="A5.T10.9.6.4.8.1" class="ltx_text" style="font-size:70%;">87.19</span><span id="A5.T10.9.6.4.8.2" class="ltx_text" style="font-size:70%;color:#F18C8E;"> (-0.48)</span>
</td>
<td id="A5.T10.9.6.4.9" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">
<span id="A5.T10.9.6.4.9.1" class="ltx_text" style="font-size:70%;">83.20</span><span id="A5.T10.9.6.4.9.2" class="ltx_text" style="font-size:70%;color:#F18C8E;"> (-0.28)</span>
</td>
<td id="A5.T10.9.6.4.10" class="ltx_td ltx_align_left ltx_border_bb">
<span id="A5.T10.9.6.4.10.1" class="ltx_text" style="font-size:70%;">54.57</span><span id="A5.T10.9.6.4.10.2" class="ltx_text" style="font-size:70%;color:#F18C8E;"> (-2.14)</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 10: </span>
Demonstrating that text augmentation on real paired VL data training is orthogonal/complementary to our approach. Comparing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> performance finetuned on LAION with syn-<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> performance finetuned on LAION + <span id="A5.T10.59.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> using a combination of our approach in
Section <a href="#S3.SS2" title="3.2 Finetuning large-scale pre-trained VL models using synthetic data ‣ 3 Method ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>
of the main paper with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>’s negatives text augmentations and additional negative losses (added from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> original code kindly shared with us by the authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>). The base model is CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> both for <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and syn-<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
Results are evaluated on VL-Checklist <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> and ARO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.
Gains and losses are highlighted in <span id="A5.T10.60.2" class="ltx_text" style="color:#4EA34E;">green</span> and <span id="A5.T10.61.3" class="ltx_text" style="color:#FF0000;">red</span> respectively. syn-<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> is comparable on ARO (with <math id="A5.T10.5.m1.1" class="ltx_Math" alttext="0.28\%" display="inline"><semantics id="A5.T10.5.m1.1b"><mrow id="A5.T10.5.m1.1.1" xref="A5.T10.5.m1.1.1.cmml"><mn id="A5.T10.5.m1.1.1.2" xref="A5.T10.5.m1.1.1.2.cmml">0.28</mn><mo id="A5.T10.5.m1.1.1.1" xref="A5.T10.5.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A5.T10.5.m1.1c"><apply id="A5.T10.5.m1.1.1.cmml" xref="A5.T10.5.m1.1.1"><csymbol cd="latexml" id="A5.T10.5.m1.1.1.1.cmml" xref="A5.T10.5.m1.1.1.1">percent</csymbol><cn type="float" id="A5.T10.5.m1.1.1.2.cmml" xref="A5.T10.5.m1.1.1.2">0.28</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T10.5.m1.1d">0.28\%</annotation></semantics></math> difference) and significantly improving on VL-Checklist (with <math id="A5.T10.6.m2.1" class="ltx_Math" alttext="8.46\%" display="inline"><semantics id="A5.T10.6.m2.1b"><mrow id="A5.T10.6.m2.1.1" xref="A5.T10.6.m2.1.1.cmml"><mn id="A5.T10.6.m2.1.1.2" xref="A5.T10.6.m2.1.1.2.cmml">8.46</mn><mo id="A5.T10.6.m2.1.1.1" xref="A5.T10.6.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A5.T10.6.m2.1c"><apply id="A5.T10.6.m2.1.1.cmml" xref="A5.T10.6.m2.1.1"><csymbol cd="latexml" id="A5.T10.6.m2.1.1.1.cmml" xref="A5.T10.6.m2.1.1.1">percent</csymbol><cn type="float" id="A5.T10.6.m2.1.1.2.cmml" xref="A5.T10.6.m2.1.1.2">8.46</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T10.6.m2.1d">8.46\%</annotation></semantics></math> average improvement), while having only a small decrease on the zero-shot evaluation, only <math id="A5.T10.7.m3.1" class="ltx_Math" alttext="2.14\%" display="inline"><semantics id="A5.T10.7.m3.1b"><mrow id="A5.T10.7.m3.1.1" xref="A5.T10.7.m3.1.1.cmml"><mn id="A5.T10.7.m3.1.1.2" xref="A5.T10.7.m3.1.1.2.cmml">2.14</mn><mo id="A5.T10.7.m3.1.1.1" xref="A5.T10.7.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A5.T10.7.m3.1c"><apply id="A5.T10.7.m3.1.1.cmml" xref="A5.T10.7.m3.1.1"><csymbol cd="latexml" id="A5.T10.7.m3.1.1.1.cmml" xref="A5.T10.7.m3.1.1.1">percent</csymbol><cn type="float" id="A5.T10.7.m3.1.1.2.cmml" xref="A5.T10.7.m3.1.1.2">2.14</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T10.7.m3.1d">2.14\%</annotation></semantics></math> w.r.t. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and even smaller <math id="A5.T10.8.m4.1" class="ltx_Math" alttext="1.5\%" display="inline"><semantics id="A5.T10.8.m4.1b"><mrow id="A5.T10.8.m4.1.1" xref="A5.T10.8.m4.1.1.cmml"><mn id="A5.T10.8.m4.1.1.2" xref="A5.T10.8.m4.1.1.2.cmml">1.5</mn><mo id="A5.T10.8.m4.1.1.1" xref="A5.T10.8.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A5.T10.8.m4.1c"><apply id="A5.T10.8.m4.1.1.cmml" xref="A5.T10.8.m4.1.1"><csymbol cd="latexml" id="A5.T10.8.m4.1.1.1.cmml" xref="A5.T10.8.m4.1.1.1">percent</csymbol><cn type="float" id="A5.T10.8.m4.1.1.2.cmml" xref="A5.T10.8.m4.1.1.2">1.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T10.8.m4.1d">1.5\%</annotation></semantics></math> compared to base CLIP model.
</figcaption>
</figure>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Metadata-driven caption text synthesis, more details</h2>

<div id="A6.p1" class="ltx_para">
<p id="A6.p1.1" class="ltx_p">This section describes how the metadata from each synthesized scene is used to generate a caption for each image in <span id="A6.p1.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span>. We outline a rule-based mechanism to deterministically generate dense captions given:</p>
<ol id="A6.I1" class="ltx_enumerate">
<li id="A6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A6.I1.i1.p1" class="ltx_para">
<p id="A6.I1.i1.p1.1" class="ltx_p">List of the objects present in the scene, each with its corresponding name and world coordinates.</p>
</div>
</li>
<li id="A6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A6.I1.i2.p1" class="ltx_para">
<p id="A6.I1.i2.p1.1" class="ltx_p">List of humanoids present in the scene, each with its world coordinates, clothing identifier, and a textual description of the action it performs.</p>
</div>
</li>
<li id="A6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A6.I1.i3.p1" class="ltx_para">
<p id="A6.I1.i3.p1.1" class="ltx_p">Segmented image that has a label for each pixel corresponding to the object or humanoid it belongs to.</p>
</div>
</li>
<li id="A6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A6.I1.i4.p1" class="ltx_para">
<p id="A6.I1.i4.p1.1" class="ltx_p">Scene identifier that maps to a textual description of the scene.</p>
</div>
</li>
</ol>
</div>
<div id="A6.p2" class="ltx_para">
<p id="A6.p2.1" class="ltx_p">We annotate a list of 115 clothing textures from the Multi-Garment  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and SURREAL  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>. Clothing annotations include a list of textual descriptions such as the colors of the clothes, the hair/beard style, as well as any features that stand out such as logos, tattoos, and accessories. Additionally, we use the original scene descriptions provided by ThreeDWorld’s scene library.</p>
</div>
<div id="A6.p3" class="ltx_para">
<p id="A6.p3.1" class="ltx_p">To generate the description of the objects in the image, we use the (3D) world positions of the objects to create positional relations between them. For objects that are horizontally aligned, we generate a description of which object is to the left or right of the other by comparing their corresponding pixels in the segmentation image. Furthermore, we generate a description of which object is in front of the other by translating the world coordinates into camera coordinates and comparing their z-coordinates. Unique identifiers for names are used to as placeholders to obtain those relationships, and object names are filled in once all positional relations are established, using indefinite articles when necessary. This process is applied to every pair of objects present in the scene.</p>
</div>
<div id="A6.p4" class="ltx_para">
<p id="A6.p4.1" class="ltx_p">Furthermore, we generate descriptions of humans while referring to them using ordinal numbers. In particular, for each human present in the scene, we retrieve its action description and place it in a sentence (e.g. ”The {first} person {walks forward}”. Additionally, we retrieve the list of textual descriptions associated with the human’s clothing, if exist. We consider each text as a separate sentence.</p>
</div>
<div id="A6.p5" class="ltx_para">
<p id="A6.p5.1" class="ltx_p">Finally, we compile a list of sentences containing a caption prefix, an enumeration of the objects, the pairwise positional relations between objects, a scene description, and action and clothing descriptions for each human. We concatenate the sentences together to get a full dense caption of the image. A simplified pseudo-code for generating a caption is shown below:</p>
</div>
<figure id="LST1" class="ltx_float ltx_lstlisting">
<figcaption class="ltx_caption" style="font-size:70%;"><span class="ltx_tag ltx_tag_float">Listing 1: </span>General Code for Caption Generation</figcaption>
<div id="LST1.3" class="ltx_listing ltx_lst_language_Python ltx_lstlisting ltx_listing">
<div class="ltx_listing_data"><a href="data:text/plain;base64,ZGVmIHNhbXBsZV9wcm9tcHQob2JqZWN0cywgc2VnX2ltYWdlLCBzY2VuZV9uYW1lKToKICBzdGF0ZW1lbnRzID0gWyJUaGlzIHNjZW5lIGNvbnRhaW5zICJdCgogICMgT2JqZWN0cwogIG9iamVjdHNfc3RhdGVtZW50ID0gIiIKICBmb3Igb2JqIGluIG9iamVjdHM6CiAgICBhcnRpY2xlID0gZ2V0X2FydGljbGUob2JqLm9ial9uYW1lWzBdKQogICAgb2JqZWN0c19zdGF0ZW1lbnQgKz0gYXJ0aWNsZSArIGYiIHtvYmoub2JqX25hbWV9LCAiCiAgb2JqZWN0c19zdGF0ZW1lbnQgKz0gZiJhbmQge2xlbihodW1hbnMpfSBodW1hbnMuIgogIHN0YXRlbWVudHMuYXBwZW5kKG9iamVjdHNfc3RhdGVtZW50KQoKICAjIFNjZW5lIHN0YXRlbWVudAogIHNjZW5lX3N0YXRlbWVudCA9ICJUaGV5IGFyZSBpbiAiCiAgc2NlbmVfYXJ0aWNsZSA9IGdldF9hcnRpY2xlKHNjZW5lX25hbWUpCiAgc2NlbmVfZGVzY3JpcHRpb24gPSBnZXRfZGVzY3JpcHRpb24oc2NlbmVfZGVzY3JpcHRpb24pCiAgc2NlbmVfc3RhdGVtZW50ICs9IHNjZW5lX2FydGljbGUgKyBzY2VuZV9kZXNjcmlwdGlvbgogIHN0YXRlbWVudHMuYXBwZW5kKHNjZW5lX3N0YXRlbWVudCkKCiAgIyBQb3NpdGlvbmFsIHJlbGF0aW9ucwogIHJlbGF0aW9ucyA9IFtdCiAgbiA9IGxlbihsZW4ob2JqZWN0cykpCiAgZm9yIGkgaW4gcmFuZ2Uobik6CiAgICBmb3IgaiBpbiByYW5nZShpICsgMSwgbik6CiAgICAgIGxlZnQsIHJpZ2h0ID0gZ2V0X2xlZnRfcmlnaHQoc2VnX2ltYWdlLCBpLCBqKQogICAgICBmcm9udCwgYmFjayA9IGdldF9mcm9udF9iYWNrKGksIGopCiAgICAgIHJlbGF0aW9ucy5leHRlbmQoWwogICAgICAgIGxlZnQgKyAiIGlzIHRvIHRoZSBsZWZ0IG9mICIgKyByaWdodCwKICAgICAgICByaWdodCArICIgaXMgdG8gdGhlIHJpZ2h0IG9mICIgKyBsZWZ0LAogICAgICBdKQogICAgICByZWxhdGlvbnMuZXh0ZW5kKFsKICAgICAgICBmcm9udCArICIgaXMgaW4gZnJvbnQgb2YgIiArIGJhY2ssCiAgICAgICAgYmFjayArICIgaXMgYmVoaW5kICIgKyBmcm9udCwKICAgICAgXSkKICBzaHVmZmxlKHJlbGF0aW9ucykKICBzdGF0ZW1lbnRzLmV4dGVuZChyZWxhdGlvbnMpCgogICMgQ2xvdGhpbmcgYW5kIGFjdGlvbgogIGZvciBoIGluIGh1bWFuczoKICAgICMgQWN0aW9uCiAgICBzX2FjdGlvbiA9IGYiVGhlIHtoLm5hbWV9IHtoLmFjdGlvbn0uIgogICAgc3RhdGVtZW50cy5hcHBlbmQoc19hY3Rpb24pCgogICAgIyBDbG90aGluZwogICAgZm9yIHMgaW4gaC5jbG90aGU6CiAgICAgIHNfY2xvdGhlID0gZiJUaGUge2gubmFtZX0ge3Muc3RyaXAoKX0uIgogICAgICBzdGF0ZW1lbnRzLmFwcGVuZChzX2Nsb3RoZSkKCiAgcmV0dXJuICIgIi5qb2luKHN0YXRlbWVudHMpLnN0cmlwKCkK" download="">⬇</a></div>
<div id="lstnumberx1" class="ltx_listingline">
<span id="lstnumberx1.1" class="ltx_text ltx_lst_keyword ltx_font_bold" style="font-size:70%;">def</span><span id="lstnumberx1.2" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx1.3" class="ltx_text ltx_lst_identifier" style="font-size:70%;">sample_prompt</span><span id="lstnumberx1.4" class="ltx_text" style="font-size:70%;">(</span><span id="lstnumberx1.5" class="ltx_text ltx_lst_identifier" style="font-size:70%;">objects</span><span id="lstnumberx1.6" class="ltx_text" style="font-size:70%;">,</span><span id="lstnumberx1.7" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx1.8" class="ltx_text ltx_lst_identifier" style="font-size:70%;">seg_image</span><span id="lstnumberx1.9" class="ltx_text" style="font-size:70%;">,</span><span id="lstnumberx1.10" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx1.11" class="ltx_text ltx_lst_identifier" style="font-size:70%;">scene_name</span><span id="lstnumberx1.12" class="ltx_text" style="font-size:70%;">):</span>
</div>
<div id="lstnumberx2" class="ltx_listingline">
<span id="lstnumberx2.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx2.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">statements</span><span id="lstnumberx2.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx2.4" class="ltx_text" style="font-size:70%;">=</span><span id="lstnumberx2.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx2.6" class="ltx_text" style="font-size:70%;">[</span><span id="lstnumberx2.7" class="ltx_text ltx_lst_string" style="font-size:70%;">”This<span id="lstnumberx2.7.1" class="ltx_text ltx_lst_space">␣</span>scene<span id="lstnumberx2.7.2" class="ltx_text ltx_lst_space">␣</span>contains<span id="lstnumberx2.7.3" class="ltx_text ltx_lst_space">␣</span>”</span><span id="lstnumberx2.8" class="ltx_text" style="font-size:70%;">]</span>
</div>
<div id="lstnumberx3" class="ltx_listingline">
</div>
<div id="lstnumberx4" class="ltx_listingline">
<span id="lstnumberx4.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx4.2" class="ltx_text ltx_lst_comment ltx_font_italic" style="font-size:70%;">#<span id="lstnumberx4.2.1" class="ltx_text ltx_lst_space"> </span>Objects</span>
</div>
<div id="lstnumberx5" class="ltx_listingline">
<span id="lstnumberx5.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx5.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">objects_statement</span><span id="lstnumberx5.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx5.4" class="ltx_text" style="font-size:70%;">=</span><span id="lstnumberx5.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx5.6" class="ltx_text ltx_lst_string" style="font-size:70%;">””</span>
</div>
<div id="lstnumberx6" class="ltx_listingline">
<span id="lstnumberx6.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx6.2" class="ltx_text ltx_lst_keyword ltx_font_bold" style="font-size:70%;">for</span><span id="lstnumberx6.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx6.4" class="ltx_text ltx_lst_identifier" style="font-size:70%;">obj</span><span id="lstnumberx6.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx6.6" class="ltx_text ltx_lst_keyword ltx_font_bold" style="font-size:70%;">in</span><span id="lstnumberx6.7" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx6.8" class="ltx_text ltx_lst_identifier" style="font-size:70%;">objects</span><span id="lstnumberx6.9" class="ltx_text" style="font-size:70%;">:</span>
</div>
<div id="lstnumberx7" class="ltx_listingline">
<span id="lstnumberx7.1" class="ltx_text ltx_lst_space" style="font-size:70%;">    </span><span id="lstnumberx7.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">article</span><span id="lstnumberx7.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx7.4" class="ltx_text" style="font-size:70%;">=</span><span id="lstnumberx7.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx7.6" class="ltx_text ltx_lst_identifier" style="font-size:70%;">get_article</span><span id="lstnumberx7.7" class="ltx_text" style="font-size:70%;">(</span><span id="lstnumberx7.8" class="ltx_text ltx_lst_identifier" style="font-size:70%;">obj</span><span id="lstnumberx7.9" class="ltx_text" style="font-size:70%;">.</span><span id="lstnumberx7.10" class="ltx_text ltx_lst_identifier" style="font-size:70%;">obj_name</span><span id="lstnumberx7.11" class="ltx_text" style="font-size:70%;">[0])</span>
</div>
<div id="lstnumberx8" class="ltx_listingline">
<span id="lstnumberx8.1" class="ltx_text ltx_lst_space" style="font-size:70%;">    </span><span id="lstnumberx8.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">objects_statement</span><span id="lstnumberx8.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx8.4" class="ltx_text" style="font-size:70%;">+=</span><span id="lstnumberx8.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx8.6" class="ltx_text ltx_lst_identifier" style="font-size:70%;">article</span><span id="lstnumberx8.7" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx8.8" class="ltx_text" style="font-size:70%;">+</span><span id="lstnumberx8.9" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx8.10" class="ltx_text ltx_lst_identifier" style="font-size:70%;">f</span><span id="lstnumberx8.11" class="ltx_text ltx_lst_string" style="font-size:70%;">”<span id="lstnumberx8.11.1" class="ltx_text ltx_lst_space">␣</span>{obj.obj_name},<span id="lstnumberx8.11.2" class="ltx_text ltx_lst_space">␣</span>”</span>
</div>
<div id="lstnumberx9" class="ltx_listingline">
<span id="lstnumberx9.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx9.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">objects_statement</span><span id="lstnumberx9.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx9.4" class="ltx_text" style="font-size:70%;">+=</span><span id="lstnumberx9.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx9.6" class="ltx_text ltx_lst_identifier" style="font-size:70%;">f</span><span id="lstnumberx9.7" class="ltx_text ltx_lst_string" style="font-size:70%;">”and<span id="lstnumberx9.7.1" class="ltx_text ltx_lst_space">␣</span>{len(humans)}<span id="lstnumberx9.7.2" class="ltx_text ltx_lst_space">␣</span>humans.”</span>
</div>
<div id="lstnumberx10" class="ltx_listingline">
<span id="lstnumberx10.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx10.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">statements</span><span id="lstnumberx10.3" class="ltx_text" style="font-size:70%;">.</span><span id="lstnumberx10.4" class="ltx_text ltx_lst_identifier" style="font-size:70%;">append</span><span id="lstnumberx10.5" class="ltx_text" style="font-size:70%;">(</span><span id="lstnumberx10.6" class="ltx_text ltx_lst_identifier" style="font-size:70%;">objects_statement</span><span id="lstnumberx10.7" class="ltx_text" style="font-size:70%;">)</span>
</div>
<div id="lstnumberx11" class="ltx_listingline">
</div>
<div id="lstnumberx12" class="ltx_listingline">
<span id="lstnumberx12.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx12.2" class="ltx_text ltx_lst_comment ltx_font_italic" style="font-size:70%;">#<span id="lstnumberx12.2.1" class="ltx_text ltx_lst_space"> </span>Scene<span id="lstnumberx12.2.2" class="ltx_text ltx_lst_space"> </span>statement</span>
</div>
<div id="lstnumberx13" class="ltx_listingline">
<span id="lstnumberx13.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx13.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">scene_statement</span><span id="lstnumberx13.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx13.4" class="ltx_text" style="font-size:70%;">=</span><span id="lstnumberx13.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx13.6" class="ltx_text ltx_lst_string" style="font-size:70%;">”They<span id="lstnumberx13.6.1" class="ltx_text ltx_lst_space">␣</span>are<span id="lstnumberx13.6.2" class="ltx_text ltx_lst_space">␣</span>in<span id="lstnumberx13.6.3" class="ltx_text ltx_lst_space">␣</span>”</span>
</div>
<div id="lstnumberx14" class="ltx_listingline">
<span id="lstnumberx14.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx14.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">scene_article</span><span id="lstnumberx14.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx14.4" class="ltx_text" style="font-size:70%;">=</span><span id="lstnumberx14.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx14.6" class="ltx_text ltx_lst_identifier" style="font-size:70%;">get_article</span><span id="lstnumberx14.7" class="ltx_text" style="font-size:70%;">(</span><span id="lstnumberx14.8" class="ltx_text ltx_lst_identifier" style="font-size:70%;">scene_name</span><span id="lstnumberx14.9" class="ltx_text" style="font-size:70%;">)</span>
</div>
<div id="lstnumberx15" class="ltx_listingline">
<span id="lstnumberx15.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx15.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">scene_description</span><span id="lstnumberx15.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx15.4" class="ltx_text" style="font-size:70%;">=</span><span id="lstnumberx15.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx15.6" class="ltx_text ltx_lst_identifier" style="font-size:70%;">get_description</span><span id="lstnumberx15.7" class="ltx_text" style="font-size:70%;">(</span><span id="lstnumberx15.8" class="ltx_text ltx_lst_identifier" style="font-size:70%;">scene_description</span><span id="lstnumberx15.9" class="ltx_text" style="font-size:70%;">)</span>
</div>
<div id="lstnumberx16" class="ltx_listingline">
<span id="lstnumberx16.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx16.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">scene_statement</span><span id="lstnumberx16.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx16.4" class="ltx_text" style="font-size:70%;">+=</span><span id="lstnumberx16.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx16.6" class="ltx_text ltx_lst_identifier" style="font-size:70%;">scene_article</span><span id="lstnumberx16.7" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx16.8" class="ltx_text" style="font-size:70%;">+</span><span id="lstnumberx16.9" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx16.10" class="ltx_text ltx_lst_identifier" style="font-size:70%;">scene_description</span>
</div>
<div id="lstnumberx17" class="ltx_listingline">
<span id="lstnumberx17.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx17.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">statements</span><span id="lstnumberx17.3" class="ltx_text" style="font-size:70%;">.</span><span id="lstnumberx17.4" class="ltx_text ltx_lst_identifier" style="font-size:70%;">append</span><span id="lstnumberx17.5" class="ltx_text" style="font-size:70%;">(</span><span id="lstnumberx17.6" class="ltx_text ltx_lst_identifier" style="font-size:70%;">scene_statement</span><span id="lstnumberx17.7" class="ltx_text" style="font-size:70%;">)</span>
</div>
<div id="lstnumberx18" class="ltx_listingline">
</div>
<div id="lstnumberx19" class="ltx_listingline">
<span id="lstnumberx19.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx19.2" class="ltx_text ltx_lst_comment ltx_font_italic" style="font-size:70%;">#<span id="lstnumberx19.2.1" class="ltx_text ltx_lst_space"> </span>Positional<span id="lstnumberx19.2.2" class="ltx_text ltx_lst_space"> </span>relations</span>
</div>
<div id="lstnumberx20" class="ltx_listingline">
<span id="lstnumberx20.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx20.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">relations</span><span id="lstnumberx20.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx20.4" class="ltx_text" style="font-size:70%;">=</span><span id="lstnumberx20.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx20.6" class="ltx_text" style="font-size:70%;">[]</span>
</div>
<div id="lstnumberx21" class="ltx_listingline">
<span id="lstnumberx21.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx21.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">n</span><span id="lstnumberx21.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx21.4" class="ltx_text" style="font-size:70%;">=</span><span id="lstnumberx21.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx21.6" class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_bold" style="font-size:70%;">len</span><span id="lstnumberx21.7" class="ltx_text" style="font-size:70%;">(</span><span id="lstnumberx21.8" class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_bold" style="font-size:70%;">len</span><span id="lstnumberx21.9" class="ltx_text" style="font-size:70%;">(</span><span id="lstnumberx21.10" class="ltx_text ltx_lst_identifier" style="font-size:70%;">objects</span><span id="lstnumberx21.11" class="ltx_text" style="font-size:70%;">))</span>
</div>
<div id="lstnumberx22" class="ltx_listingline">
<span id="lstnumberx22.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx22.2" class="ltx_text ltx_lst_keyword ltx_font_bold" style="font-size:70%;">for</span><span id="lstnumberx22.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx22.4" class="ltx_text ltx_lst_identifier" style="font-size:70%;">i</span><span id="lstnumberx22.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx22.6" class="ltx_text ltx_lst_keyword ltx_font_bold" style="font-size:70%;">in</span><span id="lstnumberx22.7" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx22.8" class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_bold" style="font-size:70%;">range</span><span id="lstnumberx22.9" class="ltx_text" style="font-size:70%;">(</span><span id="lstnumberx22.10" class="ltx_text ltx_lst_identifier" style="font-size:70%;">n</span><span id="lstnumberx22.11" class="ltx_text" style="font-size:70%;">):</span>
</div>
<div id="lstnumberx23" class="ltx_listingline">
<span id="lstnumberx23.1" class="ltx_text ltx_lst_space" style="font-size:70%;">    </span><span id="lstnumberx23.2" class="ltx_text ltx_lst_keyword ltx_font_bold" style="font-size:70%;">for</span><span id="lstnumberx23.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx23.4" class="ltx_text ltx_lst_identifier" style="font-size:70%;">j</span><span id="lstnumberx23.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx23.6" class="ltx_text ltx_lst_keyword ltx_font_bold" style="font-size:70%;">in</span><span id="lstnumberx23.7" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx23.8" class="ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_bold" style="font-size:70%;">range</span><span id="lstnumberx23.9" class="ltx_text" style="font-size:70%;">(</span><span id="lstnumberx23.10" class="ltx_text ltx_lst_identifier" style="font-size:70%;">i</span><span id="lstnumberx23.11" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx23.12" class="ltx_text" style="font-size:70%;">+</span><span id="lstnumberx23.13" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx23.14" class="ltx_text" style="font-size:70%;">1,</span><span id="lstnumberx23.15" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx23.16" class="ltx_text ltx_lst_identifier" style="font-size:70%;">n</span><span id="lstnumberx23.17" class="ltx_text" style="font-size:70%;">):</span>
</div>
<div id="lstnumberx24" class="ltx_listingline">
<span id="lstnumberx24.1" class="ltx_text ltx_lst_space" style="font-size:70%;">      </span><span id="lstnumberx24.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">left</span><span id="lstnumberx24.3" class="ltx_text" style="font-size:70%;">,</span><span id="lstnumberx24.4" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx24.5" class="ltx_text ltx_lst_identifier" style="font-size:70%;">right</span><span id="lstnumberx24.6" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx24.7" class="ltx_text" style="font-size:70%;">=</span><span id="lstnumberx24.8" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx24.9" class="ltx_text ltx_lst_identifier" style="font-size:70%;">get_left_right</span><span id="lstnumberx24.10" class="ltx_text" style="font-size:70%;">(</span><span id="lstnumberx24.11" class="ltx_text ltx_lst_identifier" style="font-size:70%;">seg_image</span><span id="lstnumberx24.12" class="ltx_text" style="font-size:70%;">,</span><span id="lstnumberx24.13" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx24.14" class="ltx_text ltx_lst_identifier" style="font-size:70%;">i</span><span id="lstnumberx24.15" class="ltx_text" style="font-size:70%;">,</span><span id="lstnumberx24.16" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx24.17" class="ltx_text ltx_lst_identifier" style="font-size:70%;">j</span><span id="lstnumberx24.18" class="ltx_text" style="font-size:70%;">)</span>
</div>
<div id="lstnumberx25" class="ltx_listingline">
<span id="lstnumberx25.1" class="ltx_text ltx_lst_space" style="font-size:70%;">      </span><span id="lstnumberx25.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">front</span><span id="lstnumberx25.3" class="ltx_text" style="font-size:70%;">,</span><span id="lstnumberx25.4" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx25.5" class="ltx_text ltx_lst_identifier" style="font-size:70%;">back</span><span id="lstnumberx25.6" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx25.7" class="ltx_text" style="font-size:70%;">=</span><span id="lstnumberx25.8" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx25.9" class="ltx_text ltx_lst_identifier" style="font-size:70%;">get_front_back</span><span id="lstnumberx25.10" class="ltx_text" style="font-size:70%;">(</span><span id="lstnumberx25.11" class="ltx_text ltx_lst_identifier" style="font-size:70%;">i</span><span id="lstnumberx25.12" class="ltx_text" style="font-size:70%;">,</span><span id="lstnumberx25.13" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx25.14" class="ltx_text ltx_lst_identifier" style="font-size:70%;">j</span><span id="lstnumberx25.15" class="ltx_text" style="font-size:70%;">)</span>
</div>
<div id="lstnumberx26" class="ltx_listingline">
<span id="lstnumberx26.1" class="ltx_text ltx_lst_space" style="font-size:70%;">      </span><span id="lstnumberx26.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">relations</span><span id="lstnumberx26.3" class="ltx_text" style="font-size:70%;">.</span><span id="lstnumberx26.4" class="ltx_text ltx_lst_identifier" style="font-size:70%;">extend</span><span id="lstnumberx26.5" class="ltx_text" style="font-size:70%;">([</span>
</div>
<div id="lstnumberx27" class="ltx_listingline">
<span id="lstnumberx27.1" class="ltx_text ltx_lst_space" style="font-size:70%;">        </span><span id="lstnumberx27.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">left</span><span id="lstnumberx27.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx27.4" class="ltx_text" style="font-size:70%;">+</span><span id="lstnumberx27.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx27.6" class="ltx_text ltx_lst_string" style="font-size:70%;">”<span id="lstnumberx27.6.1" class="ltx_text ltx_lst_space">␣</span>is<span id="lstnumberx27.6.2" class="ltx_text ltx_lst_space">␣</span>to<span id="lstnumberx27.6.3" class="ltx_text ltx_lst_space">␣</span>the<span id="lstnumberx27.6.4" class="ltx_text ltx_lst_space">␣</span>left<span id="lstnumberx27.6.5" class="ltx_text ltx_lst_space">␣</span>of<span id="lstnumberx27.6.6" class="ltx_text ltx_lst_space">␣</span>”</span><span id="lstnumberx27.7" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx27.8" class="ltx_text" style="font-size:70%;">+</span><span id="lstnumberx27.9" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx27.10" class="ltx_text ltx_lst_identifier" style="font-size:70%;">right</span><span id="lstnumberx27.11" class="ltx_text" style="font-size:70%;">,</span>
</div>
<div id="lstnumberx28" class="ltx_listingline">
<span id="lstnumberx28.1" class="ltx_text ltx_lst_space" style="font-size:70%;">        </span><span id="lstnumberx28.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">right</span><span id="lstnumberx28.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx28.4" class="ltx_text" style="font-size:70%;">+</span><span id="lstnumberx28.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx28.6" class="ltx_text ltx_lst_string" style="font-size:70%;">”<span id="lstnumberx28.6.1" class="ltx_text ltx_lst_space">␣</span>is<span id="lstnumberx28.6.2" class="ltx_text ltx_lst_space">␣</span>to<span id="lstnumberx28.6.3" class="ltx_text ltx_lst_space">␣</span>the<span id="lstnumberx28.6.4" class="ltx_text ltx_lst_space">␣</span>right<span id="lstnumberx28.6.5" class="ltx_text ltx_lst_space">␣</span>of<span id="lstnumberx28.6.6" class="ltx_text ltx_lst_space">␣</span>”</span><span id="lstnumberx28.7" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx28.8" class="ltx_text" style="font-size:70%;">+</span><span id="lstnumberx28.9" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx28.10" class="ltx_text ltx_lst_identifier" style="font-size:70%;">left</span><span id="lstnumberx28.11" class="ltx_text" style="font-size:70%;">,</span>
</div>
<div id="lstnumberx29" class="ltx_listingline">
<span id="lstnumberx29.1" class="ltx_text ltx_lst_space" style="font-size:70%;">      </span><span id="lstnumberx29.2" class="ltx_text" style="font-size:70%;">])</span>
</div>
<div id="lstnumberx30" class="ltx_listingline">
<span id="lstnumberx30.1" class="ltx_text ltx_lst_space" style="font-size:70%;">      </span><span id="lstnumberx30.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">relations</span><span id="lstnumberx30.3" class="ltx_text" style="font-size:70%;">.</span><span id="lstnumberx30.4" class="ltx_text ltx_lst_identifier" style="font-size:70%;">extend</span><span id="lstnumberx30.5" class="ltx_text" style="font-size:70%;">([</span>
</div>
<div id="lstnumberx31" class="ltx_listingline">
<span id="lstnumberx31.1" class="ltx_text ltx_lst_space" style="font-size:70%;">        </span><span id="lstnumberx31.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">front</span><span id="lstnumberx31.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx31.4" class="ltx_text" style="font-size:70%;">+</span><span id="lstnumberx31.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx31.6" class="ltx_text ltx_lst_string" style="font-size:70%;">”<span id="lstnumberx31.6.1" class="ltx_text ltx_lst_space">␣</span>is<span id="lstnumberx31.6.2" class="ltx_text ltx_lst_space">␣</span>in<span id="lstnumberx31.6.3" class="ltx_text ltx_lst_space">␣</span>front<span id="lstnumberx31.6.4" class="ltx_text ltx_lst_space">␣</span>of<span id="lstnumberx31.6.5" class="ltx_text ltx_lst_space">␣</span>”</span><span id="lstnumberx31.7" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx31.8" class="ltx_text" style="font-size:70%;">+</span><span id="lstnumberx31.9" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx31.10" class="ltx_text ltx_lst_identifier" style="font-size:70%;">back</span><span id="lstnumberx31.11" class="ltx_text" style="font-size:70%;">,</span>
</div>
<div id="lstnumberx32" class="ltx_listingline">
<span id="lstnumberx32.1" class="ltx_text ltx_lst_space" style="font-size:70%;">        </span><span id="lstnumberx32.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">back</span><span id="lstnumberx32.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx32.4" class="ltx_text" style="font-size:70%;">+</span><span id="lstnumberx32.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx32.6" class="ltx_text ltx_lst_string" style="font-size:70%;">”<span id="lstnumberx32.6.1" class="ltx_text ltx_lst_space">␣</span>is<span id="lstnumberx32.6.2" class="ltx_text ltx_lst_space">␣</span>behind<span id="lstnumberx32.6.3" class="ltx_text ltx_lst_space">␣</span>”</span><span id="lstnumberx32.7" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx32.8" class="ltx_text" style="font-size:70%;">+</span><span id="lstnumberx32.9" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx32.10" class="ltx_text ltx_lst_identifier" style="font-size:70%;">front</span><span id="lstnumberx32.11" class="ltx_text" style="font-size:70%;">,</span>
</div>
<div id="lstnumberx33" class="ltx_listingline">
<span id="lstnumberx33.1" class="ltx_text ltx_lst_space" style="font-size:70%;">      </span><span id="lstnumberx33.2" class="ltx_text" style="font-size:70%;">])</span>
</div>
<div id="lstnumberx34" class="ltx_listingline">
<span id="lstnumberx34.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx34.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">shuffle</span><span id="lstnumberx34.3" class="ltx_text" style="font-size:70%;">(</span><span id="lstnumberx34.4" class="ltx_text ltx_lst_identifier" style="font-size:70%;">relations</span><span id="lstnumberx34.5" class="ltx_text" style="font-size:70%;">)</span>
</div>
<div id="lstnumberx35" class="ltx_listingline">
<span id="lstnumberx35.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx35.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">statements</span><span id="lstnumberx35.3" class="ltx_text" style="font-size:70%;">.</span><span id="lstnumberx35.4" class="ltx_text ltx_lst_identifier" style="font-size:70%;">extend</span><span id="lstnumberx35.5" class="ltx_text" style="font-size:70%;">(</span><span id="lstnumberx35.6" class="ltx_text ltx_lst_identifier" style="font-size:70%;">relations</span><span id="lstnumberx35.7" class="ltx_text" style="font-size:70%;">)</span>
</div>
<div id="lstnumberx36" class="ltx_listingline">
</div>
<div id="lstnumberx37" class="ltx_listingline">
<span id="lstnumberx37.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx37.2" class="ltx_text ltx_lst_comment ltx_font_italic" style="font-size:70%;">#<span id="lstnumberx37.2.1" class="ltx_text ltx_lst_space"> </span>Clothing<span id="lstnumberx37.2.2" class="ltx_text ltx_lst_space"> </span>and<span id="lstnumberx37.2.3" class="ltx_text ltx_lst_space"> </span>action</span>
</div>
<div id="lstnumberx38" class="ltx_listingline">
<span id="lstnumberx38.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx38.2" class="ltx_text ltx_lst_keyword ltx_font_bold" style="font-size:70%;">for</span><span id="lstnumberx38.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx38.4" class="ltx_text ltx_lst_identifier" style="font-size:70%;">h</span><span id="lstnumberx38.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx38.6" class="ltx_text ltx_lst_keyword ltx_font_bold" style="font-size:70%;">in</span><span id="lstnumberx38.7" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx38.8" class="ltx_text ltx_lst_identifier" style="font-size:70%;">humans</span><span id="lstnumberx38.9" class="ltx_text" style="font-size:70%;">:</span>
</div>
<div id="lstnumberx39" class="ltx_listingline">
<span id="lstnumberx39.1" class="ltx_text ltx_lst_space" style="font-size:70%;">    </span><span id="lstnumberx39.2" class="ltx_text ltx_lst_comment ltx_font_italic" style="font-size:70%;">#<span id="lstnumberx39.2.1" class="ltx_text ltx_lst_space"> </span>Action</span>
</div>
<div id="lstnumberx40" class="ltx_listingline">
<span id="lstnumberx40.1" class="ltx_text ltx_lst_space" style="font-size:70%;">    </span><span id="lstnumberx40.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">s_action</span><span id="lstnumberx40.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx40.4" class="ltx_text" style="font-size:70%;">=</span><span id="lstnumberx40.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx40.6" class="ltx_text ltx_lst_identifier" style="font-size:70%;">f</span><span id="lstnumberx40.7" class="ltx_text ltx_lst_string" style="font-size:70%;">”The<span id="lstnumberx40.7.1" class="ltx_text ltx_lst_space">␣</span>{h.name}<span id="lstnumberx40.7.2" class="ltx_text ltx_lst_space">␣</span>{h.action}.”</span>
</div>
<div id="lstnumberx41" class="ltx_listingline">
<span id="lstnumberx41.1" class="ltx_text ltx_lst_space" style="font-size:70%;">    </span><span id="lstnumberx41.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">statements</span><span id="lstnumberx41.3" class="ltx_text" style="font-size:70%;">.</span><span id="lstnumberx41.4" class="ltx_text ltx_lst_identifier" style="font-size:70%;">append</span><span id="lstnumberx41.5" class="ltx_text" style="font-size:70%;">(</span><span id="lstnumberx41.6" class="ltx_text ltx_lst_identifier" style="font-size:70%;">s_action</span><span id="lstnumberx41.7" class="ltx_text" style="font-size:70%;">)</span>
</div>
<div id="lstnumberx42" class="ltx_listingline">
</div>
<div id="lstnumberx43" class="ltx_listingline">
<span id="lstnumberx43.1" class="ltx_text ltx_lst_space" style="font-size:70%;">    </span><span id="lstnumberx43.2" class="ltx_text ltx_lst_comment ltx_font_italic" style="font-size:70%;">#<span id="lstnumberx43.2.1" class="ltx_text ltx_lst_space"> </span>Clothing</span>
</div>
<div id="lstnumberx44" class="ltx_listingline">
<span id="lstnumberx44.1" class="ltx_text ltx_lst_space" style="font-size:70%;">    </span><span id="lstnumberx44.2" class="ltx_text ltx_lst_keyword ltx_font_bold" style="font-size:70%;">for</span><span id="lstnumberx44.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx44.4" class="ltx_text ltx_lst_identifier" style="font-size:70%;">s</span><span id="lstnumberx44.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx44.6" class="ltx_text ltx_lst_keyword ltx_font_bold" style="font-size:70%;">in</span><span id="lstnumberx44.7" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx44.8" class="ltx_text ltx_lst_identifier" style="font-size:70%;">h</span><span id="lstnumberx44.9" class="ltx_text" style="font-size:70%;">.</span><span id="lstnumberx44.10" class="ltx_text ltx_lst_identifier" style="font-size:70%;">clothe</span><span id="lstnumberx44.11" class="ltx_text" style="font-size:70%;">:</span>
</div>
<div id="lstnumberx45" class="ltx_listingline">
<span id="lstnumberx45.1" class="ltx_text ltx_lst_space" style="font-size:70%;">      </span><span id="lstnumberx45.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">s_clothe</span><span id="lstnumberx45.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx45.4" class="ltx_text" style="font-size:70%;">=</span><span id="lstnumberx45.5" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx45.6" class="ltx_text ltx_lst_identifier" style="font-size:70%;">f</span><span id="lstnumberx45.7" class="ltx_text ltx_lst_string" style="font-size:70%;">”The<span id="lstnumberx45.7.1" class="ltx_text ltx_lst_space">␣</span>{h.name}<span id="lstnumberx45.7.2" class="ltx_text ltx_lst_space">␣</span>{s.strip()}.”</span>
</div>
<div id="lstnumberx46" class="ltx_listingline">
<span id="lstnumberx46.1" class="ltx_text ltx_lst_space" style="font-size:70%;">      </span><span id="lstnumberx46.2" class="ltx_text ltx_lst_identifier" style="font-size:70%;">statements</span><span id="lstnumberx46.3" class="ltx_text" style="font-size:70%;">.</span><span id="lstnumberx46.4" class="ltx_text ltx_lst_identifier" style="font-size:70%;">append</span><span id="lstnumberx46.5" class="ltx_text" style="font-size:70%;">(</span><span id="lstnumberx46.6" class="ltx_text ltx_lst_identifier" style="font-size:70%;">s_clothe</span><span id="lstnumberx46.7" class="ltx_text" style="font-size:70%;">)</span>
</div>
<div id="lstnumberx47" class="ltx_listingline">
</div>
<div id="lstnumberx48" class="ltx_listingline">
<span id="lstnumberx48.1" class="ltx_text ltx_lst_space" style="font-size:70%;">  </span><span id="lstnumberx48.2" class="ltx_text ltx_lst_keyword ltx_font_bold" style="font-size:70%;">return</span><span id="lstnumberx48.3" class="ltx_text ltx_lst_space" style="font-size:70%;"> </span><span id="lstnumberx48.4" class="ltx_text ltx_lst_string" style="font-size:70%;">”<span id="lstnumberx48.4.1" class="ltx_text ltx_lst_space">␣</span>”</span><span id="lstnumberx48.5" class="ltx_text" style="font-size:70%;">.</span><span id="lstnumberx48.6" class="ltx_text ltx_lst_identifier" style="font-size:70%;">join</span><span id="lstnumberx48.7" class="ltx_text" style="font-size:70%;">(</span><span id="lstnumberx48.8" class="ltx_text ltx_lst_identifier" style="font-size:70%;">statements</span><span id="lstnumberx48.9" class="ltx_text" style="font-size:70%;">).</span><span id="lstnumberx48.10" class="ltx_text ltx_lst_identifier" style="font-size:70%;">strip</span><span id="lstnumberx48.11" class="ltx_text" style="font-size:70%;">()</span>
</div>
</div>
</figure>
<div id="A6.p6" class="ltx_para">
<p id="A6.p6.1" class="ltx_p">Evidently, dense captions tend to be way too descriptive and hence noisy to be used fully in VL training. Therefore, we add a sampling option where statements are sampled with certain probabilities following their weights. For example, instead of mentioning all pairwise positional relations, this option allows sampling a number of sentences from the positional relations category.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>LLM-Based Caption Paraphrasing</h2>

<div id="A7.p1" class="ltx_para">
<p id="A7.p1.1" class="ltx_p">We additionally experiment with using the rule-based system to guide the use of large language models for caption generation / paraphrasing. Specifically, we adapt the deterministically-generated caption (as detailed in Section <a href="#A6" title="Appendix F Metadata-driven caption text synthesis, more details ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a>) into a prompt for instruction-based text completion by replacing the prefix ”This scene contains” (in the synthesized captions) to ”Please describe a scene containing” and adding a suffix for text completion: ”In this scene, we can see”. We use the adapted texts as prompts for language models and generate text completions. We limit the generated texts to 150 tokens and use caption split averaging, as described in
Section <a href="#S3.SS2" title="3.2 Finetuning large-scale pre-trained VL models using synthetic data ‣ 3 Method ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>
in the main paper. We experiment with the Bloomz 7.1B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> and Flan-T5 XXL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> through Huggingface <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>.</p>
</div>
<div id="A7.p2" class="ltx_para">
<p id="A7.p2.1" class="ltx_p">Table <a href="#A7.T11" title="Table 11 ‣ Appendix G LLM-Based Caption Paraphrasing ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> shows the performance of syn-CLIP trained using different caption generation mechanisms. We do not observe any significant performance gains when using the captions generated by openly avaialble language models that we tried over the rule-based system. This is indeed expected, as the captions generated by current open language models tend to repeat much of the content in the prompt, often correcting verb tenses or adding appropriate punctuation marks, which don’t contribute to the semantic richness of the caption.</p>
</div>
<figure id="A7.T11" class="ltx_table">
<table id="A7.T11.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A7.T11.1.1.1" class="ltx_tr">
<th id="A7.T11.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt"></th>
<td id="A7.T11.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="A7.T11.1.1.1.2.1" class="ltx_text" style="font-size:90%;">VL-Checklist</span></td>
</tr>
<tr id="A7.T11.1.2.2" class="ltx_tr">
<th id="A7.T11.1.2.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="A7.T11.1.2.2.2" class="ltx_td ltx_align_left"><span id="A7.T11.1.2.2.2.1" class="ltx_text" style="font-size:90%;">Relation</span></td>
<td id="A7.T11.1.2.2.3" class="ltx_td ltx_align_left ltx_border_r"><span id="A7.T11.1.2.2.3.1" class="ltx_text" style="font-size:90%;">Attribute</span></td>
<td id="A7.T11.1.2.2.4" class="ltx_td ltx_align_left"><span id="A7.T11.1.2.2.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Average</span></td>
</tr>
<tr id="A7.T11.1.3.3" class="ltx_tr">
<th id="A7.T11.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="A7.T11.1.3.3.1.1" class="ltx_text" style="font-size:90%;">CLIP</span></th>
<td id="A7.T11.1.3.3.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T11.1.3.3.2.1" class="ltx_text" style="font-size:90%;">63.57</span></td>
<td id="A7.T11.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A7.T11.1.3.3.3.1" class="ltx_text" style="font-size:90%;">67.51</span></td>
<td id="A7.T11.1.3.3.4" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T11.1.3.3.4.1" class="ltx_text" style="font-size:90%;">65.54</span></td>
</tr>
<tr id="A7.T11.1.4.4" class="ltx_tr">
<th id="A7.T11.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="A7.T11.1.4.4.1.1" class="ltx_text" style="font-size:90%;">syn-CLIP with Rule-Based</span></th>
<td id="A7.T11.1.4.4.2" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T11.1.4.4.2.1" class="ltx_text" style="font-size:90%;">69.39</span></td>
<td id="A7.T11.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="A7.T11.1.4.4.3.1" class="ltx_text" style="font-size:90%;">70.37</span></td>
<td id="A7.T11.1.4.4.4" class="ltx_td ltx_align_left ltx_border_t"><span id="A7.T11.1.4.4.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">69.88</span></td>
</tr>
<tr id="A7.T11.1.5.5" class="ltx_tr">
<th id="A7.T11.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="A7.T11.1.5.5.1.1" class="ltx_text" style="font-size:90%;">syn-CLIP with Bloomz</span></th>
<td id="A7.T11.1.5.5.2" class="ltx_td ltx_align_left"><span id="A7.T11.1.5.5.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">69.66</span></td>
<td id="A7.T11.1.5.5.3" class="ltx_td ltx_align_left ltx_border_r"><span id="A7.T11.1.5.5.3.1" class="ltx_text" style="font-size:90%;">69.69</span></td>
<td id="A7.T11.1.5.5.4" class="ltx_td ltx_align_left"><span id="A7.T11.1.5.5.4.1" class="ltx_text" style="font-size:90%;">69.68</span></td>
</tr>
<tr id="A7.T11.1.6.6" class="ltx_tr">
<th id="A7.T11.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="A7.T11.1.6.6.1.1" class="ltx_text" style="font-size:90%;">syn-CLIP with Flan-T5</span></th>
<td id="A7.T11.1.6.6.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="A7.T11.1.6.6.2.1" class="ltx_text" style="font-size:90%;">68.48</span></td>
<td id="A7.T11.1.6.6.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span id="A7.T11.1.6.6.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">71.14</span></td>
<td id="A7.T11.1.6.6.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="A7.T11.1.6.6.4.1" class="ltx_text" style="font-size:90%;">69.81</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 11: </span>VL-Checklist <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> performance on variants of syn-CLIP fine-tuned on <span id="A7.T11.9.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> with captions generated using the rule-based system described in section 2 or using language models as described in section 3.</figcaption>
</figure>
<div id="A7.p3" class="ltx_para">
<p id="A7.p3.1" class="ltx_p">However, we remark that additional work on using LLMs for caption generation could investigate more powerful language models, or the use of visual grounding for caption generation as an additional information source, to yield better paraphrasing / captions.</p>
</div>
</section>
<section id="A8" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Exploration into Synthetic Data Diversity</h2>

<div id="A8.p1" class="ltx_para">
<p id="A8.p1.1" class="ltx_p">As promised in Ablations
Section <a href="#S4.SS4" title="4.4 Ablations ‣ 4 Experiments ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>
of the main paper (lines 740-741 in ”<span id="A8.p1.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> - number of models and number of samples”) we include the effect of the number of synthetic samples and the number of object models used for <span id="A8.p1.1.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> generation analysis in Figure <a href="#A8.F4" title="Figure 4 ‣ Appendix H Exploration into Synthetic Data Diversity ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. These ablations were not included in the main paper due to lack of space. As we can see the performance is improving consistently, both with adding more synthetic images (Fig. <a href="#A8.F4" title="Figure 4 ‣ Appendix H Exploration into Synthetic Data Diversity ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>a) and with adding more 3D models used for synthesis (Fig. <a href="#A8.F4" title="Figure 4 ‣ Appendix H Exploration into Synthetic Data Diversity ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>b).</p>
</div>
<figure id="A8.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2303.17590/assets/Images/scalability.png" id="A8.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="255" height="192" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2303.17590/assets/Images/diversity.png" id="A8.F4.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="255" height="192" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Exploration into Synthetic Data Diversity. (a) effect of adding more synthetic samples to <span id="A8.F4.3.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span>; (b) effect of adding more 3D object models to <span id="A8.F4.4.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span>. Comparing base CLIP and syn-CLIP performances on VL-Checklist and ARO benchmarks.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A9" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix I </span>Some Qualitative Examples with Synthetic Humans</h2>

<div id="A9.p1" class="ltx_para">
<p id="A9.p1.1" class="ltx_p">In this section, we first showcase
qualitative improvement in the compositional capabilities of CLIP after finetuning on SyViC using our proposed approach via GradCAM in Figure <a href="#A9.F5" title="Figure 5 ‣ Appendix I Some Qualitative Examples with Synthetic Humans ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Next, we show textured SMPL samples in Figure <a href="#A9.F6" title="Figure 6 ‣ Appendix I Some Qualitative Examples with Synthetic Humans ‣ Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="A9.F5" class="ltx_figure"><img src="/html/2303.17590/assets/x5.png" id="A9.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="172" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>GradCAM on a Winoground sample.
<span id="A9.F5.5.1" class="ltx_text ltx_font_italic">left</span> - a CLIP model attending incorrectly to table regions for both <span id="A9.F5.6.2" class="ltx_text ltx_font_typewriter">a table</span> and <span id="A9.F5.7.3" class="ltx_text ltx_font_typewriter">someone</span> text queries, making a mistake in prediction (red text). <span id="A9.F5.8.4" class="ltx_text ltx_font_italic">right</span> - our syn-CLIP model correctly attends to the same making no mistakes in prediction with respect to the given inputs. Best viewed in color. </figcaption>
</figure>
<figure id="A9.F6" class="ltx_figure"><img src="/html/2303.17590/assets/x6.png" id="A9.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="150" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Digital humans. We show male and female samples of Unity Prefabs containing SMPL templates, a set of <math id="A9.F6.2.m1.1" class="ltx_Math" alttext="514" display="inline"><semantics id="A9.F6.2.m1.1b"><mn id="A9.F6.2.m1.1.1" xref="A9.F6.2.m1.1.1.cmml">514</mn><annotation-xml encoding="MathML-Content" id="A9.F6.2.m1.1c"><cn type="integer" id="A9.F6.2.m1.1.1.cmml" xref="A9.F6.2.m1.1.1">514</cn></annotation-xml><annotation encoding="application/x-tex" id="A9.F6.2.m1.1d">514</annotation></semantics></math> reusable 3D object assets available in <span id="A9.F6.4.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span>. We add colliders to each model to allow interactions with other objects. </figcaption>
</figure>
<div id="A9.p2" class="ltx_para">
<p id="A9.p2.1" class="ltx_p">Finally, we showcase some visual examples from <span id="A9.p2.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">SyViC</span> along with the dense captions we generate describing human actions and detailed human-object interactions and relative position descriptions in the following pages.</p>
</div>
<figure id="A9.1" class="ltx_figure"><img src="/html/2303.17590/assets/x7.png" id="A9.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="259" alt="[Uncaptioned image]">
</figure>
<figure id="A9.2" class="ltx_figure"><img src="/html/2303.17590/assets/x8.png" id="A9.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="259" alt="[Uncaptioned image]">
</figure>
<figure id="A9.3" class="ltx_figure"><img src="/html/2303.17590/assets/x9.png" id="A9.3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="259" alt="[Uncaptioned image]">
</figure>
<figure id="A9.4" class="ltx_figure"><img src="/html/2303.17590/assets/x10.png" id="A9.4.g1" class="ltx_graphics ltx_img_landscape" width="461" height="259" alt="[Uncaptioned image]">
</figure>
<figure id="A9.5" class="ltx_figure"><img src="/html/2303.17590/assets/x11.png" id="A9.5.g1" class="ltx_graphics ltx_img_landscape" width="461" height="259" alt="[Uncaptioned image]">
</figure>
<figure id="A9.6" class="ltx_figure"><img src="/html/2303.17590/assets/x12.png" id="A9.6.g1" class="ltx_graphics ltx_img_landscape" width="461" height="259" alt="[Uncaptioned image]">
</figure>
<figure id="A9.7" class="ltx_figure"><img src="/html/2303.17590/assets/x13.png" id="A9.7.g1" class="ltx_graphics ltx_img_landscape" width="461" height="259" alt="[Uncaptioned image]">
</figure>
<figure id="A9.8" class="ltx_figure"><img src="/html/2303.17590/assets/x14.png" id="A9.8.g1" class="ltx_graphics ltx_img_landscape" width="461" height="259" alt="[Uncaptioned image]">
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2303.17589" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2303.17590" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2303.17590">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2303.17590" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2303.17591" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 17:45:09 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
