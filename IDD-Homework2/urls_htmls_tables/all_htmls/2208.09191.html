<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2208.09191] Synthetic Data in Human Analysis: A Survey</title><meta property="og:description" content="Deep neural networks have become prevalent in human analysis, boosting the performance of applications, such as biometric recognition, action recognition, as well as person re-identification. However, the performance o…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synthetic Data in Human Analysis: A Survey">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synthetic Data in Human Analysis: A Survey">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2208.09191">

<!--Generated on Wed Mar 13 19:53:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Human Analysis,  Deep Neural Networks,  Synthetic Data,  Survey
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Synthetic Data in Human Analysis: A Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Indu Joshi, Marcel Grimmer, Christian Rathgeb, Christoph Busch, Francois Bremond,
Antitza Dantcheva
</span><span class="ltx_author_notes">I. Joshi, F. Bremond, and A. Dantcheva are with the STARS team of Inria, Sophia Antipolis - Méditerranée and Université Côte d’Azur, France.
<br class="ltx_break">E-mail: indu.joshi@inria.fr, francois.bremond@inria.fr, antitza.dantcheva@inria.fr
Marcel Grimmer is with the NBL - Norwegian Biometrics Laboratory, Norwegian University of Science and Technology, Norway.
<br class="ltx_break">E-mail: marceg@ntnu.no
C. Rathgeb and C. Busch are with the da/sec - Biometrics and Internet-Security Research Group, Hochschule Darstadt, Germany.
<br class="ltx_break">E-mail: christian.rathgeb@h-da.de, christoph.busch@h-da.de
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Deep neural networks have become prevalent in human analysis, boosting the performance of applications, such as biometric recognition, action recognition, as well as person re-identification. However, the performance of such networks scales with the available training data. In human analysis, the demand for large-scale datasets poses a severe challenge, as data collection is tedious, time-expensive, costly and must comply with data protection laws. Current research investigates the generation of <span id="id1.id1.1" class="ltx_text ltx_font_italic">synthetic data</span> as an efficient and privacy-ensuring alternative to collecting real data in the field. This survey introduces the basic definitions and methodologies, essential when generating and employing synthetic data for human analysis. We conduct a survey that summarises current state-of-the-art methods and the main benefits of using synthetic data. We also provide an overview of publicly available synthetic datasets and generation models. Finally, we discuss limitations, as well as open research
problems in this field. This survey is intended for researchers and practitioners in the field of human analysis.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Human Analysis, Deep Neural Networks, Synthetic Data, Survey

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">We have witnessed remarkable advancement of deep neural networks (DNNs) in the past decade, leading to mature and robust algorithms in visual perception, natural language processing, and robotic control <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, among others. Such advancement has been fuelled by the development of <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">algorithms</span> to train DNNs, the availability of <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">large-scale</span> training <span id="S1.p1.1.3" class="ltx_text ltx_font_italic">datasets</span>, as well as the progress in <span id="S1.p1.1.4" class="ltx_text ltx_font_italic">computational power</span>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">DNN techniques have been designed, among other applications, for <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">human analysis</span>, aiming to recognize human characteristics, behaviour, and interactions with the physical world. In this context, human analysis ranges from the unique authentication of single individuals, the classification of human attributes or actions to the evaluation of crowd-based data. Despite the immense benefit of processing human data, <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">lack of annotated training data</span> still hinders DNNs from unfolding their full potential. In addition, the implementation of data protection laws, such as the <span id="S1.p2.1.3" class="ltx_text ltx_font_italic">European general data protection regulation (GDPR)</span>, defines strict rules for processing data that can reveal identity information, thus violating the data subjects’ informational self-determination. According to article 9 of the GDPR, biometric data is considered as <span id="S1.p2.1.4" class="ltx_text ltx_font_italic">sensitive data</span>, and processing without explicit consent of the data subjects is imposed with fines of up to 20 million Euro or 4% of the firm’s worldwide annual revenue from the preceding financial year (article 83).</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">One solution to overcome such limitations related to limited training data and data protection has to do with creating large-scale <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">synthetic datasets</span>. Progress of deep generative models has allowed for the generation of highly realistic synthetic human images - challenging to distinguish from real data by both humans, and computer vision algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> (see Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
While generative models have been able to produce highly realistic synthetic samples, we note that they are prone to leak information from training datasets. This is specifically of concern when human data is involved, and hence identity leaks have to be taken into account to protect personal privacy rights. In this context, current research indicates that identity leaks in deep generative networks become less likely, in case the complexity of the training dataset exceeds the complexity of the model architecture<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. The main reason for identity leaks stems from generative model overfitting to the training dataset, with the consequence of specific units in the network revealing information of single data subjects - a concept referred to as <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">generative adversarial network (GAN) memorization</span>.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2208.09191/assets/images/synth-face-stylegan2.png" id="S1.F1.sf1.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S1.F1.sf1.3.2" class="ltx_text" style="font-size:90%;">StyleGAN2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2208.09191/assets/images/synth-fingerprint-sfinge.jpg" id="S1.F1.sf2.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S1.F1.sf2.3.2" class="ltx_text" style="font-size:90%;">SFinGe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S1.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2208.09191/assets/x1.jpg" id="S1.F1.sf3.g1" class="ltx_graphics ltx_img_square" width="664" height="664" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S1.F1.sf3.3.2" class="ltx_text" style="font-size:90%;">SpoofGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite></span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2208.09191/assets/images/pose-surreal-dataset.png" id="S1.F1.sf4.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S1.F1.sf4.3.2" class="ltx_text" style="font-size:90%;">SURREAL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F1.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2208.09191/assets/images/eldersim-action-recognition.png" id="S1.F1.sf5.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf5.2.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="S1.F1.sf5.3.2" class="ltx_text" style="font-size:90%;">ElderSim <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Synthetic images generated for human analysis, namely (a) 2D face image generation, (b) fingerprint image generation, (c) fingerprint presentation attack detection, (d) 2D pose estimation, (e) and elderly action recognition </span></figcaption>
</figure>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span><span id="S1.SS1.1.1" class="ltx_text ltx_font_italic">Domains of application</span>
</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Synthetic data boosts the performance of many data-driven models in human analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. In this context, a number of training schemes have been introduced including <span id="S1.SS1.p1.1.1" class="ltx_text ltx_font_italic">data replacement</span> and <span id="S1.SS1.p1.1.2" class="ltx_text ltx_font_italic">data enrichment</span>. The motivation for replacing real samples with synthetic data (<span id="S1.SS1.p1.1.3" class="ltx_text ltx_font_italic">i.e.</span>, <span id="S1.SS1.p1.1.4" class="ltx_text ltx_font_italic">synthetic training</span>) has to do with alleviating privacy concerns. In contrast, the combination of synthetic and real data (<span id="S1.SS1.p1.1.5" class="ltx_text ltx_font_italic">i.e.</span>, <span id="S1.SS1.p1.1.6" class="ltx_text ltx_font_italic">augmented training</span>) mainly aims at reducing biases achieved by re-balancing according to observed soft characteristics. Another optimization scheme aims at initializing model weights based on synthetic data with subsequent fine-tuning on a small subset of real data, referred to as <span id="S1.SS1.p1.1.7" class="ltx_text ltx_font_italic">model initialization</span>. Finally, domain translation techniques are utilized to close the synthetic vs real domain gap (<span id="S1.SS1.p1.1.8" class="ltx_text ltx_font_italic">domain adaptation</span>), thereby increasing the realism of synthetic datasets while preserving fine-grained annotations.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">Deviating from synthetic data employed for model training, <span id="S1.SS1.p2.1.1" class="ltx_text ltx_font_italic">synthetic evaluation datasets</span> have been utilized to benchmark the performance of existing algorithms, pre-trained models, and systems. This field of research is fuelled by the increasing representativeness of synthetically generated samples, which allows interference with systems and observed outcomes similar to those expected by real evaluation datasets. The preparation of large-scale testing databases intends to detect weaknesses in the human analysis pipeline without requiring expensive data collection initiatives. Apart from the cost factor, real data from specific (demographic) subgroups may not be accessible, so synthetic samples could fill this gap on a large scale.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span><span id="S1.SS2.1.1" class="ltx_text ltx_font_italic">Structure of paper</span>
</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">Given the increasing popularity of synthetic data, the main contribution of this survey is to revisit current research in human analysis, illustrating applications, benefits, and open challenges to accelerate future research. We introduce basic <span id="S1.SS2.p1.1.1" class="ltx_text ltx_font_italic">terminology</span> and <span id="S1.SS2.p1.1.2" class="ltx_text ltx_font_italic">scope</span> in Section <a href="#S2" title="2 Synthetic data in human analysis ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, followed by Section <a href="#S2.SS3" title="2.3 Benefits of synthetic data ‣ 2 Synthetic data in human analysis ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>, which provides an overview of the main <span id="S1.SS2.p1.1.3" class="ltx_text ltx_font_italic">benefits</span> associated to synthetic data. Section <a href="#S3" title="3 How can synthetic datasets be generated? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> elaborates on <span id="S1.SS2.p1.1.4" class="ltx_text ltx_font_italic">techniques for generating synthetic data</span>, followed by the most prominent <span id="S1.SS2.p1.1.5" class="ltx_text ltx_font_italic">application scenarios</span> presented in Section <a href="#S4" title="4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Section <a href="#S5" title="5 Open-Source Availability ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> summarises <span id="S1.SS2.p1.1.6" class="ltx_text ltx_font_italic">synthetic datasets</span> and <span id="S1.SS2.p1.1.7" class="ltx_text ltx_font_italic">data generation tools</span> that are publicly available across human analysis domains. Finally, in Section <a href="#S6" title="6 Challenges and Discussion ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> we discuss <span id="S1.SS2.p1.1.8" class="ltx_text ltx_font_italic">open challenges</span> identified in the literature analysis with promising new DNN concepts outlined in Section <a href="#S7" title="7 Conclusions and Future Applications ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Synthetic data in human analysis</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The vast progress of deep generative networks has brought to the fore highly realistic synthetic data beneficial in automated human-centred analysis. To avoid ambiguity throughout this survey and prepare the reader for the following content, we establish terminology of basic concepts and terminology used in this survey.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span><span id="S2.SS1.1.1" class="ltx_text ltx_font_italic">Synthetic data</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In general, <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">synthetic data</span> can be defined as <span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_italic">digital information generated by computer algorithms to approximate information collected or measured in the real world</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Synthetic data stems generally from <span id="S2.SS1.p1.1.3" class="ltx_text ltx_font_italic">traditional modelling</span> or <span id="S2.SS1.p1.1.4" class="ltx_text ltx_font_italic">deep generative models</span>. While traditional modelling generates real-world patterns based on prior expert knowledge through the <span id="S2.SS1.p1.1.5" class="ltx_text ltx_font_italic">formulation of mathematical models</span>, deep generative models are designed to <span id="S2.SS1.p1.1.6" class="ltx_text ltx_font_italic">automatically</span> learn patterns from the training dataset. In the last decade, deep generative models have outperformed traditional modelling techniques, <span id="S2.SS1.p1.1.7" class="ltx_text ltx_font_italic">w.r.t.</span> quality and generalizability of the synthetic samples <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. In this survey, we refer to <span id="S2.SS1.p1.1.8" class="ltx_text ltx_font_italic">generative models</span> in the context of both mathematical modelling and deep generative models.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Synthetic data samples can be <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_italic">fully-synthetic</span>, as well as <span id="S2.SS1.p2.1.2" class="ltx_text ltx_font_italic">semi-synthetic</span>. Fully-synthetic samples are generated without representing an underlying real-world object <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, generally by generative models, random sampling from a learned distribution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. At the same time, semi-synthetic samples constitute representations of real subjects, whose semantics have been manipulated <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. For example, in human analysis, predicting the future appearance of a real face is considered semi-synthetic, as the image maintains the identity information while altering the age. In contrast, fingerprint images synthesized by GANs based on random noise vectors are defined as fully-synthetic. An example image for each class is demonstrated in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.1 Synthetic data ‣ 2 Synthetic data in human analysis ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2208.09191/assets/images/synth-face-example.png" id="S2.F2.sf1.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">F-S</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2208.09191/assets/images/semi-synth-example.jpg" id="S2.F2.sf2.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">S-S</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2208.09191/assets/images/real-face-example.png" id="S2.F2.sf3.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S2.F2.sf3.3.2" class="ltx_text" style="font-size:90%;">Real</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">Example images of a fully-synthetic (F-S), semi-synthetic (S-S), as well as real samples. The S-S face image (b) was generated with InterFaceGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> by editing the age of the real face image depicted on the right side <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. The F-S sample (a) was randomly generated with StyleGAN2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</span></figcaption>
</figure>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">In computer vision, real-world information is represented either at <span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_italic">sample</span> or <span id="S2.SS1.p3.1.2" class="ltx_text ltx_font_italic">feature</span> level. In particular, we refer to data samples as the analogue or digital representation of human characteristics before feature extraction. According to the harmonic biometric vocabulary of ISO/IEC 2382-37:2017 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, a feature vector is composed of <span id="S2.SS1.p3.1.3" class="ltx_text ltx_font_italic">numbers or labels extracted from the data sample</span>. Specifically, feature vectors are treated as compressed sample representations, often encapsulating information optimised for a specific downstream task, such as biometric recognition. In practice, generative models can either focus on generating “<span id="S2.SS1.p3.1.4" class="ltx_text ltx_font_italic">synthetic samples</span>” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> or “<span id="S2.SS1.p3.1.5" class="ltx_text ltx_font_italic">synthetic features</span>”<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, depending on the target application.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span><span id="S2.SS2.1.1" class="ltx_text ltx_font_italic">Data replacement versus Data enrichment</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">While deep neural networks have achieved remarkable results in various computer vision tasks, it is still challenging to unleash their full potential due to the limited availability of large-scale datasets. Generation of synthetic samples can improve scalability and diversity, motivated by the following. Firstly, existing datasets being enriched with synthetic samples can increase dataset diversity. In this context, <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">data enrichment (DE)</span> imparts balancing of the proportions of soft characteristics in order to reduce dataset biases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Note that in this survey, data enrichment signifies minor <span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_italic">data perturbations</span> such as image cropping, colour transformation, as well as noise injection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Due to plethora of data augmentation techniques, distinction between <span id="S2.SS2.p1.1.3" class="ltx_text ltx_font_italic">synthetic</span> and <span id="S2.SS2.p1.1.4" class="ltx_text ltx_font_italic">augmented</span> samples is often challenging. Therefore, we refer to augmented samples as semi-synthetic, given that the original sample is at hand. In addition, we here denote weak supervision learning as a type of DE, as both synthetic and real samples are jointly employed for model training (see <a href="#S4.SS5.SSS3" title="4.5.3 Weakly supervised learning ‣ 4.5 Providing annotated data for supervision ‣ 4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5.3</span></a>).</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Secondly, <span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_italic">data replacement (DR)</span> refers to the replacement of real data with synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. This is instigated by <span id="S2.SS2.p2.1.2" class="ltx_text ltx_font_italic">privacy</span> concerns in human analysis, where identity information can be linked with the corresponding sample.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Training human analysis models on domain-adapted synthetic datasets is considered a sub-category of DR, as only high-level information from a small subset of real data is being utilised (see Section <a href="#S4.SS5.SSS2" title="4.5.2 Unsupervised domain adaptation ‣ 4.5 Providing annotated data for supervision ‣ 4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5.2</span></a>). In contrast, the initialisation or fine-tuning of model weights with synthetic data is defined as a sub-category of DE due to the active involvement of real data that remains part of the training process (see Section <a href="#S4.SS6" title="4.6 Pre-training a deep model ‣ 4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.6</span></a> and Section <a href="#S4.SS7" title="4.7 Fine-tuning a pre-trained model ‣ 4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.7</span></a>).</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">We proceed to enlist mechanisms in which synthetic data has been employed in the context of DNNs:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Augmented Training</span> refers to learning human analysis models or classifiers from a mixed training dataset that includes both real and synthetic data samples.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Weakly-Supervised Learning</span> signifies combined training with weak labels (real data) and accurate annotations (synthetic data).</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Model Initialisation</span> denotes initial training on synthetic data with subsequent fine-tuning on real data towards reduction of the <span id="S2.I1.i3.p1.1.2" class="ltx_text ltx_font_italic">synthetic versus real</span> domain gap.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p"><span id="S2.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Model Fine-Tuning</span> refers to initial training on real data with subsequent fine-tuning on synthetic data to increase the model robustness against biases.</p>
</div>
</li>
<li id="S2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i5.p1" class="ltx_para">
<p id="S2.I1.i5.p1.1" class="ltx_p"><span id="S2.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Consistency Regularisation</span> denotes the utilisation of semi-synthetic data to enforce the consistency of model predictions for similar training samples.</p>
</div>
</li>
<li id="S2.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i6.p1" class="ltx_para">
<p id="S2.I1.i6.p1.1" class="ltx_p"><span id="S2.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">Synthetic Training</span> signifies the training of models or classifiers on datasets composed of synthetic data only.</p>
</div>
</li>
<li id="S2.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i7.p1" class="ltx_para">
<p id="S2.I1.i7.p1.1" class="ltx_p"><span id="S2.I1.i7.p1.1.1" class="ltx_text ltx_font_bold">Domain Adaptation</span> denotes the employment of models trained on synthetic data to domain adaptation techniques (<span id="S2.I1.i7.p1.1.2" class="ltx_text ltx_font_italic">e.g.,</span> Cycle-GAN), aiming to close the <span id="S2.I1.i7.p1.1.3" class="ltx_text ltx_font_italic">synthetic versus real</span> domain gap.</p>
</div>
</li>
<li id="S2.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i8.p1" class="ltx_para">
<p id="S2.I1.i8.p1.1" class="ltx_p"><span id="S2.I1.i8.p1.1.1" class="ltx_text ltx_font_bold">Synthetic Performance Evaluation</span> refers to assessing synthetic datasets generated to test the scalability and performance of systems, algorithms, or pre-trained models.</p>
</div>
</li>
</ul>
</div>
<figure id="S2.F3" class="ltx_figure">
<div id="S2.F3.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:0pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(213.5pt,0.0pt) scale(65.0430985154701,65.0430985154701) ;"><svg id="S2.F3.1.pic1" class="ltx_picture" height="1" overflow="visible" version="1.1" width="1"><g transform="translate(0,1) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignObject width="0" height="22.14" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F3.1.pic1.1.1.1.1" class="ltx_ERROR undefined">\Tree</span>
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break"></foreignObject></g></svg>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.4.2" class="ltx_text" style="font-size:90%;">Application domains of synthetic data in human analysis.</span></figcaption>
</figure>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">Motivated by the above, synthetic data has enabled a number of applications, listed in Table <a href="#S2.T1" title="TABLE I ‣ 2.4 Human analysis ‣ 2 Synthetic data in human analysis ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> and elaborated on in Section <a href="#S4" title="4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p">Further, Figure <a href="#S2.F4" title="Figure 4 ‣ 2.4 Human analysis ‣ 2 Synthetic data in human analysis ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> summarises application scenarios derived from the forthcoming literature survey.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span><span id="S2.SS3.1.1" class="ltx_text ltx_font_italic">Benefits of synthetic data</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Synthetic data can impart a performance boost to human analysis models, augment controllability and scalability, and mitigate privacy concerns. We here outline such benefits, whereas Section <a href="#S4" title="4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> revisits relevant works.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p"><span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_italic">Performance boost.</span> One ample application of synthetic data has been towards boosting the performance of human analysis models. Table <a href="#S2.T1" title="TABLE I ‣ 2.4 Human analysis ‣ 2 Synthetic data in human analysis ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> demonstrates such boost by comparing the associated performance before and after the use of synthetic data in several domains such as action recognition, crowd counting, face recognition, pose estimation, and gender classification. Moreover, Table <a href="#S2.T1" title="TABLE I ‣ 2.4 Human analysis ‣ 2 Synthetic data in human analysis ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> shows that synthetic evaluation datasets, including controlled labels, are exploited to evaluate the performance of new algorithms and pre-trained models. In human analysis, the high fidelity of evaluation datasets has been mainly fuelled by the remarkable progress in the domain of conditional image synthesis, which enables the generation of <span id="S2.SS3.p2.1.2" class="ltx_text ltx_font_italic">synthetic mated samples</span> by manipulating single image semantics.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p"><span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_italic">Controllability and scalability.</span> The advances in generative models have enabled the generation of synthetic data, incorporating fine-grained control over semantics. Consequently, synthetic datasets can be created to balance important factors of variation (<span id="S2.SS3.p3.1.2" class="ltx_text ltx_font_italic">e.g.</span>, the proportion of images pertained to male and female subjects), reducing biases caused by the unequal class distributions often observed in real-world datasets. Further, the employment of image synthesis models enables the generation of large-scale synthetic datasets, a factor known to correlate with the performance of DNNs.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p"><span id="S2.SS3.p4.1.1" class="ltx_text ltx_font_italic">Mitigating privacy concerns.</span> Finally, fully-synthetic datasets reduce privacy concerns related to the distribution and processing of sensitive human data. Despite known incidents of information leaks of GANs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, the reconstruction of training samples remains a challenge, as opposed to real data processing. We note that such <span id="S2.SS3.p4.1.2" class="ltx_text ltx_font_italic">information leakage</span> is of concern and a set of related countermeasures have been identified, such as the concepts of <span id="S2.SS3.p4.1.3" class="ltx_text ltx_font_italic">differential privacy</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and <span id="S2.SS3.p4.1.4" class="ltx_text ltx_font_italic">precision reduction</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Due to legal and privacy concerns, large-scale biometric datasets, such as MegaFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, have been withdrawn from public channels. Instead, we envision that large-scale synthetic datasets will be made publicly available for DNN training and evaluation.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span><span id="S2.SS4.1.1" class="ltx_text ltx_font_italic">Human analysis</span>
</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">This survey defines <span id="S2.SS4.p1.1.1" class="ltx_text ltx_font_italic">human analysis</span> as the analysis of human characteristics, behaviour, and interaction with the physical world. Such analysis has myriad applications, summarised in Figure <a href="#S2.F4" title="Figure 4 ‣ 2.4 Human analysis ‣ 2 Synthetic data in human analysis ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. To elaborate, we note the following applications.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p"><span id="S2.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Action recognition</span> focuses on recognizing activity of individual(s) from a series of observations from data subjects and their environment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p"><span id="S2.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Biometric recognition</span> refers to the automated recognition of individuals based on their biological and behavioural characteristics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i3.p1" class="ltx_para">
<p id="S2.I2.i3.p1.1" class="ltx_p"><span id="S2.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Emotion recognition</span> refers to the process of classifying human emotion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
</li>
<li id="S2.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i4.p1" class="ltx_para">
<p id="S2.I2.i4.p1.1" class="ltx_p"><span id="S2.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">Soft biometric classification</span> aims at automated classification of human characteristics in pre-defined categories, such as demographic, anthropometric or behavioural groups <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
</li>
<li id="S2.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i5.p1" class="ltx_para">
<p id="S2.I2.i5.p1.1" class="ltx_p"><span id="S2.I2.i5.p1.1.1" class="ltx_text ltx_font_bold">Presentation attack detection</span> (PAD) refers to the automated determination of a presentation to the biometric data capture subsystem to interfere with the operation of the biometric system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.</p>
</div>
</li>
<li id="S2.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i6.p1" class="ltx_para">
<p id="S2.I2.i6.p1.1" class="ltx_p"><span id="S2.I2.i6.p1.1.1" class="ltx_text ltx_font_bold">Human interaction recognition</span> is the task of analysing human interactions of at least two individuals who are interrelated to each other (<span id="S2.I2.i6.p1.1.2" class="ltx_text ltx_font_italic">e.g.</span>, handshaking) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
</li>
<li id="S2.I2.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i7.p1" class="ltx_para">
<p id="S2.I2.i7.p1.1" class="ltx_p"><span id="S2.I2.i7.p1.1.1" class="ltx_text ltx_font_bold">People detection/counting</span> denotes the detection or counting of individuals within a given image or video <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.</p>
</div>
</li>
<li id="S2.I2.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i8.p1" class="ltx_para">
<p id="S2.I2.i8.p1.1" class="ltx_p"><span id="S2.I2.i8.p1.1.1" class="ltx_text ltx_font_bold">Semantic segmentation</span> signifies the pixel-based image classification with the goal of tracking human bodies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> or body parts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> in a given image or video.</p>
</div>
</li>
<li id="S2.I2.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i9.p1" class="ltx_para">
<p id="S2.I2.i9.p1.1" class="ltx_p"><span id="S2.I2.i9.p1.1.1" class="ltx_text ltx_font_bold">Pose estimation</span> estimates the transformation of the human body <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> or head <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> from a reference pose, given an image or a 3D scan.</p>
</div>
</li>
<li id="S2.I2.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i10.p1" class="ltx_para">
<p id="S2.I2.i10.p1.1" class="ltx_p"><span id="S2.I2.i10.p1.1.1" class="ltx_text ltx_font_bold">Person Re-Identification</span> is the task of identifying an individual captured in images and videos acquired from different cameras or camera angles <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite></p>
</div>
</li>
<li id="S2.I2.i11" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i11.p1" class="ltx_para">
<p id="S2.I2.i11.p1.1" class="ltx_p"><span id="S2.I2.i11.p1.1.1" class="ltx_text ltx_font_bold">Anomaly detection</span> refers to classifiers trained to detect human behaviours, interactions, or movements deviating from normality <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>.</p>
</div>
</li>
<li id="S2.I2.i12" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i12.p1" class="ltx_para">
<p id="S2.I2.i12.p1.1" class="ltx_p"><span id="S2.I2.i12.p1.1.1" class="ltx_text ltx_font_bold">Medical analysis</span> refers to the automated analysis of data collected in medical applications with the greater goal of restoring and maintaining human health. In this survey, synthetic data in medical applications is considered out-of-scope, and interested readers are referred to the work of Chen <span id="S2.I2.i12.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<figure id="S2.F4" class="ltx_figure">
<div id="S2.F4.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:0pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(213.5pt,0.0pt) scale(65.0430985154701,65.0430985154701) ;"><svg id="S2.F4.1.pic1" class="ltx_picture" height="1" overflow="visible" version="1.1" width="1"><g transform="translate(0,1) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignObject width="0" height="24.91" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F4.1.pic1.1.1.1.1" class="ltx_ERROR undefined">\Tree</span>
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break">
<br class="ltx_break"></foreignObject></g></svg>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S2.F4.4.2" class="ltx_text" style="font-size:90%;">Application domains in human analysis</span></figcaption>
</figure>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.122.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S2.T1.123.2" class="ltx_text" style="font-size:90%;">Performance of human analysis models trained with and w/o synthetic data. Numbers given in % (DE=data enrichment, DR=data replacement, EER=equal error rate, MAE=mean absolute error, MSE=mean square error, FNMR=false non-match rate, U=Illumination, E=Expression, P=Pose).</span></figcaption>
<div id="S2.T1.120.120" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:340pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-254.4pt,199.2pt) scale(0.46006829178674,0.46006829178674) ;">
<table id="S2.T1.120.120.120" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.120.120.120.121.1" class="ltx_tr">
<th id="S2.T1.120.120.120.121.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_rr ltx_border_t"><span id="S2.T1.120.120.120.121.1.1.1" class="ltx_text ltx_font_bold">Reference</span></th>
<th id="S2.T1.120.120.120.121.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T1.120.120.120.121.1.2.1" class="ltx_text ltx_font_bold">Application Domain</span></th>
<th id="S2.T1.120.120.120.121.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_rr ltx_border_t"><span id="S2.T1.120.120.120.121.1.3.1" class="ltx_text ltx_font_bold">Application Type</span></th>
<th id="S2.T1.120.120.120.121.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_rr ltx_border_t"><span id="S2.T1.120.120.120.121.1.4.1" class="ltx_text ltx_font_bold">Metric</span></th>
<th id="S2.T1.120.120.120.121.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T1.120.120.120.121.1.5.1" class="ltx_text ltx_font_bold">w/o synthetic data</span></th>
<th id="S2.T1.120.120.120.121.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T1.120.120.120.121.1.6.1" class="ltx_text ltx_font_bold">DE</span></th>
<th id="S2.T1.120.120.120.121.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T1.120.120.120.121.1.7.1" class="ltx_text ltx_font_bold">DR</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_tt">Aranjuelo <span id="S2.T1.1.1.1.1.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S2.T1.1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">people detection</td>
<td id="S2.T1.1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_rr ltx_border_tt">Augmented Training</td>
<td id="S2.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_tt">Average Precision (<math id="S2.T1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S2.T1.1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">70</td>
<td id="S2.T1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">82</td>
<td id="S2.T1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">-</td>
</tr>
<tr id="S2.T1.4.4.4.4" class="ltx_tr">
<td id="S2.T1.4.4.4.4.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Wang <span id="S2.T1.4.4.4.4.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>
</td>
<td id="S2.T1.4.4.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">crowd counting</td>
<td id="S2.T1.4.4.4.4.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Synthetic Training</td>
<td id="S2.T1.2.2.2.2.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">MAE (<math id="S2.T1.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.2.2.2.2.1.m1.1a"><mo stretchy="false" id="S2.T1.2.2.2.2.1.m1.1.1" xref="S2.T1.2.2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.2.1.m1.1b"><ci id="S2.T1.2.2.2.2.1.m1.1.1.cmml" xref="S2.T1.2.2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.2.1.m1.1c">\downarrow</annotation></semantics></math>)</td>
<td id="S2.T1.3.3.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.3.3.3.3.2.m1.1" class="ltx_Math" alttext="275.5" display="inline"><semantics id="S2.T1.3.3.3.3.2.m1.1a"><mn id="S2.T1.3.3.3.3.2.m1.1.1" xref="S2.T1.3.3.3.3.2.m1.1.1.cmml">275.5</mn><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.3.2.m1.1b"><cn type="float" id="S2.T1.3.3.3.3.2.m1.1.1.cmml" xref="S2.T1.3.3.3.3.2.m1.1.1">275.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.3.2.m1.1c">275.5</annotation></semantics></math></td>
<td id="S2.T1.4.4.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.4.4.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.4.4.4.4.3.m1.1" class="ltx_Math" alttext="225.9" display="inline"><semantics id="S2.T1.4.4.4.4.3.m1.1a"><mn id="S2.T1.4.4.4.4.3.m1.1.1" xref="S2.T1.4.4.4.4.3.m1.1.1.cmml">225.9</mn><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.4.4.3.m1.1b"><cn type="float" id="S2.T1.4.4.4.4.3.m1.1.1.cmml" xref="S2.T1.4.4.4.4.3.m1.1.1">225.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.4.4.3.m1.1c">225.9</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.7.7.7.7" class="ltx_tr">
<td id="S2.T1.7.7.7.7.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Yadav <span id="S2.T1.7.7.7.7.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S2.T1.7.7.7.7.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">unseen iris PAD</td>
<td id="S2.T1.7.7.7.7.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Augmented Training</td>
<td id="S2.T1.5.5.5.5.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">EER (<math id="S2.T1.5.5.5.5.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.5.5.5.5.1.m1.1a"><mo stretchy="false" id="S2.T1.5.5.5.5.1.m1.1.1" xref="S2.T1.5.5.5.5.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.5.5.1.m1.1b"><ci id="S2.T1.5.5.5.5.1.m1.1.1.cmml" xref="S2.T1.5.5.5.5.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.5.5.1.m1.1c">\downarrow</annotation></semantics></math>)</td>
<td id="S2.T1.6.6.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.6.6.6.6.2.m1.1" class="ltx_Math" alttext="25.18" display="inline"><semantics id="S2.T1.6.6.6.6.2.m1.1a"><mn id="S2.T1.6.6.6.6.2.m1.1.1" xref="S2.T1.6.6.6.6.2.m1.1.1.cmml">25.18</mn><annotation-xml encoding="MathML-Content" id="S2.T1.6.6.6.6.2.m1.1b"><cn type="float" id="S2.T1.6.6.6.6.2.m1.1.1.cmml" xref="S2.T1.6.6.6.6.2.m1.1.1">25.18</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.6.6.6.2.m1.1c">25.18</annotation></semantics></math></td>
<td id="S2.T1.7.7.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.7.7.7.7.3.m1.1" class="ltx_Math" alttext="18.52" display="inline"><semantics id="S2.T1.7.7.7.7.3.m1.1a"><mn id="S2.T1.7.7.7.7.3.m1.1.1" xref="S2.T1.7.7.7.7.3.m1.1.1.cmml">18.52</mn><annotation-xml encoding="MathML-Content" id="S2.T1.7.7.7.7.3.m1.1b"><cn type="float" id="S2.T1.7.7.7.7.3.m1.1.1.cmml" xref="S2.T1.7.7.7.7.3.m1.1.1">18.52</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.7.7.7.3.m1.1c">18.52</annotation></semantics></math></td>
<td id="S2.T1.7.7.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.10.10.10.10" class="ltx_tr">
<td id="S2.T1.10.10.10.10.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Grosz and Jain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S2.T1.10.10.10.10.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Fingerprint PAD</td>
<td id="S2.T1.10.10.10.10.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Augmented and Synthetic Training</td>
<td id="S2.T1.8.8.8.8.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Accuracy (<math id="S2.T1.8.8.8.8.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.8.8.8.8.1.m1.1a"><mo stretchy="false" id="S2.T1.8.8.8.8.1.m1.1.1" xref="S2.T1.8.8.8.8.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.8.8.8.8.1.m1.1b"><ci id="S2.T1.8.8.8.8.1.m1.1.1.cmml" xref="S2.T1.8.8.8.8.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.8.8.8.8.1.m1.1c">\downarrow</annotation></semantics></math>)</td>
<td id="S2.T1.9.9.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.9.9.9.9.2.m1.1" class="ltx_Math" alttext="99.52" display="inline"><semantics id="S2.T1.9.9.9.9.2.m1.1a"><mn id="S2.T1.9.9.9.9.2.m1.1.1" xref="S2.T1.9.9.9.9.2.m1.1.1.cmml">99.52</mn><annotation-xml encoding="MathML-Content" id="S2.T1.9.9.9.9.2.m1.1b"><cn type="float" id="S2.T1.9.9.9.9.2.m1.1.1.cmml" xref="S2.T1.9.9.9.9.2.m1.1.1">99.52</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.9.9.9.9.2.m1.1c">99.52</annotation></semantics></math></td>
<td id="S2.T1.10.10.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">100</td>
<td id="S2.T1.10.10.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.10.10.10.10.3.m1.1" class="ltx_Math" alttext="36.53" display="inline"><semantics id="S2.T1.10.10.10.10.3.m1.1a"><mn id="S2.T1.10.10.10.10.3.m1.1.1" xref="S2.T1.10.10.10.10.3.m1.1.1.cmml">36.53</mn><annotation-xml encoding="MathML-Content" id="S2.T1.10.10.10.10.3.m1.1b"><cn type="float" id="S2.T1.10.10.10.10.3.m1.1.1.cmml" xref="S2.T1.10.10.10.10.3.m1.1.1">36.53</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.10.10.10.10.3.m1.1c">36.53</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.13.13.13.13" class="ltx_tr">
<td id="S2.T1.13.13.13.13.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Bird <span id="S2.T1.13.13.13.13.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S2.T1.13.13.13.13.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">speaker recognition</td>
<td id="S2.T1.13.13.13.13.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Model Initialization</td>
<td id="S2.T1.11.11.11.11.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Average Accuracy (<math id="S2.T1.11.11.11.11.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.11.11.11.11.1.m1.1a"><mo stretchy="false" id="S2.T1.11.11.11.11.1.m1.1.1" xref="S2.T1.11.11.11.11.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.11.11.11.11.1.m1.1b"><ci id="S2.T1.11.11.11.11.1.m1.1.1.cmml" xref="S2.T1.11.11.11.11.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.11.11.11.11.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.12.12.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.12.12.12.12.2.m1.1" class="ltx_Math" alttext="95.48" display="inline"><semantics id="S2.T1.12.12.12.12.2.m1.1a"><mn id="S2.T1.12.12.12.12.2.m1.1.1" xref="S2.T1.12.12.12.12.2.m1.1.1.cmml">95.48</mn><annotation-xml encoding="MathML-Content" id="S2.T1.12.12.12.12.2.m1.1b"><cn type="float" id="S2.T1.12.12.12.12.2.m1.1.1.cmml" xref="S2.T1.12.12.12.12.2.m1.1.1">95.48</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.12.12.12.12.2.m1.1c">95.48</annotation></semantics></math></td>
<td id="S2.T1.13.13.13.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.13.13.13.13.3.m1.1" class="ltx_Math" alttext="99.35" display="inline"><semantics id="S2.T1.13.13.13.13.3.m1.1a"><mn id="S2.T1.13.13.13.13.3.m1.1.1" xref="S2.T1.13.13.13.13.3.m1.1.1.cmml">99.35</mn><annotation-xml encoding="MathML-Content" id="S2.T1.13.13.13.13.3.m1.1b"><cn type="float" id="S2.T1.13.13.13.13.3.m1.1.1.cmml" xref="S2.T1.13.13.13.13.3.m1.1.1">99.35</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.13.13.13.13.3.m1.1c">99.35</annotation></semantics></math></td>
<td id="S2.T1.13.13.13.13.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.16.16.16.16" class="ltx_tr">
<td id="S2.T1.16.16.16.16.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Tapia <span id="S2.T1.16.16.16.16.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>
</td>
<td id="S2.T1.16.16.16.16.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Gender classification from periocular images</td>
<td id="S2.T1.16.16.16.16.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Evaluation</td>
<td id="S2.T1.14.14.14.14.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Accuracy (<math id="S2.T1.14.14.14.14.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.14.14.14.14.1.m1.1a"><mo stretchy="false" id="S2.T1.14.14.14.14.1.m1.1.1" xref="S2.T1.14.14.14.14.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.14.14.14.14.1.m1.1b"><ci id="S2.T1.14.14.14.14.1.m1.1.1.cmml" xref="S2.T1.14.14.14.14.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.14.14.14.14.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.15.15.15.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.15.15.15.15.2.m1.1" class="ltx_Math" alttext="82.76" display="inline"><semantics id="S2.T1.15.15.15.15.2.m1.1a"><mn id="S2.T1.15.15.15.15.2.m1.1.1" xref="S2.T1.15.15.15.15.2.m1.1.1.cmml">82.76</mn><annotation-xml encoding="MathML-Content" id="S2.T1.15.15.15.15.2.m1.1b"><cn type="float" id="S2.T1.15.15.15.15.2.m1.1.1.cmml" xref="S2.T1.15.15.15.15.2.m1.1.1">82.76</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.15.15.15.15.2.m1.1c">82.76</annotation></semantics></math></td>
<td id="S2.T1.16.16.16.16.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.16.16.16.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.16.16.16.16.3.m1.1" class="ltx_Math" alttext="91.9" display="inline"><semantics id="S2.T1.16.16.16.16.3.m1.1a"><mn id="S2.T1.16.16.16.16.3.m1.1.1" xref="S2.T1.16.16.16.16.3.m1.1.1.cmml">91.9</mn><annotation-xml encoding="MathML-Content" id="S2.T1.16.16.16.16.3.m1.1b"><cn type="float" id="S2.T1.16.16.16.16.3.m1.1.1.cmml" xref="S2.T1.16.16.16.16.3.m1.1.1">91.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.16.16.16.16.3.m1.1c">91.9</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.19.19.19.19" class="ltx_tr">
<td id="S2.T1.19.19.19.19.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Anton <span id="S2.T1.19.19.19.19.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>
</td>
<td id="S2.T1.19.19.19.19.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Voice Recognition</td>
<td id="S2.T1.19.19.19.19.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Augmented Training</td>
<td id="S2.T1.17.17.17.17.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Accuracy (<math id="S2.T1.17.17.17.17.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.17.17.17.17.1.m1.1a"><mo stretchy="false" id="S2.T1.17.17.17.17.1.m1.1.1" xref="S2.T1.17.17.17.17.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.17.17.17.17.1.m1.1b"><ci id="S2.T1.17.17.17.17.1.m1.1.1.cmml" xref="S2.T1.17.17.17.17.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.17.17.17.17.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.18.18.18.18.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.18.18.18.18.2.m1.1" class="ltx_Math" alttext="51.55" display="inline"><semantics id="S2.T1.18.18.18.18.2.m1.1a"><mn id="S2.T1.18.18.18.18.2.m1.1.1" xref="S2.T1.18.18.18.18.2.m1.1.1.cmml">51.55</mn><annotation-xml encoding="MathML-Content" id="S2.T1.18.18.18.18.2.m1.1b"><cn type="float" id="S2.T1.18.18.18.18.2.m1.1.1.cmml" xref="S2.T1.18.18.18.18.2.m1.1.1">51.55</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.18.18.18.18.2.m1.1c">51.55</annotation></semantics></math></td>
<td id="S2.T1.19.19.19.19.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.19.19.19.19.3.m1.1" class="ltx_Math" alttext="57.76" display="inline"><semantics id="S2.T1.19.19.19.19.3.m1.1a"><mn id="S2.T1.19.19.19.19.3.m1.1.1" xref="S2.T1.19.19.19.19.3.m1.1.1.cmml">57.76</mn><annotation-xml encoding="MathML-Content" id="S2.T1.19.19.19.19.3.m1.1b"><cn type="float" id="S2.T1.19.19.19.19.3.m1.1.1.cmml" xref="S2.T1.19.19.19.19.3.m1.1.1">57.76</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.19.19.19.19.3.m1.1c">57.76</annotation></semantics></math></td>
<td id="S2.T1.19.19.19.19.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.22.22.22.22" class="ltx_tr">
<td id="S2.T1.22.22.22.22.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Acien <span id="S2.T1.22.22.22.22.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>
</td>
<td id="S2.T1.22.22.22.22.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Bot Detection</td>
<td id="S2.T1.22.22.22.22.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Augmented Training</td>
<td id="S2.T1.20.20.20.20.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">EER (<math id="S2.T1.20.20.20.20.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.20.20.20.20.1.m1.1a"><mo stretchy="false" id="S2.T1.20.20.20.20.1.m1.1.1" xref="S2.T1.20.20.20.20.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.20.20.20.20.1.m1.1b"><ci id="S2.T1.20.20.20.20.1.m1.1.1.cmml" xref="S2.T1.20.20.20.20.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.20.20.20.20.1.m1.1c">\downarrow</annotation></semantics></math>)</td>
<td id="S2.T1.21.21.21.21.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.21.21.21.21.2.m1.1" class="ltx_Math" alttext="23.2" display="inline"><semantics id="S2.T1.21.21.21.21.2.m1.1a"><mn id="S2.T1.21.21.21.21.2.m1.1.1" xref="S2.T1.21.21.21.21.2.m1.1.1.cmml">23.2</mn><annotation-xml encoding="MathML-Content" id="S2.T1.21.21.21.21.2.m1.1b"><cn type="float" id="S2.T1.21.21.21.21.2.m1.1.1.cmml" xref="S2.T1.21.21.21.21.2.m1.1.1">23.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.21.21.21.21.2.m1.1c">23.2</annotation></semantics></math></td>
<td id="S2.T1.22.22.22.22.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.22.22.22.22.3.m1.1" class="ltx_Math" alttext="0.8" display="inline"><semantics id="S2.T1.22.22.22.22.3.m1.1a"><mn id="S2.T1.22.22.22.22.3.m1.1.1" xref="S2.T1.22.22.22.22.3.m1.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S2.T1.22.22.22.22.3.m1.1b"><cn type="float" id="S2.T1.22.22.22.22.3.m1.1.1.cmml" xref="S2.T1.22.22.22.22.3.m1.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.22.22.22.22.3.m1.1c">0.8</annotation></semantics></math></td>
<td id="S2.T1.22.22.22.22.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.24.24.24.24" class="ltx_tr">
<td id="S2.T1.24.24.24.24.3" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Han <span id="S2.T1.24.24.24.24.3.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
</td>
<td id="S2.T1.24.24.24.24.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Face Detection</td>
<td id="S2.T1.24.24.24.24.5" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Evaluation</td>
<td id="S2.T1.23.23.23.23.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Average Precision (<math id="S2.T1.23.23.23.23.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.23.23.23.23.1.m1.1a"><mo stretchy="false" id="S2.T1.23.23.23.23.1.m1.1.1" xref="S2.T1.23.23.23.23.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.23.23.23.23.1.m1.1b"><ci id="S2.T1.23.23.23.23.1.m1.1.1.cmml" xref="S2.T1.23.23.23.23.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.23.23.23.23.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.24.24.24.24.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">64</td>
<td id="S2.T1.24.24.24.24.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.24.24.24.24.2.m1.1" class="ltx_Math" alttext="74.5" display="inline"><semantics id="S2.T1.24.24.24.24.2.m1.1a"><mn id="S2.T1.24.24.24.24.2.m1.1.1" xref="S2.T1.24.24.24.24.2.m1.1.1.cmml">74.5</mn><annotation-xml encoding="MathML-Content" id="S2.T1.24.24.24.24.2.m1.1b"><cn type="float" id="S2.T1.24.24.24.24.2.m1.1.1.cmml" xref="S2.T1.24.24.24.24.2.m1.1.1">74.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.24.24.24.24.2.m1.1c">74.5</annotation></semantics></math></td>
<td id="S2.T1.24.24.24.24.7" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S2.T1.27.27.27.27" class="ltx_tr">
<td id="S2.T1.27.27.27.27.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Basak <span id="S2.T1.27.27.27.27.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>
</td>
<td id="S2.T1.27.27.27.27.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">head pose estimation</td>
<td id="S2.T1.27.27.27.27.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Domain Adaptation</td>
<td id="S2.T1.25.25.25.25.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">MAE (<math id="S2.T1.25.25.25.25.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.25.25.25.25.1.m1.1a"><mo stretchy="false" id="S2.T1.25.25.25.25.1.m1.1.1" xref="S2.T1.25.25.25.25.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.25.25.25.25.1.m1.1b"><ci id="S2.T1.25.25.25.25.1.m1.1.1.cmml" xref="S2.T1.25.25.25.25.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.25.25.25.25.1.m1.1c">\downarrow</annotation></semantics></math>)</td>
<td id="S2.T1.26.26.26.26.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.26.26.26.26.2.m1.1" class="ltx_Math" alttext="6.34" display="inline"><semantics id="S2.T1.26.26.26.26.2.m1.1a"><mn id="S2.T1.26.26.26.26.2.m1.1.1" xref="S2.T1.26.26.26.26.2.m1.1.1.cmml">6.34</mn><annotation-xml encoding="MathML-Content" id="S2.T1.26.26.26.26.2.m1.1b"><cn type="float" id="S2.T1.26.26.26.26.2.m1.1.1.cmml" xref="S2.T1.26.26.26.26.2.m1.1.1">6.34</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.26.26.26.26.2.m1.1c">6.34</annotation></semantics></math></td>
<td id="S2.T1.27.27.27.27.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.27.27.27.27.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.27.27.27.27.3.m1.1" class="ltx_Math" alttext="5.13" display="inline"><semantics id="S2.T1.27.27.27.27.3.m1.1a"><mn id="S2.T1.27.27.27.27.3.m1.1.1" xref="S2.T1.27.27.27.27.3.m1.1.1.cmml">5.13</mn><annotation-xml encoding="MathML-Content" id="S2.T1.27.27.27.27.3.m1.1b"><cn type="float" id="S2.T1.27.27.27.27.3.m1.1.1.cmml" xref="S2.T1.27.27.27.27.3.m1.1.1">5.13</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.27.27.27.27.3.m1.1c">5.13</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.30.30.30.30" class="ltx_tr">
<td id="S2.T1.30.30.30.30.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Bird <span id="S2.T1.30.30.30.30.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>
</td>
<td id="S2.T1.30.30.30.30.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">speaker recognition</td>
<td id="S2.T1.30.30.30.30.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Model Initialization</td>
<td id="S2.T1.28.28.28.28.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Accuracy (<math id="S2.T1.28.28.28.28.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.28.28.28.28.1.m1.1a"><mo stretchy="false" id="S2.T1.28.28.28.28.1.m1.1.1" xref="S2.T1.28.28.28.28.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.28.28.28.28.1.m1.1b"><ci id="S2.T1.28.28.28.28.1.m1.1.1.cmml" xref="S2.T1.28.28.28.28.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.28.28.28.28.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.29.29.29.29.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.29.29.29.29.2.m1.1" class="ltx_Math" alttext="96.58" display="inline"><semantics id="S2.T1.29.29.29.29.2.m1.1a"><mn id="S2.T1.29.29.29.29.2.m1.1.1" xref="S2.T1.29.29.29.29.2.m1.1.1.cmml">96.58</mn><annotation-xml encoding="MathML-Content" id="S2.T1.29.29.29.29.2.m1.1b"><cn type="float" id="S2.T1.29.29.29.29.2.m1.1.1.cmml" xref="S2.T1.29.29.29.29.2.m1.1.1">96.58</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.29.29.29.29.2.m1.1c">96.58</annotation></semantics></math></td>
<td id="S2.T1.30.30.30.30.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.30.30.30.30.3.m1.1" class="ltx_Math" alttext="98.83" display="inline"><semantics id="S2.T1.30.30.30.30.3.m1.1a"><mn id="S2.T1.30.30.30.30.3.m1.1.1" xref="S2.T1.30.30.30.30.3.m1.1.1.cmml">98.83</mn><annotation-xml encoding="MathML-Content" id="S2.T1.30.30.30.30.3.m1.1b"><cn type="float" id="S2.T1.30.30.30.30.3.m1.1.1.cmml" xref="S2.T1.30.30.30.30.3.m1.1.1">98.83</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.30.30.30.30.3.m1.1c">98.83</annotation></semantics></math></td>
<td id="S2.T1.30.30.30.30.7" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S2.T1.33.33.33.33" class="ltx_tr">
<td id="S2.T1.33.33.33.33.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Dou <span id="S2.T1.33.33.33.33.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</td>
<td id="S2.T1.33.33.33.33.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Gait recognition</td>
<td id="S2.T1.33.33.33.33.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Augmented Training</td>
<td id="S2.T1.31.31.31.31.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Rank-1 Accuracy (<math id="S2.T1.31.31.31.31.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.31.31.31.31.1.m1.1a"><mo stretchy="false" id="S2.T1.31.31.31.31.1.m1.1.1" xref="S2.T1.31.31.31.31.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.31.31.31.31.1.m1.1b"><ci id="S2.T1.31.31.31.31.1.m1.1.1.cmml" xref="S2.T1.31.31.31.31.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.31.31.31.31.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.32.32.32.32.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.32.32.32.32.2.m1.1" class="ltx_Math" alttext="95.0" display="inline"><semantics id="S2.T1.32.32.32.32.2.m1.1a"><mn id="S2.T1.32.32.32.32.2.m1.1.1" xref="S2.T1.32.32.32.32.2.m1.1.1.cmml">95.0</mn><annotation-xml encoding="MathML-Content" id="S2.T1.32.32.32.32.2.m1.1b"><cn type="float" id="S2.T1.32.32.32.32.2.m1.1.1.cmml" xref="S2.T1.32.32.32.32.2.m1.1.1">95.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.32.32.32.32.2.m1.1c">95.0</annotation></semantics></math></td>
<td id="S2.T1.33.33.33.33.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.33.33.33.33.3.m1.1" class="ltx_Math" alttext="96.4" display="inline"><semantics id="S2.T1.33.33.33.33.3.m1.1a"><mn id="S2.T1.33.33.33.33.3.m1.1.1" xref="S2.T1.33.33.33.33.3.m1.1.1.cmml">96.4</mn><annotation-xml encoding="MathML-Content" id="S2.T1.33.33.33.33.3.m1.1b"><cn type="float" id="S2.T1.33.33.33.33.3.m1.1.1.cmml" xref="S2.T1.33.33.33.33.3.m1.1.1">96.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.33.33.33.33.3.m1.1c">96.4</annotation></semantics></math></td>
<td id="S2.T1.33.33.33.33.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.35.35.35.35" class="ltx_tr">
<td id="S2.T1.35.35.35.35.3" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Piplani <span id="S2.T1.35.35.35.35.3.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>
</td>
<td id="S2.T1.35.35.35.35.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">passthought authentication</td>
<td id="S2.T1.35.35.35.35.5" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Augmented Training</td>
<td id="S2.T1.34.34.34.34.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Accuracy (<math id="S2.T1.34.34.34.34.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.34.34.34.34.1.m1.1a"><mo stretchy="false" id="S2.T1.34.34.34.34.1.m1.1.1" xref="S2.T1.34.34.34.34.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.34.34.34.34.1.m1.1b"><ci id="S2.T1.34.34.34.34.1.m1.1.1.cmml" xref="S2.T1.34.34.34.34.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.34.34.34.34.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.35.35.35.35.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.35.35.35.35.2.m1.1" class="ltx_Math" alttext="90.8" display="inline"><semantics id="S2.T1.35.35.35.35.2.m1.1a"><mn id="S2.T1.35.35.35.35.2.m1.1.1" xref="S2.T1.35.35.35.35.2.m1.1.1.cmml">90.8</mn><annotation-xml encoding="MathML-Content" id="S2.T1.35.35.35.35.2.m1.1b"><cn type="float" id="S2.T1.35.35.35.35.2.m1.1.1.cmml" xref="S2.T1.35.35.35.35.2.m1.1.1">90.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.35.35.35.35.2.m1.1c">90.8</annotation></semantics></math></td>
<td id="S2.T1.35.35.35.35.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">95</td>
<td id="S2.T1.35.35.35.35.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.38.38.38.38" class="ltx_tr">
<td id="S2.T1.38.38.38.38.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Buriro <span id="S2.T1.38.38.38.38.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</td>
<td id="S2.T1.38.38.38.38.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Smartphone User Authentication</td>
<td id="S2.T1.38.38.38.38.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Augmented Training</td>
<td id="S2.T1.36.36.36.36.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">true accept rate (<math id="S2.T1.36.36.36.36.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.36.36.36.36.1.m1.1a"><mo stretchy="false" id="S2.T1.36.36.36.36.1.m1.1.1" xref="S2.T1.36.36.36.36.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.36.36.36.36.1.m1.1b"><ci id="S2.T1.36.36.36.36.1.m1.1.1.cmml" xref="S2.T1.36.36.36.36.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.36.36.36.36.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.37.37.37.37.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.37.37.37.37.2.m1.1" class="ltx_Math" alttext="84.66" display="inline"><semantics id="S2.T1.37.37.37.37.2.m1.1a"><mn id="S2.T1.37.37.37.37.2.m1.1.1" xref="S2.T1.37.37.37.37.2.m1.1.1.cmml">84.66</mn><annotation-xml encoding="MathML-Content" id="S2.T1.37.37.37.37.2.m1.1b"><cn type="float" id="S2.T1.37.37.37.37.2.m1.1.1.cmml" xref="S2.T1.37.37.37.37.2.m1.1.1">84.66</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.37.37.37.37.2.m1.1c">84.66</annotation></semantics></math></td>
<td id="S2.T1.38.38.38.38.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.38.38.38.38.3.m1.1" class="ltx_Math" alttext="91.65" display="inline"><semantics id="S2.T1.38.38.38.38.3.m1.1a"><mn id="S2.T1.38.38.38.38.3.m1.1.1" xref="S2.T1.38.38.38.38.3.m1.1.1.cmml">91.65</mn><annotation-xml encoding="MathML-Content" id="S2.T1.38.38.38.38.3.m1.1b"><cn type="float" id="S2.T1.38.38.38.38.3.m1.1.1.cmml" xref="S2.T1.38.38.38.38.3.m1.1.1">91.65</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.38.38.38.38.3.m1.1c">91.65</annotation></semantics></math></td>
<td id="S2.T1.38.38.38.38.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.41.41.41.41" class="ltx_tr">
<td id="S2.T1.41.41.41.41.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Buriro <span id="S2.T1.41.41.41.41.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>
</td>
<td id="S2.T1.41.41.41.41.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Smartphone User Authentication</td>
<td id="S2.T1.41.41.41.41.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Augmented Training</td>
<td id="S2.T1.39.39.39.39.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">false accept rate (<math id="S2.T1.39.39.39.39.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.39.39.39.39.1.m1.1a"><mo stretchy="false" id="S2.T1.39.39.39.39.1.m1.1.1" xref="S2.T1.39.39.39.39.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.39.39.39.39.1.m1.1b"><ci id="S2.T1.39.39.39.39.1.m1.1.1.cmml" xref="S2.T1.39.39.39.39.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.39.39.39.39.1.m1.1c">\downarrow</annotation></semantics></math>)</td>
<td id="S2.T1.40.40.40.40.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.40.40.40.40.2.m1.1" class="ltx_Math" alttext="14.78" display="inline"><semantics id="S2.T1.40.40.40.40.2.m1.1a"><mn id="S2.T1.40.40.40.40.2.m1.1.1" xref="S2.T1.40.40.40.40.2.m1.1.1.cmml">14.78</mn><annotation-xml encoding="MathML-Content" id="S2.T1.40.40.40.40.2.m1.1b"><cn type="float" id="S2.T1.40.40.40.40.2.m1.1.1.cmml" xref="S2.T1.40.40.40.40.2.m1.1.1">14.78</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.40.40.40.40.2.m1.1c">14.78</annotation></semantics></math></td>
<td id="S2.T1.41.41.41.41.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.41.41.41.41.3.m1.1" class="ltx_Math" alttext="11.04" display="inline"><semantics id="S2.T1.41.41.41.41.3.m1.1a"><mn id="S2.T1.41.41.41.41.3.m1.1.1" xref="S2.T1.41.41.41.41.3.m1.1.1.cmml">11.04</mn><annotation-xml encoding="MathML-Content" id="S2.T1.41.41.41.41.3.m1.1b"><cn type="float" id="S2.T1.41.41.41.41.3.m1.1.1.cmml" xref="S2.T1.41.41.41.41.3.m1.1.1">11.04</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.41.41.41.41.3.m1.1c">11.04</annotation></semantics></math></td>
<td id="S2.T1.41.41.41.41.7" class="ltx_td ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S2.T1.43.43.43.43" class="ltx_tr">
<td id="S2.T1.43.43.43.43.3" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Gouiaa <span id="S2.T1.43.43.43.43.3.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</td>
<td id="S2.T1.43.43.43.43.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Posture recognition</td>
<td id="S2.T1.43.43.43.43.5" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Augmented Training</td>
<td id="S2.T1.42.42.42.42.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Accuracy (<math id="S2.T1.42.42.42.42.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.42.42.42.42.1.m1.1a"><mo stretchy="false" id="S2.T1.42.42.42.42.1.m1.1.1" xref="S2.T1.42.42.42.42.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.42.42.42.42.1.m1.1b"><ci id="S2.T1.42.42.42.42.1.m1.1.1.cmml" xref="S2.T1.42.42.42.42.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.42.42.42.42.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.43.43.43.43.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.43.43.43.43.2.m1.1" class="ltx_Math" alttext="94.58" display="inline"><semantics id="S2.T1.43.43.43.43.2.m1.1a"><mn id="S2.T1.43.43.43.43.2.m1.1.1" xref="S2.T1.43.43.43.43.2.m1.1.1.cmml">94.58</mn><annotation-xml encoding="MathML-Content" id="S2.T1.43.43.43.43.2.m1.1b"><cn type="float" id="S2.T1.43.43.43.43.2.m1.1.1.cmml" xref="S2.T1.43.43.43.43.2.m1.1.1">94.58</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.43.43.43.43.2.m1.1c">94.58</annotation></semantics></math></td>
<td id="S2.T1.43.43.43.43.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99</td>
<td id="S2.T1.43.43.43.43.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.46.46.46.46" class="ltx_tr">
<td id="S2.T1.46.46.46.46.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Ruiz <span id="S2.T1.46.46.46.46.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>
</td>
<td id="S2.T1.46.46.46.46.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Signature verification</td>
<td id="S2.T1.46.46.46.46.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Augmented Training</td>
<td id="S2.T1.44.44.44.44.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">EER (<math id="S2.T1.44.44.44.44.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.44.44.44.44.1.m1.1a"><mo stretchy="false" id="S2.T1.44.44.44.44.1.m1.1.1" xref="S2.T1.44.44.44.44.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.44.44.44.44.1.m1.1b"><ci id="S2.T1.44.44.44.44.1.m1.1.1.cmml" xref="S2.T1.44.44.44.44.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.44.44.44.44.1.m1.1c">\downarrow</annotation></semantics></math>)</td>
<td id="S2.T1.45.45.45.45.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.45.45.45.45.2.m1.1" class="ltx_Math" alttext="11.11" display="inline"><semantics id="S2.T1.45.45.45.45.2.m1.1a"><mn id="S2.T1.45.45.45.45.2.m1.1.1" xref="S2.T1.45.45.45.45.2.m1.1.1.cmml">11.11</mn><annotation-xml encoding="MathML-Content" id="S2.T1.45.45.45.45.2.m1.1b"><cn type="float" id="S2.T1.45.45.45.45.2.m1.1.1.cmml" xref="S2.T1.45.45.45.45.2.m1.1.1">11.11</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.45.45.45.45.2.m1.1c">11.11</annotation></semantics></math></td>
<td id="S2.T1.46.46.46.46.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.46.46.46.46.3.m1.1" class="ltx_Math" alttext="4.9" display="inline"><semantics id="S2.T1.46.46.46.46.3.m1.1a"><mn id="S2.T1.46.46.46.46.3.m1.1.1" xref="S2.T1.46.46.46.46.3.m1.1.1.cmml">4.9</mn><annotation-xml encoding="MathML-Content" id="S2.T1.46.46.46.46.3.m1.1b"><cn type="float" id="S2.T1.46.46.46.46.3.m1.1.1.cmml" xref="S2.T1.46.46.46.46.3.m1.1.1">4.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.46.46.46.46.3.m1.1c">4.9</annotation></semantics></math></td>
<td id="S2.T1.46.46.46.46.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.49.49.49.49" class="ltx_tr">
<td id="S2.T1.49.49.49.49.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Kortylewski <span id="S2.T1.49.49.49.49.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>
</td>
<td id="S2.T1.49.49.49.49.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Face recognition</td>
<td id="S2.T1.49.49.49.49.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Model Initialization</td>
<td id="S2.T1.47.47.47.47.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Accuracy (<math id="S2.T1.47.47.47.47.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.47.47.47.47.1.m1.1a"><mo stretchy="false" id="S2.T1.47.47.47.47.1.m1.1.1" xref="S2.T1.47.47.47.47.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.47.47.47.47.1.m1.1b"><ci id="S2.T1.47.47.47.47.1.m1.1.1.cmml" xref="S2.T1.47.47.47.47.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.47.47.47.47.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.48.48.48.48.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.48.48.48.48.2.m1.1" class="ltx_Math" alttext="91.2" display="inline"><semantics id="S2.T1.48.48.48.48.2.m1.1a"><mn id="S2.T1.48.48.48.48.2.m1.1.1" xref="S2.T1.48.48.48.48.2.m1.1.1.cmml">91.2</mn><annotation-xml encoding="MathML-Content" id="S2.T1.48.48.48.48.2.m1.1b"><cn type="float" id="S2.T1.48.48.48.48.2.m1.1.1.cmml" xref="S2.T1.48.48.48.48.2.m1.1.1">91.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.48.48.48.48.2.m1.1c">91.2</annotation></semantics></math></td>
<td id="S2.T1.49.49.49.49.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.49.49.49.49.3.m1.1" class="ltx_Math" alttext="93.3" display="inline"><semantics id="S2.T1.49.49.49.49.3.m1.1a"><mn id="S2.T1.49.49.49.49.3.m1.1.1" xref="S2.T1.49.49.49.49.3.m1.1.1.cmml">93.3</mn><annotation-xml encoding="MathML-Content" id="S2.T1.49.49.49.49.3.m1.1b"><cn type="float" id="S2.T1.49.49.49.49.3.m1.1.1.cmml" xref="S2.T1.49.49.49.49.3.m1.1.1">93.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.49.49.49.49.3.m1.1c">93.3</annotation></semantics></math></td>
<td id="S2.T1.49.49.49.49.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.52.52.52.52" class="ltx_tr">
<td id="S2.T1.52.52.52.52.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Trigueros <span id="S2.T1.52.52.52.52.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>
</td>
<td id="S2.T1.52.52.52.52.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Face recognition</td>
<td id="S2.T1.52.52.52.52.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Augmented Training</td>
<td id="S2.T1.50.50.50.50.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Accuracy (<math id="S2.T1.50.50.50.50.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.50.50.50.50.1.m1.1a"><mo stretchy="false" id="S2.T1.50.50.50.50.1.m1.1.1" xref="S2.T1.50.50.50.50.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.50.50.50.50.1.m1.1b"><ci id="S2.T1.50.50.50.50.1.m1.1.1.cmml" xref="S2.T1.50.50.50.50.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.50.50.50.50.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.51.51.51.51.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.51.51.51.51.2.m1.1" class="ltx_Math" alttext="67.58" display="inline"><semantics id="S2.T1.51.51.51.51.2.m1.1a"><mn id="S2.T1.51.51.51.51.2.m1.1.1" xref="S2.T1.51.51.51.51.2.m1.1.1.cmml">67.58</mn><annotation-xml encoding="MathML-Content" id="S2.T1.51.51.51.51.2.m1.1b"><cn type="float" id="S2.T1.51.51.51.51.2.m1.1.1.cmml" xref="S2.T1.51.51.51.51.2.m1.1.1">67.58</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.51.51.51.51.2.m1.1c">67.58</annotation></semantics></math></td>
<td id="S2.T1.52.52.52.52.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.52.52.52.52.3.m1.1" class="ltx_Math" alttext="69.02" display="inline"><semantics id="S2.T1.52.52.52.52.3.m1.1a"><mn id="S2.T1.52.52.52.52.3.m1.1.1" xref="S2.T1.52.52.52.52.3.m1.1.1.cmml">69.02</mn><annotation-xml encoding="MathML-Content" id="S2.T1.52.52.52.52.3.m1.1b"><cn type="float" id="S2.T1.52.52.52.52.3.m1.1.1.cmml" xref="S2.T1.52.52.52.52.3.m1.1.1">69.02</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.52.52.52.52.3.m1.1c">69.02</annotation></semantics></math></td>
<td id="S2.T1.52.52.52.52.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.55.55.55.55" class="ltx_tr">
<td id="S2.T1.55.55.55.55.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Zhai <span id="S2.T1.55.55.55.55.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>
</td>
<td id="S2.T1.55.55.55.55.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Face recognition</td>
<td id="S2.T1.55.55.55.55.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Augmented Training</td>
<td id="S2.T1.53.53.53.53.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">True Accept Rate (<math id="S2.T1.53.53.53.53.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.53.53.53.53.1.m1.1a"><mo stretchy="false" id="S2.T1.53.53.53.53.1.m1.1.1" xref="S2.T1.53.53.53.53.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.53.53.53.53.1.m1.1b"><ci id="S2.T1.53.53.53.53.1.m1.1.1.cmml" xref="S2.T1.53.53.53.53.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.53.53.53.53.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.54.54.54.54.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.54.54.54.54.2.m1.1" class="ltx_Math" alttext="61.72" display="inline"><semantics id="S2.T1.54.54.54.54.2.m1.1a"><mn id="S2.T1.54.54.54.54.2.m1.1.1" xref="S2.T1.54.54.54.54.2.m1.1.1.cmml">61.72</mn><annotation-xml encoding="MathML-Content" id="S2.T1.54.54.54.54.2.m1.1b"><cn type="float" id="S2.T1.54.54.54.54.2.m1.1.1.cmml" xref="S2.T1.54.54.54.54.2.m1.1.1">61.72</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.54.54.54.54.2.m1.1c">61.72</annotation></semantics></math></td>
<td id="S2.T1.55.55.55.55.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.55.55.55.55.3.m1.1" class="ltx_Math" alttext="86.66" display="inline"><semantics id="S2.T1.55.55.55.55.3.m1.1a"><mn id="S2.T1.55.55.55.55.3.m1.1.1" xref="S2.T1.55.55.55.55.3.m1.1.1.cmml">86.66</mn><annotation-xml encoding="MathML-Content" id="S2.T1.55.55.55.55.3.m1.1b"><cn type="float" id="S2.T1.55.55.55.55.3.m1.1.1.cmml" xref="S2.T1.55.55.55.55.3.m1.1.1">86.66</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.55.55.55.55.3.m1.1c">86.66</annotation></semantics></math></td>
<td id="S2.T1.55.55.55.55.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.58.58.58.58" class="ltx_tr">
<td id="S2.T1.58.58.58.58.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Chen <span id="S2.T1.58.58.58.58.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>
</td>
<td id="S2.T1.58.58.58.58.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Emotion State Classification</td>
<td id="S2.T1.58.58.58.58.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Augmented Training</td>
<td id="S2.T1.56.56.56.56.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Accuracy (<math id="S2.T1.56.56.56.56.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.56.56.56.56.1.m1.1a"><mo stretchy="false" id="S2.T1.56.56.56.56.1.m1.1.1" xref="S2.T1.56.56.56.56.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.56.56.56.56.1.m1.1b"><ci id="S2.T1.56.56.56.56.1.m1.1.1.cmml" xref="S2.T1.56.56.56.56.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.56.56.56.56.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.57.57.57.57.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.57.57.57.57.2.m1.1" class="ltx_Math" alttext="58.6" display="inline"><semantics id="S2.T1.57.57.57.57.2.m1.1a"><mn id="S2.T1.57.57.57.57.2.m1.1.1" xref="S2.T1.57.57.57.57.2.m1.1.1.cmml">58.6</mn><annotation-xml encoding="MathML-Content" id="S2.T1.57.57.57.57.2.m1.1b"><cn type="float" id="S2.T1.57.57.57.57.2.m1.1.1.cmml" xref="S2.T1.57.57.57.57.2.m1.1.1">58.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.57.57.57.57.2.m1.1c">58.6</annotation></semantics></math></td>
<td id="S2.T1.58.58.58.58.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.58.58.58.58.3.m1.1" class="ltx_Math" alttext="64.5" display="inline"><semantics id="S2.T1.58.58.58.58.3.m1.1a"><mn id="S2.T1.58.58.58.58.3.m1.1.1" xref="S2.T1.58.58.58.58.3.m1.1.1.cmml">64.5</mn><annotation-xml encoding="MathML-Content" id="S2.T1.58.58.58.58.3.m1.1b"><cn type="float" id="S2.T1.58.58.58.58.3.m1.1.1.cmml" xref="S2.T1.58.58.58.58.3.m1.1.1">64.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.58.58.58.58.3.m1.1c">64.5</annotation></semantics></math></td>
<td id="S2.T1.58.58.58.58.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.61.61.61.61" class="ltx_tr">
<td id="S2.T1.61.61.61.61.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Melo<span id="S2.T1.61.61.61.61.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>
</td>
<td id="S2.T1.61.61.61.61.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Signature Recognition</td>
<td id="S2.T1.61.61.61.61.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Synthetic Training</td>
<td id="S2.T1.59.59.59.59.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">EER (<math id="S2.T1.59.59.59.59.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.59.59.59.59.1.m1.1a"><mo stretchy="false" id="S2.T1.59.59.59.59.1.m1.1.1" xref="S2.T1.59.59.59.59.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.59.59.59.59.1.m1.1b"><ci id="S2.T1.59.59.59.59.1.m1.1.1.cmml" xref="S2.T1.59.59.59.59.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.59.59.59.59.1.m1.1c">\downarrow</annotation></semantics></math>)</td>
<td id="S2.T1.60.60.60.60.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.60.60.60.60.2.m1.1" class="ltx_Math" alttext="10.26" display="inline"><semantics id="S2.T1.60.60.60.60.2.m1.1a"><mn id="S2.T1.60.60.60.60.2.m1.1.1" xref="S2.T1.60.60.60.60.2.m1.1.1.cmml">10.26</mn><annotation-xml encoding="MathML-Content" id="S2.T1.60.60.60.60.2.m1.1b"><cn type="float" id="S2.T1.60.60.60.60.2.m1.1.1.cmml" xref="S2.T1.60.60.60.60.2.m1.1.1">10.26</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.60.60.60.60.2.m1.1c">10.26</annotation></semantics></math></td>
<td id="S2.T1.61.61.61.61.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.61.61.61.61.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.61.61.61.61.3.m1.1" class="ltx_Math" alttext="9.74" display="inline"><semantics id="S2.T1.61.61.61.61.3.m1.1a"><mn id="S2.T1.61.61.61.61.3.m1.1.1" xref="S2.T1.61.61.61.61.3.m1.1.1.cmml">9.74</mn><annotation-xml encoding="MathML-Content" id="S2.T1.61.61.61.61.3.m1.1b"><cn type="float" id="S2.T1.61.61.61.61.3.m1.1.1.cmml" xref="S2.T1.61.61.61.61.3.m1.1.1">9.74</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.61.61.61.61.3.m1.1c">9.74</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.63.63.63.63" class="ltx_tr">
<td id="S2.T1.63.63.63.63.3" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Öz <span id="S2.T1.63.63.63.63.3.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>
</td>
<td id="S2.T1.63.63.63.63.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Eye Segmentation</td>
<td id="S2.T1.63.63.63.63.5" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Augmented Training</td>
<td id="S2.T1.62.62.62.62.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">mIoU (<math id="S2.T1.62.62.62.62.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.62.62.62.62.1.m1.1a"><mo stretchy="false" id="S2.T1.62.62.62.62.1.m1.1.1" xref="S2.T1.62.62.62.62.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.62.62.62.62.1.m1.1b"><ci id="S2.T1.62.62.62.62.1.m1.1.1.cmml" xref="S2.T1.62.62.62.62.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.62.62.62.62.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.63.63.63.63.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">73</td>
<td id="S2.T1.63.63.63.63.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.63.63.63.63.2.m1.1" class="ltx_Math" alttext="75.4" display="inline"><semantics id="S2.T1.63.63.63.63.2.m1.1a"><mn id="S2.T1.63.63.63.63.2.m1.1.1" xref="S2.T1.63.63.63.63.2.m1.1.1.cmml">75.4</mn><annotation-xml encoding="MathML-Content" id="S2.T1.63.63.63.63.2.m1.1b"><cn type="float" id="S2.T1.63.63.63.63.2.m1.1.1.cmml" xref="S2.T1.63.63.63.63.2.m1.1.1">75.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.63.63.63.63.2.m1.1c">75.4</annotation></semantics></math></td>
<td id="S2.T1.63.63.63.63.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.65.65.65.65" class="ltx_tr">
<td id="S2.T1.65.65.65.65.3" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Wang <span id="S2.T1.65.65.65.65.3.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>
</td>
<td id="S2.T1.65.65.65.65.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Crowd Counting</td>
<td id="S2.T1.65.65.65.65.5" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Model Initialization</td>
<td id="S2.T1.64.64.64.64.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">MSE (<math id="S2.T1.64.64.64.64.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.64.64.64.64.1.m1.1a"><mo stretchy="false" id="S2.T1.64.64.64.64.1.m1.1.1" xref="S2.T1.64.64.64.64.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.64.64.64.64.1.m1.1b"><ci id="S2.T1.64.64.64.64.1.m1.1.1.cmml" xref="S2.T1.64.64.64.64.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.64.64.64.64.1.m1.1c">\downarrow</annotation></semantics></math>)</td>
<td id="S2.T1.65.65.65.65.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.65.65.65.65.2.m1.1" class="ltx_Math" alttext="14.3" display="inline"><semantics id="S2.T1.65.65.65.65.2.m1.1a"><mn id="S2.T1.65.65.65.65.2.m1.1.1" xref="S2.T1.65.65.65.65.2.m1.1.1.cmml">14.3</mn><annotation-xml encoding="MathML-Content" id="S2.T1.65.65.65.65.2.m1.1b"><cn type="float" id="S2.T1.65.65.65.65.2.m1.1.1.cmml" xref="S2.T1.65.65.65.65.2.m1.1.1">14.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.65.65.65.65.2.m1.1c">14.3</annotation></semantics></math></td>
<td id="S2.T1.65.65.65.65.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13</td>
<td id="S2.T1.65.65.65.65.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.69.69.69.69" class="ltx_tr">
<td id="S2.T1.69.69.69.69.5" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Irtem <span id="S2.T1.69.69.69.69.5.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S2.T1.69.69.69.69.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Fingerprint Classification</td>
<td id="S2.T1.69.69.69.69.7" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Joint and Synthetic Training</td>
<td id="S2.T1.66.66.66.66.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Classification accuracy (<math id="S2.T1.66.66.66.66.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.66.66.66.66.1.m1.1a"><mo stretchy="false" id="S2.T1.66.66.66.66.1.m1.1.1" xref="S2.T1.66.66.66.66.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.66.66.66.66.1.m1.1b"><ci id="S2.T1.66.66.66.66.1.m1.1.1.cmml" xref="S2.T1.66.66.66.66.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.66.66.66.66.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.67.67.67.67.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.67.67.67.67.2.m1.1" class="ltx_Math" alttext="91.9" display="inline"><semantics id="S2.T1.67.67.67.67.2.m1.1a"><mn id="S2.T1.67.67.67.67.2.m1.1.1" xref="S2.T1.67.67.67.67.2.m1.1.1.cmml">91.9</mn><annotation-xml encoding="MathML-Content" id="S2.T1.67.67.67.67.2.m1.1b"><cn type="float" id="S2.T1.67.67.67.67.2.m1.1.1.cmml" xref="S2.T1.67.67.67.67.2.m1.1.1">91.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.67.67.67.67.2.m1.1c">91.9</annotation></semantics></math></td>
<td id="S2.T1.68.68.68.68.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.68.68.68.68.3.m1.1" class="ltx_Math" alttext="95.53" display="inline"><semantics id="S2.T1.68.68.68.68.3.m1.1a"><mn id="S2.T1.68.68.68.68.3.m1.1.1" xref="S2.T1.68.68.68.68.3.m1.1.1.cmml">95.53</mn><annotation-xml encoding="MathML-Content" id="S2.T1.68.68.68.68.3.m1.1b"><cn type="float" id="S2.T1.68.68.68.68.3.m1.1.1.cmml" xref="S2.T1.68.68.68.68.3.m1.1.1">95.53</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.68.68.68.68.3.m1.1c">95.53</annotation></semantics></math></td>
<td id="S2.T1.69.69.69.69.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.69.69.69.69.4.m1.1" class="ltx_Math" alttext="69.47" display="inline"><semantics id="S2.T1.69.69.69.69.4.m1.1a"><mn id="S2.T1.69.69.69.69.4.m1.1.1" xref="S2.T1.69.69.69.69.4.m1.1.1.cmml">69.47</mn><annotation-xml encoding="MathML-Content" id="S2.T1.69.69.69.69.4.m1.1b"><cn type="float" id="S2.T1.69.69.69.69.4.m1.1.1.cmml" xref="S2.T1.69.69.69.69.4.m1.1.1">69.47</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.69.69.69.69.4.m1.1c">69.47</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.72.72.72.72" class="ltx_tr">
<td id="S2.T1.72.72.72.72.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Engelsma <span id="S2.T1.72.72.72.72.4.1" class="ltx_text ltx_font_italic">et al.</span>  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</td>
<td id="S2.T1.72.72.72.72.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Fingerprint Recognition</td>
<td id="S2.T1.72.72.72.72.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Model Initialization</td>
<td id="S2.T1.70.70.70.70.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">True acceptance rate (<math id="S2.T1.70.70.70.70.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.70.70.70.70.1.m1.1a"><mo stretchy="false" id="S2.T1.70.70.70.70.1.m1.1.1" xref="S2.T1.70.70.70.70.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.70.70.70.70.1.m1.1b"><ci id="S2.T1.70.70.70.70.1.m1.1.1.cmml" xref="S2.T1.70.70.70.70.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.70.70.70.70.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.71.71.71.71.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.71.71.71.71.2.m1.1" class="ltx_Math" alttext="73.37" display="inline"><semantics id="S2.T1.71.71.71.71.2.m1.1a"><mn id="S2.T1.71.71.71.71.2.m1.1.1" xref="S2.T1.71.71.71.71.2.m1.1.1.cmml">73.37</mn><annotation-xml encoding="MathML-Content" id="S2.T1.71.71.71.71.2.m1.1b"><cn type="float" id="S2.T1.71.71.71.71.2.m1.1.1.cmml" xref="S2.T1.71.71.71.71.2.m1.1.1">73.37</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.71.71.71.71.2.m1.1c">73.37</annotation></semantics></math></td>
<td id="S2.T1.72.72.72.72.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.72.72.72.72.3.m1.1" class="ltx_Math" alttext="87.03" display="inline"><semantics id="S2.T1.72.72.72.72.3.m1.1a"><mn id="S2.T1.72.72.72.72.3.m1.1.1" xref="S2.T1.72.72.72.72.3.m1.1.1.cmml">87.03</mn><annotation-xml encoding="MathML-Content" id="S2.T1.72.72.72.72.3.m1.1b"><cn type="float" id="S2.T1.72.72.72.72.3.m1.1.1.cmml" xref="S2.T1.72.72.72.72.3.m1.1.1">87.03</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.72.72.72.72.3.m1.1c">87.03</annotation></semantics></math></td>
<td id="S2.T1.72.72.72.72.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.75.75.75.75" class="ltx_tr">
<td id="S2.T1.75.75.75.75.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Bozorgtabar <span id="S2.T1.75.75.75.75.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</td>
<td id="S2.T1.75.75.75.75.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Expression Recognition</td>
<td id="S2.T1.75.75.75.75.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Domain Adaptation</td>
<td id="S2.T1.73.73.73.73.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">accuracy (<math id="S2.T1.73.73.73.73.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.73.73.73.73.1.m1.1a"><mo stretchy="false" id="S2.T1.73.73.73.73.1.m1.1.1" xref="S2.T1.73.73.73.73.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.73.73.73.73.1.m1.1b"><ci id="S2.T1.73.73.73.73.1.m1.1.1.cmml" xref="S2.T1.73.73.73.73.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.73.73.73.73.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.74.74.74.74.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.74.74.74.74.2.m1.1" class="ltx_Math" alttext="70.15" display="inline"><semantics id="S2.T1.74.74.74.74.2.m1.1a"><mn id="S2.T1.74.74.74.74.2.m1.1.1" xref="S2.T1.74.74.74.74.2.m1.1.1.cmml">70.15</mn><annotation-xml encoding="MathML-Content" id="S2.T1.74.74.74.74.2.m1.1b"><cn type="float" id="S2.T1.74.74.74.74.2.m1.1.1.cmml" xref="S2.T1.74.74.74.74.2.m1.1.1">70.15</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.74.74.74.74.2.m1.1c">70.15</annotation></semantics></math></td>
<td id="S2.T1.75.75.75.75.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.75.75.75.75.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.75.75.75.75.3.m1.1" class="ltx_Math" alttext="72.1" display="inline"><semantics id="S2.T1.75.75.75.75.3.m1.1a"><mn id="S2.T1.75.75.75.75.3.m1.1.1" xref="S2.T1.75.75.75.75.3.m1.1.1.cmml">72.1</mn><annotation-xml encoding="MathML-Content" id="S2.T1.75.75.75.75.3.m1.1b"><cn type="float" id="S2.T1.75.75.75.75.3.m1.1.1.cmml" xref="S2.T1.75.75.75.75.3.m1.1.1">72.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.75.75.75.75.3.m1.1c">72.1</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.79.79.79.79" class="ltx_tr">
<td id="S2.T1.79.79.79.79.5" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Qiu <span id="S2.T1.79.79.79.79.5.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</td>
<td id="S2.T1.79.79.79.79.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Face Recognition</td>
<td id="S2.T1.79.79.79.79.7" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Augmented and Synthetic Training</td>
<td id="S2.T1.76.76.76.76.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">accuracy (<math id="S2.T1.76.76.76.76.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.76.76.76.76.1.m1.1a"><mo stretchy="false" id="S2.T1.76.76.76.76.1.m1.1.1" xref="S2.T1.76.76.76.76.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.76.76.76.76.1.m1.1b"><ci id="S2.T1.76.76.76.76.1.m1.1.1.cmml" xref="S2.T1.76.76.76.76.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.76.76.76.76.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.77.77.77.77.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.77.77.77.77.2.m1.1" class="ltx_Math" alttext="91.22" display="inline"><semantics id="S2.T1.77.77.77.77.2.m1.1a"><mn id="S2.T1.77.77.77.77.2.m1.1.1" xref="S2.T1.77.77.77.77.2.m1.1.1.cmml">91.22</mn><annotation-xml encoding="MathML-Content" id="S2.T1.77.77.77.77.2.m1.1b"><cn type="float" id="S2.T1.77.77.77.77.2.m1.1.1.cmml" xref="S2.T1.77.77.77.77.2.m1.1.1">91.22</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.77.77.77.77.2.m1.1c">91.22</annotation></semantics></math></td>
<td id="S2.T1.78.78.78.78.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.78.78.78.78.3.m1.1" class="ltx_Math" alttext="95.78" display="inline"><semantics id="S2.T1.78.78.78.78.3.m1.1a"><mn id="S2.T1.78.78.78.78.3.m1.1.1" xref="S2.T1.78.78.78.78.3.m1.1.1.cmml">95.78</mn><annotation-xml encoding="MathML-Content" id="S2.T1.78.78.78.78.3.m1.1b"><cn type="float" id="S2.T1.78.78.78.78.3.m1.1.1.cmml" xref="S2.T1.78.78.78.78.3.m1.1.1">95.78</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.78.78.78.78.3.m1.1c">95.78</annotation></semantics></math></td>
<td id="S2.T1.79.79.79.79.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.79.79.79.79.4.m1.1" class="ltx_Math" alttext="91.97" display="inline"><semantics id="S2.T1.79.79.79.79.4.m1.1a"><mn id="S2.T1.79.79.79.79.4.m1.1.1" xref="S2.T1.79.79.79.79.4.m1.1.1.cmml">91.97</mn><annotation-xml encoding="MathML-Content" id="S2.T1.79.79.79.79.4.m1.1b"><cn type="float" id="S2.T1.79.79.79.79.4.m1.1.1.cmml" xref="S2.T1.79.79.79.79.4.m1.1.1">91.97</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.79.79.79.79.4.m1.1c">91.97</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.83.83.83.83" class="ltx_tr">
<td id="S2.T1.83.83.83.83.5" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Kortylewski <span id="S2.T1.83.83.83.83.5.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="S2.T1.83.83.83.83.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Face Recognition</td>
<td id="S2.T1.83.83.83.83.7" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Model Initialization</td>
<td id="S2.T1.80.80.80.80.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">accuracy (<math id="S2.T1.80.80.80.80.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.80.80.80.80.1.m1.1a"><mo stretchy="false" id="S2.T1.80.80.80.80.1.m1.1.1" xref="S2.T1.80.80.80.80.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.80.80.80.80.1.m1.1b"><ci id="S2.T1.80.80.80.80.1.m1.1.1.cmml" xref="S2.T1.80.80.80.80.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.80.80.80.80.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.81.81.81.81.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.81.81.81.81.2.m1.1" class="ltx_Math" alttext="91.2" display="inline"><semantics id="S2.T1.81.81.81.81.2.m1.1a"><mn id="S2.T1.81.81.81.81.2.m1.1.1" xref="S2.T1.81.81.81.81.2.m1.1.1.cmml">91.2</mn><annotation-xml encoding="MathML-Content" id="S2.T1.81.81.81.81.2.m1.1b"><cn type="float" id="S2.T1.81.81.81.81.2.m1.1.1.cmml" xref="S2.T1.81.81.81.81.2.m1.1.1">91.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.81.81.81.81.2.m1.1c">91.2</annotation></semantics></math></td>
<td id="S2.T1.82.82.82.82.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.82.82.82.82.3.m1.1" class="ltx_Math" alttext="93.3" display="inline"><semantics id="S2.T1.82.82.82.82.3.m1.1a"><mn id="S2.T1.82.82.82.82.3.m1.1.1" xref="S2.T1.82.82.82.82.3.m1.1.1.cmml">93.3</mn><annotation-xml encoding="MathML-Content" id="S2.T1.82.82.82.82.3.m1.1b"><cn type="float" id="S2.T1.82.82.82.82.3.m1.1.1.cmml" xref="S2.T1.82.82.82.82.3.m1.1.1">93.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.82.82.82.82.3.m1.1c">93.3</annotation></semantics></math></td>
<td id="S2.T1.83.83.83.83.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.83.83.83.83.4.m1.1" class="ltx_Math" alttext="88.9" display="inline"><semantics id="S2.T1.83.83.83.83.4.m1.1a"><mn id="S2.T1.83.83.83.83.4.m1.1.1" xref="S2.T1.83.83.83.83.4.m1.1.1.cmml">88.9</mn><annotation-xml encoding="MathML-Content" id="S2.T1.83.83.83.83.4.m1.1b"><cn type="float" id="S2.T1.83.83.83.83.4.m1.1.1.cmml" xref="S2.T1.83.83.83.83.4.m1.1.1">88.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.83.83.83.83.4.m1.1c">88.9</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.84.84.84.84" class="ltx_tr">
<td id="S2.T1.84.84.84.84.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Colbois <span id="S2.T1.84.84.84.84.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>
</td>
<td id="S2.T1.84.84.84.84.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Face Recognition</td>
<td id="S2.T1.84.84.84.84.4" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Evaluation</td>
<td id="S2.T1.84.84.84.84.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">FNMR U/E/P (<math id="S2.T1.84.84.84.84.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.84.84.84.84.1.m1.1a"><mo stretchy="false" id="S2.T1.84.84.84.84.1.m1.1.1" xref="S2.T1.84.84.84.84.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.84.84.84.84.1.m1.1b"><ci id="S2.T1.84.84.84.84.1.m1.1.1.cmml" xref="S2.T1.84.84.84.84.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.84.84.84.84.1.m1.1c">\downarrow</annotation></semantics></math>)</td>
<td id="S2.T1.84.84.84.84.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11/3/55</td>
<td id="S2.T1.84.84.84.84.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.84.84.84.84.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12/25/51</td>
</tr>
<tr id="S2.T1.87.87.87.87" class="ltx_tr">
<td id="S2.T1.87.87.87.87.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Marriott <span id="S2.T1.87.87.87.87.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>
</td>
<td id="S2.T1.87.87.87.87.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Pose-invariant Face Recognition</td>
<td id="S2.T1.87.87.87.87.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Augmented Training</td>
<td id="S2.T1.85.85.85.85.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">accuracy (<math id="S2.T1.85.85.85.85.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.85.85.85.85.1.m1.1a"><mo stretchy="false" id="S2.T1.85.85.85.85.1.m1.1.1" xref="S2.T1.85.85.85.85.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.85.85.85.85.1.m1.1b"><ci id="S2.T1.85.85.85.85.1.m1.1.1.cmml" xref="S2.T1.85.85.85.85.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.85.85.85.85.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.86.86.86.86.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.86.86.86.86.2.m1.1" class="ltx_Math" alttext="93.59" display="inline"><semantics id="S2.T1.86.86.86.86.2.m1.1a"><mn id="S2.T1.86.86.86.86.2.m1.1.1" xref="S2.T1.86.86.86.86.2.m1.1.1.cmml">93.59</mn><annotation-xml encoding="MathML-Content" id="S2.T1.86.86.86.86.2.m1.1b"><cn type="float" id="S2.T1.86.86.86.86.2.m1.1.1.cmml" xref="S2.T1.86.86.86.86.2.m1.1.1">93.59</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.86.86.86.86.2.m1.1c">93.59</annotation></semantics></math></td>
<td id="S2.T1.87.87.87.87.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.87.87.87.87.3.m1.1" class="ltx_Math" alttext="95.29" display="inline"><semantics id="S2.T1.87.87.87.87.3.m1.1a"><mn id="S2.T1.87.87.87.87.3.m1.1.1" xref="S2.T1.87.87.87.87.3.m1.1.1.cmml">95.29</mn><annotation-xml encoding="MathML-Content" id="S2.T1.87.87.87.87.3.m1.1b"><cn type="float" id="S2.T1.87.87.87.87.3.m1.1.1.cmml" xref="S2.T1.87.87.87.87.3.m1.1.1">95.29</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.87.87.87.87.3.m1.1c">95.29</annotation></semantics></math></td>
<td id="S2.T1.87.87.87.87.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.90.90.90.90" class="ltx_tr">
<td id="S2.T1.90.90.90.90.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Wood <span id="S2.T1.90.90.90.90.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S2.T1.90.90.90.90.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Face Segmentation</td>
<td id="S2.T1.90.90.90.90.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Synthetic Training</td>
<td id="S2.T1.89.89.89.89.2" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">
<math id="S2.T1.88.88.88.88.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S2.T1.88.88.88.88.1.m1.1a"><msub id="S2.T1.88.88.88.88.1.m1.1.1" xref="S2.T1.88.88.88.88.1.m1.1.1.cmml"><mi id="S2.T1.88.88.88.88.1.m1.1.1.2" xref="S2.T1.88.88.88.88.1.m1.1.1.2.cmml">F</mi><mn id="S2.T1.88.88.88.88.1.m1.1.1.3" xref="S2.T1.88.88.88.88.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.T1.88.88.88.88.1.m1.1b"><apply id="S2.T1.88.88.88.88.1.m1.1.1.cmml" xref="S2.T1.88.88.88.88.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.88.88.88.88.1.m1.1.1.1.cmml" xref="S2.T1.88.88.88.88.1.m1.1.1">subscript</csymbol><ci id="S2.T1.88.88.88.88.1.m1.1.1.2.cmml" xref="S2.T1.88.88.88.88.1.m1.1.1.2">𝐹</ci><cn type="integer" id="S2.T1.88.88.88.88.1.m1.1.1.3.cmml" xref="S2.T1.88.88.88.88.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.88.88.88.88.1.m1.1c">F_{1}</annotation></semantics></math> score (<math id="S2.T1.89.89.89.89.2.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.89.89.89.89.2.m2.1a"><mo stretchy="false" id="S2.T1.89.89.89.89.2.m2.1.1" xref="S2.T1.89.89.89.89.2.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.89.89.89.89.2.m2.1b"><ci id="S2.T1.89.89.89.89.2.m2.1.1.cmml" xref="S2.T1.89.89.89.89.2.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.89.89.89.89.2.m2.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.90.90.90.90.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.90.90.90.90.3.m1.1" class="ltx_Math" alttext="91.6" display="inline"><semantics id="S2.T1.90.90.90.90.3.m1.1a"><mn id="S2.T1.90.90.90.90.3.m1.1.1" xref="S2.T1.90.90.90.90.3.m1.1.1.cmml">91.6</mn><annotation-xml encoding="MathML-Content" id="S2.T1.90.90.90.90.3.m1.1b"><cn type="float" id="S2.T1.90.90.90.90.3.m1.1.1.cmml" xref="S2.T1.90.90.90.90.3.m1.1.1">91.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.90.90.90.90.3.m1.1c">91.6</annotation></semantics></math></td>
<td id="S2.T1.90.90.90.90.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.90.90.90.90.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">92</td>
</tr>
<tr id="S2.T1.93.93.93.93" class="ltx_tr">
<td id="S2.T1.93.93.93.93.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Ahmed <span id="S2.T1.93.93.93.93.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>
</td>
<td id="S2.T1.93.93.93.93.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Face Expression Classification</td>
<td id="S2.T1.93.93.93.93.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Augmented Training</td>
<td id="S2.T1.91.91.91.91.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Accuracy (<math id="S2.T1.91.91.91.91.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.91.91.91.91.1.m1.1a"><mo stretchy="false" id="S2.T1.91.91.91.91.1.m1.1.1" xref="S2.T1.91.91.91.91.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.91.91.91.91.1.m1.1b"><ci id="S2.T1.91.91.91.91.1.m1.1.1.cmml" xref="S2.T1.91.91.91.91.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.91.91.91.91.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.92.92.92.92.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.92.92.92.92.2.m1.1" class="ltx_Math" alttext="92.95" display="inline"><semantics id="S2.T1.92.92.92.92.2.m1.1a"><mn id="S2.T1.92.92.92.92.2.m1.1.1" xref="S2.T1.92.92.92.92.2.m1.1.1.cmml">92.95</mn><annotation-xml encoding="MathML-Content" id="S2.T1.92.92.92.92.2.m1.1b"><cn type="float" id="S2.T1.92.92.92.92.2.m1.1.1.cmml" xref="S2.T1.92.92.92.92.2.m1.1.1">92.95</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.92.92.92.92.2.m1.1c">92.95</annotation></semantics></math></td>
<td id="S2.T1.93.93.93.93.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.93.93.93.93.3.m1.1" class="ltx_Math" alttext="96.24" display="inline"><semantics id="S2.T1.93.93.93.93.3.m1.1a"><mn id="S2.T1.93.93.93.93.3.m1.1.1" xref="S2.T1.93.93.93.93.3.m1.1.1.cmml">96.24</mn><annotation-xml encoding="MathML-Content" id="S2.T1.93.93.93.93.3.m1.1b"><cn type="float" id="S2.T1.93.93.93.93.3.m1.1.1.cmml" xref="S2.T1.93.93.93.93.3.m1.1.1">96.24</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.93.93.93.93.3.m1.1c">96.24</annotation></semantics></math></td>
<td id="S2.T1.93.93.93.93.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.96.96.96.96" class="ltx_tr">
<td id="S2.T1.96.96.96.96.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Niinuma <span id="S2.T1.96.96.96.96.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>
</td>
<td id="S2.T1.96.96.96.96.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Face Expression Classification</td>
<td id="S2.T1.96.96.96.96.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Synthetic Training</td>
<td id="S2.T1.94.94.94.94.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Inter-rater reliability (<math id="S2.T1.94.94.94.94.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.94.94.94.94.1.m1.1a"><mo stretchy="false" id="S2.T1.94.94.94.94.1.m1.1.1" xref="S2.T1.94.94.94.94.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.94.94.94.94.1.m1.1b"><ci id="S2.T1.94.94.94.94.1.m1.1.1.cmml" xref="S2.T1.94.94.94.94.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.94.94.94.94.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.95.95.95.95.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.95.95.95.95.2.m1.1" class="ltx_Math" alttext="48.9" display="inline"><semantics id="S2.T1.95.95.95.95.2.m1.1a"><mn id="S2.T1.95.95.95.95.2.m1.1.1" xref="S2.T1.95.95.95.95.2.m1.1.1.cmml">48.9</mn><annotation-xml encoding="MathML-Content" id="S2.T1.95.95.95.95.2.m1.1b"><cn type="float" id="S2.T1.95.95.95.95.2.m1.1.1.cmml" xref="S2.T1.95.95.95.95.2.m1.1.1">48.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.95.95.95.95.2.m1.1c">48.9</annotation></semantics></math></td>
<td id="S2.T1.96.96.96.96.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.96.96.96.96.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.96.96.96.96.3.m1.1" class="ltx_Math" alttext="52.5" display="inline"><semantics id="S2.T1.96.96.96.96.3.m1.1a"><mn id="S2.T1.96.96.96.96.3.m1.1.1" xref="S2.T1.96.96.96.96.3.m1.1.1.cmml">52.5</mn><annotation-xml encoding="MathML-Content" id="S2.T1.96.96.96.96.3.m1.1b"><cn type="float" id="S2.T1.96.96.96.96.3.m1.1.1.cmml" xref="S2.T1.96.96.96.96.3.m1.1.1">52.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.96.96.96.96.3.m1.1c">52.5</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.100.100.100.100" class="ltx_tr">
<td id="S2.T1.100.100.100.100.5" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Abbasnejad <span id="S2.T1.100.100.100.100.5.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>
</td>
<td id="S2.T1.100.100.100.100.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Face Expression Classification</td>
<td id="S2.T1.100.100.100.100.7" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Augmented Training</td>
<td id="S2.T1.98.98.98.98.2" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">
<math id="S2.T1.97.97.97.97.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S2.T1.97.97.97.97.1.m1.1a"><msub id="S2.T1.97.97.97.97.1.m1.1.1" xref="S2.T1.97.97.97.97.1.m1.1.1.cmml"><mi id="S2.T1.97.97.97.97.1.m1.1.1.2" xref="S2.T1.97.97.97.97.1.m1.1.1.2.cmml">F</mi><mn id="S2.T1.97.97.97.97.1.m1.1.1.3" xref="S2.T1.97.97.97.97.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.T1.97.97.97.97.1.m1.1b"><apply id="S2.T1.97.97.97.97.1.m1.1.1.cmml" xref="S2.T1.97.97.97.97.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.97.97.97.97.1.m1.1.1.1.cmml" xref="S2.T1.97.97.97.97.1.m1.1.1">subscript</csymbol><ci id="S2.T1.97.97.97.97.1.m1.1.1.2.cmml" xref="S2.T1.97.97.97.97.1.m1.1.1.2">𝐹</ci><cn type="integer" id="S2.T1.97.97.97.97.1.m1.1.1.3.cmml" xref="S2.T1.97.97.97.97.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.97.97.97.97.1.m1.1c">F_{1}</annotation></semantics></math> score (<math id="S2.T1.98.98.98.98.2.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.98.98.98.98.2.m2.1a"><mo stretchy="false" id="S2.T1.98.98.98.98.2.m2.1.1" xref="S2.T1.98.98.98.98.2.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.98.98.98.98.2.m2.1b"><ci id="S2.T1.98.98.98.98.2.m2.1.1.cmml" xref="S2.T1.98.98.98.98.2.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.98.98.98.98.2.m2.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.99.99.99.99.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.99.99.99.99.3.m1.1" class="ltx_Math" alttext="60.52" display="inline"><semantics id="S2.T1.99.99.99.99.3.m1.1a"><mn id="S2.T1.99.99.99.99.3.m1.1.1" xref="S2.T1.99.99.99.99.3.m1.1.1.cmml">60.52</mn><annotation-xml encoding="MathML-Content" id="S2.T1.99.99.99.99.3.m1.1b"><cn type="float" id="S2.T1.99.99.99.99.3.m1.1.1.cmml" xref="S2.T1.99.99.99.99.3.m1.1.1">60.52</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.99.99.99.99.3.m1.1c">60.52</annotation></semantics></math></td>
<td id="S2.T1.100.100.100.100.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.100.100.100.100.4.m1.1" class="ltx_Math" alttext="86.59" display="inline"><semantics id="S2.T1.100.100.100.100.4.m1.1a"><mn id="S2.T1.100.100.100.100.4.m1.1.1" xref="S2.T1.100.100.100.100.4.m1.1.1.cmml">86.59</mn><annotation-xml encoding="MathML-Content" id="S2.T1.100.100.100.100.4.m1.1b"><cn type="float" id="S2.T1.100.100.100.100.4.m1.1.1.cmml" xref="S2.T1.100.100.100.100.4.m1.1.1">86.59</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.100.100.100.100.4.m1.1c">86.59</annotation></semantics></math></td>
<td id="S2.T1.100.100.100.100.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.107.107.107.107" class="ltx_tr">
<td id="S2.T1.107.107.107.107.8" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Varol <span id="S2.T1.107.107.107.107.8.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>
</td>
<td id="S2.T1.107.107.107.107.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Action Recognition</td>
<td id="S2.T1.107.107.107.107.10" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Augmented Training</td>
<td id="S2.T1.102.102.102.102.2" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Accuracy <math id="S2.T1.101.101.101.101.1.m1.1" class="ltx_Math" alttext="0^{\circ}/45^{\circ}/90^{\circ}" display="inline"><semantics id="S2.T1.101.101.101.101.1.m1.1a"><mrow id="S2.T1.101.101.101.101.1.m1.1.1" xref="S2.T1.101.101.101.101.1.m1.1.1.cmml"><msup id="S2.T1.101.101.101.101.1.m1.1.1.2" xref="S2.T1.101.101.101.101.1.m1.1.1.2.cmml"><mn id="S2.T1.101.101.101.101.1.m1.1.1.2.2" xref="S2.T1.101.101.101.101.1.m1.1.1.2.2.cmml">0</mn><mo id="S2.T1.101.101.101.101.1.m1.1.1.2.3" xref="S2.T1.101.101.101.101.1.m1.1.1.2.3.cmml">∘</mo></msup><mo id="S2.T1.101.101.101.101.1.m1.1.1.1" xref="S2.T1.101.101.101.101.1.m1.1.1.1.cmml">/</mo><msup id="S2.T1.101.101.101.101.1.m1.1.1.3" xref="S2.T1.101.101.101.101.1.m1.1.1.3.cmml"><mn id="S2.T1.101.101.101.101.1.m1.1.1.3.2" xref="S2.T1.101.101.101.101.1.m1.1.1.3.2.cmml">45</mn><mo id="S2.T1.101.101.101.101.1.m1.1.1.3.3" xref="S2.T1.101.101.101.101.1.m1.1.1.3.3.cmml">∘</mo></msup><mo id="S2.T1.101.101.101.101.1.m1.1.1.1a" xref="S2.T1.101.101.101.101.1.m1.1.1.1.cmml">/</mo><msup id="S2.T1.101.101.101.101.1.m1.1.1.4" xref="S2.T1.101.101.101.101.1.m1.1.1.4.cmml"><mn id="S2.T1.101.101.101.101.1.m1.1.1.4.2" xref="S2.T1.101.101.101.101.1.m1.1.1.4.2.cmml">90</mn><mo id="S2.T1.101.101.101.101.1.m1.1.1.4.3" xref="S2.T1.101.101.101.101.1.m1.1.1.4.3.cmml">∘</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.101.101.101.101.1.m1.1b"><apply id="S2.T1.101.101.101.101.1.m1.1.1.cmml" xref="S2.T1.101.101.101.101.1.m1.1.1"><divide id="S2.T1.101.101.101.101.1.m1.1.1.1.cmml" xref="S2.T1.101.101.101.101.1.m1.1.1.1"></divide><apply id="S2.T1.101.101.101.101.1.m1.1.1.2.cmml" xref="S2.T1.101.101.101.101.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.T1.101.101.101.101.1.m1.1.1.2.1.cmml" xref="S2.T1.101.101.101.101.1.m1.1.1.2">superscript</csymbol><cn type="integer" id="S2.T1.101.101.101.101.1.m1.1.1.2.2.cmml" xref="S2.T1.101.101.101.101.1.m1.1.1.2.2">0</cn><compose id="S2.T1.101.101.101.101.1.m1.1.1.2.3.cmml" xref="S2.T1.101.101.101.101.1.m1.1.1.2.3"></compose></apply><apply id="S2.T1.101.101.101.101.1.m1.1.1.3.cmml" xref="S2.T1.101.101.101.101.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.T1.101.101.101.101.1.m1.1.1.3.1.cmml" xref="S2.T1.101.101.101.101.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S2.T1.101.101.101.101.1.m1.1.1.3.2.cmml" xref="S2.T1.101.101.101.101.1.m1.1.1.3.2">45</cn><compose id="S2.T1.101.101.101.101.1.m1.1.1.3.3.cmml" xref="S2.T1.101.101.101.101.1.m1.1.1.3.3"></compose></apply><apply id="S2.T1.101.101.101.101.1.m1.1.1.4.cmml" xref="S2.T1.101.101.101.101.1.m1.1.1.4"><csymbol cd="ambiguous" id="S2.T1.101.101.101.101.1.m1.1.1.4.1.cmml" xref="S2.T1.101.101.101.101.1.m1.1.1.4">superscript</csymbol><cn type="integer" id="S2.T1.101.101.101.101.1.m1.1.1.4.2.cmml" xref="S2.T1.101.101.101.101.1.m1.1.1.4.2">90</cn><compose id="S2.T1.101.101.101.101.1.m1.1.1.4.3.cmml" xref="S2.T1.101.101.101.101.1.m1.1.1.4.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.101.101.101.101.1.m1.1c">0^{\circ}/45^{\circ}/90^{\circ}</annotation></semantics></math> (<math id="S2.T1.102.102.102.102.2.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.102.102.102.102.2.m2.1a"><mo stretchy="false" id="S2.T1.102.102.102.102.2.m2.1.1" xref="S2.T1.102.102.102.102.2.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.102.102.102.102.2.m2.1b"><ci id="S2.T1.102.102.102.102.2.m2.1.1.cmml" xref="S2.T1.102.102.102.102.2.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.102.102.102.102.2.m2.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.105.105.105.105.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S2.T1.103.103.103.103.3.m1.1" class="ltx_Math" alttext="88.8" display="inline"><semantics id="S2.T1.103.103.103.103.3.m1.1a"><mn id="S2.T1.103.103.103.103.3.m1.1.1" xref="S2.T1.103.103.103.103.3.m1.1.1.cmml">88.8</mn><annotation-xml encoding="MathML-Content" id="S2.T1.103.103.103.103.3.m1.1b"><cn type="float" id="S2.T1.103.103.103.103.3.m1.1.1.cmml" xref="S2.T1.103.103.103.103.3.m1.1.1">88.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.103.103.103.103.3.m1.1c">88.8</annotation></semantics></math>/<math id="S2.T1.104.104.104.104.4.m2.1" class="ltx_Math" alttext="78.2" display="inline"><semantics id="S2.T1.104.104.104.104.4.m2.1a"><mn id="S2.T1.104.104.104.104.4.m2.1.1" xref="S2.T1.104.104.104.104.4.m2.1.1.cmml">78.2</mn><annotation-xml encoding="MathML-Content" id="S2.T1.104.104.104.104.4.m2.1b"><cn type="float" id="S2.T1.104.104.104.104.4.m2.1.1.cmml" xref="S2.T1.104.104.104.104.4.m2.1.1">78.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.104.104.104.104.4.m2.1c">78.2</annotation></semantics></math>/<math id="S2.T1.105.105.105.105.5.m3.1" class="ltx_Math" alttext="57.3" display="inline"><semantics id="S2.T1.105.105.105.105.5.m3.1a"><mn id="S2.T1.105.105.105.105.5.m3.1.1" xref="S2.T1.105.105.105.105.5.m3.1.1.cmml">57.3</mn><annotation-xml encoding="MathML-Content" id="S2.T1.105.105.105.105.5.m3.1b"><cn type="float" id="S2.T1.105.105.105.105.5.m3.1.1.cmml" xref="S2.T1.105.105.105.105.5.m3.1.1">57.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.105.105.105.105.5.m3.1c">57.3</annotation></semantics></math>
</td>
<td id="S2.T1.107.107.107.107.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S2.T1.106.106.106.106.6.m1.1" class="ltx_Math" alttext="90.5" display="inline"><semantics id="S2.T1.106.106.106.106.6.m1.1a"><mn id="S2.T1.106.106.106.106.6.m1.1.1" xref="S2.T1.106.106.106.106.6.m1.1.1.cmml">90.5</mn><annotation-xml encoding="MathML-Content" id="S2.T1.106.106.106.106.6.m1.1b"><cn type="float" id="S2.T1.106.106.106.106.6.m1.1.1.cmml" xref="S2.T1.106.106.106.106.6.m1.1.1">90.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.106.106.106.106.6.m1.1c">90.5</annotation></semantics></math>/<math id="S2.T1.107.107.107.107.7.m2.1" class="ltx_Math" alttext="83.3" display="inline"><semantics id="S2.T1.107.107.107.107.7.m2.1a"><mn id="S2.T1.107.107.107.107.7.m2.1.1" xref="S2.T1.107.107.107.107.7.m2.1.1.cmml">83.3</mn><annotation-xml encoding="MathML-Content" id="S2.T1.107.107.107.107.7.m2.1b"><cn type="float" id="S2.T1.107.107.107.107.7.m2.1.1.cmml" xref="S2.T1.107.107.107.107.7.m2.1.1">83.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.107.107.107.107.7.m2.1c">83.3</annotation></semantics></math>/68</td>
<td id="S2.T1.107.107.107.107.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.110.110.110.110" class="ltx_tr">
<td id="S2.T1.110.110.110.110.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Hatay <span id="S2.T1.110.110.110.110.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>
</td>
<td id="S2.T1.110.110.110.110.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(Phone) Action Recognition</td>
<td id="S2.T1.110.110.110.110.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Model Initialization</td>
<td id="S2.T1.108.108.108.108.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Accuracy (<math id="S2.T1.108.108.108.108.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.108.108.108.108.1.m1.1a"><mo stretchy="false" id="S2.T1.108.108.108.108.1.m1.1.1" xref="S2.T1.108.108.108.108.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.108.108.108.108.1.m1.1b"><ci id="S2.T1.108.108.108.108.1.m1.1.1.cmml" xref="S2.T1.108.108.108.108.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.108.108.108.108.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.109.109.109.109.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.109.109.109.109.2.m1.1" class="ltx_Math" alttext="95.83" display="inline"><semantics id="S2.T1.109.109.109.109.2.m1.1a"><mn id="S2.T1.109.109.109.109.2.m1.1.1" xref="S2.T1.109.109.109.109.2.m1.1.1.cmml">95.83</mn><annotation-xml encoding="MathML-Content" id="S2.T1.109.109.109.109.2.m1.1b"><cn type="float" id="S2.T1.109.109.109.109.2.m1.1.1.cmml" xref="S2.T1.109.109.109.109.2.m1.1.1">95.83</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.109.109.109.109.2.m1.1c">95.83</annotation></semantics></math></td>
<td id="S2.T1.110.110.110.110.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.110.110.110.110.3.m1.1" class="ltx_Math" alttext="96.67" display="inline"><semantics id="S2.T1.110.110.110.110.3.m1.1a"><mn id="S2.T1.110.110.110.110.3.m1.1.1" xref="S2.T1.110.110.110.110.3.m1.1.1.cmml">96.67</mn><annotation-xml encoding="MathML-Content" id="S2.T1.110.110.110.110.3.m1.1b"><cn type="float" id="S2.T1.110.110.110.110.3.m1.1.1.cmml" xref="S2.T1.110.110.110.110.3.m1.1.1">96.67</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.110.110.110.110.3.m1.1c">96.67</annotation></semantics></math></td>
<td id="S2.T1.110.110.110.110.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.113.113.113.113" class="ltx_tr">
<td id="S2.T1.113.113.113.113.4" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Souza <span id="S2.T1.113.113.113.113.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>
</td>
<td id="S2.T1.113.113.113.113.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Action Recognition</td>
<td id="S2.T1.113.113.113.113.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Augmented Training</td>
<td id="S2.T1.111.111.111.111.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Accuracy (<math id="S2.T1.111.111.111.111.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.111.111.111.111.1.m1.1a"><mo stretchy="false" id="S2.T1.111.111.111.111.1.m1.1.1" xref="S2.T1.111.111.111.111.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.111.111.111.111.1.m1.1b"><ci id="S2.T1.111.111.111.111.1.m1.1.1.cmml" xref="S2.T1.111.111.111.111.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.111.111.111.111.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.112.112.112.112.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.112.112.112.112.2.m1.1" class="ltx_Math" alttext="93.3" display="inline"><semantics id="S2.T1.112.112.112.112.2.m1.1a"><mn id="S2.T1.112.112.112.112.2.m1.1.1" xref="S2.T1.112.112.112.112.2.m1.1.1.cmml">93.3</mn><annotation-xml encoding="MathML-Content" id="S2.T1.112.112.112.112.2.m1.1b"><cn type="float" id="S2.T1.112.112.112.112.2.m1.1.1.cmml" xref="S2.T1.112.112.112.112.2.m1.1.1">93.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.112.112.112.112.2.m1.1c">93.3</annotation></semantics></math></td>
<td id="S2.T1.113.113.113.113.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.113.113.113.113.3.m1.1" class="ltx_Math" alttext="92.7" display="inline"><semantics id="S2.T1.113.113.113.113.3.m1.1a"><mn id="S2.T1.113.113.113.113.3.m1.1.1" xref="S2.T1.113.113.113.113.3.m1.1.1.cmml">92.7</mn><annotation-xml encoding="MathML-Content" id="S2.T1.113.113.113.113.3.m1.1b"><cn type="float" id="S2.T1.113.113.113.113.3.m1.1.1.cmml" xref="S2.T1.113.113.113.113.3.m1.1.1">92.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.113.113.113.113.3.m1.1c">92.7</annotation></semantics></math></td>
<td id="S2.T1.113.113.113.113.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S2.T1.117.117.117.117" class="ltx_tr">
<td id="S2.T1.117.117.117.117.5" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Varol <span id="S2.T1.117.117.117.117.5.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</td>
<td id="S2.T1.117.117.117.117.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Human Body Segmentation</td>
<td id="S2.T1.117.117.117.117.7" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Model Initialization</td>
<td id="S2.T1.114.114.114.114.1" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Accuracy (<math id="S2.T1.114.114.114.114.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.114.114.114.114.1.m1.1a"><mo stretchy="false" id="S2.T1.114.114.114.114.1.m1.1.1" xref="S2.T1.114.114.114.114.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T1.114.114.114.114.1.m1.1b"><ci id="S2.T1.114.114.114.114.1.m1.1.1.cmml" xref="S2.T1.114.114.114.114.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.114.114.114.114.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
<td id="S2.T1.115.115.115.115.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.115.115.115.115.2.m1.1" class="ltx_Math" alttext="58.54" display="inline"><semantics id="S2.T1.115.115.115.115.2.m1.1a"><mn id="S2.T1.115.115.115.115.2.m1.1.1" xref="S2.T1.115.115.115.115.2.m1.1.1.cmml">58.54</mn><annotation-xml encoding="MathML-Content" id="S2.T1.115.115.115.115.2.m1.1b"><cn type="float" id="S2.T1.115.115.115.115.2.m1.1.1.cmml" xref="S2.T1.115.115.115.115.2.m1.1.1">58.54</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.115.115.115.115.2.m1.1c">58.54</annotation></semantics></math></td>
<td id="S2.T1.116.116.116.116.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.116.116.116.116.3.m1.1" class="ltx_Math" alttext="67.72" display="inline"><semantics id="S2.T1.116.116.116.116.3.m1.1a"><mn id="S2.T1.116.116.116.116.3.m1.1.1" xref="S2.T1.116.116.116.116.3.m1.1.1.cmml">67.72</mn><annotation-xml encoding="MathML-Content" id="S2.T1.116.116.116.116.3.m1.1b"><cn type="float" id="S2.T1.116.116.116.116.3.m1.1.1.cmml" xref="S2.T1.116.116.116.116.3.m1.1.1">67.72</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.116.116.116.116.3.m1.1c">67.72</annotation></semantics></math></td>
<td id="S2.T1.117.117.117.117.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S2.T1.117.117.117.117.4.m1.1" class="ltx_Math" alttext="56.51" display="inline"><semantics id="S2.T1.117.117.117.117.4.m1.1a"><mn id="S2.T1.117.117.117.117.4.m1.1.1" xref="S2.T1.117.117.117.117.4.m1.1.1.cmml">56.51</mn><annotation-xml encoding="MathML-Content" id="S2.T1.117.117.117.117.4.m1.1b"><cn type="float" id="S2.T1.117.117.117.117.4.m1.1.1.cmml" xref="S2.T1.117.117.117.117.4.m1.1.1">56.51</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.117.117.117.117.4.m1.1c">56.51</annotation></semantics></math></td>
</tr>
<tr id="S2.T1.120.120.120.120" class="ltx_tr">
<td id="S2.T1.120.120.120.120.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_rr ltx_border_t">Priesnitz <span id="S2.T1.120.120.120.120.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>
</td>
<td id="S2.T1.120.120.120.120.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Contactless Fingerprint Recognition</td>
<td id="S2.T1.120.120.120.120.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_rr ltx_border_t">Evaluation</td>
<td id="S2.T1.118.118.118.118.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_rr ltx_border_t">Avg. EER (<math id="S2.T1.118.118.118.118.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.118.118.118.118.1.m1.1a"><mo stretchy="false" id="S2.T1.118.118.118.118.1.m1.1.1" xref="S2.T1.118.118.118.118.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T1.118.118.118.118.1.m1.1b"><ci id="S2.T1.118.118.118.118.1.m1.1.1.cmml" xref="S2.T1.118.118.118.118.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.118.118.118.118.1.m1.1c">\downarrow</annotation></semantics></math>)</td>
<td id="S2.T1.119.119.119.119.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S2.T1.119.119.119.119.2.m1.1" class="ltx_Math" alttext="30.93" display="inline"><semantics id="S2.T1.119.119.119.119.2.m1.1a"><mn id="S2.T1.119.119.119.119.2.m1.1.1" xref="S2.T1.119.119.119.119.2.m1.1.1.cmml">30.93</mn><annotation-xml encoding="MathML-Content" id="S2.T1.119.119.119.119.2.m1.1b"><cn type="float" id="S2.T1.119.119.119.119.2.m1.1.1.cmml" xref="S2.T1.119.119.119.119.2.m1.1.1">30.93</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.119.119.119.119.2.m1.1c">30.93</annotation></semantics></math></td>
<td id="S2.T1.120.120.120.120.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">-</td>
<td id="S2.T1.120.120.120.120.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S2.T1.120.120.120.120.3.m1.1" class="ltx_Math" alttext="3.55" display="inline"><semantics id="S2.T1.120.120.120.120.3.m1.1a"><mn id="S2.T1.120.120.120.120.3.m1.1.1" xref="S2.T1.120.120.120.120.3.m1.1.1.cmml">3.55</mn><annotation-xml encoding="MathML-Content" id="S2.T1.120.120.120.120.3.m1.1b"><cn type="float" id="S2.T1.120.120.120.120.3.m1.1.1.cmml" xref="S2.T1.120.120.120.120.3.m1.1.1">3.55</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.120.120.120.120.3.m1.1c">3.55</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">How can synthetic datasets be generated?</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Initial approaches for synthetic data generation generally exploit <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">mathematical modelling</span>, <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">3D rendering tools</span> or <span id="S3.p1.1.3" class="ltx_text ltx_font_italic">perturbations using classical and hand-crafted</span> means. However, the success of deep neural networks in image generation has catapulted <span id="S3.p1.1.4" class="ltx_text ltx_font_italic">dynamic perturbations</span> and <span id="S3.p1.1.5" class="ltx_text ltx_font_italic">deep neural networks</span> as primary generation models. We proceed to provide details on such synthetic data generation methods.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2208.09191/assets/x2.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="157" height="83" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.4.2" class="ltx_text" style="font-size:90%;">Cappelli <span id="S3.F5.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> mathematically modelled the distribution of fingerprints and proposed SFinGe to generate synthetic fingerprints.</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><span id="S3.SS1.1.1" class="ltx_text ltx_font_italic">Mathematical modelling</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Mathematical modelling constitutes an early approach for generating human data aimed at approximating the distribution of real human data through mathematical modelling. Sampling from the approximated model can then be used to generate synthetic samples and exploit them in downstream human analysis tasks. Approximation of the mathematical model pertaining to the human data requires domain expertise and a careful understanding of model parameters. A popular mathematical modelling-based synthetic fingerprint generation (SFinGe) is proposed by Cappelli <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> (see Figure <a href="#S3.F5" title="Figure 5 ‣ 3 How can synthetic datasets be generated? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). The authors exploit domain expertise to define a fingerprint orientation model characterized by the number and location of the fingerprint cores and deltas. The synthetic fingerprint generation starts from initializing the locations of core and deltas, followed by ridge orientation and density generation. Subsequently, the authors apply space-invariant linear filtering to obtain a binarized good quality fingerprint image. Lastly, domain-specific noise is introduced to simulate realistic greyscale fingerprint images. Approaches exploiting mathematical modelling using domain knowledge for synthetic data generation include handwriting recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib75" title="" class="ltx_ref">75</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>, finger vein recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>, hand shape recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>, face recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>, keystroke recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> and iris recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span id="S3.SS2.1.1" class="ltx_text ltx_font_italic">3D rendering tools</span>
</h3>

<figure id="S3.F6" class="ltx_figure"><img src="/html/2208.09191/assets/x3.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="184" height="96" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S3.F6.4.2" class="ltx_text" style="font-size:90%;">Aranjuelo <span id="S3.F6.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> utilized 3ds Max software to virtually render humans (right) on a real scene (left) for detection of moving subjects.</span></figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Several studies exploit 3D modelling to create mathematical representations of the three-dimensional surface of the object of interest. Subsequently, a 3D rendering tool is exploited to render images corresponding to a 3D model. Han <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> argued that the generation of synthetic samples in 3D space allows for the incorporation of extreme changes in illumination, viewpoint, occlusion, scale, and background. Additionally, rendering engines allow precise control over environmental conditions such as pose variations, lighting, and object geometry leading to accurate annotations, which are often acquired for a real dataset. Most popular 3D rendering tools include Blender<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.blender.org/</span></span></span></span>, Maya<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.autodesk.fr/products/maya/overview</span></span></span></span>, 3ds Max<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.autodesk.com/products/3ds-max/overview</span></span></span></span>, Cinema 4D<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.maxon.net/en/cinema-4d</span></span></span></span>, Unity<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.unity3D.com</span></span></span></span>, and Unreal Engine<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.unrealengine.com</span></span></span></span>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Aranjuelo <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> virtually rendered humans on real scenes for application in detection of individuals (see Figure <a href="#S3.F6" title="Figure 6 ‣ 3.2 3D rendering tools ‣ 3 How can synthetic datasets be generated? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). Similarly, Öz <span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> used a 3D rendering tool to generate synthetic eye images and exploit the generated samples to learn eye region segmentation (see Figure <a href="#S3.F7" title="Figure 7 ‣ 3.2 3D rendering tools ‣ 3 How can synthetic datasets be generated? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). Other studies exploiting 3D rendering tools for generating synthetic data spanned applications in re-identification of individuals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, face recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> and gait recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2208.09191/assets/x4.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="193" height="25" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S3.F7.4.2" class="ltx_text" style="font-size:90%;">Öz <span id="S3.F7.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> generated synthetic eye images employing UnityEyes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>, a 3D rendering tool. The synthetic data is used for learning eye region segmentation.</span></figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span><span id="S3.SS3.1.1" class="ltx_text ltx_font_italic">Input perturbations</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Perturbation of a given input is widely used to generate synthetic data. Input perturbation imparts either the introduction of noise through classical and hand-crafted methods or a learning-based approach. We proceed to provide a brief discussion on both approach types.</p>
</div>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2208.09191/assets/x5.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="183" height="120" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S3.F8.4.2" class="ltx_text" style="font-size:90%;">Ibsen <span id="S3.F8.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> generated synthetic face images with tattoos to learn tattoo removal from faces. Image processing operations were used to blend tattoos on faces. Further, operations such as colour-adjustment and Gaussian blurring were applied to increase realism in the synthetically tattooed faces.</span></figcaption>
</figure>
<figure id="S3.F9" class="ltx_figure"><img src="/html/2208.09191/assets/x6.png" id="S3.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="160" height="86" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.3.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S3.F9.4.2" class="ltx_text" style="font-size:90%;">Cappelli <span id="S3.F9.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> exploited morphological operations such as erosion and dilation to vary ridge thickness while generating multiple impressions of a fingerprint image.</span></figcaption>
</figure>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Perturbations using classical and hand-crafted methods</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">Classical and hand-crafted methods can perturb a given input to either introduce variations in the available data or simulate cases that are difficult to capture otherwise. Most prominent classical and hand-crafted methods include Gaussian blurring, image blending, colour jittering, horizontal and vertical flipping, rotation, translation, as well as affine transformations. Some studies utilize morphological operations such as erosion and dilation to generate synthetic data samples. Following this direction of synthetic data generation, Ibsen <span id="S3.SS3.SSS1.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> exploited image processing techniques to synthetically blend tattoos on human faces (see Figure <a href="#S3.F8" title="Figure 8 ‣ 3.3 Input perturbations ‣ 3 How can synthetic datasets be generated? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>). Similarly, Cappelli <span id="S3.SS3.SSS1.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> generated synthetic multiple impressions from a given input fingerprint using morphological operations (see Figure <a href="#S3.F9" title="Figure 9 ‣ 3.3 Input perturbations ‣ 3 How can synthetic datasets be generated? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>). Other studies that generate synthetic data using classical methods have been instrumental in face recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, fingerprint recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>, iris recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> and re-identification of individuals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>.</p>
</div>
<figure id="S3.F10" class="ltx_figure"><img src="/html/2208.09191/assets/x7.png" id="S3.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="198" height="66" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F10.3.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S3.F10.4.2" class="ltx_text" style="font-size:90%;">Jain <span id="S3.F10.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> dynamically perturb a face image using six different adversarial training mechanisms (top row). The corresponding perturbations are provided in the bottom row. The authors demonstrate that synthetic faces generated using dynamic perturbations can increase face comparison score (obtained using ArcFace) in non-mated comparison trials.</span></figcaption>
</figure>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Dynamic perturbations</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">A dynamic perturbation is defined as an input-specific perturbation introduced through an adversarial training mechanism such that a learning-based human analysis model is likely to make an erroneous prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>. Training a human analysis model with the synthetic data generated using dynamic perturbations is beneficial for regularization and improvement of robustness. Following this approach, several studies generate synthetic data using adversarial training. Jain <span id="S3.SS3.SSS2.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> generated synthetic non-mated facial images using dynamic perturbations that obtain high comparison scores (see Figure <a href="#S3.F10" title="Figure 10 ‣ 3.3.1 Perturbations using classical and hand-crafted methods ‣ 3.3 Input perturbations ‣ 3 How can synthetic datasets be generated? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>). Other studies in human analysis exploiting dynamic perturbations include applications in speaker identification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite>, re-identification of individuals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>, face recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>, iris recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite> and fingerprint recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span><span id="S3.SS4.1.1" class="ltx_text ltx_font_italic">Deep neural networks</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Deep neural networks (DNNs) represent state-of-the-art architectures for generating synthetic data for among others, applications in human analysis. By revisiting related literature, we identify following categories for doing so.</p>
</div>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Recurrent neural networks</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">A recurrent neural network (RNN) is a DNN designed to process time-series, as well as sequential or variable-length input data. Such models are designed for applications, where input data samples depend on the previous data samples, as RNNs are aimed at capturing dependencies between data samples. Towards capturing long-range dependencies, state-of-the-art RNNs exploit long short-term memory (LSTM) and gated recurrent units (GRU) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite> to store information from previous inputs or states and generate the subsequent output of the input sequence. An LSTM comprises three gates: input, output and forget gate, while a GRU incorporates a reset and an update gate. These gates determine the most informative part of the input to make a prediction in the future.</p>
</div>
<div id="S3.SS4.SSS1.p2" class="ltx_para">
<p id="S3.SS4.SSS1.p2.1" class="ltx_p">One of the applications exploiting RNN to generate synthetic data is the contribution of Bird <span id="S3.SS4.SSS1.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, where a character-level RNN is exploited to generate audio sentences for speaker identification. In addition, RNNs are employed for generating deep fakes, where these architecture render continuous realistic flow in audio or video <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite>.</p>
</div>
<figure id="S3.F11" class="ltx_figure"><img src="/html/2208.09191/assets/x8.png" id="S3.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="207" height="121" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F11.3.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S3.F11.4.2" class="ltx_text" style="font-size:90%;">Mondal <span id="S3.F11.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite> proposed a variational auto-encoder model for generating synthetic faces, controlling attributes such as big nose, makeup, black hair, smile and gender.</span></figcaption>
</figure>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Auto-Encoders</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p">Auto-Encoder (AE) based generative models constitute a pair of encoder and decoder networks. While the encoder network learns an efficient representation of the input, the decoder network generates an output corresponding to the given latent vector provided as output by the encoder network. These models generate synthetic data by learning the joint distribution of the latent space and the training data. Such models are generally regularized by imposing a prior distribution on the latent space to facilitate generation during inference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>. Prominent auto-encoder architectures for synthetic data generation include variational auto-encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>, adversarial autoencoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite> and Wasserstein auto-encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite>, which includes a Gaussian prior. However, the Gaussian prior is simplistic and might fail to capture complex latent distributions. To alleviate this limitation, rich classes of distributional priors have been explored <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>, <a href="#bib.bib107" title="" class="ltx_ref">107</a>, <a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>. Several research efforts have attempted to learn disentangled representations in the latent space of the VAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>, <a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite> (see Figure <a href="#S3.F11" title="Figure 11 ‣ 3.4.1 Recurrent neural networks ‣ 3.4 Deep neural networks ‣ 3 How can synthetic datasets be generated? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>). Such a factored representation is beneficial in interpolating the latent space leading to the generation of diverse samples and plausible modification in input data. Despite offering interpretable inference, stable training, and an efficient sampling procedure, the generation quality of VAEs is not as impressive as the one achieved by GANs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>, <a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite>. Next, we discuss the most widely used state-of-the-art deep generative framework, namely GANs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite>.</p>
</div>
<figure id="S3.F12" class="ltx_figure"><img src="/html/2208.09191/assets/x9.png" id="S3.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="221" height="92" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F12.2.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="S3.F12.3.2" class="ltx_text" style="font-size:90%;">Cao and Jain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> generated synthetic fingerprints using improved Wasserstein GAN that was trained to upscale a random noise vector into a fingerprint image.</span></figcaption>
</figure>
<figure id="S3.F13" class="ltx_figure"><img src="/html/2208.09191/assets/x10.png" id="S3.F13.g1" class="ltx_graphics ltx_centering ltx_img_square" width="221" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F13.3.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="S3.F13.4.2" class="ltx_text" style="font-size:90%;">Choi <span id="S3.F13.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite> proposed StarGAN, a generative adversarial network that was able to alter attributes of a given face image. The generated synthetic faces have been commonly used as deepfakes.</span></figcaption>
</figure>
</section>
<section id="S3.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3 </span>Generative adversarial networks (GANs)</h4>

<div id="S3.SS4.SSS3.p1" class="ltx_para">
<p id="S3.SS4.SSS3.p1.1" class="ltx_p">Goodfellow <span id="S3.SS4.SSS3.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> in their seminal work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite> proposed a framework incorporating two networks, a generator and a discriminator. The generator is learns distribution of training samples, whereas the discriminator network is aimed at classifying whether the input samples stem from the training set or are generated by the generator (real or fake). Both networks are trained in an adversarial manner (zero-sum game), and the framework targets to facilitate improved approximation of true distribution by the generative model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>]</cite>. Hence, the name <span id="S3.SS4.SSS3.p1.1.2" class="ltx_text ltx_font_italic">generative adversarial network</span>. GANs are broadly categorized as <span id="S3.SS4.SSS3.p1.1.3" class="ltx_text ltx_font_italic">noise to image translation GANs</span> or <span id="S3.SS4.SSS3.p1.1.4" class="ltx_text ltx_font_italic">image to image translation GANs</span>. Noise to image translation GANs are trained to upscale a randomly sampled noise vector to a realistic image, whereas the image to image translation GANs are trained to transform a given image to another image.</p>
</div>
<div id="S3.SS4.SSS3.p2" class="ltx_para">
<p id="S3.SS4.SSS3.p2.1" class="ltx_p">Prominent noise to image translation GANs include DCGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite> and Wasserstein GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite>, whereas frequently empolyed image to image translation GANs include pix2pix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite> and Cycle-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite>. Several studies in human exploited GANs to generate synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite>. One such study includes the contribution of Cao and Jain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>. The authors generated synthetic fingerprints using noise to image translation GAN (see Figure <a href="#S3.F12" title="Figure 12 ‣ 3.4.2 Auto-Encoders ‣ 3.4 Deep neural networks ‣ 3 How can synthetic datasets be generated? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>). Similarly, Choi <span id="S3.SS4.SSS3.p2.1.1" class="ltx_text ltx_font_italic">etal.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite> proposed an image to image translation GAN to modify attributes in facial images (see Figure <a href="#S3.F13" title="Figure 13 ‣ 3.4.2 Auto-Encoders ‣ 3.4 Deep neural networks ‣ 3 How can synthetic datasets be generated? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>).</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">How can synthetic data be utilized?</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Synthetic data is frequently used to <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">simulate complex scenarios</span> for which the data collection is particularly challenging, <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">overcome privacy issues</span> observed for collection of real human analysis datasets, <span id="S4.p1.1.3" class="ltx_text ltx_font_italic">increase the size and diversity of training datasets</span>, as well as <span id="S4.p1.1.4" class="ltx_text ltx_font_italic">mitigate bias</span> in real training datasets. Furthermore, looking at the challenge in collecting large-scale datasets, synthetic data is widely used in <span id="S4.p1.1.5" class="ltx_text ltx_font_italic">scalability analysis</span> of systems. Additionally, as obtaining annotations can be both time-consuming and expensive, <span id="S4.p1.1.6" class="ltx_text ltx_font_italic">synthetic data, whose annotations can be automatically derived</span> is popularly used. With <span id="S4.p1.1.7" class="ltx_text ltx_font_italic">consistency regularization</span> techniques, synthetic data is used to learn generalizable models. Synthetic data can also be employed to produce <span id="S4.p1.1.8" class="ltx_text ltx_font_italic">presentation attacks</span> on human authentication systems. We proceed to provide details on different usage of synthetic data.</p>
</div>
<figure id="S4.F14" class="ltx_figure"><img src="/html/2208.09191/assets/x11.png" id="S4.F14.g1" class="ltx_graphics ltx_centering ltx_img_square" width="166" height="176" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F14.3.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>: </span><span id="S4.F14.4.2" class="ltx_text" style="font-size:90%;">Dou <span id="S4.F14.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> discussed the limitations of existing real databases for video-based gait recognition in capturing complex scenarios. For instance, the authors discussed that (a) real datasets are only acquired with a single camera pitch angle and (b) on the other hand, the synthetic dataset is generated with a diverse range of camera pitch angles. Thus, synthetic data can be used to simulate complex scenarios, which are otherwise difficult to acquire for a real dataset for human analysis.</span></figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span><span id="S4.SS1.1.1" class="ltx_text ltx_font_italic">Simulating complex scenarios</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Dou <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> argued that existing real databases for video-based gait recognition do not possess examples of complicated scenarios that can be crucial to obtaining satisfactory performance in real-world applications. For instance, real datasets are captured under ideal settings with only a single camera pitch angle (see Figure <a href="#S4.F14" title="Figure 14 ‣ 4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>). Specifically in the OU-MVLP dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite> for gait recognition subjects only walk twice without the change of bag or clothing, with only one subject appearing per video frame. However, real-world scenarios naturally include multiple walking individuals. Towards bridging this gap, the authors generated approximately one million synthetic silhouette sequences of 11,000 subjects. The resulting synthetic dataset VersatileGait comprises of gait sequences with a diverse range of camera pitch angles and fine-grained annotations of attributes. Furthermore, to promote the design of multi-person gait recognition algorithms, the authors also generated multi-person walking scenarios with up to three people walking simultaneously.
</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Similarly, Aranjuelo <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> argued that existing real datasets for human detection do not exploit omnidirectional cameras to capture a 360° view in surveillance videos. To take advantage of the 360° view, the authors proposed the subject detection model to be trained with synthetic data. Other applications, exploiting synthetic data to simulate complex scenarios include the contributions of Lai <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite> for generating synthetic skilled forgery attacks, Tabassi <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite> for simulating altered fingerprints and the contributions of Arifoglu and Bouchachia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite> for the simulation of (abnormal) behaviour observed for dementia patients.</p>
</div>
<figure id="S4.F15" class="ltx_figure"><img src="/html/2208.09191/assets/x12.png" id="S4.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="142" height="88" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F15.3.1.1" class="ltx_text" style="font-size:90%;">Figure 15</span>: </span><span id="S4.F15.4.2" class="ltx_text" style="font-size:90%;">A comparison of (a) real face image (from FRGC-V2 face database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>) and its associated quality score compared to synthetic face images generated by Zhang <span id="S4.F15.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite> using (b) StyleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and (c) StyleGAN2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. The synthetic faces have similar quality scores to those obtained for a real face. The authors proposed exploiting synthetic images to assess the performance of face recognition systems. This is instrumental in addressing and responding to privacy constraints in sharing datasets.</span></figcaption>
</figure>
<figure id="S4.F16" class="ltx_figure"><img src="/html/2208.09191/assets/x13.png" id="S4.F16.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="106" height="79" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F16.3.1.1" class="ltx_text" style="font-size:90%;">Figure 16</span>: </span><span id="S4.F16.4.2" class="ltx_text" style="font-size:90%;">Zhang <span id="S4.F16.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite> compared the distribution of non-mated comparison scores of real (FRGC-V2 face database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>) versus synthetic faces generated. Only minor differences in non-mated comparison scores corresponding to synthetic data were observed, as seen in real data. These results illustrated the potential of synthetic data being utilized instead of real data, alleviating privacy issues.</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span><span id="S4.SS2.1.1" class="ltx_text ltx_font_italic">Addressing privacy concerns</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Data collection is often governed by strict rules to preserve the identity of individuals. For settings, in which data collection is challenging, generated synthetic data and perform experiments on such synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>. However, a challenge with these applications has been to ensure that synthetic data has a similar distribution (for instance, distribution of minutiae in fingerprints <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>, or distribution of sample quality scores <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>, <a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite>) as the real data. Many studies demonstrated that synthetic data with similar characteristics to the real data can be generated and used, rather than the privacy-constrained real data. One such study includes generation of <math id="S4.SS2.p1.1.m1.2" class="ltx_Math" alttext="50,000" display="inline"><semantics id="S4.SS2.p1.1.m1.2a"><mrow id="S4.SS2.p1.1.m1.2.3.2" xref="S4.SS2.p1.1.m1.2.3.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">50</mn><mo id="S4.SS2.p1.1.m1.2.3.2.1" xref="S4.SS2.p1.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS2.p1.1.m1.2.2" xref="S4.SS2.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.2b"><list id="S4.SS2.p1.1.m1.2.3.1.cmml" xref="S4.SS2.p1.1.m1.2.3.2"><cn type="integer" id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">50</cn><cn type="integer" id="S4.SS2.p1.1.m1.2.2.cmml" xref="S4.SS2.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.2c">50,000</annotation></semantics></math> synthetic face images each using StyleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and StyleGAN2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> for face recognition applications in face recognition systems at the Schengen border <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite>. The authors demonstrated that realistic face images with image quality scores similar to real faces can be generated (see Figure <a href="#S4.F15" title="Figure 15 ‣ 4.1 Simulating complex scenarios ‣ 4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>). In addition, the authors compared face recognition performance of models trained on synthetic and real data and reported only minor differences, see Figure <a href="#S4.F16" title="Figure 16 ‣ 4.1 Simulating complex scenarios ‣ 4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>. Similar to Zhang <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite>, Bozkir <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>]</cite> and Hillerström <span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite> proposed to generate synthetic data for applications implying gaze estimation and finger vein recognition, respectively, in order to circumvent privacy issues, occurring when publicly sharing human data.</p>
</div>
<figure id="S4.F17" class="ltx_figure"><img src="/html/2208.09191/assets/x14.png" id="S4.F17.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="190" height="96" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F17.4.1.1" class="ltx_text" style="font-size:90%;">Figure 17</span>: </span><span id="S4.F17.5.2" class="ltx_text" style="font-size:90%;">Feng <span id="S4.F17.5.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> generated synthetic faces with 11 yaw rotations (ranging from −50° to 50° over a step size of 5°) and 5 pitch rotations (ranging from −30° to 30° over a step size of 5°). Therefore, augmenting the synthetic dataset with the real training set increased the size of the training set. Furthermore, synthetic data provided face images with diverse pose variations. As a result, facial landmark detection performance improved <span id="S4.F17.5.2.2" class="ltx_text ltx_font_italic">w.r.t.</span> variations in facial poses.</span></figcaption>
</figure>
<figure id="S4.F18" class="ltx_figure"><img src="/html/2208.09191/assets/x15.png" id="S4.F18.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="135" height="106" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F18.7.3.1" class="ltx_text" style="font-size:90%;">Figure 18</span>: </span><span id="S4.F18.4.2" class="ltx_text" style="font-size:90%;">Feng <span id="S4.F18.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> selected <math id="S4.F18.3.1.m1.2" class="ltx_Math" alttext="44,820" display="inline"><semantics id="S4.F18.3.1.m1.2b"><mrow id="S4.F18.3.1.m1.2.3.2" xref="S4.F18.3.1.m1.2.3.1.cmml"><mn id="S4.F18.3.1.m1.1.1" xref="S4.F18.3.1.m1.1.1.cmml">44</mn><mo id="S4.F18.3.1.m1.2.3.2.1" xref="S4.F18.3.1.m1.2.3.1.cmml">,</mo><mn id="S4.F18.3.1.m1.2.2" xref="S4.F18.3.1.m1.2.2.cmml">820</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F18.3.1.m1.2c"><list id="S4.F18.3.1.m1.2.3.1.cmml" xref="S4.F18.3.1.m1.2.3.2"><cn type="integer" id="S4.F18.3.1.m1.1.1.cmml" xref="S4.F18.3.1.m1.1.1">44</cn><cn type="integer" id="S4.F18.3.1.m1.2.2.cmml" xref="S4.F18.3.1.m1.2.2">820</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.F18.3.1.m1.2d">44,820</annotation></semantics></math> images from the Multi-PIE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite> as the training set and augmented it with <math id="S4.F18.4.2.m2.2" class="ltx_Math" alttext="8,965" display="inline"><semantics id="S4.F18.4.2.m2.2b"><mrow id="S4.F18.4.2.m2.2.3.2" xref="S4.F18.4.2.m2.2.3.1.cmml"><mn id="S4.F18.4.2.m2.1.1" xref="S4.F18.4.2.m2.1.1.cmml">8</mn><mo id="S4.F18.4.2.m2.2.3.2.1" xref="S4.F18.4.2.m2.2.3.1.cmml">,</mo><mn id="S4.F18.4.2.m2.2.2" xref="S4.F18.4.2.m2.2.2.cmml">965</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F18.4.2.m2.2c"><list id="S4.F18.4.2.m2.2.3.1.cmml" xref="S4.F18.4.2.m2.2.3.2"><cn type="integer" id="S4.F18.4.2.m2.1.1.cmml" xref="S4.F18.4.2.m2.1.1">8</cn><cn type="integer" id="S4.F18.4.2.m2.2.2.cmml" xref="S4.F18.4.2.m2.2.2">965</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.F18.4.2.m2.2d">8,965</annotation></semantics></math> synthetic 2D face images. The authors demonstrated that the face detection error of the cascaded regression (CR) based method significantly decreased, when trained with augmented data (plot in red) compared to when the landmark detection model was trained on only real faces (plot in blue). Motivated by this observation, the augmented data was used to train the proposed method based on cascaded collaborative regression (CCR, plot in black) to achieve the best face detection performance.</span></figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span><span id="S4.SS3.1.1" class="ltx_text ltx_font_italic">Increasing the size and diversity of training dataset</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Training deep neural networks requires a tremendous amount of data. At the same time, datasets in human analysis applications have often very limited samples. However, training with smaller datasets may lead to poor generalization of the real-world test examples. Therefore, several studies in human analysis advocate augmentation through synthetic data. Augmentation with synthetic data improves diversity by introducing more variations in training data, as well as increases the size of the training set. Training with a more extensive and diverse set leads to improved training and generalizability of the trained model on the test data.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.3" class="ltx_p">Feng <span id="S4.SS3.p2.3.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> discussed the limited availability of annotated datasets to train a facial landmark detection model. The authors generated <math id="S4.SS3.p2.1.m1.2" class="ltx_Math" alttext="8,965" display="inline"><semantics id="S4.SS3.p2.1.m1.2a"><mrow id="S4.SS3.p2.1.m1.2.3.2" xref="S4.SS3.p2.1.m1.2.3.1.cmml"><mn id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">8</mn><mo id="S4.SS3.p2.1.m1.2.3.2.1" xref="S4.SS3.p2.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS3.p2.1.m1.2.2" xref="S4.SS3.p2.1.m1.2.2.cmml">965</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.2b"><list id="S4.SS3.p2.1.m1.2.3.1.cmml" xref="S4.SS3.p2.1.m1.2.3.2"><cn type="integer" id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">8</cn><cn type="integer" id="S4.SS3.p2.1.m1.2.2.cmml" xref="S4.SS3.p2.1.m1.2.2">965</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.2c">8,965</annotation></semantics></math> synthetic 2D face images to address this limitation with 11 different yaw rotations and five pitch rotations (see Figure <a href="#S4.F17" title="Figure 17 ‣ 4.2 Addressing privacy concerns ‣ 4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>). The authors augmented the training set for landmark detection and found that the face detection error reduces significantly after training on the augmented dataset (see Figure <a href="#S4.F18" title="Figure 18 ‣ 4.2 Addressing privacy concerns ‣ 4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a>). Similarly, Masi <span id="S4.SS3.p2.3.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib130" title="" class="ltx_ref">130</a>]</cite> augmented the training set of face images using augmentations that introduce variations in pose and shape. The authors demonstrated that rank-1 face recognition accuracy on the IJB-A dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite> improved from <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="94.6\%" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mrow id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mn id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">94.6</mn><mo id="S4.SS3.p2.2.m2.1.1.1" xref="S4.SS3.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">94.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">94.6\%</annotation></semantics></math> to <math id="S4.SS3.p2.3.m3.1" class="ltx_Math" alttext="96.2\%" display="inline"><semantics id="S4.SS3.p2.3.m3.1a"><mrow id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml"><mn id="S4.SS3.p2.3.m3.1.1.2" xref="S4.SS3.p2.3.m3.1.1.2.cmml">96.2</mn><mo id="S4.SS3.p2.3.m3.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><apply id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS3.p2.3.m3.1.1.2.cmml" xref="S4.SS3.p2.3.m3.1.1.2">96.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">96.2\%</annotation></semantics></math> after augmentation with synthetic samples. Several other studies additionally advocated augmenting the training set with synthetic data. Some of these studies include applications in human posture recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, brain-based authentication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>, signature verification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>, face photo-sketch recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>, <a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>, face recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>, cross spectral face recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>]</cite> and facial expression analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<figure id="S4.F19" class="ltx_figure"><img src="/html/2208.09191/assets/x16.png" id="S4.F19.g1" class="ltx_graphics ltx_centering ltx_img_square" width="142" height="118" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F19.2.1.1" class="ltx_text" style="font-size:90%;">Figure 19</span>: </span><span id="S4.F19.3.2" class="ltx_text" style="font-size:90%;">Cao and Jain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> generated synthetic fingerprints and augmented the gallery of standard real fingerprint databases to assess the scalability of state-of-the-art fingerprint search algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite>. The authors found that the rank-20 identification accuracies dropped as the gallery was augmented with synthetic fingerprints. These results signified the usefulness of synthetic data, in order to assess the scalability of systems.</span></figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span><span id="S4.SS4.1.1" class="ltx_text ltx_font_italic">Assessing scalability of systems</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Evaluation of scalability of large-scale systems such as the Aadhar database maintained by the unique identification authority of India requires assessment of a system’s performance for a colossal number of enrollees, sometimes up to a billion (Aadhar has 1.32 billion enrollments till 31 October 2021<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://uidai.gov.in/</span></span></span></span>). Scalability analysis of automated systems is crucial to assess whether these can be deployed for large-scale real-world applications. However, the collection of such large-scale datasets pertaining to humans is often challenging. To address this problem, researchers proposed to generate large-scale synthetic data instead of relying on real large-scale datasets. Such synthetic data is instrumental in performing scalability analyses of human analysis systems.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">We note that the scalability can either be evaluated with system-relevant metrics (<span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_italic">e.g.</span>, throughput rate) or metrics that reflect the employed algorithms’ performance or pre-trained models. According to the work of Sumi <span id="S4.SS4.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib136" title="" class="ltx_ref">136</a>]</cite>, synthetic evaluation datasets have to comply with following three criteria.</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Privacy.</span> There shall not be a link between a synthetic sample to one of the individuals contained in the training dataset.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Precision.</span> The performance of a pre-trained model evaluated with synthetic data shall be equal to the performance reported based on real data.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Universality.</span> The precision shall be consistent across the evaluation of different pre-trained models.</p>
</div>
</li>
</ol>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.4" class="ltx_p">Wilson <span id="S4.SS4.p3.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite> demonstrated that the identification performance of a fingerprint recognition system drops linearly with the increase in enrolment records in the gallery. This observation motivated Cao and Jain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> to generate 10 million synthetic rolled fingerprints using I-WGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite>, in order to evaluate the scalability of fingerprint search algorithms. Similar to the trend observed for real data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite>, the authors found that the rank-20 accuracy on NIST SD4 <span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.nist.gov/srd/nist-special-database-4</span></span></span></span> accuracy drops from <math id="S4.SS4.p3.1.m1.1" class="ltx_Math" alttext="98.7\%" display="inline"><semantics id="S4.SS4.p3.1.m1.1a"><mrow id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml"><mn id="S4.SS4.p3.1.m1.1.1.2" xref="S4.SS4.p3.1.m1.1.1.2.cmml">98.7</mn><mo id="S4.SS4.p3.1.m1.1.1.1" xref="S4.SS4.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><apply id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS4.p3.1.m1.1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p3.1.m1.1.1.2.cmml" xref="S4.SS4.p3.1.m1.1.1.2">98.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">98.7\%</annotation></semantics></math> to <math id="S4.SS4.p3.2.m2.1" class="ltx_Math" alttext="96.1\%" display="inline"><semantics id="S4.SS4.p3.2.m2.1a"><mrow id="S4.SS4.p3.2.m2.1.1" xref="S4.SS4.p3.2.m2.1.1.cmml"><mn id="S4.SS4.p3.2.m2.1.1.2" xref="S4.SS4.p3.2.m2.1.1.2.cmml">96.1</mn><mo id="S4.SS4.p3.2.m2.1.1.1" xref="S4.SS4.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><apply id="S4.SS4.p3.2.m2.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1"><csymbol cd="latexml" id="S4.SS4.p3.2.m2.1.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p3.2.m2.1.1.2.cmml" xref="S4.SS4.p3.2.m2.1.1.2">96.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">96.1\%</annotation></semantics></math> after the gallery is augmented with 250K synthetic fingerprints generated by the authors. Related to that, the report NIST SD14 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib139" title="" class="ltx_ref">139</a>]</cite> indicated that the rank-20 accuracy drops from <math id="S4.SS4.p3.3.m3.1" class="ltx_Math" alttext="98.7\%" display="inline"><semantics id="S4.SS4.p3.3.m3.1a"><mrow id="S4.SS4.p3.3.m3.1.1" xref="S4.SS4.p3.3.m3.1.1.cmml"><mn id="S4.SS4.p3.3.m3.1.1.2" xref="S4.SS4.p3.3.m3.1.1.2.cmml">98.7</mn><mo id="S4.SS4.p3.3.m3.1.1.1" xref="S4.SS4.p3.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.3.m3.1b"><apply id="S4.SS4.p3.3.m3.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1"><csymbol cd="latexml" id="S4.SS4.p3.3.m3.1.1.1.cmml" xref="S4.SS4.p3.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p3.3.m3.1.1.2.cmml" xref="S4.SS4.p3.3.m3.1.1.2">98.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.3.m3.1c">98.7\%</annotation></semantics></math> to <math id="S4.SS4.p3.4.m4.1" class="ltx_Math" alttext="95.0\%" display="inline"><semantics id="S4.SS4.p3.4.m4.1a"><mrow id="S4.SS4.p3.4.m4.1.1" xref="S4.SS4.p3.4.m4.1.1.cmml"><mn id="S4.SS4.p3.4.m4.1.1.2" xref="S4.SS4.p3.4.m4.1.1.2.cmml">95.0</mn><mo id="S4.SS4.p3.4.m4.1.1.1" xref="S4.SS4.p3.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.4.m4.1b"><apply id="S4.SS4.p3.4.m4.1.1.cmml" xref="S4.SS4.p3.4.m4.1.1"><csymbol cd="latexml" id="S4.SS4.p3.4.m4.1.1.1.cmml" xref="S4.SS4.p3.4.m4.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p3.4.m4.1.1.2.cmml" xref="S4.SS4.p3.4.m4.1.1.2">95.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.4.m4.1c">95.0\%</annotation></semantics></math> (see Figure <a href="#S4.F19" title="Figure 19 ‣ 4.3 Increasing the size and diversity of training dataset ‣ 4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a>).</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">Recently, Colbois <span id="S4.SS4.p4.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> analysed the verification accuracy and privacy of synthetic face images generated with StyleGAN2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and InterFaceGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. The authors introduced a synthetic version of the Multi-PIE dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite> (Synth-Multi-PIE), representing the same factors of variation. The precision was assessed following the evaluation protocol of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite>, identifying only minor performance differences between Synth-Multi-PIE and Multi-PIE. Similar studies on scalability analysis using synthetic data have been conducted for signature verification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>, hand-shape biometrics recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib140" title="" class="ltx_ref">140</a>]</cite>, face recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>, iris verification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and keystroke dynamics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>.</p>
</div>
<figure id="S4.F20" class="ltx_figure"><img src="/html/2208.09191/assets/images/synthetic_annotations.png" id="S4.F20.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="192" height="135" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F20.5.2.1" class="ltx_text" style="font-size:90%;">Figure 20</span>: </span><span id="S4.F20.2.1" class="ltx_text" style="font-size:90%;">Joshi <span id="S4.F20.2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite> argued that a pixel-level annotated training set to train a supervised fingerprint enhancement model was unavailable. To alleviate this challenge, the authors generated a training set of <math id="S4.F20.2.1.m1.2" class="ltx_Math" alttext="9,042" display="inline"><semantics id="S4.F20.2.1.m1.2b"><mrow id="S4.F20.2.1.m1.2.3.2" xref="S4.F20.2.1.m1.2.3.1.cmml"><mn id="S4.F20.2.1.m1.1.1" xref="S4.F20.2.1.m1.1.1.cmml">9</mn><mo id="S4.F20.2.1.m1.2.3.2.1" xref="S4.F20.2.1.m1.2.3.1.cmml">,</mo><mn id="S4.F20.2.1.m1.2.2" xref="S4.F20.2.1.m1.2.2.cmml">042</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F20.2.1.m1.2c"><list id="S4.F20.2.1.m1.2.3.1.cmml" xref="S4.F20.2.1.m1.2.3.2"><cn type="integer" id="S4.F20.2.1.m1.1.1.cmml" xref="S4.F20.2.1.m1.1.1">9</cn><cn type="integer" id="S4.F20.2.1.m1.2.2.cmml" xref="S4.F20.2.1.m1.2.2">042</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.F20.2.1.m1.2d">9,042</annotation></semantics></math> synthetically distorted fingerprints and the corresponding annotations. In this figure, all fingerprints have a common ground truth enhanced image (third row, fourth column). Thus, synthetic data can be used as a proxy dataset for several applications that originally lack an unannotated training dataset, however need it for training the model.</span></figcaption>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span><span id="S4.SS5.1.1" class="ltx_text ltx_font_italic">Providing annotated data for supervision</span>
</h3>

<section id="S4.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.1 </span>Supervised Learning</h4>

<div id="S4.SS5.SSS1.p1" class="ltx_para">
<p id="S4.SS5.SSS1.p1.1" class="ltx_p">Numerous applications can be formulated as a supervised learning problem, however, real annotated data cannot be obtained for them. For such applications, representative synthetic samples and their annotations are generated in order to train models in supervised learning manner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>, <a href="#bib.bib143" title="" class="ltx_ref">143</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib144" title="" class="ltx_ref">144</a>, <a href="#bib.bib145" title="" class="ltx_ref">145</a>, <a href="#bib.bib146" title="" class="ltx_ref">146</a>, <a href="#bib.bib147" title="" class="ltx_ref">147</a>, <a href="#bib.bib87" title="" class="ltx_ref">87</a>, <a href="#bib.bib148" title="" class="ltx_ref">148</a>, <a href="#bib.bib149" title="" class="ltx_ref">149</a>]</cite> (see Figure <a href="#S4.F20" title="Figure 20 ‣ 4.4 Assessing scalability of systems ‣ 4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a>). Feng <span id="S4.SS5.SSS1.p1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> argued that manually annotated facial landmarks are often inaccurate for occluded facial regions. While the annotations of synthetic faces generated from a 3D model are correct for all different pose variations as these are direct projections to 2D from 3D. Therefore, the authors used a synthetic dataset to obtain reliable and consistent annotations for various image variations. Similarly, Liu <span id="S4.SS5.SSS1.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> employed synthetic data with dense point-to-point correspondence maps towards learning a 3D face model. Some applications have exploited synthetic data to learn a transformation from distorted to clean samples. Associated to this direction, Dieckmann <span id="S4.SS5.SSS1.p1.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite> proposed to learn the pre-aligning of fingerprint images through horizontally and vertically translated and rotated synthetic fingerprints. Likewise, Zhang <span id="S4.SS5.SSS1.p1.1.4" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib150" title="" class="ltx_ref">150</a>]</cite>, Joshi <span id="S4.SS5.SSS1.p1.1.5" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite> and Nojavanasghari <span id="S4.SS5.SSS1.p1.1.6" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite> utilized synthetic data to learn blind inpainting of face images, enhancement of fingerprints and transformation from occluded to non-occluded faces, respectively.</p>
</div>
<figure id="S4.F21" class="ltx_figure"><img src="/html/2208.09191/assets/x17.png" id="S4.F21.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="195" height="68" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F21.3.1.1" class="ltx_text" style="font-size:90%;">Figure 21</span>: </span><span id="S4.F21.4.2" class="ltx_text" style="font-size:90%;">Costa <span id="S4.F21.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>]</cite> introduced a large-scale synthetic human action recognition dataset to promote the design of unsupervised domain adaptation methods for minimizing the cost and human effort in acquiring a large annotated dataset for human action recognition.</span></figcaption>
</figure>
</section>
<section id="S4.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.2 </span>Unsupervised domain adaptation</h4>

<div id="S4.SS5.SSS2.p1" class="ltx_para">
<p id="S4.SS5.SSS2.p1.4" class="ltx_p">Supervised deep neural networks require a massive amount of manually annotated training data. However, collection, and particularly annotation of such is tedious, time-consuming and expensive. Furthermore, many human analysis applications require annotations by domain experts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib152" title="" class="ltx_ref">152</a>]</cite>, or reliable annotations cannot be obtained for the real data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib153" title="" class="ltx_ref">153</a>]</cite>. To address this challenge, researchers proposed to train models on a synthetic training dataset whose annotations can be computationally acquired. However, a huge gap in model performance was observed between real and synthetic data due to the visible <span id="S4.SS5.SSS2.p1.4.1" class="ltx_text ltx_font_italic">domain shift</span> (see Figure <a href="#S4.F21" title="Figure 21 ‣ 4.5.1 Supervised Learning ‣ 4.5 Providing annotated data for supervision ‣ 4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21</span></a>). Researchers adapted models to unannotated real-world datasets, in order to reduce the performance gap between real and synthetic data. An important application of unsupervised domain adaptation of human analysis models includes the contributions of Wang <span id="S4.SS5.SSS2.p1.4.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>. The authors exploited <math id="S4.SS5.SSS2.p1.1.m1.2" class="ltx_Math" alttext="15,212" display="inline"><semantics id="S4.SS5.SSS2.p1.1.m1.2a"><mrow id="S4.SS5.SSS2.p1.1.m1.2.3.2" xref="S4.SS5.SSS2.p1.1.m1.2.3.1.cmml"><mn id="S4.SS5.SSS2.p1.1.m1.1.1" xref="S4.SS5.SSS2.p1.1.m1.1.1.cmml">15</mn><mo id="S4.SS5.SSS2.p1.1.m1.2.3.2.1" xref="S4.SS5.SSS2.p1.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS5.SSS2.p1.1.m1.2.2" xref="S4.SS5.SSS2.p1.1.m1.2.2.cmml">212</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS2.p1.1.m1.2b"><list id="S4.SS5.SSS2.p1.1.m1.2.3.1.cmml" xref="S4.SS5.SSS2.p1.1.m1.2.3.2"><cn type="integer" id="S4.SS5.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS5.SSS2.p1.1.m1.1.1">15</cn><cn type="integer" id="S4.SS5.SSS2.p1.1.m1.2.2.cmml" xref="S4.SS5.SSS2.p1.1.m1.2.2">212</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS2.p1.1.m1.2c">15,212</annotation></semantics></math> synthetic labelled crowd scene images containing more than <math id="S4.SS5.SSS2.p1.2.m2.3" class="ltx_Math" alttext="7,000,000" display="inline"><semantics id="S4.SS5.SSS2.p1.2.m2.3a"><mrow id="S4.SS5.SSS2.p1.2.m2.3.4.2" xref="S4.SS5.SSS2.p1.2.m2.3.4.1.cmml"><mn id="S4.SS5.SSS2.p1.2.m2.1.1" xref="S4.SS5.SSS2.p1.2.m2.1.1.cmml">7</mn><mo id="S4.SS5.SSS2.p1.2.m2.3.4.2.1" xref="S4.SS5.SSS2.p1.2.m2.3.4.1.cmml">,</mo><mn id="S4.SS5.SSS2.p1.2.m2.2.2" xref="S4.SS5.SSS2.p1.2.m2.2.2.cmml">000</mn><mo id="S4.SS5.SSS2.p1.2.m2.3.4.2.2" xref="S4.SS5.SSS2.p1.2.m2.3.4.1.cmml">,</mo><mn id="S4.SS5.SSS2.p1.2.m2.3.3" xref="S4.SS5.SSS2.p1.2.m2.3.3.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS2.p1.2.m2.3b"><list id="S4.SS5.SSS2.p1.2.m2.3.4.1.cmml" xref="S4.SS5.SSS2.p1.2.m2.3.4.2"><cn type="integer" id="S4.SS5.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS5.SSS2.p1.2.m2.1.1">7</cn><cn type="integer" id="S4.SS5.SSS2.p1.2.m2.2.2.cmml" xref="S4.SS5.SSS2.p1.2.m2.2.2">000</cn><cn type="integer" id="S4.SS5.SSS2.p1.2.m2.3.3.cmml" xref="S4.SS5.SSS2.p1.2.m2.3.3">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS2.p1.2.m2.3c">7,000,000</annotation></semantics></math> subjects for the purpose of training a model for pixel-level understanding in a crowd. However, instead of directly using the synthetic data, the authors firstly translated synthetic images into realistic images using a GAN. This was beneficial in reducing the domain gap between synthetic and real data. Next, the model was trained on translated images instead of actual real images. The authors reported that the structural similarity index measure (SSIM) value improved from <math id="S4.SS5.SSS2.p1.3.m3.1" class="ltx_Math" alttext="0.554" display="inline"><semantics id="S4.SS5.SSS2.p1.3.m3.1a"><mn id="S4.SS5.SSS2.p1.3.m3.1.1" xref="S4.SS5.SSS2.p1.3.m3.1.1.cmml">0.554</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS2.p1.3.m3.1b"><cn type="float" id="S4.SS5.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS5.SSS2.p1.3.m3.1.1">0.554</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS2.p1.3.m3.1c">0.554</annotation></semantics></math> to <math id="S4.SS5.SSS2.p1.4.m4.1" class="ltx_Math" alttext="0.660" display="inline"><semantics id="S4.SS5.SSS2.p1.4.m4.1a"><mn id="S4.SS5.SSS2.p1.4.m4.1.1" xref="S4.SS5.SSS2.p1.4.m4.1.1.cmml">0.660</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS2.p1.4.m4.1b"><cn type="float" id="S4.SS5.SSS2.p1.4.m4.1.1.cmml" xref="S4.SS5.SSS2.p1.4.m4.1.1">0.660</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS2.p1.4.m4.1c">0.660</annotation></semantics></math> after exploiting the synthetic crowd counting dataset.</p>
</div>
<div id="S4.SS5.SSS2.p2" class="ltx_para">
<p id="S4.SS5.SSS2.p2.1" class="ltx_p">Joshi <span id="S4.SS5.SSS2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib152" title="" class="ltx_ref">152</a>]</cite> highlighted the dependence of state-of-the-art fingerprint segmentation models on annotated data as a means to obtain satisfactory performance on a newly introduced fingerprint capture device. To mitigate this limitation, the authors only used synthetic data (source domain) annotations to learn fingerprint segmentation. To adapt the model to a new fingerprint capture device (target domain), the authors aligned the source and target domain features using recurrent adversarial learning. Extending the theme of unsupervised domain adaptation, Bondi <span id="S4.SS5.SSS2.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib153" title="" class="ltx_ref">153</a>]</cite> argued that annotations of thermal infrared videos were often erroneous and therefore proposed to train the detection and tracking model on a synthetic dataset, adapting subsequently to a real dataset. Several applications spanning areas such as face recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib154" title="" class="ltx_ref">154</a>]</cite>, person re-identification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib155" title="" class="ltx_ref">155</a>]</cite>, human action recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>]</cite> and head pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib156" title="" class="ltx_ref">156</a>]</cite> successfully exploited synthetic data to eliminate the need for annotations of real data through unsupervised domain adaptation.</p>
</div>
<figure id="S4.F22" class="ltx_figure"><img src="/html/2208.09191/assets/x18.png" id="S4.F22.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="161" height="103" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F22.3.1.1" class="ltx_text" style="font-size:90%;">Figure 22</span>: </span><span id="S4.F22.4.2" class="ltx_text" style="font-size:90%;">Sindagi <span id="S4.F22.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib157" title="" class="ltx_ref">157</a>]</cite> studied domain adaptation of a crowd counting model. While the source had pixel-level annotations, the target data was annotated on image level and only provided weak supervision.</span></figcaption>
</figure>
<figure id="S4.F23" class="ltx_figure"><img src="/html/2208.09191/assets/x19.png" id="S4.F23.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="190" height="104" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F23.3.1.1" class="ltx_text" style="font-size:90%;">Figure 23</span>: </span><span id="S4.F23.4.2" class="ltx_text" style="font-size:90%;">Mequanint <span id="S4.F23.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib158" title="" class="ltx_ref">158</a>]</cite> proposed weakly supervised learning in an eye-closeness estimation model. The model exploited synthetic annotated data that provided a degree of openness of eyes, whereas the real data only provided weak supervision, whether the eye is open or closed. Thus, annotated synthetic data can be used to enable learning in weakly supervised learning.</span></figcaption>
</figure>
</section>
<section id="S4.SS5.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.3 </span>Weakly supervised learning</h4>

<div id="S4.SS5.SSS3.p1" class="ltx_para">
<p id="S4.SS5.SSS3.p1.4" class="ltx_p">Synthetic annotated data has been utilized in weakly supervised learning (see Figure <a href="#S4.F22" title="Figure 22 ‣ 4.5.2 Unsupervised domain adaptation ‣ 4.5 Providing annotated data for supervision ‣ 4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22</span></a>) aiming to introduce a higher degree of supervision. For instance, Mequanint <span id="S4.SS5.SSS3.p1.4.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib158" title="" class="ltx_ref">158</a>]</cite> highlighted the unavailability of annotated data for training an eye-openness estimation model. To alleviate this issue, the authors generated <math id="S4.SS5.SSS3.p1.1.m1.1" class="ltx_Math" alttext="1.3" display="inline"><semantics id="S4.SS5.SSS3.p1.1.m1.1a"><mn id="S4.SS5.SSS3.p1.1.m1.1.1" xref="S4.SS5.SSS3.p1.1.m1.1.1.cmml">1.3</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS3.p1.1.m1.1b"><cn type="float" id="S4.SS5.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS5.SSS3.p1.1.m1.1.1">1.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS3.p1.1.m1.1c">1.3</annotation></semantics></math> million annotated synthetic eye images with varying levels of eye openness to enable supervised learning. Furthermore, to counter the domain shift between real and synthetic eye images, the authors exploited weak supervision (eyes simply open or closed). It was demonstrated that the classification (open/close) accuracy improves from <math id="S4.SS5.SSS3.p1.2.m2.1" class="ltx_Math" alttext="96.30\%" display="inline"><semantics id="S4.SS5.SSS3.p1.2.m2.1a"><mrow id="S4.SS5.SSS3.p1.2.m2.1.1" xref="S4.SS5.SSS3.p1.2.m2.1.1.cmml"><mn id="S4.SS5.SSS3.p1.2.m2.1.1.2" xref="S4.SS5.SSS3.p1.2.m2.1.1.2.cmml">96.30</mn><mo id="S4.SS5.SSS3.p1.2.m2.1.1.1" xref="S4.SS5.SSS3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS3.p1.2.m2.1b"><apply id="S4.SS5.SSS3.p1.2.m2.1.1.cmml" xref="S4.SS5.SSS3.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS5.SSS3.p1.2.m2.1.1.1.cmml" xref="S4.SS5.SSS3.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS5.SSS3.p1.2.m2.1.1.2.cmml" xref="S4.SS5.SSS3.p1.2.m2.1.1.2">96.30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS3.p1.2.m2.1c">96.30\%</annotation></semantics></math> to 100% after utilizing synthetic data. Deviating from the above, Zhang <span id="S4.SS5.SSS3.p1.4.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib159" title="" class="ltx_ref">159</a>]</cite> generated weakly labelled face images (labels as bounding box and class) using a deep convolutional generative adversarial network (DCGAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite> and used a limited amount of fully annotated real data (labels as landmark vector, bounding box and class). A weakly supervised learning framework was used to train the facial landmark detection model, which improved the average error distance for landmark detection on the labelled face parts in the wild (LFPW) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib160" title="" class="ltx_ref">160</a>]</cite> from <math id="S4.SS5.SSS3.p1.3.m3.1" class="ltx_Math" alttext="4.25" display="inline"><semantics id="S4.SS5.SSS3.p1.3.m3.1a"><mn id="S4.SS5.SSS3.p1.3.m3.1.1" xref="S4.SS5.SSS3.p1.3.m3.1.1.cmml">4.25</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS3.p1.3.m3.1b"><cn type="float" id="S4.SS5.SSS3.p1.3.m3.1.1.cmml" xref="S4.SS5.SSS3.p1.3.m3.1.1">4.25</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS3.p1.3.m3.1c">4.25</annotation></semantics></math> to <math id="S4.SS5.SSS3.p1.4.m4.1" class="ltx_Math" alttext="3.12" display="inline"><semantics id="S4.SS5.SSS3.p1.4.m4.1a"><mn id="S4.SS5.SSS3.p1.4.m4.1.1" xref="S4.SS5.SSS3.p1.4.m4.1.1.cmml">3.12</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.SSS3.p1.4.m4.1b"><cn type="float" id="S4.SS5.SSS3.p1.4.m4.1.1.cmml" xref="S4.SS5.SSS3.p1.4.m4.1.1">3.12</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.SSS3.p1.4.m4.1c">3.12</annotation></semantics></math> after utilizing synthetic faces.</p>
</div>
</section>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span><span id="S4.SS6.1.1" class="ltx_text ltx_font_italic">Pre-training a deep model</span>
</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.6" class="ltx_p">Deep neural network models impart a large number of parameters and, therefore, require a large amount of training data to avoid over-fitting. We have that the ImageNet dataset incorporates approximately <math id="S4.SS6.p1.1.m1.1" class="ltx_Math" alttext="1.2" display="inline"><semantics id="S4.SS6.p1.1.m1.1a"><mn id="S4.SS6.p1.1.m1.1.1" xref="S4.SS6.p1.1.m1.1.1.cmml">1.2</mn><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.1.m1.1b"><cn type="float" id="S4.SS6.p1.1.m1.1.1.cmml" xref="S4.SS6.p1.1.m1.1.1">1.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.1.m1.1c">1.2</annotation></semantics></math> million annotated images. However, in human analysis often only limited annotated training sets are publicly available, including <span id="S4.SS6.p1.6.1" class="ltx_text ltx_font_italic">e.g.,</span> hundreds or thousands of images. Therefore, once again synthetic data is advantageous in alleviating the need for a large amount of training data required for training data-hungry deep models. It is common practice to generate annotated synthetic datasets and use such to pre-train deep models, which are then fine-tuned with annotated real data. A number of studies demonstrated that such pre-training with synthetic datasets leads to better performance than training directly on the real dataset. In one of the recent studies, Engelsma <span id="S4.SS6.p1.6.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> demonstrated that performance gain was observed by a DNN-based fingerprint recognition model (DeepPrint) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite> that was pre-trained on synthetic fingerprints and fine-tuned on real fingerprints. The authors generated <math id="S4.SS6.p1.2.m2.1" class="ltx_Math" alttext="525" display="inline"><semantics id="S4.SS6.p1.2.m2.1a"><mn id="S4.SS6.p1.2.m2.1.1" xref="S4.SS6.p1.2.m2.1.1.cmml">525</mn><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.2.m2.1b"><cn type="integer" id="S4.SS6.p1.2.m2.1.1.cmml" xref="S4.SS6.p1.2.m2.1.1">525</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.2.m2.1c">525</annotation></semantics></math>K synthetic fingerprints for pre-training DeepPrint and fine-tuned it on <math id="S4.SS6.p1.3.m3.1" class="ltx_Math" alttext="25" display="inline"><semantics id="S4.SS6.p1.3.m3.1a"><mn id="S4.SS6.p1.3.m3.1.1" xref="S4.SS6.p1.3.m3.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.3.m3.1b"><cn type="integer" id="S4.SS6.p1.3.m3.1.1.cmml" xref="S4.SS6.p1.3.m3.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.3.m3.1c">25</annotation></semantics></math>K fingerprints from the NIST SD302 database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib162" title="" class="ltx_ref">162</a>]</cite>. The authors then assessed the fingerprint recognition performance of DeepPrint on NIST SD4 database<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.nist.gov/srd/nist-special-database-4</span></span></span></span>, with and without pre-training with synthetic data. The authors observed that the true acceptance rate (TAR) @ false acceptance rate (FAR)=<math id="S4.SS6.p1.4.m4.1" class="ltx_Math" alttext="0.01\%" display="inline"><semantics id="S4.SS6.p1.4.m4.1a"><mrow id="S4.SS6.p1.4.m4.1.1" xref="S4.SS6.p1.4.m4.1.1.cmml"><mn id="S4.SS6.p1.4.m4.1.1.2" xref="S4.SS6.p1.4.m4.1.1.2.cmml">0.01</mn><mo id="S4.SS6.p1.4.m4.1.1.1" xref="S4.SS6.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.4.m4.1b"><apply id="S4.SS6.p1.4.m4.1.1.cmml" xref="S4.SS6.p1.4.m4.1.1"><csymbol cd="latexml" id="S4.SS6.p1.4.m4.1.1.1.cmml" xref="S4.SS6.p1.4.m4.1.1.1">percent</csymbol><cn type="float" id="S4.SS6.p1.4.m4.1.1.2.cmml" xref="S4.SS6.p1.4.m4.1.1.2">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.4.m4.1c">0.01\%</annotation></semantics></math> improves from <math id="S4.SS6.p1.5.m5.1" class="ltx_Math" alttext="73.37\%" display="inline"><semantics id="S4.SS6.p1.5.m5.1a"><mrow id="S4.SS6.p1.5.m5.1.1" xref="S4.SS6.p1.5.m5.1.1.cmml"><mn id="S4.SS6.p1.5.m5.1.1.2" xref="S4.SS6.p1.5.m5.1.1.2.cmml">73.37</mn><mo id="S4.SS6.p1.5.m5.1.1.1" xref="S4.SS6.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.5.m5.1b"><apply id="S4.SS6.p1.5.m5.1.1.cmml" xref="S4.SS6.p1.5.m5.1.1"><csymbol cd="latexml" id="S4.SS6.p1.5.m5.1.1.1.cmml" xref="S4.SS6.p1.5.m5.1.1.1">percent</csymbol><cn type="float" id="S4.SS6.p1.5.m5.1.1.2.cmml" xref="S4.SS6.p1.5.m5.1.1.2">73.37</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.5.m5.1c">73.37\%</annotation></semantics></math> to <math id="S4.SS6.p1.6.m6.1" class="ltx_Math" alttext="87.03\%" display="inline"><semantics id="S4.SS6.p1.6.m6.1a"><mrow id="S4.SS6.p1.6.m6.1.1" xref="S4.SS6.p1.6.m6.1.1.cmml"><mn id="S4.SS6.p1.6.m6.1.1.2" xref="S4.SS6.p1.6.m6.1.1.2.cmml">87.03</mn><mo id="S4.SS6.p1.6.m6.1.1.1" xref="S4.SS6.p1.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.6.m6.1b"><apply id="S4.SS6.p1.6.m6.1.1.cmml" xref="S4.SS6.p1.6.m6.1.1"><csymbol cd="latexml" id="S4.SS6.p1.6.m6.1.1.1.cmml" xref="S4.SS6.p1.6.m6.1.1.1">percent</csymbol><cn type="float" id="S4.SS6.p1.6.m6.1.1.2.cmml" xref="S4.SS6.p1.6.m6.1.1.2">87.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.6.m6.1c">87.03\%</annotation></semantics></math>, when pre-trained with synthetically generated fingerprints.</p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p id="S4.SS6.p2.3" class="ltx_p">Similarly, Wang <span id="S4.SS6.p2.3.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> trained a pixel-level crowd understanding model on large-scale synthetic data (<math id="S4.SS6.p2.1.m1.2" class="ltx_Math" alttext="15,212" display="inline"><semantics id="S4.SS6.p2.1.m1.2a"><mrow id="S4.SS6.p2.1.m1.2.3.2" xref="S4.SS6.p2.1.m1.2.3.1.cmml"><mn id="S4.SS6.p2.1.m1.1.1" xref="S4.SS6.p2.1.m1.1.1.cmml">15</mn><mo id="S4.SS6.p2.1.m1.2.3.2.1" xref="S4.SS6.p2.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS6.p2.1.m1.2.2" xref="S4.SS6.p2.1.m1.2.2.cmml">212</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.1.m1.2b"><list id="S4.SS6.p2.1.m1.2.3.1.cmml" xref="S4.SS6.p2.1.m1.2.3.2"><cn type="integer" id="S4.SS6.p2.1.m1.1.1.cmml" xref="S4.SS6.p2.1.m1.1.1">15</cn><cn type="integer" id="S4.SS6.p2.1.m1.2.2.cmml" xref="S4.SS6.p2.1.m1.2.2">212</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.1.m1.2c">15,212</annotation></semantics></math> images of <math id="S4.SS6.p2.2.m2.3" class="ltx_Math" alttext="7,625,843" display="inline"><semantics id="S4.SS6.p2.2.m2.3a"><mrow id="S4.SS6.p2.2.m2.3.4.2" xref="S4.SS6.p2.2.m2.3.4.1.cmml"><mn id="S4.SS6.p2.2.m2.1.1" xref="S4.SS6.p2.2.m2.1.1.cmml">7</mn><mo id="S4.SS6.p2.2.m2.3.4.2.1" xref="S4.SS6.p2.2.m2.3.4.1.cmml">,</mo><mn id="S4.SS6.p2.2.m2.2.2" xref="S4.SS6.p2.2.m2.2.2.cmml">625</mn><mo id="S4.SS6.p2.2.m2.3.4.2.2" xref="S4.SS6.p2.2.m2.3.4.1.cmml">,</mo><mn id="S4.SS6.p2.2.m2.3.3" xref="S4.SS6.p2.2.m2.3.3.cmml">843</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.2.m2.3b"><list id="S4.SS6.p2.2.m2.3.4.1.cmml" xref="S4.SS6.p2.2.m2.3.4.2"><cn type="integer" id="S4.SS6.p2.2.m2.1.1.cmml" xref="S4.SS6.p2.2.m2.1.1">7</cn><cn type="integer" id="S4.SS6.p2.2.m2.2.2.cmml" xref="S4.SS6.p2.2.m2.2.2">625</cn><cn type="integer" id="S4.SS6.p2.2.m2.3.3.cmml" xref="S4.SS6.p2.2.m2.3.3">843</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.2.m2.3c">7,625,843</annotation></semantics></math> subjects) and fine-tuned it on labelled real data. The mean square error decreased by <math id="S4.SS6.p2.3.m3.1" class="ltx_Math" alttext="14.1\%" display="inline"><semantics id="S4.SS6.p2.3.m3.1a"><mrow id="S4.SS6.p2.3.m3.1.1" xref="S4.SS6.p2.3.m3.1.1.cmml"><mn id="S4.SS6.p2.3.m3.1.1.2" xref="S4.SS6.p2.3.m3.1.1.2.cmml">14.1</mn><mo id="S4.SS6.p2.3.m3.1.1.1" xref="S4.SS6.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.p2.3.m3.1b"><apply id="S4.SS6.p2.3.m3.1.1.cmml" xref="S4.SS6.p2.3.m3.1.1"><csymbol cd="latexml" id="S4.SS6.p2.3.m3.1.1.1.cmml" xref="S4.SS6.p2.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS6.p2.3.m3.1.1.2.cmml" xref="S4.SS6.p2.3.m3.1.1.2">14.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p2.3.m3.1c">14.1\%</annotation></semantics></math> after pre-training on synthetic data was noted, compared to the performance of the crowd counting model pre-trained on ImageNet dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>. Similar trends were observed for other applications analyzing human data including speech recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, hand shape recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>]</cite>, head pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, eye gaze tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib165" title="" class="ltx_ref">165</a>]</cite>, re-identification of individuals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>]</cite> and face recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib167" title="" class="ltx_ref">167</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span><span id="S4.SS7.1.1" class="ltx_text ltx_font_italic">Fine-tuning a pre-trained model</span>
</h3>

<div id="S4.SS7.p1" class="ltx_para">
<p id="S4.SS7.p1.5" class="ltx_p">In addition to pre-training with synthetic data, the following counteracts the need for large-scale annotated data. The parameters of deep neural networks are initialized with pre-trained weights of standard deep models trained on large-scale datasets of real images. Such networks are then fine-tuned with annotated synthetic dataset associated to the respective application. Following this approach, Dou <span id="S4.SS7.p1.5.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite> proposed a 3D face reconstruction model, whose weights were initialized with the parameters of the VGG-Face model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>]</cite> (a standard model trained on real human faces) and which was then fine-tuned on <math id="S4.SS7.p1.1.m1.1" class="ltx_Math" alttext="250" display="inline"><semantics id="S4.SS7.p1.1.m1.1a"><mn id="S4.SS7.p1.1.m1.1.1" xref="S4.SS7.p1.1.m1.1.1.cmml">250</mn><annotation-xml encoding="MathML-Content" id="S4.SS7.p1.1.m1.1b"><cn type="integer" id="S4.SS7.p1.1.m1.1.1.cmml" xref="S4.SS7.p1.1.m1.1.1">250</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.p1.1.m1.1c">250</annotation></semantics></math>K synthetic <math id="S4.SS7.p1.2.m2.1" class="ltx_Math" alttext="2D" display="inline"><semantics id="S4.SS7.p1.2.m2.1a"><mrow id="S4.SS7.p1.2.m2.1.1" xref="S4.SS7.p1.2.m2.1.1.cmml"><mn id="S4.SS7.p1.2.m2.1.1.2" xref="S4.SS7.p1.2.m2.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.SS7.p1.2.m2.1.1.1" xref="S4.SS7.p1.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS7.p1.2.m2.1.1.3" xref="S4.SS7.p1.2.m2.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS7.p1.2.m2.1b"><apply id="S4.SS7.p1.2.m2.1.1.cmml" xref="S4.SS7.p1.2.m2.1.1"><times id="S4.SS7.p1.2.m2.1.1.1.cmml" xref="S4.SS7.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS7.p1.2.m2.1.1.2.cmml" xref="S4.SS7.p1.2.m2.1.1.2">2</cn><ci id="S4.SS7.p1.2.m2.1.1.3.cmml" xref="S4.SS7.p1.2.m2.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.p1.2.m2.1c">2D</annotation></semantics></math> face images. Kim <span id="S4.SS7.p1.5.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite> pre-trained the near-infrared (NIR) face recognition model on <math id="S4.SS7.p1.3.m3.2" class="ltx_Math" alttext="453,414" display="inline"><semantics id="S4.SS7.p1.3.m3.2a"><mrow id="S4.SS7.p1.3.m3.2.3.2" xref="S4.SS7.p1.3.m3.2.3.1.cmml"><mn id="S4.SS7.p1.3.m3.1.1" xref="S4.SS7.p1.3.m3.1.1.cmml">453</mn><mo id="S4.SS7.p1.3.m3.2.3.2.1" xref="S4.SS7.p1.3.m3.2.3.1.cmml">,</mo><mn id="S4.SS7.p1.3.m3.2.2" xref="S4.SS7.p1.3.m3.2.2.cmml">414</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS7.p1.3.m3.2b"><list id="S4.SS7.p1.3.m3.2.3.1.cmml" xref="S4.SS7.p1.3.m3.2.3.2"><cn type="integer" id="S4.SS7.p1.3.m3.1.1.cmml" xref="S4.SS7.p1.3.m3.1.1">453</cn><cn type="integer" id="S4.SS7.p1.3.m3.2.2.cmml" xref="S4.SS7.p1.3.m3.2.2">414</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.p1.3.m3.2c">453,414</annotation></semantics></math> RGB face images of the CASIA WebFace dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib171" title="" class="ltx_ref">171</a>]</cite>. Next, <math id="S4.SS7.p1.4.m4.2" class="ltx_Math" alttext="32,992" display="inline"><semantics id="S4.SS7.p1.4.m4.2a"><mrow id="S4.SS7.p1.4.m4.2.3.2" xref="S4.SS7.p1.4.m4.2.3.1.cmml"><mn id="S4.SS7.p1.4.m4.1.1" xref="S4.SS7.p1.4.m4.1.1.cmml">32</mn><mo id="S4.SS7.p1.4.m4.2.3.2.1" xref="S4.SS7.p1.4.m4.2.3.1.cmml">,</mo><mn id="S4.SS7.p1.4.m4.2.2" xref="S4.SS7.p1.4.m4.2.2.cmml">992</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS7.p1.4.m4.2b"><list id="S4.SS7.p1.4.m4.2.3.1.cmml" xref="S4.SS7.p1.4.m4.2.3.2"><cn type="integer" id="S4.SS7.p1.4.m4.1.1.cmml" xref="S4.SS7.p1.4.m4.1.1">32</cn><cn type="integer" id="S4.SS7.p1.4.m4.2.2.cmml" xref="S4.SS7.p1.4.m4.2.2">992</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.p1.4.m4.2c">32,992</annotation></semantics></math> synthetic NIR face images were generated using CycleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite>, augmenting the training set for fine-tuning. The authors reported a <math id="S4.SS7.p1.5.m5.1" class="ltx_Math" alttext="0.31\%" display="inline"><semantics id="S4.SS7.p1.5.m5.1a"><mrow id="S4.SS7.p1.5.m5.1.1" xref="S4.SS7.p1.5.m5.1.1.cmml"><mn id="S4.SS7.p1.5.m5.1.1.2" xref="S4.SS7.p1.5.m5.1.1.2.cmml">0.31</mn><mo id="S4.SS7.p1.5.m5.1.1.1" xref="S4.SS7.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS7.p1.5.m5.1b"><apply id="S4.SS7.p1.5.m5.1.1.cmml" xref="S4.SS7.p1.5.m5.1.1"><csymbol cd="latexml" id="S4.SS7.p1.5.m5.1.1.1.cmml" xref="S4.SS7.p1.5.m5.1.1.1">percent</csymbol><cn type="float" id="S4.SS7.p1.5.m5.1.1.2.cmml" xref="S4.SS7.p1.5.m5.1.1.2">0.31</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.p1.5.m5.1c">0.31\%</annotation></semantics></math> increase in face identification rate after fine-tuning the augmented data compared to training the model from scratch. Similar applications of synthetic data for fine-tuning of a pre-trained model include finger vein recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib172" title="" class="ltx_ref">172</a>]</cite>, iris PAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib173" title="" class="ltx_ref">173</a>]</cite> and fingerprint PAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib174" title="" class="ltx_ref">174</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.8 </span><span id="S4.SS8.1.1" class="ltx_text ltx_font_italic">Enforcing consistency regularization</span>
</h3>

<section id="S4.SS8.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.8.1 </span>Contrastive Learning</h4>

<div id="S4.SS8.SSS1.p1" class="ltx_para">
<p id="S4.SS8.SSS1.p1.2" class="ltx_p">Contrastive learning is a learning paradigm that ensures that representations of similar samples must be close, whereas representations of dissimilar samples are far apart in the latent space. Various studies exploited synthetically augmented data to generate similar samples for a given input. Subsequently, using contrastive learning, the model was encouraged to have similar representations for the original and the augmented input samples. Ryoo <span id="S4.SS8.SSS1.p1.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite> introduced different low resolution (LR) transformations into videos and trained an activity recognition model such that the images obtained from the same scene, pertaining to different pixel values due to LR transformation shared a common embedding. The authors demonstrated that the classification accuracy under low-resolution constraints improves from <math id="S4.SS8.SSS1.p1.1.m1.1" class="ltx_Math" alttext="31.50\%" display="inline"><semantics id="S4.SS8.SSS1.p1.1.m1.1a"><mrow id="S4.SS8.SSS1.p1.1.m1.1.1" xref="S4.SS8.SSS1.p1.1.m1.1.1.cmml"><mn id="S4.SS8.SSS1.p1.1.m1.1.1.2" xref="S4.SS8.SSS1.p1.1.m1.1.1.2.cmml">31.50</mn><mo id="S4.SS8.SSS1.p1.1.m1.1.1.1" xref="S4.SS8.SSS1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS8.SSS1.p1.1.m1.1b"><apply id="S4.SS8.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS8.SSS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS8.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS8.SSS1.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS8.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS8.SSS1.p1.1.m1.1.1.2">31.50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS8.SSS1.p1.1.m1.1c">31.50\%</annotation></semantics></math> to <math id="S4.SS8.SSS1.p1.2.m2.1" class="ltx_Math" alttext="37.70\%" display="inline"><semantics id="S4.SS8.SSS1.p1.2.m2.1a"><mrow id="S4.SS8.SSS1.p1.2.m2.1.1" xref="S4.SS8.SSS1.p1.2.m2.1.1.cmml"><mn id="S4.SS8.SSS1.p1.2.m2.1.1.2" xref="S4.SS8.SSS1.p1.2.m2.1.1.2.cmml">37.70</mn><mo id="S4.SS8.SSS1.p1.2.m2.1.1.1" xref="S4.SS8.SSS1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS8.SSS1.p1.2.m2.1b"><apply id="S4.SS8.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS8.SSS1.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS8.SSS1.p1.2.m2.1.1.1.cmml" xref="S4.SS8.SSS1.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS8.SSS1.p1.2.m2.1.1.2.cmml" xref="S4.SS8.SSS1.p1.2.m2.1.1.2">37.70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS8.SSS1.p1.2.m2.1c">37.70\%</annotation></semantics></math> after using synthetic data. Neto <span id="S4.SS8.SSS1.p1.2.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib176" title="" class="ltx_ref">176</a>]</cite> applied augmentation techniques to generate synthetically masked faces. Contrastive learning brought then representations of masked and unmasked faces of the same data subject close to each other. The authors demonstrated that the model trained using the synthetically masked images outperformed existing standard face recognition systems on masked face recognition. Several other applications in speaker recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib177" title="" class="ltx_ref">177</a>]</cite>, face recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib178" title="" class="ltx_ref">178</a>]</cite>, person re-identification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite> and electrocardiogram (ECG) based authentication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>]</cite> proposed to generate synthetic data for exploiting contrastive learning.</p>
</div>
<figure id="S4.F24" class="ltx_figure"><img src="/html/2208.09191/assets/x20.png" id="S4.F24.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="152" height="83" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F24.3.1.1" class="ltx_text" style="font-size:90%;">Figure 24</span>: </span><span id="S4.F24.4.2" class="ltx_text" style="font-size:90%;">Ryoo <span id="S4.F24.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite> exploited different low-resolution transformations towards synthetically generating videos with the same scene and different pixel values due to changes in resolution. Later, a Siamese network was trained that ensures that the feature representations of the original and augmented video frames were similar. Thus, synthetic data can be used to ensure consistency among representations learnt by a human analysis model.</span></figcaption>
</figure>
<figure id="S4.F25" class="ltx_figure"><img src="/html/2208.09191/assets/x21.png" id="S4.F25.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="197" height="118" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F25.3.1.1" class="ltx_text" style="font-size:90%;">Figure 25</span>: </span><span id="S4.F25.4.2" class="ltx_text" style="font-size:90%;">Zhou <span id="S4.F25.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite> proposed rotate-and-render augmentation that given a face image generates a synthetic face image with a varying pose. Later, the synthetic face was rendered back to the original pose. Such an augmentation ensured self-supervision in face recognition models. As a result, the model learned to preserve consistency in identity information while varying facial poses.</span></figcaption>
</figure>
</section>
<section id="S4.SS8.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.8.2 </span>Self-supervision</h4>

<div id="S4.SS8.SSS2.p1" class="ltx_para">
<p id="S4.SS8.SSS2.p1.5" class="ltx_p">Self-supervision is an unsupervised learning paradigm
through which a model can be regularized by introducing an auxiliary task. Several approaches in human analysis have introduced transformations to an input data to generate synthetic labelled data for training the auxiliary task in a supervised manner. For example Zhou <span id="S4.SS8.SSS2.p1.5.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite> proposed rotate-and-render, an augmentation technique that rotates faces back and forth in <math id="S4.SS8.SSS2.p1.1.m1.1" class="ltx_Math" alttext="3D" display="inline"><semantics id="S4.SS8.SSS2.p1.1.m1.1a"><mrow id="S4.SS8.SSS2.p1.1.m1.1.1" xref="S4.SS8.SSS2.p1.1.m1.1.1.cmml"><mn id="S4.SS8.SSS2.p1.1.m1.1.1.2" xref="S4.SS8.SSS2.p1.1.m1.1.1.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S4.SS8.SSS2.p1.1.m1.1.1.1" xref="S4.SS8.SSS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS8.SSS2.p1.1.m1.1.1.3" xref="S4.SS8.SSS2.p1.1.m1.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS8.SSS2.p1.1.m1.1b"><apply id="S4.SS8.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS8.SSS2.p1.1.m1.1.1"><times id="S4.SS8.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS8.SSS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS8.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS8.SSS2.p1.1.m1.1.1.2">3</cn><ci id="S4.SS8.SSS2.p1.1.m1.1.1.3.cmml" xref="S4.SS8.SSS2.p1.1.m1.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS8.SSS2.p1.1.m1.1c">3D</annotation></semantics></math> space and subsequently renders them back in <math id="S4.SS8.SSS2.p1.2.m2.1" class="ltx_Math" alttext="2D" display="inline"><semantics id="S4.SS8.SSS2.p1.2.m2.1a"><mrow id="S4.SS8.SSS2.p1.2.m2.1.1" xref="S4.SS8.SSS2.p1.2.m2.1.1.cmml"><mn id="S4.SS8.SSS2.p1.2.m2.1.1.2" xref="S4.SS8.SSS2.p1.2.m2.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.SS8.SSS2.p1.2.m2.1.1.1" xref="S4.SS8.SSS2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS8.SSS2.p1.2.m2.1.1.3" xref="S4.SS8.SSS2.p1.2.m2.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS8.SSS2.p1.2.m2.1b"><apply id="S4.SS8.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS8.SSS2.p1.2.m2.1.1"><times id="S4.SS8.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS8.SSS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS8.SSS2.p1.2.m2.1.1.2.cmml" xref="S4.SS8.SSS2.p1.2.m2.1.1.2">2</cn><ci id="S4.SS8.SSS2.p1.2.m2.1.1.3.cmml" xref="S4.SS8.SSS2.p1.2.m2.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS8.SSS2.p1.2.m2.1c">2D</annotation></semantics></math> (see Figure <a href="#S4.F25" title="Figure 25 ‣ 4.8.1 Contrastive Learning ‣ 4.8 Enforcing consistency regularization ‣ 4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">25</span></a>). Such augmentation strategy ensured consistency regularization, while training face recognition models. As a result, <math id="S4.SS8.SSS2.p1.3.m3.1" class="ltx_Math" alttext="TAR@FAR=0.001" display="inline"><semantics id="S4.SS8.SSS2.p1.3.m3.1a"><mrow id="S4.SS8.SSS2.p1.3.m3.1.1" xref="S4.SS8.SSS2.p1.3.m3.1.1.cmml"><mrow id="S4.SS8.SSS2.p1.3.m3.1.1.2" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.cmml"><mi id="S4.SS8.SSS2.p1.3.m3.1.1.2.2" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS8.SSS2.p1.3.m3.1.1.2.1" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.1.cmml">​</mo><mi id="S4.SS8.SSS2.p1.3.m3.1.1.2.3" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS8.SSS2.p1.3.m3.1.1.2.1a" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.1.cmml">​</mo><mi id="S4.SS8.SSS2.p1.3.m3.1.1.2.4" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.4.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS8.SSS2.p1.3.m3.1.1.2.1b" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="S4.SS8.SSS2.p1.3.m3.1.1.2.5" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.5.cmml">@</mi><mo lspace="0em" rspace="0em" id="S4.SS8.SSS2.p1.3.m3.1.1.2.1c" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.1.cmml">​</mo><mi id="S4.SS8.SSS2.p1.3.m3.1.1.2.6" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.6.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.SS8.SSS2.p1.3.m3.1.1.2.1d" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.1.cmml">​</mo><mi id="S4.SS8.SSS2.p1.3.m3.1.1.2.7" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.7.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.SS8.SSS2.p1.3.m3.1.1.2.1e" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.1.cmml">​</mo><mi id="S4.SS8.SSS2.p1.3.m3.1.1.2.8" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.8.cmml">R</mi></mrow><mo id="S4.SS8.SSS2.p1.3.m3.1.1.1" xref="S4.SS8.SSS2.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS8.SSS2.p1.3.m3.1.1.3" xref="S4.SS8.SSS2.p1.3.m3.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS8.SSS2.p1.3.m3.1b"><apply id="S4.SS8.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS8.SSS2.p1.3.m3.1.1"><eq id="S4.SS8.SSS2.p1.3.m3.1.1.1.cmml" xref="S4.SS8.SSS2.p1.3.m3.1.1.1"></eq><apply id="S4.SS8.SSS2.p1.3.m3.1.1.2.cmml" xref="S4.SS8.SSS2.p1.3.m3.1.1.2"><times id="S4.SS8.SSS2.p1.3.m3.1.1.2.1.cmml" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.1"></times><ci id="S4.SS8.SSS2.p1.3.m3.1.1.2.2.cmml" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.2">𝑇</ci><ci id="S4.SS8.SSS2.p1.3.m3.1.1.2.3.cmml" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.3">𝐴</ci><ci id="S4.SS8.SSS2.p1.3.m3.1.1.2.4.cmml" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.4">𝑅</ci><ci id="S4.SS8.SSS2.p1.3.m3.1.1.2.5.cmml" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.5">@</ci><ci id="S4.SS8.SSS2.p1.3.m3.1.1.2.6.cmml" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.6">𝐹</ci><ci id="S4.SS8.SSS2.p1.3.m3.1.1.2.7.cmml" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.7">𝐴</ci><ci id="S4.SS8.SSS2.p1.3.m3.1.1.2.8.cmml" xref="S4.SS8.SSS2.p1.3.m3.1.1.2.8">𝑅</ci></apply><cn type="float" id="S4.SS8.SSS2.p1.3.m3.1.1.3.cmml" xref="S4.SS8.SSS2.p1.3.m3.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS8.SSS2.p1.3.m3.1c">TAR@FAR=0.001</annotation></semantics></math> on the IJB-A dataset improved from <math id="S4.SS8.SSS2.p1.4.m4.1" class="ltx_Math" alttext="80.00\%" display="inline"><semantics id="S4.SS8.SSS2.p1.4.m4.1a"><mrow id="S4.SS8.SSS2.p1.4.m4.1.1" xref="S4.SS8.SSS2.p1.4.m4.1.1.cmml"><mn id="S4.SS8.SSS2.p1.4.m4.1.1.2" xref="S4.SS8.SSS2.p1.4.m4.1.1.2.cmml">80.00</mn><mo id="S4.SS8.SSS2.p1.4.m4.1.1.1" xref="S4.SS8.SSS2.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS8.SSS2.p1.4.m4.1b"><apply id="S4.SS8.SSS2.p1.4.m4.1.1.cmml" xref="S4.SS8.SSS2.p1.4.m4.1.1"><csymbol cd="latexml" id="S4.SS8.SSS2.p1.4.m4.1.1.1.cmml" xref="S4.SS8.SSS2.p1.4.m4.1.1.1">percent</csymbol><cn type="float" id="S4.SS8.SSS2.p1.4.m4.1.1.2.cmml" xref="S4.SS8.SSS2.p1.4.m4.1.1.2">80.00</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS8.SSS2.p1.4.m4.1c">80.00\%</annotation></semantics></math> to <math id="S4.SS8.SSS2.p1.5.m5.1" class="ltx_Math" alttext="82.48\%" display="inline"><semantics id="S4.SS8.SSS2.p1.5.m5.1a"><mrow id="S4.SS8.SSS2.p1.5.m5.1.1" xref="S4.SS8.SSS2.p1.5.m5.1.1.cmml"><mn id="S4.SS8.SSS2.p1.5.m5.1.1.2" xref="S4.SS8.SSS2.p1.5.m5.1.1.2.cmml">82.48</mn><mo id="S4.SS8.SSS2.p1.5.m5.1.1.1" xref="S4.SS8.SSS2.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS8.SSS2.p1.5.m5.1b"><apply id="S4.SS8.SSS2.p1.5.m5.1.1.cmml" xref="S4.SS8.SSS2.p1.5.m5.1.1"><csymbol cd="latexml" id="S4.SS8.SSS2.p1.5.m5.1.1.1.cmml" xref="S4.SS8.SSS2.p1.5.m5.1.1.1">percent</csymbol><cn type="float" id="S4.SS8.SSS2.p1.5.m5.1.1.2.cmml" xref="S4.SS8.SSS2.p1.5.m5.1.1.2">82.48</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS8.SSS2.p1.5.m5.1c">82.48\%</annotation></semantics></math> after introducing self-supervision through the proposed augmentation strategy. Other applications utilizing synthetic data for self-supervision include deepfake detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite>, facial expression recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib182" title="" class="ltx_ref">182</a>]</cite>, face recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib183" title="" class="ltx_ref">183</a>]</cite> and sleep recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib184" title="" class="ltx_ref">184</a>]</cite>.</p>
</div>
<figure id="S4.F26" class="ltx_figure"><img src="/html/2208.09191/assets/x22.png" id="S4.F26.g1" class="ltx_graphics ltx_centering ltx_img_square" width="137" height="163" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F26.3.1.1" class="ltx_text" style="font-size:90%;">Figure 26</span>: </span><span id="S4.F26.4.2" class="ltx_text" style="font-size:90%;">Ge <span id="S4.F26.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib185" title="" class="ltx_ref">185</a>]</cite> proposed a knowledge distillation framework for few-shot face recognition in the wild. The authors exploited consistency regularization among the output of the teacher model for the high-quality input and output of the student model for synthetic low-quality face images. Therefore, synthetic data can be used to enforce consistency regularization for improved performance of the human analysis model in few-shot learning scenarios.</span></figcaption>
</figure>
</section>
<section id="S4.SS8.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.8.3 </span>Few-shot learning</h4>

<div id="S4.SS8.SSS3.p1" class="ltx_para">
<p id="S4.SS8.SSS3.p1.2" class="ltx_p">Few-shot learning is characterized by learning with a limited number of samples. Specifically, in order to compensate for limited availability of data and promote the <span id="S4.SS8.SSS3.p1.2.1" class="ltx_text ltx_font_italic">learning to learn</span> paradigm, augmentation strategies simulate challenging real-world scenarios and ensure consistency in prediction for real and augmented input sample. Ge <span id="S4.SS8.SSS3.p1.2.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib185" title="" class="ltx_ref">185</a>]</cite> proposed in this context a knowledge distillation framework to improve face recognition performance under limited data and low resolution constraints. The face recognition model was trained on high-resolution face images, serving as teacher network. The authors then synthetically generated low-resolution face images and trained the student model such that the output of the student model on the synthetic low-resolution face was close to the output of the teacher model on the real high-quality face image (see Figure <a href="#S4.F26" title="Figure 26 ‣ 4.8.2 Self-supervision ‣ 4.8 Enforcing consistency regularization ‣ 4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">26</span></a>). The associated performance of face verification on the UMDFace dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib186" title="" class="ltx_ref">186</a>]</cite> improved from <math id="S4.SS8.SSS3.p1.1.m1.1" class="ltx_Math" alttext="67.59\%" display="inline"><semantics id="S4.SS8.SSS3.p1.1.m1.1a"><mrow id="S4.SS8.SSS3.p1.1.m1.1.1" xref="S4.SS8.SSS3.p1.1.m1.1.1.cmml"><mn id="S4.SS8.SSS3.p1.1.m1.1.1.2" xref="S4.SS8.SSS3.p1.1.m1.1.1.2.cmml">67.59</mn><mo id="S4.SS8.SSS3.p1.1.m1.1.1.1" xref="S4.SS8.SSS3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS8.SSS3.p1.1.m1.1b"><apply id="S4.SS8.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS8.SSS3.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS8.SSS3.p1.1.m1.1.1.1.cmml" xref="S4.SS8.SSS3.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS8.SSS3.p1.1.m1.1.1.2.cmml" xref="S4.SS8.SSS3.p1.1.m1.1.1.2">67.59</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS8.SSS3.p1.1.m1.1c">67.59\%</annotation></semantics></math> to <math id="S4.SS8.SSS3.p1.2.m2.1" class="ltx_Math" alttext="73.58\%" display="inline"><semantics id="S4.SS8.SSS3.p1.2.m2.1a"><mrow id="S4.SS8.SSS3.p1.2.m2.1.1" xref="S4.SS8.SSS3.p1.2.m2.1.1.cmml"><mn id="S4.SS8.SSS3.p1.2.m2.1.1.2" xref="S4.SS8.SSS3.p1.2.m2.1.1.2.cmml">73.58</mn><mo id="S4.SS8.SSS3.p1.2.m2.1.1.1" xref="S4.SS8.SSS3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS8.SSS3.p1.2.m2.1b"><apply id="S4.SS8.SSS3.p1.2.m2.1.1.cmml" xref="S4.SS8.SSS3.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS8.SSS3.p1.2.m2.1.1.1.cmml" xref="S4.SS8.SSS3.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS8.SSS3.p1.2.m2.1.1.2.cmml" xref="S4.SS8.SSS3.p1.2.m2.1.1.2">73.58</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS8.SSS3.p1.2.m2.1c">73.58\%</annotation></semantics></math> after knowledge distillation compared to training the student model directly on synthetic faces. Thus, consistency regularization between real and synthetic data improved the face recognition performance with few-shot learning. Other studies utilizing synthetic data for few-shot learning in human analysis include applications in attribute-based person search <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib187" title="" class="ltx_ref">187</a>]</cite>, deepfake detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib188" title="" class="ltx_ref">188</a>]</cite>, login authentication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib189" title="" class="ltx_ref">189</a>]</cite>, signature verification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib190" title="" class="ltx_ref">190</a>]</cite>, speaker recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> and gaze estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>.</p>
</div>
<figure id="S4.F27" class="ltx_figure"><img src="/html/2208.09191/assets/x23.png" id="S4.F27.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="76" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F27.3.1.1" class="ltx_text" style="font-size:90%;">Figure 27</span>: </span><span id="S4.F27.4.2" class="ltx_text" style="font-size:90%;">Georgopoulos <span id="S4.F27.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib191" title="" class="ltx_ref">191</a>]</cite> improved intra-class diversity in the training set by transferring demographic attributes (left to right): age, skin tone and gender. The authors demonstrated that training with diverse synthetic samples of the same subject is instrumental in mitigating demographic bias observed in face recognition models.</span></figcaption>
</figure>
<figure id="S4.F28" class="ltx_figure"><img src="/html/2208.09191/assets/x24.png" id="S4.F28.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="172" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F28.4.1.1" class="ltx_text" style="font-size:90%;">Figure 28</span>: </span><span id="S4.F28.5.2" class="ltx_text" style="font-size:90%;">Niinuma <span id="S4.F28.5.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> generated synthetic faces, in order to create a training set that is balanced <span id="S4.F28.5.2.2" class="ltx_text ltx_font_italic">w.r.t.</span> action unit (AU) intensity labels. The authors demonstrated that training on the balanced training set improved detection of facial actions.</span></figcaption>
</figure>
</section>
</section>
<section id="S4.SS9" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.9 </span><span id="S4.SS9.1.1" class="ltx_text ltx_font_italic">Mitigating dataset bias and ensuring fairness</span>
</h3>

<div id="S4.SS9.p1" class="ltx_para">
<p id="S4.SS9.p1.2" class="ltx_p">Human datasets often contain demographic bias <span id="S4.SS9.p1.2.1" class="ltx_text ltx_font_italic">w.r.t.</span> attributes such as ethnicity, gender, or age <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib192" title="" class="ltx_ref">192</a>]</cite>. In addition, collected datasets might be biased to a certain group of labels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>. Synthetic data is able to balance and unbias datasets beneficial in training and designing fair and unbiased human analysis models. Georgopoulos <span id="S4.SS9.p1.2.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib191" title="" class="ltx_ref">191</a>]</cite> exploited an attribute-transfer based approach to balance underrepresented demographic groups in training datasets. Attributes such as skin tone, gender, and age were transferred into given training samples (see Figure <a href="#S4.F27" title="Figure 27 ‣ 4.8.3 Few-shot learning ‣ 4.8 Enforcing consistency regularization ‣ 4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">27</span></a>) towards creation of an unbiased training dataset. In the related study the accuracy of face recognition on dark-skinned women over <math id="S4.SS9.p1.1.m1.1" class="ltx_Math" alttext="60" display="inline"><semantics id="S4.SS9.p1.1.m1.1a"><mn id="S4.SS9.p1.1.m1.1.1" xref="S4.SS9.p1.1.m1.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S4.SS9.p1.1.m1.1b"><cn type="integer" id="S4.SS9.p1.1.m1.1.1.cmml" xref="S4.SS9.p1.1.m1.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS9.p1.1.m1.1c">60</annotation></semantics></math> years old characterized by true positive rate (TPR) improved by <math id="S4.SS9.p1.2.m2.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S4.SS9.p1.2.m2.1a"><mrow id="S4.SS9.p1.2.m2.1.1" xref="S4.SS9.p1.2.m2.1.1.cmml"><mn id="S4.SS9.p1.2.m2.1.1.2" xref="S4.SS9.p1.2.m2.1.1.2.cmml">20</mn><mo id="S4.SS9.p1.2.m2.1.1.1" xref="S4.SS9.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS9.p1.2.m2.1b"><apply id="S4.SS9.p1.2.m2.1.1.cmml" xref="S4.SS9.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS9.p1.2.m2.1.1.1.cmml" xref="S4.SS9.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS9.p1.2.m2.1.1.2.cmml" xref="S4.SS9.p1.2.m2.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS9.p1.2.m2.1c">20\%</annotation></semantics></math> on the UNCW dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib193" title="" class="ltx_ref">193</a>]</cite> after training on the training set augmented with synthetic faces.</p>
</div>
<div id="S4.SS9.p2" class="ltx_para">
<p id="S4.SS9.p2.2" class="ltx_p">Similarly, Niinuma <span id="S4.SS9.p2.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> discussed that real datasets employed for facial action detection are not balanced <span id="S4.SS9.p2.2.2" class="ltx_text ltx_font_italic">w.r.t.</span> action unit (AU) intensity labels. To address this limitation, the authors generates a balanced training set using GANimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib194" title="" class="ltx_ref">194</a>]</cite> (see Figure <a href="#S4.F28" title="Figure 28 ‣ 4.8.3 Few-shot learning ‣ 4.8 Enforcing consistency regularization ‣ 4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">28</span></a>). The generated balanced training dataset was used to train the facial action detector, with the related inter-rater reliability score of AU intensity level estimation improving from <math id="S4.SS9.p2.1.m1.1" class="ltx_Math" alttext="48.90\%" display="inline"><semantics id="S4.SS9.p2.1.m1.1a"><mrow id="S4.SS9.p2.1.m1.1.1" xref="S4.SS9.p2.1.m1.1.1.cmml"><mn id="S4.SS9.p2.1.m1.1.1.2" xref="S4.SS9.p2.1.m1.1.1.2.cmml">48.90</mn><mo id="S4.SS9.p2.1.m1.1.1.1" xref="S4.SS9.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS9.p2.1.m1.1b"><apply id="S4.SS9.p2.1.m1.1.1.cmml" xref="S4.SS9.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS9.p2.1.m1.1.1.1.cmml" xref="S4.SS9.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS9.p2.1.m1.1.1.2.cmml" xref="S4.SS9.p2.1.m1.1.1.2">48.90</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS9.p2.1.m1.1c">48.90\%</annotation></semantics></math> to <math id="S4.SS9.p2.2.m2.1" class="ltx_Math" alttext="52.50\%" display="inline"><semantics id="S4.SS9.p2.2.m2.1a"><mrow id="S4.SS9.p2.2.m2.1.1" xref="S4.SS9.p2.2.m2.1.1.cmml"><mn id="S4.SS9.p2.2.m2.1.1.2" xref="S4.SS9.p2.2.m2.1.1.2.cmml">52.50</mn><mo id="S4.SS9.p2.2.m2.1.1.1" xref="S4.SS9.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS9.p2.2.m2.1b"><apply id="S4.SS9.p2.2.m2.1.1.cmml" xref="S4.SS9.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS9.p2.2.m2.1.1.1.cmml" xref="S4.SS9.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS9.p2.2.m2.1.1.2.cmml" xref="S4.SS9.p2.2.m2.1.1.2">52.50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS9.p2.2.m2.1c">52.50\%</annotation></semantics></math> after training the model on synthetic data, as opposed to training on real data. Several other studies in face recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib195" title="" class="ltx_ref">195</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib196" title="" class="ltx_ref">196</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib197" title="" class="ltx_ref">197</a>]</cite> confirmed the ability of synthetic data to train unbiased and fair models.</p>
</div>
<figure id="S4.F29" class="ltx_figure"><img src="/html/2208.09191/assets/x25.png" id="S4.F29.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="186" height="118" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F29.10.4.1" class="ltx_text" style="font-size:90%;">Figure 29</span>: </span><span id="S4.F29.6.3" class="ltx_text" style="font-size:90%;">DeepMasterPrints <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib198" title="" class="ltx_ref">198</a>]</cite> by Bontrager <span id="S4.F29.6.3.1" class="ltx_text ltx_font_italic">et al.</span> constitutes a synthetic fingerprint <span id="S4.F29.6.3.2" class="ltx_text ltx_font_italic">masterprint</span> aimed at presentation attacks on fingerprint recognition systems. The top and bottom illustrate the masterprints for the rolled fingerprints and fingerprints acquired using a capacitive capture device. The first, second and third columns represent the masterprint to achieve a false match rate (FMR) of <math id="S4.F29.4.1.m1.1" class="ltx_Math" alttext="0.01\%" display="inline"><semantics id="S4.F29.4.1.m1.1b"><mrow id="S4.F29.4.1.m1.1.1" xref="S4.F29.4.1.m1.1.1.cmml"><mn id="S4.F29.4.1.m1.1.1.2" xref="S4.F29.4.1.m1.1.1.2.cmml">0.01</mn><mo id="S4.F29.4.1.m1.1.1.1" xref="S4.F29.4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F29.4.1.m1.1c"><apply id="S4.F29.4.1.m1.1.1.cmml" xref="S4.F29.4.1.m1.1.1"><csymbol cd="latexml" id="S4.F29.4.1.m1.1.1.1.cmml" xref="S4.F29.4.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.F29.4.1.m1.1.1.2.cmml" xref="S4.F29.4.1.m1.1.1.2">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F29.4.1.m1.1d">0.01\%</annotation></semantics></math>, <math id="S4.F29.5.2.m2.1" class="ltx_Math" alttext="0.1\%" display="inline"><semantics id="S4.F29.5.2.m2.1b"><mrow id="S4.F29.5.2.m2.1.1" xref="S4.F29.5.2.m2.1.1.cmml"><mn id="S4.F29.5.2.m2.1.1.2" xref="S4.F29.5.2.m2.1.1.2.cmml">0.1</mn><mo id="S4.F29.5.2.m2.1.1.1" xref="S4.F29.5.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F29.5.2.m2.1c"><apply id="S4.F29.5.2.m2.1.1.cmml" xref="S4.F29.5.2.m2.1.1"><csymbol cd="latexml" id="S4.F29.5.2.m2.1.1.1.cmml" xref="S4.F29.5.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.F29.5.2.m2.1.1.2.cmml" xref="S4.F29.5.2.m2.1.1.2">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F29.5.2.m2.1d">0.1\%</annotation></semantics></math>, and <math id="S4.F29.6.3.m3.1" class="ltx_Math" alttext="1\%" display="inline"><semantics id="S4.F29.6.3.m3.1b"><mrow id="S4.F29.6.3.m3.1.1" xref="S4.F29.6.3.m3.1.1.cmml"><mn id="S4.F29.6.3.m3.1.1.2" xref="S4.F29.6.3.m3.1.1.2.cmml">1</mn><mo id="S4.F29.6.3.m3.1.1.1" xref="S4.F29.6.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F29.6.3.m3.1c"><apply id="S4.F29.6.3.m3.1.1.cmml" xref="S4.F29.6.3.m3.1.1"><csymbol cd="latexml" id="S4.F29.6.3.m3.1.1.1.cmml" xref="S4.F29.6.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S4.F29.6.3.m3.1.1.2.cmml" xref="S4.F29.6.3.m3.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F29.6.3.m3.1d">1\%</annotation></semantics></math>, respectively.</span></figcaption>
</figure>
</section>
<section id="S4.SS10" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.10 </span><span id="S4.SS10.1.1" class="ltx_text ltx_font_italic">Inducing digital perturbation attacks</span>
</h3>

<div id="S4.SS10.p1" class="ltx_para">
<p id="S4.SS10.p1.3" class="ltx_p">Synthetic data is particularly instrumental in creating novel attacks on biometric systems.
One prominent study in this context constitutes DeepMasterPrints by Bontrager <span id="S4.SS10.p1.3.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib198" title="" class="ltx_ref">198</a>]</cite>, which aimed to generate one masterprint, namely a synthetic fingerprint that was designed to impersonate a set of fingerprints and falsely match with a large number of non-mated enrollees in the enrolment database (see Figure <a href="#S4.F29" title="Figure 29 ‣ 4.9 Mitigating dataset bias and ensuring fairness ‣ 4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">29</span></a>). This presentation attack for fingerprint recognition systems employed GAN, where the latent input variables in the generator network were obtained using a covariance matrix adaptation evolution technique. The associated false match rate (FMR) of <math id="S4.SS10.p1.1.m1.1" class="ltx_Math" alttext="0.1\%" display="inline"><semantics id="S4.SS10.p1.1.m1.1a"><mrow id="S4.SS10.p1.1.m1.1.1" xref="S4.SS10.p1.1.m1.1.1.cmml"><mn id="S4.SS10.p1.1.m1.1.1.2" xref="S4.SS10.p1.1.m1.1.1.2.cmml">0.1</mn><mo id="S4.SS10.p1.1.m1.1.1.1" xref="S4.SS10.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS10.p1.1.m1.1b"><apply id="S4.SS10.p1.1.m1.1.1.cmml" xref="S4.SS10.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS10.p1.1.m1.1.1.1.cmml" xref="S4.SS10.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS10.p1.1.m1.1.1.2.cmml" xref="S4.SS10.p1.1.m1.1.1.2">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS10.p1.1.m1.1c">0.1\%</annotation></semantics></math> increased via DeepMasterPrints to <math id="S4.SS10.p1.2.m2.1" class="ltx_Math" alttext="8.61\%" display="inline"><semantics id="S4.SS10.p1.2.m2.1a"><mrow id="S4.SS10.p1.2.m2.1.1" xref="S4.SS10.p1.2.m2.1.1.cmml"><mn id="S4.SS10.p1.2.m2.1.1.2" xref="S4.SS10.p1.2.m2.1.1.2.cmml">8.61</mn><mo id="S4.SS10.p1.2.m2.1.1.1" xref="S4.SS10.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS10.p1.2.m2.1b"><apply id="S4.SS10.p1.2.m2.1.1.cmml" xref="S4.SS10.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS10.p1.2.m2.1.1.1.cmml" xref="S4.SS10.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS10.p1.2.m2.1.1.2.cmml" xref="S4.SS10.p1.2.m2.1.1.2">8.61</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS10.p1.2.m2.1c">8.61\%</annotation></semantics></math> on the NIST 9 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib139" title="" class="ltx_ref">139</a>]</cite>, as well as to <math id="S4.SS10.p1.3.m3.1" class="ltx_Math" alttext="22.50\%" display="inline"><semantics id="S4.SS10.p1.3.m3.1a"><mrow id="S4.SS10.p1.3.m3.1.1" xref="S4.SS10.p1.3.m3.1.1.cmml"><mn id="S4.SS10.p1.3.m3.1.1.2" xref="S4.SS10.p1.3.m3.1.1.2.cmml">22.50</mn><mo id="S4.SS10.p1.3.m3.1.1.1" xref="S4.SS10.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS10.p1.3.m3.1b"><apply id="S4.SS10.p1.3.m3.1.1.cmml" xref="S4.SS10.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS10.p1.3.m3.1.1.1.cmml" xref="S4.SS10.p1.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS10.p1.3.m3.1.1.2.cmml" xref="S4.SS10.p1.3.m3.1.1.2">22.50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS10.p1.3.m3.1c">22.50\%</annotation></semantics></math> on the FingerPass DB7 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib199" title="" class="ltx_ref">199</a>]</cite>. Additional attacks facilitated by synthetic data include those in iris recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib200" title="" class="ltx_ref">200</a>, <a href="#bib.bib201" title="" class="ltx_ref">201</a>]</cite>, face recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib202" title="" class="ltx_ref">202</a>, <a href="#bib.bib203" title="" class="ltx_ref">203</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib204" title="" class="ltx_ref">204</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib205" title="" class="ltx_ref">205</a>]</cite> and fingerprint recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib206" title="" class="ltx_ref">206</a>]</cite>.</p>
</div>
<figure id="S4.F30" class="ltx_figure"><img src="/html/2208.09191/assets/x26.png" id="S4.F30.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F30.2.1.1" class="ltx_text" style="font-size:90%;">Figure 30</span>: </span><span id="S4.F30.3.2" class="ltx_text" style="font-size:90%;">Synthetic videos called Deepfakes with varying attributes, can be generated with th induce an attack to ruin the public perception of an individual <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite>.</span></figcaption>
</figure>
<div id="S4.SS10.p2" class="ltx_para">
<p id="S4.SS10.p2.1" class="ltx_p">A related direction has to do with digital human creation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib207" title="" class="ltx_ref">207</a>, <a href="#bib.bib208" title="" class="ltx_ref">208</a>, <a href="#bib.bib209" title="" class="ltx_ref">209</a>, <a href="#bib.bib210" title="" class="ltx_ref">210</a>, <a href="#bib.bib211" title="" class="ltx_ref">211</a>]</cite>, as well as with manipulation of human faces <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib212" title="" class="ltx_ref">212</a>, <a href="#bib.bib213" title="" class="ltx_ref">213</a>, <a href="#bib.bib214" title="" class="ltx_ref">214</a>]</cite>. Specifically,
a face image of a target individual being superimposed on a video of a source individual has been widely accepted and referred to as <span id="S4.SS10.p2.1.1" class="ltx_text ltx_font_italic">deepfake</span> (see Figure <a href="#S4.F30" title="Figure 30 ‣ 4.10 Inducing digital perturbation attacks ‣ 4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">30</span></a>). Deepfakes entail several challenges and threats, given that (a) such manipulations can fabricate
animations of subjects involved in actions that have not taken place and (b) such
manipulated data can be circumvented nowadays rapidly via social media. Deepfakes are considered in human analysis as digital perturbation attacks, attracting large interest by their own right, with overview articles focusing on deepfake creation and detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>, <a href="#bib.bib215" title="" class="ltx_ref">215</a>, <a href="#bib.bib216" title="" class="ltx_ref">216</a>]</cite>, as well as adversarial attacks and defences in images, graphs, and text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib217" title="" class="ltx_ref">217</a>]</cite>. We note that similarly morphing attacks can be introduced using synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib218" title="" class="ltx_ref">218</a>]</cite>. A morphing attack is characterized by a synthetic image for which the authentication system is compelled to match with two contributing subjects instead of one. A morphed image is usually generated by aligning and blending images of two different contributors. For a comprehensive survey on published morphing attacks and associated detection methods, we refer to
related overview articles <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib219" title="" class="ltx_ref">219</a>, <a href="#bib.bib220" title="" class="ltx_ref">220</a>, <a href="#bib.bib221" title="" class="ltx_ref">221</a>]</cite>.</p>
</div>
<figure id="S4.F31" class="ltx_figure"><img src="/html/2208.09191/assets/x27.png" id="S4.F31.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="323" height="134" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F31.3.1.1" class="ltx_text" style="font-size:90%;">Figure 31</span>: </span><span id="S4.F31.4.2" class="ltx_text" style="font-size:90%;">Yadav <span id="S4.F31.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib222" title="" class="ltx_ref">222</a>]</cite> proposed RaSGAN which was (a) trained to generate synthetic iris images. The discriminator network was trained to classify a given input iris sample as real or synthetic. (b) After training, the discriminator was exploited as a presentation attack detector to distinguish between bona fide and presentation attack iris samples. Thus, generative modelling to synthesize synthetic data benefits in improving model generalization on new and unseen test samples.</span></figcaption>
</figure>
</section>
<section id="S4.SS11" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.11 </span><span id="S4.SS11.1.1" class="ltx_text ltx_font_italic">Learning by synthesis</span>
</h3>

<div id="S4.SS11.p1" class="ltx_para">
<p id="S4.SS11.p1.6" class="ltx_p">A machine learning model can be categorized as a discriminative or generative model. The former learns a conditional distribution <math id="S4.SS11.p1.1.m1.3" class="ltx_Math" alttext="p(y|x;\theta)" display="inline"><semantics id="S4.SS11.p1.1.m1.3a"><mrow id="S4.SS11.p1.1.m1.3.3" xref="S4.SS11.p1.1.m1.3.3.cmml"><mi id="S4.SS11.p1.1.m1.3.3.3" xref="S4.SS11.p1.1.m1.3.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS11.p1.1.m1.3.3.2" xref="S4.SS11.p1.1.m1.3.3.2.cmml">​</mo><mrow id="S4.SS11.p1.1.m1.3.3.1.1" xref="S4.SS11.p1.1.m1.3.3.1.1.1.cmml"><mo stretchy="false" id="S4.SS11.p1.1.m1.3.3.1.1.2" xref="S4.SS11.p1.1.m1.3.3.1.1.1.cmml">(</mo><mrow id="S4.SS11.p1.1.m1.3.3.1.1.1" xref="S4.SS11.p1.1.m1.3.3.1.1.1.cmml"><mi id="S4.SS11.p1.1.m1.3.3.1.1.1.2" xref="S4.SS11.p1.1.m1.3.3.1.1.1.2.cmml">y</mi><mo fence="false" id="S4.SS11.p1.1.m1.3.3.1.1.1.1" xref="S4.SS11.p1.1.m1.3.3.1.1.1.1.cmml">|</mo><mrow id="S4.SS11.p1.1.m1.3.3.1.1.1.3.2" xref="S4.SS11.p1.1.m1.3.3.1.1.1.3.1.cmml"><mi id="S4.SS11.p1.1.m1.1.1" xref="S4.SS11.p1.1.m1.1.1.cmml">x</mi><mo id="S4.SS11.p1.1.m1.3.3.1.1.1.3.2.1" xref="S4.SS11.p1.1.m1.3.3.1.1.1.3.1.cmml">;</mo><mi id="S4.SS11.p1.1.m1.2.2" xref="S4.SS11.p1.1.m1.2.2.cmml">θ</mi></mrow></mrow><mo stretchy="false" id="S4.SS11.p1.1.m1.3.3.1.1.3" xref="S4.SS11.p1.1.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS11.p1.1.m1.3b"><apply id="S4.SS11.p1.1.m1.3.3.cmml" xref="S4.SS11.p1.1.m1.3.3"><times id="S4.SS11.p1.1.m1.3.3.2.cmml" xref="S4.SS11.p1.1.m1.3.3.2"></times><ci id="S4.SS11.p1.1.m1.3.3.3.cmml" xref="S4.SS11.p1.1.m1.3.3.3">𝑝</ci><apply id="S4.SS11.p1.1.m1.3.3.1.1.1.cmml" xref="S4.SS11.p1.1.m1.3.3.1.1"><csymbol cd="latexml" id="S4.SS11.p1.1.m1.3.3.1.1.1.1.cmml" xref="S4.SS11.p1.1.m1.3.3.1.1.1.1">conditional</csymbol><ci id="S4.SS11.p1.1.m1.3.3.1.1.1.2.cmml" xref="S4.SS11.p1.1.m1.3.3.1.1.1.2">𝑦</ci><list id="S4.SS11.p1.1.m1.3.3.1.1.1.3.1.cmml" xref="S4.SS11.p1.1.m1.3.3.1.1.1.3.2"><ci id="S4.SS11.p1.1.m1.1.1.cmml" xref="S4.SS11.p1.1.m1.1.1">𝑥</ci><ci id="S4.SS11.p1.1.m1.2.2.cmml" xref="S4.SS11.p1.1.m1.2.2">𝜃</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS11.p1.1.m1.3c">p(y|x;\theta)</annotation></semantics></math>, where <math id="S4.SS11.p1.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS11.p1.2.m2.1a"><mi id="S4.SS11.p1.2.m2.1.1" xref="S4.SS11.p1.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS11.p1.2.m2.1b"><ci id="S4.SS11.p1.2.m2.1.1.cmml" xref="S4.SS11.p1.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS11.p1.2.m2.1c">y</annotation></semantics></math> denotes the output <math id="S4.SS11.p1.3.m3.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS11.p1.3.m3.1a"><mi id="S4.SS11.p1.3.m3.1.1" xref="S4.SS11.p1.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS11.p1.3.m3.1b"><ci id="S4.SS11.p1.3.m3.1.1.cmml" xref="S4.SS11.p1.3.m3.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS11.p1.3.m3.1c">y</annotation></semantics></math> for the input sample <math id="S4.SS11.p1.4.m4.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS11.p1.4.m4.1a"><mi id="S4.SS11.p1.4.m4.1.1" xref="S4.SS11.p1.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS11.p1.4.m4.1b"><ci id="S4.SS11.p1.4.m4.1.1.cmml" xref="S4.SS11.p1.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS11.p1.4.m4.1c">x</annotation></semantics></math> and <math id="S4.SS11.p1.5.m5.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S4.SS11.p1.5.m5.1a"><mi id="S4.SS11.p1.5.m5.1.1" xref="S4.SS11.p1.5.m5.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S4.SS11.p1.5.m5.1b"><ci id="S4.SS11.p1.5.m5.1.1.cmml" xref="S4.SS11.p1.5.m5.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS11.p1.5.m5.1c">\theta</annotation></semantics></math> signifies model parameters. A generative model learns the joint distribution <math id="S4.SS11.p1.6.m6.2" class="ltx_Math" alttext="p(x,y)" display="inline"><semantics id="S4.SS11.p1.6.m6.2a"><mrow id="S4.SS11.p1.6.m6.2.3" xref="S4.SS11.p1.6.m6.2.3.cmml"><mi id="S4.SS11.p1.6.m6.2.3.2" xref="S4.SS11.p1.6.m6.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS11.p1.6.m6.2.3.1" xref="S4.SS11.p1.6.m6.2.3.1.cmml">​</mo><mrow id="S4.SS11.p1.6.m6.2.3.3.2" xref="S4.SS11.p1.6.m6.2.3.3.1.cmml"><mo stretchy="false" id="S4.SS11.p1.6.m6.2.3.3.2.1" xref="S4.SS11.p1.6.m6.2.3.3.1.cmml">(</mo><mi id="S4.SS11.p1.6.m6.1.1" xref="S4.SS11.p1.6.m6.1.1.cmml">x</mi><mo id="S4.SS11.p1.6.m6.2.3.3.2.2" xref="S4.SS11.p1.6.m6.2.3.3.1.cmml">,</mo><mi id="S4.SS11.p1.6.m6.2.2" xref="S4.SS11.p1.6.m6.2.2.cmml">y</mi><mo stretchy="false" id="S4.SS11.p1.6.m6.2.3.3.2.3" xref="S4.SS11.p1.6.m6.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS11.p1.6.m6.2b"><apply id="S4.SS11.p1.6.m6.2.3.cmml" xref="S4.SS11.p1.6.m6.2.3"><times id="S4.SS11.p1.6.m6.2.3.1.cmml" xref="S4.SS11.p1.6.m6.2.3.1"></times><ci id="S4.SS11.p1.6.m6.2.3.2.cmml" xref="S4.SS11.p1.6.m6.2.3.2">𝑝</ci><interval closure="open" id="S4.SS11.p1.6.m6.2.3.3.1.cmml" xref="S4.SS11.p1.6.m6.2.3.3.2"><ci id="S4.SS11.p1.6.m6.1.1.cmml" xref="S4.SS11.p1.6.m6.1.1">𝑥</ci><ci id="S4.SS11.p1.6.m6.2.2.cmml" xref="S4.SS11.p1.6.m6.2.2">𝑦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS11.p1.6.m6.2c">p(x,y)</annotation></semantics></math> and hence learns the distribution of data by learning to generate synthetic data. Such model is able to generalize on new and unseen test examples. A related seminal work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib223" title="" class="ltx_ref">223</a>]</cite> presented a hierarchical generative model, which jointly synthesizes eye images in a top-down approach, while estimating eye gaze in a bottom-to-up approach. A further generative modelling-based approach includes relativistic average standard generative adversarial network (RaSGAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib222" title="" class="ltx_ref">222</a>]</cite> by Yadav <span id="S4.SS11.p1.6.1" class="ltx_text ltx_font_italic">et al.</span>. RaSGAN was trained to generate synthetic iris images, demonstrating the ability of its discriminator to generalize better on new and unseen presentation attacks (see Figure <a href="#S4.F31" title="Figure 31 ‣ 4.10 Inducing digital perturbation attacks ‣ 4 How can synthetic data be utilized? ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">31</span></a>). Several approaches learning to synthesize data for improved model performance were proposed for re-identification of individuals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib224" title="" class="ltx_ref">224</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite> and face recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib225" title="" class="ltx_ref">225</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib226" title="" class="ltx_ref">226</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Open-Source Availability</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This Section provides an overview of synthetic datasets and synthetic data generation tools available for public usage. We emphasize the importance of sharing datasets and tools within the research community for improved reproducibility of results. That is, Table <a href="#S5.T2" title="TABLE II ‣ 5 Open-Source Availability ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> presents publicly-available datasets comprised of synthetic data only. Further, Table <a href="#S5.T3" title="TABLE III ‣ 5 Open-Source Availability ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> introduces synthetic data generation tools to enable new researchers in the field of human analysis to build custom-generated datasets tailored to their needs.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.10.1.1" class="ltx_text" style="font-size:90%;">TABLE II</span>: </span><span id="S5.T2.11.2" class="ltx_text" style="font-size:90%;">Publicly available synthetic datasets. </span></figcaption>
<div id="S5.T2.8.8" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:91.7pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-168.9pt,35.5pt) scale(0.562110176618625,0.562110176618625) ;">
<table id="S5.T2.8.8.8" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.8.8.8.9.1" class="ltx_tr">
<th id="S5.T2.8.8.8.9.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T2.8.8.8.9.1.1.1" class="ltx_text ltx_font_bold">Reference</span></th>
<th id="S5.T2.8.8.8.9.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.8.8.8.9.1.2.1" class="ltx_text ltx_font_bold">Name</span></th>
<th id="S5.T2.8.8.8.9.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.8.8.8.9.1.3.1" class="ltx_text ltx_font_bold">Application</span></th>
<th id="S5.T2.8.8.8.9.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.8.8.8.9.1.4.1" class="ltx_text ltx_font_bold">Year</span></th>
<th id="S5.T2.8.8.8.9.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.8.8.8.9.1.5.1" class="ltx_text ltx_font_bold">Data Type</span></th>
<th id="S5.T2.8.8.8.9.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.8.8.8.9.1.6.1" class="ltx_text ltx_font_bold">Dataset Size</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.1.1" class="ltx_tr">
<td id="S5.T2.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt">Wood <span id="S5.T2.1.1.1.1.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S5.T2.1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Microsoft Face Synthetics</td>
<td id="S5.T2.1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Landmark localization, Face parsing</td>
<td id="S5.T2.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">2021</td>
<td id="S5.T2.1.1.1.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Images</td>
<td id="S5.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><math id="S5.T2.1.1.1.1.1.m1.2" class="ltx_Math" alttext="100,000" display="inline"><semantics id="S5.T2.1.1.1.1.1.m1.2a"><mrow id="S5.T2.1.1.1.1.1.m1.2.3.2" xref="S5.T2.1.1.1.1.1.m1.2.3.1.cmml"><mn id="S5.T2.1.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.1.m1.1.1.cmml">100</mn><mo id="S5.T2.1.1.1.1.1.m1.2.3.2.1" xref="S5.T2.1.1.1.1.1.m1.2.3.1.cmml">,</mo><mn id="S5.T2.1.1.1.1.1.m1.2.2" xref="S5.T2.1.1.1.1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.1.m1.2b"><list id="S5.T2.1.1.1.1.1.m1.2.3.1.cmml" xref="S5.T2.1.1.1.1.1.m1.2.3.2"><cn type="integer" id="S5.T2.1.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.1.m1.1.1">100</cn><cn type="integer" id="S5.T2.1.1.1.1.1.m1.2.2.cmml" xref="S5.T2.1.1.1.1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.1.m1.2c">100,000</annotation></semantics></math></td>
</tr>
<tr id="S5.T2.2.2.2.2" class="ltx_tr">
<td id="S5.T2.2.2.2.2.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Varol <span id="S5.T2.2.2.2.2.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</td>
<td id="S5.T2.2.2.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">SURREAL</td>
<td id="S5.T2.2.2.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Human Pose Estimation</td>
<td id="S5.T2.2.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2017</td>
<td id="S5.T2.2.2.2.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Video Frames</td>
<td id="S5.T2.2.2.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S5.T2.2.2.2.2.1.m1.3" class="ltx_Math" alttext="6,000,000" display="inline"><semantics id="S5.T2.2.2.2.2.1.m1.3a"><mrow id="S5.T2.2.2.2.2.1.m1.3.4.2" xref="S5.T2.2.2.2.2.1.m1.3.4.1.cmml"><mn id="S5.T2.2.2.2.2.1.m1.1.1" xref="S5.T2.2.2.2.2.1.m1.1.1.cmml">6</mn><mo id="S5.T2.2.2.2.2.1.m1.3.4.2.1" xref="S5.T2.2.2.2.2.1.m1.3.4.1.cmml">,</mo><mn id="S5.T2.2.2.2.2.1.m1.2.2" xref="S5.T2.2.2.2.2.1.m1.2.2.cmml">000</mn><mo id="S5.T2.2.2.2.2.1.m1.3.4.2.2" xref="S5.T2.2.2.2.2.1.m1.3.4.1.cmml">,</mo><mn id="S5.T2.2.2.2.2.1.m1.3.3" xref="S5.T2.2.2.2.2.1.m1.3.3.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.2.1.m1.3b"><list id="S5.T2.2.2.2.2.1.m1.3.4.1.cmml" xref="S5.T2.2.2.2.2.1.m1.3.4.2"><cn type="integer" id="S5.T2.2.2.2.2.1.m1.1.1.cmml" xref="S5.T2.2.2.2.2.1.m1.1.1">6</cn><cn type="integer" id="S5.T2.2.2.2.2.1.m1.2.2.cmml" xref="S5.T2.2.2.2.2.1.m1.2.2">000</cn><cn type="integer" id="S5.T2.2.2.2.2.1.m1.3.3.cmml" xref="S5.T2.2.2.2.2.1.m1.3.3">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.2.1.m1.3c">6,000,000</annotation></semantics></math></td>
</tr>
<tr id="S5.T2.3.3.3.3" class="ltx_tr">
<td id="S5.T2.3.3.3.3.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Barbosa <span id="S5.T2.3.3.3.3.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>]</cite>
</td>
<td id="S5.T2.3.3.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">SOMASet</td>
<td id="S5.T2.3.3.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Person re-identification</td>
<td id="S5.T2.3.3.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2017</td>
<td id="S5.T2.3.3.3.3.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Images</td>
<td id="S5.T2.3.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S5.T2.3.3.3.3.1.m1.2" class="ltx_Math" alttext="100,000" display="inline"><semantics id="S5.T2.3.3.3.3.1.m1.2a"><mrow id="S5.T2.3.3.3.3.1.m1.2.3.2" xref="S5.T2.3.3.3.3.1.m1.2.3.1.cmml"><mn id="S5.T2.3.3.3.3.1.m1.1.1" xref="S5.T2.3.3.3.3.1.m1.1.1.cmml">100</mn><mo id="S5.T2.3.3.3.3.1.m1.2.3.2.1" xref="S5.T2.3.3.3.3.1.m1.2.3.1.cmml">,</mo><mn id="S5.T2.3.3.3.3.1.m1.2.2" xref="S5.T2.3.3.3.3.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.3.1.m1.2b"><list id="S5.T2.3.3.3.3.1.m1.2.3.1.cmml" xref="S5.T2.3.3.3.3.1.m1.2.3.2"><cn type="integer" id="S5.T2.3.3.3.3.1.m1.1.1.cmml" xref="S5.T2.3.3.3.3.1.m1.1.1">100</cn><cn type="integer" id="S5.T2.3.3.3.3.1.m1.2.2.cmml" xref="S5.T2.3.3.3.3.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.3.1.m1.2c">100,000</annotation></semantics></math></td>
</tr>
<tr id="S5.T2.4.4.4.4" class="ltx_tr">
<td id="S5.T2.4.4.4.4.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Varol <span id="S5.T2.4.4.4.4.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>
</td>
<td id="S5.T2.4.4.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">SURREACT</td>
<td id="S5.T2.4.4.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Action Recognition</td>
<td id="S5.T2.4.4.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2021</td>
<td id="S5.T2.4.4.4.4.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Videos</td>
<td id="S5.T2.4.4.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S5.T2.4.4.4.4.1.m1.2" class="ltx_Math" alttext="106,000" display="inline"><semantics id="S5.T2.4.4.4.4.1.m1.2a"><mrow id="S5.T2.4.4.4.4.1.m1.2.3.2" xref="S5.T2.4.4.4.4.1.m1.2.3.1.cmml"><mn id="S5.T2.4.4.4.4.1.m1.1.1" xref="S5.T2.4.4.4.4.1.m1.1.1.cmml">106</mn><mo id="S5.T2.4.4.4.4.1.m1.2.3.2.1" xref="S5.T2.4.4.4.4.1.m1.2.3.1.cmml">,</mo><mn id="S5.T2.4.4.4.4.1.m1.2.2" xref="S5.T2.4.4.4.4.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.4.1.m1.2b"><list id="S5.T2.4.4.4.4.1.m1.2.3.1.cmml" xref="S5.T2.4.4.4.4.1.m1.2.3.2"><cn type="integer" id="S5.T2.4.4.4.4.1.m1.1.1.cmml" xref="S5.T2.4.4.4.4.1.m1.1.1">106</cn><cn type="integer" id="S5.T2.4.4.4.4.1.m1.2.2.cmml" xref="S5.T2.4.4.4.4.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.4.1.m1.2c">106,000</annotation></semantics></math></td>
</tr>
<tr id="S5.T2.5.5.5.5" class="ltx_tr">
<td id="S5.T2.5.5.5.5.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Da <span id="S5.T2.5.5.5.5.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>]</cite>
</td>
<td id="S5.T2.5.5.5.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Mixamo Kinetics</td>
<td id="S5.T2.5.5.5.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Action Recognition</td>
<td id="S5.T2.5.5.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2020</td>
<td id="S5.T2.5.5.5.5.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Videos</td>
<td id="S5.T2.5.5.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S5.T2.5.5.5.5.1.m1.2" class="ltx_Math" alttext="36,195" display="inline"><semantics id="S5.T2.5.5.5.5.1.m1.2a"><mrow id="S5.T2.5.5.5.5.1.m1.2.3.2" xref="S5.T2.5.5.5.5.1.m1.2.3.1.cmml"><mn id="S5.T2.5.5.5.5.1.m1.1.1" xref="S5.T2.5.5.5.5.1.m1.1.1.cmml">36</mn><mo id="S5.T2.5.5.5.5.1.m1.2.3.2.1" xref="S5.T2.5.5.5.5.1.m1.2.3.1.cmml">,</mo><mn id="S5.T2.5.5.5.5.1.m1.2.2" xref="S5.T2.5.5.5.5.1.m1.2.2.cmml">195</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.5.5.1.m1.2b"><list id="S5.T2.5.5.5.5.1.m1.2.3.1.cmml" xref="S5.T2.5.5.5.5.1.m1.2.3.2"><cn type="integer" id="S5.T2.5.5.5.5.1.m1.1.1.cmml" xref="S5.T2.5.5.5.5.1.m1.1.1">36</cn><cn type="integer" id="S5.T2.5.5.5.5.1.m1.2.2.cmml" xref="S5.T2.5.5.5.5.1.m1.2.2">195</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.5.5.1.m1.2c">36,195</annotation></semantics></math></td>
</tr>
<tr id="S5.T2.6.6.6.6" class="ltx_tr">
<td id="S5.T2.6.6.6.6.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Ariz <span id="S5.T2.6.6.6.6.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib227" title="" class="ltx_ref">227</a>]</cite>
</td>
<td id="S5.T2.6.6.6.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">UPNA Synthetic Head Pose Database</td>
<td id="S5.T2.6.6.6.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Head Pose Estimation</td>
<td id="S5.T2.6.6.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2016</td>
<td id="S5.T2.6.6.6.6.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Videos</td>
<td id="S5.T2.6.6.6.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><math id="S5.T2.6.6.6.6.1.m1.1" class="ltx_Math" alttext="120" display="inline"><semantics id="S5.T2.6.6.6.6.1.m1.1a"><mn id="S5.T2.6.6.6.6.1.m1.1.1" xref="S5.T2.6.6.6.6.1.m1.1.1.cmml">120</mn><annotation-xml encoding="MathML-Content" id="S5.T2.6.6.6.6.1.m1.1b"><cn type="integer" id="S5.T2.6.6.6.6.1.m1.1.1.cmml" xref="S5.T2.6.6.6.6.1.m1.1.1">120</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.6.6.6.1.m1.1c">120</annotation></semantics></math></td>
</tr>
<tr id="S5.T2.7.7.7.7" class="ltx_tr">
<td id="S5.T2.7.7.7.7.2" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Roitberg<span id="S5.T2.7.7.7.7.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib228" title="" class="ltx_ref">228</a>]</cite>
</td>
<td id="S5.T2.7.7.7.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Sims4Action</td>
<td id="S5.T2.7.7.7.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Action Recognition</td>
<td id="S5.T2.7.7.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2021</td>
<td id="S5.T2.7.7.7.7.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Videos</td>
<td id="S5.T2.7.7.7.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S5.T2.7.7.7.7.1.m1.1" class="ltx_Math" alttext="625.6" display="inline"><semantics id="S5.T2.7.7.7.7.1.m1.1a"><mn id="S5.T2.7.7.7.7.1.m1.1.1" xref="S5.T2.7.7.7.7.1.m1.1.1.cmml">625.6</mn><annotation-xml encoding="MathML-Content" id="S5.T2.7.7.7.7.1.m1.1b"><cn type="float" id="S5.T2.7.7.7.7.1.m1.1.1.cmml" xref="S5.T2.7.7.7.7.1.m1.1.1">625.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.7.7.7.1.m1.1c">625.6</annotation></semantics></math> minutes</td>
</tr>
<tr id="S5.T2.8.8.8.8" class="ltx_tr">
<td id="S5.T2.8.8.8.8.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Hwang<span id="S5.T2.8.8.8.8.2.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</td>
<td id="S5.T2.8.8.8.8.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">KIST SynADL</td>
<td id="S5.T2.8.8.8.8.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Elderly Action Recognition</td>
<td id="S5.T2.8.8.8.8.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2020</td>
<td id="S5.T2.8.8.8.8.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Videos</td>
<td id="S5.T2.8.8.8.8.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><math id="S5.T2.8.8.8.8.1.m1.2" class="ltx_Math" alttext="462,000" display="inline"><semantics id="S5.T2.8.8.8.8.1.m1.2a"><mrow id="S5.T2.8.8.8.8.1.m1.2.3.2" xref="S5.T2.8.8.8.8.1.m1.2.3.1.cmml"><mn id="S5.T2.8.8.8.8.1.m1.1.1" xref="S5.T2.8.8.8.8.1.m1.1.1.cmml">462</mn><mo id="S5.T2.8.8.8.8.1.m1.2.3.2.1" xref="S5.T2.8.8.8.8.1.m1.2.3.1.cmml">,</mo><mn id="S5.T2.8.8.8.8.1.m1.2.2" xref="S5.T2.8.8.8.8.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.8.8.8.8.1.m1.2b"><list id="S5.T2.8.8.8.8.1.m1.2.3.1.cmml" xref="S5.T2.8.8.8.8.1.m1.2.3.2"><cn type="integer" id="S5.T2.8.8.8.8.1.m1.1.1.cmml" xref="S5.T2.8.8.8.8.1.m1.1.1">462</cn><cn type="integer" id="S5.T2.8.8.8.8.1.m1.2.2.cmml" xref="S5.T2.8.8.8.8.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.8.8.8.8.1.m1.2c">462,000</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.2.1.1" class="ltx_text" style="font-size:90%;">TABLE III</span>: </span><span id="S5.T3.3.2" class="ltx_text" style="font-size:90%;">Publicly available synthetic data generation models. </span></figcaption>
<table id="S5.T3.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.4.1.1" class="ltx_tr">
<th id="S5.T3.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_rr ltx_border_t"><span id="S5.T3.4.1.1.1.1" class="ltx_text ltx_font_bold">Reference</span></th>
<th id="S5.T3.4.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_rr ltx_border_t"><span id="S5.T3.4.1.1.2.1" class="ltx_text ltx_font_bold">Application</span></th>
<th id="S5.T3.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T3.4.1.1.3.1" class="ltx_text ltx_font_bold">Year</span></th>
<th id="S5.T3.4.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T3.4.1.1.4.1" class="ltx_text ltx_font_bold">Method</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.4.2.1" class="ltx_tr">
<td id="S5.T3.4.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_tt">Drozdowski <span id="S5.T3.4.2.1.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
<td id="S5.T3.4.2.1.2" class="ltx_td ltx_align_left ltx_border_rr ltx_border_tt">Synthetic Iris Code Generator</td>
<td id="S5.T3.4.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">2017</td>
<td id="S5.T3.4.2.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">Handcrafted</td>
</tr>
<tr id="S5.T3.4.3.2" class="ltx_tr">
<td id="S5.T3.4.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Li <span id="S5.T3.4.3.2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib229" title="" class="ltx_ref">229</a>]</cite>
</td>
<td id="S5.T3.4.3.2.2" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">3D Face Model Generation (FLAME)</td>
<td id="S5.T3.4.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2019</td>
<td id="S5.T3.4.3.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Handcrafted</td>
</tr>
<tr id="S5.T3.4.4.3" class="ltx_tr">
<td id="S5.T3.4.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Feng <span id="S5.T3.4.4.3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib230" title="" class="ltx_ref">230</a>]</cite>
</td>
<td id="S5.T3.4.4.3.2" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">3D Face Model Registration (FLAME)</td>
<td id="S5.T3.4.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2021</td>
<td id="S5.T3.4.4.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Deep Neural Network</td>
</tr>
<tr id="S5.T3.4.5.4" class="ltx_tr">
<td id="S5.T3.4.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Colbois <span id="S5.T3.4.5.4.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>
</td>
<td id="S5.T3.4.5.4.2" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Syn Multi-PIE Face Generation</td>
<td id="S5.T3.4.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2021</td>
<td id="S5.T3.4.5.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Deep Neural Network</td>
</tr>
<tr id="S5.T3.4.6.5" class="ltx_tr">
<td id="S5.T3.4.6.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Gerig <span id="S5.T3.4.6.5.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib231" title="" class="ltx_ref">231</a>]</cite>
</td>
<td id="S5.T3.4.6.5.2" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">3D Face Model Registration</td>
<td id="S5.T3.4.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2018</td>
<td id="S5.T3.4.6.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Handcrafted</td>
</tr>
<tr id="S5.T3.4.7.6" class="ltx_tr">
<td id="S5.T3.4.7.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Chan <span id="S5.T3.4.7.6.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib232" title="" class="ltx_ref">232</a>]</cite>
</td>
<td id="S5.T3.4.7.6.2" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">3D Face Image Generation (EG3D)</td>
<td id="S5.T3.4.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2022</td>
<td id="S5.T3.4.7.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Deep Neural Network</td>
</tr>
<tr id="S5.T3.4.8.7" class="ltx_tr">
<td id="S5.T3.4.8.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Seneviratne <span id="S5.T3.4.8.7.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib233" title="" class="ltx_ref">233</a>]</cite>
</td>
<td id="S5.T3.4.8.7.2" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Masked and unmasked Face Recognition</td>
<td id="S5.T3.4.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2021</td>
<td id="S5.T3.4.8.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Deep Neural Network</td>
</tr>
<tr id="S5.T3.4.9.8" class="ltx_tr">
<td id="S5.T3.4.9.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Karras <span id="S5.T3.4.9.8.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</td>
<td id="S5.T3.4.9.8.2" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Face Image Generation (StyleGAN2)</td>
<td id="S5.T3.4.9.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2020</td>
<td id="S5.T3.4.9.8.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Deep Neural Network</td>
</tr>
<tr id="S5.T3.4.10.9" class="ltx_tr">
<td id="S5.T3.4.10.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Karras <span id="S5.T3.4.10.9.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</td>
<td id="S5.T3.4.10.9.2" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Face Generation (StyleGAN3)</td>
<td id="S5.T3.4.10.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2021</td>
<td id="S5.T3.4.10.9.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Deep Neural Network</td>
</tr>
<tr id="S5.T3.4.11.10" class="ltx_tr">
<td id="S5.T3.4.11.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Maltoni <span id="S5.T3.4.11.10.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S5.T3.4.11.10.2" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Fingerprint Image Generator (SFinGe)</td>
<td id="S5.T3.4.11.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2009</td>
<td id="S5.T3.4.11.10.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Handcrafted</td>
</tr>
<tr id="S5.T3.4.12.11" class="ltx_tr">
<td id="S5.T3.4.12.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t">Sun <span id="S5.T3.4.12.11.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
<td id="S5.T3.4.12.11.2" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Person Re-Identification (PersonX)</td>
<td id="S5.T3.4.12.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2019</td>
<td id="S5.T3.4.12.11.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">3D Scenes and Models</td>
</tr>
<tr id="S5.T3.4.13.12" class="ltx_tr">
<td id="S5.T3.4.13.12.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_rr ltx_border_t">Hwang <span id="S5.T3.4.13.12.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</td>
<td id="S5.T3.4.13.12.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_rr ltx_border_t">Elderly Action Recognition</td>
<td id="S5.T3.4.13.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2020</td>
<td id="S5.T3.4.13.12.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">3D Scenes and Models</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Challenges and Discussion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We discussed benefits and means to generate and use synthetic datasets, placing emphasis on synthetic datasets being instrumental in mitigating challenges associated to real datasets. Despite related advances, there are a number of open research problems in this expanding field.</p>
</div>
<div id="S6.p2" class="ltx_para">
<ol id="S6.I1" class="ltx_enumerate">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p"><span id="S6.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Identity leakage</span>. Studies that advocate using synthetic data for alleviating the privacy issues related to human data frequently do not conduct supporting experiments to show that there is no identity leakage from the training dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Such an assessment is critical to address privacy concerns related to sharing data for applications in human analysis. For instance, Engelsma <span id="S6.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> computed comparison scores between training samples and the synthetically generated fingerprints. Only <math id="S6.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="0.04\%" display="inline"><semantics id="S6.I1.i1.p1.1.m1.1a"><mrow id="S6.I1.i1.p1.1.m1.1.1" xref="S6.I1.i1.p1.1.m1.1.1.cmml"><mn id="S6.I1.i1.p1.1.m1.1.1.2" xref="S6.I1.i1.p1.1.m1.1.1.2.cmml">0.04</mn><mo id="S6.I1.i1.p1.1.m1.1.1.1" xref="S6.I1.i1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.I1.i1.p1.1.m1.1b"><apply id="S6.I1.i1.p1.1.m1.1.1.cmml" xref="S6.I1.i1.p1.1.m1.1.1"><csymbol cd="latexml" id="S6.I1.i1.p1.1.m1.1.1.1.cmml" xref="S6.I1.i1.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S6.I1.i1.p1.1.m1.1.1.2.cmml" xref="S6.I1.i1.p1.1.m1.1.1.2">0.04</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.I1.i1.p1.1.m1.1c">0.04\%</annotation></semantics></math> of the training samples obtained comparison scores above a threshold, and all such samples were removed from the synthetic dataset before introducing it in the public domain. Similar practices need to be adopted by the research community working in human analysis to mitigate any identity leakage.</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p"><span id="S6.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Lack of diversity</span>. The development of synthetic datasets in human analysis, generally speaking requires the generation of mated and non-mated samples. Recently, Grimmer <span id="S6.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib234" title="" class="ltx_ref">234</a>]</cite> emphasised the challenge of approximating the full intra-identity variation of real datasets. Mated samples were obtained through minor manipulations of various semantic attributes in a given sample. However, the generated datasets still lacked diversity compared to real-world datasets. Another challenge has to do with creating synthetic datasets balanced <span id="S6.I1.i2.p1.1.3" class="ltx_text ltx_font_italic">w.r.t.</span> demographics. Generative models are often trained on biased datasets, thus lowering the generation quality of synthetic samples from underrepresented classes. We note that the current working draft of ISO/IEC 19795-10 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib235" title="" class="ltx_ref">235</a>]</cite> aims at quantifying the biometric system performance variation across demographic groups, hence providing a standardized and consistent evaluation framework to assess the diversity of synthetic datasets.</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p"><span id="S6.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Representation ability</span>. Numerous scientific work, particularly in biometrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>, <a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite>, have observed that while the generated synthetic data appears realistic, its characteristics represent notable differences from real biometric samples. Such observations question the representation ability of generated synthetic data and motivate the design of representative synthetic data generation methods.
For instance, synthetic videos (deepfakes) frequently incorporate artefacts <span id="S6.I1.i3.p1.1.2" class="ltx_text ltx_font_italic">e.g.</span>, in the eye or lip region. In addition, characteristics/semantics in synthetic data differs from those in real samples. For instance, Gottschlich and Huckemann <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib236" title="" class="ltx_ref">236</a>]</cite> demonstrated the distribution of minutiae in synthetic fingerprints generated by SFinGe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> was different from the one observed for real fingerprints. Therefore, the representation ability of synthetic data needs to be carefully validated before exploiting it for real-world applications.</p>
</div>
<figure id="S6.F32" class="ltx_figure"><img src="/html/2208.09191/assets/x28.png" id="S6.F32.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="207" height="108" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F32.2.1.1" class="ltx_text" style="font-size:90%;">Figure 32</span>: </span><span id="S6.F32.3.2" class="ltx_text" style="font-size:90%;">First row: synthetically distorted training samples. Second row: real testing samples for fingerprint enhancement algorithms, as used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>. The performance of the fingerprint enhancement model was directly dependent on how well the synthetic data modelled the noise observed in real fingerprints. Therefore, synthetically distorted training data must be publicly available to ensure fair comparison among different fingerprint enhancement algorithms.</span></figcaption>
</figure>
</li>
<li id="S6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S6.I1.i4.p1" class="ltx_para">
<p id="S6.I1.i4.p1.1" class="ltx_p"><span id="S6.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">Lack of comparison</span>. While scientific works in human analysis have been gradually exploiting methods for generating synthetic data, the related generated synthetic datasets are often not shared publicly. This is crucial, as the performance of human analysis models is directly dependent on how well synthetic data aligns with the testing dataset (see Figure <a href="#S6.F32" title="Figure 32 ‣ item 3 ‣ 6 Challenges and Discussion ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">32</span></a>). In the case of Figure <a href="#S6.F32" title="Figure 32 ‣ item 3 ‣ 6 Challenges and Discussion ‣ Synthetic Data in Human Analysis: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">32</span></a>, the fingerprint enhancement performance is dependent not only on the enhancement model but also on how carefully curated synthetic training data is. Therefore, to foster reproducibility and ensure a fair comparison among different methods, there is a need to share synthetic datasets publicly.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusions and Future Applications</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">A review of the human analysis literature suggests that research in synthetic data is on the rise. This expansion is due to the large number of associated benefits in settings including
enrichment and replacement of existing real datasets.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">In this article, we reviewed some of the methods that have been developed for generation and exploitation of synthetic data in human analysis. In particular, we discussed techniques for generating semi-synthetic and fully synthetic data.
Examples of related applications, we elaborated on include simulation of complex scenarios, mitigating bias and privacy concerns, increasing the size and diversity of training datasets, assessing scalability of systems, providing additional data for supervision, pre-training and fine-tuning of deep neural networks, enforcing consistency regularization, as well as adversarial attacks. Finally,
we discussed some of the open research problems in synthetic data research.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">We believe that synthetic data has the ability to mitigate issues related to privacy, scalability, and generalization of unseen data. Although so far synthetic data is abundantly utilized in human analysis, we believe that research directions including active learning, knowledge distillation and source-free domain adaptation will benefit in future from synthetic data.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This research work has been supported by the French Government, by the National Research Agency (ANR) under Grant ANR-18-CE92-0024, project RESPECT, as well as by the
German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Nature</span>, vol. 521,
no. 7553, pp. 436–444, 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
T. Karras, M. Aittala, S. Laine, E. Härkönen, J. Hellsten, J. Lehtinen,
and T. Aila, “Alias-free generative adversarial networks,” <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Advances in
Neural Information Processing Systems</span>, vol. 34, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
R. Gal, D. C. Hochberg, A. Bermano, and D. Cohen-Or, “Swagan: A style-based
wavelet-driven generative model,” <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">ACM Trans. on Graphics</span>, vol. 40,
no. 4, pp. 1–11, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Q. Feng, C. Guo, F. Benitez-Quiroz, and A. M. Martinez, “When do gans
replicate? on the choice of dataset size,” in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF
Intl. Conf. on Computer Vision</span>, pp. 6701–6710, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila,
“Analyzing and improving the image quality of stylegan,” in <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proc. of
the IEEE/CVF Conf. on Computer Vision and Pattern Recognition</span>,
pp. 8110–8119, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
D. Maltoni, D. Maio, A. K. Jain, and S. Prabhakar, “Synthetic fingerprint
generation,” <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Handbook of Fingerprint Recognition</span>, pp. 271–302, 2009.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S. A. Grosz and A. K. Jain, “Spoofgan: Synthetic fingerprint spoof images,”
<span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2204.06498</span>, 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
G. Varol, J. Romero, X. Martin, N. Mahmood, M. J. Black, I. Laptev, and
C. Schmid, “Learning from synthetic humans,” in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE
Conf. on Computer Vision and Pattern Recognition</span>, pp. 109–117, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
H. Hwang, C. Jang, G. Park, J. Cho, and I.-J. Kim, “Eldersim: A synthetic data
generation platform for human action recognition in eldercare applications,”
<span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.14742</span>, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
H. Dou, W. Zhang, P. Zhang, Y. Zhao, S. Li, Z. Qin, F. Wu, L. Dong, and X. Li,
“Versatilegait: A large-scale synthetic gait dataset with fine-grained
attributes and complicated scenarios,” <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2101.01394</span>, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
E. Wood, T. Baltrušaitis, C. Hewitt, S. Dziadzio, T. J. Cashman, and
J. Shotton, “Fake it till you make it: Face analysis in the wild using
synthetic data alone,” in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF Intl. Conf. on Computer
Vision</span>, pp. 3681–3691, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
P. İrtem, E. İrtem, and N. Erdoğmuş, “Impact of variations
in synthetic training data on fingerprint classification,” in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Intl.
Conf. of the Biometrics Special Interest Group</span>, pp. 1–4, IEEE, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
F. K. Dankar and M. Ibrahim, “Fake it till you make it: Guidelines for
effective synthetic data generation,” <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Applied Sciences</span>, vol. 11,
no. 5, p. 2158, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing of gans
for improved quality, stability, and variation,” <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:1710.10196</span>, 2017.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M. S. Riazi, S. M. Chavoshian, and F. Koushanfar, “Synfi: Automatic synthetic
fingerprint generation,” <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2002.08900</span>, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S. Yadav, C. Chen, and A. Ross, “Synthesizing iris images using rasgan with
application in presentation attack detection,” in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF
Conf. on Computer Vision and Pattern Recognition Workshops</span>, pp. 0–0, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
S. I. Nikolenko, <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Synthetic data for deep learning</span>, vol. 174.

</span>
<span class="ltx_bibblock">Springer, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J. J. Engelsma, S. A. Grosz, and A. K. Jain, “Printsgan: Synthetic fingerprint
generator,” <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2201.03674</span>, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
T. Karras, S. Laine, and T. Aila, “A style-based generator architecture for
generative adversarial networks,” in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proc of the IEEE/CVF Conf. on
Computer Vision and Pattern Recognition</span>, pp. 4401–4410, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Y. Alaluf, O. Patashnik, and D. Cohen-Or, “Only a matter of style: Age
transformation using a style-based regression model,” <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">ACM Trans. on
Graphics (TOG)</span>, vol. 40, no. 4, pp. 1–12, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. S. Joshi, A. Dabouei, J. Dawson, and N. M. Nasrabadi, “Fdeblur-gan:
Fingerprint deblurring using generative adversarial network,” in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">2021
IEEE Intl. Joint Conf. on Biometrics (IJCB)</span>, pp. 1–8, IEEE, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Y. Shen, J. Gu, X. Tang, and B. Zhou, “Interpreting the latent space of gans
for semantic face editing,” in <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF Conf. on Computer
Vision and Pattern Recognition</span>, pp. 9243–9252, 2020.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
P. J. Phillips, P. J. Flynn, T. Scruggs, K. W. Bowyer, J. Chang, K. Hoffman,
J. Marques, J. Min, and W. Worek, “Overview of the face recognition grand
challenge,” in <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">IEEE computer society Conf. on Computer Vision and
Pattern Recognition</span>, vol. 1, pp. 947–954, 2005.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
ISO/IEC JTC1 SC37 Biometrics, <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">ISO/IEC 2382-37:2017 Information
Technology - Vocabulary - Part 37: Biometrics</span>.

</span>
<span class="ltx_bibblock">Intl. Organization for Standardization, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
P. Drozdowski, C. Rathgeb, and C. Busch, “Sic-gen: A synthetic iris-code
generator,” in <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Intl. Conf. of the Biometrics Special Interest Group</span>,
pp. 1–6, IEEE, 2017.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. Kortylewski, B. Egger, A. Schneider, T. Gerig, A. Morel-Forster, and
T. Vetter, “Analyzing and reducing the damage of dataset bias to face
recognition with synthetic data,” in <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF Conf. on
Computer Vision and Pattern Recognition Workshops</span>, pp. 0–0, 2019.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
C. Shorten and T. M. Khoshgoftaar, “A survey on image data augmentation for
deep learning,” <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Journal of big data</span>, vol. 6, no. 1, pp. 1–48, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
H. Qiu, B. Yu, D. Gong, Z. Li, W. Liu, and D. Tao, “Synface: Face recognition
with synthetic data,” in <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF Intl. Conf. on Computer
Vision</span>, pp. 10880–10890, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
P. Tinsley, A. Czajka, and P. Flynn, “This face does not exist… but it might
be yours! identity leakage in generative models,” in <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Proc. of the
IEEE/CVF Winter Conf. on Applications of Computer Vision</span>, pp. 1320–1328,
2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
M. Fredrikson, S. Jha, and T. Ristenpart, “Model inversion attacks that
exploit confidence information and basic countermeasures,” in <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Proc. of
the 22nd ACM SIGSAC Conf. on Computer and Communications Security</span>,
pp. 1322–1333, 2015.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
C. Xu, J. Ren, D. Zhang, Y. Zhang, Z. Qin, and K. Ren, “Ganobfuscator:
Mitigating information leakage under gan via differential privacy,” <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Information Forensics and Security</span>, vol. 14, no. 9,
pp. 2358–2371, 2019.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
I. Kemelmacher-Shlizerman, S. M. Seitz, D. Miller, and E. Brossard, “The
megaface benchmark: 1 million faces for recognition at scale,” in <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Proc.
of the IEEE Conf. on Computer Vision and Pattern Recognition</span>,
pp. 4873–4882, 2016.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
A. Roitberg, D. Schneider, A. Djamal, C. Seibold, S. Reiß, and
R. Stiefelhagen, “Let’s play for action: Recognizing activities of daily
living by learning from life simulation video games,” in <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">IEEE/RSJ Intl.
Conf. on Intelligent Robots and Systems</span>, pp. 8563–8569, IEEE, 2021.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
B. Bozorgtabar, M. S. Rad, H. K. Ekenel, and J.-P. Thiran, “Using
photorealistic face synthesis and domain adaptation to improve facial
expression analysis,” in <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">14th IEEE Intl. Conf. on Automatic Face &amp;
Gesture Recognition</span>, pp. 1–8, 2019.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
A. Dantcheva, P. Elia, and A. Ross, “What else does your biometric data
reveal? a survey on soft biometrics,” <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Information
Forensics and Security</span>, vol. 11, no. 3, pp. 441–467, 2016.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
ISO/IEC JTC1 SC37 Biometrics, <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">ISO/IEC 30107-1. Information Technology
- Biometric Presentation Attack Detection - Part 1: Framework</span>.

</span>
<span class="ltx_bibblock">Intl. Organization for Standardization, 2016.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
X. Shu, J. Tang, G.-J. Qi, W. Liu, and J. Yang, “Hierarchical long short-term
concurrent memory for human interaction recognition,” <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">IEEE Transactions
on Pattern Analysis and Machine Intelligence</span>, vol. 43, no. 3,
pp. 1110–1118, 2019.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
N. Aranjuelo, S. García, E. Loyo, L. Unzueta, and O. Otaegui, “Key
strategies for synthetic data generation for training intelligent systems
based on people detection from omnidirectional cameras,” <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Computers &amp;
Electrical Engineering</span>, vol. 92, p. 107105, 2021.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Q. Wang, J. Gao, W. Lin, and Y. Yuan, “Pixel-wise crowd understanding via
synthetic data,” <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">Intl. Journal of Computer Vision</span>, vol. 129, no. 1,
pp. 225–245, 2021.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
C. Ménier, E. Boyer, and B. Raffin, “3d skeleton-based body pose
recovery,” in <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">3rd Intl. Symposium on 3D Data Processing, Visualization,
and Transmission</span>, pp. 389–396, IEEE, 2006.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
T. Hempel, A. A. Abdelrahman, and A. Al-Hamadi, “6d rotation representation
for unconstrained head pose estimation,” <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2202.12555</span>, 2022.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
D. Fu, D. Chen, J. Bao, H. Yang, L. Yuan, L. Zhang, H. Li, and D. Chen,
“Unsupervised pre-training for person re-identification,” in <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">Proc. of
the IEEE/CVF Conf. on Computer Vision and Pattern Recognition</span>,
pp. 14750–14759, 2021.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
X. Sun and L. Zheng, “Dissecting person re-identification from the viewpoint
of viewpoint,” in <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF Conf. on Computer Vision and
Pattern Recognition</span>, 2019.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
J. Kolberg, M. Grimmer, M. Gomez-Barrero, and C. Busch, “Anomaly detection
with convolutional autoencoders for fingerprint presentation attack
detection,” <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Biometrics, Behavior, and Identity Science</span>,
vol. 3, no. 2, pp. 190–202, 2021.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
R. J. Chen, M. Y. Lu, T. Y. Chen, D. F. Williamson, and F. Mahmood, “Synthetic
data in machine learning for medicine and healthcare,” <span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">Nature
Biomedical Engineering</span>, vol. 5, no. 6, pp. 493–497, 2021.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
J. J. Bird, D. R. Faria, A. Ekárt, C. Premebida, and P. P. Ayrosa, “Lstm
and gpt-2 synthetic speech transfer learning for speaker recognition to
overcome data scarcity,” <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2007.00659</span>, 2020.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
J. E. Tapia and C. Arellano, “Soft-biometrics encoding conditional gan for
synthesis of nir periocular images,” <span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">Future Generation Computer
Systems</span>, vol. 97, pp. 503–511, 2019.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
S. Anton, T. Artem, P. Andrey, and K. Igor, “Modification of vgg neural
network architecture for unimodal and multimodal biometrics,” in <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">IEEE
East-West Design &amp; Test Symposium</span>, pp. 1–4, IEEE, 2020.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
A. Acien, A. Morales, J. Fierrez, R. Vera-Rodriguez, and O. Delgado-Mohatar,
“Becaptcha: Bot detection in smartphone interaction using touchscreen
biometrics and mobile sensors,” in <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">AAAI Workshop on Artificial
Intelligence for Cyber Security</span>, 2020.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
J. Han, S. Karaoglu, H.-A. Le, and T. Gevers, “Improving face detection
performance with 3d-rendered synthetic data,” <span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:1812.07363</span>, 2018.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
S. Basak, P. Corcoran, F. Khan, R. Mcdonnell, and M. Schukat, “Learning 3d
head pose from synthetic data: A semi-supervised approach,” <span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">IEEE
Access</span>, vol. 9, pp. 37557–37573, 2021.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
J. J. Bird, D. R. Faria, C. Premebida, A. Ekárt, and P. P. Ayrosa,
“Overcoming data scarcity in speaker identification: Dataset augmentation
with synthetic mfccs via character-level rnn,” in <span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">IEEE Intl. Conf. on
Autonomous Robot Systems and Competitions</span>, pp. 146–151, IEEE, 2020.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
T. Piplani, N. Merill, and J. Chuang, “Faking it, making it: Fooling and
improving brain-based authentication with generative adversarial networks,”
in <span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">IEEE 9th Intl. Conf. on Biometrics Theory, Applications and Systems</span>,
pp. 1–7, IEEE, 2018.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
A. Buriro, F. Ricci, and B. Crispo, “Swipegan: Swiping data augmentation using
generative adversarial networks for smartphone user authentication,” in <span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">Proc. of the 3rd ACM Workshop on Wireless Security and Machine Learning</span>,
pp. 85–90, 2021.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
R. Gouiaa and J. Meunier, “Learning cast shadow appearance for human posture
recognition,” <span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">Pattern Recognition Letters</span>, vol. 97, pp. 54–60, 2017.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
V. Ruiz, I. Linares, A. Sanchez, and J. F. Velez, “Off-line handwritten
signature verification using compositional synthetic generation of signatures
and siamese neural networks,” <span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">Neurocomputing</span>, vol. 374, pp. 30–41,
2020.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
A. Kortylewski, A. Schneider, T. Gerig, B. Egger, A. Morel-Forster, and
T. Vetter, “Training deep face recognition systems with synthetic data,”
<span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1802.05891</span>, 2018.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
D. S. Trigueros, L. Meng, and M. Hartnett, “Generating photo-realistic
training data to improve face recognition accuracy,” <span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:1811.00112</span>, 2018.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Z. Zhai, P. Yang, X. Zhang, M. Huang, H. Cheng, X. Yan, C. Wang, and S. Pu,
“Demodalizing face recognition with synthetic samples,” in <span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">Proc. of
the AAAI Conf. on Artificial Intelligence</span>, vol. 35, pp. 3278–3286, 2021.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
G. Chen, Y. Zhu, Z. Hong, and Z. Yang, “Emotionalgan: generating ecg to
enhance emotion state classification,” in <span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">Proc. of the Intl. Conf. on
Artificial Intelligence and Computer Science</span>, pp. 309–313, 2019.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
V. K. Melo, B. L. D. Bezerra, D. Impedovo, G. Pirlo, and A. Lundgren, “Deep
learning approach to generate offline handwritten signatures based on online
samples,” <span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">IET Biometrics</span>, vol. 8, no. 3, pp. 215–220, 2019.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
M. Öz, T. Danisman, M. Günay, E. Z. Sanal, Ö. Duman, and J. W.
Ledet, “The use of synthetic data to facilitate eye segmentation using
deeplabv3+,” <span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">Annals of Emerging Technologies in Computing</span>, vol. 5,
no. 3, pp. 1–10, 2021.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Q. Wang, J. Gao, W. Lin, and Y. Yuan, “Learning from synthetic data for crowd
counting in the wild,” in <span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF Conf. on Computer
Vision and Pattern Recognition</span>, pp. 8198–8207, 2019.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
L. Colbois, T. de Freitas Pereira, and S. Marcel, “On the use of automatically
generated synthetic image datasets for benchmarking face recognition,” in
<span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">2021 IEEE Intl. Joint Conf. on Biometrics (IJCB)</span>, pp. 1–8, IEEE, 2021.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
R. T. Marriott, S. Romdhani, and L. Chen, “A 3d gan for improved large-pose
facial recognition,” in <span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF Conf. on Computer Vision
and Pattern Recognition</span>, pp. 13445–13455, 2021.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
T. U. Ahmed, S. Hossain, M. S. Hossain, R. ul Islam, and K. Andersson, “Facial
expression recognition using convolutional neural network with data
augmentation,” in <span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">2019 Joint 8th Intl. Conf. on Informatics,
Electronics &amp; Vision (ICIEV) and 2019 3rd Intl. Conf. on Imaging, Vision &amp;
Pattern Recognition (icIVPR)</span>, pp. 336–341, IEEE, 2019.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
K. Niinuma, I. O. Ertugrul, J. F. Cohn, and L. A. Jeni, “Synthetic expressions
are better than real for learning to detect facial actions,” in <span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">Proc.
of the IEEE/CVF Winter Conf. on Applications of Computer Vision</span>,
pp. 1248–1257, 2021.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
I. Abbasnejad, S. Sridharan, D. Nguyen, S. Denman, C. Fookes, and S. Lucey,
“Using synthetic data to improve facial expression analysis with 3d
convolutional networks,” in <span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE Intl. Conf. on Computer
Vision Workshops</span>, pp. 1609–1618, 2017.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
G. Varol, I. Laptev, C. Schmid, and A. Zisserman, “Synthetic humans for action
recognition from unseen viewpoints,” <span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">Intl. Journal of Computer Vision</span>,
vol. 129, no. 7, pp. 2264–2287, 2021.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
E. Hatay, J. Ma, H. Sun, J. Fang, Z. Gao, and H. Yu, “Learning to detect
phone-related pedestrian distracted behaviors with synthetic data,” in <span id="bib.bib70.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition</span>,
pp. 2981–2989, 2021.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
C. R. de Souza12, A. Gaidon, Y. Cabon, and A. M. López, “Procedural
generation of videos to train deep action recognition networks,” 2017.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
J. Priesnitz, C. Rathgeb, N. Buchmann, and C. Busch, “Syncolfinger: Synthetic
contactless fingerprint generator,” <span id="bib.bib72.1.1" class="ltx_text ltx_font_italic">Pattern Recognition Letters</span>, 2022.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
C. Carmona-Duarte, M. A. Ferrer, A. Parziale, and A. Marcelli, “Temporal
evolution in synthetic handwriting,” <span id="bib.bib73.1.1" class="ltx_text ltx_font_italic">Pattern Recognition</span>, vol. 68,
pp. 233–244, 2017.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
M. A. Ferrer, M. Diaz-Cabrera, and A. Morales, “Synthetic off-line signature
image generation,” in <span id="bib.bib74.1.1" class="ltx_text ltx_font_italic">Intl. Conf. on Biometrics</span>, pp. 1–7, 2013.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
C. O’Reilly and R. Plamondon, “Development of a sigma–lognormal
representation for on-line signatures,” <span id="bib.bib75.1.1" class="ltx_text ltx_font_italic">Pattern recognition</span>, vol. 42,
no. 12, pp. 3324–3337, 2009.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
M. A. Ferrer, M. Diaz-Cabrera, and A. Morales, “Static signature synthesis: A
neuromotor inspired approach for biometrics,” <span id="bib.bib76.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Pattern
Analysis and Machine Intelligence</span>, vol. 37, no. 3, pp. 667–680, 2014.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
F. Hillerström, A. Kumar, and R. N. J. Veldhuis, “Generating and analyzing
synthetic finger vein images,” in <span id="bib.bib77.1.1" class="ltx_text ltx_font_italic">BIOSIG 2014</span> (A. Brömme and
C. Busch, eds.), (Bonn), pp. 121–132, Gesellschaft für Informatik e.V.,
2014.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham, “Active shape
models-their training and application,” <span id="bib.bib78.1.1" class="ltx_text ltx_font_italic">Computer Vision and Image
Understanding</span>, vol. 61, no. 1, pp. 38–59, 1995.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
M. Osadchy, Y. Wang, O. Dunkelman, S. Gibson, J. Hernandez-Castro, and
C. Solomon, “Genface: Improving cyber security using realistic synthetic
face generation,” in <span id="bib.bib79.1.1" class="ltx_text ltx_font_italic">Intl. Conf. on Cyber Security Cryptography and
Machine Learning</span>, pp. 19–33, Springer, 2017.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
D. Migdal and C. Rosenberger, “Statistical modeling of keystroke dynamics
samples for the generation of synthetic datasets,” <span id="bib.bib80.1.1" class="ltx_text ltx_font_italic">Future Generation
Computer Systems</span>, vol. 100, pp. 907–920, 2019.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
L. Friedman and O. Komogortsev, “Synthetic database for evaluation of general,
fundamental biometric principles,” <span id="bib.bib81.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1707.09543</span>,
2017.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
F. Wan, Y. Wu, X. Qian, Y. Chen, and Y. Fu, “When person re-identification
meets changing clothes,” in <span id="bib.bib82.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF Conf. on Computer
Vision and Pattern Recognition Workshops</span>, pp. 830–831, 2020.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Z.-H. Feng, G. Hu, J. Kittler, W. Christmas, and X.-J. Wu, “Cascaded
collaborative regression for robust facial landmark detection trained using a
mixture of synthetic and real images with dynamic weighting,” <span id="bib.bib83.1.1" class="ltx_text ltx_font_italic">IEEE
Trans. on Image Processing</span>, vol. 24, no. 11, pp. 3425–3440, 2015.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
F. Liu, L. Tran, and X. Liu, “3d face modeling from diverse raw scan data,”
in <span id="bib.bib84.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF Intl. Conf. on Computer Vision</span>,
pp. 9408–9418, 2019.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
C. C. Charalambous and A. A. Bharath, “A data augmentation methodology for
training machine/deep learning gait recognition algorithms,” <span id="bib.bib85.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:1610.07570</span>, 2016.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
E. Wood, T. Baltrušaitis, L.-P. Morency, P. Robinson, and A. Bulling,
“Learning an appearance-based gaze estimator from one million synthesised
images,” in <span id="bib.bib86.1.1" class="ltx_text ltx_font_italic">Proc. of the Ninth Biennial ACM Symposium on Eye Tracking
Research &amp; Applications</span>, pp. 131–138, 2016.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
M. Ibsen, C. Rathgeb, P. Drozdowski, and C. Busch, “Face beneath the ink:
Synthetic data and tattoo removal with application to face recognition,”
<span id="bib.bib87.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2202.05297</span>, 2022.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
B. Nojavanasghari, C. E. Hughes, T. Baltrušaitis, and L.-P. Morency,
“Hand2face: Automatic synthesis and recognition of hand over face
occlusions,” in <span id="bib.bib88.1.1" class="ltx_text ltx_font_italic">7th Intl. Conf. on Affective Computing and Intelligent
Interaction</span>, pp. 209–215, 2017.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
K. Cao and A. K. Jain, “Latent orientation field estimation via convolutional
neural network,” in <span id="bib.bib89.1.1" class="ltx_text ltx_font_italic">Intl. Conf. on Biometrics</span>, pp. 349–356, 2015.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
I. Joshi, A. Utkarsh, R. Kothari, V. K. Kurmi, A. Dantcheva, S. D. Roy, and
P. K. Kalra, “Data uncertainty guided noise-aware preprocessing of
fingerprints,” in <span id="bib.bib90.1.1" class="ltx_text ltx_font_italic">Intl. Joint Conf. on Neural Networks</span>, pp. 1–8,
2021.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
B. Dieckmann, J. Merkle, and C. Rathgeb, “Fingerprint pre-alignment based on
deep learning,” in <span id="bib.bib91.1.1" class="ltx_text ltx_font_italic">Intl. Conf. of the Biometrics Special Interest
Group</span>, pp. 1–6, 2019.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
L. Cardoso, A. Barbosa, F. Silva, A. M. Pinheiro, and H. Proença, “Iris
biometrics: Synthesis of degraded ocular images,” <span id="bib.bib92.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on
information forensics and security</span>, vol. 8, no. 7, pp. 1115–1125, 2013.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
F. Fuentes-Hurtado, V. Naranjo, J. A. Diego-Mas, and M. Alcañiz, “A hybrid
method for accurate iris segmentation on at-a-distance visible-wavelength
images,” <span id="bib.bib93.1.1" class="ltx_text ltx_font_italic">Journal on Image and Video Processing</span>, vol. 2019, no. 1,
pp. 1–14, 2019.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
J. Yin, S. Zhang, J. Xie, Z. Ma, and J. Guo, “Unsupervised person
re-identification via simultaneous clustering and mask prediction,” <span id="bib.bib94.1.1" class="ltx_text ltx_font_italic">Pattern Recognition</span>, p. 108568, 2022.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
A. K. Jain, D. Deb, and J. J. Engelsma, “Biometrics: Trust, but verify,” <span id="bib.bib95.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Biometrics, Behavior, and Identity Science</span>, 2021.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
D. Deb, X. Liu, and A. K. Jain, “Faceguard: A self-supervised defense against
adversarial face images,” <span id="bib.bib96.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2011.14218</span>, 2020.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
R. Li, J.-Y. Jiang, J. L. Li, C.-C. Hsieh, and W. Wang, “Automatic speaker
recognition with limited data,” in <span id="bib.bib97.1.1" class="ltx_text ltx_font_italic">Proc. of the 13th Intl. Conf. on Web
Search and Data Mining</span>, pp. 340–348, 2020.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
H. Wang, G. Wang, Y. Li, D. Zhang, and L. Lin, “Transferable, controllable,
and inconspicuous adversarial attacks on person re-identification with deep
mis-ranking,” in <span id="bib.bib98.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF Conf. on Computer Vision and
Pattern Recognition</span>, pp. 342–351, 2020.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
Y. Zhong and W. Deng, “Towards transferable adversarial attack against deep
face recognition,” <span id="bib.bib99.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Information Forensics and Security</span>,
vol. 16, pp. 1452–1466, 2020.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
S. Soleymani, A. Dabouei, J. Dawson, and N. M. Nasrabadi, “Adversarial
examples to fool iris recognition systems,” in <span id="bib.bib100.1.1" class="ltx_text ltx_font_italic">Intl. Conf. on
Biometrics</span>, pp. 1–8, IEEE, 2019.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
S. Marrone and C. Sansone, “On the transferability of adversarial perturbation
attacks against fingerprint based authentication systems,” <span id="bib.bib101.1.1" class="ltx_text ltx_font_italic">Pattern
Recognition Letters</span>, vol. 152, pp. 253–259, 2021.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Y. Mirsky and W. Lee, “The creation and detection of deepfakes: A survey,”
<span id="bib.bib102.1.1" class="ltx_text ltx_font_italic">ACM Computing Surveys</span>, vol. 54, no. 1, pp. 1–41, 2021.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
A. K. Mondal, H. Asnani, P. Singla, and A. Prathosh, “Flexae: Flexibly
learning latent priors for wasserstein auto-encoders,” in <span id="bib.bib103.1.1" class="ltx_text ltx_font_italic">Uncertainty
in Artificial Intelligence</span>, pp. 525–535, PMLR, 2021.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” <span id="bib.bib104.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:1312.6114</span>, 2013.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey, “Adversarial
autoencoders,” <span id="bib.bib105.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.05644</span>, 2015.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
I. Tolstikhin, O. Bousquet, S. Gelly, and B. Schoelkopf, “Wasserstein
auto-encoders,” in <span id="bib.bib106.1.1" class="ltx_text ltx_font_italic">Intl. Conf. on Learning Representations</span>, 2018.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
J. Tomczak and M. Welling, “Vae with a vampprior,” in <span id="bib.bib107.1.1" class="ltx_text ltx_font_italic">Intl. Conf. on
Artificial Intelligence and Statistics</span>, pp. 1214–1223, 2018.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
B. Dai and D. Wipf, “Diagnosing and enhancing vae models,” in <span id="bib.bib108.1.1" class="ltx_text ltx_font_italic">Intl.
Conf. on Learning Representations</span>, 2018.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick,
S. Mohamed, and A. Lerchner, “beta-vae: Learning basic visual concepts with
a constrained variational framework,” 2016.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
H. Kim and A. Mnih, “Disentangling by factorising,” in <span id="bib.bib110.1.1" class="ltx_text ltx_font_italic">Intl. Conf. on
Machine Learning</span>, pp. 2649–2658, 2018.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
A. Mondal, S. Pal Chowdhury, A. Jayendran, H. Asnani, P. Singla, and P. A P,
“Maskaae: Latent space optimization for adversarial auto-encoders,” 2020.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, and Y. Bengio, “Generative adversarial nets,” <span id="bib.bib112.1.1" class="ltx_text ltx_font_italic">Advances
in Neural Information Processing Systems</span>, vol. 27, 2014.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
K. Cao and A. Jain, “Fingerprint synthesis: Evaluating fingerprint search at
scale,” in <span id="bib.bib113.1.1" class="ltx_text ltx_font_italic">Intl. Conf. on Biometrics</span>, pp. 31–38, 2018.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo, “Stargan: Unified
generative adversarial networks for multi-domain image-to-image
translation,” in <span id="bib.bib114.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE Conf. on Computer Vision and Pattern
Recognition</span>, pp. 8789–8797, 2018.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
A. Mondal, A. Bhattacharjee, S. Mukherjee, H. Asnani, S. Kannan, and
A. Prathosh, “C-mi-gan: Estimation of conditional mutual information using
minmax formulation,” in <span id="bib.bib115.1.1" class="ltx_text ltx_font_italic">Conf. on Uncertainty in Artificial
Intelligence</span>, pp. 849–858, PMLR, 2020.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning
with deep convolutional generative adversarial networks,” <span id="bib.bib116.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:1511.06434</span>, 2015.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein generative adversarial
networks,” in <span id="bib.bib117.1.1" class="ltx_text ltx_font_italic">Intl. Conf. on Machine Learning</span>, pp. 214–223, PMLR,
2017.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation
with conditional adversarial networks,” in <span id="bib.bib118.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE Conf. on
Computer Vision and Pattern Recognition</span>, pp. 1125–1134, 2017.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
translation using cycle-consistent adversarial networks,” in <span id="bib.bib119.1.1" class="ltx_text ltx_font_italic">Proc. of
the IEEE international Conf. on Computer Vision</span>, pp. 2223–2232, 2017.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
H. Chen, Y. Wang, B. Lagadec, A. Dantcheva, and F. Bremond, “Joint generative
and contrastive learning for unsupervised person re-identification,” in <span id="bib.bib120.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition</span>,
pp. 2004–2013, 2021.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
X. Li, Y. Makihara, C. Xu, Y. Yagi, and M. Ren, “Gait recognition via
semi-supervised disentangled representation learning to identity and
covariate features,” in <span id="bib.bib121.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF Conf. on Computer Vision
and Pattern Recognition</span>, pp. 13309–13319, 2020.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
N. Takemura, Y. Makihara, D. Muramatsu, T. Echigo, and Y. Yagi, “Multi-view
large population gait dataset and its performance evaluation for cross-view
gait recognition,” <span id="bib.bib122.1.1" class="ltx_text ltx_font_italic">Trans. on Computer Vision and Applications</span>,
vol. 10, no. 1, pp. 1–14, 2018.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
S. Lai, L. Jin, L. Lin, Y. Zhu, and H. Mao, “Synsig2vec: Learning
representations from synthetic dynamic signatures for real-world
verification,” in <span id="bib.bib123.1.1" class="ltx_text ltx_font_italic">Proc. of the AAAI Conf. on Artificial Intelligence</span>,
vol. 34, pp. 735–742, 2020.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
E. Tabassi, T. Chugh, D. Deb, and A. K. Jain, “Altered fingerprints: Detection
and localization,” in <span id="bib.bib124.1.1" class="ltx_text ltx_font_italic">IEEE 9th Intl. Conf. on Biometrics Theory,
Applications and Systems</span>, pp. 1–9, 2018.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
D. Arifoglu and A. Bouchachia, “Abnormal behaviour detection for dementia
sufferers via transfer learning and recursive auto-encoders,” in <span id="bib.bib125.1.1" class="ltx_text ltx_font_italic">IEEE
Intl. Conf. on Pervasive Computing and Communications Workshops</span>,
pp. 529–534, 2019.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
H. Zhang, M. Grimmer, R. Ramachandra, K. Raja, and C. Busch, “On the
applicability of synthetic data for face recognition,” in <span id="bib.bib126.1.1" class="ltx_text ltx_font_italic">IEEE Intl.
Workshop on Biometrics and Forensics</span>, pp. 1–6, 2021.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
E. Bozkir, A. B. Ünal, M. Akgün, E. Kasneci, and N. Pfeifer, “Privacy
preserving gaze estimation using synthetic images via a randomized encoding
based framework,” in <span id="bib.bib127.1.1" class="ltx_text ltx_font_italic">ACM Symposium on Eye Tracking Research and
Applications</span>, pp. 1–5, 2020.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
F. Hillerström and A. Kumar, “On generation and analysis of synthetic
finger-vein images for biometrics identification,” <span id="bib.bib128.1.1" class="ltx_text ltx_font_italic">Technical Report No.
COMP-K-17</span>, 2014.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker, “Multi-pie,” <span id="bib.bib129.1.1" class="ltx_text ltx_font_italic">Image and vision computing</span>, vol. 28, no. 5, pp. 807–813, 2010.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
I. Masi, A. T. Tran, T. Hassner, G. Sahin, and G. Medioni, “Face-specific data
augmentation for unconstrained face recognition,” <span id="bib.bib130.1.1" class="ltx_text ltx_font_italic">Intl. Journal of
Computer Vision</span>, vol. 127, no. 6, pp. 642–667, 2019.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
B. F. Klare, B. Klein, E. Taborsky, A. Blanton, J. Cheney, K. Allen,
P. Grother, A. Mah, and A. K. Jain, “Pushing the frontiers of unconstrained
face detection and recognition: Iarpa janus benchmark a,” in <span id="bib.bib131.1.1" class="ltx_text ltx_font_italic">Proc. of
the IEEE Conf. on Computer Vision and Pattern Recognition</span>, pp. 1931–1939,
2015.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
C. Galea and R. A. Farrugia, “Forensic face photo-sketch recognition using a
deep learning-based architecture,” <span id="bib.bib132.1.1" class="ltx_text ltx_font_italic">IEEE Signal Processing Letters</span>,
vol. 24, no. 11, pp. 1586–1590, 2017.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
Y. Yu, G. Liu, and J.-M. Odobez, “Improving few-shot user-specific gaze
adaptation via gaze redirection synthesis,” in <span id="bib.bib133.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF
Conf. on Computer Vision and Pattern Recognition</span>, pp. 11937–11946, 2019.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
D. Anghelone, C. Chen, P. Faure, A. Ross, and A. Dantcheva, “Explainable
thermal to visible face recognition using latent-guided generative
adversarial network,” in <span id="bib.bib134.1.1" class="ltx_text ltx_font_italic">16th IEEE Intl. Conf. on Automatic Face and
Gesture Recognition</span>, pp. 1–8, IEEE, 2021.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
K. Cao and A. K. Jain, “Fingerprint indexing and matching: An integrated
approach,” in <span id="bib.bib135.1.1" class="ltx_text ltx_font_italic">IEEE Intl. Joint Conf. on Biometrics</span>, pp. 437–445,
2017.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
K. Sumi, C. Liu, and T. Matsuyama, “Study on synthetic face database for
performance evaluation,” in <span id="bib.bib136.1.1" class="ltx_text ltx_font_italic">Intl. Conf. on Biometrics</span>, pp. 598–604,
Springer, 2006.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
C. L. Wilson, C. I. Watson, M. D. Garris, A. Hicklin, <span id="bib.bib137.1.1" class="ltx_text ltx_font_italic">et al.</span>, <span id="bib.bib137.2.2" class="ltx_text ltx_font_italic">Studies of fingerprint matching using the NIST verification test bed (VTB)</span>.

</span>
<span class="ltx_bibblock">US Department of Commerce, Technology Administration, National
Institute of …, 2003.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville,
“Improved training of wasserstein gans,” <span id="bib.bib138.1.1" class="ltx_text ltx_font_italic">Adv. in Neural Information
Processing Systems</span>, vol. 30, 2017.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
C. Watson, “Nist special database 14-mated fingerprint card pairs 2,” <span id="bib.bib139.1.1" class="ltx_text ltx_font_italic">National Institute of Standards and Technology</span>, 1993.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
A. Morales, M. A. Ferrer, R. Cappelli, D. Maltoni, J. Fierrez, and
J. Ortega-Garcia, “Synthesis of large scale hand-shape databases for
biometric applications,” <span id="bib.bib140.1.1" class="ltx_text ltx_font_italic">Pattern Recognition Letters</span>, vol. 68,
pp. 183–189, 2015.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
I. Joshi, A. Anand, M. Vatsa, R. Singh, S. D. Roy, and P. Kalra, “Latent
fingerprint enhancement using generative adversarial networks,” in <span id="bib.bib141.1.1" class="ltx_text ltx_font_italic">IEEE
Winter Conf. on Applications of Computer Vision</span>, pp. 895–903, 2019.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
I. Joshi, A. Anand, S. D. Roy, and P. K. Kalra, “On training generative
adversarial network for enhancement of latent fingerprints,” in <span id="bib.bib142.1.1" class="ltx_text ltx_font_italic">AI and
Deep Learning in Biometric Security</span>, pp. 51–79, 2021.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
E. Richardson, M. Sela, and R. Kimmel, “3d face reconstruction by learning
from synthetic data,” in <span id="bib.bib143.1.1" class="ltx_text ltx_font_italic">4th Intl. Conf. on 3D vision</span>, pp. 460–469,
2016.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
S. Basak, H. Javidnia, F. Khan, R. McDonnell, and M. Schukat, “Methodology for
building synthetic datasets with virtual humans,” in <span id="bib.bib144.1.1" class="ltx_text ltx_font_italic">31st Irish Signals
and Systems Conf.</span>, pp. 1–6, 2020.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
S. Park, X. Zhang, A. Bulling, and O. Hilliges, “Learning to find eye region
landmarks for remote gaze estimation in unconstrained settings,” in <span id="bib.bib145.1.1" class="ltx_text ltx_font_italic">Proc. of the 2018 ACM Symposium on Eye Tracking Research &amp; Applications</span>,
pp. 1–10, 2018.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
C. Roberto de Souza, A. Gaidon, Y. Cabon, and A. Manuel Lopez, “Procedural
generation of videos to train deep action recognition networks,” in <span id="bib.bib146.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition</span>,
pp. 4757–4767, 2017.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
Y. Xu, Y. Wang, J. Liang, and Y. Jiang, “Augmentation data synthesis via gans:
Boosting latent fingerprint reconstruction,” in <span id="bib.bib147.1.1" class="ltx_text ltx_font_italic">IEEE Intl. Conf. on
Acoustics, Speech and Signal Processing</span>, pp. 2932–2936, 2020.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
D. Anghelone, S. Lannes, V. Strizhkova, P. Faure, C. Chen, and A. Dantcheva,
“TFLD: Thermal face and landmark detection for unconstrained
cross-spectral face recognition,” in <span id="bib.bib148.1.1" class="ltx_text ltx_font_italic">Intl. Joint Conf. on Biometrics</span>,
2022.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
E. Richardson, M. Sela, R. Or-El, and R. Kimmel, “Learning detailed face
reconstruction from a single image,” in <span id="bib.bib149.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE Conf. on
Computer Vision and Pattern Recognition</span>, pp. 1259–1268, 2017.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
S. Zhang, R. He, Z. Sun, and T. Tan, “Multi-task convnet for blind face
inpainting with application to face verification,” in <span id="bib.bib150.1.1" class="ltx_text ltx_font_italic">Intl. Conf. on
Biometrics</span>, pp. 1–8, 2016.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
V. G. T. da Costa, G. Zara, P. Rota, T. Oliveira-Santos, N. Sebe, V. Murino,
and E. Ricci, “Dual-head contrastive domain adaptation for video action
recognition,” in <span id="bib.bib151.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF Winter Conf. on Applications of
Computer Vision</span>, pp. 1181–1190, 2022.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
I. Joshi, A. Utkarsh, R. Kothari, V. K. Kurmi, A. Dantcheva, S. D. Roy, and
P. K. Kalra, “Sensor-invariant fingerprint roi segmentation using recurrent
adversarial learning,” in <span id="bib.bib152.1.1" class="ltx_text ltx_font_italic">Intl. Joint Conf. on Neural Networks</span>,
pp. 1–8, 2021.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
E. Bondi, R. Jain, P. Aggrawal, S. Anand, R. Hannaford, A. Kapoor, J. Piavis,
S. Shah, L. Joppa, B. Dilkina, <span id="bib.bib153.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Birdsai: A dataset for
detection and tracking in aerial thermal infrared videos,” in <span id="bib.bib153.2.2" class="ltx_text ltx_font_italic">Proc. of
the IEEE/CVF Winter Conf. on Applications of Computer Vision</span>,
pp. 1747–1756, 2020.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
Y. Zhong, Y. Pei, P. Li, Y. Guo, G. Ma, M. Liu, W. Bai, W. Wu, and H. Zha,
“Depth-based 3d face reconstruction and pose estimation using
shape-preserving domain adaptation,” <span id="bib.bib154.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Biometrics,
Behavior, and Identity Science</span>, vol. 3, no. 1, pp. 6–15, 2020.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
S. Bak, P. Carr, and J.-F. Lalonde, “Domain adaptation through synthesis for
unsupervised person re-identification,” in <span id="bib.bib155.1.1" class="ltx_text ltx_font_italic">Proc. of the European Conf.
on Computer Vision</span>, pp. 189–205, 2018.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
F. Kuhnke and J. Ostermann, “Deep head pose estimation using synthetic images
and partial adversarial domain adaption for continuous label spaces,” in
<span id="bib.bib156.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF Intl. Conf. on Computer Vision</span>, pp. 10164–10173,
2019.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
V. A. Sindagi and V. M. Patel, “Ha-ccn: Hierarchical attention-based crowd
counting network,” <span id="bib.bib157.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Image Processing</span>, vol. 29,
pp. 323–335, 2019.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
E. Mequanint, S. Zhang, B. Forutanpour, Y. Qi, and N. Bi, “Weakly-supervised
degree of eye-closeness estimation,” in <span id="bib.bib158.1.1" class="ltx_text ltx_font_italic">IEEE/CVF Intl. Conf. on
Computer Vision Workshop</span>, pp. 4416–4424, 2019.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
R. Zhang, C. Mu, M. Xu, L. Xu, and X. Xu, “Facial component-landmark detection
with weakly-supervised lr-cnn,” <span id="bib.bib159.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, vol. 7, pp. 10263–10277,
2019.

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
P. N. Belhumeur, D. W. Jacobs, D. J. Kriegman, and N. Kumar, “Localizing parts
of faces using a consensus of exemplars,” <span id="bib.bib160.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern
Analysis and Machine Intelligence</span>, vol. 35, no. 12, pp. 2930–2940, 2013.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
J. J. Engelsma, K. Cao, and A. K. Jain, “Learning a fixed-length fingerprint
representation,” <span id="bib.bib161.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on pattern analysis and machine
intelligence</span>, vol. 43, no. 6, pp. 1981–1997, 2019.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
G. P. Fiumara, P. A. Flanagan, J. D. Grantham, K. Ko, K. Marshall, M. Schwarz,
E. Tabassi, B. Woodgate, C. Boehnen, <span id="bib.bib162.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Nist special database
302: Nail to nail fingerprint challenge,” 2019.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
J. Deng, “A large-scale hierarchical image database,” <span id="bib.bib163.1.1" class="ltx_text ltx_font_italic">Proc. of IEEE
Computer Vision and Pattern Recognition, 2009</span>, 2009.

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
J. Svoboda, P. Astolfi, D. Boscaini, J. Masci, and M. Bronstein, “Clustered
dynamic graph cnn for biometric 3d hand shape recognition,” in <span id="bib.bib164.1.1" class="ltx_text ltx_font_italic">2020
IEEE Intl. Joint Conf. on Biometrics (IJCB)</span>, pp. 1–9, IEEE, 2020.

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
R. Ranjan, S. De Mello, and J. Kautz, “Light-weight head pose invariant gaze
tracking,” in <span id="bib.bib165.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE Conf. on Computer Vision and Pattern
Recognition Workshops</span>, pp. 2156–2164, 2018.

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
I. B. Barbosa, M. Cristani, B. Caputo, A. Rognhaugen, and T. Theoharis,
“Looking beyond appearances: Synthetic training data for deep cnns in
re-identification,” <span id="bib.bib166.1.1" class="ltx_text ltx_font_italic">Computer Vision and Image Understanding</span>, vol. 167,
pp. 50–62, 2018.

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
H. Kazemi, S. Soleymani, A. Dabouei, M. Iranmanesh, and N. M. Nasrabadi,
“Attribute-centered loss for soft-biometrics guided face sketch-photo
recognition,” in <span id="bib.bib167.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE Conf. on Computer Vision and Pattern
Recognition Workshops</span>, pp. 499–507, 2018.

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
P. Dou, S. K. Shah, and I. A. Kakadiaris, “End-to-end 3d face reconstruction
with deep neural networks,” in <span id="bib.bib168.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE Conf. on Computer
Vision and Pattern Recognition</span>, pp. 5908–5917, 2017.

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
O. M. Parkhi, A. Vedaldi, and A. Zisserman, “Deep face recognition,” 2015.

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
J. Kim, M. Ra, and W.-Y. Kim, “A dcnn-based fast nir face recognition system
robust to reflected light from eyeglasses,” <span id="bib.bib170.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, vol. 8,
pp. 80948–80963, 2020.

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
D. Yi, Z. Lei, S. Liao, and S. Z. Li, “Learning face representation from
scratch,” <span id="bib.bib171.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1411.7923</span>, 2014.

</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
W.-F. Ou, L.-M. Po, C. Zhou, Y. A. U. Rehman, P.-F. Xian, and Y.-J. Zhang,
“Fusion loss and inter-class data augmentation for deep finger vein feature
learning,” <span id="bib.bib172.1.1" class="ltx_text ltx_font_italic">Expert Systems with Applications</span>, vol. 171, p. 114584,
2021.

</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
M. Fang, N. Damer, F. Boutros, F. Kirchbuchner, and A. Kuijper, “The
overlapping effect and fusion protocols of data augmentation techniques in
iris pad,” <span id="bib.bib173.1.1" class="ltx_text ltx_font_italic">Machine Vision and Applications</span>, vol. 33, no. 1, pp. 1–21,
2022.

</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
T. Rohrer and J. Kolberg, “Gan pretraining for deep convolutional autoencoders
applied to software-based fingerprint presentation attack detection,” <span id="bib.bib174.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2105.10213</span>, 2021.

</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
M. Ryoo, K. Kim, and H. Yang, “Extreme low resolution activity recognition
with multi-siamese embedding learning,” in <span id="bib.bib175.1.1" class="ltx_text ltx_font_italic">Proc. of the AAAI Conf. on
Artificial Intelligence</span>, vol. 32, 2018.

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
P. C. Neto, F. Boutros, J. R. Pinto, N. Darner, A. F. Sequeira, and J. S.
Cardoso, “Focusface: Multi-task contrastive learning for masked face
recognition,” in <span id="bib.bib176.1.1" class="ltx_text ltx_font_italic">16th IEEE Intl. Conf. on Automatic Face and Gesture
Recognition</span>, pp. 01–08, 2021.

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
J. Huh, H. S. Heo, J. Kang, S. Watanabe, and J. S. Chung, “Augmentation
adversarial training for self-supervised speaker recognition,” <span id="bib.bib177.1.1" class="ltx_text ltx_font_italic">arXiv
preprint arXiv:2007.12085</span>, 2020.

</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
Y.-L. Lee, M.-Y. Tseng, Y.-C. Luo, D.-R. Yu, and W.-C. Chiu, “Learning face
recognition unsupervisedly by disentanglement and self-augmentation,” in
<span id="bib.bib178.1.1" class="ltx_text ltx_font_italic">IEEE Intl. Conf. on Robotics and Automation</span>, pp. 3018–3024, 2020.

</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
J. R. Pinto and J. S. Cardoso, “Self-learning with stochastic triplet loss,”
in <span id="bib.bib179.1.1" class="ltx_text ltx_font_italic">Intl. Joint Conf. on Neural Networks</span>, pp. 1–8, 2020.

</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
H. Zhou, J. Liu, Z. Liu, Y. Liu, and X. Wang, “Rotate-and-render: Unsupervised
photorealistic face rotation from single-view images,” in <span id="bib.bib180.1.1" class="ltx_text ltx_font_italic">Proc. of the
IEEE/CVF Conf. on Computer Vision and Pattern Recognition</span>, pp. 5911–5920,
2020.

</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
T. Zhao, X. Xu, M. Xu, H. Ding, Y. Xiong, and W. Xia, “Learning
self-consistency for deepfake detection,” in <span id="bib.bib181.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF
Intl. Conf. on Computer Vision</span>, pp. 15023–15033, 2021.

</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
Y. Li, Y. Gao, B. Chen, Z. Zhang, G. Lu, and D. Zhang, “Self-supervised
exclusive-inclusive interactive learning for multi-label facial expression
recognition in the wild,” <span id="bib.bib182.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Circuits and Systems for Video
Technology</span>, 2021.

</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
Y.-J. Ju, G.-H. Lee, J.-H. Hong, and S.-W. Lee, “Complete face recovery gan:
Unsupervised joint face rotation and de-occlusion from a single-view image,”
in <span id="bib.bib183.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF Winter Conf. on Applications of Computer
Vision</span>, pp. 3711–3721, 2022.

</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
A. Zhao, J. Dong, and H. Zhou, “Self-supervised learning from multi-sensor
data for sleep recognition,” <span id="bib.bib184.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, vol. 8, pp. 93907–93921,
2020.

</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
S. Ge, S. Zhao, X. Gao, and J. Li, “Fewer-shots and lower-resolutions: Towards
ultrafast face recognition in the wild,” in <span id="bib.bib185.1.1" class="ltx_text ltx_font_italic">Proc. of the 27th ACM Intl.
Conf. on Multimedia</span>, pp. 229–237, 2019.

</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock">
A. Bansal, A. Nanduri, C. D. Castillo, R. Ranjan, and R. Chellappa, “Umdfaces:
An annotated face dataset for training deep networks,” in <span id="bib.bib186.1.1" class="ltx_text ltx_font_italic">IEEE Intl.
Joint Conf. on Biometrics</span>, pp. 464–473, 2017.

</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock">
Y.-T. Cao, J. Wang, and D. Tao, “Symbiotic adversarial learning for
attribute-based person search,” in <span id="bib.bib187.1.1" class="ltx_text ltx_font_italic">European Conf. on Computer Vision</span>,
pp. 230–247, Springer, 2020.

</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock">
P. Korshunov and S. Marcel, “Improving generalization of deepfake detection
with data farming and few-shot learning,” <span id="bib.bib188.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Biometrics,
Behavior, and Identity Science</span>, 2022.

</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock">
J. Solano, L. Tengana, A. Castelblanco, E. Rivera, C. Lopez, and M. Ochoa, “A
few-shot practical behavioral biometrics model for login authentication in
web applications,” in <span id="bib.bib189.1.1" class="ltx_text ltx_font_italic">NDSS Workshop on Measurements, Attacks, and
Defenses for the Web</span>, 2020.

</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock">
R. Tolosana, P. Delgado-Santos, A. Perez-Uribe, R. Vera-Rodriguez, J. Fierrez,
and A. Morales, “Deepwritesyn: On-line handwriting synthesis via deep
short-term representations,” in <span id="bib.bib190.1.1" class="ltx_text ltx_font_italic">Proc. AAAI Conf. on Artificial
Intelligence</span>, 2021.

</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock">
M. Georgopoulos, J. Oldfield, M. A. Nicolaou, Y. Panagakis, and M. Pantic,
“Mitigating demographic bias in facial datasets with style-based
multi-attribute transfer,” <span id="bib.bib191.1.1" class="ltx_text ltx_font_italic">Intl. Journal of Computer Vision</span>, vol. 129,
no. 7, pp. 2288–2307, 2021.

</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock">
P. Drozdowski, C. Rathgeb, A. Dantcheva, N. Damer, and C. Busch, “Demographic
bias in biometrics: A survey on an emerging challenge,” <span id="bib.bib192.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on
Technology and Society</span>, vol. 1, no. 2, pp. 89–103, 2020.

</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[193]</span>
<span class="ltx_bibblock">
K. Ricanek and T. Tesafaye, “Morph: A longitudinal image database of normal
adult age-progression,” in <span id="bib.bib193.1.1" class="ltx_text ltx_font_italic">7th Intl. Conf. on Automatic Face and
Gesture Recognition</span>, pp. 341–345, 2006.

</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[194]</span>
<span class="ltx_bibblock">
A. Pumarola, A. Agudo, A. M. Martinez, A. Sanfeliu, and F. Moreno-Noguer,
“Ganimation: One-shot anatomically consistent facial animation,” <span id="bib.bib194.1.1" class="ltx_text ltx_font_italic">Intl.
Journal of Computer Vision</span>, vol. 128, no. 3, pp. 698–713, 2020.

</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[195]</span>
<span class="ltx_bibblock">
A. Kortylewski, B. Egger, A. Schneider, T. Gerig, A. Morel-Forster, and
T. Vetter, “Empirically analyzing the effect of dataset biases on deep face
recognition systems,” in <span id="bib.bib195.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE Conf. on Computer Vision and
Pattern Recognition Workshops</span>, pp. 2093–2102, 2018.

</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[196]</span>
<span class="ltx_bibblock">
T. de Freitas Pereira and S. Marcel, “Fairness in biometrics: a figure of
merit to assess biometric verification systems,” <span id="bib.bib196.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:2011.02395</span>, 2020.

</span>
</li>
<li id="bib.bib197" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[197]</span>
<span class="ltx_bibblock">
D. McDuff, X. Liu, J. Hernandez, E. Wood, and T. Baltrusaitis, “Synthetic data
for multi-parameter camera-based physiological sensing,” in <span id="bib.bib197.1.1" class="ltx_text ltx_font_italic">43rd Annual
Intl. Conf. of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</span>,
pp. 3742–3748, 2021.

</span>
</li>
<li id="bib.bib198" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[198]</span>
<span class="ltx_bibblock">
P. Bontrager, A. Roy, J. Togelius, N. Memon, and A. Ross, “Deepmasterprints:
Generating masterprints for dictionary attacks via latent variable
evolution,” in <span id="bib.bib198.1.1" class="ltx_text ltx_font_italic">IEEE 9th Intl. Conf. on Biometrics Theory, Applications
and Systems</span>, pp. 1–9, 2018.

</span>
</li>
<li id="bib.bib199" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[199]</span>
<span class="ltx_bibblock">
X. Jia, X. Yang, Y. Zang, N. Zhang, and J. Tian, “A cross-device matching
fingerprint database from multi-type sensors,” in <span id="bib.bib199.1.1" class="ltx_text ltx_font_italic">Proc. of the 21st
Intl. Conf. on Pattern Recognition</span>, pp. 3001–3004, 2012.

</span>
</li>
<li id="bib.bib200" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[200]</span>
<span class="ltx_bibblock">
N. Kohli, D. Yadav, M. Vatsa, R. Singh, and A. Noore, “Synthetic iris
presentation attack using idcgan,” in <span id="bib.bib200.1.1" class="ltx_text ltx_font_italic">IEEE Intl. Joint Conf. on
Biometrics</span>, pp. 674–680, 2017.

</span>
</li>
<li id="bib.bib201" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[201]</span>
<span class="ltx_bibblock">
F. Boutros, N. Damer, K. Raja, R. Ramachandra, F. Kirchbuchner, and A. Kuijper,
“Iris and periocular biometrics for head mounted displays: Segmentation,
recognition, and synthetic data generation,” <span id="bib.bib201.1.1" class="ltx_text ltx_font_italic">Image and Vision
Computing</span>, vol. 104, p. 104007, 2020.

</span>
</li>
<li id="bib.bib202" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[202]</span>
<span class="ltx_bibblock">
N. Damer, F. Boutros, A. M. Saladie, F. Kirchbuchner, and A. Kuijper,
“Realistic dreams: Cascaded enhancement of gan-generated images with an
example in face morphing attacks,” in <span id="bib.bib202.1.1" class="ltx_text ltx_font_italic">IEEE 10th Intl. Conf. on
Biometrics Theory, Applications and Systems</span>, pp. 1–10, 2019.

</span>
</li>
<li id="bib.bib203" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[203]</span>
<span class="ltx_bibblock">
H. H. Nguyen, J. Yamagishi, I. Echizen, and S. Marcel, “Generating master
faces for use in performing wolf attacks on face recognition systems,” in
<span id="bib.bib203.1.1" class="ltx_text ltx_font_italic">IEEE Intl. Joint Conf. on Biometrics</span>, pp. 1–10, 2020.

</span>
</li>
<li id="bib.bib204" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[204]</span>
<span class="ltx_bibblock">
H. Zhang, S. Venkatesh, R. Ramachandra, K. Raja, N. Damer, and C. Busch,
“Mipgan—generating strong and high quality morphing attacks using identity
prior driven gan,” <span id="bib.bib204.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Biometrics, Behavior, and Identity
Science</span>, vol. 3, no. 3, pp. 365–383, 2021.

</span>
</li>
<li id="bib.bib205" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[205]</span>
<span class="ltx_bibblock">
N. Damer, K. Raja, M. Süßmilch, S. Venkatesh, F. Boutros, M. Fang,
F. Kirchbuchner, R. Ramachandra, and A. Kuijper, “Regenmorph: visibly
realistic gan generated face morphing attacks by attack re-generation,” in
<span id="bib.bib205.1.1" class="ltx_text ltx_font_italic">Intl. Symposium on Visual Computing</span>, pp. 251–264, 2021.

</span>
</li>
<li id="bib.bib206" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[206]</span>
<span class="ltx_bibblock">
R. Bouzaglo and Y. Keller, “Synthesis and reconstruction of fingerprints using
generative adversarial networks,” <span id="bib.bib206.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2201.06164</span>,
2022.

</span>
</li>
<li id="bib.bib207" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[207]</span>
<span class="ltx_bibblock">
T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila,
“Analyzing and improving the image quality of stylegan,” in <span id="bib.bib207.1.1" class="ltx_text ltx_font_italic">Proc. of
the IEEE/CVF Conf. on Computer Vision and Pattern Recognition</span>,
pp. 8110–8119, 2020.

</span>
</li>
<li id="bib.bib208" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[208]</span>
<span class="ltx_bibblock">
M. Saito, E. Matsumoto, and S. Saito, “Temporal generative adversarial nets
with singular value clipping,” in <span id="bib.bib208.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE Intl. Conf. on
computer vision</span>, pp. 2830–2839, 2017.

</span>
</li>
<li id="bib.bib209" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[209]</span>
<span class="ltx_bibblock">
S. Tulyakov, M.-Y. Liu, X. Yang, and J. Kautz, “Mocogan: Decomposing motion
and content for video generation,” in <span id="bib.bib209.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE Conf. on
Computer Vision and Pattern Recognition</span>, pp. 1526–1535, 2018.

</span>
</li>
<li id="bib.bib210" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[210]</span>
<span class="ltx_bibblock">
Y. Wang, P. Bilinski, F. Bremond, and A. Dantcheva, “G3an: Disentangling
appearance and motion for video generation,” in <span id="bib.bib210.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF
Conf. on Computer Vision and Pattern Recognition</span>, pp. 5264–5273, 2020.

</span>
</li>
<li id="bib.bib211" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[211]</span>
<span class="ltx_bibblock">
Y. Wang, F. Bremond, and A. Dantcheva, “Inmodegan: Interpretable motion
decomposition generative adversarial network for video generation,” <span id="bib.bib211.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2101.03049</span>, 2021.

</span>
</li>
<li id="bib.bib212" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[212]</span>
<span class="ltx_bibblock">
J. Thies, M. Zollhöfer, and M. Nießner, “Deferred neural rendering:
Image synthesis using neural textures,” <span id="bib.bib212.1.1" class="ltx_text ltx_font_italic">ACM Trans. on Graphics</span>,
vol. 38, no. 4, pp. 1–12, 2019.

</span>
</li>
<li id="bib.bib213" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[213]</span>
<span class="ltx_bibblock">
A. Siarohin, S. Lathuilière, S. Tulyakov, E. Ricci, and N. Sebe, “First
order motion model for image animation,” <span id="bib.bib213.1.1" class="ltx_text ltx_font_italic">Adv. in Neural Information
Processing Systems</span>, vol. 32, 2019.

</span>
</li>
<li id="bib.bib214" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[214]</span>
<span class="ltx_bibblock">
Y. Wang, D. Yang, F. Bremond, and A. Dantcheva, “Latent image animator:
Learning to animate images via latent space navigation,” in <span id="bib.bib214.1.1" class="ltx_text ltx_font_italic">Proc. of
the Intl. Conf. on Learning Representations</span>, 2022.

</span>
</li>
<li id="bib.bib215" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[215]</span>
<span class="ltx_bibblock">
R. Tolosana, R. Vera-Rodriguez, J. Fierrez, A. Morales, and J. Ortega-Garcia,
“Deepfakes and beyond: A survey of face manipulation and fake detection,”
<span id="bib.bib215.1.1" class="ltx_text ltx_font_italic">Information Fusion</span>, vol. 64, pp. 131–148, 2020.

</span>
</li>
<li id="bib.bib216" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[216]</span>
<span class="ltx_bibblock">
C. Rathgeb, <span id="bib.bib216.1.1" class="ltx_text ltx_font_italic">Handbook of Digital Face Manipulation and Detection: From
DeepFakes to Morphing Attacks</span>.

</span>
<span class="ltx_bibblock">Springer Nature, 2021.

</span>
</li>
<li id="bib.bib217" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[217]</span>
<span class="ltx_bibblock">
H. Xu, Y. Ma, H.-C. Liu, D. Deb, H. Liu, J.-L. Tang, and A. K. Jain,
“Adversarial attacks and defenses in images, graphs and text: A review,”
<span id="bib.bib217.1.1" class="ltx_text ltx_font_italic">Intl. Journal of Automation and Computing</span>, vol. 17, no. 2,
pp. 151–178, 2020.

</span>
</li>
<li id="bib.bib218" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[218]</span>
<span class="ltx_bibblock">
K. Raja, M. Ferrara, A. Franco, L. Spreeuwers, I. Batskos, F. de Wit,
M. Gomez-Barrero, U. Scherhag, D. Fischer, S. K. Venkatesh, <span id="bib.bib218.1.1" class="ltx_text ltx_font_italic">et al.</span>,
“Morphing attack detection-database, evaluation platform, and
benchmarking,” <span id="bib.bib218.2.2" class="ltx_text ltx_font_italic">IEEE Trans. on Information Forensics and Security</span>,
vol. 16, pp. 4336–4351, 2020.

</span>
</li>
<li id="bib.bib219" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[219]</span>
<span class="ltx_bibblock">
U. Scherhag, A. Nautsch, C. Rathgeb, M. Gomez-Barrero, R. N. Veldhuis,
L. Spreeuwers, M. Schils, D. Maltoni, P. Grother, S. Marcel, <span id="bib.bib219.1.1" class="ltx_text ltx_font_italic">et al.</span>,
“Biometric systems under morphing attacks: Assessment of morphing techniques
and vulnerability reporting,” in <span id="bib.bib219.2.2" class="ltx_text ltx_font_italic">Intl. Conf. of the Biometrics Special
Interest Group</span>, pp. 1–7, IEEE, 2017.

</span>
</li>
<li id="bib.bib220" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[220]</span>
<span class="ltx_bibblock">
S. Venkatesh, R. Ramachandra, K. Raja, and C. Busch, “Face morphing attack
generation &amp; detection: A comprehensive survey,” <span id="bib.bib220.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on
Technology and Society</span>, 2021.

</span>
</li>
<li id="bib.bib221" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[221]</span>
<span class="ltx_bibblock">
U. Scherhag, C. Rathgeb, J. Merkle, R. Breithaupt, and C. Busch, “Face
recognition systems under morphing attacks: A survey,” <span id="bib.bib221.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>,
vol. 7, pp. 23012–23026, 2019.

</span>
</li>
<li id="bib.bib222" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[222]</span>
<span class="ltx_bibblock">
S. Yadav, C. Chen, and A. Ross, “Relativistic discriminator: A one-class
classifier for generalized iris presentation attack detection,” in <span id="bib.bib222.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF Winter Conf. on Applications of Computer Vision</span>,
pp. 2635–2644, 2020.

</span>
</li>
<li id="bib.bib223" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[223]</span>
<span class="ltx_bibblock">
K. Wang, R. Zhao, and Q. Ji, “A hierarchical generative model for eye image
synthesis and eye gaze estimation,” in <span id="bib.bib223.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE Conf. on
Computer Vision and Pattern Recognition</span>, pp. 440–448, 2018.

</span>
</li>
<li id="bib.bib224" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[224]</span>
<span class="ltx_bibblock">
L. An, Z. Qin, X. Chen, and S. Yang, “Multi-level common space learning for
person re-identification,” <span id="bib.bib224.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Circuits and Systems for
Video Technology</span>, vol. 28, no. 8, pp. 1777–1787, 2017.

</span>
</li>
<li id="bib.bib225" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[225]</span>
<span class="ltx_bibblock">
Y. Gao, N. Xiong, W. Yu, and H. J. Lee, “Learning identity-aware face features
across poses based on deep siamese networks,” <span id="bib.bib225.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, vol. 7,
pp. 105789–105799, 2019.

</span>
</li>
<li id="bib.bib226" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[226]</span>
<span class="ltx_bibblock">
C. Fu, X. Wu, Y. Hu, H. Huang, and R. He, “Dvg-face: Dual variational
generation for heterogeneous face recognition,” <span id="bib.bib226.1.1" class="ltx_text ltx_font_italic">IEEE Trans. on Pattern
Analysis and Machine Intelligence</span>, 2021.

</span>
</li>
<li id="bib.bib227" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[227]</span>
<span class="ltx_bibblock">
M. Ariz, J. J. Bengoechea, A. Villanueva, and R. Cabeza, “A novel 2d/3d
database with automatic face annotation for head tracking and pose
estimation,” <span id="bib.bib227.1.1" class="ltx_text ltx_font_italic">Computer Vision and Image Understanding</span>, vol. 148,
pp. 201–210, 2016.

</span>
</li>
<li id="bib.bib228" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[228]</span>
<span class="ltx_bibblock">
A. Roitberg, D. Schneider, A. Djamal, C. Seibold, S. Reiß, and
R. Stiefelhagen, “Let’s play for action: Recognizing activities of daily
living by learning from life simulation video games,” 2021.

</span>
</li>
<li id="bib.bib229" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[229]</span>
<span class="ltx_bibblock">
T. Li, T. Bolkart, M. J. Black, H. Li, and J. Romero, “Learning a model of
facial shape and expression from 4d scans,” <span id="bib.bib229.1.1" class="ltx_text ltx_font_italic">ACM Trans. Graph.</span>,
vol. 36, no. 6, pp. 194–1, 2017.

</span>
</li>
<li id="bib.bib230" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[230]</span>
<span class="ltx_bibblock">
Y. Feng, H. Feng, M. J. Black, and T. Bolkart, “Learning an animatable
detailed 3d face model from in-the-wild images,” <span id="bib.bib230.1.1" class="ltx_text ltx_font_italic">ACM Trans. on
Graphics</span>, vol. 40, no. 4, pp. 1–13, 2021.

</span>
</li>
<li id="bib.bib231" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[231]</span>
<span class="ltx_bibblock">
T. Gerig, A. Morel-Forster, C. Blumer, B. Egger, M. Luthi, S. Schönborn,
and T. Vetter, “Morphable face models-an open framework,” in <span id="bib.bib231.1.1" class="ltx_text ltx_font_italic">13th IEEE
Intl. Conf. on Automatic Face &amp; Gesture Recognition</span>, pp. 75–82, 2018.

</span>
</li>
<li id="bib.bib232" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[232]</span>
<span class="ltx_bibblock">
E. R. Chan, C. Z. Lin, M. A. Chan, K. Nagano, B. Pan, S. De Mello, O. Gallo,
L. J. Guibas, J. Tremblay, S. Khamis, <span id="bib.bib232.1.1" class="ltx_text ltx_font_italic">et al.</span>, “Efficient
geometry-aware 3d generative adversarial networks,” in <span id="bib.bib232.2.2" class="ltx_text ltx_font_italic">Proc. of the
IEEE/CVF Conf. on Computer Vision and Pattern Recognition</span>, pp. 16123–16133,
2022.

</span>
</li>
<li id="bib.bib233" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[233]</span>
<span class="ltx_bibblock">
S. Seneviratne, N. Kasthuriaarachchi, and S. Rasnayaka, “Multi-dataset
benchmarks for masked identification using contrastive representation
learning,” <span id="bib.bib233.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.05596</span>, 2021.

</span>
</li>
<li id="bib.bib234" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[234]</span>
<span class="ltx_bibblock">
M. Grimmer, H. Zhang, R. Ramachandra, K. Raja, and C. Busch, “Generation of
non-deterministic synthetic face datasets guided by identity priors,” <span id="bib.bib234.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2112.03632</span>, 2021.

</span>
</li>
<li id="bib.bib235" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[235]</span>
<span class="ltx_bibblock">
ISO/IEC JTC1 SC37 Biometrics, <span id="bib.bib235.1.1" class="ltx_text ltx_font_italic">ISO/IEC WD 19795-10:E. Information
Technology – Biometric Performance Testing and Reporting – Part 10:
Quantifying biometric system performance variation across demographic
groups</span>.

</span>
<span class="ltx_bibblock">Intl. Organization for Standardization.

</span>
</li>
<li id="bib.bib236" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[236]</span>
<span class="ltx_bibblock">
C. Gottschlich and S. Huckemann, “Separating the real from the synthetic:
minutiae histograms as fingerprints of fingerprints,” <span id="bib.bib236.1.1" class="ltx_text ltx_font_italic">IET Biometrics</span>,
vol. 3, no. 4, pp. 291–301.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2208.09190" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2208.09191" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2208.09191">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2208.09191" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2208.09192" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar 13 19:53:57 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
